# Arxiv Papers in cs.CV on 2011-08-25
### Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1108.4973v12
- **DOI**: 10.3390/e16021002
- **Categories**: **cs.IT**, cs.AI, cs.CV, math.IT, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1108.4973v12)
- **Published**: 2011-08-25 00:50:42+00:00
- **Updated**: 2013-10-16 20:35:39+00:00
- **Authors**: Alexandre L. M. Levada
- **Comment**: 46 pages, 16 Figures
- **Journal**: Entropy, v. 16, n. 2, Special Issue on Information Geometry, 2014
- **Summary**: Markov Random Field models are powerful tools for the study of complex systems. However, little is known about how the interactions between the elements of such systems are encoded, especially from an information-theoretic perspective. In this paper, our goal is to enlight the connection between Fisher information, Shannon entropy, information geometry and the behavior of complex systems modeled by isotropic pairwise Gaussian Markov random fields. We propose analytical expressions to compute local and global versions of these measures using Besag's pseudo-likelihood function, characterizing the system's behavior through its \emph{Fisher curve}, a parametric trajectory accross the information space that provides a geometric representation for the study of complex systems. Computational experiments show how the proposed tools can be useful in extrating relevant information from complex patterns. The obtained results quantify and support our main conclusion, which is: in terms of information, moving towards higher entropy states (A --> B) is different from moving towards lower entropy states (B --> A), since the \emph{Fisher curves} are not the same given a natural orientation (the direction of time).



