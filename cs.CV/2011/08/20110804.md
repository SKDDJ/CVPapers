# Arxiv Papers in cs.CV on 2011-08-04
### Leveraging Billions of Faces to Overcome Performance Barriers in Unconstrained Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1108.1122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1108.1122v1)
- **Published**: 2011-08-04 15:51:19+00:00
- **Updated**: 2011-08-04 15:51:19+00:00
- **Authors**: Yaniv Taigman, Lior Wolf
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: We employ the face recognition technology developed in house at face.com to a well accepted benchmark and show that without any tuning we are able to considerably surpass state of the art results. Much of the improvement is concentrated in the high-valued performance point of zero false positive matches, where the obtained recall rate almost doubles the best reported result to date. We discuss the various components and innovations of our system that enable this significant performance gap. These components include extensive utilization of an accurate 3D reconstructed shape model dealing with challenges arising from pose and illumination. In addition, discriminative models based on billions of faces are used in order to overcome aging and facial expression as well as low light and overexposure. Finally, we identify a challenging set of identification queries that might provide useful focus for future research.



### Learning Representations by Maximizing Compression
- **Arxiv ID**: http://arxiv.org/abs/1108.1169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1108.1169v1)
- **Published**: 2011-08-04 19:00:14+00:00
- **Updated**: 2011-08-04 19:00:14+00:00
- **Authors**: Karol Gregor, Yann LeCun
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.



