# Arxiv Papers in cs.CV on 2011-10-02
### Robust artificial neural networks and outlier detection. Technical report
- **Arxiv ID**: http://arxiv.org/abs/1110.0169v1
- **DOI**: 10.1080/02331934.2012.674946
- **Categories**: **math.OC**, cs.CV, cs.NA, cs.NE, math.NA, stat.ME, 90C26, 65D15
- **Links**: [PDF](http://arxiv.org/pdf/1110.0169v1)
- **Published**: 2011-10-02 10:56:07+00:00
- **Updated**: 2011-10-02 10:56:07+00:00
- **Authors**: Gleb Beliakov, Andrei Kelarev, John Yearwood
- **Comment**: None
- **Journal**: None
- **Summary**: Large outliers break down linear and nonlinear regression models. Robust regression methods allow one to filter out the outliers when building a model. By replacing the traditional least squares criterion with the least trimmed squares criterion, in which half of data is treated as potential outliers, one can fit accurate regression models to strongly contaminated data. High-breakdown methods have become very well established in linear regression, but have started being applied for non-linear regression only recently. In this work, we examine the problem of fitting artificial neural networks to contaminated data using least trimmed squares criterion. We introduce a penalized least trimmed squares criterion which prevents unnecessary removal of valid data. Training of ANNs leads to a challenging non-smooth global optimization problem. We compare the efficiency of several derivative-free optimization methods in solving it, and show that our approach identifies the outliers correctly when ANNs are used for nonlinear regression.



### Eclectic Extraction of Propositional Rules from Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1110.0214v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1110.0214v1)
- **Published**: 2011-10-02 18:59:42+00:00
- **Updated**: 2011-10-02 18:59:42+00:00
- **Authors**: Ridwan Al Iqbal
- **Comment**: ICCIT 2011, Dhaka, Bangladesh
- **Journal**: None
- **Summary**: Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a "Black Box" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.



