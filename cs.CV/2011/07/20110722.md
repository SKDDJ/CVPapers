# Arxiv Papers in cs.CV on 2011-07-22
### Constant-time filtering using shiftable kernels
- **Arxiv ID**: http://arxiv.org/abs/1107.4617v4
- **DOI**: 10.1109/LSP.2011.2167967
- **Categories**: **cs.CV**, cs.DS, B.2.4; G.1.2
- **Links**: [PDF](http://arxiv.org/pdf/1107.4617v4)
- **Published**: 2011-07-22 20:18:16+00:00
- **Updated**: 2011-09-08 16:11:25+00:00
- **Authors**: Kunal Narayan Chaudhury
- **Comment**: Accepted in IEEE Signal Processing Letters
- **Journal**: IEEE Signal Processing Letters, vol. 18(11), pp. 651 - 654, 2011
- **Summary**: It was recently demonstrated in [5] that the non-linear bilateral filter [14] can be efficiently implemented using a constant-time or O(1) algorithm. At the heart of this algorithm was the idea of approximating the Gaussian range kernel of the bilateral filter using trigonometric functions. In this letter, we explain how the idea in [5] can be extended to few other linear and non-linear filters [14, 17, 2]. While some of these filters have received a lot of attention in recent years, they are known to be computationally intensive. To extend the idea in [5], we identify a central property of trigonometric functions, called shiftability, that allows us to exploit the redundancy inherent in the filtering operations. In particular, using shiftable kernels, we show how certain complex filtering can be reduced to simply that of computing the moving sum of a stack of images. Each image in the stack is obtained through an elementary pointwise transform of the input image. This has a two-fold advantage. First, we can use fast recursive algorithms for computing the moving sum [15, 6], and, secondly, we can use parallel computation to further speed up the computation. We also show how shiftable kernels can also be used to approximate the (non-shiftable) Gaussian kernel that is ubiquitously used in image filtering.



### On the Hilbert transform of wavelets
- **Arxiv ID**: http://arxiv.org/abs/1107.4619v1
- **DOI**: None
- **Categories**: **math.FA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1107.4619v1)
- **Published**: 2011-07-22 20:33:13+00:00
- **Updated**: 2011-07-22 20:33:13+00:00
- **Authors**: Kunal Narayan Chaudhury, Michael Unser
- **Comment**: Appears in IEEE Transactions on Signal Processing, vol. 59, no. 4,
  pp. 1890-1894, 2011
- **Journal**: IEEE Transactions on Signal Processing, vol. 19(11), pp. 1890 -
  1894, 2011
- **Summary**: A wavelet is a localized function having a prescribed number of vanishing moments. In this correspondence, we provide precise arguments as to why the Hilbert transform of a wavelet is again a wavelet. In particular, we provide sharp estimates of the localization, vanishing moments, and smoothness of the transformed wavelet. We work in the general setting of non-compactly supported wavelets. Our main result is that, in the presence of some minimal smoothness and decay, the Hilbert transform of a wavelet is again as smooth and oscillating as the original wavelet, whereas its localization is controlled by the number of vanishing moments of the original wavelet. We motivate our results using concrete examples.



### Efficient variational inference in large-scale Bayesian compressed sensing
- **Arxiv ID**: http://arxiv.org/abs/1107.4637v2
- **DOI**: 10.1109/ICCVW.2011.6130406
- **Categories**: **cs.CV**, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1107.4637v2)
- **Published**: 2011-07-22 22:10:38+00:00
- **Updated**: 2011-09-05 02:04:00+00:00
- **Authors**: George Papandreou, Alan Yuille
- **Comment**: 8 pages, 3 figures, appears in Proc. IEEE Workshop on Information
  Theory in Computer Vision and Pattern Recognition (in conjunction with
  ICCV-11), Barcelona, Spain, Nov. 2011
- **Journal**: Proc. IEEE Workshop on Information Theory in Computer Vision and
  Pattern Recognition (in conjunction with ICCV-11), pp. 1332-1339, Barcelona,
  Spain, Nov. 2011
- **Summary**: We study linear models under heavy-tailed priors from a probabilistic viewpoint. Instead of computing a single sparse most probable (MAP) solution as in standard deterministic approaches, the focus in the Bayesian compressed sensing framework shifts towards capturing the full posterior distribution on the latent variables, which allows quantifying the estimation uncertainty and learning model parameters using maximum likelihood. The exact posterior distribution under the sparse linear model is intractable and we concentrate on variational Bayesian techniques to approximate it. Repeatedly computing Gaussian variances turns out to be a key requisite and constitutes the main computational bottleneck in applying variational techniques in large-scale problems. We leverage on the recently proposed Perturb-and-MAP algorithm for drawing exact samples from Gaussian Markov random fields (GMRF). The main technical contribution of our paper is to show that estimating Gaussian variances using a relatively small number of such efficiently drawn random samples is much more effective than alternative general-purpose variance estimation techniques. By reducing the problem of variance estimation to standard optimization primitives, the resulting variational algorithms are fully scalable and parallelizable, allowing Bayesian computations in extremely large-scale problems with the same memory and time complexity requirements as conventional point estimation techniques. We illustrate these ideas with experiments in image deblurring.



