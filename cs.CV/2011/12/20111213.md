# Arxiv Papers in cs.CV on 2011-12-13
### Large Scale Correlation Clustering Optimization
- **Arxiv ID**: http://arxiv.org/abs/1112.2903v1
- **DOI**: None
- **Categories**: **cs.CV**, G.1.6; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1112.2903v1)
- **Published**: 2011-12-13 14:28:12+00:00
- **Updated**: 2011-12-13 14:28:12+00:00
- **Authors**: Shai Bagon, Meirav Galun
- **Comment**: 9 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Clustering is a fundamental task in unsupervised learning. The focus of this paper is the Correlation Clustering functional which combines positive and negative affinities between the data points. The contribution of this paper is two fold: (i) Provide a theoretic analysis of the functional. (ii) New optimization algorithms which can cope with large scale problems (>100K variables) that are infeasible using existing methods. Our theoretic analysis provides a probabilistic generative interpretation for the functional, and justifies its intrinsic "model-selection" capability. Furthermore, we draw an analogy between optimizing this functional and the well known Potts energy minimization. This analogy allows us to suggest several new optimization algorithms, which exploit the intrinsic "model-selection" capability of the functional to automatically recover the underlying number of clusters. We compare our algorithms to existing methods on both synthetic and real data. In addition we suggest two new applications that are made possible by our algorithms: unsupervised face identification and interactive multi-object segmentation by rough boundary delineation.



### Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns
- **Arxiv ID**: http://arxiv.org/abs/1112.2988v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1112.2988v2)
- **Published**: 2011-12-13 18:10:11+00:00
- **Updated**: 2012-06-23 16:35:37+00:00
- **Authors**: Tsvi Achler
- **Comment**: 2 figures, 1 table
- **Journal**: None
- **Summary**: Matching animal-like flexibility in recognition and the ability to quickly incorporate new information remains difficult. Limits are yet to be adequately addressed in neural models and recognition algorithms. This work proposes a configuration for recognition that maintains the same function of conventional algorithms but avoids combinatorial problems. Feedforward recognition algorithms such as classical artificial neural networks and machine learning algorithms are known to be subject to catastrophic interference and forgetting. Modifying or learning new information (associations between patterns and labels) causes loss of previously learned information. I demonstrate using mathematical analysis how supervised generative models, with feedforward and feedback connections, can emulate feedforward algorithms yet avoid catastrophic interference and forgetting. Learned information in generative models is stored in a more intuitive form that represents the fixed points or solutions of the network and moreover displays similar difficulties as cognitive phenomena. Brain-like capabilities and limits associated with generative models suggest the brain may perform recognition and store information using a similar approach. Because of the central role of recognition, progress understanding the underlying principles may reveal significant insight on how to better study and integrate with the brain.



### A new variational principle for the Euclidean distance function: Linear approach to the non-linear eikonal problem
- **Arxiv ID**: http://arxiv.org/abs/1112.3010v4
- **DOI**: None
- **Categories**: **cs.CV**, math.NA, 65D18, 65M80
- **Links**: [PDF](http://arxiv.org/pdf/1112.3010v4)
- **Published**: 2011-12-13 20:03:33+00:00
- **Updated**: 2015-02-08 10:06:36+00:00
- **Authors**: Karthik S. Gurumoorthy, Anand Rangarajan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fast convolution-based technique for computing an approximate, signed Euclidean distance function $S$ on a set of 2D and 3D grid locations. Instead of solving the non-linear, static Hamilton-Jacobi equation ($\|\nabla S\|=1$), our solution stems from first solving for a scalar field $\phi$ in a linear differential equation and then deriving the solution for $S$ by taking the negative logarithm. In other words, when $S$ and $\phi$ are related by $\phi = \exp \left(-\frac{S}{\tau} \right)$ and $\phi$ satisfies a specific linear differential equation corresponding to the extremum of a variational problem, we obtain the approximate Euclidean distance function $S = -\tau \log(\phi)$ which converges to the true solution in the limit as $\tau \rightarrow 0$. This is in sharp contrast to techniques like the fast marching and fast sweeping methods which directly solve the Hamilton-Jacobi equation by the Godunov upwind discretization scheme. Our linear formulation results in a closed-form solution to the approximate Euclidean distance function expressible as a discrete convolution, and hence efficiently computable using the fast Fourier transform (FFT). Our solution also circumvents the need for spatial discretization of the derivative operator. As $\tau\rightarrow0$ we show the convergence of our results to the true solution and also bound the error for a given value of $\tau$. The differentiability of our solution allows us to compute---using a set of convolutions---the first and second derivatives of the approximate distance function. In order to determine the sign of the distance function (defined to be positive inside a closed region and negative outside), we compute the winding number in 2D and the topological degree in 3D, whose computations can also be performed via fast convolutions. We demonstrate the efficacy of our method through a set of experimental results.



### Data Processing For Atomic Resolution EELS
- **Arxiv ID**: http://arxiv.org/abs/1112.3059v1
- **DOI**: 10.1017/S1431927612006708
- **Categories**: **cond-mat.mtrl-sci**, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1112.3059v1)
- **Published**: 2011-12-13 22:18:52+00:00
- **Updated**: 2011-12-13 22:18:52+00:00
- **Authors**: Paul Cueva, Robert Hovden, Julia A. Mundy, Huolin L. Xin, David A. Muller
- **Comment**: None
- **Journal**: Microscopy and Microanalysis, Vol. 18 pp 667-675 (2012)
- **Summary**: The high beam current and sub-angstrom resolution of aberration-corrected scanning transmission electron microscopes has enabled electron energy loss spectroscopic (EELS) mapping with atomic resolution. These spectral maps are often dose-limited and spatially oversampled, leading to low counts/channel and are thus highly sensitive to errors in background estimation. However, by taking advantage of redundancy in the dataset map one can improve background estimation and increase chemical sensitivity. We consider two such approaches- linear combination of power laws and local background averaging-that reduce background error and improve signal extraction. Principal components analysis (PCA) can also be used to analyze spectrum images, but the poor peak-to-background ratio in EELS can lead to serious artifacts if raw EELS data is PCA filtered. We identify common artifacts and discuss alternative approaches. These algorithms are implemented within the Cornell Spectrum Imager, an open source software package for spectroscopic analysis.



