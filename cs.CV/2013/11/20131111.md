# Arxiv Papers in cs.CV on 2013-11-11
### Notes on Elementary Spectral Graph Theory. Applications to Graph Clustering Using Normalized Cuts
- **Arxiv ID**: http://arxiv.org/abs/1311.2492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1311.2492v1)
- **Published**: 2013-11-11 16:45:03+00:00
- **Updated**: 2013-11-11 16:45:03+00:00
- **Authors**: Jean Gallier
- **Comment**: 76 pages
- **Journal**: None
- **Summary**: These are notes on the method of normalized graph cuts and its applications to graph clustering. I provide a fairly thorough treatment of this deeply original method due to Shi and Malik, including complete proofs. I include the necessary background on graphs and graph Laplacians. I then explain in detail how the eigenvectors of the graph Laplacian can be used to draw a graph. This is an attractive application of graph Laplacians. The main thrust of this paper is the method of normalized cuts. I give a detailed account for K = 2 clusters, and also for K > 2 clusters, based on the work of Yu and Shi. Three points that do not appear to have been clearly articulated before are elaborated:   1. The solutions of the main optimization problem should be viewed as tuples in the K-fold cartesian product of projective space RP^{N-1}.   2. When K > 2, the solutions of the relaxed problem should be viewed as elements of the Grassmannian G(K,N).   3. Two possible Riemannian distances are available to compare the closeness of solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on the Grassmannian.   I also clarify what should be the necessary and sufficient conditions for a matrix to represent a partition of the vertices of a graph to be clustered.



### Rich feature hierarchies for accurate object detection and semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1311.2524v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1311.2524v5)
- **Published**: 2013-11-11 18:43:49+00:00
- **Updated**: 2014-10-22 17:23:20+00:00
- **Authors**: Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik
- **Comment**: Extended version of our CVPR 2014 paper; latest update (v5) includes
  results using deeper networks (see Appendix G. Changelog)
- **Journal**: None
- **Summary**: Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.



### Performing edge detection by difference of Gaussians using q-Gaussian kernels
- **Arxiv ID**: http://arxiv.org/abs/1311.2561v2
- **DOI**: 10.1088/1742-6596/490/1/012020
- **Categories**: **cs.CV**, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/1311.2561v2)
- **Published**: 2013-11-11 20:14:11+00:00
- **Updated**: 2013-11-12 02:48:51+00:00
- **Authors**: Lucas Assirati, NÃºbia R. da Silva, Lilian Berton, Alneu de A. Lopes, Odemir M. Bruno
- **Comment**: 5 pages, 5 figures, IC-MSQUARE 2013
- **Journal**: None
- **Summary**: In image processing, edge detection is a valuable tool to perform the extraction of features from an image. This detection reduces the amount of information to be processed, since the redundant information (considered less relevant) can be unconsidered. The technique of edge detection consists of determining the points of a digital image whose intensity changes sharply. This changes are due to the discontinuities of the orientation on a surface for example. A well known method of edge detection is the Difference of Gaussians (DoG). The method consists of subtracting two Gaussians, where a kernel has a standard deviation smaller than the previous one. The convolution between the subtraction of kernels and the input image results in the edge detection of this image. This paper introduces a method of extracting edges using DoG with kernels based on the q-Gaussian probability distribution, derived from the q-statistic proposed by Constantino Tsallis. To demonstrate the method's potential, we compare the introduced method with the traditional DoG using Gaussians kernels. The results showed that the proposed method can extract edges with more accurate details.



### Stitched Panoramas from Toy Airborne Video Cameras
- **Arxiv ID**: http://arxiv.org/abs/1311.6500v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3.3; I.4; J.5
- **Links**: [PDF](http://arxiv.org/pdf/1311.6500v1)
- **Published**: 2013-11-11 20:32:50+00:00
- **Updated**: 2013-11-11 20:32:50+00:00
- **Authors**: Camille Goudeseune
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: Effective panoramic photographs are taken from vantage points that are high. High vantage points have recently become easier to reach as the cost of quadrotor helicopters has dropped to nearly disposable levels. Although cameras carried by such aircraft weigh only a few grams, their low-quality video can be converted into panoramas of high quality and high resolution. Also, the small size of these aircraft vastly reduces the risks inherent to flight.



### Determining Leishmania Infection Levels by Automatic Analysis of Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1311.2621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1311.2621v1)
- **Published**: 2013-11-11 21:42:51+00:00
- **Updated**: 2013-11-11 21:42:51+00:00
- **Authors**: P. A Nogueira
- **Comment**: MSc thesis, 105 pages
- **Journal**: None
- **Summary**: Analysis of microscopy images is one important tool in many fields of biomedical research, as it allows the quantification of a multitude of parameters at the cellular level. However, manual counting of these images is both tiring and unreliable and ultimately very time-consuming for biomedical researchers. Not only does this slow down the overall research process, it also introduces counting errors due to a lack of objectivity and consistency inherent to the researchers own human nature.   This thesis addresses this issue by automatically determining infection indexes of macrophages parasite by Leishmania in microscopy images using computer vision and pattern recognition methodologies. Initially images are submitted to a pre-processing stage that consists in a normalization of illumination conditions. Three algorithms are then applied in parallel to each image. Algorithm A intends to detect macrophage nuclei and consists of segmentation via adaptive multi-threshold, and classification of resulting regions using a set of collected features. Algorithm B intends to detect parasites and is similar to Algorithm A but the adaptive multi-threshold is parameterized with a different constraints vector. Algorithm C intends to detect the macrophages and parasites cytoplasm and consists of a cut-off version of the previous two algorithms, where the classification step is skipped. Regions with multiple nuclei or parasites are processed by a voting system that employs both a Support Vector Machine and a set of region features for determining the number of objects present in each region. The previous vote is then taken into account as the number of mixtures to be used in a Gaussian Mixture Model to decluster the said region. Finally each parasite is assigned to, at most, a single macrophage using minimum Euclidean distance to a cell nucleus, thus quantifying Leishmania infection levels.



### Second-order Shape Optimization for Geometric Inverse Problems in Vision
- **Arxiv ID**: http://arxiv.org/abs/1311.2626v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1311.2626v5)
- **Published**: 2013-11-11 21:53:28+00:00
- **Updated**: 2014-04-13 18:09:17+00:00
- **Authors**: J. Balzer, S. Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a method for optimization in shape spaces, i.e., sets of surfaces modulo re-parametrization. Unlike previously proposed gradient flows, we achieve superlinear convergence rates through a subtle approximation of the shape Hessian, which is generally hard to compute and suffers from a series of degeneracies. Our analysis highlights the role of mean curvature motion in comparison with first-order schemes: instead of surface area, our approach penalizes deformation, either by its Dirichlet energy or total variation. Latter regularizer sparks the development of an alternating direction method of multipliers on triangular meshes. Therein, a conjugate-gradients solver enables us to bypass formation of the Gaussian normal equations appearing in the course of the overall optimization. We combine all of the aforementioned ideas in a versatile geometric variation-regularized Levenberg-Marquardt-type method applicable to a variety of shape functionals, depending on intrinsic properties of the surface such as normal field and curvature as well as its embedding into space. Promising experimental results are reported.



### Volumetric Reconstruction Applied to Perceptual Studies of Size and Weight
- **Arxiv ID**: http://arxiv.org/abs/1311.2642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1311.2642v1)
- **Published**: 2013-11-11 22:59:33+00:00
- **Updated**: 2013-11-11 22:59:33+00:00
- **Authors**: J. Balzer, M. Peters, S. Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the application of volumetric reconstruction from structured-light sensors in cognitive neuroscience, specifically in the quantification of the size-weight illusion, whereby humans tend to systematically perceive smaller objects as heavier. We investigate the performance of two commercial structured-light scanning systems in comparison to one we developed specifically for this application. Our method has two main distinct features: First, it only samples a sparse series of viewpoints, unlike other systems such as the Kinect Fusion. Second, instead of building a distance field for the purpose of points-to-surface conversion directly, we pursue a first-order approach: the distance function is recovered from its gradient by a screened Poisson reconstruction, which is very resilient to noise and yet preserves high-frequency signal components. Our experiments show that the quality of metric reconstruction from structured light sensors is subject to systematic biases, and highlights the factors that influence it. Our main performance index rates estimates of volume (a proxy of size), for which we review a well-known formula applicable to incomplete meshes. Our code and data will be made publicly available upon completion of the anonymous review process.



