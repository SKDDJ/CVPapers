# Arxiv Papers in cs.CV on 2013-11-25
### On Approximate Inference for Generalized Gaussian Process Models
- **Arxiv ID**: http://arxiv.org/abs/1311.6371v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1311.6371v3)
- **Published**: 2013-11-25 17:22:22+00:00
- **Updated**: 2013-11-27 07:43:48+00:00
- **Authors**: Lifeng Shang, Antoni B. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: A generalized Gaussian process model (GGPM) is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the exponential family distribution (EFD). In this paper, we consider efficient algorithms for approximate inference on GGPMs using the general form of the EFD. A particular GP model and its associated inference algorithms can then be formed by changing the parameters of the EFD, thus greatly simplifying its creation for task-specific output domains. We demonstrate the efficacy of this framework by creating several new GP models for regressing to non-negative reals and to real intervals. We also consider a closed-form Taylor approximation for efficient inference on GGPMs, and elaborate on its connections with other model-specific heuristic closed-form approximations. Finally, we present a comprehensive set of experiments to compare approximate inference algorithms on a wide variety of GGPMs.



### Are all training examples equally valuable?
- **Arxiv ID**: http://arxiv.org/abs/1311.6510v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1311.6510v1)
- **Published**: 2013-11-25 22:59:24+00:00
- **Updated**: 2013-11-25 22:59:24+00:00
- **Authors**: Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: When learning a new concept, not all training examples may prove equally useful for training: some may have higher or lower training value than others. The goal of this paper is to bring to the attention of the vision community the following considerations: (1) some examples are better than others for training detectors or classifiers, and (2) in the presence of better examples, some examples may negatively impact performance and removing them may be beneficial. In this paper, we propose an approach for measuring the training value of an example, and use it for ranking and greedily sorting examples. We test our methods on different vision tasks, models, datasets and classifiers. Our experiments show that the performance of current state-of-the-art detectors and classifiers can be improved when training on a subset, rather than the whole training set.



