# Arxiv Papers in cs.CV on 2013-01-16
### Deep Predictive Coding Networks
- **Arxiv ID**: http://arxiv.org/abs/1301.3541v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1301.3541v3)
- **Published**: 2013-01-16 01:27:15+00:00
- **Updated**: 2013-03-15 19:25:30+00:00
- **Authors**: Rakesh Chalasani, Jose C. Principe
- **Comment**: 13 Pages, 7 figures, submission for ICLR 2013
- **Journal**: None
- **Summary**: The quality of data representation in deep learning methods is directly related to the prior model imposed on the representations; however, generally used fixed priors are not capable of adjusting to the context in the data. To address this issue, we propose deep predictive coding networks, a hierarchical generative model that empirically alters priors on the latent representations in a dynamic and context-sensitive manner. This model captures the temporal dependencies in time-varying signals and uses top-down information to modulate the representation in lower layers. The centerpiece of our model is a novel procedure to infer sparse states of a dynamic model which is used for feature extraction. We also extend this feature extraction block to introduce a pooling function that captures locally invariant representations. When applied on a natural video data, we show that our method is able to learn high-level visual features. We also demonstrate the role of the top-down connections by showing the robustness of the proposed model to structured noise.



### Information Theoretic Learning with Infinitely Divisible Kernels
- **Arxiv ID**: http://arxiv.org/abs/1301.3551v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1301.3551v6)
- **Published**: 2013-01-16 01:49:52+00:00
- **Updated**: 2013-06-04 04:42:39+00:00
- **Authors**: Luis G. Sanchez Giraldo, Jose C. Principe
- **Comment**: Modified submission for International Conference on Learning
  Representations 2013
- **Journal**: None
- **Summary**: In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art.



### Complexity of Representation and Inference in Compositional Models with Part Sharing
- **Arxiv ID**: http://arxiv.org/abs/1301.3560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1301.3560v1)
- **Published**: 2013-01-16 02:29:15+00:00
- **Updated**: 2013-01-16 02:29:15+00:00
- **Authors**: Alan L. Yuille, Roozbeh Mottaghi
- **Comment**: ICLR 2013
- **Journal**: None
- **Summary**: This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., "neurons") for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.



### Indoor Semantic Segmentation using depth information
- **Arxiv ID**: http://arxiv.org/abs/1301.3572v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1301.3572v2)
- **Published**: 2013-01-16 03:31:30+00:00
- **Updated**: 2013-03-14 18:18:17+00:00
- **Authors**: Camille Couprie, Cl√©ment Farabet, Laurent Najman, Yann LeCun
- **Comment**: 8 pages, 3 figures
- **Journal**: None
- **Summary**: This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.



### Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative Clustering
- **Arxiv ID**: http://arxiv.org/abs/1301.3575v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1301.3575v1)
- **Published**: 2013-01-16 03:52:09+00:00
- **Updated**: 2013-01-16 03:52:09+00:00
- **Authors**: Boyi Xie, Shuheng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Large scale agglomerative clustering is hindered by computational burdens. We propose a novel scheme where exact inter-instance distance calculation is replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally, we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time.



### Big Neural Networks Waste Capacity
- **Arxiv ID**: http://arxiv.org/abs/1301.3583v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1301.3583v4)
- **Published**: 2013-01-16 04:45:29+00:00
- **Updated**: 2013-03-14 20:49:20+00:00
- **Authors**: Yann N. Dauphin, Yoshua Bengio
- **Comment**: None
- **Journal**: None
- **Summary**: This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact there are highly diminishing returns for capacity in terms of training error, leading to underfitting. This suggests that the optimization method - first order gradient descent - fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.



### Tree structured sparse coding on cubes
- **Arxiv ID**: http://arxiv.org/abs/1301.3590v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1301.3590v1)
- **Published**: 2013-01-16 05:20:01+00:00
- **Updated**: 2013-01-16 05:20:01+00:00
- **Authors**: Arthur Szlam
- **Comment**: None
- **Journal**: None
- **Summary**: A brief description of tree structured sparse coding on the binary cube.



### Deep Learning for Detecting Robotic Grasps
- **Arxiv ID**: http://arxiv.org/abs/1301.3592v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1301.3592v6)
- **Published**: 2013-01-16 05:33:56+00:00
- **Updated**: 2014-08-21 18:17:37+00:00
- **Authors**: Ian Lenz, Honglak Lee, Ashutosh Saxena
- **Comment**: Current version was accepted to IJRR Special Issue on Robot Vision
  2014 Workshop version accepted to ICLR 2013. Conference version accepted to
  RSS 2013
- **Journal**: None
- **Summary**: We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization. We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.



### Regularized Discriminant Embedding for Visual Descriptor Learning
- **Arxiv ID**: http://arxiv.org/abs/1301.3644v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1301.3644v1)
- **Published**: 2013-01-16 10:12:37+00:00
- **Updated**: 2013-01-16 10:12:37+00:00
- **Authors**: Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi
- **Comment**: 3 pages + 1 additional page containing only cited references; The
  full version of this manuscript is currently under review in an international
  journal
- **Journal**: None
- **Summary**: Images can vary according to changes in viewpoint, resolution, noise, and illumination. In this paper, we aim to learn representations for an image, which are robust to wide changes in such environmental conditions, using training pairs of matching and non-matching local image patches that are collected under various environmental conditions. We present a regularized discriminant analysis that emphasizes two challenging categories among the given training pairs: (1) matching, but far apart pairs and (2) non-matching, but close pairs in the original feature space (e.g., SIFT feature space). Compared to existing work on metric learning and discriminant analysis, our method can better distinguish relevant images from irrelevant, but look-alike images.



### Zero-Shot Learning Through Cross-Modal Transfer
- **Arxiv ID**: http://arxiv.org/abs/1301.3666v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1301.3666v2)
- **Published**: 2013-01-16 12:01:34+00:00
- **Updated**: 2013-03-20 00:44:08+00:00
- **Authors**: Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, Andrew Y. Ng
- **Comment**: None
- **Journal**: None
- **Summary**: This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.



### Convex Variational Image Restoration with Histogram Priors
- **Arxiv ID**: http://arxiv.org/abs/1301.3683v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, G.1.6; I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1301.3683v2)
- **Published**: 2013-01-16 13:06:35+00:00
- **Updated**: 2013-07-17 12:35:09+00:00
- **Authors**: Paul Swoboda, Christoph Schn√∂rr
- **Comment**: 20 pages, 11 figures
- **Journal**: None
- **Summary**: We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information for restoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations is established and cases where optimality holds are discussed. Additionally, we present an efficient algorithmic scheme for the numerical treatment of the presented model. Experiments using the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach.



### Learning Graphical Models of Images, Videos and Their Spatial Transformations
- **Arxiv ID**: http://arxiv.org/abs/1301.3854v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1301.3854v1)
- **Published**: 2013-01-16 15:50:06+00:00
- **Updated**: 2013-01-16 15:50:06+00:00
- **Authors**: Brendan J. Frey, Nebojsa Jojic
- **Comment**: Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)
- **Journal**: None
- **Summary**: Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markov models are staples of static and dynamic data modeling and image and video modeling in particular. We show how topographic transformations in the input, such as translation and shearing in images, can be accounted for in these models by including a discrete transformation variable. The resulting models perform clustering, dimensionality reduction and time-series analysis in a way that is invariant to transformations in the input. Using the EM algorithm, these transformation-invariant models can be fit to static data and time series. We give results on filtering microscopy images, face and facial pose clustering, handwritten digit modeling and recognition, video clustering, object tracking, and removal of distractions from video sequences.



### Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models
- **Arxiv ID**: http://arxiv.org/abs/1301.3755v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1301.3755v1)
- **Published**: 2013-01-16 17:05:57+00:00
- **Updated**: 2013-01-16 17:05:57+00:00
- **Authors**: Derek Rose, Itamar Arel
- **Comment**: 3 pages, 2 figures, submitted to ICLR2013 workshop
- **Journal**: None
- **Summary**: Hyper-parameter selection remains a daunting task when building a pattern recognition architecture which performs well, particularly in recently constructed visual pipeline models for feature extraction. We re-formulate pooling in an existing pipeline as a function of adjustable pooling map weight parameters and propose the use of supervised error signals from gradient descent to tune the established maps within the model. This technique allows us to learn what would otherwise be a design choice within the model and specialize the maps to aggregate areas of invariance for the task presented. Preliminary results show moderate potential gains in classification accuracy and highlight areas of importance within the intermediate feature representation space.



### Discriminative Recurrent Sparse Auto-Encoders
- **Arxiv ID**: http://arxiv.org/abs/1301.3775v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1301.3775v4)
- **Published**: 2013-01-16 18:07:01+00:00
- **Updated**: 2013-03-19 18:43:29+00:00
- **Authors**: Jason Tyler Rolfe, Yann LeCun
- **Comment**: Added clarifications suggested by reviewers. 15 pages, 10 figures
- **Journal**: None
- **Summary**: We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters.   From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.



