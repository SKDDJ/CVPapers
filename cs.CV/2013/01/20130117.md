# Arxiv Papers in cs.CV on 2013-01-17
### Multiscale Discriminant Saliency for Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1301.3964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1301.3964v1)
- **Published**: 2013-01-17 02:12:48+00:00
- **Updated**: 2013-01-17 02:12:48+00:00
- **Authors**: Anh Cat Le Ngo, Kenneth Ang Li-Minn, Guoping Qiu, Jasmine Seng Kah-Phooi
- **Comment**: 16 pages, ICCSA 2013 - BIOCA session
- **Journal**: None
- **Summary**: The bottom-up saliency, an early stage of humans' visual attention, can be considered as a binary classification problem between center and surround classes. Discriminant power of features for the classification is measured as mutual information between features and two classes distribution. The estimated discrepancy of two feature classes very much depends on considered scale levels; then, multi-scale structure and discriminant power are integrated by employing discrete wavelet features and Hidden markov tree (HMT). With wavelet coefficients and Hidden Markov Tree parameters, quad-tree like label structures are constructed and utilized in maximum a posterior probability (MAP) of hidden class variables at corresponding dyadic sub-squares. Then, saliency value for each dyadic square at each scale level is computed with discriminant power principle and the MAP. Finally, across multiple scales is integrated the final saliency map by an information maximization rule. Both standard quantitative tools such as NSS, LCC, AUC and qualitative assessments are used for evaluating the proposed multiscale discriminant saliency method (MDIS) against the well-know information-based saliency method AIM on its Bruce Database wity eye-tracking data. Simulation results are presented and analyzed to verify the validity of MDIS as well as point out its disadvantages for further research direction.



### Knowledge Matters: Importance of Prior Information for Optimization
- **Arxiv ID**: http://arxiv.org/abs/1301.4083v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1301.4083v6)
- **Published**: 2013-01-17 13:06:52+00:00
- **Updated**: 2013-07-13 16:38:36+00:00
- **Authors**: Çağlar Gülçehre, Yoshua Bengio
- **Comment**: 37 Pages, 5 figures, 5 tables JMLR Special Topics on Representation
  Learning Submission
- **Journal**: None
- **Summary**: We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.



### On the Product Rule for Classification Problems
- **Arxiv ID**: http://arxiv.org/abs/1301.4157v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1301.4157v1)
- **Published**: 2013-01-17 16:48:46+00:00
- **Updated**: 2013-01-17 16:48:46+00:00
- **Authors**: Marcelo Cicconet
- **Comment**: 3 pages, no figures
- **Journal**: None
- **Summary**: We discuss theoretical aspects of the product rule for classification problems in supervised machine learning for the case of combining classifiers. We show that (1) the product rule arises from the MAP classifier supposing equivalent priors and conditional independence given a class; (2) under some conditions, the product rule is equivalent to minimizing the sum of the squared distances to the respective centers of the classes related with different features, such distances being weighted by the spread of the classes; (3) observing some hypothesis, the product rule is equivalent to concatenating the vectors of features.



