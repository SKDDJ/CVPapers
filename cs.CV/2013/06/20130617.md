# Arxiv Papers in cs.CV on 2013-06-17
### Non-Uniform Blind Deblurring with a Spatially-Adaptive Sparse Prior
- **Arxiv ID**: http://arxiv.org/abs/1306.3828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1306.3828v1)
- **Published**: 2013-06-17 12:12:22+00:00
- **Updated**: 2013-06-17 12:12:22+00:00
- **Authors**: Haichao Zhang, David Wipf
- **Comment**: None
- **Journal**: None
- **Summary**: Typical blur from camera shake often deviates from the standard uniform convolutional script, in part because of problematic rotations which create greater blurring away from some unknown center point. Consequently, successful blind deconvolution requires the estimation of a spatially-varying or non-uniform blur operator. Using ideas from Bayesian inference and convex analysis, this paper derives a non-uniform blind deblurring algorithm with several desirable, yet previously-unexplored attributes. The underlying objective function includes a spatially adaptive penalty which couples the latent sharp image, non-uniform blur operator, and noise level together. This coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted. Remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics. The algorithm can be implemented using a majorization-minimization strategy that is virtually parameter free. Detailed theoretical analysis and empirical validation on real images serve to validate the proposed method.



### Two-View Matching with View Synthesis Revisited
- **Arxiv ID**: http://arxiv.org/abs/1306.3855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1306.3855v2)
- **Published**: 2013-06-17 13:44:25+00:00
- **Updated**: 2013-11-11 17:41:22+00:00
- **Authors**: Dmytro Mishkin, Michal Perdoch, Jiri Matas
- **Comment**: 25 pages, 14 figures
- **Journal**: None
- **Summary**: Wide-baseline matching focussing on problems with extreme viewpoint change is considered. We introduce the use of view synthesis with affine-covariant detectors to solve such problems and show that matching with the Hessian-Affine or MSER detectors outperforms the state-of-the-art ASIFT.   To minimise the loss of speed caused by view synthesis, we propose the Matching On Demand with view Synthesis algorithm (MODS) that uses progressively more synthesized images and more (time-consuming) detectors until reliable estimation of geometry is possible. We show experimentally that the MODS algorithm solves problems beyond the state-of-the-art and yet is comparable in speed to standard wide-baseline matchers on simpler problems.   Minor contributions include an improved method for tentative correspondence selection, applicable both with and without view synthesis and a view synthesis setup greatly improving MSER robustness to blur and scale change that increase its running time by 10% only.



### Classifying and Visualizing Motion Capture Sequences using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1306.3874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1306.3874v2)
- **Published**: 2013-06-17 14:26:52+00:00
- **Updated**: 2014-09-01 16:03:02+00:00
- **Authors**: Kyunghyun Cho, Xi Chen
- **Comment**: VISAPP 2014
- **Journal**: None
- **Summary**: The gesture recognition using motion capture data and depth sensors has recently drawn more attention in vision recognition. Currently most systems only classify dataset with a couple of dozens different actions. Moreover, feature extraction from the data is often computational complex. In this paper, we propose a novel system to recognize the actions from skeleton data with simple, but effective, features using deep neural networks. Features are extracted for each frame based on the relative positions of joints (PO), temporal differences (TD), and normalized trajectories of motion (NT). Given these features a hybrid multi-layer perceptron is trained, which simultaneously classifies and reconstructs input data. We use deep autoencoder to visualize learnt features, and the experiments show that deep neural networks can capture more discriminative information than, for instance, principal component analysis can. We test our system on a public database with 65 classes and more than 2,000 motion sequences. We obtain an accuracy above 95% which is, to our knowledge, the state of the art result for such a large dataset.



### Multi-view in Lensless Compressive Imaging
- **Arxiv ID**: http://arxiv.org/abs/1306.3946v2
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1306.3946v2)
- **Published**: 2013-06-17 18:14:59+00:00
- **Updated**: 2013-09-12 16:01:12+00:00
- **Authors**: Hong Jiang, Gang Huang, Paul Wilford
- **Comment**: Accepted for presentation at PCS 2013 as Paper #1021; 4 pages, 4
  figures. arXiv admin note: text overlap with arXiv:1302.1789
- **Journal**: None
- **Summary**: Multi-view images are acquired by a lensless compressive imaging architecture, which consists of an aperture assembly and multiple sensors. The aperture assembly consists of a two dimensional array of aperture elements whose transmittance can be individually controlled to implement a compressive sensing matrix. For each transmittance pattern of the aperture assembly, each of the sensors takes a measurement. The measurement vectors from the multiple sensors represent multi-view images of the same scene. We present theoretical framework for multi-view reconstruction and experimental results for enhancing quality of image using multi-view.



