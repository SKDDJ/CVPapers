# Arxiv Papers in cs.CV on 2013-09-25
### Diffeomorphic Metric Mapping and Probabilistic Atlas Generation of Hybrid Diffusion Imaging based on BFOR Signal Basis
- **Arxiv ID**: http://arxiv.org/abs/1309.6379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1309.6379v1)
- **Published**: 2013-09-25 01:57:50+00:00
- **Updated**: 2013-09-25 01:57:50+00:00
- **Authors**: Jia Du, A. Pasha Hosseinbor, Moo K. Chung, Barbara B. Bendlin, Gaurav Suryawanshi, Andrew L. Alexander, Anqi Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a large deformation diffeomorphic metric mapping algorithm to align multiple b-value diffusion weighted imaging (mDWI) data, specifically acquired via hybrid diffusion imaging (HYDI), denoted as LDDMM-HYDI. We then propose a Bayesian model for estimating the white matter atlas from HYDIs. We adopt the work given in Hosseinbor et al. (2012) and represent the q-space diffusion signal with the Bessel Fourier orientation reconstruction (BFOR) signal basis. The BFOR framework provides the representation of mDWI in the q-space and thus reduces memory requirement. In addition, since the BFOR signal basis is orthonormal, the L2 norm that quantifies the differences in the q-space signals of any two mDWI datasets can be easily computed as the sum of the squared differences in the BFOR expansion coefficients. In this work, we show that the reorientation of the $q$-space signal due to spatial transformation can be easily defined on the BFOR signal basis. We incorporate the BFOR signal basis into the LDDMM framework and derive the gradient descent algorithm for LDDMM-HYDI with explicit orientation optimization. Additionally, we extend the previous Bayesian atlas estimation framework for scalar-valued images to HYDIs and derive the expectation-maximization algorithm for solving the HYDI atlas estimation problem. Using real HYDI datasets, we show the Bayesian model generates the white matter atlas with anatomical details. Moreover, we show that it is important to consider the variation of mDWI reorientation due to a small change in diffeomorphic transformation in the LDDMM-HYDI optimization and to incorporate the full information of HYDI for aligning mDWI.



### Contextually learnt detection of unusual motion-based behaviour in crowded public spaces
- **Arxiv ID**: http://arxiv.org/abs/1309.6390v1
- **DOI**: 10.1007/978-1-4471-2155-8_51
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1309.6390v1)
- **Published**: 2013-09-25 03:22:59+00:00
- **Updated**: 2013-09-25 03:22:59+00:00
- **Authors**: Ognjen Arandjelović
- **Comment**: None
- **Journal**: Computer and Information Sciences II Computer and Information
  Sciences II, pp 403-410, 2012
- **Summary**: In this paper we are interested in analyzing behaviour in crowded public places at the level of holistic motion. Our aim is to learn, without user input, strong scene priors or labelled data, the scope of "normal behaviour" for a particular scene and thus alert to novelty in unseen footage. The first contribution is a low-level motion model based on what we term tracklet primitives, which are scene-specific elementary motions. We propose a clustering-based algorithm for tracklet estimation from local approximations to tracks of appearance features. This is followed by two methods for motion novelty inference from tracklet primitives: (a) we describe an approach based on a non-hierarchial ensemble of Markov chains as a means of capturing behavioural characteristics at different scales, and (b) a more flexible alternative which exhibits a higher generalizing power by accounting for constraints introduced by intentionality and goal-oriented planning of human motion in a particular scene. Evaluated on a 2h long video of a busy city marketplace, both algorithms are shown to be successful at inferring unusual behaviour, the latter model achieving better performance for novelties at a larger spatial scale.



### Multiple-object tracking in cluttered and crowded public spaces
- **Arxiv ID**: http://arxiv.org/abs/1309.6391v1
- **DOI**: 10.1007/978-3-642-17277-9_10
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1309.6391v1)
- **Published**: 2013-09-25 03:34:01+00:00
- **Updated**: 2013-09-25 03:34:01+00:00
- **Authors**: Rhys Martin, Ognjen Arandjelović
- **Comment**: None
- **Journal**: Lecture Notes in Computer Science, volume 6455, pp 89-98, 2010
- **Summary**: This paper addresses the problem of tracking moving objects of variable appearance in challenging scenes rich with features and texture. Reliable tracking is of pivotal importance in surveillance applications. It is made particularly difficult by the nature of objects encountered in such scenes: these too change in appearance and scale, and are often articulated (e.g. humans). We propose a method which uses fast motion detection and segmentation as a constraint for both building appearance models and their robust propagation (matching) in time. The appearance model is based on sets of local appearances automatically clustered using spatio-kinetic similarity, and is updated with each new appearance seen. This integration of all seen appearances of a tracked object makes it extremely resilient to errors caused by occlusion and the lack of permanence of due to low data quality, appearance change or background clutter. These theoretical strengths of our algorithm are empirically demonstrated on two hour long video footage of a busy city marketplace.



### A Unified Framework for Representation-based Subspace Clustering of Out-of-sample and Large-scale Data
- **Arxiv ID**: http://arxiv.org/abs/1309.6487v2
- **DOI**: 10.1109/TNNLS.2015.2490080
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1309.6487v2)
- **Published**: 2013-09-25 12:53:13+00:00
- **Updated**: 2015-10-30 14:43:50+00:00
- **Authors**: Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, Shijie Xiao
- **Comment**: in IEEE Trans. on Neural Networks and Learning Systems, 2015
- **Journal**: IEEE Trans. on Neural Networks and Learning Systems, vol. 27, no.
  12, pp. 2499-2512, Dec. 2016
- **Summary**: Under the framework of spectral clustering, the key of subspace clustering is building a similarity graph which describes the neighborhood relations among data points. Some recent works build the graph using sparse, low-rank, and $\ell_2$-norm-based representation, and have achieved state-of-the-art performance. However, these methods have suffered from the following two limitations. First, the time complexities of these methods are at least proportional to the cube of the data size, which make those methods inefficient for solving large-scale problems. Second, they cannot cope with out-of-sample data that are not used to construct the similarity graph. To cluster each out-of-sample datum, the methods have to recalculate the similarity graph and the cluster membership of the whole data set. In this paper, we propose a unified framework which makes representation-based subspace clustering algorithms feasible to cluster both out-of-sample and large-scale data. Under our framework, the large-scale problem is tackled by converting it as out-of-sample problem in the manner of "sampling, clustering, coding, and classifying". Furthermore, we give an estimation for the error bounds by treating each subspace as a point in a hyperspace. Extensive experimental results on various benchmark data sets show that our methods outperform several recently-proposed scalable methods in clustering large-scale data set.



### Characterness: An Indicator of Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1309.6691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1309.6691v1)
- **Published**: 2013-09-25 23:30:18+00:00
- **Updated**: 2013-09-25 23:30:18+00:00
- **Authors**: Yao Li, Wenjing Jia, Chunhua Shen, Anton van den Hengel
- **Comment**: 11 pages; Appearing in IEEE Trans. on Image Processing
- **Journal**: None
- **Summary**: Text in an image provides vital information for interpreting its contents, and text in a scene can aide with a variety of tasks from navigation, to obstacle avoidance, and odometry. Despite its value, however, identifying general text in images remains a challenging research problem. Motivated by the need to consider the widely varying forms of natural text, we propose a bottom-up approach to the problem which reflects the `characterness' of an image region. In this sense our approach mirrors the move from saliency detection methods to measures of `objectness'. In order to measure the characterness we develop three novel cues that are tailored for character detection, and a Bayesian method for their integration. Because text is made up of sets of characters, we then design a Markov random field (MRF) model so as to exploit the inherent dependencies between characters.   We experimentally demonstrate the effectiveness of our characterness cues as well as the advantage of Bayesian multi-cue integration. The proposed text detector outperforms state-of-the-art methods on a few benchmark scene text detection datasets. We also show that our measurement of `characterness' is superior than state-of-the-art saliency detection models when applied to the same task.



