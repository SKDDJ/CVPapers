# Arxiv Papers in cs.CV on 2013-08-29
### GNCGCP - Graduated NonConvexity and Graduated Concavity Procedure
- **Arxiv ID**: http://arxiv.org/abs/1308.6388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1308.6388v1)
- **Published**: 2013-08-29 08:00:20+00:00
- **Updated**: 2013-08-29 08:00:20+00:00
- **Authors**: Zhi-Yong Liu, Hong Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose the Graduated NonConvexity and Graduated Concavity Procedure (GNCGCP) as a general optimization framework to approximately solve the combinatorial optimization problems on the set of partial permutation matrices. GNCGCP comprises two sub-procedures, graduated nonconvexity (GNC) which realizes a convex relaxation and graduated concavity (GC) which realizes a concave relaxation. It is proved that GNCGCP realizes exactly a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation without needing convex or concave relaxation in an explicit way. Actually, GNCGCP involves only the gradient of the objective function and is therefore very easy to use in practical applications. Two typical NP-hard problems, (sub)graph matching and quadratic assignment problem (QAP), are employed to demonstrate its simplicity and state-of-the-art performance.



### A Synergistic Approach for Recovering Occlusion-Free Textured 3D Maps of Urban Facades from Heterogeneous Cartographic Data
- **Arxiv ID**: http://arxiv.org/abs/1308.6401v1
- **DOI**: 10.5772/56570
- **Categories**: **cs.CV**, I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1308.6401v1)
- **Published**: 2013-08-29 08:47:09+00:00
- **Updated**: 2013-08-29 08:47:09+00:00
- **Authors**: Karim Hammoudi, Fadi Dornaika, Bahman Soheilian, Bruno Vallet, John McDonald, Nicolas Paparoditis
- **Comment**: None
- **Journal**: International Journal of Advanced Robotic Systems, vol. 10, 10p.,
  2013
- **Summary**: In this paper we present a practical approach for generating an occlusion-free textured 3D map of urban facades by the synergistic use of terrestrial images, 3D point clouds and area-based information. Particularly in dense urban environments, the high presence of urban objects in front of the facades causes significant difficulties for several stages in computational building modeling. Major challenges lie on the one hand in extracting complete 3D facade quadrilateral delimitations and on the other hand in generating occlusion-free facade textures. For these reasons, we describe a straightforward approach for completing and recovering facade geometry and textures by exploiting the data complementarity of terrestrial multi-source imagery and area-based information.



### A New Algorithm of Speckle Filtering using Stochastic Distances
- **Arxiv ID**: http://arxiv.org/abs/1308.6487v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, cs.GR, math.IT, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1308.6487v1)
- **Published**: 2013-08-29 14:56:01+00:00
- **Updated**: 2013-08-29 14:56:01+00:00
- **Authors**: Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery
- **Comment**: Accepted for publication on the proceedings of the IEEE Geoscience
  and Remote Sensing Symposium (IGARSS 2012), to be published in IEEE Press.
  Available: http://www.igarss2012.org/Papers/viewpapers.asp?papernum=4877
- **Journal**: None
- **Summary**: This paper presents a new approach for filter design based on stochastic distances and tests between distributions. A window is defined around each pixel, overlapping samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value. The technique is applied to intensity SAR data with homogeneous regions using the Gamma model. The proposal is compared with the Lee's filter using a protocol based on Monte Carlo. Among the criteria used to quantify the quality of filters, we employ the equivalent number of looks, line and edge preservation. Moreover, we also assessed the filters by the Universal Image Quality Index and the Pearson's correlation on edges regions.



### Joint Video and Text Parsing for Understanding Events and Answering Queries
- **Arxiv ID**: http://arxiv.org/abs/1308.6628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1308.6628v2)
- **Published**: 2013-08-29 23:45:02+00:00
- **Updated**: 2014-02-21 05:24:09+00:00
- **Authors**: Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.



