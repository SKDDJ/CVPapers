# Arxiv Papers in cs.CV on 2013-07-01
### Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo Search
- **Arxiv ID**: http://arxiv.org/abs/1307.0277v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.0277v1)
- **Published**: 2013-07-01 06:50:23+00:00
- **Updated**: 2013-07-01 06:50:23+00:00
- **Authors**: Sourav Samantaa, Nilanjan Dey, Poulami Das, Suvojit Acharjee, Sheli Sinha Chaudhuri
- **Comment**: 8 Pages,7 figures,ICECIT2012,Anatapur,India. arXiv admin note: text
  overlap with arXiv:1003.1594, arXiv:1005.2908 by other authors
- **Journal**: None
- **Summary**: Image Segmentation is a technique of partitioning the original image into some distinct classes. Many possible solutions may be available for segmenting an image into a certain number of classes, each one having different quality of segmentation. In our proposed method, multilevel thresholding technique has been used for image segmentation. A new approach of Cuckoo Search (CS) is used for selection of optimal threshold value. In other words, the algorithm is used to achieve the best solution from the initial random threshold values or solutions and to evaluate the quality of a solution correlation function is used. Finally, MSE and PSNR are measured to understand the segmentation quality.



### An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1307.0426v3
- **DOI**: 10.1109/TIP.2016.2544703
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1307.0426v3)
- **Published**: 2013-07-01 16:16:40+00:00
- **Updated**: 2016-04-26 11:05:18+00:00
- **Authors**: Thomas A. Lampert, André Stumpf, Pierre Gançarski
- **Comment**: 16 pages
- **Journal**: IEEE Transactions on Image Processing 25(6), 2557-2572, 2016
- **Summary**: Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed.



