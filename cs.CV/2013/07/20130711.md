# Arxiv Papers in cs.CV on 2013-07-11
### Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images
- **Arxiv ID**: http://arxiv.org/abs/1307.2965v2
- **DOI**: 10.1007/978-3-319-05530-5_11
- **Categories**: **cs.CV**, cs.LG, q-bio.TO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1307.2965v2)
- **Published**: 2013-07-11 03:29:51+00:00
- **Updated**: 2014-04-22 16:01:12+00:00
- **Authors**: Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer, Shaohua Kevin Zhou
- **Comment**: MICCAI 2013: Workshop on Medical Computer Vision
- **Journal**: None
- **Summary**: The automatic segmentation of human knee cartilage from 3D MR images is a useful yet challenging task due to the thin sheet structure of the cartilage with diffuse boundaries and inhomogeneous intensities. In this paper, we present an iterative multi-class learning method to segment the femoral, tibial and patellar cartilage simultaneously, which effectively exploits the spatial contextual constraints between bone and cartilage, and also between different cartilages. First, based on the fact that the cartilage grows in only certain area of the corresponding bone surface, we extract the distance features of not only to the surface of the bone, but more informatively, to the densely registered anatomical landmarks on the bone surface. Second, we introduce a set of iterative discriminative classifiers that at each iteration, probability comparison features are constructed from the class confidence maps derived by previously learned classifiers. These features automatically embed the semantic context information between different cartilages of interest. Validated on a total of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the proposed approach demonstrates high robustness and accuracy of segmentation in comparison with existing state-of-the-art MR cartilage segmentation methods.



### Accuracy of MAP segmentation with hidden Potts and Markov mesh prior models via Path Constrained Viterbi Training, Iterated Conditional Modes and Graph Cut based algorithms
- **Arxiv ID**: http://arxiv.org/abs/1307.2971v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1307.2971v1)
- **Published**: 2013-07-11 04:49:11+00:00
- **Updated**: 2013-07-11 04:49:11+00:00
- **Authors**: Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study statistical classification accuracy of two different Markov field environments for pixelwise image segmentation, considering the labels of the image as hidden states and solving the estimation of such labels as a solution of the MAP equation. The emission distribution is assumed the same in all models, and the difference lays in the Markovian prior hypothesis made over the labeling random field. The a priori labeling knowledge will be modeled with a) a second order anisotropic Markov Mesh and b) a classical isotropic Potts model. Under such models, we will consider three different segmentation procedures, 2D Path Constrained Viterbi training for the Hidden Markov Mesh, a Graph Cut based segmentation for the first order isotropic Potts model, and ICM (Iterated Conditional Modes) for the second order isotropic Potts model.   We provide a unified view of all three methods, and investigate goodness of fit for classification, studying the influence of parameter estimation, computational gain, and extent of automation in the statistical measures Overall Accuracy, Relative Improvement and Kappa coefficient, allowing robust and accurate statistical analysis on synthetic and real-life experimental data coming from the field of Dental Diagnostic Radiography. All algorithms, using the learned parameters, generate good segmentations with little interaction when the images have a clear multimodal histogram. Suboptimal learning proves to be frail in the case of non-distinctive modes, which limits the complexity of usable models, and hence the achievable error rate as well.   All Matlab code written is provided in a toolbox available for download from our website, following the Reproducible Research Paradigm.



### Fast Exact Search in Hamming Space with Multi-Index Hashing
- **Arxiv ID**: http://arxiv.org/abs/1307.2982v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.DS, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1307.2982v3)
- **Published**: 2013-07-11 05:52:21+00:00
- **Updated**: 2014-04-25 01:31:55+00:00
- **Authors**: Mohammad Norouzi, Ali Punjani, David J. Fleet
- **Comment**: None
- **Journal**: None
- **Summary**: There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is storage efficient and straightforward to implement. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speedups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.



### Conversion of Braille to Text in English, Hindi and Tamil Languages
- **Arxiv ID**: http://arxiv.org/abs/1307.2997v1
- **DOI**: 10.5121/ijcsea.2013.3303
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.2997v1)
- **Published**: 2013-07-11 07:24:16+00:00
- **Updated**: 2013-07-11 07:24:16+00:00
- **Authors**: S. Padmavathi, Manojna K. S. S, S. Sphoorthy Reddy, D. Meenakshy
- **Comment**: 14 pages, 20 figures, 4 tables
- **Journal**: International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.3, No.3, June 2013
- **Summary**: The Braille system has been used by the visually impaired for reading and writing. Due to limited availability of the Braille text books an efficient usage of the books becomes a necessity. This paper proposes a method to convert a scanned Braille document to text which can be read out to many through the computer. The Braille documents are pre processed to enhance the dots and reduce the noise. The Braille cells are segmented and the dots from each cell is extracted and converted in to a number sequence. These are mapped to the appropriate alphabets of the language. The converted text is spoken out through a speech synthesizer. The paper also provides a mechanism to type the Braille characters through the number pad of the keyboard. The typed Braille character is mapped to the alphabet and spoken out. The Braille cell has a standard representation but the mapping differs for each language. In this paper mapping of English, Hindi and Tamil are considered.



### Between Sense and Sensibility: Declarative narrativisation of mental models as a basis and benchmark for visuo-spatial cognition and computation focussed collaborative cognitive systems
- **Arxiv ID**: http://arxiv.org/abs/1307.3040v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1307.3040v2)
- **Published**: 2013-07-11 10:01:29+00:00
- **Updated**: 2014-03-31 10:22:29+00:00
- **Authors**: Mehul Bhatt
- **Comment**: 5 pages, research statement summarising recent publications
- **Journal**: None
- **Summary**: What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words, what kind of cognitive processes mediate sensing capability, and the formation of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory formation, beliefs and their revision, argument formation--- in domain-specific problem solving, or in regular activities of everyday living, working and simply going around in the environment? How can knowledge and reasoning about such capabilities, as exhibited by humans in particular problem contexts, be used as a model and benchmark for the development of collaborative cognitive (interaction) systems concerned with human assistance, assurance, and empowerment?   We pose these questions in the context of a range of assistive technologies concerned with \emph{visuo-spatial perception and cognition} tasks encompassing aspects such as commonsense, creativity, and the application of specialist domain knowledge and problem-solving thought processes. Assistive technologies being considered include: (a) human activity interpretation; (b) high-level cognitive rovotics; (c) people-centred creative design in domains such as architecture & digital media creation, and (d) qualitative analyses geographic information systems. Computational narratives not only provide a rich cognitive basis, but they also serve as a benchmark of functional performance in our development of computational cognitive assistance systems. We posit that computational narrativisation pertaining to space, actions, and change provides a useful model of \emph{visual} and \emph{spatio-temporal thinking} within a wide-range of problem-solving tasks and application areas where collaborative cognitive systems could serve an assistive and empowering function.



### A two-layer Conditional Random Field for the classification of partially occluded objects
- **Arxiv ID**: http://arxiv.org/abs/1307.3043v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.3043v2)
- **Published**: 2013-07-11 10:07:19+00:00
- **Updated**: 2013-09-13 15:48:23+00:00
- **Authors**: Sergey Kosov, Pushmeet Kohli, Franz Rottensteiner, Christian Heipke
- **Comment**: Conference Submission
- **Journal**: None
- **Summary**: Conditional Random Fields (CRF) are among the most popular techniques for image labelling because of their flexibility in modelling dependencies between the labels and the image features. This paper proposes a novel CRF-framework for image labeling problems which is capable to classify partially occluded objects. Our approach is evaluated on aerial near-vertical images as well as on urban street-view images and compared with another methods.



### Contrast Enhancement And Brightness Preservation Using Multi- Decomposition Histogram Equalization
- **Arxiv ID**: http://arxiv.org/abs/1307.3054v1
- **DOI**: 10.5121/sipij.2013.4308
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.3054v1)
- **Published**: 2013-07-11 11:02:57+00:00
- **Updated**: 2013-07-11 11:02:57+00:00
- **Authors**: Sayali Nimkar, Sanal Varghese, Sucheta Shrivastava
- **Comment**: 9 pages,13 figures
- **Journal**: SIPIJ, Vol.4, Issue.3, pp. 85-93
- **Summary**: Histogram Equalization (HE) has been an essential addition to the Image Enhancement world. Enhancement techniques like Classical Histogram Equalization (CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE) and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhance contrast, however, brightness is not well preserved with these methods, which gives an unpleasant look to the final image obtained. Thus, we introduce a novel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminate the drawbacks of the earlier methods. In MDHE, we have decomposed the input sixty-four parts, applied CHE in each of the sub-images and then finally interpolated them in correct order. The final image after MDHE results in contrast enhanced and brightness preserved image compared to all other techniques mentioned above. We have calculated the various parameters like PSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supported by bar graphs, histograms and the parameter calculations at the end.



### Fuzzy Fibers: Uncertainty in dMRI Tractography
- **Arxiv ID**: http://arxiv.org/abs/1307.3271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.3271v1)
- **Published**: 2013-07-11 21:01:23+00:00
- **Updated**: 2013-07-11 21:01:23+00:00
- **Authors**: Thomas Schultz, Anna Vilanova, Ralph Brecheisen, Gordon Kindlmann
- **Comment**: None
- **Journal**: None
- **Summary**: Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI) allows for noninvasive reconstruction of fiber bundles in the human brain. In this chapter, we discuss sources of error and uncertainty in this technique, and review strategies that afford a more reliable interpretation of the results. This includes methods for computing and rendering probabilistic tractograms, which estimate precision in the face of measurement noise and artifacts. However, we also address aspects that have received less attention so far, such as model selection, partial voluming, and the impact of parameters, both in preprocessing and in fiber tracking itself. We conclude by giving impulses for future research.



