# Arxiv Papers in cs.CV on 2013-07-22
### A Novel Equation based Classifier for Detecting Human in Images
- **Arxiv ID**: http://arxiv.org/abs/1307.5591v1
- **DOI**: 10.5120/12496-7272
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5591v1)
- **Published**: 2013-07-22 05:13:03+00:00
- **Updated**: 2013-07-22 05:13:03+00:00
- **Authors**: Subra Mukherjee, Karen Das
- **Comment**: published with international journal of Computer Applications (IJCA)
- **Journal**: International Journal of Computer Applications 72(6):9-16, June
  2013
- **Summary**: Shape based classification is one of the most challenging tasks in the field of computer vision. Shapes play a vital role in object recognition. The basic shapes in an image can occur in varying scale, position and orientation. And specially when detecting human, the task becomes more challenging owing to the largely varying size, shape, posture and clothing of human. So, in our work we detect human, based on the head-shoulder shape as it is the most unvarying part of human body. Here, firstly a new and a novel equation named as the Omega Equation that describes the shape of human head-shoulder is developed and based on this equation, a classifier is designed particularly for detecting human presence in a scene. The classifier detects human by analyzing some of the discriminative features of the values of the parameters obtained from the Omega equation. The proposed method has been tested on a variety of shape dataset taking into consideration the complexities of human head-shoulder shape. In all the experiments the proposed method demonstrated satisfactory results.



### Online Tracking Parameter Adaptation based on Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1307.5653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5653v1)
- **Published**: 2013-07-22 11:09:33+00:00
- **Updated**: 2013-07-22 11:09:33+00:00
- **Authors**: Duc Phu Chau, Julien Badie, François Bremond, Monique Thonnat
- **Comment**: IEEE International Conference on Advanced Video and Signal-based
  Surveillance (2013)
- **Journal**: None
- **Summary**: Parameter tuning is a common issue for many tracking algorithms. In order to solve this problem, this paper proposes an online parameter tuning to adapt a tracking algorithm to various scene contexts. In an offline training phase, this approach learns how to tune the tracker parameters to cope with different contexts. In the online control phase, once the tracking quality is evaluated as not good enough, the proposed approach computes the current context and tunes the tracking parameters using the learned values. The experimental results show that the proposed approach improves the performance of the tracking algorithm and outperforms recent state of the art trackers. This paper brings two contributions: (1) an online tracking evaluation, and (2) a method to adapt online tracking parameters to scene contexts.



### 6th International Symposium on Attention in Cognitive Systems 2013
- **Arxiv ID**: http://arxiv.org/abs/1307.6170v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.6170v2)
- **Published**: 2013-07-22 12:27:26+00:00
- **Updated**: 2013-07-30 07:43:09+00:00
- **Authors**: Lucas Paletta, Laurent Itti, Björn Schuller, Fang Fang
- **Comment**: conference
- **Journal**: None
- **Summary**: This volume contains the papers accepted at the 6th International Symposium on Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5, 2013. The aim of this symposium is to highlight the central role of attention on various kinds of performance in cognitive systems processing. It brings together researchers and developers from both academia and industry, from computer vision, robotics, perception psychology, psychophysics and neuroscience, in order to provide an interdisciplinary forum to present and communicate on computational models of attention, with the focus on interdependencies with visual cognition. Furthermore, it intends to investigate relevant objectives for performance comparison, to document and to investigate promising application domains, and to discuss visual attention with reference to other aspects of AI enabled systems.



### Using a Dynamic Neural Field Model to Explore a Direct Collicular Inhibition Account of Inhibition of Return
- **Arxiv ID**: http://arxiv.org/abs/1307.5684v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1307.5684v1)
- **Published**: 2013-07-22 12:47:18+00:00
- **Updated**: 2013-07-22 12:47:18+00:00
- **Authors**: Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang, Raymond M. Klein
- **Comment**: None
- **Journal**: None
- **Summary**: When the interval between a transient ash of light (a "cue") and a second visual response signal (a "target") exceeds at least 200ms, responding is slowest in the direction indicated by the first signal. This phenomenon is commonly referred to as inhibition of return (IOR). The dynamic neural field model (DNF) has proven to have broad explanatory power for IOR, effectively capturing many empirical results. Previous work has used a short-term depression (STD) implementation of IOR, but this approach fails to explain many behavioral phenomena observed in the literature. Here, we explore a variant model of IOR involving a combination of STD and delayed direct collicular inhibition. We demonstrate that this hybrid model can better reproduce established behavioural results. We use the results of this model to propose several experiments that would yield particularly valuable insight into the nature of the neurophysiological mechanisms underlying IOR.



### A study of parameters affecting visual saliency assessment
- **Arxiv ID**: http://arxiv.org/abs/1307.5691v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5691v1)
- **Published**: 2013-07-22 13:01:36+00:00
- **Updated**: 2013-07-22 13:01:36+00:00
- **Authors**: Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit
- **Comment**: None
- **Journal**: None
- **Summary**: Since the early 2000s, computational visual saliency has been a very active research area. Each year, more and more new models are published in the main computer vision conferences. Nowadays, one of the big challenges is to find a way to fairly evaluate all of these models. In this paper, a new framework is proposed to assess models of visual saliency. This evaluation is divided into three experiments leading to the proposition of a new evaluation framework. Each experiment is based on a basic question: 1) there are two ground truths for saliency evaluation: what are the differences between eye fixations and manually segmented salient regions?, 2) the properties of the salient regions: for example, do large, medium and small salient regions present different difficulties for saliency models? and 3) the metrics used to assess saliency models: what advantages would there be to mix them with PCA? Statistical analysis is used here to answer each of these three questions.



### Visual saliency estimation by integrating features using multiple kernel learning
- **Arxiv ID**: http://arxiv.org/abs/1307.5693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5693v1)
- **Published**: 2013-07-22 13:09:12+00:00
- **Updated**: 2013-07-22 13:09:12+00:00
- **Authors**: Yasin Kavak, Erkut Erdem, Aykut Erdem
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as Object-Bank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models.



### Is Bottom-Up Attention Useful for Scene Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1307.5702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5702v1)
- **Published**: 2013-07-22 13:38:16+00:00
- **Updated**: 2013-07-22 13:38:16+00:00
- **Authors**: Samuel F. Dodge, Lina J. Karam
- **Comment**: None
- **Journal**: None
- **Summary**: The human visual system employs a selective attention mechanism to understand the visual world in an eficient manner. In this paper, we show how computational models of this mechanism can be exploited for the computer vision application of scene recognition. First, we consider saliency weighting and saliency pruning, and provide a comparison of the performance of different attention models in these approaches in terms of classification accuracy. Pruning can achieve a high degree of computational savings without significantly sacrificing classification accuracy. In saliency weighting, however, we found that classification performance does not improve. In addition, we present a new method to incorporate salient and non-salient regions for improved classification accuracy. We treat the salient and non-salient regions separately and combine them using Multiple Kernel Learning. We evaluate our approach using the UIUC sports dataset and find that with a small training size, our method improves upon the classification accuracy of the baseline bag of features approach.



### Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based Artificial Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1307.5710v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5710v1)
- **Published**: 2013-07-22 13:48:13+00:00
- **Updated**: 2013-07-22 13:48:13+00:00
- **Authors**: Jan Tünnermann, Dieter Enns, Bärbel Mertsching
- **Comment**: None
- **Journal**: None
- **Summary**: Region-based artificial attention constitutes a framework for bio-inspired attentional processes on an intermediate abstraction level for the use in computer vision and mobile robotics. Segmentation algorithms produce regions of coherently colored pixels. These serve as proto-objects on which the attentional processes determine image portions of relevance. A single region---which not necessarily represents a full object---constitutes the focus of attention. For many post-attentional tasks, however, such as identifying or tracking objects, single segments are not sufficient. Here, we present a saliency-guided approach that groups regions that potentially belong to the same object based on proximity and similarity of motion. We compare our results to object selection by thresholding saliency maps and a further attention-guided strategy.



### Understanding Humans' Strategies in Maze Solving
- **Arxiv ID**: http://arxiv.org/abs/1307.5713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1307.5713v1)
- **Published**: 2013-07-22 13:57:10+00:00
- **Updated**: 2013-07-22 13:57:10+00:00
- **Authors**: Min Zhao, Andre G. Marquez
- **Comment**: None
- **Journal**: None
- **Summary**: Navigating through a visual maze relies on the strategic use of eye movements to select and identify the route. When navigating the maze, there are trade-offs between exploring to the environment and relying on memory. This study examined strategies used to navigating through novel and familiar mazes that were viewed from above and traversed by a mouse cursor. Eye and mouse movements revealed two modes that almost never occurred concurrently: exploration and guidance. Analyses showed that people learned mazes and were able to devise and carry out complex, multi-faceted strategies that traded-off visual exploration against active motor performance. These strategies took into account available visual information, memory, confidence, the estimated cost in time for exploration, and idiosyncratic tolerance for error. Understanding the strategies humans used for maze solving is valuable for applications in cognitive neuroscience as well as in AI, robotics and human-robot interactions.



### Top-down and Bottom-up Feature Combination for Multi-sensor Attentive Robots
- **Arxiv ID**: http://arxiv.org/abs/1307.5720v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1307.5720v1)
- **Published**: 2013-07-22 14:09:30+00:00
- **Updated**: 2013-07-22 14:09:30+00:00
- **Authors**: Esther L. Colombini, Alexandre S. Simões, Carlos H. C. Ribeiro
- **Comment**: None
- **Journal**: None
- **Summary**: The information available to robots in real tasks is widely distributed both in time and space, requiring the agent to search for relevant data. In humans, that face the same problem when sounds, images and smells are presented to their sensors in a daily scene, a natural system is applied: Attention. As vision plays an important role in our routine, most research regarding attention has involved this sensorial system and the same has been replicated to the robotics field. However,most of the robotics tasks nowadays do not rely only in visual data, that are still costly. To allow the use of attentive concepts with other robotics sensors that are usually used in tasks such as navigation, self-localization, searching and mapping, a generic attentional model has been previously proposed. In this work, feature mapping functions were designed to build feature maps to this attentive model from data from range scanner and sonar sensors. Experiments were performed in a high fidelity simulated robotics environment and results have demonstrated the capability of the model on dealing with both salient stimuli and goal-driven attention over multiple features extracted from multiple sensors.



### Appearance Descriptors for Person Re-identification: a Comprehensive Review
- **Arxiv ID**: http://arxiv.org/abs/1307.5748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5748v1)
- **Published**: 2013-07-22 15:41:57+00:00
- **Updated**: 2013-07-22 15:41:57+00:00
- **Authors**: Riccardo Satta
- **Comment**: None
- **Journal**: None
- **Summary**: In video-surveillance, person re-identification is the task of recognising whether an individual has already been observed over a network of cameras. Typically, this is achieved by exploiting the clothing appearance, as classical biometric traits like the face are impractical in real-world video surveillance scenarios. Clothing appearance is represented by means of low-level \textit{local} and/or \textit{global} features of the image, usually extracted according to some part-based body model to treat different body parts (e.g. torso and legs) independently. This paper provides a comprehensive review of current approaches to build appearance descriptors for person re-identification. The most relevant techniques are described in detail, and categorised according to the body models and features used. The aim of this work is to provide a structured body of knowledge and a starting point for researchers willing to conduct novel investigations on this challenging topic.



### An Adaptive GMM Approach to Background Subtraction for Application in Real Time Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1307.5800v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1307.5800v1)
- **Published**: 2013-07-22 17:59:28+00:00
- **Updated**: 2013-07-22 17:59:28+00:00
- **Authors**: Subra Mukherjee, Karen Das
- **Comment**: 5 Pages
- **Journal**: International Journal of Research in Engineering and
  Technology,Volume 2, Issue 1, Jan-2013
- **Summary**: Efficient security management has become an important parameter in todays world. As the problem is growing, there is an urgent need for the introduction of advanced technology and equipment to improve the state-of art of surveillance. In this paper we propose a model for real time background subtraction using AGMM. The proposed model is robust and adaptable to dynamic background, fast illumination changes, repetitive motion. Also we have incorporated a method for detecting shadows using the Horpresert color model. The proposed model can be employed for monitoring areas where movement or entry is highly restricted. So on detection of any unexpected events in the scene an alarm can be triggered and hence we can achieve real time surveillance even in the absence of constant human monitoring.



