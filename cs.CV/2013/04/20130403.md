# Arxiv Papers in cs.CV on 2013-04-03
### Lie Algebrized Gaussians for Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1304.0823v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1304.0823v2)
- **Published**: 2013-04-03 02:38:01+00:00
- **Updated**: 2017-05-09 23:39:21+00:00
- **Authors**: Liyu Gong, Meng Chen, Chunlong Hu
- **Comment**: None
- **Journal**: None
- **Summary**: We present an image representation method which is derived from analyzing Gaussian probability density function (\emph{pdf}) space using Lie group theory. In our proposed method, images are modeled by Gaussian mixture models (GMMs) which are adapted from a globally trained GMM called universal background model (UBM). Then we vectorize the GMMs based on two facts: (1) components of image-specific GMMs are closely grouped together around their corresponding component of the UBM due to the characteristic of the UBM adaption procedure; (2) Gaussian \emph{pdf}s form a Lie group, which is a differentiable manifold rather than a vector space. We map each Gaussian component to the tangent vector space (named Lie algebra) of Lie group at the manifold position of UBM. The final feature vector, named Lie algebrized Gaussians (LAG) is then constructed by combining the Lie algebrized Gaussian components with mixture weights. We apply LAG features to scene category recognition problem and observe state-of-the-art performance on 15Scenes benchmark.



### Multiscale Hybrid Non-local Means Filtering Using Modified Similarity Measure
- **Arxiv ID**: http://arxiv.org/abs/1304.0839v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, 68U05, 65D18
- **Links**: [PDF](http://arxiv.org/pdf/1304.0839v1)
- **Published**: 2013-04-03 04:24:21+00:00
- **Updated**: 2013-04-03 04:24:21+00:00
- **Authors**: Zahid Hussain Shamsi, Dai-Gyoung Kim
- **Comment**: 7 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: A new multiscale implementation of non-local means filtering for image denoising is proposed. The proposed algorithm also introduces a modification of similarity measure for patch comparison. The standard Euclidean norm is replaced by weighted Euclidean norm for patch based comparison. Assuming the patch as an oriented surface, notion of normal vector patch is being associated with each patch. The inner product of these normal vector patches is then used in weighted Euclidean distance of photometric patches as the weight factor. The algorithm involves two steps: The first step is multiscale implementation of an accelerated non-local means filtering in the stationary wavelet domain to obtain a refined version of the noisy patches for later comparison. This step is inspired by a preselection phase of finding similar patches in various non-local means approaches. The next step is to apply the modified non-local means filtering to the noisy image using the reference patches obtained in the first step. These refined patches contain less noise, and consequently the computation of normal vectors and partial derivatives is more accurate. Experimental results indicate equivalent or better performance of proposed algorithm as compared to various state of the art algorithms.



### A Fast Semidefinite Approach to Solving Binary Quadratic Problems
- **Arxiv ID**: http://arxiv.org/abs/1304.0840v1
- **DOI**: 10.1109/CVPR.2013.173
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1304.0840v1)
- **Published**: 2013-04-03 04:31:10+00:00
- **Updated**: 2013-04-03 04:31:10+00:00
- **Authors**: Peng Wang, Chunhua Shen, Anton van den Hengel
- **Comment**: Appearing in Proc. IEEE Conf. Computer Vision and Pattern
  Recognition, 2013
- **Journal**: None
- **Summary**: Many computer vision problems can be formulated as binary quadratic programs (BQPs). Two classic relaxation methods are widely used for solving BQPs, namely, spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high for large scale problems. We present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. Extensive experiments on various applications including clustering, image segmentation, co-segmentation and registration demonstrate the usefulness of our SDP formulation for solving large-scale BQPs.



### Patch-based Probabilistic Image Quality Assessment for Face Selection and Improved Video-based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1304.0869v2
- **DOI**: 10.1109/CVPRW.2011.5981881
- **Categories**: **cs.CV**, stat.AP, I.4.1; I.4.6; I.4.7; I.4.9; I.4.10; I.5.1; I.5.4; G.3
- **Links**: [PDF](http://arxiv.org/pdf/1304.0869v2)
- **Published**: 2013-04-03 08:41:23+00:00
- **Updated**: 2014-03-14 15:53:31+00:00
- **Authors**: Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C. Lovell
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), pp. 74-81, 2011
- **Summary**: In video based face recognition, face images are typically captured over multiple frames in uncontrolled conditions, where head pose, illumination, shadowing, motion blur and focus change over the sequence. Additionally, inaccuracies in face localisation can also introduce scale and alignment variations. Using all face images, including images of poor quality, can actually degrade face recognition performance. While one solution it to use only the "best" subset of images, current face selection techniques are incapable of simultaneously handling all of the abovementioned issues. We propose an efficient patch-based face image quality assessment algorithm which quantifies the similarity of a face image to a probabilistic face model, representing an "ideal" face. Image characteristics that affect recognition are taken into account, including variations in geometric alignment (shift, rotation and scale), sharpness, head pose and cast shadows. Experiments on FERET and PIE datasets show that the proposed algorithm is able to identify images which are simultaneously the most frontal, aligned, sharp and well illuminated. Further experiments on a new video surveillance dataset (termed ChokePoint) show that the proposed method provides better face subsets than existing face selection techniques, leading to significant improvements in recognition accuracy.



### Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of Foreground Speed, Size and Texture
- **Arxiv ID**: http://arxiv.org/abs/1304.0886v1
- **DOI**: 10.1109/CVPRW.2011.5981799
- **Categories**: **cs.CV**, I.2.10; I.4.6; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1304.0886v1)
- **Published**: 2013-04-03 09:31:27+00:00
- **Updated**: 2013-04-03 09:31:27+00:00
- **Authors**: Vikas Reddy, Conrad Sanderson, Brian C. Lovell
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), pp. 55-61, 2011
- **Summary**: A robust and efficient anomaly detection technique is proposed, capable of dealing with crowded scenes where traditional tracking based approaches tend to fail. Initial foreground segmentation of the input frames confines the analysis to foreground objects and effectively ignores irrelevant background dynamics. Input frames are split into non-overlapping cells, followed by extracting features based on motion, size and texture from each cell. Each feature type is independently analysed for the presence of an anomaly. Unlike most methods, a refined estimate of object motion is achieved by computing the optical flow of only the foreground pixels. The motion and size features are modelled by an approximated version of kernel density estimation, which is computationally efficient even for large training datasets. Texture features are modelled by an adaptively grown codebook, with the number of entries in the codebook selected in an online fashion. Experiments on the recently published UCSD Anomaly Detection dataset show that the proposed method obtains considerably better results than three recent approaches: MPPCA, social force, and mixture of dynamic textures (MDT). The proposed method is also several orders of magnitude faster than MDT, the next best performing method.



### A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale SVM Training
- **Arxiv ID**: http://arxiv.org/abs/1304.1014v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1304.1014v2)
- **Published**: 2013-04-03 17:15:43+00:00
- **Updated**: 2013-10-13 09:50:26+00:00
- **Authors**: Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori
- **Comment**: REVISED VERSION (October 2013) -- Title and abstract have been
  revised. Section 5 was added. Some proofs have been summarized (full-length
  proofs available in the previous version)
- **Journal**: Information Sciences 285, 66-99, 2014
- **Summary**: Recently, there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as {the Frank-Wolfe (FW) method}. In particular, this procedure has been successfully applied to train large-scale instances of non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has allowed to obtain efficient algorithms but also important theoretical results, including convergence analysis of training algorithms and new characterizations of model sparsity.   In this paper, we present and analyze a novel variant of the FW method based on a new way to perform away steps, a classic strategy used to accelerate the convergence of the basic FW procedure. Our formulation and analysis is focused on a general concave maximization problem on the simplex. However, the specialization of our algorithm to quadratic forms is strongly related to some classic methods in computational geometry, namely the Gilbert and MDM algorithms.   On the theoretical side, we demonstrate that the method matches the guarantees in terms of convergence rate and number of iterations obtained by using classic away steps. In particular, the method enjoys a linear rate of convergence, a result that has been recently proved for MDM on quadratic forms.   On the practical side, we provide experiments on several classification datasets, and evaluate the results using statistical tests. Experiments show that our method is faster than the FW method with classic away steps, and works well even in the cases in which classic away steps slow down the algorithm. Furthermore, these improvements are obtained without sacrificing the predictive accuracy of the obtained SVM model.



### A software for aging faces applied to ancient marble busts
- **Arxiv ID**: http://arxiv.org/abs/1304.1022v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1304.1022v1)
- **Published**: 2013-04-03 17:34:20+00:00
- **Updated**: 2013-04-03 17:34:20+00:00
- **Authors**: Amelia Carolina Sparavigna
- **Comment**: Image processing. Aging faces. Freely available software. Ancient
  marble busts. Augustus
- **Journal**: None
- **Summary**: The study and development of software able to show the effect of aging of faces is one of the tasks of face recognition technologies. Some software solutions are used for investigations, some others to show the effects of drugs on healthy appearance, however some other applications can be proposed for the analysis of visual arts. Here we use a freely available software, which is providing interesting results, for the comparison of ancient marble busts. An analysis of Augustus busts is proposed.



### Highly comparative time-series analysis: The empirical structure of time series and their methods
- **Arxiv ID**: http://arxiv.org/abs/1304.1209v1
- **DOI**: 10.1098/rsif.2013.0048
- **Categories**: **physics.data-an**, cs.CV, physics.bio-ph, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1304.1209v1)
- **Published**: 2013-04-03 23:24:02+00:00
- **Updated**: 2013-04-03 23:24:02+00:00
- **Authors**: Ben D. Fulcher, Max A. Little, Nick S. Jones
- **Comment**: None
- **Journal**: J. R. Soc. Interface vol. 10 no. 83 20130048 (2013)
- **Summary**: The process of collecting and organizing sets of observations represents a common theme throughout the history of science. However, despite the ubiquity of scientists measuring, recording, and analyzing the dynamics of different processes, an extensive organization of scientific time-series data and analysis methods has never been performed. Addressing this, annotated collections of over 35 000 real-world and model-generated time series and over 9000 time-series analysis algorithms are analyzed in this work. We introduce reduced representations of both time series, in terms of their properties measured by diverse scientific methods, and of time-series analysis methods, in terms of their behaviour on empirical time series, and use them to organize these interdisciplinary resources. This new approach to comparing across diverse scientific data and methods allows us to organize time-series datasets automatically according to their properties, retrieve alternatives to particular analysis methods developed in other scientific disciplines, and automate the selection of useful methods for time-series classification and regression tasks. The broad scientific utility of these tools is demonstrated on datasets of electroencephalograms, self-affine time series, heart beat intervals, speech signals, and others, in each case contributing novel analysis techniques to the existing literature. Highly comparative techniques that compare across an interdisciplinary literature can thus be used to guide more focused research in time-series analysis for applications across the scientific disciplines.



