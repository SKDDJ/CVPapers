# Arxiv Papers in cs.CV on 2013-10-16
### Mapping the stereotyped behaviour of freely-moving fruit flies
- **Arxiv ID**: http://arxiv.org/abs/1310.4249v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, physics.bio-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1310.4249v2)
- **Published**: 2013-10-16 02:44:56+00:00
- **Updated**: 2014-08-12 02:26:51+00:00
- **Authors**: Gordon J. Berman, Daniel M. Choi, William Bialek, Joshua W. Shaevitz
- **Comment**: 21 pages, 17 figures. Email GJB (gberman@princeton.edu) to see
  supplementary movies, Journal of the Royal Society Interface, 2014
- **Journal**: None
- **Summary**: Most animals possess the ability to actuate a vast diversity of movements, ostensibly constrained only by morphology and physics. In practice, however, a frequent assumption in behavioral science is that most of an animal's activities can be described in terms of a small set of stereotyped motifs. Here we introduce a method for mapping the behavioral space of organisms, relying only upon the underlying structure of postural movement data to organize and classify behaviors. We find that six different drosophilid species each perform a mix of non-stereotyped actions and over one hundred hierarchically-organized, stereotyped behaviors. Moreover, we use this approach to compare these species' behavioral spaces, systematically identifying subtle behavioral differences between closely-related species.



### ImageSpirit: Verbal Guided Image Parsing
- **Arxiv ID**: http://arxiv.org/abs/1310.4389v2
- **DOI**: 10.1145/2682628
- **Categories**: **cs.GR**, cs.CV, I.3.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1310.4389v2)
- **Published**: 2013-10-16 14:16:31+00:00
- **Updated**: 2014-05-21 16:56:03+00:00
- **Authors**: Ming-Ming Cheng, Shuai Zheng, Wen-Yan Lin, Jonathan Warrell, Vibhav Vineet, Paul Sturgess, Nigel Crook, Niloy Mitra, Philip Torr
- **Comment**: http://mmcheng.net/imagespirit/
- **Journal**: ACM Transactions on Graphics, 2014
- **Summary**: Humans describe images in terms of nouns and adjectives while algorithms operate on images represented as sets of pixels. Bridging this gap between how humans would like to access images versus their typical representation is the goal of image parsing, which involves assigning object and attribute labels to pixel. In this paper we propose treating nouns as object labels and adjectives as visual attribute labels. This allows us to formulate the image parsing problem as one of jointly estimating per-pixel object and attribute labels from a set of training images. We propose an efficient (interactive time) solution. Using the extracted labels as handles, our system empowers a user to verbally refine the results. This enables hands-free parsing of an image into pixel-wise object/attribute labels that correspond to human semantics. Verbally selecting objects of interests enables a novel and natural interaction modality that can possibly be used to interact with new generation devices (e.g. smart phones, Google Glass, living room devices). We demonstrate our system on a large number of real-world images with varying complexity. To help understand the tradeoffs compared to traditional mouse based interactions, results are reported for both a large scale quantitative evaluation and a user study.



