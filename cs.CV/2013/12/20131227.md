# Arxiv Papers in cs.CV on 2013-12-27
### Near-separable Non-negative Matrix Factorization with $\ell_1$- and Bregman Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/1312.7167v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1312.7167v1)
- **Published**: 2013-12-27 01:10:00+00:00
- **Updated**: 2013-12-27 01:10:00+00:00
- **Authors**: Abhishek Kumar, Vikas Sindhwani
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a family of tractable NMF algorithms have been proposed under the assumption that the data matrix satisfies a separability condition Donoho & Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulates the NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. In this paper, we develop several extensions of the conical hull procedures of Kumar et al. (2013) for robust ($\ell_1$) approximations and Bregman divergences. Our methods inherit all the advantages of Kumar et al. (2013) including scalability and noise-tolerance. We show that on foreground-background separation problems in computer vision, robust near-separable NMFs match the performance of Robust PCA, considered state of the art on these problems, with an order of magnitude faster training time. We also demonstrate applications in exemplar selection settings.



### Combining persistent homology and invariance groups for shape comparison
- **Arxiv ID**: http://arxiv.org/abs/1312.7219v4
- **DOI**: None
- **Categories**: **math.AT**, cs.CG, cs.CV, 55N35 (Primary), 22F99, 47H09, 54H15, 57S10, 68U05, 65D18
  (Secondary), I.4.7; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1312.7219v4)
- **Published**: 2013-12-27 09:09:36+00:00
- **Updated**: 2016-01-28 10:13:00+00:00
- **Authors**: Patrizio Frosini, Grzegorz Jablonski
- **Comment**: 33 pages, 12 figures, 1 table; corrected typos
- **Journal**: None
- **Summary**: In many applications concerning the comparison of data expressed by $\mathbb{R}^m$-valued functions defined on a topological space $X$, the invariance with respect to a given group $G$ of self-homeomorphisms of $X$ is required. While persistent homology is quite efficient in the topological and qualitative comparison of this kind of data when the invariance group $G$ is the group $\mathrm{Homeo}(X)$ of all self-homeomorphisms of $X$, this theory is not tailored to manage the case in which $G$ is a proper subgroup of $\mathrm{Homeo}(X)$, and its invariance appears too general for several tasks. This paper proposes a way to adapt persistent homology in order to get invariance just with respect to a given group of self-homeomorphisms of $X$. The main idea consists in a dual approach, based on considering the set of all $G$-invariant non-expanding operators defined on the space of the admissible filtering functions on $X$. Some theoretical results concerning this approach are proven and two experiments are presented. An experiment illustrates the application of the proposed technique to compare 1D-signals, when the invariance is expressed by the group of affinities, the group of orientation-preserving affinities, the group of isometries, the group of translations and the identity group. Another experiment shows how our technique can be used for image comparison.



### Learning Human Pose Estimation Features with Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1312.7302v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.7302v6)
- **Published**: 2013-12-27 17:41:13+00:00
- **Updated**: 2014-04-23 19:23:46+00:00
- **Authors**: Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor, Christoph Bregler
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a new architecture for human pose estimation using a multi- layer convolutional network architecture and a modified learning technique that learns low-level features and higher-level weak spatial models. Unconstrained human pose estimation is one of the hardest problems in computer vision, and our new architecture and learning schema shows significant improvement over the current state-of-the-art results. The main contribution of this paper is showing, for the first time, that a specific variation of deep learning is able to outperform all existing traditional architectures on this task. The paper also discusses several lessons learned while researching alternatives, most notably, that it is possible to learn strong low-level feature detectors on features that might even just cover a few pixels in the image. Higher-level spatial models improve somewhat the overall result, but to a much lesser extent then expected. Many researchers previously argued that the kinematic structure and top-down information is crucial for this domain, but with our purely bottom up, and weak spatial model, we could improve other more complicated architectures that currently produce the best results. This mirrors what many other researchers, like those in the speech recognition, object recognition, and other domains have experienced.



### Monte Carlo non local means: Random sampling for large-scale image filtering
- **Arxiv ID**: http://arxiv.org/abs/1312.7366v3
- **DOI**: 10.1109/TIP.2014.2327813
- **Categories**: **cs.CV**, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1312.7366v3)
- **Published**: 2013-12-27 23:31:42+00:00
- **Updated**: 2014-05-14 19:35:11+00:00
- **Authors**: Stanley H. Chan, Todd Zickler, Yue M. Lu
- **Comment**: submitted for publication
- **Journal**: None
- **Summary**: We propose a randomized version of the non-local means (NLM) algorithm for large-scale image filtering. The new algorithm, called Monte Carlo non-local means (MCNLM), speeds up the classical NLM by computing a small subset of image patch distances, which are randomly selected according to a designed sampling pattern. We make two contributions. First, we analyze the performance of the MCNLM algorithm and show that, for large images or large external image databases, the random outcomes of MCNLM are tightly concentrated around the deterministic full NLM result. In particular, our error probability bounds show that, at any given sampling ratio, the probability for MCNLM to have a large deviation from the original NLM solution decays exponentially as the size of the image or database grows. Second, we derive explicit formulas for optimal sampling patterns that minimize the error probability bound by exploiting partial knowledge of the pairwise similarity weights. Numerical experiments show that MCNLM is competitive with other state-of-the-art fast NLM algorithms for single-image denoising. When applied to denoising images using an external database containing ten billion patches, MCNLM returns a randomized solution that is within 0.2 dB of the full NLM solution while reducing the runtime by three orders of magnitude.



