# Arxiv Papers in cs.CV on 2013-12-21
### GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training
- **Arxiv ID**: http://arxiv.org/abs/1312.6186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.6186v1)
- **Published**: 2013-12-21 00:56:56+00:00
- **Updated**: 2013-12-21 00:56:56+00:00
- **Authors**: Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.



### Intriguing properties of neural networks
- **Arxiv ID**: http://arxiv.org/abs/1312.6199v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.6199v4)
- **Published**: 2013-12-21 03:36:08+00:00
- **Updated**: 2014-02-19 16:33:14+00:00
- **Authors**: Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.   First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.   Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.



### Spectral Networks and Locally Connected Networks on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1312.6203v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.6203v3)
- **Published**: 2013-12-21 04:25:53+00:00
- **Updated**: 2014-05-21 16:27:09+00:00
- **Authors**: Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.



### One-Shot Adaptation of Supervised Deep Convolutional Models
- **Arxiv ID**: http://arxiv.org/abs/1312.6204v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.6204v2)
- **Published**: 2013-12-21 04:32:51+00:00
- **Updated**: 2014-02-18 02:57:42+00:00
- **Authors**: Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko, Trevor Darrell
- **Comment**: None
- **Journal**: ICLR Workshop 2014
- **Summary**: Dataset bias remains a significant barrier towards solving real world computer vision tasks. Though deep convolutional networks have proven to be a competitive approach for image classification, a question remains: have these models have solved the dataset bias problem? In general, training or fine-tuning a state-of-the-art deep model on a new domain requires a significant amount of data, which for many applications is simply not available. Transfer of models directly to new domains without adaptation has historically led to poor recognition performance. In this paper, we pose the following question: is a single image dataset, much larger than previously explored for adaptation, comprehensive enough to learn general deep models that may be effectively applied to new image domains? In other words, are deep CNNs trained on large amounts of labeled data as susceptible to dataset bias as previous methods have been shown to be? We show that a generic supervised deep CNN model trained on a large dataset reduces, but does not remove, dataset bias. Furthermore, we propose several methods for adaptation with deep models that are able to operate with little (one example per category) or no labeled domain specific data. Our experiments show that adaptation of deep models on benchmark visual domain adaptation datasets can provide a significant performance boost.



### Total variation with overlapping group sparsity for image deblurring under impulse noise
- **Arxiv ID**: http://arxiv.org/abs/1312.6208v1
- **DOI**: 10.1371/journal.pone.0122562
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1312.6208v1)
- **Published**: 2013-12-21 05:40:27+00:00
- **Updated**: 2013-12-21 05:40:27+00:00
- **Authors**: Gang Liu, Ting-Zhu Huang, Jun Liu, Xiao-Guang Lv
- **Comment**: 22 pages, 57 figures, submitted
- **Journal**: PLOS ONE 2015 10(4): e0122562
- **Summary**: The total variation (TV) regularization method is an effective method for image deblurring in preserving edges. However, the TV based solutions usually have some staircase effects. In this paper, in order to alleviate the staircase effect, we propose a new model for restoring blurred images with impulse noise. The model consists of an $\ell_1$-fidelity term and a TV with overlapping group sparsity (OGS) regularization term. Moreover, we impose a box constraint to the proposed model for getting more accurate solutions. An efficient and effective algorithm is proposed to solve the model under the framework of the alternating direction method of multipliers (ADMM). We use an inner loop which is nested inside the majorization minimization (MM) iteration for the subproblem of the proposed method. Compared with other methods, numerical results illustrate that the proposed method, can significantly improve the restoration quality, both in avoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR) and relative error (ReE).



### Extracting Region of Interest for Palm Print Authentication
- **Arxiv ID**: http://arxiv.org/abs/1312.6219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6219v1)
- **Published**: 2013-12-21 07:55:08+00:00
- **Updated**: 2013-12-21 07:55:08+00:00
- **Authors**: Kasturika B. Ray
- **Comment**: 8 pages,4 figures, 1 table, 1 photo of author and 1 photo of
  co-author, 3 paper has published (2011, 2011, 2012)
- **Journal**: IJASCSE Journal, Volume 2, Issue 6, December2013
- **Summary**: Biometrics authentication is an effective method for automatically recognizing individuals. The authentication consists of an enrollment phase and an identification or verification phase. In the stages of enrollment known (training) samples after the pre-processing stage are used for suitable feature extraction to generate the template database. In the verification stage, the test sample is similarly pre processed and subjected to feature extraction modules, and then it is matched with the training feature templates to decide whether it is a genuine or not. This paper presents use of a region of interest (ROI) for palm print technology. First some of the existing methods for palm print identification have been introduced. Then focus has been given on extraction of a suitable smaller region from the acquired palm print to improve the identification method accuracy. Several existing work in the topic of region extraction have been examined. Subsequently, a simple and original method has then proposed for locating the ROI that can be effectively used for palm print analysis. The ROI extracted using this new technique is suitable for different types of processing as it creates a rectangular or square area around the center of activity represented by the lines, wrinkles and ridges of the palm print. The effectiveness of the ROI approach has been tested by integrating it with a texture based identification / authentication system proposed earlier. The improvement has been shown by comparing the identification accuracy rate before and after the ROI pre-processing.



### OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1312.6229v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6229v4)
- **Published**: 2013-12-21 09:52:33+00:00
- **Updated**: 2014-02-24 03:38:17+00:00
- **Authors**: Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.



