# Arxiv Papers in cs.CV on 2013-12-20
### Unsupervised Feature Learning by Deep Sparse Coding
- **Arxiv ID**: http://arxiv.org/abs/1312.5783v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.5783v1)
- **Published**: 2013-12-20 00:21:36+00:00
- **Updated**: 2013-12-20 00:21:36+00:00
- **Authors**: Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi
- **Comment**: 9 pages, submitted to ICLR
- **Journal**: None
- **Summary**: In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.



### EXMOVES: Classifier-based Features for Scalable Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1312.5785v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.5785v3)
- **Published**: 2013-12-20 00:37:16+00:00
- **Updated**: 2014-03-28 01:55:14+00:00
- **Authors**: Du Tran, Lorenzo Torresani
- **Comment**: In Proceedings of the 2nd International Conference on Learning
  Representations, Banff, Canada, 2014
- **Journal**: None
- **Summary**: This paper introduces EXMOVES, learned exemplar-based features for efficient recognition of actions in videos. The entries in our descriptor are produced by evaluating a set of movement classifiers over spatial-temporal volumes of the input sequence. Each movement classifier is a simple exemplar-SVM trained on low-level features, i.e., an SVM learned using a single annotated positive space-time volume and a large number of unannotated videos.   Our representation offers two main advantages. First, since our mid-level features are learned from individual video exemplars, they require minimal amount of supervision. Second, we show that simple linear classification models trained on our global video descriptor yield action recognition accuracy approaching the state-of-the-art but at orders of magnitude lower cost, since at test-time no sliding window is necessary and linear models are efficient to train and test. This enables scalable action recognition, i.e., efficient classification of a large number of different actions even in large video databases. We show the generality of our approach by building our mid-level descriptors from two different low-level feature representations. The accuracy and efficiency of the approach are demonstrated on several large-scale action recognition benchmarks.



### Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks
- **Arxiv ID**: http://arxiv.org/abs/1312.5845v7
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1312.5845v7)
- **Published**: 2013-12-20 08:24:48+00:00
- **Updated**: 2015-02-16 09:37:18+00:00
- **Authors**: Takashi Shinozaki, Yasushi Naruse
- **Comment**: This paper has been withdrawn by the author since the review process
  for the conference to which it was applied ended
- **Journal**: None
- **Summary**: We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network.



### Fast Training of Convolutional Networks through FFTs
- **Arxiv ID**: http://arxiv.org/abs/1312.5851v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.5851v5)
- **Published**: 2013-12-20 08:42:21+00:00
- **Updated**: 2014-03-06 23:27:18+00:00
- **Authors**: Michael Mathieu, Mikael Henaff, Yann LeCun
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.



### Generic Deep Networks with Wavelet Scattering
- **Arxiv ID**: http://arxiv.org/abs/1312.5940v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.5940v3)
- **Published**: 2013-12-20 13:48:20+00:00
- **Updated**: 2014-03-10 18:44:50+00:00
- **Authors**: Edouard Oyallon, St√©phane Mallat, Laurent Sifre
- **Comment**: Workshop, 3 pages, prepared for ICLR 2014
- **Journal**: None
- **Summary**: We introduce a two-layer wavelet scattering network, for object classification. This scattering transform computes a spatial wavelet transform on the first layer and a new joint wavelet transform along spatial, angular and scale variables in the second layer. Numerical experiments demonstrate that this two layer convolution network, which involves no learning and no max pooling, performs efficiently on complex image data sets such as CalTech, with structural objects variability and clutter. It opens the possibility to simplify deep neural network learning by initializing the first layers with wavelet filters.



### Sequentially Generated Instance-Dependent Image Representations for Classification
- **Arxiv ID**: http://arxiv.org/abs/1312.6594v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1312.6594v3)
- **Published**: 2013-12-20 16:36:40+00:00
- **Updated**: 2014-02-11 17:07:21+00:00
- **Authors**: Gabriel Dulac-Arnold, Ludovic Denoyer, Nicolas Thome, Matthieu Cord, Patrick Gallinari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate a new framework for image classification that adaptively generates spatial representations. Our strategy is based on a sequential process that learns to explore the different regions of any image in order to infer its category. In particular, the choice of regions is specific to each image, directed by the actual content of previously selected regions.The capacity of the system to handle incomplete image information as well as its adaptive region selection allow the system to perform well in budgeted classification tasks by exploiting a dynamicly generated representation of each image. We demonstrate the system's abilities in a series of image-based exploration and classification tasks that highlight its learned exploration and inference abilities.



### Occupancy Detection in Vehicles Using Fisher Vector Image Representation
- **Arxiv ID**: http://arxiv.org/abs/1312.6024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6024v1)
- **Published**: 2013-12-20 16:37:46+00:00
- **Updated**: 2013-12-20 16:37:46+00:00
- **Authors**: Yusuf Artan, Peter Paul
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the high volume of traffic on modern roadways, transportation agencies have proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling (HOT) lanes to promote car pooling. However, enforcement of the rules of these lanes is currently performed by roadside enforcement officers using visual observation. Manual roadside enforcement is known to be inefficient, costly, potentially dangerous, and ultimately ineffective. Violation rates up to 50%-80% have been reported, while manual enforcement rates of less than 10% are typical. Therefore, there is a need for automated vehicle occupancy detection to support HOV/HOT lane enforcement. A key component of determining vehicle occupancy is to determine whether or not the vehicle's front passenger seat is occupied. In this paper, we examine two methods of determining vehicle front seat occupancy using a near infrared (NIR) camera system pointed at the vehicle's front windshield. The first method examines a state-of-the-art deformable part model (DPM) based face detection system that is robust to facial pose. The second method examines state-of- the-art local aggregation based image classification using bag-of-visual-words (BOW) and Fisher vectors (FV). A dataset of 3000 images was collected on a public roadway and is used to perform the comparison. From these experiments it is clear that the image classification approach is superior for this problem.



### Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1312.6034v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6034v2)
- **Published**: 2013-12-20 16:45:54+00:00
- **Updated**: 2014-04-19 11:54:52+00:00
- **Authors**: Karen Simonyan, Andrea Vedaldi, Andrew Zisserman
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].



### Efficient Visual Coding: From Retina To V2
- **Arxiv ID**: http://arxiv.org/abs/1312.6077v2
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1312.6077v2)
- **Published**: 2013-12-20 19:09:38+00:00
- **Updated**: 2014-12-18 01:03:58+00:00
- **Authors**: Honghao Shan, Garrison Cottrell
- **Comment**: For the ICLR 2014 conference
- **Journal**: None
- **Summary**: The human visual system has a hierarchical structure consisting of layers of processing, such as the retina, V1, V2, etc. Understanding the functional roles of these visual processing layers would help to integrate the psychophysiological and neurophysiological models into a consistent theory of human vision, and would also provide insights to computer vision research. One classical theory of the early visual pathway hypothesizes that it serves to capture the statistical structure of the visual inputs by efficiently coding the visual information in its outputs. Until recently, most computational models following this theory have focused upon explaining the receptive field properties of one or two visual layers. Recent work in deep networks has eliminated this concern, however, there is till the retinal layer to consider. Here we improve on a previously-described hierarchical model Recursive ICA (RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA, followed by a component-wise nonlinearity derived from considerations of the variable distributions expected by ICA. This process is then repeated. In this work, we improve on this model by using a new version of sparse PCA (sPCA), which results in biologically-plausible receptive fields for both the sPCA and ICA/sparse coding. When applied to natural image patches, our model learns visual features exhibiting the receptive field properties of retinal ganglion cells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complex cells, and V2 cells. Our work provides predictions for experimental neuroscience studies. For example, our result suggests that a previous neurophysiological study improperly discarded some of their recorded neurons; we predict that their discarded neurons capture the shape contour of objects.



### Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1312.6082v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6082v4)
- **Published**: 2013-12-20 19:25:44+00:00
- **Updated**: 2014-04-14 05:25:54+00:00
- **Authors**: Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localization, segmentation, and recognition steps. In this paper we propose a unified approach that integrates these three steps via the use of a deep convolutional neural network that operates directly on the image pixels. We employ the DistBelief implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the performance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over $96\%$ accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over $90\%$ accuracy. To further explore the applicability of the proposed system to broader text recognition tasks, we apply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of the most secure reverse turing tests that uses distorted text to distinguish humans from bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA. Our evaluations on both tasks indicate that at specific operating thresholds, the performance of the proposed system is comparable to, and in some cases exceeds, that of human operators.



### Correlation-based construction of neighborhood and edge features
- **Arxiv ID**: http://arxiv.org/abs/1312.7335v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1312.7335v2)
- **Published**: 2013-12-20 19:36:51+00:00
- **Updated**: 2014-02-16 23:17:39+00:00
- **Authors**: Bal√°zs K√©gl
- **Comment**: None
- **Journal**: None
- **Summary**: Motivated by an abstract notion of low-level edge detector filters, we propose a simple method of unsupervised feature construction based on pairwise statistics of features. In the first step, we construct neighborhoods of features by regrouping features that correlate. Then we use these subsets as filters to produce new neighborhood features. Next, we connect neighborhood features that correlate, and construct edge features by subtracting the correlated neighborhood features of each other. To validate the usefulness of the constructed features, we ran AdaBoost.MH on four multi-class classification problems. Our most significant result is a test error of 0.94% on MNIST with an algorithm which is essentially free of any image-specific priors. On CIFAR-10 our method is suboptimal compared to today's best deep learning techniques, nevertheless, we show that the proposed method outperforms not only boosting on the raw pixels, but also boosting on Haar filters.



### Multi-View Priors for Learning Detectors from Sparse Viewpoint Data
- **Arxiv ID**: http://arxiv.org/abs/1312.6095v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6095v2)
- **Published**: 2013-12-20 20:12:07+00:00
- **Updated**: 2014-02-16 10:39:35+00:00
- **Authors**: Bojan Pepik, Michael Stark, Peter Gehler, Bernt Schiele
- **Comment**: 13 pages, 7 figures, 4 tables, International Conference on Learning
  Representations 2014
- **Journal**: None
- **Summary**: While the majority of today's object class models provide only 2D bounding boxes, far richer output hypotheses are desirable including viewpoint, fine-grained category, and 3D geometry estimate. However, models trained to provide richer output require larger amounts of training data, preferably well covering the relevant aspects such as viewpoint and fine-grained categories. In this paper, we address this issue from the perspective of transfer learning, and design an object class model that explicitly leverages correlations between visual features. Specifically, our model represents prior distributions over permissible multi-view detectors in a parametric way -- the priors are learned once from training data of a source object class, and can later be used to facilitate the learning of a detector for a target class. As we show in our experiments, this transfer is not only beneficial for detectors based on basic-level category representations, but also enables the robust learning of detectors that represent classes at finer levels of granularity, where training data is typically even scarcer and more unbalanced. As a result, we report largely improved performance in simultaneous 2D object localization and viewpoint estimation on a recent dataset of challenging street scenes.



### Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
- **Arxiv ID**: http://arxiv.org/abs/1312.6120v3
- **DOI**: None
- **Categories**: **cs.NE**, cond-mat.dis-nn, cs.CV, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1312.6120v3)
- **Published**: 2013-12-20 20:24:00+00:00
- **Updated**: 2014-02-19 17:26:57+00:00
- **Authors**: Andrew M. Saxe, James L. McClelland, Surya Ganguli
- **Comment**: Submission to ICLR2014. Revised based on reviewer feedback
- **Journal**: None
- **Summary**: Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.



### Learning Generative Models with Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1312.6110v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6110v3)
- **Published**: 2013-12-20 20:50:43+00:00
- **Updated**: 2015-02-21 22:21:15+00:00
- **Authors**: Yichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov
- **Comment**: In the proceedings of Neural Information Processing Systems, 2014
- **Journal**: None
- **Summary**: Attention has long been proposed by psychologists as important for effectively dealing with the enormous sensory stimulus available in the neocortex. Inspired by the visual attention models in computational neuroscience and the need of object-centric data for generative models, we describe for generative learning framework using attentional mechanisms. Attentional mechanisms can propagate signals from region of interest in a scene to an aligned canonical representation, where generative modeling takes place. By ignoring background clutter, generative models can concentrate their resources on the object of interest. Our model is a proper graphical model where the 2D Similarity transformation is a part of the top-down process. A ConvNet is employed to provide good initializations during posterior inference which is based on Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to face regions of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.



### Deep Belief Networks for Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1312.6158v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1312.6158v2)
- **Published**: 2013-12-20 21:56:38+00:00
- **Updated**: 2014-01-02 17:04:35+00:00
- **Authors**: Mohammad Ali Keyvanrad, Mohammad Pezeshki, Mohammad Ali Homayounpour
- **Comment**: ICLR 2014 Conference track
- **Journal**: None
- **Summary**: Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs' ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images.



### Learned versus Hand-Designed Feature Representations for 3d Agglomeration
- **Arxiv ID**: http://arxiv.org/abs/1312.6159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1312.6159v1)
- **Published**: 2013-12-20 21:59:03+00:00
- **Updated**: 2013-12-20 21:59:03+00:00
- **Authors**: John A. Bogovic, Gary B. Huang, Viren Jain
- **Comment**: None
- **Journal**: None
- **Summary**: For image recognition and labeling tasks, recent results suggest that machine learning methods that rely on manually specified feature representations may be outperformed by methods that automatically derive feature representations based on the data. Yet for problems that involve analysis of 3d objects, such as mesh segmentation, shape retrieval, or neuron fragment agglomeration, there remains a strong reliance on hand-designed feature descriptors. In this paper, we evaluate a large set of hand-designed 3d feature descriptors alongside features learned from the raw data using both end-to-end and unsupervised learning techniques, in the context of agglomeration of 3d neuron fragments. By combining unsupervised learning techniques with a novel dynamic pooling scheme, we show how pure learning-based methods are for the first time competitive with hand-designed 3d shape descriptors. We investigate data augmentation strategies for dramatically increasing the size of the training set, and show how combining both learned and hand-designed features leads to the highest accuracy.



### Learning Paired-associate Images with An Unsupervised Deep Learning Architecture
- **Arxiv ID**: http://arxiv.org/abs/1312.6171v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1312.6171v2)
- **Published**: 2013-12-20 23:07:25+00:00
- **Updated**: 2014-01-10 23:19:26+00:00
- **Authors**: Ti Wang, Daniel L. Silver
- **Comment**: 9 pages, for ICLR-2014
- **Journal**: None
- **Summary**: This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities, or channels, such that input on one channel will correctly generate the associated response at the other and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of a bi-directional network and unsupervised learning from either paired or non-paired training examples.



