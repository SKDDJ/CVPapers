# Arxiv Papers in cs.CV on 2012-08-27
### A Missing and Found Recognition System for Hajj and Umrah
- **Arxiv ID**: http://arxiv.org/abs/1208.5365v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1208.5365v1)
- **Published**: 2012-08-27 11:20:49+00:00
- **Updated**: 2012-08-27 11:20:49+00:00
- **Authors**: Salah A. Aly
- **Comment**: website available via http://www.mfhajj.com
- **Journal**: None
- **Summary**: This note describes an integrated recognition system for identifying missing and found objects as well as missing, dead, and found people during Hajj and Umrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of Saudi Arabia. It is assumed that the total estimated number of pilgrims will reach 20 millions during the next decade. The ultimate goal of this system is to integrate facial recognition and object identification solutions into the Hajj and Umrah rituals. The missing and found computerized system is part of the CrowdSensing system for Hajj and Umrah crowd estimation, management and safety.



### Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity Analysis from a Single Video
- **Arxiv ID**: http://arxiv.org/abs/1208.5451v1
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, I.4.7; I.5.3; I.6.4
- **Links**: [PDF](http://arxiv.org/pdf/1208.5451v1)
- **Published**: 2012-08-27 17:21:39+00:00
- **Updated**: 2012-08-27 17:21:39+00:00
- **Authors**: Zhongwei Tang, Alexey Castrodad, Mariano Tepper, Guillermo Sapiro
- **Comment**: None
- **Journal**: None
- **Summary**: A framework for unsupervised group activity analysis from a single video is here presented. Our working hypothesis is that human actions lie on a union of low-dimensional subspaces, and thus can be efficiently modeled as sparse linear combinations of atoms from a learned dictionary representing the action's primitives. Contrary to prior art, and with the primary goal of spatio-temporal action grouping, in this work only one single video segment is available for both unsupervised learning and analysis without any prior training information. After extracting simple features at a single spatio-temporal scale, we learn a dictionary for each individual in the video during each short time lapse. These dictionaries allow us to compare the individuals' actions by producing an affinity matrix which contains sufficient discriminative information about the actions in the scene leading to grouping with simple and efficient tools. With diverse publicly available real videos, we demonstrate the effectiveness of the proposed framework and its robustness to cluttered backgrounds, changes of human appearance, and action variability.



