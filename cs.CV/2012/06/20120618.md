# Arxiv Papers in cs.CV on 2012-06-18
### On multi-view feature learning
- **Arxiv ID**: http://arxiv.org/abs/1206.4609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4609v1)
- **Published**: 2012-06-18 14:45:17+00:00
- **Updated**: 2012-06-18 14:45:17+00:00
- **Authors**: Roland Memisevic
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.



### Manifold Relevance Determination
- **Arxiv ID**: http://arxiv.org/abs/1206.4610v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4610v1)
- **Published**: 2012-06-18 14:45:37+00:00
- **Updated**: 2012-06-18 14:45:37+00:00
- **Authors**: Andreas Damianou, Carl Ek, Michalis Titsias, Neil Lawrence
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a "softly" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.



### Modeling Latent Variable Uncertainty for Loss-based Learning
- **Arxiv ID**: http://arxiv.org/abs/1206.4636v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1206.4636v1)
- **Published**: 2012-06-18 15:15:13+00:00
- **Updated**: 2012-06-18 15:15:13+00:00
- **Authors**: M. Pawan Kumar, Ben Packer, Daphne Koller
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent SVM in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems---object detection and action detection---using publicly available datasets.



### Total Variation and Euler's Elastica for Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1206.4641v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4641v1)
- **Published**: 2012-06-18 15:18:20+00:00
- **Updated**: 2012-06-18 15:18:20+00:00
- **Authors**: Tong Lin, Hanlin Xue, Ling Wang, Hongbin Zha
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: In recent years, total variation (TV) and Euler's elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler's elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.



### Learning Efficient Structured Sparse Models
- **Arxiv ID**: http://arxiv.org/abs/1206.4649v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4649v1)
- **Published**: 2012-06-18 15:23:19+00:00
- **Updated**: 2012-06-18 15:23:19+00:00
- **Authors**: Alex Bronstein, Pablo Sprechmann, Guillermo Sapiro
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we develop a novel block-coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems, and show an efficient feed forward architecture derived from its iteration. This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre-given dictionary, as in earlier formulations, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.



### Is margin preserved after random projection?
- **Arxiv ID**: http://arxiv.org/abs/1206.4651v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4651v1)
- **Published**: 2012-06-18 15:24:01+00:00
- **Updated**: 2012-06-18 15:24:01+00:00
- **Authors**: Qinfeng Shi, Chunhua Shen, Rhys Hill, Anton van den Hengel
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data.



### Dimensionality Reduction by Local Discriminative Gaussians
- **Arxiv ID**: http://arxiv.org/abs/1206.4653v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4653v1)
- **Published**: 2012-06-18 15:24:49+00:00
- **Updated**: 2012-06-18 15:24:49+00:00
- **Authors**: Nathan Parrish, Maya Gupta
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.



### Clustering by Low-Rank Doubly Stochastic Matrix Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1206.4676v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4676v1)
- **Published**: 2012-06-18 15:36:49+00:00
- **Updated**: 2012-06-18 15:36:49+00:00
- **Authors**: Zhirong Yang, Erkki Oja
- **Comment**: ICML2012
- **Journal**: None
- **Summary**: Clustering analysis by nonnegative low-rank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a two-step bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.



### The Ultrasound Visualization Pipeline - A Survey
- **Arxiv ID**: http://arxiv.org/abs/1206.3975v1
- **DOI**: 10.1007/978-1-4471-6497-5_24
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1206.3975v1)
- **Published**: 2012-06-18 16:05:47+00:00
- **Updated**: 2012-06-18 16:05:47+00:00
- **Authors**: Åsmund Birkeland, Veronika Solteszova, Dieter Hönigmann, Odd Helge Gilja, Svein Brekke, Timo Ropinski, Ivan Viola
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound is one of the most frequently used imaging modality in medicine. The high spatial resolution, its interactive nature and non-invasiveness makes it the first choice in many examinations. Image interpretation is one of ultrasound's main challenges. Much training is required to obtain a confident skill level in ultrasound-based diagnostics. State-of-the-art graphics techniques is needed to provide meaningful visualizations of ultrasound in real-time. In this paper we present the process-pipeline for ultrasound visualization, including an overview of the tasks performed in the specific steps. To provide an insight into the trends of ultrasound visualization research, we have selected a set of significant publications and divided them into a technique-based taxonomy covering the topics pre-processing, segmentation, registration, rendering and augmented reality. For the different technique types we discuss the difference between ultrasound-based techniques and techniques for other modalities.



### A Linear Approximation to the chi^2 Kernel with Geometric Convergence
- **Arxiv ID**: http://arxiv.org/abs/1206.4074v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1206.4074v3)
- **Published**: 2012-06-18 21:05:16+00:00
- **Updated**: 2013-06-12 19:29:18+00:00
- **Authors**: Fuxin Li, Guy Lebanon, Cristian Sminchisescu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new analytical approximation to the $\chi^2$ kernel that converges geometrically. The analytical approximation is derived with elementary methods and adapts to the input distribution for optimal convergence rate. Experiments show the new approximation leads to improved performance in image classification and semantic segmentation tasks using a random Fourier feature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-core principal component analysis (PCA) methods are introduced to reduce the dimensionality of the approximation and achieve better performance at the expense of only an additional constant factor to the time complexity. Moreover, when PCA is performed jointly on the training and unlabeled testing data, further performance improvements can be obtained. Experiments conducted on the PASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show statistically significant improvements over alternative approximation methods.



