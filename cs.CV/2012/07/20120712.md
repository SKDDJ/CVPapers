# Arxiv Papers in cs.CV on 2012-07-12
### ROI Segmentation for Feature Extraction from Human Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1207.2922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1207.2922v1)
- **Published**: 2012-07-12 11:11:35+00:00
- **Updated**: 2012-07-12 11:11:35+00:00
- **Authors**: Surbhi, Vishal Arora
- **Comment**: 4 pages, 2 figures; International Journal of Research in Computer
  Science, pp. 61-64 (2012)
- **Journal**: None
- **Summary**: Human Computer Interaction (HCI) is the biggest goal of computer vision researchers. Features form the different facial images are able to provide a very deep knowledge about the activities performed by the different facial movements. In this paper we presented a technique for feature extraction from various regions of interest with the help of Skin color segmentation technique, Thresholding, knowledge based technique for face recognition.



### Non-Local Euclidean Medians
- **Arxiv ID**: http://arxiv.org/abs/1207.3056v2
- **DOI**: 10.1109/LSP.2012.2217329
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1207.3056v2)
- **Published**: 2012-07-12 18:39:10+00:00
- **Updated**: 2012-08-24 21:25:51+00:00
- **Authors**: Kunal N. Chaudhury, Amit Singer
- **Comment**: 6 figures, 1 table
- **Journal**: IEEE Signal Processing Letters, vol. 19(11), pp. 745 - 748, 2012
- **Summary**: In this letter, we note that the denoising performance of Non-Local Means (NLM) at large noise levels can be improved by replacing the mean by the Euclidean median. We call this new denoising algorithm the Non-Local Euclidean Medians (NLEM). At the heart of NLEM is the observation that the median is more robust to outliers than the mean. In particular, we provide a simple geometric insight that explains why NLEM performs better than NLM in the vicinity of edges, particularly at large noise levels. NLEM can be efficiently implemented using iteratively reweighted least squares, and its computational complexity is comparable to that of NLM. We provide some preliminary results to study the proposed algorithm and to compare it with NLM.



### Supervised Texture Classification Using a Novel Compression-Based Similarity Measure
- **Arxiv ID**: http://arxiv.org/abs/1207.3071v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1207.3071v2)
- **Published**: 2012-07-12 19:37:13+00:00
- **Updated**: 2013-11-26 17:32:56+00:00
- **Authors**: Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel
- **Comment**: This paper has been withdrawn by the author since it has already been
  appeared in the proceedings of International Conference on Computer vision
  and Graphics (ICCVG)
- **Journal**: None
- **Summary**: Supervised pixel-based texture classification is usually performed in the feature space. We propose to perform this task in (dis)similarity space by introducing a new compression-based (dis)similarity measure. The proposed measure utilizes two dimensional MPEG-1 encoder, which takes into consideration the spatial locality and connectivity of pixels in the images. The proposed formulation has been carefully designed based on MPEG encoder functionality. To this end, by design, it solely uses P-frame coding to find the (dis)similarity among patches/images. We show that the proposed measure works properly on both small and large patch sizes. Experimental results show that the proposed approach significantly improves the performance of supervised pixel-based texture classification on Brodatz and outdoor images compared to other compression-based dissimilarity measures as well as approaches performed in feature space. It also improves the computation speed by about 40% compared to its rivals.



### Probabilistic index maps for modeling natural signals
- **Arxiv ID**: http://arxiv.org/abs/1207.4179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1207.4179v1)
- **Published**: 2012-07-12 19:47:14+00:00
- **Updated**: 2012-07-12 19:47:14+00:00
- **Authors**: Nebojsa Jojic, Yaron Caspi, Manuel Reyes-Gomez
- **Comment**: Appears in Proceedings of the Twentieth Conference on Uncertainty in
  Artificial Intelligence (UAI2004)
- **Journal**: None
- **Summary**: One of the major problems in modeling natural signals is that signals with very similar structure may locally have completely different measurements, e.g., images taken under different illumination conditions, or the speech signal captured in different environments. While there have been many successful attempts to address these problems in application-specific settings, we believe that underlying a large set of problems in signal representation is a representational deficiency of intensity-derived local measurements that are the basis of most efficient models. We argue that interesting structure in signals is better captured when the signal is de- fined as a matrix whose entries are discrete indices to a separate palette of possible measurements. In order to model the variability in signal structure, we define a signal class not by a single index map, but by a probability distribution over the index maps, which can be estimated from the data, and which we call probabilistic index maps. The existing algorithm can be adapted to work with this representation. Furthermore, the probabilistic index map representation leads to algorithms with computational costs proportional to either the size of the palette or the log of the size of the palette, making the cost of significantly increased invariance to non-structural changes quite bearable. We illustrate the benefits of the probabilistic index map representation in several applications in computer vision and speech processing.



