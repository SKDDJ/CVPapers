# Arxiv Papers in cs.CV on 2012-07-10
### Improvement of ISOM by using filter
- **Arxiv ID**: http://arxiv.org/abs/1207.2268v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1207.2268v1)
- **Published**: 2012-07-10 08:49:48+00:00
- **Updated**: 2012-07-10 08:49:48+00:00
- **Authors**: Imen Chaabouni, Wiem Fourati, Med Salim Bouhlel
- **Comment**: 6 pages, 3 figures, 2 tables; JCSI (May 2012 issue, Volume 9, Issue
  3) and having paper id IJCSI-2012-9-3-2860
- **Journal**: None
- **Summary**: Image compression helps in storing the transmitted data in proficient way by decreasing its redundancy. This technique helps in transferring more digital or multimedia data over internet as it increases the storage space. It is important to maintain the image quality even if it is compressed to certain extent. Depend upon this the image compression is classified into two categories : lossy and lossless image compression. There are many lossy digital image compression techniques exists. Among this Incremental Self Organizing Map is a familiar one. The good pictures quality can be retrieved if image denoising technique is used for compression and also provides better compression ratio. Image denoising is an important pre-processing step for many image analysis and computer vision system. It refers to the task of recovering a good estimate of the true image from a degraded observation without altering and changing useful structure in the image such as discontinuities and edges. Many approaches have been proposed to remove the noise effectively while preserving the original image details and features as much as possible. This paper proposes a technique for image compression using Incremental Self Organizing Map (ISOM) with Discret Wavelet Transform (DWT) by applying filtering techniques which play a crucial role in enhancing the quality of a reconstructed image. The experimental result shows that the proposed technique obtained better compression ratio value.



### Cups Products in Z2-Cohomology of 3D Polyhedral Complexes
- **Arxiv ID**: http://arxiv.org/abs/1207.2346v3
- **DOI**: None
- **Categories**: **cs.CV**, 55-XX
- **Links**: [PDF](http://arxiv.org/pdf/1207.2346v3)
- **Published**: 2012-07-10 13:40:40+00:00
- **Updated**: 2013-07-09 21:18:05+00:00
- **Authors**: Rocio Gonalez-Diaz, Javier Lamar, Ronald Umble
- **Comment**: None
- **Journal**: None
- **Summary**: Let $I=(\mathbb{Z}^3,26,6,B)$ be a 3D digital image, let $Q(I)$ be the associated cubical complex and let $\partial Q(I)$ be the subcomplex of $Q(I)$ whose maximal cells are the quadrangles of $Q(I)$ shared by a voxel of $B$ in the foreground -- the object under study -- and by a voxel of $\mathbb{Z}^3\smallsetminus B$ in the background -- the ambient space. We show how to simplify the combinatorial structure of $\partial Q(I)$ and obtain a 3D polyhedral complex $P(I)$ homeomorphic to $\partial Q(I)$ but with fewer cells. We introduce an algorithm that computes cup products on $H^*(P(I);\mathbb{Z}_2)$ directly from the combinatorics. The computational method introduced here can be effectively applied to any polyhedral complex embedded in $\mathbb{R}^3$.



### Dual-Space Analysis of the Sparse Linear Model
- **Arxiv ID**: http://arxiv.org/abs/1207.2422v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1207.2422v1)
- **Published**: 2012-07-10 17:38:18+00:00
- **Updated**: 2012-07-10 17:38:18+00:00
- **Authors**: David Wipf, Yi Wu
- **Comment**: 9 pages, 2 figures, submission to NIPS 2012
- **Journal**: None
- **Summary**: Sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients. These priors can conveniently be expressed as a maximization over zero-mean Gaussians with different variance hyperparameters. Standard MAP estimation (Type I) involves maximizing over both the hyperparameters and coefficients, while an empirical Bayesian alternative (Type II) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation. The underlying cost functions can be related via a dual-space framework from Wipf et al. (2011), which allows both the Type I or Type II objectives to be expressed in either coefficient or hyperparmeter space. This perspective is useful because some analyses or extensions are more conducive to development in one space or the other. Herein we consider the estimation of a trade-off parameter balancing sparsity and data fit. As this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from Type II to solve what is much less intuitive for Type I. In contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for Type I and apply them to Type II. For example, this allows us to prove that Type II-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (RIP) lead to failure of popular L1 reconstructions. It also facilitates the analysis of Type II when non-Gaussian likelihood models lead to intractable integrations.



### A Multi-Agents Architecture to Learn Vision Operators and their Parameters
- **Arxiv ID**: http://arxiv.org/abs/1207.2426v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1207.2426v1)
- **Published**: 2012-07-10 17:56:00+00:00
- **Updated**: 2012-07-10 17:56:00+00:00
- **Authors**: Issam Qaffou, Mohammed Sadgal, Abdelaziz Elfazziki
- **Comment**: IJCSI, May 2012
- **Journal**: None
- **Summary**: In a vision system, every task needs that the operators to apply should be {\guillemotleft} well chosen {\guillemotright} and their parameters should be also {\guillemotleft} well adjusted {\guillemotright}. The diversity of operators and the multitude of their parameters constitute a big challenge for users. As it is very difficult to make the {\guillemotleft} right {\guillemotright} choice, lack of a specific rule, many disadvantages appear and affect the computation time and especially the quality of results. In this paper we present a multi-agent architecture to learn the best operators to apply and their best parameters for a class of images. Our architecture consists of three types of agents: User Agent, Operator Agent and Parameter Agent. The User Agent determines the phases of treatment, a library of operators and the possible values of their parameters. The Operator Agent constructs all possible combinations of operators and the Parameter Agent, the core of the architecture, adjusts the parameters of each combination by treating a large number of images. Through the reinforcement learning mechanism, our architecture does not consider only the system opportunities but also the user preferences.



### Non-Convex Rank Minimization via an Empirical Bayesian Approach
- **Arxiv ID**: http://arxiv.org/abs/1207.2440v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1207.2440v1)
- **Published**: 2012-07-10 18:41:04+00:00
- **Updated**: 2012-07-10 18:41:04+00:00
- **Authors**: David Wipf
- **Comment**: 10 pages, 6 figures, UAI 2012 paper
- **Journal**: None
- **Summary**: In many applications that require matrix solutions of minimal rank, the underlying cost function is non-convex leading to an intractable, NP-hard optimization problem. Consequently, the convex nuclear norm is frequently used as a surrogate penalty term for matrix rank. The problem is that in many practical scenarios there is no longer any guarantee that we can correctly estimate generative low-rank matrices of interest, theoretical special cases notwithstanding. Consequently, this paper proposes an alternative empirical Bayesian procedure build upon a variational approximation that, unlike the nuclear norm, retains the same globally minimizing point estimate as the rank function under many useful constraints. However, locally minimizing solutions are largely smoothed away via marginalization, allowing the algorithm to succeed when standard convex relaxations completely fail. While the proposed methodology is generally applicable to a wide range of low-rank applications, we focus our attention on the robust principal component analysis problem (RPCA), which involves estimating an unknown low-rank matrix with unknown sparse corruptions. Theoretical and empirical evidence are presented to show that our method is potentially superior to related MAP-based approaches, for which the convex principle component pursuit (PCP) algorithm (Candes et al., 2011) can be viewed as a special case.



### Kernelized Supervised Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1207.2488v4
- **DOI**: 10.1109/TSP.2013.2274276
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1207.2488v4)
- **Published**: 2012-07-10 20:52:46+00:00
- **Updated**: 2013-11-26 17:29:04+00:00
- **Authors**: Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel
- **Comment**: This paper has been withdrawn by the author as it has been already
  published by the IEEE Trans. on Signal Processing and is now available online
- **Journal**: None
- **Summary**: In this paper, we propose supervised dictionary learning (SDL) by incorporating information on class labels into the learning of the dictionary. To this end, we propose to learn the dictionary in a space where the dependency between the signals and their corresponding labels is maximized. To maximize this dependency, the recently introduced Hilbert Schmidt independence criterion (HSIC) is used. One of the main advantages of this novel approach for SDL is that it can be easily kernelized by incorporating a kernel, particularly a data-derived kernel such as normalized compression distance, into the formulation. The learned dictionary is compact and the proposed approach is fast. We show that it outperforms other unsupervised and supervised dictionary learning approaches in the literature, using real-world data.



