# Arxiv Papers in cs.CV on 2012-12-31
### Autonomously Learning to Visually Detect Where Manipulation Will Succeed
- **Arxiv ID**: http://arxiv.org/abs/1212.6837v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1212.6837v1)
- **Published**: 2012-12-31 08:39:14+00:00
- **Updated**: 2012-12-31 08:39:14+00:00
- **Authors**: Hai Nguyen, Charles C. Kemp
- **Comment**: 15 pages, 10 figures. Submitted to the Autonomous Robots Journal
  Special Issue "Beyond Grasping - Modern Approaches for Dexterous
  Manipulation"
- **Journal**: None
- **Summary**: Visual features can help predict if a manipulation behavior will succeed at a given location. For example, the success of a behavior that flips light switches depends on the location of the switch. Within this paper, we present methods that enable a mobile manipulator to autonomously learn a function that takes an RGB image and a registered 3D point cloud as input and returns a 3D location at which a manipulation behavior is likely to succeed. Given a pair of manipulation behaviors that can change the state of the world between two sets (e.g., light switch up and light switch down), classifiers that detect when each behavior has been successful, and an initial hint as to where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine (SVM) classifiers by trying out the behaviors at locations in the world and observing the results. When an image feature vector associated with a 3D location is provided as input to one of the SVMs, the SVM predicts if the associated manipulation behavior will be successful at the 3D location. To evaluate our approach, we performed experiments with a PR2 robot from Willow Garage in a simulated home using behaviors that flip a light switch, push a rocker-type light switch, and operate a drawer. By using active learning, the robot efficiently learned SVMs that enabled it to consistently succeed at these tasks. After training, the robot also continued to learn in order to adapt in the event of failure.



### On Automation and Medical Image Interpretation, With Applications for Laryngeal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1212.6933v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1212.6933v3)
- **Published**: 2012-12-31 17:38:02+00:00
- **Updated**: 2013-01-14 23:25:20+00:00
- **Authors**: H. J. Moukalled
- **Comment**: 18 pages, 9 figures, 41 references
- **Journal**: None
- **Summary**: Indeed, these are exciting times. We are in the heart of a digital renaissance. Automation and computer technology allow engineers and scientists to fabricate processes that amalgamate quality of life. We anticipate much growth in medical image interpretation and understanding, due to the influx of computer technologies. This work should serve as a guide to introduce the reader to core themes in theoretical computer science, as well as imaging applications for understanding vocal-fold vibrations. In this work, we motivate the use of automation, review some mathematical models of computation. We present a proof of a classical problem in image analysis that cannot be automated by means of algorithms. Furthermore, discuss some applications for processing medical images of the vocal folds, and discuss some of the exhilarating directions the art of automation will take vocal-fold image interpretation and quite possibly other areas of biomedical image analysis.



