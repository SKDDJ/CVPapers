# Arxiv Papers in cs.CV on 2012-12-04
### G-invariant Persistent Homology
- **Arxiv ID**: http://arxiv.org/abs/1212.0655v5
- **DOI**: None
- **Categories**: **math.AT**, cs.CG, cs.CV, 55N35 (Primary) 68U05 (Secondary), I.4.7; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1212.0655v5)
- **Published**: 2012-12-04 09:57:59+00:00
- **Updated**: 2013-12-21 10:54:55+00:00
- **Authors**: Patrizio Frosini
- **Comment**: 14 pages, 4 figures. Remark 4.2 has been expanded to become
  Subsection 4.2, including a new example (Example 4.3). The section
  "Discussion and further research" and some references have been added. Small
  changes in the text
- **Journal**: None
- **Summary**: Classical persistent homology is a powerful mathematical tool for shape comparison. Unfortunately, it is not tailored to study the action of transformation groups that are different from the group Homeo(X) of all self-homeomorphisms of a topological space X. This fact restricts its use in applications. In order to obtain better lower bounds for the natural pseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adapt persistent homology and consider G-invariant persistent homology. Roughly speaking, the main idea consists in defining persistent homology by means of a set of chains that is invariant under the action of G. In this paper we formalize this idea, and prove the stability of the persistent Betti number functions in G-invariant persistent homology with respect to the natural pseudo-distance d_G. We also show how G-invariant persistent homology could be used in applications concerning shape comparison, when the invariance group is a proper subgroup of the group of all self-homeomorphisms of a topological space. In this paper we will assume that the space X is triangulable, in order to guarantee that the persistent Betti number functions are finite without using any tameness assumption.



### Training Support Vector Machines Using Frank-Wolfe Optimization Methods
- **Arxiv ID**: http://arxiv.org/abs/1212.0695v1
- **DOI**: 10.1142/S0218001413600033
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1212.0695v1)
- **Published**: 2012-12-04 12:05:31+00:00
- **Updated**: 2012-12-04 12:05:31+00:00
- **Authors**: Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi, Claudio Sartori
- **Comment**: None
- **Journal**: International Journal on Pattern Recognition and Artificial
  Intelligence, 27(3), 2013
- **Summary**: Training a Support Vector Machine (SVM) requires the solution of a quadratic programming problem (QP) whose computational complexity becomes prohibitively expensive for large scale datasets. Traditional optimization methods cannot be directly applied in these cases, mainly due to memory restrictions.   By adopting a slightly different objective function and under mild conditions on the kernel used within the model, efficient algorithms to train SVMs have been devised under the name of Core Vector Machines (CVMs). This framework exploits the equivalence of the resulting learning problem with the task of building a Minimal Enclosing Ball (MEB) problem in a feature space, where data is implicitly embedded by a kernel function.   In this paper, we improve on the CVM approach by proposing two novel methods to build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast method to approximate the solution of a MEB problem. In contrast to CVMs, our algorithms do not require to compute the solutions of a sequence of increasingly complex QPs and are defined by using only analytic optimization steps. Experiments on a large collection of datasets show that our methods scale better than CVMs in most cases, sometimes at the price of a slightly lower accuracy. As CVMs, the proposed methods can be easily extended to machine learning problems other than binary classification. However, effective classifiers are also obtained using kernels which do not satisfy the condition required by CVMs and can thus be used for a wider set of problems.



### A Topological Code for Plane Images
- **Arxiv ID**: http://arxiv.org/abs/1212.0819v1
- **DOI**: None
- **Categories**: **cs.CV**, math.GT
- **Links**: [PDF](http://arxiv.org/pdf/1212.0819v1)
- **Published**: 2012-12-04 18:39:14+00:00
- **Updated**: 2012-12-04 18:39:14+00:00
- **Authors**: Evgeny Shchepin
- **Comment**: None
- **Journal**: None
- **Summary**: It is proposed a new code for contours of plane images. This code was applied for optical character recognition of printed and handwritten characters. One can apply it to recognition of any visual images.



### Unmixing of Hyperspectral Data Using Robust Statistics-based NMF
- **Arxiv ID**: http://arxiv.org/abs/1212.0888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1212.0888v1)
- **Published**: 2012-12-04 21:59:35+00:00
- **Updated**: 2012-12-04 21:59:35+00:00
- **Authors**: Roozbeh Rajabi, Hassan Ghassemian
- **Comment**: 4 pages, conference
- **Journal**: None
- **Summary**: Mixed pixels are presented in hyperspectral images due to low spatial resolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels spectra into endmembers spectra and abundance fractions. In this paper using of robust statistics-based nonnegative matrix factorization (RNMF) for spectral unmixing of hyperspectral data is investigated. RNMF uses a robust cost function and iterative updating procedure, so is not sensitive to outliers. This method has been applied to simulated data using USGS spectral library, AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF method based on SAD and AAD measures. Results demonstrate that this method can be used efficiently for hyperspectral unmixing purposes.



