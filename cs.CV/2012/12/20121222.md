# Arxiv Papers in cs.CV on 2012-12-22
### High-precision camera distortion measurements with a "calibration harp"
- **Arxiv ID**: http://arxiv.org/abs/1212.5656v1
- **DOI**: 10.1364/JOSAA.29.002134
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1212.5656v1)
- **Published**: 2012-12-22 05:00:01+00:00
- **Updated**: 2012-12-22 05:00:01+00:00
- **Authors**: Zhongwei Tang, Rafael Grompone von Gioi, Pascal Monasse, Jean-Michel Morel
- **Comment**: None
- **Journal**: JOSA A, Vol. 29, Issue 10, pp. 2134-2143 (2012)
- **Summary**: This paper addresses the high precision measurement of the distortion of a digital camera from photographs. Traditionally, this distortion is measured from photographs of a flat pattern which contains aligned elements. Nevertheless, it is nearly impossible to fabricate a very flat pattern and to validate its flatness. This fact limits the attainable measurable precisions. In contrast, it is much easier to obtain physically very precise straight lines by tightly stretching good quality strings on a frame. Taking literally "plumb-line methods", we built a "calibration harp" instead of the classic flat patterns to obtain a high precision measurement tool, demonstrably reaching 2/100 pixel precisions. The harp is complemented with the algorithms computing automatically from harp photographs two different and complementary lens distortion measurements. The precision of the method is evaluated on images corrected by state-of-the-art distortion correction algorithms, and by popular software. Three applications are shown: first an objective and reliable measurement of the result of any distortion correction. Second, the harp permits to control state-of-the art global camera calibration algorithms: It permits to select the right distortion model, thus avoiding internal compensation errors inherent to these methods. Third, the method replaces manual procedures in other distortion correction methods, makes them fully automatic, and increases their reliability and precision.



### Normalized Compression Distance of Multisets with Applications
- **Arxiv ID**: http://arxiv.org/abs/1212.5711v4
- **DOI**: 10.1109/TPAMI.2014.2375175
- **Categories**: **cs.CV**, cs.IT, math.IT, physics.data-an, I.5.3; H.3.3; E.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1212.5711v4)
- **Published**: 2012-12-22 17:37:03+00:00
- **Updated**: 2013-03-29 17:00:20+00:00
- **Authors**: Andrew R. Cohen, Paul M. B. Vitanyi
- **Comment**: LaTeX 28 pages, 3 figures. This version is changed from the
  preliminary version to the final version. Updates of the theory. How to
  compute it, special recepies for classification, more applications and better
  results (see abstract and especially the detailed results in the paper). The
  title was changed to reflect this. In v4 corrected the proof of Theorem III-7
- **Journal**: IEEE Trans. Pattern Analysis and Machine Intelligence, 37:8(2015),
  1602-1614
- **Summary**: Normalized compression distance (NCD) is a parameter-free, feature-free, alignment-free, similarity measure between a pair of finite objects based on compression. However, it is not sufficient for all applications. We propose an NCD of finite multisets (a.k.a. multiples) of finite objects that is also a metric. Previously, attempts to obtain such an NCD failed. We cover the entire trajectory from theoretical underpinning to feasible practice. The new NCD for multisets is applied to retinal progenitor cell classification questions and to related synthetically generated data that were earlier treated with the pairwise NCD. With the new method we achieved significantly better results. Similarly for questions about axonal organelle transport. We also applied the new NCD to handwritten digit recognition and improved classification accuracy significantly over that of pairwise NCD by incorporating both the pairwise and NCD for multisets. In the analysis we use the incomputable Kolmogorov complexity that for practical purposes is approximated from above by the length of the compressed version of the file involved, using a real-world compression program.   Index Terms--- Normalized compression distance, multisets or multiples, pattern recognition, data mining, similarity, classification, Kolmogorov complexity, retinal progenitor cells, synthetic data, organelle transport, handwritten character recognition



### Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Sampling in Kendall's Shape Space
- **Arxiv ID**: http://arxiv.org/abs/1212.5720v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1212.5720v2)
- **Published**: 2012-12-22 20:27:22+00:00
- **Updated**: 2013-01-10 20:33:55+00:00
- **Authors**: Yen-Yun Yu, P. Thomas Fletcher, Suyash P. Awate
- **Comment**: 9 pages, 7 figures, International Conference on Machine Learning 2013
- **Journal**: None
- **Summary**: This paper proposes a novel framework for multi-group shape analysis relying on a hierarchical graphical statistical model on shapes within a population.The framework represents individual shapes as point setsmodulo translation, rotation, and scale, following the notion in Kendall shape space.While individual shapes are derived from their group shape model, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and the top level in the hierarchy provides a common frame of reference for multigroup shape analysis, e.g. classification and hypothesis testing. Unlike typical shape-modeling approaches, the proposed model is a generative model that defines a joint distribution of object-boundary data and the shape-model variables. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the so-called correspondence problem. The proposed inference scheme employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of pointsets, in Kendall shape space, from their posterior distribution, for which we exploit a highly-efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform (1) hypothesis testing for comparison between pairs of groups using permutation testing and (2) classification for image retrieval. The paper validates the proposed framework on simulated data and demonstrates results on real data.



