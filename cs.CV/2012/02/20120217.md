# Arxiv Papers in cs.CV on 2012-02-17
### A feature extraction technique based on character geometry for character recognition
- **Arxiv ID**: http://arxiv.org/abs/1202.3884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1202.3884v1)
- **Published**: 2012-02-17 11:41:28+00:00
- **Updated**: 2012-02-17 11:41:28+00:00
- **Authors**: Dinesh Dileep Gaurav, Renu Ramesh
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a geometry based technique for feature extraction applicable to segmentation-based word recognition systems. The proposed system extracts the geometric features of the character contour. This features are based on the basic line types that forms the character skeleton. The system gives a feature vector as its output. The feature vectors so generated from a training set, were then used to train a pattern recognition engine based on Neural Networks so that the system can be benchmarked.



### Generalized Principal Component Analysis (GPCA)
- **Arxiv ID**: http://arxiv.org/abs/1202.4002v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1202.4002v1)
- **Published**: 2012-02-17 20:07:25+00:00
- **Updated**: 2012-02-17 20:07:25+00:00
- **Authors**: Rene Vidal, Yi Ma, Shankar Sastry
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol 27, no 12, 2005
- **Summary**: This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a high- dimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3D motion segmentation from point correspondences in multiple affine views.



