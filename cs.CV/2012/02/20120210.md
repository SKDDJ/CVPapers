# Arxiv Papers in cs.CV on 2012-02-10
### Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers
- **Arxiv ID**: http://arxiv.org/abs/1202.2160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1202.2160v2)
- **Published**: 2012-02-10 00:30:48+00:00
- **Updated**: 2012-07-13 21:32:24+00:00
- **Authors**: Cl√©ment Farabet, Camille Couprie, Laurent Najman, Yann LeCun
- **Comment**: 9 pages, 4 figures - Published in 29th International Conference on
  Machine Learning (ICML 2012), Jun 2012, Edinburgh, United Kingdom
- **Journal**: None
- **Summary**: Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image.   The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free.   The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \times 240 image labeling in less than 1 second.



### Streaming an image through the eye: The retina seen as a dithered scalable image coder
- **Arxiv ID**: http://arxiv.org/abs/1202.2350v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1202.2350v1)
- **Published**: 2012-02-10 09:16:43+00:00
- **Updated**: 2012-02-10 09:16:43+00:00
- **Authors**: Khaled Masmoudi, Marc Antonini, Pierre Kornprobst
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1104.1550
- **Journal**: None
- **Summary**: We propose the design of an original scalable image coder/decoder that is inspired from the mammalians retina. Our coder accounts for the time-dependent and also nondeterministic behavior of the actual retina. The present work brings two main contributions: As a first step, (i) we design a deterministic image coder mimicking most of the retinal processing stages and then (ii) we introduce a retinal noise in the coding process, that we model here as a dither signal, to gain interesting perceptual features. Regarding our first contribution, our main source of inspiration will be the biologically plausible model of the retina called Virtual Retina. The main novelty of this coder is to show that the time-dependent behavior of the retina cells could ensure, in an implicit way, scalability and bit allocation. Regarding our second contribution, we reconsider the inner layers of the retina. We emit a possible interpretation for the non-determinism observed by neurophysiologists in their output. For this sake, we model the retinal noise that occurs in these layers by a dither signal. The dithering process that we propose adds several interesting features to our image coder. The dither noise whitens the reconstruction error and decorrelates it from the input stimuli. Furthermore, integrating the dither noise in our coder allows a faster recognition of the fine details of the image during the decoding process. Our present paper goal is twofold. First, we aim at mimicking as closely as possible the retina for the design of a novel image coder while keeping encouraging performances. Second, we bring a new insight concerning the non-deterministic behavior of the retina.



### An evaluation of local shape descriptors for 3D shape retrieval
- **Arxiv ID**: http://arxiv.org/abs/1202.2368v1
- **DOI**: 10.1117/12.912153
- **Categories**: **cs.CV**, cs.CG, cs.DL, cs.IR, cs.MM, I.2.10; I.4.8; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1202.2368v1)
- **Published**: 2012-02-10 21:02:39+00:00
- **Updated**: 2012-02-10 21:02:39+00:00
- **Authors**: Sarah Tang, Afzal Godil
- **Comment**: IS&T/SPIE Electronic Imaging 2012, Proceedings Vol. 8290
  Three-Dimensional Image Processing (3DIP) and Applications II, Atilla M.
  Baskurt; Robert Sitnik, Editors, 82900N Dates: Tuesday-Thursday 24 - 26
  January 2012, Paper 8290-22
- **Journal**: None
- **Summary**: As the usage of 3D models increases, so does the importance of developing accurate 3D shape retrieval algorithms. A common approach is to calculate a shape descriptor for each object, which can then be compared to determine two objects' similarity. However, these descriptors are often evaluated independently and on different datasets, making them difficult to compare. Using the SHREC 2011 Shape Retrieval Contest of Non-rigid 3D Watertight Meshes dataset, we systematically evaluate a collection of local shape descriptors. We apply each descriptor to the bag-of-words paradigm and assess the effects of varying the dictionary's size and the number of sample points. In addition, several salient point detection methods are used to choose sample points; these methods are compared to each other and to random selection. Finally, information from two local descriptors is combined in two ways and changes in performance are investigated. This paper presents results of these experiment



