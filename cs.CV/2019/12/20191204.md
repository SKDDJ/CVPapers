# Arxiv Papers in cs.CV on 2019-12-04
### Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.01756v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01756v4)
- **Published**: 2019-12-04 01:15:51+00:00
- **Updated**: 2021-06-07 03:42:13+00:00
- **Authors**: Fuyang Zhang, Nelson Nauata, Yasutaka Furukawa
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: This paper proposes a novel message passing neural (MPN) architecture Conv-MPN, which reconstructs an outdoor building as a planar graph from a single RGB image. Conv-MPN is specifically designed for cases where nodes of a graph have explicit spatial embedding. In our problem, nodes correspond to building edges in an image. Conv-MPN is different from MPN in that 1) the feature associated with a node is represented as a feature volume instead of a 1D vector; and 2) convolutions encode messages instead of fully connected layers. Conv-MPN learns to select a true subset of nodes (i.e., building edges) to reconstruct a building planar graph. Our qualitative and quantitative evaluations over 2,000 buildings show that Conv-MPN makes significant improvements over the existing fully neural solutions. We believe that the paper has a potential to open a new line of graph neural network research for structured geometry reconstruction.



### Spectral-GANs for High-Resolution 3D Point-cloud Generation
- **Arxiv ID**: http://arxiv.org/abs/1912.01800v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01800v2)
- **Published**: 2019-12-04 05:11:16+00:00
- **Updated**: 2020-07-19 08:50:33+00:00
- **Authors**: Sameera Ramasinghe, Salman Khan, Nick Barnes, Stephen Gould
- **Comment**: 1 page: Added affiliations
- **Journal**: None
- **Summary**: Point-clouds are a popular choice for vision and graphics tasks due to their accurate shape description and direct acquisition from range-scanners. This demands the ability to synthesize and reconstruct high-quality point-clouds. Current deep generative models for 3D data generally work on simplified representations (e.g., voxelized objects) and cannot deal with the inherent redundancy and irregularity in point-clouds. A few recent efforts on 3D point-cloud generation offer limited resolution and their complexity grows with the increase in output resolution. In this paper, we develop a principled approach to synthesize 3D point-clouds using a spectral-domain Generative Adversarial Network (GAN). Our spectral representation is highly structured and allows us to disentangle various frequency bands such that the learning task is simplified for a GAN model. As compared to spatial-domain generative approaches, our formulation allows us to generate arbitrary number of points high-resolution point-clouds with minimal computational overhead. Furthermore, we propose a fully differentiable block to transform from {the} spectral to the spatial domain and back, thereby allowing us to integrate knowledge from well-established spatial models. We demonstrate that Spectral-GAN performs well for point-cloud generation task. Additionally, it can learn {a} highly discriminative representation in an unsupervised fashion and can be used to accurately reconstruct 3D objects.



### Adversarial Domain Adaptation with Domain Mixup
- **Arxiv ID**: http://arxiv.org/abs/1912.01805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01805v1)
- **Published**: 2019-12-04 05:45:43+00:00
- **Updated**: 2019-12-04 05:45:43+00:00
- **Authors**: Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, Wenjun Zhang
- **Comment**: Accepted as oral presentation at 34th AAAI Conference on Artificial
  Intelligence, 2020
- **Journal**: None
- **Summary**: Recent works on domain adaptation reveal the effectiveness of adversarial learning on filling the discrepancy between source and target domains. However, two common limitations exist in current adversarial-learning-based methods. First, samples from two domains alone are not sufficient to ensure domain-invariance at most part of latent space. Second, the domain discriminator involved in these methods can only judge real or fake with the guidance of hard label, while it is more reasonable to use soft scores to evaluate the generated images or features, i.e., to fully utilize the inter-domain information. In this paper, we present adversarial domain adaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a more continuous latent space and guides the domain discriminator in judging samples' difference relative to source and target domains. Domain mixup is jointly conducted on pixel and feature level to improve the robustness of models. Extensive experiments prove that the proposed approach can achieve superior performance on tasks with various degrees of domain shift and data complexity.



### Drone-based Joint Density Map Estimation, Localization and Tracking with Space-Time Multi-Scale Attention Network
- **Arxiv ID**: http://arxiv.org/abs/1912.01811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01811v1)
- **Published**: 2019-12-04 06:04:03+00:00
- **Updated**: 2019-12-04 06:04:03+00:00
- **Authors**: Longyin Wen, Dawei Du, Pengfei Zhu, Qinghua Hu, Qilong Wang, Liefeng Bo, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a space-time multi-scale attention network (STANet) to solve density map estimation, localization and tracking in dense crowds of video clips captured by drones with arbitrary crowd density, perspective, and flight altitude. Our STANet method aggregates multi-scale feature maps in sequential frames to exploit the temporal coherency, and then predict the density maps, localize the targets, and associate them in crowds simultaneously. A coarse-to-fine process is designed to gradually apply the attention module on the aggregated multi-scale feature maps to enforce the network to exploit the discriminative space-time features for better performance. The whole network is trained in an end-to-end manner with the multi-task loss, formed by three terms, i.e., the density map loss, localization loss and association loss. The non-maximal suppression followed by the min-cost flow framework is used to generate the trajectories of targets' in scenarios. Since existing crowd counting datasets merely focus on crowd counting in static cameras rather than density map estimation, counting and tracking in crowds on drones, we have collected a new large-scale drone-based dataset, DroneCrowd, formed by 112 video clips with 33,600 high resolution frames (i.e., 1920x1080) captured in 70 different scenarios. With intensive amount of effort, our dataset provides 20,800 people trajectories with 4.8 million head annotations and several video-level attributes in sequences. Extensive experiments are conducted on two challenging public datasets, i.e., Shanghaitech and UCF-QNRF, and our DroneCrowd, to demonstrate that STANet achieves favorable performance against the state-of-the-arts. The datasets and codes can be found at https://github.com/VisDrone.



### A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the Reconstruction of Portuguese Tile Panels
- **Arxiv ID**: http://arxiv.org/abs/1912.02707v1
- **DOI**: 10.1145/3321707.3321821
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02707v1)
- **Published**: 2019-12-04 06:24:21+00:00
- **Updated**: 2019-12-04 06:24:21+00:00
- **Authors**: Daniel Rika, Dror Sholomon, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: ACM Genetic and Evolutionary Computation Conference (GECCO), pages
  1319-1327, Prague, Czech Republic, July 2019
- **Summary**: This paper presents a novel scheme, based on a unique combination of genetic algorithms (GAs) and deep learning (DL), for the automatic reconstruction of Portuguese tile panels, a challenging real-world variant of the jigsaw puzzle problem (JPP) with important national heritage implications. Specifically, we introduce an enhanced GA-based puzzle solver, whose integration with a novel DL-based compatibility measure (DLCM) yields state-of-the-art performance, regarding the above application. Current compatibility measures consider typically (the chromatic information of) edge pixels (between adjacent tiles), and help achieve high accuracy for the synthetic JPP variant. However, such measures exhibit rather poor performance when applied to the Portuguese tile panels, which are susceptible to various real-world effects, e.g., monochromatic panels, non-squared tiles, edge degradation, etc. To overcome such difficulties, we have developed a novel DLCM to extract high-level texture/color statistics from the entire tile information.   Integrating this measure with our enhanced GA-based puzzle solver, we have demonstrated, for the first time, how to deal most effectively with large-scale real-world problems, such as the Portuguese tile problem. Specifically, we have achieved 82% accuracy for the reconstruction of Portuguese tile panels with unknown piece rotation and puzzle dimension (compared to merely 3.5% average accuracy achieved by the best method known for solving this problem variant). The proposed method outperforms even human experts in several cases, correcting their mistakes in the manual tile assembly.



### Handwriting-Based Gender Classification Using End-to-End Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.01816v1
- **DOI**: 10.1007/978-3-030-01424-7_60
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01816v1)
- **Published**: 2019-12-04 06:24:31+00:00
- **Updated**: 2019-12-04 06:24:31+00:00
- **Authors**: Evyatar Illouz, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 11141, pp. 613-621, Rhodes, Greece, October 2018
- **Summary**: Handwriting-based gender classification is a well-researched problem that has been approached mainly by traditional machine learning techniques. In this paper, we propose a novel deep learning-based approach for this task. Specifically, we present a convolutional neural network (CNN), which performs automatic feature extraction from a given handwritten image, followed by classification of the writer's gender. Also, we introduce a new dataset of labeled handwritten samples, in Hebrew and English, of 405 participants. Comparing the gender classification accuracy on this dataset against human examiners, our results show that the proposed deep learning-based approach is substantially more accurate than that of humans.



### Efficient feature embedding of 3D brain MRI images for content-based image retrieval with deep metric learning
- **Arxiv ID**: http://arxiv.org/abs/1912.01824v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01824v1)
- **Published**: 2019-12-04 06:59:11+00:00
- **Updated**: 2019-12-04 06:59:11+00:00
- **Authors**: Yuto Onga, Shingo Fujiyama, Hayato Arai, Yusuke Chayama, Hitoshi Iyatomi, Kenichi Oishi
- **Comment**: To appear in the IEEE BigData 2019 Workshop on Advances in High
  Dimensional (AdHD) Big Data
- **Journal**: None
- **Summary**: Increasing numbers of MRI brain scans, improvements in image resolution, and advancements in MRI acquisition technology are causing significant increases in the demand for and burden on radiologists' efforts in terms of reading and interpreting brain MRIs. Content-based image retrieval (CBIR) is an emerging technology for reducing this burden by supporting the reading of medical images. High dimensionality is a major challenge in developing a CBIR system that is applicable for 3D brain MRIs. In this study, we propose a system called disease-oriented data concentration with metric learning (DDCML). In DDCML, we introduce deep metric learning to a 3D convolutional autoencoder (CAE). Our proposed DDCML scheme achieves a high dimensional compression rate (4096:1) while preserving the disease-related anatomical features that are important for medical image classification. The low-dimensional representation obtained by DDCML improved the clustering performance by 29.1\% compared to plain 3D-CAE in terms of discriminating Alzheimer's disease patients from healthy subjects, and successfully reproduced the relationships of the severity of disease categories that were not included in the training.



### PiiGAN: Generative Adversarial Networks for Pluralistic Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1912.01834v2
- **DOI**: 10.1109/ACCESS.2020.2979348
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01834v2)
- **Published**: 2019-12-04 07:44:41+00:00
- **Updated**: 2020-03-19 07:49:07+00:00
- **Authors**: Weiwei Cai, Zhanguo Wei
- **Comment**: None
- **Journal**: in IEEE Access, vol. 8, pp. 48451-48463, 2020
- **Summary**: The latest methods based on deep learning have achieved amazing results regarding the complex work of inpainting large missing areas in an image. But this type of method generally attempts to generate one single "optimal" result, ignoring many other plausible results. Considering the uncertainty of the inpainting task, one sole result can hardly be regarded as a desired regeneration of the missing area. In view of this weakness, which is related to the design of the previous algorithms, we propose a novel deep generative model equipped with a brand new style extractor which can extract the style feature (latent vector) from the ground truth. Once obtained, the extracted style feature and the ground truth are both input into the generator. We also craft a consistency loss that guides the generated image to approximate the ground truth. After iterations, our generator is able to learn the mapping of styles corresponding to multiple sets of vectors. The proposed model can generate a large number of results consistent with the context semantics of the image. Moreover, we evaluated the effectiveness of our model on three datasets, i.e., CelebA, PlantVillage, and MauFlex. Compared to state-of-the-art inpainting methods, this model is able to offer desirable inpainting results with both better quality and higher diversity. The code and model will be made available on https://github.com/vivitsai/PiiGAN.



### Knee Cartilage Segmentation Using Diffusion-Weighted MRI
- **Arxiv ID**: http://arxiv.org/abs/1912.01838v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.01838v1)
- **Published**: 2019-12-04 08:00:11+00:00
- **Updated**: 2019-12-04 08:00:11+00:00
- **Authors**: Alejandra Duarte, Chaitra V. Hegde, Aakash Kaku, Sreyas Mohan, José G. Raya
- **Comment**: Accepted to Medical Imaging Meets NeurIPS 2019
- **Journal**: None
- **Summary**: The integrity of articular cartilage is a crucial aspect in the early diagnosis of osteoarthritis (OA). Many novel MRI techniques have the potential to assess compositional changes of the cartilage extracellular matrix. Among these techniques, diffusion tensor imaging (DTI) of cartilage provides a simultaneous assessment of the two principal components of the solid matrix: collagen structure and proteoglycan concentration. DTI, as for any other compositional MRI technique, require a human expert to perform segmentation manually. The manual segmentation is error-prone and time-consuming ($\sim$ few hours per subject). We use an ensemble of modified U-Nets to automate this segmentation task. We benchmark our model against a human expert test-retest segmentation and conclude that our model is superior for Patellar and Tibial cartilage using dice score as the comparison metric. In the end, we do a perturbation analysis to understand the sensitivity of our model to the different components of our input. We also provide confidence maps for the predictions so that radiologists can tweak the model predictions as required. The model has been deployed in practice. In conclusion, cartilage segmentation on DW-MRI images with modified U-Nets achieves accuracy that outperforms the human segmenter. Code is available at https://github.com/aakashrkaku/knee-cartilage-segmentation



### Explorable Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1912.01839v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01839v3)
- **Published**: 2019-12-04 08:01:58+00:00
- **Updated**: 2020-06-22 08:29:10+00:00
- **Authors**: Yuval Bahat, Tomer Michaeli
- **Comment**: None
- **Journal**: Proceedings .of
  .the.IEEE/CVF.Conference.on.Computer.Vision.and.Pattern.Recognition. (2020)
  2716-2725
- **Summary**: Single image super resolution (SR) has seen major performance leaps in recent years. However, existing methods do not allow exploring the infinitely many plausible reconstructions that might have given rise to the observed low-resolution (LR) image. These different explanations to the LR image may dramatically vary in their textures and fine details, and may often encode completely different semantic information. In this paper, we introduce the task of explorable super resolution. We propose a framework comprising a graphical user interface with a neural network backend, allowing editing the SR output so as to explore the abundance of plausible HR explanations to the LR input. At the heart of our method is a novel module that can wrap any existing SR network, analytically guaranteeing that its SR outputs would precisely match the LR input, when downsampled. Besides its importance in our setting, this module is guaranteed to decrease the reconstruction error of any SR network it wraps, and can be used to cope with blur kernels that are different from the one the network was trained for. We illustrate our approach in a variety of use cases, ranging from medical imaging and forensics, to graphics.



### Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1912.01842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1912.01842v1)
- **Published**: 2019-12-04 08:08:28+00:00
- **Updated**: 2019-12-04 08:08:28+00:00
- **Authors**: Ignacio Serna, Aythami Morales, Julian Fierrez, Manuel Cebrian, Nick Obradovich, Iyad Rahwan
- **Comment**: None
- **Journal**: AAAI Workshop on Artificial Intelligence Safety (SafeAI), New
  York, NY, USA, 2020
- **Summary**: The most popular face recognition benchmarks assume a distribution of subjects without much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. The main aim of this study is focused on a better understanding of the feature space generated by deep models, and the performance achieved over different demographic groups. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments are conducted over the new DiveFace database composed of 24K identities from six different demographic groups. Two popular face recognition models are considered in the experimental framework: ResNet-50 and VGG-Face. We experimentally show that demographic groups highly represented in popular face databases have led to popular pre-trained deep face models presenting strong algorithmic discrimination. That discrimination can be observed both qualitatively at the feature space of the deep models and quantitatively in large performance differences when applying those models in different demographic groups, e.g. for face biometrics.



### Object Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.01844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01844v1)
- **Published**: 2019-12-04 08:18:54+00:00
- **Updated**: 2019-12-04 08:18:54+00:00
- **Authors**: Kaidong Li, Wenchi Ma, Usman Sajid, Yuanwei Wu, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this chapter, we present a brief overview of the recent development in object detection using convolutional neural networks (CNN). Several classical CNN-based detectors are presented. Some developments are based on the detector architectures, while others are focused on solving certain problems, like model degradation and small-scale object detection. The chapter also presents some performance comparison results of different models on several benchmark datasets. Through the discussion of these models, we hope to give readers a general idea about the developments of CNN-based object detection.



### Adjusting Decision Boundary for Class Imbalanced Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.01857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01857v2)
- **Published**: 2019-12-04 09:14:04+00:00
- **Updated**: 2020-03-11 08:15:57+00:00
- **Authors**: Byungju Kim, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Training of deep neural networks heavily depends on the data distribution. In particular, the networks easily suffer from class imbalance. The trained networks would recognize the frequent classes better than the infrequent classes. To resolve this problem, existing approaches typically propose novel loss functions to obtain better feature embedding. In this paper, we argue that drawing a better decision boundary is as important as learning better features. Inspired by observations, we investigate how the class imbalance affects the decision boundary and deteriorates the performance. We also investigate the feature distributional discrepancy between training and test time. As a result, we propose a novel, yet simple method for class imbalanced learning. Despite its simplicity, our method shows outstanding performance. In particular, the experimental results show that we can significantly improve the network by scaling the weight vectors, even without additional training process.



### StarGAN v2: Diverse Image Synthesis for Multiple Domains
- **Arxiv ID**: http://arxiv.org/abs/1912.01865v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01865v2)
- **Published**: 2019-12-04 09:42:22+00:00
- **Updated**: 2020-04-26 07:09:56+00:00
- **Authors**: Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.



### A Fully-Integrated Sensing and Control System for High-Accuracy Mobile Robotic Building Construction
- **Arxiv ID**: http://arxiv.org/abs/1912.01870v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CG, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01870v1)
- **Published**: 2019-12-04 10:01:37+00:00
- **Updated**: 2019-12-04 10:01:37+00:00
- **Authors**: Abel Gawel, Hermann Blum, Johannes Pankert, Koen Krämer, Luca Bartolomei, Selen Ercan, Farbod Farshidian, Margarita Chli, Fabio Gramazio, Roland Siegwart, Marco Hutter, Timothy Sandy
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully-integrated sensing and control system which enables mobile manipulator robots to execute building tasks with millimeter-scale accuracy on building construction sites. The approach leverages multi-modal sensing capabilities for state estimation, tight integration with digital building models, and integrated trajectory planning and whole-body motion control. A novel method for high-accuracy localization updates relative to the known building structure is proposed. The approach is implemented on a real platform and tested under realistic construction conditions. We show that the system can achieve sub-cm end-effector positioning accuracy during fully autonomous operation using solely on-board sensing.



### 3D Hand Pose Estimation via Regularized Graph Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.01875v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01875v4)
- **Published**: 2019-12-04 10:13:06+00:00
- **Updated**: 2021-06-03 13:06:47+00:00
- **Authors**: Yiming He, Wei Hu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D hand pose estimation from a monocular RGB image. While previous methods have shown great success, the structure of hands has not been fully exploited, which is critical in pose estimation. To this end, we propose a regularized graph representation learning under a conditional adversarial learning framework for 3D hand pose estimation, aiming to capture structural inter-dependencies of hand joints. In particular, we estimate an initial hand pose from a parametric hand model as a prior of hand structure, which regularizes the inference of the structural deformation in the prior pose for accurate graph representation learning via residual graph convolution. To optimize the hand structure further, we propose two bone-constrained loss functions, which characterize the morphable structure of hand poses explicitly. Also, we introduce an adversarial learning framework conditioned on the input image with a multi-source discriminator, which imposes the structural constraints onto the distribution of generated 3D hand poses for anthropomorphically valid hand poses. Extensive experiments demonstrate that our model sets the new state-of-the-art in 3D hand pose estimation from a monocular image on five standard benchmarks.



### Veni Vidi Dixi: Reliable Wireless Communication with Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1912.01879v1
- **DOI**: 10.1145/3359989.3365418
- **Categories**: **cs.IT**, cs.CV, cs.LG, cs.NI, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1912.01879v1)
- **Published**: 2019-12-04 10:21:34+00:00
- **Updated**: 2019-12-04 10:21:34+00:00
- **Authors**: Serkut Ayvaşık, H. Murat Gürsu, Wolfgang Kellerer
- **Comment**: Accepted for publication in CoNext 2019 with reproducibility badges.
  The measurements and the processing codes are available at
  https://gitlab.lrz.de/lkn_measurements/vvd_measurements for your evaluation
- **Journal**: None
- **Summary**: The upcoming industrial revolution requires deployment of critical wireless sensor networks for automation and monitoring purposes. However, the reliability of the wireless communication is rendered unpredictable by mobile elements in the communication environment such as humans or mobile robots which lead to dynamically changing radio environments. Changes in the wireless channel can be monitored with frequent pilot transmission. However, that would stress the battery life of sensors. In this work a new wireless channel estimation technique, Veni Vidi Dixi, VVD, is proposed. VVD leverages the redundant information in depth images obtained from the surveillance cameras in the communication environment and utilizes Convolutional Neural Networks CNNs to map the depth images of the communication environment to complex wireless channel estimations. VVD increases the wireless communication reliability without the need for frequent pilot transmission and with no additional complexity on the receiver. The proposed method is tested by conducting measurements in an indoor environment with a single mobile human. Up to authors best knowledge our work is the first to obtain complex wireless channel estimation from only depth images without any pilot transmission. The collected wireless trace, depth images and codes are publicly available.



### Better Understanding Hierarchical Visual Relationship for Image Caption
- **Arxiv ID**: http://arxiv.org/abs/1912.01881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01881v1)
- **Published**: 2019-12-04 10:26:11+00:00
- **Updated**: 2019-12-04 10:26:11+00:00
- **Authors**: Zheng-cong Fei
- **Comment**: NeurIPS 2019 workshop on New In ML
- **Journal**: None
- **Summary**: The Convolutional Neural Network (CNN) has been the dominant image feature extractor in computer vision for years. However, it fails to get the relationship between images/objects and their hierarchical interactions which can be helpful for representing and describing an image. In this paper, we propose a new design for image caption under a general encoder-decoder framework. It takes into account the hierarchical interactions between different abstraction levels of visual information in the images and their bounding-boxes. Specifically, we present CNN plus Graph Convolutional Network (GCN) architecture that novelly integrates both semantic and spatial visual relationships into image encoder. The representations of regions in an image and the connections between images are refined by leveraging graph structure through GCN. With the learned multi-level features, our model capitalizes on the Transformer-based decoder for description generation. We conduct experiments on the COCO image captioning dataset. Evaluations show that our proposed model outperforms the previous state-of-the-art models in the task of image caption, leading to a better performance in terms of all evaluation metrics.



### A Method of Detecting End-To-End Curves of Limited Curvature
- **Arxiv ID**: http://arxiv.org/abs/1912.01884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01884v1)
- **Published**: 2019-12-04 10:36:14+00:00
- **Updated**: 2019-12-04 10:36:14+00:00
- **Authors**: Ekaterina Panfilova, Mikhail Aliev, Irina Kunina, Vasiliy Postnikov, Dmitry Nikolaev
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we consider a method for detecting end-to-end curves of limited curvature like the k-link polylines with bending angle between adjacent segments in a given range. The approximation accuracy is achieved by maximization of the quality function in the image matrix. The method is based on a dynamic programming scheme constructed over Fast Hough Transform calculation results for image bands. The proposed method asymptotic complexity is $O(h \cdot (w+ \frac{h}{k}) \cdot log(\frac{h}{k}))$, where $h$ and $w$ are the image size, and $k$ is the approximating polyline links number, which is an analogue of the complexity of the fast Fourier transform or the fast Hough transform. We also show the results of the proposed method on synthetic and real data.



### Fast Projective Image Rectification for Planar Objects with Manhattan Structure
- **Arxiv ID**: http://arxiv.org/abs/1912.01892v1
- **DOI**: None
- **Categories**: **cs.CV**, 51A45
- **Links**: [PDF](http://arxiv.org/pdf/1912.01892v1)
- **Published**: 2019-12-04 10:59:22+00:00
- **Updated**: 2019-12-04 10:59:22+00:00
- **Authors**: Julia Shemiakina, Ivan Konovalenko, Daniil Tropin, Igor Faradjev
- **Comment**: Accepted and presented on ICMV conference 2019
- **Journal**: None
- **Summary**: This paper presents a method for metric rectification of planar objects that preserves angles and length ratios. An inner structure of an object is assumed to follow the laws of Manhattan World i.e. the majority of line segments are aligned with two orthogonal directions of the object. For that purpose we introduce the method that estimates the position of two vanishing points corresponding to the main object directions. It is based on an original optimization function of segments that estimates a vanishing point position. For calculation of the rectification homography with two vanishing points we propose a new method based on estimation of the camera rotation so that the camera axis is perpendicular to the object plane. The proposed method can be applied for rectification of various objects such as documents or building facades. Also since the camera rotation is estimated the method can be employed for estimation of object orientation (for example, during a surgery with radiograph of osteosynthesis implants). The method was evaluated on the MIDV-500 dataset containing projectively distorted images of documents with complex background. According to the experimental results an accuracy of the proposed method is better or equal to the-state-of-the-art if the background occupies no more than half of the image. Runtime of the method is around 3ms on core i7 3610qm CPU.



### A Method of Fluorescent Fibers Detection on Identity Documents under Ultraviolet Light
- **Arxiv ID**: http://arxiv.org/abs/1912.01916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01916v1)
- **Published**: 2019-12-04 12:08:33+00:00
- **Updated**: 2019-12-04 12:08:33+00:00
- **Authors**: Kunina I. A., Aliev M. A., Arlazarov N. V., Polevoy D. V
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: In this work we consider the problem of the fluorescent security fibers detection on the images of identity documents captured under ultraviolet light. As an example we use images of the second and third pages of the Russian passport and show features that render known methods and approaches based on image binarization non applicable. We propose a solution based on ridge detection in the gray-scale image of the document with preliminary normalized background. The algorithm was tested on a private dataset consisting of both authentic and model passports. Abandonment of binarization allowed to provide reliable and stable functioning of the proposed detector on a target dataset.



### A Low Computational Approach for Price Tag Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.01923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01923v1)
- **Published**: 2019-12-04 12:17:26+00:00
- **Updated**: 2019-12-04 12:17:26+00:00
- **Authors**: M. A. Aliev, D. A. Bocharov, I. A. Kunina, D. P. Nikolaev
- **Comment**: 9 pages, 7 figures, 3 tables
- **Journal**: None
- **Summary**: In this work we discuss the task of search, localization and recognition of price zone within a photograph of the price tag. The task is being addressed for the case when image is acquired by small-scale digital camera and calculation device has significant resource constraints. The proposed approach is based on Niblack binarization algorithm, analysis and clasterization of connected components in conditions of known price tag geometrical model. The algorithm was tested on a private dataset and has shown high quality.



### Trajectory-Based Recognition of Dynamic Persian Sign Language Using Hidden Markov Model
- **Arxiv ID**: http://arxiv.org/abs/1912.01944v1
- **DOI**: 10.1016/j.csl.2019.101053
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01944v1)
- **Published**: 2019-12-04 13:08:58+00:00
- **Updated**: 2019-12-04 13:08:58+00:00
- **Authors**: Saeideh Ghanbari Azar, Hadi Seyedarabi
- **Comment**: None
- **Journal**: None
- **Summary**: Sign Language Recognition (SLR) is an important step in facilitating the communication among deaf people and the rest of society. Existing Persian sign language recognition systems are mainly restricted to static signs which are not very useful in everyday communications. In this study, a dynamic Persian sign language recognition system is presented. A collection of 1200 videos were captured from 12 individuals performing 20 dynamic signs with a simple white glove. The trajectory of the hands, along with hand shape information were extracted from each video using a simple region-growing technique. These time-varying trajectories were then modeled using Hidden Markov Model (HMM) with Gaussian probability density functions as observations. The performance of the system was evaluated in different experimental strategies. Signer-independent and signer-dependent experiments were performed on the proposed system and the average accuracy of 97.48% was obtained. The experimental results demonstrated that the performance of the system is independent of the subject and it can also perform excellently even with a limited number of training data.



### EmbedMask: Embedding Coupling for One-stage Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.01954v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01954v2)
- **Published**: 2019-12-04 13:31:52+00:00
- **Updated**: 2019-12-05 10:42:25+00:00
- **Authors**: Hui Ying, Zhaojin Huang, Shu Liu, Tianjia Shao, Kun Zhou
- **Comment**: Code is available at github.com/yinghdb/EmbedMask
- **Journal**: None
- **Summary**: Current instance segmentation methods can be categorized into segmentation-based methods that segment first then do clustering, and proposal-based methods that detect first then predict masks for each instance proposal using repooling. In this work, we propose a one-stage method, named EmbedMask, that unifies both methods by taking advantages of them. Like proposal-based methods, EmbedMask builds on top of detection models making it strong in detection capability. Meanwhile, EmbedMask applies extra embedding modules to generate embeddings for pixels and proposals, where pixel embeddings are guided by proposal embeddings if they belong to the same instance. Through this embedding coupling process, pixels are assigned to the mask of the proposal if their embeddings are similar. The pixel-level clustering enables EmbedMask to generate high-resolution masks without missing details from repooling, and the existence of proposal embedding simplifies and strengthens the clustering procedure to achieve high speed with higher performance than segmentation-based methods. Without any bells and whistles, EmbedMask achieves comparable performance as Mask R-CNN, which is the representative two-stage method, and can produce more detailed masks at a higher speed. Code is available at github.com/yinghdb/EmbedMask.



### Epoch-wise label attacks for robustness against label noise
- **Arxiv ID**: http://arxiv.org/abs/1912.01966v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01966v2)
- **Published**: 2019-12-04 13:39:21+00:00
- **Updated**: 2019-12-12 14:47:37+00:00
- **Authors**: Sebastian Guendel, Andreas Maier
- **Comment**: Accepted at BVM 2020
- **Journal**: None
- **Summary**: The current accessibility to large medical datasets for training convolutional neural networks is tremendously high. The associated dataset labels are always considered to be the real "ground truth". However, the labeling procedures often seem to be inaccurate and many wrong labels are integrated. This may have fatal consequences on the performance of both training and evaluation. In this paper, we show the impact of label noise in the training set on a specific medical problem based on chest X-ray images. With a simple one-class problem, the classification of tuberculosis, we measure the performance on a clean evaluation set when training with label-corrupt data. We develop a method to compete with incorrectly labeled data during training by randomly attacking labels on individual epochs. The network tends to be robust when flipping correct labels for a single epoch and initiates a good step to the optimal minimum on the error surface when flipping noisy labels. On a baseline with an AUC (Area under Curve) score of 0.924, the performance drops to 0.809 when 30% of our training data is misclassified. With our approach the baseline performance could almost be maintained, the performance raised to 0.918.



### Self-Supervised Learning of Pretext-Invariant Representations
- **Arxiv ID**: http://arxiv.org/abs/1912.01991v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01991v1)
- **Published**: 2019-12-04 13:59:48+00:00
- **Updated**: 2019-12-04 13:59:48+00:00
- **Authors**: Ishan Misra, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.



### Research on dynamic target detection and tracking system of hexapod robot
- **Arxiv ID**: http://arxiv.org/abs/1912.01992v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01992v1)
- **Published**: 2019-12-04 14:02:57+00:00
- **Updated**: 2019-12-04 14:02:57+00:00
- **Authors**: Dexin Wang
- **Comment**: in Chinese
- **Journal**: None
- **Summary**: Dynamic target detection and target tracking are hot issues in the field of image. In order to explore its application value in the field of mobile robot, a dynamic target detection and tracking system is designed based on hexapod robot. Firstly, the dynamic target detection method is introduced with region merging and adaptive external point filtering based on motion compensation method. This method achieves the accurate compensation of the moving background through symmetric matching and adaptive external point filtering, and achieves complete detection of non-rigid objects by region merging. Secondly, the application of target tracking algorithm based on KCF in hexapod robot platform is studied, and the Angle tracking of moving target is realized by adaptive adjustment of tracking speed. The last, the architecture of robot monitoring system is designed, which consists of operator, processor, hexapod robot and vision sensor, and the moving object detection and tracking algorithm proposed in this paper is applied to the system. The experimental results show that the improved algorithm can effectively detect and track the moving target when applied to the system of the mobile hexapod robot.



### Template co-updating in multi-modal human activity recognition systems
- **Arxiv ID**: http://arxiv.org/abs/1912.02024v1
- **DOI**: 10.1145/3341105.3374085
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02024v1)
- **Published**: 2019-12-04 14:39:25+00:00
- **Updated**: 2019-12-04 14:39:25+00:00
- **Authors**: Annalisa Franco, Antonio Magnani, Dario Maio
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal systems are quite common in the context of human activity recognition; widely used RGB-D sensors (Kinect is the most prominent example) give access to parallel data streams, typically RGB images, depth data, skeleton information. The richness of multimodal information has been largely exploited in many works in the literature, while an analysis of their effectiveness for incremental template updating has not been investigated so far. This paper is aimed at defining a general framework for unsupervised template updating in multi-modal systems, where the different data sources can provide complementary information, increasing the effectiveness of the updating procedure and reducing at the same time the probability of incorrect template modifications.



### AdversarialNAS: Adversarial Neural Architecture Search for GANs
- **Arxiv ID**: http://arxiv.org/abs/1912.02037v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02037v2)
- **Published**: 2019-12-04 15:02:03+00:00
- **Updated**: 2020-04-08 14:01:02+00:00
- **Authors**: Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) that aims to automate the procedure of architecture design has achieved promising results in many computer vision fields. In this paper, we propose an AdversarialNAS method specially tailored for Generative Adversarial Networks (GANs) to search for a superior generative model on the task of unconditional image generation. The AdversarialNAS is the first method that can search the architectures of generator and discriminator simultaneously in a differentiable manner. During searching, the designed adversarial search algorithm does not need to comput any extra metric to evaluate the performance of the searched architecture, and the search paradigm considers the relevance between the two network architectures and improves their mutual balance. Therefore, AdversarialNAS is very efficient and only takes 1 GPU day to search for a superior generative model in the proposed large search space ($10^{38}$). Experiments demonstrate the effectiveness and superiority of our method. The discovered generative model sets a new state-of-the-art FID score of $10.87$ and highly competitive Inception Score of $8.74$ on CIFAR-10. Its transferability is also proven by setting new state-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10. Code is at: \url{https://github.com/chengaopro/AdversarialNAS}.



### Learning to synthesise the ageing brain without longitudinal data
- **Arxiv ID**: http://arxiv.org/abs/1912.02620v6
- **DOI**: 10.1016/j.media.2021.102169
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.02620v6)
- **Published**: 2019-12-04 15:12:19+00:00
- **Updated**: 2021-09-30 14:13:19+00:00
- **Authors**: Tian Xia, Agisilaos Chartsias, Chengjia Wang, Sotirios A. Tsaftaris
- **Comment**: None
- **Journal**: Medical Image Analysis, 2021, 73: 102169
- **Summary**: How will my face look when I get older? Or, for a more challenging question: How will my brain look when I get older? To answer this question one must devise (and learn from data) a multivariate auto-regressive function which given an image and a desired target age generates an output image. While collecting data for faces may be easier, collecting longitudinal brain data is not trivial. We propose a deep learning-based method that learns to simulate subject-specific brain ageing trajectories without relying on longitudinal data. Our method synthesises images conditioned on two factors: age (a continuous variable), and status of Alzheimer's Disease (AD, an ordinal variable). With an adversarial formulation we learn the joint distribution of brain appearance, age and AD status, and define reconstruction losses to address the challenging problem of preserving subject identity. We compare with several benchmarks using two widely used datasets. We evaluate the quality and realism of synthesised images using ground-truth longitudinal data and a pre-trained age predictor. We show that, despite the use of cross-sectional data, our model learns patterns of gray matter atrophy in the middle temporal gyrus in patients with AD. To demonstrate generalisation ability, we train on one dataset and evaluate predictions on the other. In conclusion, our model shows an ability to separate age, disease influence and anatomy using only 2D cross-sectional data that should be useful in large studies into neurodegenerative disease, that aim to combine several data sources. To facilitate such future studies by the community at large our code is made available at https://github.com/xiat0616/BrainAgeing.



### Siamese Natural Language Tracker: Tracking by Natural Language Descriptions with Siamese Trackers
- **Arxiv ID**: http://arxiv.org/abs/1912.02048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02048v2)
- **Published**: 2019-12-04 15:16:32+00:00
- **Updated**: 2021-04-05 18:03:24+00:00
- **Authors**: Qi Feng, Vitaly Ablavsky, Qinxun Bai, Stan Sclaroff
- **Comment**: CVPR 2021
- **Journal**: None
- **Summary**: We propose a novel Siamese Natural Language Tracker (SNLT), which brings the advancements in visual tracking to the tracking by natural language (NL) descriptions task. The proposed SNLT is applicable to a wide range of Siamese trackers, providing a new class of baselines for the tracking by NL task and promising future improvements from the advancements of Siamese trackers. The carefully designed architecture of the Siamese Natural Language Region Proposal Network (SNL-RPN), together with the Dynamic Aggregation of vision and language modalities, is introduced to perform the tracking by NL task. Empirical results over tracking benchmarks with NL annotations show that the proposed SNLT improves Siamese trackers by 3 to 7 percentage points with a slight tradeoff of speed. The proposed SNLT outperforms all NL trackers to-date and is competitive among state-of-the-art real-time trackers on LaSOT benchmarks while running at 50 frames per second on a single GPU.



### Detecting Hardly Visible Roads in Low-Resolution Satellite Time Series Data
- **Arxiv ID**: http://arxiv.org/abs/1912.05026v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05026v1)
- **Published**: 2019-12-04 15:40:43+00:00
- **Updated**: 2019-12-04 15:40:43+00:00
- **Authors**: Stefan Oehmcke, Christoffer Thrysøe, Andreas Borgstad, Marcos Antonio Vaz Salles, Martin Brandt, Fabian Gieseke
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Massive amounts of satellite data have been gathered over time, holding the potential to unveil a spatiotemporal chronicle of the surface of Earth. These data allow scientists to investigate various important issues, such as land use changes, on a global scale. However, not all land-use phenomena are equally visible on satellite imagery. In particular, the creation of an inventory of the planet's road infrastructure remains a challenge, despite being crucial to analyze urbanization patterns and their impact. Towards this end, this work advances data-driven approaches for the automatic identification of roads based on open satellite data. Given the typical resolutions of these historical satellite data, we observe that there is inherent variation in the visibility of different road types. Based on this observation, we propose two deep learning frameworks that extend state-of-the-art deep learning methods by formalizing road detection as an ordinal classification task. In contrast to related schemes, one of the two models also resorts to satellite time series data that are potentially affected by missing data and cloud occlusion. Taking these time series data into account eliminates the need to manually curate datasets of high-quality image tiles, substantially simplifying the application of such models on a global scale. We evaluate our approaches on a dataset that is based on Sentinel~2 satellite imagery and OpenStreetMap vector data. Our results indicate that the proposed models can successfully identify large and medium-sized roads. We also discuss opportunities and challenges related to the detection of roads and other infrastructure on a global scale.



### FocusNet++: Attentive Aggregated Transformations for Efficient and Accurate Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.02079v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02079v2)
- **Published**: 2019-12-04 16:10:26+00:00
- **Updated**: 2021-04-07 23:20:48+00:00
- **Authors**: Chaitanya Kaul, Nick Pears, Hang Dai, Roderick Murray-Smith, Suresh Manandhar
- **Comment**: Published at ISBI 2021
- **Journal**: None
- **Summary**: We propose a new residual block for convolutional neural networks and demonstrate its state-of-the-art performance in medical image segmentation. We combine attention mechanisms with group convolutions to create our group attention mechanism, which forms the fundamental building block of our network, FocusNet++. We employ a hybrid loss based on balanced cross entropy, Tversky loss and the adaptive logarithmic loss to enhance the performance along with fast convergence. Our results show that FocusNet++ achieves state-of-the-art results across various benchmark metrics for the ISIC 2018 melanoma segmentation and the cell nuclei segmentation datasets with fewer parameters and FLOPs.



### Mining Domain Knowledge: Improved Framework towards Automatically Standardizing Anatomical Structure Nomenclature in Radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/1912.02084v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02084v3)
- **Published**: 2019-12-04 16:28:23+00:00
- **Updated**: 2020-05-10 13:34:54+00:00
- **Authors**: Qiming Yang, Hongyang Chao, Dan Nguyen, Steve Jiang
- **Comment**: 15 pages, 8 figures
- **Journal**: None
- **Summary**: The automatic standardization of nomenclature for anatomical structures in radiotherapy (RT) clinical data is a critical prerequisite for data curation and data-driven research in the era of big data and artificial intelligence, but it is currently an unmet need. Existing methods either cannot handle cross-institutional datasets or suffer from heavy imbalance and poor-quality delineation in clinical RT datasets. To solve these problems, we propose an automated structure nomenclature standardization framework, 3D Non-local Network with Voting (3DNNV). This framework consists of an improved data processing strategy, namely, adaptive sampling and adaptive cropping (ASAC) with voting, and an optimized feature extraction module. The framework simulates clinicians' domain knowledge and recognition mechanisms to identify small-volume organs at risk (OARs) with heavily imbalanced data better than other methods. We used partial data from an open-source head-and-neck cancer dataset to train the model, then tested the model on three cross-institutional datasets to demonstrate its generalizability. 3DNNV outperformed the baseline model, achieving higher average true positive rates (TPR) overall categories on the three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More importantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to 91.17%, in terms of F1 score for a small-volume OAR with only 9 training samples. The results show that 3DNNV can be applied to identify OARs, even error-prone ones. Furthermore, we discussed the limitations and applicability of the framework in practical scenarios. The framework we developed can assist in standardizing structure nomenclature to facilitate data-driven clinical research in cancer radiotherapy.



### Protecting Geolocation Privacy of Photo Collections
- **Arxiv ID**: http://arxiv.org/abs/1912.02085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02085v1)
- **Published**: 2019-12-04 16:28:52+00:00
- **Updated**: 2019-12-04 16:28:52+00:00
- **Authors**: Jinghan Yang, Ayan Chakrabarti, Yevgeniy Vorobeychik
- **Comment**: AAAI 20
- **Journal**: None
- **Summary**: People increasingly share personal information, including their photos and photo collections, on social media. This information, however, can compromise individual privacy, particularly as social media platforms use it to infer detailed models of user behavior, including tracking their location. We consider the specific issue of location privacy as potentially revealed by posting photo collections, which facilitate accurate geolocation with the help of deep learning methods even in the absence of geotags. One means to limit associated inadvertent geolocation privacy disclosure is by carefully pruning select photos from photo collections before these are posted publicly. We study this problem formally as a combinatorial optimization problem in the context of geolocation prediction facilitated by deep learning. We first demonstrate the complexity both by showing that a natural greedy algorithm can be arbitrarily bad and by proving that the problem is NP-Hard. We then exhibit an important tractable special case, as well as a more general approach based on mixed-integer linear programming. Through extensive experiments on real photo collections, we demonstrate that our approaches are indeed highly effective at preserving geolocation privacy.



### Learning Multi-Object Tracking and Segmentation from Automatic Annotations
- **Arxiv ID**: http://arxiv.org/abs/1912.02096v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02096v3)
- **Published**: 2019-12-04 16:38:24+00:00
- **Updated**: 2020-03-30 09:13:31+00:00
- **Authors**: Lorenzo Porzi, Markus Hofinger, Idoia Ruiz, Joan Serrat, Samuel Rota Bulò, Peter Kontschieder
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we contribute a novel pipeline to automatically generate training data, and to improve over state-of-the-art multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm turns raw street-level videos into high-fidelity MOTS training data, is scalable and overcomes the need of expensive and time-consuming manual annotation approaches. We leverage state-of-the-art instance segmentation results in combination with optical flow predictions, also trained on automatically harvested training data. Our second major contribution is MOTSNet - a deep learning, tracking-by-detection architecture for MOTS - deploying a novel mask-pooling layer for improved object association over time. Training MOTSNet with our automatically extracted data leads to significantly improved sMOTSA scores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and MOTSNet improves by +4.1% over previously best methods on the MOTSChallenge dataset. Our most impressive finding is that we can improve over previous best-performing works, even in complete absence of manually annotated MOTS training data.



### Towards Robust Image Classification Using Sequential Attention Models
- **Arxiv ID**: http://arxiv.org/abs/1912.02184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02184v1)
- **Published**: 2019-12-04 17:58:20+00:00
- **Updated**: 2019-12-04 17:58:20+00:00
- **Authors**: Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott, Pushmeet Kohl
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a "computational race" between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.



### Walking on the Edge: Fast, Low-Distortion Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1912.02153v2
- **DOI**: 10.1109/TIFS.2020.3021899
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02153v2)
- **Published**: 2019-12-04 18:04:53+00:00
- **Updated**: 2019-12-05 12:07:15+00:00
- **Authors**: Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Adversarial examples of deep neural networks are receiving ever increasing attention because they help in understanding and reducing the sensitivity to their input. This is natural given the increasing applications of deep neural networks in our everyday lives. When white-box attacks are almost always successful, it is typically only the distortion of the perturbations that matters in their evaluation.   In this work, we argue that speed is important as well, especially when considering that fast attacks are required by adversarial training. Given more time, iterative methods can always find better solutions. We investigate this speed-distortion trade-off in some depth and introduce a new attack called boundary projection (BP) that improves upon existing methods by a large margin. Our key idea is that the classification boundary is a manifold in the image space: we therefore quickly reach the boundary and then optimize distortion on this manifold.



### Visual Reaction: Learning to Play Catch with Your Drone
- **Arxiv ID**: http://arxiv.org/abs/1912.02155v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02155v2)
- **Published**: 2019-12-04 18:11:26+00:00
- **Updated**: 2020-04-10 04:45:20+00:00
- **Authors**: Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: In this paper we address the problem of visual reaction: the task of interacting with dynamic environments where the changes in the environment are not necessarily caused by the agent itself. Visual reaction entails predicting the future changes in a visual environment and planning accordingly. We study the problem of visual reaction in the context of playing catch with a drone in visually rich synthetic environments. This is a challenging problem since the agent is required to learn (1) how objects with different physical properties and shapes move, (2) what sequence of actions should be taken according to the prediction, (3) how to adjust the actions based on the visual feedback from the dynamic environment (e.g., when objects bouncing off a wall), and (4) how to reason and act with an unexpected state change in a timely manner. We propose a new dataset for this task, which includes 30K throws of 20 types of objects in different directions with different forces. Our results show that our model that integrates a forecaster with a planner outperforms a set of strong baselines that are based on tracking as well as pure model-based and model-free RL baselines. The code and dataset are available at github.com/KuoHaoZeng/Visual_Reaction.



### MORPHOLO C++ Library for glasses-free multi-view stereo vision and streaming of live 3D video
- **Arxiv ID**: http://arxiv.org/abs/1912.02202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02202v1)
- **Published**: 2019-12-04 19:00:15+00:00
- **Updated**: 2019-12-04 19:00:15+00:00
- **Authors**: Enrique Canessa, Livio Tenze
- **Comment**: 28 pages
- **Journal**: None
- **Summary**: The MORPHOLO C++ extended Library allows to convert a specific stereoscopic snapshot into a Native multi-view image through morphing algorithms taking into account display calibration data for specific slanted lenticular 3D monitors. MORPHOLO can also be implemented for glasses-free live applicatons of 3D video streaming, and for diverse innovative scientific, engineering and 3D video game applications -see http://www.morpholo.it



### Learnt dynamics generalizes across tasks, datasets, and populations
- **Arxiv ID**: http://arxiv.org/abs/1912.03130v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03130v1)
- **Published**: 2019-12-04 20:21:50+00:00
- **Updated**: 2019-12-04 20:21:50+00:00
- **Authors**: U. Mahmood, M. M. Rahman, A. Fedorov, Z. Fu, V. D. Calhoun, S. M. Plis
- **Comment**: 11 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1911.06813
- **Journal**: None
- **Summary**: Differentiating multivariate dynamic signals is a difficult learning problem as the feature space may be large yet often only a few training examples are available. Traditional approaches to this problem either proceed from handcrafted features or require large datasets to combat the m >> n problem. In this paper, we show that the source of the problem---signal dynamics---can be used to our advantage and noticeably improve classification performance on a range of discrimination tasks when training data is scarce. We demonstrate that self-supervised pre-training guided by signal dynamics produces embedding that generalizes across tasks, datasets, data collection sites, and data distributions. We perform an extensive evaluation of this approach on a range of tasks including simulated data, keyword detection problem, and a range of functional neuroimaging data, where we show that a single embedding learnt on healthy subjects generalizes across a number of disorders, age groups, and datasets.



### Learning from Interventions using Hierarchical Policies for Safe Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02241v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02241v1)
- **Published**: 2019-12-04 20:28:51+00:00
- **Updated**: 2019-12-04 20:28:51+00:00
- **Authors**: Jing Bi, Vikas Dhiman, Tianyou Xiao, Chenliang Xu
- **Comment**: Accepted for publication at the Thirty-Fourth AAAI Conference on
  Artificial Intelligence (AAAI-20)
- **Journal**: None
- **Summary**: Learning from Demonstrations (LfD) via Behavior Cloning (BC) works well on multiple complex tasks. However, a limitation of the typical LfD approach is that it requires expert demonstrations for all scenarios, including those in which the algorithm is already well-trained. The recently proposed Learning from Interventions (LfI) overcomes this limitation by using an expert overseer. The expert overseer only intervenes when it suspects that an unsafe action is about to be taken. Although LfI significantly improves over LfD, the state-of-the-art LfI fails to account for delay caused by the expert's reaction time and only learns short-term behavior. We address these limitations by 1) interpolating the expert's interventions back in time, and 2) by splitting the policy into two hierarchical levels, one that generates sub-goals for the future and another that generates actions to reach those desired sub-goals. This sub-goal prediction forces the algorithm to learn long-term behavior while also being robust to the expert's reaction time. Our experiments show that LfI using sub-goals in a hierarchical policy framework trains faster and achieves better asymptotic performance than typical LfD.



### An Automated Deep Learning Approach for Bacterial Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.08765v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08765v1)
- **Published**: 2019-12-04 20:38:31+00:00
- **Updated**: 2019-12-04 20:38:31+00:00
- **Authors**: Muhammed Talo
- **Comment**: None
- **Journal**: None
- **Summary**: Automated recognition and classification of bacteria species from microscopic images have significant importance in clinical microbiology. Bacteria classification is usually carried out manually by biologists using different shapes and morphologic characteristics of bacteria species. The manual taxonomy of bacteria types from microscopy images is time-consuming and a challenging task for even experienced biologists. In this study, an automated deep learning based classification approach has been proposed to classify bacterial images into different categories. The ResNet-50 pre-trained CNN architecture has been used to classify digital bacteria images into 33 categories. The transfer learning technique was employed to accelerate the training process of the network and improve the classification performance of the network. The proposed method achieved an average classification accuracy of 99.2%. The experimental results demonstrate that the proposed technique surpasses state-of-the-art methods in the literature and can be used for any type of bacteria classification tasks.



### Let's Get Dirty: GAN Based Data Augmentation for Camera Lens Soiling Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1912.02249v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.02249v3)
- **Published**: 2019-12-04 21:01:06+00:00
- **Updated**: 2020-11-14 21:13:56+00:00
- **Authors**: Michal Uricar, Ganesh Sistu, Hazem Rashed, Antonin Vobecky, Varun Ravi Kumar, Pavel Krizek, Fabian Burger, Senthil Yogamani
- **Comment**: Camera ready version + supplementary material. Accepted for
  presentation at Winter Conference on Applications of Computer Vision 2021
- **Journal**: None
- **Summary**: Wide-angle fisheye cameras are commonly used in automated driving for parking and low-speed navigation tasks. Four of such cameras form a surround-view system that provides a complete and detailed view of the vehicle. These cameras are directly exposed to harsh environmental settings and can get soiled very easily by mud, dust, water, frost. Soiling on the camera lens can severely degrade the visual perception algorithms, and a camera cleaning system triggered by a soiling detection algorithm is increasingly being deployed. While adverse weather conditions, such as rain, are getting attention recently, there is only limited work on general soiling. The main reason is the difficulty in collecting a diverse dataset as it is a relatively rare event. We propose a novel GAN based algorithm for generating unseen patterns of soiled images. Additionally, the proposed method automatically provides the corresponding soiling masks eliminating the manual annotation cost. Augmentation of the generated soiled images for training improves the accuracy of soiling detection tasks significantly by 18% demonstrating its usefulness. The manually annotated soiling dataset and the generated augmentation dataset will be made public. We demonstrate the generalization of our fisheye trained GAN model on the Cityscapes dataset. We provide an empirical evaluation of the degradation of the semantic segmentation algorithm with the soiled data.



### Multiple Anchor Learning for Visual Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.02252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02252v1)
- **Published**: 2019-12-04 21:25:07+00:00
- **Updated**: 2019-12-04 21:25:07+00:00
- **Authors**: Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, Dong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Classification and localization are two pillars of visual object detectors. However, in CNN-based detectors, these two modules are usually optimized under a fixed set of candidate (or anchor) bounding boxes. This configuration significantly limits the possibility to jointly optimize classification and localization. In this paper, we propose a Multiple Instance Learning (MIL) approach that selects anchors and jointly optimizes the two modules of a CNN-based object detector. Our approach, referred to as Multiple Anchor Learning (MAL), constructs anchor bags and selects the most representative anchors from each bag. Such an iterative selection process is potentially NP-hard to optimize. To address this issue, we solve MAL by repetitively depressing the confidence of selected anchors by perturbing their corresponding features. In an adversarial selection-depression manner, MAL not only pursues optimal solutions but also fully leverages multiple anchors/features to learn a detection model. Experiments show that MAL improves the baseline RetinaNet with significant margins on the commonly used MS-COCO object detection benchmark and achieves new state-of-the-art detection performance compared with recent methods.



### Compositional Temporal Visual Grounding of Natural Language Event Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1912.02256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02256v1)
- **Published**: 2019-12-04 21:36:16+00:00
- **Updated**: 2019-12-04 21:36:16+00:00
- **Authors**: Jonathan C. Stroud, Ryan McCaffrey, Rada Mihalcea, Jia Deng, Olga Russakovsky
- **Comment**: Project page: jonathancstroud.com/ctg
- **Journal**: None
- **Summary**: Temporal grounding entails establishing a correspondence between natural language event descriptions and their visual depictions. Compositional modeling becomes central: we first ground atomic descriptions "girl eating an apple," "batter hitting the ball" to short video segments, and then establish the temporal relationships between the segments. This compositional structure enables models to recognize a wider variety of events not seen during training through recognizing their atomic sub-events. Explicit temporal modeling accounts for a wide variety of temporal relationships that can be expressed in language: e.g., in the description "girl stands up from the table after eating an apple" the visual ordering of the events is reversed, with first "eating an apple" followed by "standing up from the table." We leverage these observations to develop a unified deep architecture, CTG-Net, to perform temporal grounding of natural language event descriptions to videos. We demonstrate that our system outperforms prior state-of-the-art methods on the DiDeMo, Tempo-TL, and Tempo-HL temporal grounding datasets.



### Extending the Morphological Hit-or-Miss Transform to Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.02259v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02259v2)
- **Published**: 2019-12-04 21:42:34+00:00
- **Updated**: 2020-09-28 00:11:43+00:00
- **Authors**: Muhammad Aminul Islam, Bryce Murray, Andrew Buck, Derek T. Anderson, Grant Scott, Mihail Popescu, James Keller
- **Comment**: None
- **Journal**: None
- **Summary**: While most deep learning architectures are built on convolution, alternative foundations like morphology are being explored for purposes like interpretability and its connection to the analysis and processing of geometric structures. The morphological hit-or-miss operation has the advantage that it takes into account both foreground and background information when evaluating target shape in an image. Herein, we identify limitations in existing hit-or-miss neural definitions and we formulate an optimization problem to learn the transform relative to deeper architectures. To this end, we model the semantically important condition that the intersection of the hit and miss structuring elements (SEs) should be empty and we present a way to express Don't Care (DNC), which is important for denoting regions of an SE that are not relevant to detecting a target pattern. Our analysis shows that convolution, in fact, acts like a hit-miss transform through semantic interpretation of its filter differences. On these premises, we introduce an extension that outperforms conventional convolution on benchmark data. Quantitative experiments are provided on synthetic and benchmark data, showing that the direct encoding hit-or-miss transform provides better interpretability on learned shapes consistent with objects whereas our morphologically inspired generalized convolution yields higher classification accuracy. Last, qualitative hit and miss filter visualizations are provided relative to single morphological layer.



### Angular Visual Hardness
- **Arxiv ID**: http://arxiv.org/abs/1912.02279v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02279v4)
- **Published**: 2019-12-04 22:12:42+00:00
- **Updated**: 2020-07-10 20:58:01+00:00
- **Authors**: Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava, Animesh Garg, Anima Anandkumar
- **Comment**: None
- **Journal**: None
- **Summary**: Recent convolutional neural networks (CNNs) have led to impressive performance but often suffer from poor calibration. They tend to be overconfident, with the model confidence not always reflecting the underlying true ambiguity and hardness. In this paper, we propose angular visual hardness (AVH), a score given by the normalized angular distance between the sample feature embedding and the target classifier to measure sample hardness. We validate this score with an in-depth and extensive scientific study, and observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models improve on the classification of harder examples. We observe that the training dynamics of AVH is vastly different compared to the training loss. Specifically, AVH quickly reaches a plateau for all samples even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. We also find that AVH has a statistically significant correlation with human visual hardness. Finally, we demonstrate the benefit of AVH to a variety of applications such as self-training for domain adaptation and domain generalization.



### Deep Double Descent: Where Bigger Models and More Data Hurt
- **Arxiv ID**: http://arxiv.org/abs/1912.02292v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02292v1)
- **Published**: 2019-12-04 22:47:31+00:00
- **Updated**: 2019-12-04 22:47:31+00:00
- **Authors**: Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever
- **Comment**: G.K. and Y.B. contributed equally
- **Journal**: None
- **Summary**: We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.



