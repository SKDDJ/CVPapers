# Arxiv Papers in cs.CV on 2019-12-15
### What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance
- **Arxiv ID**: http://arxiv.org/abs/1912.06960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06960v1)
- **Published**: 2019-12-15 02:07:05+00:00
- **Updated**: 2019-12-15 02:07:05+00:00
- **Authors**: Mahmoud Afifi, Michael S Brown
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets.



### Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.06971v1
- **DOI**: 10.1109/TIP.2020.3028207
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06971v1)
- **Published**: 2019-12-15 04:10:48+00:00
- **Updated**: 2019-12-15 04:10:48+00:00
- **Authors**: Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Graph convolutional networks (GCNs), which generalize CNNs to more generic non-Euclidean structures, have achieved remarkable performance for skeleton-based action recognition. However, there still exist several issues in the previous GCN-based models. First, the topology of the graph is set heuristically and fixed over all the model layers and input data. This may not be suitable for the hierarchy of the GCN model and the diversity of the data in action recognition tasks. Second, the second-order information of the skeleton data, i.e., the length and orientation of the bones, is rarely investigated, which is naturally more informative and discriminative for the human action recognition. In this work, we propose a novel multi-stream attention-enhanced adaptive graph convolutional neural network (MS-AAGCN) for skeleton-based action recognition. The graph topology in our model can be either uniformly or individually learned based on the input data in an end-to-end manner. This data-driven approach increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Besides, the proposed adaptive graph convolutional layer is further enhanced by a spatial-temporal-channel attention module, which helps the model pay more attention to important joints, frames and features. Moreover, the information of both the joints and bones, together with their motion information, are simultaneously modeled in a multi-stream framework, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.



### Multi-task Deep Learning for Real-Time 3D Human Pose Estimation and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.08077v2
- **DOI**: 10.1109/TPAMI.2020.2976014
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08077v2)
- **Published**: 2019-12-15 04:14:54+00:00
- **Updated**: 2020-03-04 02:10:31+00:00
- **Authors**: Diogo C Luvizon, Hedi Tabia, David Picard
- **Comment**: Accepted to TPAMI. arXiv admin note: text overlap with
  arXiv:1802.09232
- **Journal**: None
- **Summary**: Human pose estimation and action recognition are related tasks since both problems are strongly dependent on the human body representation and analysis. Nonetheless, most recent methods in the literature handle the two problems separately. In this work, we propose a multi-task framework for jointly estimating 2D or 3D human poses from monocular color images and classifying human actions from video sequences. We show that a single architecture can be used to solve both problems in an efficient way and still achieves state-of-the-art or comparable results at each task while running at more than 100 frames per second. The proposed method benefits from high parameters sharing between the two tasks by unifying still images and video clips processing in a single pipeline, allowing the model to be trained with data from different categories simultaneously and in a seamlessly way. Additionally, we provide important insights for end-to-end training the proposed multi-task model by decoupling key prediction parts, which consistently leads to better accuracy on both tasks. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU RGB+D) demonstrate the effectiveness of our method on the targeted tasks. Our source code and trained weights are publicly available at https://github.com/dluvizon/deephar.



### Brain-Inspired Inference on Missing Video Sequence
- **Arxiv ID**: http://arxiv.org/abs/1912.06980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06980v1)
- **Published**: 2019-12-15 05:38:41+00:00
- **Updated**: 2019-12-15 05:38:41+00:00
- **Authors**: Weimian Li, Baoyang Chen, Wenmin Wang
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end architecture that could generate a variety of plausible video sequences correlating two given discontinuous frames. Our work is inspired by the human ability of inference. Specifically, given two static images, human are capable of inferring what might happen in between as well as present diverse versions of their inference. We firstly train our model to learn the transformation to understand the movement trends within given frames. For the sake of imitating the inference of human, we introduce a latent variable sampled from Gaussian distribution. By means of integrating different latent variables with learned transformation features, the model could learn more various possible motion modes. Then applying these motion modes on the original frame, we could acquire various corresponding intermediate video sequence. Moreover, the framework is trained in adversarial fashion with unsupervised learning. Evaluating on the moving Mnist dataset and the 2D Shapes dataset, we show that our model is capable of imitating the human inference to some extent.



### Action Genome: Actions as Composition of Spatio-temporal Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1912.06992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06992v1)
- **Published**: 2019-12-15 06:56:33+00:00
- **Updated**: 2019-12-15 06:56:33+00:00
- **Authors**: Jingwei Ji, Ranjay Krishna, Li Fei-Fei, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: Action recognition has typically treated actions and activities as monolithic events that occur in videos. However, there is evidence from Cognitive Science and Neuroscience that people actively encode activities into consistent hierarchical part structures. However in Computer Vision, few explorations on representations encoding event partonomies have been made. Inspired by evidence that the prototypical unit of an event is an action-object interaction, we introduce Action Genome, a representation that decomposes actions into spatio-temporal scene graphs. Action Genome captures changes between objects and their pairwise relationships while an action occurs. It contains 10K videos with 0.4M objects and 1.7M visual relationships annotated. With Action Genome, we extend an existing action recognition model by incorporating scene graphs as spatio-temporal feature banks to achieve better performance on the Charades dataset. Next, by decomposing and learning the temporal changes in visual relationships that result in an action, we demonstrate the utility of a hierarchical event decomposition by enabling few-shot action recognition, achieving 42.7% mAP using as few as 10 examples. Finally, we benchmark existing scene graph models on the new task of spatio-temporal scene graph prediction.



### Joint Learning of Generative Translator and Classifier for Visually Similar Classes
- **Arxiv ID**: http://arxiv.org/abs/1912.06994v2
- **DOI**: 10.1109/ACCESS.2020.3042302
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06994v2)
- **Published**: 2019-12-15 07:08:44+00:00
- **Updated**: 2020-12-02 15:16:38+00:00
- **Authors**: ByungIn Yoo, Tristan Sylvain, Yoshua Bengio, Junmo Kim
- **Comment**: 14 pages, 17 figures, 13 tables
- **Journal**: None
- **Summary**: In this paper, we propose a Generative Translation Classification Network (GTCN) for improving visual classification accuracy in settings where classes are visually similar and data is scarce. For this purpose, we propose joint learning from a scratch to train a classifier and a generative stochastic translation network end-to-end. The translation network is used to perform on-line data augmentation across classes, whereas previous works have mostly involved domain adaptation. To help the model further benefit from this data-augmentation, we introduce an adaptive fade-in loss and a quadruplet loss. We perform experiments on multiple datasets to demonstrate the proposed method's performance in varied settings. Of particular interest, training on 40% of the dataset is enough for our model to surpass the performance of baselines trained on the full dataset. When our architecture is trained on the full dataset, we achieve comparable performance with state-of-the-art methods despite using a light-weight architecture.



### C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.07009v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07009v2)
- **Published**: 2019-12-15 09:10:24+00:00
- **Updated**: 2020-04-03 11:17:23+00:00
- **Authors**: Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: Flow-based generative models have highly desirable properties like exact log-likelihood evaluation and exact latent-variable inference, however they are still in their infancy and have not received as much attention as alternative generative models. In this paper, we introduce C-Flow, a novel conditioning scheme that brings normalizing flows to an entirely new scenario with great possibilities for multi-modal data modeling. C-Flow is based on a parallel sequence of invertible mappings in which a source flow guides the target flow at every step, enabling fine-grained control over the generation process. We also devise a new strategy to model unordered 3D point clouds that, in combination with the conditioning scheme, makes it possible to address 3D reconstruction from a single image and its inverse problem of rendering an image given a point cloud. We demonstrate our conditioning method to be very adaptable, being also applicable to image manipulation, style transfer and multi-modal image-to-image mapping in a diversity of domains, including RGB images, segmentation maps, and edge masks.



### A Shape Transformation-based Dataset Augmentation Framework for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.07010v1
- **DOI**: 10.1007/s11263-020-01412-0
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07010v1)
- **Published**: 2019-12-15 09:12:04+00:00
- **Updated**: 2019-12-15 09:12:04+00:00
- **Authors**: Zhe Chen, Wanli Ouyang, Tongliang Liu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based computer vision is usually data-hungry. Many researchers attempt to augment datasets with synthesized data to improve model robustness. However, the augmentation of popular pedestrian datasets, such as Caltech and Citypersons, can be extremely challenging because real pedestrians are commonly in low quality. Due to the factors like occlusions, blurs, and low-resolution, it is significantly difficult for existing augmentation approaches, which generally synthesize data using 3D engines or generative adversarial networks (GANs), to generate realistic-looking pedestrians. Alternatively, to access much more natural-looking pedestrians, we propose to augment pedestrian detection datasets by transforming real pedestrians from the same dataset into different shapes. Accordingly, we propose the Shape Transformation-based Dataset Augmentation (STDA) framework. The proposed framework is composed of two subsequent modules, i.e. the shape-guided deformation and the environment adaptation. In the first module, we introduce a shape-guided warping field to help deform the shape of a real pedestrian into a different shape. Then, in the second stage, we propose an environment-aware blending map to better adapt the deformed pedestrians into surrounding environments, obtaining more realistic-looking pedestrians and more beneficial augmentation results for pedestrian detection. Extensive empirical studies on different pedestrian detection benchmarks show that the proposed STDA framework consistently produces much better augmentation results than other pedestrian synthesis approaches using low-quality pedestrians. By augmenting the original datasets, our proposed framework also improves the baseline pedestrian detector by up to 38% on the evaluated benchmarks, achieving state-of-the-art performance.



### BatVision: Learning to See 3D Spatial Layout with Two Ears
- **Arxiv ID**: http://arxiv.org/abs/1912.07011v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.07011v3)
- **Published**: 2019-12-15 09:33:04+00:00
- **Updated**: 2020-03-19 07:57:28+00:00
- **Authors**: Jesper Haahr Christensen, Sascha Hornauer, Stella Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception. Inspired by bats' echolocation mechanism, we design a low-cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.



### DerainCycleGAN: Rain Attentive CycleGAN for Single Image Deraining and Rainmaking
- **Arxiv ID**: http://arxiv.org/abs/1912.07015v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07015v4)
- **Published**: 2019-12-15 10:26:54+00:00
- **Updated**: 2021-04-07 08:46:11+00:00
- **Authors**: Yanyan Wei, Zhao Zhang, Yang Wang, Mingliang Xu, Yi Yang, Shuicheng Yan, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image deraining (SID) is an important and challenging topic in emerging vision applications, and most of emerged deraining methods are supervised relying on the ground truth (i.e., paired images) in recent years. However, in practice it is rather common to have no un-paired images in real deraining task, in such cases how to remove the rain streaks in an unsupervised way will be a very challenging task due to lack of constraints between images and hence suffering from low-quality recovery results. In this paper, we explore the unsupervised SID task using unpaired data and propose a novel net called Attention-guided Deraining by Constrained CycleGAN (or shortly, DerainCycleGAN), which can fully utilize the constrained transfer learning abilitiy and circulatory structure of CycleGAN. Specifically, we design an unsu-pervised attention guided rain streak extractor (U-ARSE) that utilizes a memory to extract the rain streak masks with two constrained cycle-consistency branches jointly by paying attention to both the rainy and rain-free image domains. As a by-product, we also contribute a new paired rain image dataset called Rain200A, which is constructed by our network automatically. Compared with existing synthesis datasets, the rainy streaks in Rain200A contains more obvious and diverse shapes and directions. As a result, existing supervised methods trained on Rain200A can perform much better for processing real rainy images. Extensive experiments on synthesis and real datasets show that our net is superior to existing unsupervised deraining networks, and is also very competitive to other related supervised networks.



### Compressed DenseNet for Lightweight Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.07016v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07016v3)
- **Published**: 2019-12-15 10:26:59+00:00
- **Updated**: 2020-07-22 11:28:17+00:00
- **Authors**: Zhao Zhang, Zemin Tang, Yang Wang, Haijun Zhang, Shuicheng Yan, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Recurrent Neural Network (CRNN) is a popular network for recognizing texts in images. Advances like the variant of CRNN, such as Dense Convolutional Network with Connectionist Temporal Classification, has reduced the running time of the network, but exposing the inner computation cost and weight size of the convolutional networks as a bottleneck. Specifically, the DenseNet based models utilize the dense blocks as the core module, but the inner features are combined in the form of concatenation in dense blocks. As such, the number of channels of combined features delivered as the input of the layers close to the output and the relevant computational cost grows rapidly with the dense blocks getting deeper. This will severely bring heavy computational cost and big weight size, which restrict the depth of dense blocks. In this paper, we propose a compressed convolution block called Lightweight Dense Block (LDB). To reduce the computing cost and weight size, we re-define and re-design the way of combining internal features of the dense blocks. LDB is a convolutional block similarly as dense block, but it can reduce the computation cost and weight size to (1/L, 2/L), compared with original ones, where L is the number of layers in blocks. Moreover, LDB can be used to replace the original dense block in any DenseNet based models. Based on the LDBs, we propose a Compressed DenseNet (CDenseNet) for the lightweight character recognition. Extensive experiments demonstrate that CDenseNet can effectively reduce the weight size while delivering the promising recognition results.



### Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1912.07025v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1912.07025v1)
- **Published**: 2019-12-15 11:42:27+00:00
- **Updated**: 2019-12-15 11:42:27+00:00
- **Authors**: Abhishek Prusty, Sowmya Aitha, Abhishek Trivedi, Ravi Kiran Sarvadevabhatla
- **Comment**: Oral presentation at International Conference on Document Analysis
  and Recognition (ICDAR) - 2019. For dataset, pre-trained networks and
  additional details, visit project page at http://ihdia.iiit.ac.in/
- **Journal**: None
- **Summary**: Historical palm-leaf manuscript and early paper documents from Indian subcontinent form an important part of the world's literary and cultural heritage. Despite their importance, large-scale annotated Indic manuscript image datasets do not exist. To address this deficiency, we introduce Indiscapes, the first ever dataset with multi-regional layout annotations for historical Indic manuscripts. To address the challenge of large diversity in scripts and presence of dense, irregular layout elements (e.g. text lines, pictures, multiple documents per image), we adapt a Fully Convolutional Deep Neural Network architecture for fully automatic, instance-level spatial layout parsing of manuscript images. We demonstrate the effectiveness of proposed architecture on images from the Indiscapes dataset. For annotation flexibility and keeping the non-technical nature of domain experts in mind, we also contribute a custom, web-based GUI annotation tool and a dashboard-style analytics portal. Overall, our contributions set the stage for enabling downstream applications such as OCR and word-spotting in historical Indic manuscripts at scale.



### Necessary and Sufficient Polynomial Constraints on Compatible Triplets of Essential Matrices
- **Arxiv ID**: http://arxiv.org/abs/1912.11987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11987v1)
- **Published**: 2019-12-15 18:03:00+00:00
- **Updated**: 2019-12-15 18:03:00+00:00
- **Authors**: E. V. Martyushev
- **Comment**: 11 pages, 1 figure
- **Journal**: None
- **Summary**: The essential matrix incorporates relative rotation and translation parameters of two calibrated cameras. The well-known algebraic characterization of essential matrices, i.e. necessary and sufficient conditions under which an arbitrary matrix (of rank two) becomes essential, consists of a unique matrix equation of degree three. Based on this equation, a number of efficient algorithmic solutions to different relative pose estimation problems have been proposed. In three views, a possible way to describe the geometry of three calibrated cameras comes from considering compatible triplets of essential matrices. The compatibility is meant the correspondence of a triplet to a certain configuration of calibrated cameras. The main goal of this paper is to give an algebraic characterization of compatible triplets of essential matrices. Specifically, we propose necessary and sufficient polynomial constraints on a triplet of real rank-two essential matrices that ensure its compatibility. The constraints are given in the form of six cubic matrix equations, one quartic and one sextic scalar equations. An important advantage of the proposed constraints is their sufficiency even in the case of cameras with collinear centers. The applications of the constraints may include relative camera pose estimation in three and more views, averaging of essential matrices for incremental structure from motion, multiview camera auto-calibration, etc.



### Semantic-Aware Label Placement for Augmented Reality in Street View
- **Arxiv ID**: http://arxiv.org/abs/1912.07105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07105v1)
- **Published**: 2019-12-15 20:29:37+00:00
- **Updated**: 2019-12-15 20:29:37+00:00
- **Authors**: Jianqing Jia, Semir Elezovikj, Heng Fan, Shuojin Yang, Jing Liu, Wei Guo, Chiu C. Tan, Haibin Ling
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: In an augmented reality (AR) application, placing labels in a manner that is clear and readable without occluding the critical information from the real-world can be a challenging problem. This paper introduces a label placement technique for AR used in street view scenarios. We propose a semantic-aware task-specific label placement method by identifying potentially important image regions through a novel feature map, which we refer to as guidance map. Given an input image, its saliency information, semantic information and the task-specific importance prior are integrated into the guidance map for our labeling task. To learn the task prior, we created a label placement dataset with the users' labeling preferences, as well as use it for evaluation. Our solution encodes the constraints for placing labels in an optimization problem to obtain the final label layout, and the labels will be placed in appropriate positions to reduce the chances of overlaying important real-world objects in street view AR scenarios. The experimental validation shows clearly the benefits of our method over previous solutions in the AR street view navigation and similar applications.



### Towards Building a Real Time Mobile Device Bird Counting System Through Synthetic Data Training and Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1912.07106v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07106v2)
- **Published**: 2019-12-15 20:30:24+00:00
- **Updated**: 2019-12-28 02:37:49+00:00
- **Authors**: Runde Yang
- **Comment**: The paper is in a wrong format for ICML. I really need to withdraw
  the paper to modify the content and submit it to other computer vision
  conferences. Some sections need to be completely rewritten and I recognize
  certain parts that are not consistent with the major theme of the paper
- **Journal**: None
- **Summary**: Counting the number of birds in an open sky setting has been an challenging problem due to the large number of bird flocks and the birds can overlap. Another difficulty is the lack of accurate training samples since the cost of labeling images of bird flocks can be extremely high and each sample picture can contain thousands of birds in a high resolution image. Inspired by recent work on training with synthetic data to perform crowd counting, we design a mechanism to generate synthetic bird dataset with precise bird count and the corresponding density maps. We then train a Unet model on the synthetic dataset to perform density map estimation that produces the count for each input. Our method is able to achieve MSE of approximately 12.4 on real dataset. In order to build a scalable system for fast bird counting under storage and computational constraints, we use model compression techniques and efficient model structures to increase the inference speed and save storage cost. We are able to reduce storage cost from 55MB to less than 5MB for the model with minimum loss of accuracy. This paper describes the pipelines of building an efficient bird counting system.



### SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization
- **Arxiv ID**: http://arxiv.org/abs/1912.07109v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07109v2)
- **Published**: 2019-12-15 21:06:46+00:00
- **Updated**: 2022-02-22 15:39:10+00:00
- **Authors**: Yue Jiang, Dantong Ji, Zhizhong Han, Matthias Zwicker
- **Comment**: CVPR2020 Full Paper (Oral Top 5%)
- **Journal**: None
- **Summary**: We propose SDFDiff, a novel approach for image-based shape optimization using differentiable rendering of 3D shapes represented by signed distance functions (SDFs). Compared to other representations, SDFs have the advantage that they can represent shapes with arbitrary topology, and that they guarantee watertight surfaces. We apply our approach to the problem of multi-view 3D reconstruction, where we achieve high reconstruction quality and can capture complex topology of 3D objects. In addition, we employ a multi-resolution strategy to obtain a robust optimization algorithm. We further demonstrate that our SDF-based differentiable renderer can be integrated with deep learning models, which opens up options for learning approaches on 3D objects without 3D supervision. In particular, we apply our method to single-view 3D reconstruction and achieve state-of-the-art results.



### Image Processing Using Multi-Code GAN Prior
- **Arxiv ID**: http://arxiv.org/abs/1912.07116v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07116v2)
- **Published**: 2019-12-15 21:35:43+00:00
- **Updated**: 2020-03-31 10:20:50+00:00
- **Authors**: Jinjin Gu, Yujun Shen, Bolei Zhou
- **Comment**: CVPR2020 camera-ready
- **Journal**: None
- **Summary**: Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.



### Domain Agnostic Feature Learning for Image and Video Based Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1912.07124v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07124v2)
- **Published**: 2019-12-15 22:46:53+00:00
- **Updated**: 2020-04-13 16:09:31+00:00
- **Authors**: Suman Saha, Wenhao Xu, Menelaos Kanakis, Stamatios Georgoulis, Yuhua Chen, Danda Pani Paudel, Luc Van Gool
- **Comment**: CVPR 2020 Biometrics Workshop
- **Journal**: None
- **Summary**: Nowadays, the increasingly growing number of mobile and computing devices has led to a demand for safer user authentication systems. Face anti-spoofing is a measure towards this direction for bio-metric user authentication, and in particular face recognition, that tries to prevent spoof attacks. The state-of-the-art anti-spoofing techniques leverage the ability of deep neural networks to learn discriminative features, based on cues from the training set images or video samples, in an effort to detect spoof attacks. However, due to the particular nature of the problem, i.e. large variability due to factors like different backgrounds, lighting conditions, camera resolutions, spoof materials, etc., these techniques typically fail to generalize to new samples. In this paper, we explicitly tackle this problem and propose a class-conditional domain discriminator module, that, coupled with a gradient reversal layer, tries to generate live and spoof features that are discriminative, but at the same time robust against the aforementioned variability factors. Extensive experimental analysis shows the effectiveness of the proposed method over existing image- and video-based anti-spoofing techniques, both in terms of numerical improvement as well as when visualizing the learned features.



### Digital filters with vanishing moments for shape analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.07133v4
- **DOI**: 10.1117/1.JEI.27.5.051219
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07133v4)
- **Published**: 2019-12-15 23:24:58+00:00
- **Updated**: 2020-04-09 00:07:09+00:00
- **Authors**: Hugh L. Kennedy
- **Comment**: Fixed some cut-and-paste typos in Table V
- **Journal**: SPIE Journal of Electronic Imaging, vol. 27, no. 5, 051219, May
  2018
- **Summary**: Shape- and scale-selective digital-filters, with steerable finite/infinite impulse responses (FIR/IIRs) and non-recursive/recursive realizations, that are separable in both spatial dimensions and adequately isotropic, are derived. The filters are conveniently designed in the frequency domain via derivative constraints at dc, which guarantees orthogonality and monomial selectivity in the pixel domain (i.e. vanishing moments), unlike more commonly used FIR filters derived from Gaussian functions. A two-stage low-pass/high-pass architecture, for blur/derivative operations, is recommended. Expressions for the coefficients of a low-order IIR blur filter with repeated poles are provided, as a function of scale; discrete Butterworth (IIR), and colored Savitzky-Golay (FIR), blurs are also examined. Parallel software implementations on central processing units (CPUs) and graphics processing units (GPUs), for scale-selective blob-detection in aerial surveillance imagery, are analyzed. It is shown that recursive IIR filters are significantly faster than non-recursive FIR filters when detecting large objects at coarse scales, i.e. using filters with long impulse responses; however, the margin of outperformance decreases as the degree of parallelization increases.



