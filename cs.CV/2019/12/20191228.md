# Arxiv Papers in cs.CV on 2019-12-28
### Deep neural network models for computational histopathology: A survey
- **Arxiv ID**: http://arxiv.org/abs/1912.12378v2
- **DOI**: 10.1016/j.media.2020.101813
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12378v2)
- **Published**: 2019-12-28 01:04:25+00:00
- **Updated**: 2020-10-26 19:49:53+00:00
- **Authors**: Chetan L. Srinidhi, Ozan Ciga, Anne L. Martel
- **Comment**: Published in Medical Image Analysis, Vol. 67, Jan 2021.
  (10.1016/j.media.2020.101813)
- **Journal**: Medical Image Analysis, Vol. 67, Jan 2021
- **Summary**: Histopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to diseases progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting cancer histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the fields progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.



### Application of Deep Learning in Generating Desired Design Options: Experiments Using Synthetic Training Dataset
- **Arxiv ID**: http://arxiv.org/abs/2001.05849v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05849v2)
- **Published**: 2019-12-28 01:26:20+00:00
- **Updated**: 2021-06-07 19:33:48+00:00
- **Authors**: Zohreh Shaghaghian, Wei Yan
- **Comment**: 10 pages, 12 figures, 1 table
- **Journal**: Proceedings of the 2020 Building Performance Analysis Conference
  and SimBuild. 2020 535-544
- **Summary**: Most design methods contain a forward framework, asking for primary specifications of a building to generate an output or assess its performance. However, architects urge for specific objectives though uncertain of the proper design parameters. Deep Learning (DL) algorithms provide an intelligent workflow in which the system can learn from sequential training experiments. This study applies a method using DL algorithms towards generating demanded design options. In this study, an object recognition problem is investigated to initially predict the label of unseen sample images based on training dataset consisting of different types of synthetic 2D shapes; later, a generative DL algorithm is applied to be trained and generate new shapes for given labels. In the next step, the algorithm is trained to generate a window/wall pattern for desired light/shadow performance based on the spatial daylight autonomy (sDA) metrics. The experiments show promising results both in predicting unseen sample shapes and generating new design options.



### Statistical Loss and Analysis for Deep Learning in Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.12385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12385v2)
- **Published**: 2019-12-28 02:41:49+00:00
- **Updated**: 2020-03-11 07:01:15+00:00
- **Authors**: Zhiqiang Gong, Ping Zhong, Weidong Hu
- **Comment**: Accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Nowadays, deep learning methods, especially the convolutional neural networks (CNNs), have shown impressive performance on extracting abstract and high-level features from the hyperspectral image. However, general training process of CNNs mainly considers the pixel-wise information or the samples' correlation to formulate the penalization while ignores the statistical properties especially the spectral variability of each class in the hyperspectral image. These samples-based penalizations would lead to the uncertainty of the training process due to the imbalanced and limited number of training samples. To overcome this problem, this work characterizes each class from the hyperspectral image as a statistical distribution and further develops a novel statistical loss with the distributions, not directly with samples for deep learning. Based on the Fisher discrimination criterion, the loss penalizes the sample variance of each class distribution to decrease the intra-class variance of the training samples. Moreover, an additional diversity-promoting condition is added to enlarge the inter-class variance between different class distributions and this could better discriminate samples from different classes in hyperspectral image. Finally, the statistical estimation form of the statistical loss is developed with the training samples through multi-variant statistical analysis. Experiments over the real-world hyperspectral images show the effectiveness of the developed statistical loss for deep learning.



### Opportunities and Challenges of Deep Learning Methods for Electrocardiogram Data: A Systematic Review
- **Arxiv ID**: http://arxiv.org/abs/2001.01550v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.01550v3)
- **Published**: 2019-12-28 02:44:29+00:00
- **Updated**: 2020-04-30 18:42:49+00:00
- **Authors**: Shenda Hong, Yuxi Zhou, Junyuan Shang, Cao Xiao, Jimeng Sun
- **Comment**: Accepted by Computers in Biology and Medicine
- **Journal**: None
- **Summary**: Background:The electrocardiogram (ECG) is one of the most commonly used diagnostic tools in medicine and healthcare. Deep learning methods have achieved promising results on predictive healthcare tasks using ECG signals. Objective:This paper presents a systematic review of deep learning methods for ECG data from both modeling and application perspectives. Methods:We extracted papers that applied deep learning (deep neural network) models to ECG data that were published between Jan. 1st of 2010 and Feb. 29th of 2020 from Google Scholar, PubMed, and the DBLP. We then analyzed each article according to three factors: tasks, models, and data. Finally, we discuss open challenges and unsolved problems in this area. Results: The total number of papers extracted was 191. Among these papers, 108 were published after 2019. Different deep learning architectures have been used in various ECG analytics tasks, such as disease detection/classification, annotation/localization, sleep staging, biometric human identification, and denoising. Conclusion: The number of works on deep learning for ECG data has grown explosively in recent years. Such works have achieved accuracy comparable to that of traditional feature-based approaches and ensembles of multiple approaches can achieve even better results. Specifically, we found that a hybrid architecture of a convolutional neural network and recurrent neural network ensemble using expert features yields the best results. However, there are some new challenges and problems related to interpretability, scalability, and efficiency that must be addressed. Furthermore, it is also worth investigating new applications from the perspectives of datasets and methods. Significance: This paper summarizes existing deep learning research using ECG data from multiple perspectives and highlights existing challenges and problems to identify potential future research directions.



### Translating multispectral imagery to nighttime imagery via conditional generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/2001.05848v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05848v1)
- **Published**: 2019-12-28 03:20:29+00:00
- **Updated**: 2019-12-28 03:20:29+00:00
- **Authors**: Xiao Huang, Dong Xu, Zhenlong Li, Cuizhen Wang
- **Comment**: 4 pages, 3 figures, submitted to the 2020 IEEE International
  Geoscience and Remote Sensing Symposium
- **Journal**: None
- **Summary**: Nighttime satellite imagery has been applied in a wide range of fields. However, our limited understanding of how observed light intensity is formed and whether it can be simulated greatly hinders its further application. This study explores the potential of conditional Generative Adversarial Networks (cGAN) in translating multispectral imagery to nighttime imagery. A popular cGAN framework, pix2pix, was adopted and modified to facilitate this translation using gridded training image pairs derived from Landsat 8 and Visible Infrared Imaging Radiometer Suite (VIIRS). The results of this study prove the possibility of multispectral-to-nighttime translation and further indicate that, with the additional social media data, the generated nighttime imagery can be very similar to the ground-truth imagery. This study fills the gap in understanding the composition of satellite observed nighttime light and provides new paradigms to solve the emerging problems in nighttime remote sensing fields, including nighttime series construction, light desaturation, and multi-sensor calibration.



### All-in-One Image-Grounded Conversational Agents
- **Arxiv ID**: http://arxiv.org/abs/1912.12394v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12394v2)
- **Published**: 2019-12-28 03:51:52+00:00
- **Updated**: 2020-01-15 23:10:55+00:00
- **Authors**: Da Ju, Kurt Shuster, Y-Lan Boureau, Jason Weston
- **Comment**: None
- **Journal**: None
- **Summary**: As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore. In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective. We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks. We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks. Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.



### MulGAN: Facial Attribute Editing by Exemplar
- **Arxiv ID**: http://arxiv.org/abs/1912.12396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12396v1)
- **Published**: 2019-12-28 04:02:15+00:00
- **Updated**: 2019-12-28 04:02:15+00:00
- **Authors**: Jingtao Guo, Zhenzhen Qian, Zuowei Zhou, Yi Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies on face attribute editing by exemplars have achieved promising results due to the increasing power of deep convolutional networks and generative adversarial networks. These methods encode attribute-related information in images into the predefined region of the latent feature space by employing a pair of images with opposite attributes as input to train model, the face attribute transfer between the input image and the exemplar can be achieved by exchanging their attribute-related latent feature region. However, they suffer from three limitations: (1) the model must be trained using a pair of images with opposite attributes as input; (2) weak capability of editing multiple attributes by exemplars; (3) poor quality of generating image. Instead of imposing opposite-attribute constraints on the input image in order to make the attribute information of images be encoded in the predefined region of the latent feature space, in this work we directly apply the attribute labels constraint to the predefined region of the latent feature space. Meanwhile, an attribute classification loss is employed to make the model learn to extract the attribute-related information of images into the predefined latent feature region of the corresponding attribute, which enables our method to transfer multiple attributes of the exemplar simultaneously. Besides, a novel model structure is designed to enhance attribute transfer capabilities by exemplars while improve the quality of the generated image. Experiments demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA dataset.



### A Genetic Algorithm based Kernel-size Selection Approach for a Multi-column Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1912.12405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12405v2)
- **Published**: 2019-12-28 05:37:28+00:00
- **Updated**: 2020-03-16 17:06:44+00:00
- **Authors**: Animesh Singh, Sandip Saha, Ritesh Sarkhel, Mahantapas Kundu, Mita Nasipuri, Nibaran Das
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural network-based architectures give promising results in various domains including pattern recognition. Finding the optimal combination of the hyper-parameters of such a large-sized architecture is tedious and requires a large number of laboratory experiments. But, identifying the optimal combination of a hyper-parameter or appropriate kernel size for a given architecture of deep learning is always a challenging and tedious task. Here, we introduced a genetic algorithm-based technique to reduce the efforts of finding the optimal combination of a hyper-parameter (kernel size) of a convolutional neural network-based architecture. The method is evaluated on three popular datasets of different handwritten Bangla characters and digits. The implementation of the proposed methodology can be found in the following link: https://github.com/DeepQn/GA-Based-Kernel-Size.



### RoadTagger: Robust Road Attribute Inference with Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12408v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12408v1)
- **Published**: 2019-12-28 06:09:13+00:00
- **Updated**: 2019-12-28 06:09:13+00:00
- **Authors**: Songtao He, Favyen Bastani, Satvat Jagwani, Edward Park, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Samuel Madden, Mohammad Amin Sadeghi
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring road attributes such as lane count and road type from satellite imagery is challenging. Often, due to the occlusion in satellite imagery and the spatial correlation of road attributes, a road attribute at one position on a road may only be apparent when considering far-away segments of the road. Thus, to robustly infer road attributes, the model must integrate scattered information and capture the spatial correlation of features along roads. Existing solutions that rely on image classifiers fail to capture this correlation, resulting in poor accuracy. We find this failure is caused by a fundamental limitation -- the limited effective receptive field of image classifiers. To overcome this limitation, we propose RoadTagger, an end-to-end architecture which combines both Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) to infer road attributes. The usage of graph neural networks allows information propagation on the road network graph and eliminates the receptive field limitation of image classifiers. We evaluate RoadTagger on both a large real-world dataset covering 688 km^2 area in 20 U.S. cities and a synthesized micro-dataset. In the evaluation, RoadTagger improves inference accuracy over the CNN image classifier based approaches. RoadTagger also demonstrates strong robustness against different disruptions in the satellite imagery and the ability to learn complicated inductive rules for aggregating scattered information along the road network.



### Classifying topological sector via machine learning
- **Arxiv ID**: http://arxiv.org/abs/1912.12410v1
- **DOI**: None
- **Categories**: **hep-lat**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12410v1)
- **Published**: 2019-12-28 06:22:41+00:00
- **Updated**: 2019-12-28 06:22:41+00:00
- **Authors**: Masakiyo Kitazawa, Takuya Matsumoto, Yasuhiro Kohno
- **Comment**: 7 pages, 4 figures, 4 tables, talk presented at the 37th
  International Symposium on Lattice Field Theory - Lattice 2019, 16-22 June
  2019, Wuhan, China
- **Journal**: None
- **Summary**: We employ a machine learning technique for an estimate of the topological charge $Q$ of gauge configurations in SU(3) Yang-Mills theory in vacuum. As a first trial, we feed the four-dimensional topological charge density with and without smoothing into the convolutional neural network and train it to estimate the value of $Q$. We find that the trained neural network can estimate the value of $Q$ from the topological charge density at small flow time with high accuracy. Next, we perform the dimensional reduction of the input data as a preprocessing and analyze lower dimensional data by the neural network. We find that the accuracy of the neural network does not have statistically-significant dependence on the dimension of the input data. From this result we argue that the neural network does not find characteristic features responsible for the determination of $Q$ in the higher dimensional space.



### TextScanner: Reading Characters in Order for Robust Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.12422v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12422v2)
- **Published**: 2019-12-28 07:52:00+00:00
- **Updated**: 2020-01-01 10:18:26+00:00
- **Authors**: Zhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, Cong Yao
- **Comment**: Accepted by AAAI-2020
- **Journal**: None
- **Summary**: Driven by deep learning and the large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention based methods have dominated this field, but suffer from the problem of \textit{attention drift} in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such Chinese transcripts and aligning with target characters.



### Hybrid Channel Based Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.12431v2
- **DOI**: 10.1016/j.neucom.2019.12.110
- **Categories**: **cs.CV**, I.4.7; I.4.9; I.5.2; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1912.12431v2)
- **Published**: 2019-12-28 09:55:35+00:00
- **Updated**: 2020-01-30 04:20:34+00:00
- **Authors**: Fiseha B. Tesema, Hong Wu, Mingjian Chen, Junpeng Lin, William Zhu, Kaizhu Huang
- **Comment**: 9 pages, 4 figures, Submitted to Neurocomputing, The 5th line of
  table 3 was accidentally mistaken. The data have been corrected and the
  related descriptions in section 4.4 have also be revised accordingly. Typos
  corrected, references corrected
- **Journal**: Neurocomputing, 389(5), 2020, 1-8
- **Summary**: Pedestrian detection has achieved great improvements with the help of Convolutional Neural Networks (CNNs). CNN can learn high-level features from input images, but the insufficient spatial resolution of CNN feature channels (feature maps) may cause a loss of information, which is harmful especially to small instances. In this paper, we propose a new pedestrian detection framework, which extends the successful RPN+BF framework to combine handcrafted features and CNN features. RoI-pooling is used to extract features from both handcrafted channels (e.g. HOG+LUV, CheckerBoards or RotatedFilters) and CNN channels. Since handcrafted channels always have higher spatial resolution than CNN channels, we apply RoI-pooling with larger output resolution to handcrafted channels to keep more detailed information. Our ablation experiments show that the developed handcrafted features can reach better detection accuracy than the CNN features extracted from the VGG-16 net, and a performance gain can be achieved by combining them. Experimental results on Caltech pedestrian dataset with the original annotations and the improved annotations demonstrate the effectiveness of the proposed approach. When using a more advanced RPN in our framework, our approach can be further improved and get competitive results on both benchmarks.



### Silhouette-Net: 3D Hand Pose Estimation from Silhouettes
- **Arxiv ID**: http://arxiv.org/abs/1912.12436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12436v1)
- **Published**: 2019-12-28 10:29:42+00:00
- **Updated**: 2019-12-28 10:29:42+00:00
- **Authors**: Kuo-Wei Lee, Shih-Hung Liu, Hwann-Tzong Chen, Koichi Ito
- **Comment**: None
- **Journal**: None
- **Summary**: 3D hand pose estimation has received a lot of attention for its wide range of applications and has made great progress owing to the development of deep learning. Existing approaches mainly consider different input modalities and settings, such as monocular RGB, multi-view RGB, depth, or point cloud, to provide sufficient cues for resolving variations caused by self occlusion and viewpoint change. In contrast, this work aims to address the less-explored idea of using minimal information to estimate 3D hand poses. We present a new architecture that automatically learns a guidance from implicit depth perception and solves the ambiguity of hand pose through end-to-end training. The experimental results show that 3D hand poses can be accurately estimated from solely {\em hand silhouettes} without using depth maps. Extensive evaluations on the {\em 2017 Hands In the Million Challenge} (HIM2017) benchmark dataset further demonstrate that our method achieves comparable or even better performance than recent depth-based approaches and serves as the state-of-the-art of its own kind on estimating 3D hand poses from silhouettes.



### End-to-End Pixel-Based Deep Active Inference for Body Perception and Action
- **Arxiv ID**: http://arxiv.org/abs/2001.05847v3
- **DOI**: 10.1109/ICDL-EpiRob48136.2020.9278105
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/2001.05847v3)
- **Published**: 2019-12-28 12:19:09+00:00
- **Updated**: 2020-05-29 21:30:12+00:00
- **Authors**: Cansu Sancaktar, Marcel van Gerven, Pablo Lanillos
- **Comment**: None
- **Journal**: None
- **Summary**: We present a pixel-based deep active inference algorithm (PixelAI) inspired by human body perception and action. Our algorithm combines the free-energy principle from neuroscience, rooted in variational inference, with deep convolutional decoders to scale the algorithm to directly deal with raw visual input and provide online adaptive inference. Our approach is validated by studying body perception and action in a simulated and a real Nao robot. Results show that our approach allows the robot to perform 1) dynamical body estimation of its arm using only monocular camera images and 2) autonomous reaching to "imagined" arm poses in the visual space. This suggests that robot and human body perception and action can be efficiently solved by viewing both as an active inference problem guided by ongoing sensory input.



### Transfer Learning for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.12452v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12452v2)
- **Published**: 2019-12-28 12:45:34+00:00
- **Updated**: 2020-11-26 22:12:40+00:00
- **Authors**: Jonas Wacker, Marcelo Ladeira, José Eduardo Vaz Nascimento
- **Comment**: 11 pages, 4 figures, MICCAI BraTS 2020 workshop
- **Journal**: None
- **Summary**: Gliomas are the most common malignant brain tumors that are treated with chemoradiotherapy and surgery. Magnetic Resonance Imaging (MRI) is used by radiotherapists to manually segment brain lesions and to observe their development throughout the therapy. The manual image segmentation process is time-consuming and results tend to vary among different human raters. Therefore, there is a substantial demand for automatic image segmentation algorithms that produce a reliable and accurate segmentation of various brain tissue types. Recent advances in deep learning have led to convolutional neural network architectures that excel at various visual recognition tasks. They have been successfully applied to the medical context including medical image segmentation. In particular, fully convolutional networks (FCNs) such as the U-Net produce state-of-the-art results in the automatic segmentation of brain tumors. MRI brain scans are volumetric and exist in various co-registered modalities that serve as input channels for these FCN architectures. Training algorithms for brain tumor segmentation on this complex input requires large amounts of computational resources and is prone to overfitting. In this work, we construct FCNs with pretrained convolutional encoders. We show that we can stabilize the training process this way and achieve an improvement with respect to dice scores and Hausdorff distances. We also test our method on a privately obtained clinical dataset.



### Energy-based Graph Convolutional Networks for Scoring Protein Docking Models
- **Arxiv ID**: http://arxiv.org/abs/1912.12476v1
- **DOI**: 10.1002/prot.25888
- **Categories**: **q-bio.BM**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12476v1)
- **Published**: 2019-12-28 15:57:17+00:00
- **Updated**: 2019-12-28 15:57:17+00:00
- **Authors**: Yue Cao, Yang Shen
- **Comment**: None
- **Journal**: Proteins: Structure, Function, and Bioinformatics 88, no. 8
  (2020): 1091-1099
- **Summary**: Structural information about protein-protein interactions, often missing at the interactome scale, is important for mechanistic understanding of cells and rational discovery of therapeutics. Protein docking provides a computational alternative to predict such information. However, ranking near-native docked models high among a large number of candidates, often known as the scoring problem, remains a critical challenge. Moreover, estimating model quality, also known as the quality assessment problem, is rarely addressed in protein docking.   In this study the two challenging problems in protein docking are regarded as relative and absolute scoring, respectively, and addressed in one physics-inspired deep learning framework. We represent proteins' and encounter complexes' 3D structures as intra- and inter-molecular residue contact graphs with atom-resolution node and edge features. And we propose a novel graph convolutional kernel that pool interacting nodes' features through edge features so that generalized interaction energies can be learned directly from graph data. The resulting energy-based graph convolutional networks (EGCN) with multi-head attention are trained to predict intra- and inter-molecular energies, binding affinities, and quality measures (interface RMSD) for encounter complexes. Compared to a state-of-the-art scoring function for model ranking, EGCN has significantly improved ranking for a CAPRI test set involving homology docking; and is comparable for Score_set, a CAPRI benchmark set generated by diverse community-wide docking protocols not known to training data. For Score_set quality assessment, EGCN shows about 27% improvement to our previous efforts. Directly learning from 3D structure data in graph representation, EGCN represents the first successful development of graph convolutional networks for protein docking.



### Alleviation of Gradient Exploding in GANs: Fake Can Be Real
- **Arxiv ID**: http://arxiv.org/abs/1912.12485v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12485v2)
- **Published**: 2019-12-28 16:49:13+00:00
- **Updated**: 2020-03-16 10:34:08+00:00
- **Authors**: Song Tao, Jia Wang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: In order to alleviate the notorious mode collapse phenomenon in generative adversarial networks (GANs), we propose a novel training method of GANs in which certain fake samples are considered as real ones during the training process. This strategy can reduce the gradient value that generator receives in the region where gradient exploding happens. We show the process of an unbalanced generation and a vicious circle issue resulted from gradient exploding in practical training, which explains the instability of GANs. We also theoretically prove that gradient exploding can be alleviated by penalizing the difference between discriminator outputs and fake-as-real consideration for very close real and fake samples. Accordingly, Fake-As-Real GAN (FARGAN) is proposed with a more stable training process and a more faithful generated distribution. Experiments on different datasets verify our theoretical analysis.



### Detecting Out-of-Distribution Examples with In-distribution Examples and Gram Matrices
- **Arxiv ID**: http://arxiv.org/abs/1912.12510v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12510v2)
- **Published**: 2019-12-28 19:44:03+00:00
- **Updated**: 2020-01-09 15:17:55+00:00
- **Authors**: Chandramouli Shama Sastry, Sageev Oore
- **Comment**: NeurIPS 2019 Workshop on Safety and Robustness in Decision Making
- **Journal**: None
- **Summary**: When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and class predicted. We find that characterizing activity patterns by Gram matrices and identifying anomalies in gram matrix values can yield high OOD detection rates. We identify anomalies in the gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and does not require access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. The method is applicable across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far-from-distribution out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).



### NAS evaluation is frustratingly hard
- **Arxiv ID**: http://arxiv.org/abs/1912.12522v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12522v3)
- **Published**: 2019-12-28 21:24:12+00:00
- **Updated**: 2020-02-13 22:10:12+00:00
- **Authors**: Antoine Yang, Pedro M. Esperança, Fabio M. Carlucci
- **Comment**: Published as a conference paper at ICLR2020; 13 pages; 10 figures
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of $8$ NAS methods on $5$ datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macro-structure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between $8$ and $20$ cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls. The code used is available at https://github.com/antoyang/NAS-Benchmark.



### Learning to segment images with classification labels
- **Arxiv ID**: http://arxiv.org/abs/1912.12533v2
- **DOI**: 10.1016/j.media.2020.101912
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12533v2)
- **Published**: 2019-12-28 22:01:39+00:00
- **Updated**: 2020-11-29 19:56:32+00:00
- **Authors**: Ozan Ciga, Anne L. Martel
- **Comment**: Published on Elsevier, Medical Image Analysis
- **Journal**: Medical Image Analysis, Volume 68, 2021, 101912, ISSN 1361-8415
- **Summary**: Two of the most common tasks in medical imaging are classification and segmentation. Either task requires labeled data annotated by experts, which is scarce and expensive to collect. Annotating data for segmentation is generally considered to be more laborious as the annotator has to draw around the boundaries of regions of interest, as opposed to assigning image patches a class label. Furthermore, in tasks such as breast cancer histopathology, any realistic clinical application often includes working with whole slide images, whereas most publicly available training data are in the form of image patches, which are given a class label. We propose an architecture that can alleviate the requirements for segmentation-level ground truth by making use of image-level labels to reduce the amount of time spent on data curation. In addition, this architecture can help unlock the potential of previously acquired image-level datasets on segmentation tasks by annotating a small number of regions of interest. In our experiments, we show using only one segmentation-level annotation per class, we can achieve performance comparable to a fully annotated dataset.



