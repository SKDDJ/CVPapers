# Arxiv Papers in cs.CV on 2019-12-02
### Detecting GAN generated errors
- **Arxiv ID**: http://arxiv.org/abs/1912.00527v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00527v1)
- **Published**: 2019-12-02 00:24:35+00:00
- **Updated**: 2019-12-02 00:24:35+00:00
- **Authors**: Xiru Zhu, Fengdi Che, Tianzi Yang, Tzuyang Yu, David Meger, Gregory Dudek
- **Comment**: None
- **Journal**: None
- **Summary**: Despite an impressive performance from the latest GAN for generating hyper-realistic images, GAN discriminators have difficulty evaluating the quality of an individual generated sample. This is because the task of evaluating the quality of a generated image differs from deciding if an image is real or fake. A generated image could be perfect except in a single area but still be detected as fake. Instead, we propose a novel approach for detecting where errors occur within a generated image. By collaging real images with generated images, we compute for each pixel, whether it belongs to the real distribution or generated distribution. Furthermore, we leverage attention to model long-range dependency; this allows detection of errors which are reasonable locally but not holistically. For evaluation, we show that our error detection can act as a quality metric for an individual image, unlike FID and IS. We leverage Improved Wasserstein, BigGAN, and StyleGAN to show a ranking based on our metric correlates impressively with FID scores. Our work opens the door for better understanding of GAN and the ability to select the best samples from a GAN model.



### Deep Learning for Visual Tracking: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/1912.00535v2
- **DOI**: 10.1109/TITS.2020.3046478
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00535v2)
- **Published**: 2019-12-02 01:05:54+00:00
- **Updated**: 2021-01-26 09:05:50+00:00
- **Authors**: Seyed Mojtaba Marvasti-Zadeh, Li Cheng, Hossein Ghanei-Yakhdan, Shohreh Kasaei
- **Comment**: Accepted Manuscript in IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: Visual target tracking is one of the most sought-after yet challenging research topics in computer vision. Given the ill-posed nature of the problem and its popularity in a broad range of real-world scenarios, a number of large-scale benchmark datasets have been established, on which considerable methods have been developed and demonstrated with significant progress in recent years -- predominantly by recent deep learning (DL)-based methods. This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics. It also extensively evaluates and analyzes the leading visual tracking methods. First, the fundamental characteristics, primary motivations, and contributions of DL-based methods are summarized from nine key aspects of: network architecture, network exploitation, network training for visual tracking, network objective, network output, exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Second, popular visual tracking benchmarks and their respective properties are compared, and their evaluation metrics are summarized. Third, the state-of-the-art DL-based methods are comprehensively examined on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by conducting critical analyses of these state-of-the-art trackers quantitatively and qualitatively, their pros and cons under various common scenarios are investigated. It may serve as a gentle use guide for practitioners to weigh when and under what conditions to choose which method(s). It also facilitates a discussion on ongoing issues and sheds light on promising research directions.



### Pyramid Convolutional RNN for MRI Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.00543v7
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00543v7)
- **Published**: 2019-12-02 02:06:46+00:00
- **Updated**: 2022-02-22 03:19:23+00:00
- **Authors**: Eric Z. Chen, Puyang Wang, Xiao Chen, Terrence Chen, Shanhui Sun
- **Comment**: Copyright\copyright 2022 IEEE. Personal use of this material is
  permitted. However, permission to use this material for any other purposes
  must be obtained from the IEEE by sending a request to
  pubs-permissions@ieee.org
- **Journal**: None
- **Summary**: Fast and accurate MRI image reconstruction from undersampled data is crucial in clinical practice. Deep learning based reconstruction methods have shown promising advances in recent years. However, recovering fine details from undersampled data is still challenging. In this paper, we introduce a novel deep learning based method, Pyramid Convolutional RNN (PC-RNN), to reconstruct images from multiple scales. Based on the formulation of MRI reconstruction as an inverse problem, we design the PC-RNN model with three convolutional RNN (ConvRNN) modules to iteratively learn the features in multiple scales. Each ConvRNN module reconstructs images at different scales and the reconstructed images are combined by a final CNN module in a pyramid fashion. The multi-scale ConvRNN modules learn a coarse-to-fine image reconstruction. Unlike other common reconstruction methods for parallel imaging, PC-RNN does not employ coil sensitive maps for multi-coil data and directly model the multiple coils as multi-channel inputs. The coil compression technique is applied to standardize data with various coil numbers, leading to more efficient training. We evaluate our model on the fastMRI knee and brain datasets and the results show that the proposed model outperforms other methods and can recover more details. The proposed method is one of the winner solutions in the 2019 fastMRI competition.



### A Robust Stereo Camera Localization Method with Prior LiDAR Map Constrains
- **Arxiv ID**: http://arxiv.org/abs/1912.05023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.05023v1)
- **Published**: 2019-12-02 03:17:44+00:00
- **Updated**: 2019-12-02 03:17:44+00:00
- **Authors**: Dong Han, Zuhao Zou, Lujia Wang, Cheng-Zhong Xu
- **Comment**: 6 pages, 5 figures, The 2019 IEEE International Conference on
  Robotics and Biomimetics
- **Journal**: None
- **Summary**: In complex environments, low-cost and robust localization is a challenging problem. For example, in a GPSdenied environment, LiDAR can provide accurate position information, but the cost is high. In general, visual SLAM based localization methods become unreliable when the sunlight changes greatly. Therefore, inexpensive and reliable methods are required. In this paper, we propose a stereo visual localization method based on the prior LiDAR map. Different from the conventional visual localization system, we design a novel visual optimization model by matching planar information between the LiDAR map and visual image. Bundle adjustment is built by using coplanarity constraints. To solve the optimization problem, we use a graph-based optimization algorithm and a local window optimization method. Finally, we estimate a full six degrees of freedom (DOF) pose without scale drift. To validate the efficiency, the proposed method has been tested on the KITTI dataset. The results show that our method is more robust and accurate than the state-of-art ORB-SLAM2.



### Fastened CROWN: Tightened Neural Network Robustness Certificates
- **Arxiv ID**: http://arxiv.org/abs/1912.00574v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00574v1)
- **Published**: 2019-12-02 03:54:20+00:00
- **Updated**: 2019-12-02 03:54:20+00:00
- **Authors**: Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, Luca Daniel
- **Comment**: Zhaoyang Lyu and Ching-Yun Ko contributed equally, accepted to AAAI
  2020
- **Journal**: None
- **Summary**: The rapid growth of deep learning applications in real life is accompanied by severe safety concerns. To mitigate this uneasy phenomenon, much research has been done providing reliable evaluations of the fragility level in different deep neural networks. Apart from devising adversarial attacks, quantifiers that certify safeguarded regions have also been designed in the past five years. The summarizing work of Salman et al. unifies a family of existing verifiers under a convex relaxation framework. We draw inspiration from such work and further demonstrate the optimality of deterministic CROWN (Zhang et al. 2018) solutions in a given linear programming problem under mild constraints. Given this theoretical result, the computationally expensive linear programming based method is shown to be unnecessary. We then propose an optimization-based approach \textit{FROWN} (\textbf{F}astened C\textbf{ROWN}): a general algorithm to tighten robustness certificates for neural networks. Extensive experiments on various networks trained individually verify the effectiveness of FROWN in safeguarding larger robust regions.



### Skeleton based Activity Recognition by Fusing Part-wise Spatio-temporal and Attention Driven Residues
- **Arxiv ID**: http://arxiv.org/abs/1912.00576v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00576v1)
- **Published**: 2019-12-02 04:09:22+00:00
- **Updated**: 2019-12-02 04:09:22+00:00
- **Authors**: Chhavi Dhiman, Dinesh Kumar Vishwakarma, Paras Aggarwal
- **Comment**: 20 pages, 9 figures
- **Journal**: None
- **Summary**: There exist a wide range of intra class variations of the same actions and inter class similarity among the actions, at the same time, which makes the action recognition in videos very challenging. In this paper, we present a novel skeleton-based part-wise Spatiotemporal CNN RIAC Network-based 3D human action recognition framework to visualise the action dynamics in part wise manner and utilise each part for action recognition by applying weighted late fusion mechanism. Part wise skeleton based motion dynamics helps to highlight local features of the skeleton which is performed by partitioning the complete skeleton in five parts such as Head to Spine, Left Leg, Right Leg, Left Hand, Right Hand. The RIAFNet architecture is greatly inspired by the InceptionV4 architecture which unified the ResNet and Inception based Spatio-temporal feature representation concept and achieving the highest top-1 accuracy till date. To extract and learn salient features for action recognition, attention driven residues are used which enhance the performance of residual components for effective 3D skeleton-based Spatio-temporal action representation. The robustness of the proposed framework is evaluated by performing extensive experiments on three challenging datasets such as UT Kinect Action 3D, Florence 3D action Dataset, and MSR Daily Action3D datasets, which consistently demonstrate the superiority of our method



### Exposing and Correcting the Gender Bias in Image Captioning Datasets and Models
- **Arxiv ID**: http://arxiv.org/abs/1912.00578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00578v1)
- **Published**: 2019-12-02 04:14:39+00:00
- **Updated**: 2019-12-02 04:14:39+00:00
- **Authors**: Shruti Bhargava, David Forsyth
- **Comment**: 44 pages
- **Journal**: None
- **Summary**: The task of image captioning implicitly involves gender identification. However, due to the gender bias in data, gender identification by an image captioning model suffers. Also, the gender-activity bias, owing to the word-by-word prediction, influences other words in the caption prediction, resulting in the well-known problem of label bias. In this work, we investigate gender bias in the COCO captioning dataset and show that it engenders not only from the statistical distribution of genders with contexts but also from the flawed annotation by the human annotators. We look at the issues created by this bias in the trained models. We propose a technique to get rid of the bias by splitting the task into 2 subtasks: gender-neutral image captioning and gender classification. By this decoupling, the gender-context influence can be eradicated. We train the gender-neutral image captioning model, which gives comparable results to a gendered model even when evaluating against a dataset that possesses a similar bias as the training data. Interestingly, the predictions by this model on images with no humans, are also visibly different from the one trained on gendered captions. We train gender classifiers using the available bounding box and mask-based annotations for the person in the image. This allows us to get rid of the context and focus on the person to predict the gender. By substituting the genders into the gender-neutral captions, we get the final gendered predictions. Our predictions achieve similar performance to a model trained with gender, and at the same time are devoid of gender bias. Finally, our main result is that on an anti-stereotypical dataset, our model outperforms a popular image captioning model which is trained with gender.



### Anomaly Detection in Particulate Matter Sensor using Hypothesis Pruning Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1912.00583v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00583v2)
- **Published**: 2019-12-02 05:43:20+00:00
- **Updated**: 2020-02-14 01:30:31+00:00
- **Authors**: YeongHyeon Park, Won Seok Park, Yeong Beom Kim
- **Comment**: None
- **Journal**: None
- **Summary**: World Health Organization (WHO) provides the guideline for managing the Particulate Matter (PM) level because when the PM level is higher, it threats the human health. For managing PM level, the procedure for measuring PM value is needed firstly. We use Tapered Element Oscillating Microbalance (TEOM)-based PM measuring sensors because it shows higher cost-effectiveness than Beta Attenuation Monitor (BAM)-based sensor. However, TEOM-based sensor has higher probability of malfunctioning than BAM-based sensor. In this paper, we call the overall malfunction as an anomaly, and we aim to detect anomalies for the maintenance of PM measuring sensors. We propose a novel architecture for solving the above aim that named as Hypothesis Pruning Generative Adversarial Network (HP-GAN). We experimentally compare the several anomaly detection architectures to certify ours performing better.



### Flow Contrastive Estimation of Energy-Based Models
- **Arxiv ID**: http://arxiv.org/abs/1912.00589v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00589v2)
- **Published**: 2019-12-02 06:29:36+00:00
- **Updated**: 2020-04-01 17:53:41+00:00
- **Authors**: Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, Ying Nian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.



### Face Detection with Feature Pyramids and Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1912.00596v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00596v2)
- **Published**: 2019-12-02 06:54:13+00:00
- **Updated**: 2019-12-18 03:39:25+00:00
- **Authors**: Samuel W. F. Earp, Pavit Noinongyao, Justin A. Cairns, Ankush Ganguly
- **Comment**: 12 pages, 2 figures, whitepaper
- **Journal**: None
- **Summary**: Accurate face detection and facial landmark localization are crucial to any face recognition system. We present a series of three single-stage RCNNs with different sized backbones (MobileNetV2-25, MobileNetV2-100, and ResNet101) and a six-layer feature pyramid trained exclusively on the WIDER FACE dataset. We compare the face detection and landmark accuracies using eight context module architectures, four proposed by previous research and four modified versions. We find no evidence that any of the proposed architectures significantly overperform and postulate that the random initialization of the additional layers is at least of equal importance. To show this we present a model that achieves near state-of-the-art performance on WIDER FACE and also provides high accuracy landmarks with a simple context module. We also present results using MobileNetV2 backbones, which achieve over 90% average precision on the WIDER FACE hard validation set while being able to run in real-time. By comparing to other authors, we show that our models exceed the state-of-the-art for similar-sized RCNNs and match the performance of much heavier networks.



### SPSTracker: Sub-Peak Suppression of Response Map for Robust Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.00597v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00597v3)
- **Published**: 2019-12-02 07:02:32+00:00
- **Updated**: 2020-05-03 14:21:07+00:00
- **Authors**: Qintao Hu, Lijun Zhou, Xiaoxiao Wang, Yao Mao, Jianlin Zhang, Qixiang Ye
- **Comment**: Accepted as oral paper at AAAI2020
- **Journal**: None
- **Summary**: Modern visual trackers usually construct online learning models under the assumption that the feature response has a Gaussian distribution with target-centered peak response. Nevertheless, such an assumption is implausible when there is progressive interference from other targets and/or background noise, which produce sub-peaks on the tracking response map and cause model drift. In this paper, we propose a rectified online learning approach for sub-peak response suppression and peak response enforcement and target at handling progressive interference in a systematic way. Our approach, referred to as SPSTracker, applies simple-yet-efficient Peak Response Pooling (PRP) to aggregate and align discriminative features, as well as leveraging a Boundary Response Truncation (BRT) to reduce the variance of feature response. By fusing with multi-scale features, SPSTracker aggregates the response distribution of multiple sub-peaks to a single maximum peak, which enforces the discriminative capability of features for robust object tracking. Experiments on the OTB, NFS and VOT2018 benchmarks demonstrate that SPSTrack outperforms the state-of-the-art real-time trackers with significant margins.



### DEGAS: Differentiable Efficient Generator Search
- **Arxiv ID**: http://arxiv.org/abs/1912.00606v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00606v3)
- **Published**: 2019-12-02 07:30:28+00:00
- **Updated**: 2020-12-29 08:01:50+00:00
- **Authors**: Sivan Doveh, Raja Giryes
- **Comment**: appeared at NeurIPS 2019, Meta-Learning Workshop
- **Journal**: None
- **Summary**: Network architecture search (NAS) achieves state-of-the-art results in various tasks such as classification and semantic segmentation. Recently, a reinforcement learning-based approach has been proposed for Generative Adversarial Networks (GANs) search. In this work, we propose an alternative strategy for GAN search by using a method called DEGAS (Differentiable Efficient GenerAtor Search), which focuses on efficiently finding the generator in the GAN. Our search algorithm is inspired by the differential architecture search strategy and the Global Latent Optimization (GLO) procedure. This leads to both an efficient and stable GAN search. After the generator architecture is found, it can be plugged into any existing framework for GAN training. For CTGAN, which we use in this work, the new model outperforms the original inception score results by 0.25 for CIFAR-10 and 0.77 for STL. It also gets better results than the RL based GAN search methods in shorter search time.



### Patchy Image Structure Classification Using Multi-Orientation Region Transform
- **Arxiv ID**: http://arxiv.org/abs/1912.00622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1912.00622v1)
- **Published**: 2019-12-02 08:19:30+00:00
- **Updated**: 2019-12-02 08:19:30+00:00
- **Authors**: Xiaohan Yu, Yang Zhao, Yongsheng Gao, Shengwu Xiong, Xiaohui Yuan
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Exterior contour and interior structure are both vital features for classifying objects. However, most of the existing methods consider exterior contour feature and internal structure feature separately, and thus fail to function when classifying patchy image structures that have similar contours and flexible structures. To address above limitations, this paper proposes a novel Multi-Orientation Region Transform (MORT), which can effectively characterize both contour and structure features simultaneously, for patchy image structure classification. MORT is performed over multiple orientation regions at multiple scales to effectively integrate patchy features, and thus enables a better description of the shape in a coarse-to-fine manner. Moreover, the proposed MORT can be extended to combine with the deep convolutional neural network techniques, for further enhancement of classification accuracy. Very encouraging experimental results on the challenging ultra-fine-grained cultivar recognition task, insect wing recognition task, and large variation butterfly recognition task are obtained, which demonstrate the effectiveness and superiority of the proposed MORT over the state-of-the-art methods in classifying patchy image structures. Our code and three patchy image structure datasets are available at: https://github.com/XiaohanYu-GU/MReT2019.



### Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task
- **Arxiv ID**: http://arxiv.org/abs/1912.00623v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00623v2)
- **Published**: 2019-12-02 08:23:10+00:00
- **Updated**: 2020-03-20 08:41:46+00:00
- **Authors**: Aritra Bhowmik, Stefan Gumhold, Carsten Rother, Eric Brachmann
- **Comment**: CVPR 2020 (oral)
- **Journal**: None
- **Summary**: We address a core problem of computer vision: Detection and description of 2D feature points for image matching. For a long time, hand-crafted designs, like the seminal SIFT algorithm, were unsurpassed in accuracy and efficiency. Recently, learned feature detectors emerged that implement detection and description using neural networks. Training these networks usually resorts to optimizing low-level matching scores, often pre-defining sets of image patches which should or should not match, or which should or should not contain key points. Unfortunately, increased accuracy for these low-level matching scores does not necessarily translate to better performance in high-level vision tasks. We propose a new training methodology which embeds the feature detector in a complete vision pipeline, and where the learnable parameters are trained in an end-to-end fashion. We overcome the discrete nature of key point selection and descriptor matching using principles from reinforcement learning. As an example, we address the task of relative pose estimation between a pair of images. We demonstrate that the accuracy of a state-of-the-art learning-based feature detector can be increased when trained for the task it is supposed to solve at test time. Our training methodology poses little restrictions on the task to learn, and works for any architecture which predicts key point heat maps, and descriptors for key point locations.



### IPG-Net: Image Pyramid Guidance Network for Small Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.00632v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00632v3)
- **Published**: 2019-12-02 08:47:12+00:00
- **Updated**: 2020-05-07 09:15:27+00:00
- **Authors**: Ziming Liu, Guangyu Gao, Lin Sun, Li Fang
- **Comment**: Accepted by CVPR2020 Anti-UVA workshop
- **Journal**: None
- **Summary**: For Convolutional Neural Network-based object detection, there is a typical dilemma: the spatial information is well kept in the shallow layers which unfortunately do not have enough semantic information, while the deep layers have a high semantic concept but lost a lot of spatial information, resulting in serious information imbalance. To acquire enough semantic information for shallow layers, Feature Pyramid Networks (FPN) is used to build a top-down propagated path. In this paper, except for top-down combining of information for shallow layers, we propose a novel network called Image Pyramid Guidance Network (IPG-Net) to make sure both the spatial information and semantic information are abundant for each layer. Our IPG-Net has two main parts: the image pyramid guidance transformation module and the image pyramid guidance fusion module. Our main idea is to introduce the image pyramid guidance into the backbone stream to solve the information imbalance problem, which alleviates the vanishment of the small object features. This IPG transformation module promises even in the deepest stage of the backbone, there is enough spatial information for bounding box regression and classification. Furthermore, we designed an effective fusion module to fuse the features from the image pyramid and features from the backbone stream. We have tried to apply this novel network to both one-stage and two-stage detection models, state of the art results are obtained on the most popular benchmark data sets, i.e. MS COCO and Pascal VOC.



### An Attention-Based Speaker Naming Method for Online Adaptation in Non-Fixed Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1912.00649v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.00649v1)
- **Published**: 2019-12-02 09:30:27+00:00
- **Updated**: 2019-12-02 09:30:27+00:00
- **Authors**: Jungwoo Pyo, Joohyun Lee, Youngjune Park, Tien-Cuong Bui, Sang Kyun Cha
- **Comment**: AAAI 2020 Workshop on Interactive and Conversational Recommendation
  Systems(WICRS)
- **Journal**: None
- **Summary**: A speaker naming task, which finds and identifies the active speaker in a certain movie or drama scene, is crucial for dealing with high-level video analysis applications such as automatic subtitle labeling and video summarization. Modern approaches have usually exploited biometric features with a gradient-based method instead of rule-based algorithms. In a certain situation, however, a naive gradient-based method does not work efficiently. For example, when new characters are added to the target identification list, the neural network needs to be frequently retrained to identify new people and it causes delays in model preparation. In this paper, we present an attention-based method which reduces the model setup time by updating the newly added data via online adaptation without a gradient update process. We comparatively analyzed with three evaluation metrics(accuracy, memory usage, setup time) of the attention-based method and existing gradient-based methods under various controlled settings of speaker naming. Also, we applied existing speaker naming models and the attention-based model to real video to prove that our approach shows comparable accuracy to the existing state-of-the-art models and even higher accuracy in some cases.



### DAL -- A Deep Depth-aware Long-term Tracker
- **Arxiv ID**: http://arxiv.org/abs/1912.00660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00660v1)
- **Published**: 2019-12-02 10:02:40+00:00
- **Updated**: 2019-12-02 10:02:40+00:00
- **Authors**: Yanlin Qian, Alan Lukežič, Matej Kristan, Joni-Kristian Kämäräinen, Jiri Matas
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The best RGBD trackers provide high accuracy but are slow to run. On the other hand, the best RGB trackers are fast but clearly inferior on the RGBD datasets. In this work, we propose a deep depth-aware long-term tracker that achieves state-of-the-art RGBD tracking performance and is fast to run. We reformulate deep discriminative correlation filter (DCF) to embed the depth information into deep features. Moreover, the same depth-aware correlation filter is used for target re-detection. Comprehensive evaluations show that the proposed tracker achieves state-of-the-art performance on the Princeton RGBD, STC, and the newly-released CDTB benchmarks and runs 20 fps.



### Training the Convolutional Neural Network with Statistical Dependence of the Response on the Input Data Distortion
- **Arxiv ID**: http://arxiv.org/abs/1912.00664v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00664v1)
- **Published**: 2019-12-02 10:09:21+00:00
- **Updated**: 2019-12-02 10:09:21+00:00
- **Authors**: Igor Janiszewski, Dmitry Slugin, Vladimir V. Arlazarov
- **Comment**: Submitted and presented at The 12th International Conference on
  Machine Vision (ICMV 2019). 8 pages, 7 figures, 14 references
- **Journal**: None
- **Summary**: The paper proposes an approach to training a convolutional neural network using information on the level of distortion of input data. The learning process is modified with an additional layer, which is subsequently deleted, so the architecture of the original network does not change. As an example, the LeNet5 architecture network with training data based on the MNIST symbols and a distortion model as Gaussian blur with a variable level of distortion is considered. This approach does not have quality loss of the network and has a significant error-free zone in responses on the test data which is absent in the traditional approach to training. The responses are statistically dependent on the level of input image's distortions and there is a presence of a strong relationship between them.



### Efficient Convolutional Neural Networks for Depth-Based Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.00711v1
- **DOI**: 10.1109/TCSVT.2019.2952779
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00711v1)
- **Published**: 2019-12-02 12:18:31+00:00
- **Updated**: 2019-12-02 12:18:31+00:00
- **Authors**: Angel Martínez-González, Michael Villamizar, Olivier Canévet, Jean-Marc Odobez
- **Comment**: Published in IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO
  TECHNOLOGY
- **Journal**: None
- **Summary**: Achieving robust multi-person 2D body landmark localization and pose estimation is essential for human behavior and interaction understanding as encountered for instance in HRI settings. Accurate methods have been proposed recently, but they usually rely on rather deep Convolutional Neural Network (CNN) architecture, thus requiring large computational and training resources. In this paper, we investigate different architectures and methodologies to address these issues and achieve fast and accurate multi-person 2D pose estimation. To foster speed, we propose to work with depth images, whose structure contains sufficient information about body landmarks while being simpler than textured color images and thus potentially requiring less complex CNNs for processing. In this context, we make the following contributions. i) we study several CNN architecture designs combining pose machines relying on the cascade of detectors concept with lightweight and efficient CNN structures; ii) to address the need for large training datasets with high variability, we rely on semi-synthetic data combining multi-person synthetic depth data with real sensor backgrounds; iii) we explore domain adaptation techniques to address the performance gap introduced by testing on real depth images; iv) to increase the accuracy of our fast lightweight CNN models, we investigate knowledge distillation at several architecture levels which effectively enhance performance. Experiments and results on synthetic and real data highlight the impact of our design choices, providing insights into methods addressing standard issues normally faced in practical applications, and resulting in architectures effectively matching our goal in both performance and speed.



### Learning scale-variant features for robust iris authentication with deep learning based ensemble framework
- **Arxiv ID**: http://arxiv.org/abs/1912.00756v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00756v2)
- **Published**: 2019-12-02 13:25:05+00:00
- **Updated**: 2020-06-13 15:27:43+00:00
- **Authors**: Siming Zheng, Rahmita Wirza O. K. Rahmat, Fatimah Khalid, Nurul Amelina Nasharuddin
- **Comment**: This is the second revision for updating the formation, logical and
  image captions
- **Journal**: None
- **Summary**: In recent years, mobile Internet has accelerated the proliferation of smart mobile development. The mobile payment, mobile security and privacy protection have become the focus of widespread attention. Iris recognition becomes a high-security authentication technology in these fields, it is widely used in distinct science fields in biometric authentication fields. The Convolutional Neural Network (CNN) is one of the mainstream deep learning approaches for image recognition, whereas its anti-noise ability is weak and needs a certain amount of memory to train in image classification tasks. Under these conditions we put forward a fine-tuning neural network model based on the Mask R-CNN and Inception V4 neural network model, which integrates every component in an overall system that combines the iris detection, extraction, and recognition function as an iris recognition system. The proposed framework has the characteristics of scalability and high availability; it not only can learn part-whole relationships of the iris image but also enhancing the robustness of the whole framework. Importantly, the proposed model can be trained using the different spectrum of samples, such as Visible Wavelength (VW) and Near Infrared (NIR) iris biometric databases. The recognition average accuracy of 99.10% is achieved while executing in the mobile edge calculation device of the Jetson Nano.



### Mixture Dense Regression for Object Detection and Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.00821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00821v2)
- **Published**: 2019-12-02 14:31:52+00:00
- **Updated**: 2020-04-20 01:20:13+00:00
- **Authors**: Ali Varamesh, Tinne Tuytelaars
- **Comment**: None
- **Journal**: None
- **Summary**: Mixture models are well-established learning approaches that, in computer vision, have mostly been applied to inverse or ill-defined problems. However, they are general-purpose divide-and-conquer techniques, splitting the input space into relatively homogeneous subsets in a data-driven manner. Not only ill-defined but also well-defined complex problems should benefit from them. To this end, we devise a framework for spatial regression using mixture density networks. We realize the framework for object detection and human pose estimation. For both tasks, a mixture model yields higher accuracy and divides the input space into interpretable modes. For object detection, mixture components focus on object scale, with the distribution of components closely following that of ground truth the object scale. This practically alleviates the need for multi-scale testing, providing a superior speed-accuracy trade-off. For human pose estimation, a mixture model divides the data based on viewpoint and uncertainty -- namely, front and back views, with back view imposing higher uncertainty. We conduct experiments on the MS COCO dataset and do not face any mode collapse.



### Improving Model Drift for Robust Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.00826v1
- **DOI**: 10.1007/s11042-020-09032-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00826v1)
- **Published**: 2019-12-02 14:41:51+00:00
- **Updated**: 2019-12-02 14:41:51+00:00
- **Authors**: Qiujie Dong, Xuedong He, Haiyan Ge, Qin Liu, Aifu Han, Shengzong Zhou
- **Comment**: 7 pages, 6 figures, 4 tables
- **Journal**: Multimedia Tools and Applications. 79 (2020) 25801-25815
- **Summary**: Discriminative correlation filters show excellent performance in object tracking. However, in complex scenes, the apparent characteristics of the tracked target are variable, which makes it easy to pollute the model and cause the model drift. In this paper, considering that the secondary peak has a greater impact on the model update, we propose a method for detecting the primary and secondary peaks of the response map. Secondly, a novel confidence function which uses the adaptive update discriminant mechanism is proposed, which yield good robustness. Thirdly, we propose a robust tracker with correlation filters, which uses hand-crafted features and can improve model drift in complex scenes. Finally, in order to cope with the current trackers' multi-feature response merge, we propose a simple exponential adaptive merge approach. Extensive experiments are performed on OTB2013, OTB100 and TC128 datasets. Our approach performs superiorly against several state-of-the-art trackers while runs at speed in real time.



### Information bottleneck through variational glasses
- **Arxiv ID**: http://arxiv.org/abs/1912.00830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00830v2)
- **Published**: 2019-12-02 14:52:29+00:00
- **Updated**: 2019-12-05 17:15:52+00:00
- **Authors**: Slava Voloshynovskiy, Mouad Kondah, Shideh Rezaeifar, Olga Taran, Taras Holotyak, Danilo Jimenez Rezende
- **Comment**: None
- **Journal**: None
- **Summary**: Information bottleneck (IB) principle [1] has become an important element in information-theoretic analysis of deep models. Many state-of-the-art generative models of both Variational Autoencoder (VAE) [2; 3] and Generative Adversarial Networks (GAN) [4] families use various bounds on mutual information terms to introduce certain regularization constraints [5; 6; 7; 8; 9; 10]. Accordingly, the main difference between these models consists in add regularization constraints and targeted objectives.   In this work, we will consider the IB framework for three classes of models that include supervised, unsupervised and adversarial generative models. We will apply a variational decomposition leading a common structure and allowing easily establish connections between these models and analyze underlying assumptions.   Based on these results, we focus our analysis on unsupervised setup and reconsider the VAE family. In particular, we present a new interpretation of VAE family based on the IB framework using a direct decomposition of mutual information terms and show some interesting connections to existing methods such as VAE [2; 3], beta-VAE [11], AAE [12], InfoVAE [5] and VAE/GAN [13]. Instead of adding regularization constraints to an evidence lower bound (ELBO) [2; 3], which itself is a lower bound, we show that many known methods can be considered as a product of variational decomposition of mutual information terms in the IB framework. The proposed decomposition might also contribute to the interpretability of generative models of both VAE and GAN families and create a new insights to a generative compression [14; 15; 16; 17]. It can also be of interest for the analysis of novelty detection based on one-class classifiers [18] with the IB based discriminators.



### Efficient Relaxed Gradient Support Pursuit for Sparsity Constrained Non-convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1912.00858v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00858v1)
- **Published**: 2019-12-02 15:25:31+00:00
- **Updated**: 2019-12-02 15:25:31+00:00
- **Authors**: Fanhua Shang, Bingkun Wei, Hongying Liu, Yuanyuan Liu, Jiacheng Zhuo
- **Comment**: 7 pages, 3 figures, Appeared at the Data Science Meets Optimization
  Workshop (DSO) at IJCAI'19
- **Journal**: None
- **Summary**: Large-scale non-convex sparsity-constrained problems have recently gained extensive attention. Most existing deterministic optimization methods (e.g., GraSP) are not suitable for large-scale and high-dimensional problems, and thus stochastic optimization methods with hard thresholding (e.g., SVRGHT) become more attractive. Inspired by GraSP, this paper proposes a new general relaxed gradient support pursuit (RGraSP) framework, in which the sub-algorithm only requires to satisfy a slack descent condition. We also design two specific semi-stochastic gradient hard thresholding algorithms. In particular, our algorithms have much less hard thresholding operations than SVRGHT, and their average per-iteration cost is much lower (i.e., O(d) vs. O(d log(d)) for SVRGHT), which leads to faster convergence. Our experimental results on both synthetic and real-world datasets show that our algorithms are superior to the state-of-the-art gradient hard thresholding methods.



### More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1912.00869v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00869v1)
- **Published**: 2019-12-02 15:35:31+00:00
- **Updated**: 2019-12-02 15:35:31+00:00
- **Authors**: Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, David Cox
- **Comment**: Accepted at NeurIPS 2019, codes and models are available at
  https://github.com/IBM/bLVNet-TAM
- **Journal**: Advances in Neural Information Processing Systems (Neurips 2019)
- **Summary**: Current state-of-the-art models for video action recognition are mostly based on expensive 3D ConvNets. This results in a need for large GPU clusters to train and evaluate such architectures. To address this problem, we present a lightweight and memory-friendly architecture for action recognition that performs on par with or better than current architectures by using only a fraction of resources. The proposed architecture is based on a combination of a deep subnet operating on low-resolution frames with a compact subnet operating on high-resolution frames, allowing for high efficiency and accuracy at the same time. We demonstrate that our approach achieves a reduction by $3\sim4$ times in FLOPs and $\sim2$ times in memory usage compared to the baseline. This enables training deeper models with more input frames under the same computational budget. To further obviate the need for large-scale 3D convolutions, a temporal aggregation module is proposed to model temporal dependencies in a video at very small additional computational costs. Our models achieve strong performance on several action recognition benchmarks including Kinetics, Something-Something and Moments-in-time. The code and models are available at https://github.com/IBM/bLVNet-TAM.



### Detecting mechanical loosening of total hip replacement implant from plain radiograph using deep convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1912.00943v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00943v2)
- **Published**: 2019-12-02 17:15:58+00:00
- **Updated**: 2022-07-05 16:09:33+00:00
- **Authors**: Alireza Borjali, Antonia F. Chen, Orhun K. Muratoglu, Mohammad A. Morid, Kartik M. Varadarajan
- **Comment**: None
- **Journal**: None
- **Summary**: Plain radiography is widely used to detect mechanical loosening of total hip replacement (THR) implants. Currently, radiographs are assessed manually by medical professionals, which may be prone to poor inter and intra observer reliability and low accuracy. Furthermore, manual detection of mechanical loosening of THR implants requires experienced clinicians who might not always be readily available, potentially resulting in delayed diagnosis. In this study, we present a novel, fully automatic and interpretable approach to detect mechanical loosening of THR implants from plain radiographs using deep convolutional neural network (CNN). We trained a CNN on 40 patients anteroposterior hip x rays using five fold cross validation and compared its performance with a high volume board certified orthopaedic surgeon (AFC). To increase the confidence in the machine outcome, we also implemented saliency maps to visualize where the CNN looked at to make a diagnosis. CNN outperformed the orthopaedic surgeon in diagnosing mechanical loosening of THR implants achieving significantly higher sensitively (0.94) than the orthopaedic surgeon (0.53) with the same specificity (0.96). The saliency maps showed that the CNN looked at clinically relevant features to make a diagnosis. Such CNNs can be used for automatic radiologic assessment of mechanical loosening of THR implants to supplement the practitioners decision making process, increasing their diagnostic accuracy, and freeing them to engage in more patient centric care.



### Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic Chemistry Simulation
- **Arxiv ID**: http://arxiv.org/abs/1912.00951v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00951v1)
- **Published**: 2019-12-02 17:24:10+00:00
- **Updated**: 2019-12-02 17:24:10+00:00
- **Authors**: Sumeet Batra, John Klingner, Nikolaus Correll
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to register individual members of a robotic swarm in an augmented reality display while showing relevant information about swarm dynamics to the user that would be otherwise hidden. Individual swarm members and clusters of the same group are identified by their color, and by blinking at a specific time interval that is distinct from the time interval at which their neighbors blink. We show that this problem is an instance of the graph coloring problem, which can be solved in a distributed manner in O(log(n)) time. We demonstrate our approach using a swarm chemistry simulation in which robots simulate individual atoms that form molecules following the rules of chemistry. Augmented reality is then used to display information about the internal state of individual swarm members as well as their topological relationship, corresponding to molecular bonds.



### Applying Knowledge Transfer for Water Body Segmentation in Peru
- **Arxiv ID**: http://arxiv.org/abs/1912.00957v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00957v1)
- **Published**: 2019-12-02 17:41:20+00:00
- **Updated**: 2019-12-02 17:41:20+00:00
- **Authors**: Jessenia Gonzalez, Debjani Bhowmick, Cesar Beltran, Kris Sankaran, Yoshua Bengio
- **Comment**: 5 pages, 3 figures, 1 table, NeurIPS 2019 Workshop on Machine
  Learning for the Developing World
- **Journal**: None
- **Summary**: In this work, we present the application of convolutional neural networks for segmenting water bodies in satellite images. We first use a variant of the U-Net model to segment rivers and lakes from very high-resolution images from Peru. To circumvent the issue of scarce labelled data, we investigate the applicability of a knowledge transfer-based model that learns the mapping from high-resolution labelled images and combines it with the very high-resolution mapping so that better segmentation can be achieved. We train this model in a single process, end-to-end. Our preliminary results show that adding the information from the available high-resolution images does not help out-of-the-box, and in fact worsen results. This leads us to infer that the high-resolution data could be from a different distribution, and its addition leads to increased variance in our results.



### IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation Aerial Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.00969v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00969v2)
- **Published**: 2019-12-02 18:02:30+00:00
- **Updated**: 2021-03-29 03:58:08+00:00
- **Authors**: Youtian Lin, Pengming Feng, Jian Guan, Wenwu Wang, Jonathon Chambers
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in aerial images is a challenging task due to the lack of visible features and variant orientation of objects. Significant progress has been made recently for predicting targets from aerial images with horizontal bounding boxes (HBBs) and oriented bounding boxes (OBBs) using two-stage detectors with region based convolutional neural networks (R-CNN), involving object localization in one stage and object classification in the other. However, the computational complexity in two-stage detectors is often high, especially for orientational object detection, due to anchor matching and using regions of interest (RoI) pooling for feature extraction. In this paper, we propose a one-stage anchor free detector for orientational object detection, namely, an interactive embranchment network (IENet), which is built upon a detector with prediction in per-pixel fashion. First, a novel geometric transformation is employed to better represent the oriented object in angle prediction, then a branch interactive module with a self-attention mechanism is developed to fuse features from classification and box regression branches. Finally, we introduce an enhanced intersection over union (IoU) loss for OBB detection, which is computationally more efficient than regular polygon IoU. Experiments conducted demonstrate the effectiveness and the superiority of our proposed method, as compared with state-of-the-art detectors.



### KernelNet: A Data-Dependent Kernel Parameterization for Deep Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/1912.00979v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00979v3)
- **Published**: 2019-12-02 18:15:43+00:00
- **Updated**: 2020-06-25 02:50:37+00:00
- **Authors**: Yufan Zhou, Changyou Chen, Jinhui Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning with kernels is an important concept in machine learning. Standard approaches for kernel methods often use predefined kernels that require careful selection of hyperparameters. To mitigate this burden, we propose in this paper a framework to construct and learn a data-dependent kernel based on random features and implicit spectral distributions that are parameterized by deep neural networks. The constructed network (called KernelNet) can be applied to deep generative modeling in various scenarios, including two popular learning paradigms in deep generative models, MMD-GAN and implicit Variational Autoencoder (VAE). We show that our proposed kernel indeed exists in applications and is guaranteed to be positive definite. Furthermore, the induced Maximum Mean Discrepancy (MMD) can endow the continuity property in weak topology by simple regularization. Extensive experiments indicate that our proposed KernelNet consistently achieves better performance compared to related methods.



### Adversarial normalization for multi domain image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.00993v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00993v2)
- **Published**: 2019-12-02 18:52:45+00:00
- **Updated**: 2020-02-01 18:43:05+00:00
- **Authors**: Pierre-Luc Delisle, Benoit Anctil-Robitaille, Christian Desrosiers, Herve Lombaert
- **Comment**: Submitted to ISBI 2020
- **Journal**: None
- **Summary**: Image normalization is a critical step in medical imaging. This step is often done on a per-dataset basis, preventing current segmentation algorithms from the full potential of exploiting jointly normalized information across multiple datasets. To solve this problem, we propose an adversarial normalization approach for image segmentation which learns common normalizing functions across multiple datasets while retaining image realism. The adversarial training provides an optimal normalizer that improves both the segmentation accuracy and the discrimination of unrealistic normalizing functions. Our contribution therefore leverages common imaging information from multiple domains. The optimality of our common normalizer is evaluated by combining brain images from both infants and adults. Results on the challenging iSEG and MRBrainS datasets reveal the potential of our adversarial normalization approach for segmentation, with Dice improvements of up to 59.6% over the baseline.



### A Multigrid Method for Efficiently Training Video Models
- **Arxiv ID**: http://arxiv.org/abs/1912.00998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00998v2)
- **Published**: 2019-12-02 18:59:13+00:00
- **Updated**: 2020-06-10 03:05:26+00:00
- **Authors**: Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Krähenbühl
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training assumes a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but they are inaccurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, non-local, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5x faster (wall-clock time, same hardware) while also improving accuracy (+0.8% absolute) on Kinetics-400 compared to the baseline training method. Code is available online.



### View-Invariant Probabilistic Embedding for Human Pose
- **Arxiv ID**: http://arxiv.org/abs/1912.01001v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01001v4)
- **Published**: 2019-12-02 18:59:56+00:00
- **Updated**: 2020-10-22 19:52:08+00:00
- **Authors**: Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Ting Liu
- **Comment**: Accepted to ECCV 2020 (Spotlight presentation). Code is available at
  https://github.com/google-research/google-research/tree/master/poem . Video
  synchronization results are available at
  https://drive.google.com/corp/drive/folders/1kTc_UT0Eq0H2ZBgfEoh8qEJMFBouC-Wv
- **Journal**: None
- **Summary**: Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at https://github.com/google-research/google-research/tree/master/poem.



### The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 Challenge
- **Arxiv ID**: http://arxiv.org/abs/1912.01054v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01054v2)
- **Published**: 2019-12-02 19:32:53+00:00
- **Updated**: 2020-08-07 19:38:51+00:00
- **Authors**: Nicholas Heller, Fabian Isensee, Klaus H. Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin, Miofei Han, Guang Yao, Yaozong Gao, Yao Zhang, Yixin Wang, Feng Hou, Jiawei Yang, Guangwei Xiong, Jiang Tian, Cheng Zhong, Jun Ma, Jack Rickman, Joshua Dean, Bethany Stai, Resha Tejpaul, Makinna Oestreich, Paul Blake, Heather Kaluzniak, Shaneabbas Raza, Joel Rosenberg, Keenan Moore, Edward Walczak, Zachary Rengel, Zach Edgerton, Ranveer Vasdev, Matthew Peterson, Sean McSweeney, Sarah Peterson, Arveen Kalapara, Niranjan Sathianathen, Nikolaos Papanikolopoulos, Christopher Weight
- **Comment**: 24 pages, 11 figures
- **Journal**: None
- **Summary**: There is a large body of literature linking anatomic and geometric characteristics of kidney tumors to perioperative and oncologic outcomes. Semantic segmentation of these tumors and their host kidneys is a promising tool for quantitatively characterizing these lesions, but its adoption is limited due to the manual effort required to produce high-quality 3D segmentations of these structures. Recently, methods based on deep learning have shown excellent results in automatic 3D segmentation, but they require large datasets for training, and there remains little consensus on which methods perform best. The 2019 Kidney and Kidney Tumor Segmentation challenge (KiTS19) was a competition held in conjunction with the 2019 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) which sought to address these issues and stimulate progress on this automatic segmentation problem. A training set of 210 cross sectional CT images with kidney tumors was publicly released with corresponding semantic segmentation masks. 106 teams from five continents used this data to develop automated systems to predict the true segmentation masks on a test set of 90 CT images for which the corresponding ground truth segmentations were kept private. These predictions were scored and ranked according to their average So rensen-Dice coefficient between the kidney and tumor across all 90 cases. The winning team achieved a Dice of 0.974 for kidney and 0.851 for tumor, approaching the inter-annotator performance on kidney (0.983) but falling short on tumor (0.923). This challenge has now entered an "open leaderboard" phase where it serves as a challenging benchmark in 3D semantic segmentation.



### GGNN: Graph-based GPU Nearest Neighbor Search
- **Arxiv ID**: http://arxiv.org/abs/1912.01059v4
- **DOI**: 10.1109/TBDATA.2022.3161156
- **Categories**: **cs.CV**, cs.DB, cs.DS, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1912.01059v4)
- **Published**: 2019-12-02 19:46:13+00:00
- **Updated**: 2022-04-07 14:49:40+00:00
- **Authors**: Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT, FAISS, and SONG started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today's state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed.



### A Keyframe-based Continuous Visual SLAM for RGB-D Cameras via Nonparametric Joint Geometric and Appearance Representation
- **Arxiv ID**: http://arxiv.org/abs/1912.01064v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01064v1)
- **Published**: 2019-12-02 20:10:21+00:00
- **Updated**: 2019-12-02 20:10:21+00:00
- **Authors**: Xi Lin, Dingyi Sun, Tzu-Yuan Lin, Ryan M. Eustice, Maani Ghaffari
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: This paper reports on a robust RGB-D SLAM system that performs well in scarcely textured and structured environments. We present a novel keyframe-based continuous visual odometry that builds on the recently developed continuous sensor registration framework. A joint geometric and appearance representation is the result of transforming the RGB-D images into functions that live in a Reproducing Kernel Hilbert Space (RKHS). We solve both registration and keyframe selection problems via the inner product structure available in the RKHS. We also extend the proposed keyframe-based odometry method to a SLAM system using indirect ORB loop-closure constraints. The experimental evaluations using publicly available RGB-D benchmarks show that the developed keyframe selection technique using continuous visual odometry outperforms its robust dense (and direct) visual odometry equivalent. In addition, the developed SLAM system has better generalization across different training and validation sequences; it is robust to the lack of texture and structure in the scene; and shows comparable performance with the state-of-the-art SLAM systems.



### A Bayesian Inference Framework for Procedural Material Parameter Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.01067v5
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01067v5)
- **Published**: 2019-12-02 20:19:07+00:00
- **Updated**: 2020-11-04 01:23:13+00:00
- **Authors**: Yu Guo, Milos Hasan, Lingqi Yan, Shuang Zhao
- **Comment**: 12 pages, 13 figures, Pacific Graphics 2020
- **Journal**: None
- **Summary**: Procedural material models have been gaining traction in many applications thanks to their flexibility, compactness, and easy editability. We explore the inverse rendering problem of procedural material parameter estimation from photographs, presenting a unified view of the problem in a Bayesian framework. In addition to computing point estimates of the parameters by optimization, our framework uses a Markov Chain Monte Carlo approach to sample the space of plausible material parameters, providing a collection of plausible matches that a user can choose from, and efficiently handling both discrete and continuous model parameters. To demonstrate the effectiveness of our framework, we fit procedural models of a range of materials---wall plaster, leather, wood, anisotropic brushed metals and layered metallic paints---to both synthetic and real target images.



### Latent Replay for Real-Time Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.01100v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01100v2)
- **Published**: 2019-12-02 22:16:32+00:00
- **Updated**: 2020-03-04 09:50:32+00:00
- **Authors**: Lorenzo Pellegrini, Gabriele Graffieti, Vincenzo Lomonaco, Davide Maltoni
- **Comment**: Pre-print v3: 13 pages, 9 figures, 10 tables, 1 algorithm
- **Journal**: None
- **Summary**: Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named "Latent Replay" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.



### Offset Sampling Improves Deep Learning based Accelerated MRI Reconstructions by Exploiting Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1912.01101v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01101v2)
- **Published**: 2019-12-02 22:29:26+00:00
- **Updated**: 2020-02-04 17:06:58+00:00
- **Authors**: Aaron Defazio
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning approaches to accelerated MRI take a matrix of sampled Fourier-space lines as input and produce a spatial image as output. In this work we show that by careful choice of the offset used in the sampling procedure, the symmetries in k-space can be better exploited, producing higher quality reconstructions than given by standard equally-spaced samples or randomized samples motivated by compressed sensing.



### MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1912.01106v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01106v2)
- **Published**: 2019-12-02 22:42:43+00:00
- **Updated**: 2020-07-30 18:22:02+00:00
- **Authors**: Bo Chen, Golnaz Ghiasi, Hanxiao Liu, Tsung-Yi Lin, Dmitry Kalenichenko, Hartwig Adams, Quoc V. Le
- **Comment**: 10 pages, 7 figures
- **Journal**: None
- **Summary**: Despite the blooming success of architecture search for vision tasks in resource-constrained environments, the design of on-device object detection architectures have mostly been manual. The few automated search efforts are either centered around non-mobile-friendly search spaces or not guided by on-device latency. We propose MnasFPN, a mobile-friendly search space for the detection head, and combine it with latency-aware architecture search to produce efficient object detection models. The learned MnasFPN head, when paired with MobileNetV2 body, outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel. It is also both 1.0 mAP more accurate and 10% faster than NAS-FPNLite. Ablation studies show that the majority of the performance gain comes from innovations in the search space. Further explorations reveal an interesting coupling between the search space design and the search algorithm, and that the complexity of MnasFPN search space may be at a local optimum.



### Automated speech-based screening of depression using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1912.01115v1
- **DOI**: 10.1016/j.procs.2019.12.228
- **Categories**: **cs.LG**, cs.CV, cs.CY, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01115v1)
- **Published**: 2019-12-02 22:58:40+00:00
- **Updated**: 2019-12-02 22:58:40+00:00
- **Authors**: Karol Chlasta, Krzysztof Wołk, Izabela Krejtz
- **Comment**: 10 pages, 8 figures and 2 tables, HCist 2019 - 8th International
  Conference on Health and Social Care Information Systems and Technologies
  (16-18 October 2019, Sousse, Tunisia)
- **Journal**: Procedia Computer Science 164 (2019) 618-628
- **Summary**: Early detection and treatment of depression is essential in promoting remission, preventing relapse, and reducing the emotional burden of the disease. Current diagnoses are primarily subjective, inconsistent across professionals, and expensive for individuals who may be in urgent need of help. This paper proposes a novel approach to automated depression detection in speech using convolutional neural network (CNN) and multipart interactive training. The model was tested using 2568 voice samples obtained from 77 non-depressed and 30 depressed individuals. In experiment conducted, data were applied to residual CNNs in the form of spectrograms, images auto-generated from audio samples. The experimental results obtained using different ResNet architectures gave a promising baseline accuracy reaching 77%.



### Deep Bayesian Active Learning for Multiple Correct Outputs
- **Arxiv ID**: http://arxiv.org/abs/1912.01119v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1912.01119v2)
- **Published**: 2019-12-02 23:09:16+00:00
- **Updated**: 2019-12-08 06:36:35+00:00
- **Authors**: Khaled Jedoui, Ranjay Krishna, Michael Bernstein, Li Fei-Fei
- **Comment**: 18 pages, 9 figures
- **Journal**: None
- **Summary**: Typical active learning strategies are designed for tasks, such as classification, with the assumption that the output space is mutually exclusive. The assumption that these tasks always have exactly one correct answer has resulted in the creation of numerous uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. Unfortunately, many real-world vision tasks, like visual question answering and image captioning, have multiple correct answers, causing these measurements to overestimate uncertainty and sometimes perform worse than a random sampling baseline. In this paper, we propose a new paradigm that estimates uncertainty in the model's internal hidden space instead of the model's output space. We specifically study a manifestation of this problem for visual question answer generation (VQA), where the aim is not to classify the correct answer but to produce a natural language answer, given an image and a question. Our method overcomes the paraphrastic nature of language. It requires a semantic space that structures the model's output concepts and that enables the usage of techniques like dropout-based Bayesian uncertainty. We build a visual-semantic space that embeds paraphrases close together for any existing VQA model. We empirically show state-of-art active learning results on the task of VQA on two datasets, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0.



### BERT for Large-scale Video Segment Classification with Test-time Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.01127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01127v1)
- **Published**: 2019-12-02 23:26:43+00:00
- **Updated**: 2019-12-02 23:26:43+00:00
- **Authors**: Tianqi Liu, Qizhan Shao
- **Comment**: ICCV 2019 YouTube8M workshop
- **Journal**: ICCV 2019
- **Summary**: This paper presents our approach to the third YouTube-8M video understanding competition that challenges par-ticipants to localize video-level labels at scale to the pre-cise time in the video where the label actually occurs. Ourmodel is an ensemble of frame-level models such as GatedNetVLAD and NeXtVLAD and various BERT models withtest-time augmentation. We explore multiple ways to ag-gregate BERT outputs as video representation and variousways to combine visual and audio information. We proposetest-time augmentation as shifting video frames to one leftor right unit, which adds variety to the predictions and em-pirically shows improvement in evaluation metrics. We firstpre-train the model on the 4M training video-level data, andthen fine-tune the model on 237K annotated video segment-level data. We achieve MAP@100K 0.7871 on private test-ing video segment data, which is ranked 9th over 283 teams.



