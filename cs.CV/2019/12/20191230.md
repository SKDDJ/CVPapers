# Arxiv Papers in cs.CV on 2019-12-30
### Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots
- **Arxiv ID**: http://arxiv.org/abs/1912.12791v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12791v3)
- **Published**: 2019-12-30 03:02:22+00:00
- **Updated**: 2020-10-13 05:04:42+00:00
- **Authors**: Qi Chen, Lin Sun, Zhixin Wang, Kui Jia, Alan Yuille
- **Comment**: small affiliation fix
- **Journal**: None
- **Summary**: Accurate 3D object detection in LiDAR based point clouds suffers from the challenges of data sparsity and irregularities. Existing methods strive to organize the points regularly, e.g. voxelize, pass them through a designed 2D/3D neural network, and then define object-level anchors that predict offsets of 3D bounding boxes using collective evidences from all the points on the objects of interest. Contrary to the state-of-the-art anchor-based methods, based on the very nature of data sparsity, we observe that even points on an individual object part are informative about semantic information of the object. We thus argue in this paper for an approach opposite to existing methods using object-level anchors. Inspired by compositional models, which represent an object as parts and their spatial relations, we propose to represent an object as composition of its interior non-empty voxels, termed hotspots, and the spatial relations of hotspots. This gives rise to the representation of Object as Hotspots (OHS). Based on OHS, we further propose an anchor-free detection head with a novel ground truth assignment strategy that deals with inter-object point-sparsity imbalance to prevent the network from biasing towards objects with more points. Experimental results show that our proposed method works remarkably well on objects with a small number of points. Notably, our approach ranked 1st on KITTI 3D Detection Benchmark for cyclist and pedestrian detection, and achieved state-of-the-art performance on NuScenes 3D Detection Benchmark.



### PMC-GANs: Generating Multi-Scale High-Quality Pedestrian with Multimodal Cascaded GANs
- **Arxiv ID**: http://arxiv.org/abs/1912.12799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12799v1)
- **Published**: 2019-12-30 03:30:51+00:00
- **Updated**: 2019-12-30 03:30:51+00:00
- **Authors**: Jie Wu, Ying Peng, Chenghao Zheng, Zongbo Hao, Jian Zhang
- **Comment**: Accepted by The British Machine Vision Conference (BMVC2019)
- **Journal**: None
- **Summary**: Recently, generative adversarial networks (GANs) have shown great advantages in synthesizing images, leading to a boost of explorations of using faked images to augment data. This paper proposes a multimodal cascaded generative adversarial networks (PMC-GANs) to generate realistic and diversified pedestrian images and augment pedestrian detection data. The generator of our model applies a residual U-net structure, with multi-scale residual blocks to encode features, and attention residual blocks to help decode and rebuild pedestrian images. The model constructs in a coarse-to-fine fashion and adopts cascade structure, which is beneficial to produce high-resolution pedestrians. PMC-GANs outperforms baselines, and when used for data augmentation, it improves pedestrian detection results.



### Rethinking Convolutional Features in Correlation Filter Based Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.12811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12811v1)
- **Published**: 2019-12-30 04:39:38+00:00
- **Updated**: 2019-12-30 04:39:38+00:00
- **Authors**: Fang Liang, Wenjun Peng, Qinghao Liu, Haijin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Both accuracy and efficiency are of significant importance to the task of visual object tracking. In recent years, as the surge of deep learning, Deep Convolutional NeuralNetwork (DCNN) becomes a very popular choice among the tracking community. However, due to the high computational complexity, end-to-end visual object trackers can hardly achieve an acceptable inference time and therefore can difficult to be utilized in many real-world applications. In this paper, we revisit a hierarchical deep feature-based visual tracker and found that both the performance and efficiency of the deep tracker are limited by the poor feature quality. Therefore, we propose a feature selection module to select more discriminative features for the trackers. After removing redundant features, our proposed tracker achieves significant improvements in both performance and efficiency. Finally, comparisons with state-of-the-art trackers are provided.



### RC-DARTS: Resource Constrained Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.12814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.12814v1)
- **Published**: 2019-12-30 05:02:38+00:00
- **Updated**: 2019-12-30 05:02:38+00:00
- **Authors**: Xiaojie Jin, Jiang Wang, Joshua Slocum, Ming-Hsuan Yang, Shengyang Dai, Shuicheng Yan, Jiashi Feng
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Recent advances show that Neural Architectural Search (NAS) method is able to find state-of-the-art image classification deep architectures. In this paper, we consider the one-shot NAS problem for resource constrained applications. This problem is of great interest because it is critical to choose different architectures according to task complexity when the resource is constrained. Previous techniques are either too slow for one-shot learning or does not take the resource constraint into consideration. In this paper, we propose the resource constrained differentiable architecture search (RC-DARTS) method to learn architectures that are significantly smaller and faster while achieving comparable accuracy. Specifically, we propose to formulate the RC-DARTS task as a constrained optimization problem by adding the resource constraint. An iterative projection method is proposed to solve the given constrained optimization problem. We also propose a multi-level search strategy to enable layers at different depths to adaptively learn different types of neural architectures. Through extensive experiments on the Cifar10 and ImageNet datasets, we show that the RC-DARTS method learns lightweight neural architectures which have smaller model size and lower computational complexity while achieving comparable or better performances than the state-of-the-art methods.



### An End-to-End Joint Learning Scheme of Image Compression and Quality Enhancement with Improved Entropy Minimization
- **Arxiv ID**: http://arxiv.org/abs/1912.12817v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12817v2)
- **Published**: 2019-12-30 05:10:05+00:00
- **Updated**: 2020-03-13 08:45:53+00:00
- **Authors**: Jooyoung Lee, Seunghyun Cho, Munchurl Kim
- **Comment**: 25 pages, 14 figures
- **Journal**: None
- **Summary**: Recently, learned image compression methods have been actively studied. Among them, entropy-minimization based approaches have achieved superior results compared to conventional image codecs such as BPG and JPEG2000. However, the quality enhancement and rate-minimization are conflictively coupled in the process of image compression. That is, maintaining high image quality entails less compression and vice versa. However, by jointly training separate quality enhancement in conjunction with image compression, the coding efficiency can be improved. In this paper, we propose a novel joint learning scheme of image compression and quality enhancement, called JointIQ-Net, as well as entropy model improvement, thus achieving significantly improved coding efficiency against the previous methods. Our proposed JointIQ-Net combines an image compression sub-network and a quality enhancement sub-network in a cascade, both of which are end-to-end trained in a combined manner within the JointIQ-Net. Also the JointIQ-Net benefits from improved entropy-minimization that newly adopts a Gussian Mixture Model (GMM) and further exploits global context to estimate the probabilities of latent representations. In order to show the effectiveness of our proposed JointIQ-Net, extensive experiments have been performed, and showed that the JointIQ-Net achieves a remarkable performance improvement in coding efficiency in terms of both PSNR and MS-SSIM, compared to the previous learned image compression methods and the conventional codecs such as VVC Intra (VTM 7.1), BPG, and JPEG2000. To the best of our knowledge, this is the first end-to-end optimized image compression method that outperforms VTM 7.1 (Intra), the latest reference software of the VVC standard, in terms of the PSNR and MS-SSIM.



### Early Detection of Diabetic Retinopathy and Severity Scale Measurement: A Progressive Review & Scopes
- **Arxiv ID**: http://arxiv.org/abs/1912.12829v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12829v1)
- **Published**: 2019-12-30 07:06:45+00:00
- **Updated**: 2019-12-30 07:06:45+00:00
- **Authors**: Asma Khatun, Sk. Golam Sarowar Hossain
- **Comment**: None
- **Journal**: None
- **Summary**: Early detection of diabetic retinopathy prevents visual loss and blindness of a human eye. Based on the types of feature extraction method used, DR detection method can be broadly classified as Deep Convolutional Neural Network (CNN) based and traditional feature extraction (machine learning) based. This paper presents a comprehensive survey of existing feature extraction methods based on Deep CNN and conventional feature extraction for DR detection. In addition to that, this paper focuses on the severity scale measurement of the DR detection and to the best of our knowledge this is the first survey paper which covers severity grading scale. It is also necessary to mention that this is the first study which reviews the proposed Deep CNN based method in the state of the art for DR detection methods. This study discovers that recently proposed deep learning based DR detection methods provides higher accuracy than existing traditional feature extraction methods in the literature and also useful in large scale datasets. However, deep learning based methods require GPU implementation to get the desirable output. The one of the other major finding of this paper is that there are no obvious standard severity scale detection criteria to measure the grading. Some used binary class while many other used multi stage class.



### Adversarial Example Generation using Evolutionary Multi-objective Optimization
- **Arxiv ID**: http://arxiv.org/abs/2001.05844v1
- **DOI**: 10.1109/CEC.2019.8790123
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.05844v1)
- **Published**: 2019-12-30 07:34:09+00:00
- **Updated**: 2019-12-30 07:34:09+00:00
- **Authors**: Takahiro Suzuki, Shingo Takeshita, Satoshi Ono
- **Comment**: None
- **Journal**: 2019 IEEE Congress on Evolutionary Computation (CEC), Wellington,
  New Zealand, 2019, pp. 2136-2144
- **Summary**: This paper proposes Evolutionary Multi-objective Optimization (EMO)-based Adversarial Example (AE) design method that performs under black-box setting. Previous gradient-based methods produce AEs by changing all pixels of a target image, while previous EC-based method changes small number of pixels to produce AEs. Thanks to EMO's property of population based-search, the proposed method produces various types of AEs involving ones locating between AEs generated by the previous two approaches, which helps to know the characteristics of a target model or to know unknown attack patterns. Experimental results showed the potential of the proposed method, e.g., it can generate robust AEs and, with the aid of DCT-based perturbation pattern generation, AEs for high resolution images.



### Multi-modality super-resolution loss for GAN-based super-resolution of clinical CT images using micro CT image database
- **Arxiv ID**: http://arxiv.org/abs/1912.12838v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12838v2)
- **Published**: 2019-12-30 07:49:58+00:00
- **Updated**: 2020-04-07 11:06:05+00:00
- **Authors**: Tong Zheng, Hirohisa Oda, Takayasu Moriya, Shota Nakamura, Masahiro Oda, Masaki Mori, Horitsugu Takabatake, Hiroshi Natori, Kensaku Mori
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: This paper newly introduces multi-modality loss function for GAN-based super-resolution that can maintain image structure and intensity on unpaired training dataset of clinical CT and micro CT volumes. Precise non-invasive diagnosis of lung cancer mainly utilizes 3D multidetector computed-tomography (CT) data. On the other hand, we can take micro CT images of resected lung specimen in 50 micro meter or higher resolution. However, micro CT scanning cannot be applied to living human imaging. For obtaining highly detailed information such as cancer invasion area from pre-operative clinical CT volumes of lung cancer patients, super-resolution (SR) of clinical CT volumes to $\mu$CT level might be one of substitutive solutions. While most SR methods require paired low- and high-resolution images for training, it is infeasible to obtain precisely paired clinical CT and micro CT volumes. We aim to propose unpaired SR approaches for clincial CT using micro CT images based on unpaired image translation methods such as CycleGAN or UNIT. Since clinical CT and micro CT are very different in structure and intensity, direct application of GAN-based unpaired image translation methods in super-resolution tends to generate arbitrary images. Aiming to solve this problem, we propose new loss function called multi-modality loss function to maintain the similarity of input images and corresponding output images in super-resolution task. Experimental results demonstrated that the newly proposed loss function made CycleGAN and UNIT to successfully perform SR of clinical CT images of lung cancer patients into micro CT level resolution, while original CycleGAN and UNIT failed in super-resolution.



### Generative Memorize-Then-Recall framework for low bit-rate Surveillance Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1912.12847v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12847v3)
- **Published**: 2019-12-30 08:34:32+00:00
- **Updated**: 2020-05-06 14:28:58+00:00
- **Authors**: Yaojun Wu, Tianyu He, Zhibo Chen
- **Comment**: 11 pages, 8 figures
- **Journal**: None
- **Summary**: Applications of surveillance video have developed rapidly in recent years to protect public safety and daily life, which often detect and recognize objects in video sequences. Traditional coding frameworks remove temporal redundancy in surveillance video by block-wise motion compensation, lacking the extraction and utilization of inherent structure information. In this paper, we figure out this issue by disentangling surveillance video into the structure of a global spatio-temporal feature (memory) for Group of Picture (GoP) and skeleton for each frame (clue). The memory is obtained by sequentially feeding frame inside GoP into a recurrent neural network, describing appearance for objects that appeared inside GoP. While the skeleton is calculated by a pose estimator, it is regarded as a clue to recall memory. Furthermore, an attention mechanism is introduced to obtain the relation between appearance and skeletons. Finally, we employ generative adversarial network to reconstruct each frame. Experimental results indicate that our method effectively generates realistic reconstruction based on appearance and skeleton, which show much higher compression performance on surveillance video compared with the latest video compression standard H.265.



### Defending from adversarial examples with a two-stream architecture
- **Arxiv ID**: http://arxiv.org/abs/1912.12859v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.0
- **Links**: [PDF](http://arxiv.org/pdf/1912.12859v1)
- **Published**: 2019-12-30 09:15:40+00:00
- **Updated**: 2019-12-30 09:15:40+00:00
- **Authors**: Hao Ge, Xiaoguang Tu, Mei Xie, Zheng Ma
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: In recent years, deep learning has shown impressive performance on many tasks. However, recent researches showed that deep learning systems are vulnerable to small, specially crafted perturbations that are imperceptible to humans. Images with such perturbations are the so called adversarial examples, which have proven to be an indisputable threat to the DNN based applications. The lack of better understanding of the DNNs has prevented the development of efficient defenses against adversarial examples. In this paper, we propose a two-stream architecture to protect CNN from attacking by adversarial examples. Our model draws on the idea of "two-stream" which commonly used in the security field, and successfully defends different kinds of attack methods by the differences of "high-resolution" and "low-resolution" networks in feature extraction. We provide a reasonable interpretation on why our two-stream architecture is difficult to defeat, and show experimentally that our method is hard to defeat with state-of-the-art attacks. We demonstrate that our two-stream architecture is robust to adversarial examples built by currently known attacking algorithms.



### Video Depth Estimation by Fusing Flow-to-Depth Proposals
- **Arxiv ID**: http://arxiv.org/abs/1912.12874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12874v2)
- **Published**: 2019-12-30 10:45:57+00:00
- **Updated**: 2020-03-03 06:34:04+00:00
- **Authors**: Jiaxin Xie, Chenyang Lei, Zhuwen Li, Li Erran Li, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present an approach with a differentiable flow-to-depth layer for video depth estimation. The model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera pose, our flow-to-depth layer generates depth proposals and the corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize depth proposals and their confidence maps inferred from different adjacent frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to improve the results further. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable cross dataset generalization capability: our model trained on KITTI still performs well on the unseen Waymo dataset.



### Integration of Regularized l1 Tracking and Instance Segmentation for Video Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.12883v1
- **DOI**: 10.1016/j.neucom.2020.09.072
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12883v1)
- **Published**: 2019-12-30 11:14:14+00:00
- **Updated**: 2019-12-30 11:14:14+00:00
- **Authors**: Filiz Gurkan, Bilge Gunsel
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a tracking-by-detection method that integrates a deep object detector with a particle filter tracker under the regularization framework where the tracked object is represented by a sparse dictionary. A novel observation model which establishes consensus between the detector and tracker is formulated that enables us to update the dictionary with the guidance of the deep detector. This yields an efficient representation of the object appearance through the video sequence hence improves robustness to occlusion and pose changes. Moreover we propose a new state vector consisting of translation, rotation, scaling and shearing parameters that allows tracking the deformed object bounding boxes hence significantly increases robustness to scale changes. Numerical results reported on challenging VOT2016 and VOT2018 benchmarking data sets demonstrate that the introduced tracker, L1DPF-M, achieves comparable robustness on both data sets while it outperforms state-of-the-art trackers on both data sets where the improvement achieved in success rate at IoU-th=0.5 is 11% and 9%, respectively.



### Real-time Segmentation and Facial Skin Tones Grading
- **Arxiv ID**: http://arxiv.org/abs/1912.12888v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12888v2)
- **Published**: 2019-12-30 11:35:36+00:00
- **Updated**: 2020-01-09 03:52:04+00:00
- **Authors**: Ling Luo, Dingyu Xue, Xinglong Feng, Yichun Yu, Peng Wang
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: Modern approaches for semantic segmention usually pay too much attention to the accuracy of the model, and therefore it is strongly recommended to introduce cumbersome backbones, which brings heavy computation burden and memory footprint. To alleviate this problem, we propose an efficient segmentation method based on deep convolutional neural networks (DCNNs) for the task of hair and facial skin segmentation, which achieving remarkable trade-off between speed and performance on three benchmark datasets. As far as we know, the accuracy of skin tones classification is usually unsatisfactory due to the influence of external environmental factors such as illumination and background noise. Therefore, we use the segmentated face to obtain a specific face area, and further exploit the color moment algorithm to extract its color features. Specifically, for a 224 x 224 standard input, using our high-resolution spatial detail information and low-resolution contextual information fusion network (HLNet), we achieve 90.73% Pixel Accuracy on Figaro1k dataset at over 16 FPS in the case of CPU environment. Additional experiments on CamVid dataset further confirm the universality of the proposed model. We further use masked color moment for skin tones grade evaluation and approximate 80% classification accuracy demonstrate the feasibility of the proposed scheme.Code is available at https://github.com/JACKYLUO1991/Face-skin-hair-segmentaiton-and-skin-color-evaluation.



### PPDM: Parallel Point Detection and Matching for Real-time Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.12898v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12898v3)
- **Published**: 2019-12-30 12:00:55+00:00
- **Updated**: 2020-03-25 12:29:07+00:00
- **Authors**: Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, Jiashi Feng
- **Comment**: Accepted to CVPR 2020. Code is available at
  https://github.com/YueLiao/PPDM
- **Journal**: None
- **Summary**: We propose a single-stage Human-Object Interaction (HOI) detection method that has outperformed all existing methods on HICO-DET dataset at 37 fps on a single Titan XP GPU. It is the first real-time HOI detection method. Conventional HOI detection methods are composed of two stages, i.e., human-object proposals generation, and proposals classification. Their effectiveness and efficiency are limited by the sequential and separate architecture. In this paper, we propose a Parallel Point Detection and Matching (PPDM) HOI detection framework. In PPDM, an HOI is defined as a point triplet < human point, interaction point, object point>. Human and object points are the center of the detection boxes, and the interaction point is the midpoint of the human and object points. PPDM contains two parallel branches, namely point detection branch and point matching branch. The point detection branch predicts three points. Simultaneously, the point matching branch predicts two displacements from the interaction point to its corresponding human and object points. The human point and the object point originated from the same interaction point are considered as matched pairs. In our novel parallel architecture, the interaction points implicitly provide context and regularization for human and object detection. The isolated detection boxes are unlikely to form meaning HOI triplets are suppressed, which increases the precision of HOI detection. Moreover, the matching between human and object detection boxes is only applied around limited numbers of filtered candidate interaction points, which saves much computational cost. Additionally, we build a new application-oriented database named HOI-A, which severs as a good supplement to the existing datasets. The source code and the dataset will be made publicly available to facilitate the development of HOI detection.



### Wi2Vi: Generating Video Frames from WiFi CSI Samples
- **Arxiv ID**: http://arxiv.org/abs/2001.05842v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/2001.05842v1)
- **Published**: 2019-12-30 13:36:37+00:00
- **Updated**: 2019-12-30 13:36:37+00:00
- **Authors**: Mohammad Hadi Kefayati, Vahid Pourahmadi, Hassan Aghaeinia
- **Comment**: None
- **Journal**: None
- **Summary**: Objects in an environment affect electromagnetic waves. While this effect varies across frequencies, there exists a correlation between them, and a model with enough capacity can capture this correlation between the measurements in different frequencies. In this paper, we propose the Wi2Vi model for associating variations in the WiFi channel state information with video frames. The proposed Wi2Vi system can generate video frames entirely using CSI measurements. The produced video frames by the Wi2Vi provide auxiliary information to the conventional surveillance system in critical circumstances. Our implementation of the Wi2Vi system confirms the feasibility of constructing a system capable of deriving the correlations between measurements in different frequency spectrums.



### Supervised and Unsupervised Learning of Parameterized Color Enhancement
- **Arxiv ID**: http://arxiv.org/abs/2001.05843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05843v1)
- **Published**: 2019-12-30 13:57:06+00:00
- **Updated**: 2019-12-30 13:57:06+00:00
- **Authors**: Yoav Chai, Raja Giryes, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We treat the problem of color enhancement as an image translation task, which we tackle using both supervised and unsupervised learning. Unlike traditional image to image generators, our translation is performed using a global parameterized color transformation instead of learning to directly map image information. In the supervised case, every training image is paired with a desired target image and a convolutional neural network (CNN) learns from the expert retouched images the parameters of the transformation. In the unpaired case, we employ two-way generative adversarial networks (GANs) to learn these parameters and apply a circularity constraint. We achieve state-of-the-art results compared to both supervised (paired data) and unsupervised (unpaired data) image enhancement methods on the MIT-Adobe FiveK benchmark. Moreover, we show the generalization capability of our method, by applying it on photos from the early 20th century and to dark video frames.



### A New Approach for Explainable Multiple Organ Annotation with Few Data
- **Arxiv ID**: http://arxiv.org/abs/1912.12932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12932v1)
- **Published**: 2019-12-30 14:06:32+00:00
- **Updated**: 2019-12-30 14:06:32+00:00
- **Authors**: Régis Pierrard, Jean-Philippe Poli, Céline Hudelot
- **Comment**: None
- **Journal**: IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAI),
  Aug 2019, Macao, Macau SAR China
- **Summary**: Despite the recent successes of deep learning, such models are still far from some human abilities like learning from few examples, reasoning and explaining decisions. In this paper, we focus on organ annotation in medical images and we introduce a reasoning framework that is based on learning fuzzy relations on a small dataset for generating explanations. Given a catalogue of relations, it efficiently induces the most relevant relations and combines them for building constraints in order to both solve the organ annotation task and generate explanations. We test our approach on a publicly available dataset of medical images where several organs are already segmented. A demonstration of our model is proposed with an example of explained annotations. It was trained on a small training set containing as few as a couple of examples.



### Discovering Latent Classes for Semi-Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.12936v4
- **DOI**: 10.1007/978-3-030-71278-5_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12936v4)
- **Published**: 2019-12-30 14:16:24+00:00
- **Updated**: 2021-03-08 21:16:53+00:00
- **Authors**: Olga Zatsarynna, Johann Sawatzky, Juergen Gall
- **Comment**: In DAGM German Conference on Pattern Recognition (GCPR'20)
- **Journal**: None
- **Summary**: High annotation costs are a major bottleneck for the training of semantic segmentation systems. Therefore, methods working with less annotation effort are of special interest. This paper studies the problem of semi-supervised semantic segmentation. This means that only a small subset of the training images is annotated while the other training images do not contain any annotation. In order to leverage the information present in the unlabeled images, we propose to learn a second task that is related to semantic segmentation but easier. On labeled images, we learn latent classes consistent with semantic classes so that the variety of semantic classes assigned to a latent class is as low as possible. On unlabeled images, we predict a probability map for latent classes and use it as a supervision signal to learn semantic segmentation. The latent classes, as well as the semantic classes, are simultaneously predicted by a two-branch network. In our experiments on Pascal VOC and Cityscapes, we show that the latent classes learned this way have an intuitive meaning and that the proposed method achieves state of the art results for semi-supervised semantic segmentation.



### Image retrieval approach based on local texture information derived from predefined patterns and spatial domain information
- **Arxiv ID**: http://arxiv.org/abs/1912.12978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12978v1)
- **Published**: 2019-12-30 16:11:04+00:00
- **Updated**: 2019-12-30 16:11:04+00:00
- **Authors**: Nazgol Hor, Shervan Fekri-Ershad
- **Comment**: Faculty of computer engineering, Najafabad Branch, Islamic azad
  university, Najafabad, Iran
- **Journal**: International Journal of Computer Science Engineering (IJCSE),
  Vol. 8 No.06, pp. 246-254, Nov-Dec 2019,
- **Summary**: With the development of Information technology and communication, a large part of the databases is dedicated to images and videos. Thus retrieving images related to a query image from a large database has become an important area of research in computer vision. Until now, there are various methods of image retrieval that try to define image contents by texture, color or shape properties. In this paper, a method is presented for image retrieval based on a combination of local texture information derived from two different texture descriptors. First, the color channels of the input image are separated. The texture information is extracted using two descriptors such as evaluated local binary patterns and predefined pattern units. After extracting the features, the similarity matching is done based on distance criteria. The performance of the proposed method is evaluated in terms of precision and recall on the Simplicity database. The comparative results showed that the proposed approach offers higher precision rate than many known methods.



### Characteristic Regularisation for Super-Resolving Face Images
- **Arxiv ID**: http://arxiv.org/abs/1912.12987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12987v1)
- **Published**: 2019-12-30 16:27:24+00:00
- **Updated**: 2019-12-30 16:27:24+00:00
- **Authors**: Zhiyi Cheng, Xiatian Zhu, Shaogang Gong
- **Comment**: Accepted by WACV2020
- **Journal**: None
- **Summary**: Existing facial image super-resolution (SR) methods focus mostly on improving artificially down-sampled low-resolution (LR) imagery. Such SR models, although strong at handling artificial LR images, often suffer from significant performance drop on genuine LR test data. Previous unsupervised domain adaptation (UDA) methods address this issue by training a model using unpaired genuine LR and HR data as well as cycle consistency loss formulation. However, this renders the model overstretched with two tasks: consistifying the visual characteristics and enhancing the image resolution. Importantly, this makes the end-to-end model training ineffective due to the difficulty of back-propagating gradients through two concatenated CNNs. To solve this problem, we formulate a method that joins the advantages of conventional SR and UDA models. Specifically, we separate and control the optimisations for characteristics consistifying and image super-resolving by introducing Characteristic Regularisation (CR) between them. This task split makes the model training more effective and computationally tractable. Extensive evaluations demonstrate the performance superiority of our method over state-of-the-art SR and UDA models on both genuine and artificial LR facial imagery data.



### Recognizing Instagram Filtered Images with Feature De-stylization
- **Arxiv ID**: http://arxiv.org/abs/1912.13000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13000v1)
- **Published**: 2019-12-30 16:48:16+00:00
- **Updated**: 2019-12-30 16:48:16+00:00
- **Authors**: Zhe Wu, Zuxuan Wu, Bharat Singh, Larry S. Davis
- **Comment**: Accepted in AAAI 2020 as an oral presentation paper
- **Journal**: None
- **Summary**: Deep neural networks have been shown to suffer from poor generalization when small perturbations are added (like Gaussian noise), yet little work has been done to evaluate their robustness to more natural image transformations like photo filters. This paper presents a study on how popular pretrained models are affected by commonly used Instagram filters. To this end, we introduce ImageNet-Instagram, a filtered version of ImageNet, where 20 popular Instagram filters are applied to each image in ImageNet. Our analysis suggests that simple structure preserving filters which only alter the global appearance of an image can lead to large differences in the convolutional feature space. To improve generalization, we introduce a lightweight de-stylization module that predicts parameters used for scaling and shifting feature maps to "undo" the changes incurred by filters, inverting the process of style transfer tasks. We further demonstrate the module can be readily plugged into modern CNN architectures together with skip connections. We conduct extensive studies on ImageNet-Instagram, and show quantitatively and qualitatively, that the proposed module, among other things, can effectively improve generalization by simply learning normalization parameters without retraining the entire network, thus recovering the alterations in the feature space caused by the filters.



### Learning Selective Sensor Fusion for States Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.13077v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.13077v2)
- **Published**: 2019-12-30 20:25:16+00:00
- **Updated**: 2022-05-18 10:42:16+00:00
- **Authors**: Changhao Chen, Stefano Rosa, Chris Xiaoxuan Lu, Bing Wang, Niki Trigoni, Andrew Markham
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS). arXiv admin note: text overlap with arXiv:1903.01534
- **Journal**: None
- **Summary**: Autonomous vehicles and mobile robotic systems are typically equipped with multiple sensors to provide redundancy. By integrating the observations from different sensors, these mobile agents are able to perceive the environment and estimate system states, e.g. locations and orientations. Although deep learning approaches for multimodal odometry estimation and localization have gained traction, they rarely focus on the issue of robust sensor fusion - a necessary consideration to deal with noisy or incomplete sensor observations in the real world. Moreover, current deep odometry models suffer from a lack of interpretability. To this extent, we propose SelectFusion, an end-to-end selective sensor fusion module which can be applied to useful pairs of sensor modalities such as monocular images and inertial measurements, depth images and LIDAR point clouds. Our model is a uniform framework that is not restricted to specific modality or task. During prediction, the network is able to assess the reliability of the latent features from different sensor modalities and estimate trajectory both at scale and global pose. In particular, we propose two fusion modules - a deterministic soft fusion and a stochastic hard fusion, and offer a comprehensive study of the new strategies compared to trivial direct fusion. We extensively evaluate all fusion strategies in both public datasets and on progressively degraded datasets that present synthetic occlusions, noisy and missing data and time misalignment between sensors, and we investigate the effectiveness of the different fusion strategies in attending the most reliable features, which in itself, provides insights into the operation of the various models.



### Basis Pursuit and Orthogonal Matching Pursuit for Subspace-preserving Recovery: Theoretical Analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.13091v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.13091v1)
- **Published**: 2019-12-30 21:31:15+00:00
- **Updated**: 2019-12-30 21:31:15+00:00
- **Authors**: Daniel P. Robinson, Rene Vidal, Chong You
- **Comment**: 31 pages, 6 figures
- **Journal**: None
- **Summary**: Given an overcomplete dictionary $A$ and a signal $b = Ac^*$ for some sparse vector $c^*$ whose nonzero entries correspond to linearly independent columns of $A$, classical sparse signal recovery theory considers the problem of whether $c^*$ can be recovered as the unique sparsest solution to $b = A c$. It is now well-understood that such recovery is possible by practical algorithms when the dictionary $A$ is incoherent or restricted isometric. In this paper, we consider the more general case where $b$ lies in a subspace $\mathcal{S}_0$ spanned by a subset of linearly dependent columns of $A$, and the remaining columns are outside of the subspace. In this case, the sparsest representation may not be unique, and the dictionary may not be incoherent or restricted isometric. The goal is to have the representation $c$ correctly identify the subspace, i.e. the nonzero entries of $c$ should correspond to columns of $A$ that are in the subspace $\mathcal{S}_0$. Such a representation $c$ is called subspace-preserving, a key concept that has found important applications for learning low-dimensional structures in high-dimensional data. We present various geometric conditions that guarantee subspace-preserving recovery. Among them, the major results are characterized by the covering radius and the angular distance, which capture the distribution of points in the subspace and the similarity between points in the subspace and points outside the subspace, respectively. Importantly, these conditions do not require the dictionary to be incoherent or restricted isometric. By establishing that the subspace-preserving recovery problem and the classical sparse signal recovery problem are equivalent under common assumptions on the latter, we show that several of our proposed conditions are generalizations of some well-known conditions in the sparse signal recovery literature.



