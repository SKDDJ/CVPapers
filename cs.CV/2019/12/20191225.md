# Arxiv Papers in cs.CV on 2019-12-25
### SketchTransfer: A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.11570v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11570v1)
- **Published**: 2019-12-25 00:38:47+00:00
- **Updated**: 2019-12-25 00:38:47+00:00
- **Authors**: Alex Lamb, Sherjil Ozair, Vikas Verma, David Ha
- **Comment**: Accepted WACV 2020
- **Journal**: None
- **Summary**: Deep networks have achieved excellent results in perceptual tasks, yet their ability to generalize to variations not seen during training has come under increasing scrutiny. In this work we focus on their ability to have invariance towards the presence or absence of details. For example, humans are able to watch cartoons, which are missing many visual details, without being explicitly trained to do so. As another example, 3D rendering software is a relatively recent development, yet people are able to understand such rendered scenes even though they are missing details (consider a film like Toy Story). The failure of machine learning algorithms to do this indicates a significant gap in generalization between human abilities and the abilities of deep networks. We propose a dataset that will make it easier to study the detail-invariance problem concretely. We produce a concrete task for this: SketchTransfer, and we show that state-of-the-art domain transfer algorithms still struggle with this task. The state-of-the-art technique which achieves over 95\% on MNIST $\xrightarrow{}$ SVHN transfer only achieves 59\% accuracy on the SketchTransfer task, which is much better than random (11\% accuracy) but falls short of the 87\% accuracy of a classifier trained directly on labeled sketches. This indicates that this task is approachable with today's best methods but has substantial room for improvement.



### Inverse Rendering Techniques for Physically Grounded Image Editing
- **Arxiv ID**: http://arxiv.org/abs/2001.00986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00986v1)
- **Published**: 2019-12-25 04:01:34+00:00
- **Updated**: 2019-12-25 04:01:34+00:00
- **Authors**: Kevin Karsch
- **Comment**: PhD thesis, Computer Science, University of Illinois at
  Urbana-Champaign, 2015
- **Journal**: None
- **Summary**: From a single picture of a scene, people can typically grasp the spatial layout immediately and even make good guesses at materials properties and where light is coming from to illuminate the scene. For example, we can reliably tell which objects occlude others, what an object is made of and its rough shape, regions that are illuminated or in shadow, and so on. It is interesting how little is known about our ability to make these determinations; as such, we are still not able to robustly "teach" computers to make the same high-level observations as people. This document presents algorithms for understanding intrinsic scene properties from single images. The goal of these inverse rendering techniques is to estimate the configurations of scene elements (geometry, materials, luminaires, camera parameters, etc) using only information visible in an image. Such algorithms have applications in robotics and computer graphics. One such application is in physically grounded image editing: photo editing made easier by leveraging knowledge of the physical space. These applications allow sophisticated editing operations to be performed in a matter of seconds, enabling seamless addition, removal, or relocation of objects in images.



### Effective Data Augmentation with Multi-Domain Learning GANs
- **Arxiv ID**: http://arxiv.org/abs/1912.11597v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11597v1)
- **Published**: 2019-12-25 05:39:45+00:00
- **Updated**: 2019-12-25 05:39:45+00:00
- **Authors**: Shin'ya Yamaguchi, Sekitoshi Kanai, Takeharu Eda
- **Comment**: AAAI-2020
- **Journal**: None
- **Summary**: For deep learning applications, the massive data development (e.g., collecting, labeling), which is an essential process in building practical applications, still incurs seriously high costs. In this work, we propose an effective data augmentation method based on generative adversarial networks (GANs), called Domain Fusion. Our key idea is to import the knowledge contained in an outer dataset to a target model by using a multi-domain learning GAN. The multi-domain learning GAN simultaneously learns the outer and target dataset and generates new samples for the target tasks. The simultaneous learning process makes GANs generate the target samples with high fidelity and variety. As a result, we can obtain accurate models for the target tasks by using these generated samples even if we only have an extremely low volume target dataset. We experimentally evaluate the advantages of Domain Fusion in image classification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor Scene Recognition. When trained on each target dataset reduced the samples to 5,000 images, Domain Fusion achieves better classification accuracy than the data augmentation using fine-tuned GANs. Furthermore, we show that Domain Fusion improves the quality of generated samples, and the improvements can contribute to higher accuracy.



### Image Enhanced Rotation Prediction for Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.11603v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11603v2)
- **Published**: 2019-12-25 06:11:35+00:00
- **Updated**: 2021-06-04 08:12:54+00:00
- **Authors**: Shin'ya Yamaguchi, Sekitoshi Kanai, Tetsuya Shioda, Shoichiro Takeda
- **Comment**: Accepted to IEEE ICIP 2021. The title has been changed from "Multiple
  Pretext-Task for Self-Supervised Learning via Mixing Multiple Image
  Transformations"
- **Journal**: None
- **Summary**: The rotation prediction (Rotation) is a simple pretext-task for self-supervised learning (SSL), where models learn useful representations for target vision tasks by solving pretext-tasks. Although Rotation captures information of object shapes, it hardly captures information of textures. To tackle this problem, we introduce a novel pretext-task called image enhanced rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and another pretext-task based on image enhancement (e.g., sharpening and solarizing) while maintaining simplicity. Through the simultaneous prediction of rotation and image enhancement, models learn representations to capture the information of not only object shapes but also textures. Our experimental results show that IE-Rot models outperform Rotation on various standard benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO detection/segmentation.



### InSphereNet: a Concise Representation and Classification Method for 3D Object
- **Arxiv ID**: http://arxiv.org/abs/1912.11606v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11606v2)
- **Published**: 2019-12-25 06:26:20+00:00
- **Updated**: 2020-01-03 01:48:16+00:00
- **Authors**: Hui Cao, Haikuan Du, Siyu Zhang, Shen Cai
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present an InSphereNet method for the problem of 3D object classification. Unlike previous methods that use points, voxels, or multi-view images as inputs of deep neural network (DNN), the proposed method constructs a class of more representative features named infilling spheres from signed distance field (SDF). Because of the admirable spatial representation of infilling spheres, we can not only utilize very fewer number of spheres to accomplish classification task, but also design a lightweight InSphereNet with less layers and parameters than previous methods. Experiments on ModelNet40 show that the proposed method leads to superior performance than PointNet and PointNet++ in accuracy. In particular, if there are only a few dozen sphere inputs or about 100000 DNN parameters, the accuracy of our method remains at a very high level (over 88%). This further validates the conciseness and effectiveness of the proposed InSphere 3D representation. Keywords: 3D object classification , signed distance field , deep learning , infilling sphere



### Concise and Effective Network for 3D Human Modeling from Orthogonal Silhouettes
- **Arxiv ID**: http://arxiv.org/abs/1912.11616v3
- **DOI**: 10.1115/1.4054001
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11616v3)
- **Published**: 2019-12-25 08:05:37+00:00
- **Updated**: 2022-02-14 05:08:56+00:00
- **Authors**: Bin Liu, Xiuping Liu, Zhixin Yang, Charlie C. L. Wang
- **Comment**: 13 pages, 15 figures
- **Journal**: J. Comput. Inf. Sci. Eng. October 2022; 22(5): 051004
- **Summary**: In this paper, we revisit the problem of 3D human modeling from two orthogonal silhouettes of individuals (i.e., front and side views). Different from our prior work, a supervised learning approach based on convolutional neural network (CNN) is investigated to solve the problem by establishing a mapping function that can effectively extract features from two silhouettes and fuse them into coefficients in the shape space of human bodies. A new CNN structure is proposed in our work to exact not only the discriminative features of front and side views and also their mixed features for the mapping function. 3D human models with high accuracy are synthesized from coefficients generated by the mapping function. Existing CNN approaches for 3D human modeling usually learn a large number of parameters (from 8.5M to 355.4M) from two binary images. Differently, we investigate a new network architecture and conduct the samples on silhouettes as input. As a consequence, more accurate models can be generated by our network with only 2.4M coefficients. The training of our network is conducted on samples obtained by augmenting a publicly accessible dataset. Learning transfer by using datasets with a smaller number of scanned models is applied to our network to enable the function of generating results with gender-oriented (or geographical) patterns.



### Learn to Segment Retinal Lesions and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1912.11619v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11619v3)
- **Published**: 2019-12-25 08:14:04+00:00
- **Updated**: 2020-10-18 03:43:34+00:00
- **Authors**: Qijie Wei, Xirong Li, Weihong Yu, Xiao Zhang, Yongpeng Zhang, Bojie Hu, Bin Mo, Di Gong, Ning Chen, Dayong Ding, Youxin Chen
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: Towards automated retinal screening, this paper makes an endeavor to simultaneously achieve pixel-level retinal lesion segmentation and image-level disease classification. Such a multi-task approach is crucial for accurate and clinically interpretable disease diagnosis. Prior art is insufficient due to three challenges, i.e., lesions lacking objective boundaries, clinical importance of lesions irrelevant to their size, and the lack of one-to-one correspondence between lesion and disease classes. This paper attacks the three challenges in the context of diabetic retinopathy (DR) grading. We propose Lesion-Net, a new variant of fully convolutional networks, with its expansive path re-designed to tackle the first challenge. A dual Dice loss that leverages both semantic segmentation and image classification losses is introduced to resolve the second challenge. Lastly, we build a multi-task network that employs Lesion-Net as a side-attention branch for both DR grading and result interpretation. A set of 12K fundus images is manually segmented by 45 ophthalmologists for 8 DR-related lesions, resulting in 290K manual segments in total. Extensive experiments on this large-scale dataset show that our proposed approach surpasses the prior art for multiple tasks including lesion segmentation, lesion classification and DR grading



### Ranking and Classification driven Feature Learning for Person Re_identification
- **Arxiv ID**: http://arxiv.org/abs/1912.11630v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.0; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.11630v1)
- **Published**: 2019-12-25 10:03:13+00:00
- **Updated**: 2019-12-25 10:03:13+00:00
- **Authors**: Zhiguang Zhang
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: Person re-identification has attracted many researchers' attention for its wide application, but it is still a very challenging task because only part of the image information can be used for personnel matching. Most of current methods uses CNN to learn to embeddings that can capture semantic similarity information among data points. Many of the state-of-the-arts methods use complex network structures with multiple branches that fuse multiple features while training or testing, using classification loss, Triplet loss or a combination of the two as loss function. However, the method that using Triplet loss as loss function converges slowly, and the method in which pull features of the same class as close as possible in features space leads to poor feature stability. This paper will combine the ranking motivated structured loss, proposed a new metric learning loss function that make the features of the same class are sparsely distributed into the range of small hyperspheres and the features of different classes are uniformly distributed at a clearly angle. And adopted a new single-branch network structure that only using global feature can also get great performance. The validity of our method is verified on the Market1501 and DukeMTMC-ReID person re-identification datasets. Finally acquires 90.9% rank-1 accuracy and 80.8% mAP on DukeMTMC-reID, 95.3% rank-1 accuracy and 88.7% mAP on Market1501. Codes and models are available in Github.https://github.com/Qidian213/Ranked_Person_ReID.



### Competing Ratio Loss for Discriminative Multi-class Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.11642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11642v2)
- **Published**: 2019-12-25 11:38:23+00:00
- **Updated**: 2021-08-27 12:46:24+00:00
- **Authors**: Ke Zhang, Yurong Guo, Xinsheng Wang, Dongliang Chang, Zhenbing Zhao, Zhanyu Ma, Tony X. Han
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1907.13349
- **Journal**: None
- **Summary**: The development of deep convolutional neural network architecture is critical to the improvement of image classification task performance. Many image classification studies use deep convolutional neural network and focus on modifying the network structure to improve image classification performance. Conversely, our study focuses on loss function design. Cross-entropy Loss (CEL) has been widely used for training deep convolutional neural network for the task of multi-class classification. Although CEL has been successfully implemented in several image classification tasks, it only focuses on the posterior probability of the correct class. For this reason, a negative log likelihood ratio loss (NLLR) was proposed to better differentiate between the correct class and the competing incorrect ones. However, during the training of the deep convolutional neural network, the value of NLLR is not always positive or negative, which severely affects the convergence of NLLR. Our proposed competing ratio loss (CRL) calculates the posterior probability ratio between the correct class and the competing incorrect classes to further enlarge the probability difference between the correct and incorrect classes. We added hyperparameters to CRL, thereby ensuring its value to be positive and that the update size of backpropagation is suitable for the CRL's fast convergence. To demonstrate the performance of CRL, we conducted experiments on general image classification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image classification tasks (CUB200-2011 and Stanford Car), and the challenging face age estimation task (using Adience). Experimental results show the effectiveness and robustness of the proposed loss function on different deep convolutional neural network architectures and different image classification tasks.



### Extending Multi-Object Tracking systems to better exploit appearance and 3D information
- **Arxiv ID**: http://arxiv.org/abs/1912.11651v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.11651v1)
- **Published**: 2019-12-25 12:16:38+00:00
- **Updated**: 2019-12-25 12:16:38+00:00
- **Authors**: Kanchana Ranasinghe, Sahan Liyanaarachchi, Harsha Ranasinghe, Mayuka Jayawardhana
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: Tracking multiple objects in real time is essential for a variety of real-world applications, with self-driving industry being at the foremost. This work involves exploiting temporally varying appearance and motion information for tracking. Siamese networks have recently become highly successful at appearance based single object tracking and Recurrent Neural Networks have started dominating both motion and appearance based tracking. Our work focuses on combining Siamese networks and RNNs to exploit appearance and motion information respectively to build a joint system capable of real time multi-object tracking. We further explore heuristics based constraints for tracking in the Birds Eye View Space for efficiently exploiting 3D information as a constrained optimization problem for track prediction.



### DDI-100: Dataset for Text Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.11658v1
- **DOI**: 10.1145/3440084.3441192
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11658v1)
- **Published**: 2019-12-25 12:47:35+00:00
- **Updated**: 2019-12-25 12:47:35+00:00
- **Authors**: Ilia Zharikov, Filipp Nikitin, Ilia Vasiliev, Vladimir Dokholyan
- **Comment**: Accepted by CCVPR 2019. Dataset is available here:
  https://github.com/machine-intelligence-laboratory/DDI-100
- **Journal**: None
- **Summary**: Nowadays document analysis and recognition remain challenging tasks. However, only a few datasets designed for text detection (TD) and optical character recognition (OCR) problems exist. In this paper we present Distorted Document Images dataset (DDI-100) and demonstrate its usefulness in a wide range of document analysis problems. DDI-100 dataset is a synthetic dataset based on 7000 real unique document pages and consists of more than 100000 augmented images. Ground truth comprises text and stamp masks, text and characters bounding boxes with relevant annotations. Validation of DDI-100 dataset was conducted using several TD and OCR models that show high-quality performance on real data.



### Improving Visual Recognition using Ambient Sound for Supervision
- **Arxiv ID**: http://arxiv.org/abs/1912.11659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11659v1)
- **Published**: 2019-12-25 12:48:53+00:00
- **Updated**: 2019-12-25 12:48:53+00:00
- **Authors**: Rohan Mahadev, Hongyu Lu
- **Comment**: 8 pages, 8 figures
- **Journal**: None
- **Summary**: Our brains combine vision and hearing to create a more elaborate interpretation of the world. When the visual input is insufficient, a rich panoply of sounds can be used to describe our surroundings. Since more than 1,000 hours of videos are uploaded to the internet everyday, it is arduous, if not impossible, to manually annotate these videos. Therefore, incorporating audio along with visual data without annotations is crucial for leveraging this explosion of data for recognizing and understanding objects and scenes. Owens,et.al suggest that a rich representation of the physical world can be learned by using a convolutional neural network to predict sound textures associated with a given video frame. We attempt to reproduce the claims from their experiments, of which the code is not publicly available. In addition, we propose improvements in the pretext task that result in better performance in other downstream computer vision tasks.



### Asymmetric GAN for Unpaired Image-to-image Translation
- **Arxiv ID**: http://arxiv.org/abs/1912.11660v1
- **DOI**: 10.1109/TIP.2019.2922854
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11660v1)
- **Published**: 2019-12-25 12:49:41+00:00
- **Updated**: 2019-12-25 12:49:41+00:00
- **Authors**: Yu Li, Sheng Tang, Rui Zhang, Yongdong Zhang, Jintao Li, Shuicheng Yan
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP) 2019
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Unpaired image-to-image translation problem aims to model the mapping from one domain to another with unpaired training data. Current works like the well-acknowledged Cycle GAN provide a general solution for any two domains through modeling injective mappings with a symmetric structure. While in situations where two domains are asymmetric in complexity, i.e., the amount of information between two domains is different, these approaches pose problems of poor generation quality, mapping ambiguity, and model sensitivity. To address these issues, we propose Asymmetric GAN (AsymGAN) to adapt the asymmetric domains by introducing an auxiliary variable (aux) to learn the extra information for transferring from the information-poor domain to the information-rich domain, which improves the performance of state-of-the-art approaches in the following ways. First, aux better balances the information between two domains which benefits the quality of generation. Second, the imbalance of information commonly leads to mapping ambiguity, where we are able to model one-to-many mappings by tuning aux, and furthermore, our aux is controllable. Third, the training of Cycle GAN can easily make the generator pair sensitive to small disturbances and variations while our model decouples the ill-conditioned relevance of generators by injecting aux during training. We verify the effectiveness of our proposed method both qualitatively and quantitatively on asymmetric situation, label-photo task, on Cityscapes and Helen datasets, and show many applications of asymmetric image translations. In conclusion, our AsymGAN provides a better solution for unpaired image-to-image translation in asymmetric domains.



### Deep Learning-based Vehicle Behaviour Prediction For Autonomous Driving Applications: A Review
- **Arxiv ID**: http://arxiv.org/abs/1912.11676v2
- **DOI**: 10.1109/TITS.2020.3012034
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11676v2)
- **Published**: 2019-12-25 14:22:41+00:00
- **Updated**: 2020-07-23 15:52:45+00:00
- **Authors**: Sajjad Mozaffari, Omar Y. Al-Jarrah, Mehrdad Dianati, Paul Jennings, Alexandros Mouzakitis
- **Comment**: None
- **Journal**: None
- **Summary**: Behaviour prediction function of an autonomous vehicle predicts the future states of the nearby vehicles based on the current and past observations of the surrounding environment. This helps enhance their awareness of the imminent hazards. However, conventional behaviour prediction solutions are applicable in simple driving scenarios that require short prediction horizons. Most recently, deep learning-based approaches have become popular due to their superior performance in more complex environments compared to the conventional approaches. Motivated by this increased popularity, we provide a comprehensive review of the state-of-the-art of deep learning-based approaches for vehicle behaviour prediction in this paper. We firstly give an overview of the generic problem of vehicle behaviour prediction and discuss its challenges, followed by classification and review of the most recent deep learning-based solutions based on three criteria: input representation, output type, and prediction method. The paper also discusses the performance of several well-known solutions, identifies the research gaps in the literature and outlines potential new research directions.



### Neural ODEs for Image Segmentation with Level Sets
- **Arxiv ID**: http://arxiv.org/abs/1912.11683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11683v1)
- **Published**: 2019-12-25 15:02:24+00:00
- **Updated**: 2019-12-25 15:02:24+00:00
- **Authors**: Rafael Valle, Fitsum Reda, Mohammad Shoeybi, Patrick Legresley, Andrew Tao, Bryan Catanzaro
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for image segmentation that combines Neural Ordinary Differential Equations (NODEs) and the Level Set method. Our approach parametrizes the evolution of an initial contour with a NODE that implicitly learns from data a speed function describing the evolution. In addition, for cases where an initial contour is not available and to alleviate the need for careful choice or design of contour embedding functions, we propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space. We evaluate our methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial contours provided by deep learning models while using a fraction of their number of parameters, our approach achieves F scores that are higher than several state-of-the-art deep learning algorithms.



### Look, Listen, and Act: Towards Audio-Visual Embodied Navigation
- **Arxiv ID**: http://arxiv.org/abs/1912.11684v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.11684v2)
- **Published**: 2019-12-25 15:07:26+00:00
- **Updated**: 2020-03-08 00:18:49+00:00
- **Authors**: Chuang Gan, Yiwei Zhang, Jiajun Wu, Boqing Gong, Joshua B. Tenenbaum
- **Comment**: Accepted by ICRA 2020. Project page: http://avn.csail.mit.edu
- **Journal**: None
- **Summary**: A crucial ability of mobile intelligent agents is to integrate the evidence from multiple sensory inputs in an environment and to make a sequence of actions to reach their goals. In this paper, we attempt to approach the problem of Audio-Visual Embodied Navigation, the task of planning the shortest path from a random starting location in a scene to the sound source in an indoor environment, given only raw egocentric visual and audio sensory data. To accomplish this task, the agent is required to learn from various modalities, i.e. relating the audio signal to the visual environment. Here we describe an approach to audio-visual embodied navigation that takes advantage of both visual and audio pieces of evidence. Our solution is based on three key ideas: a visual perception mapper module that constructs its spatial memory of the environment, a sound perception module that infers the relative location of the sound source from the agent, and a dynamic path planner that plans a sequence of actions based on the audio-visual observations and the spatial memory of the environment to navigate toward the goal. Experimental results on a newly collected Visual-Audio-Room dataset using the simulated multi-modal environment demonstrate the effectiveness of our approach over several competitive baselines.



### Multi-Modal Attention-based Fusion Model for Semantic Segmentation of RGB-Depth Images
- **Arxiv ID**: http://arxiv.org/abs/1912.11691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1912.11691v1)
- **Published**: 2019-12-25 16:53:31+00:00
- **Updated**: 2019-12-25 16:53:31+00:00
- **Authors**: Fahimeh Fooladgar, Shohreh Kasaei
- **Comment**: None
- **Journal**: None
- **Summary**: The 3D scene understanding is mainly considered as a crucial requirement in computer vision and robotics applications. One of the high-level tasks in 3D scene understanding is semantic segmentation of RGB-Depth images. With the availability of RGB-D cameras, it is desired to improve the accuracy of the scene understanding process by exploiting the depth features along with the appearance features. As depth images are independent of illumination, they can improve the quality of semantic labeling alongside RGB images. Consideration of both common and specific features of these two modalities improves the performance of semantic segmentation. One of the main problems in RGB-Depth semantic segmentation is how to fuse or combine these two modalities to achieve more advantages of each modality while being computationally efficient. Recently, the methods that encounter deep convolutional neural networks have reached the state-of-the-art results by early, late, and middle fusion strategies. In this paper, an efficient encoder-decoder model with the attention-based fusion block is proposed to integrate mutual influences between feature maps of these two modalities. This block explicitly extracts the interdependences among concatenated feature maps of these modalities to exploit more powerful feature maps from RGB-Depth images. The extensive experimental results on three main challenging datasets of NYU-V2, SUN RGB-D, and Stanford 2D-3D-Semantic show that the proposed network outperforms the state-of-the-art models with respect to computational cost as well as model size. Experimental results also illustrate the effectiveness of the proposed lightweight attention-based fusion model in terms of accuracy.



### Extreme Relative Pose Network under Hybrid Representations
- **Arxiv ID**: http://arxiv.org/abs/1912.11695v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11695v2)
- **Published**: 2019-12-25 17:12:52+00:00
- **Updated**: 2020-04-03 18:14:22+00:00
- **Authors**: Zhenpei Yang, Siming Yan, Qixing Huang
- **Comment**: Accepted in CVPR 2020
- **Journal**: None
- **Summary**: In this paper, we introduce a novel RGB-D based relative pose estimation approach that is suitable for small-overlapping or non-overlapping scans and can output multiple relative poses. Our method performs scene completion and matches the completed scans. However, instead of using a fixed representation for completion, the key idea is to utilize hybrid representations that combine 360-image, 2D image-based layout, and planar patches. This approach offers adaptively feature representations for relative pose estimation. Besides, we introduce a global-2-local matching procedure, which utilizes initial relative poses obtained during the global phase to detect and then integrate geometric relations for pose refinement. Experimental results justify the potential of this approach across a wide range of benchmark datasets. For example, on ScanNet, the rotation translation errors of the top-1/top-5 predictions of our approach are 28.6/0.90m and 16.8/0.76m, respectively. Our approach also considerably boosts the performance of multi-scan reconstruction in few-view reconstruction settings.



### Controllable and Progressive Image Extrapolation
- **Arxiv ID**: http://arxiv.org/abs/1912.11711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11711v1)
- **Published**: 2019-12-25 19:22:02+00:00
- **Updated**: 2019-12-25 19:22:02+00:00
- **Authors**: Yijun Li, Lu Jiang, Ming-Hsuan Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Image extrapolation aims at expanding the narrow field of view of a given image patch. Existing models mainly deal with natural scene images of homogeneous regions and have no control of the content generation process. In this work, we study conditional image extrapolation to synthesize new images guided by the input structured text. The text is represented as a graph to specify the objects and their spatial relation to the unknown regions of the image. Inspired by drawing techniques, we propose a progressive generative model of three stages, i.e., generating a coarse bounding-boxes layout, refining it to a finer segmentation layout, and mapping the layout to a realistic output. Such a multi-stage design is shown to facilitate the training process and generate more controllable results. We validate the effectiveness of the proposed method on the face and human clothing dataset in terms of visual results, quantitative evaluations and flexible controls.



### A Closer Look at Mobile App Usage as a Persistent Biometric: A Small Case Study
- **Arxiv ID**: http://arxiv.org/abs/1912.11721v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11721v1)
- **Published**: 2019-12-25 22:11:24+00:00
- **Updated**: 2019-12-25 22:11:24+00:00
- **Authors**: Md A. Noor, G. Kaptan, V. Cherukupally, P. Gera, T. Neal
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore mobile app use as a behavioral biometric identifier. While several efforts have also taken on this challenge, many have alluded to the inconsistency in human behavior, resulting in updating the biometric template frequently and periodically. Here, we represent app usage as simple images wherein each pixel value provides some information about the user's app usage. Then, we feed use these images to train a deep learning network (convolutional neural net) to classify the user's identity. Our contribution lies in the random order in which the images are fed to the classifier, thereby presenting novel evidence that there are some aspects of app usage that are indeed persistent. Our results yield a 96.8% $F$-score without any updates to the template data.



