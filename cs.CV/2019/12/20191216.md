# Arxiv Papers in cs.CV on 2019-12-16
### Semantic Segmentation for Compound figures
- **Arxiv ID**: http://arxiv.org/abs/1912.07142v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07142v3)
- **Published**: 2019-12-16 00:42:06+00:00
- **Updated**: 2021-02-04 17:37:44+00:00
- **Authors**: Weixin Jiang, Eric Schwenker, Maria Chan, Oliver Cossairt
- **Comment**: This paper has been withdrawn by the authors. This paper has been
  superseded by arXiv:2101.09903
- **Journal**: None
- **Summary**: Scientific literature contains large volumes of unstructured data,with over 30\% of figures constructed as a combination of multiple images, these compound figures cannot be analyzed directly with existing information retrieval tools. In this paper, we propose a semantic segmentation approach for compound figure separation, decomposing the compound figures into "master images". Each master image is one part of a compound figure governed by a subfigure label (typically "(a), (b), (c), etc"). In this way, the separated subfigures can be easily associated with the description information in the caption. In particular, we propose an anchor-based master image detection algorithm, which leverages the correlation between master images and subfigure labels and locates the master images in a two-step manner. First, a subfigure label detector is built to extract the global layout information of the compound figure. Second, the layout information is combined with local features to locate the master images. We validate the effectiveness of proposed method on our labeled testing dataset both quantitatively and qualitatively.



### Predicting the Future: A Jointly Learnt Model for Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/1912.07148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07148v1)
- **Published**: 2019-12-16 01:03:45+00:00
- **Updated**: 2019-12-16 01:03:45+00:00
- **Authors**: Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.



### Single Image Deraining: From Model-Based to Data-Driven and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1912.07150v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07150v2)
- **Published**: 2019-12-16 01:17:05+00:00
- **Updated**: 2019-12-27 18:48:55+00:00
- **Authors**: Wenhan Yang, Robby T. Tan, Shiqi Wang, Yuming Fang, Jiaying Liu
- **Comment**: https://flyywh.github.io/Single_rain_removal_survey/
- **Journal**: None
- **Summary**: The goal of single-image deraining is to restore the rain-free background scenes of an image degraded by rain streaks and rain accumulation. The early single-image deraining methods employ a cost function, where various priors are developed to represent the properties of rain and background layers. Since 2017, single-image deraining methods step into a deep-learning era, and exploit various types of networks, i.e. convolutional neural networks, recurrent neural networks, generative adversarial networks, etc., demonstrating impressive performance. Given the current rapid development, in this paper, we provide a comprehensive survey of deraining methods over the last decade. We summarize the rain appearance models, and discuss two categories of deraining approaches: model-based and data-driven approaches. For the former, we organize the literature based on their basic models and priors. For the latter, we discuss developed ideas related to architectures, constraints, loss functions, and training datasets. We present milestones of single-image deraining methods, review a broad selection of previous works in different categories, and provide insights on the historical development route from the model-based to data-driven methods. We also summarize performance comparisons quantitatively and qualitatively. Beyond discussing the technicality of deraining methods, we also discuss the future directions.



### DAmageNet: A Universal Adversarial Dataset
- **Arxiv ID**: http://arxiv.org/abs/1912.07160v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07160v1)
- **Published**: 2019-12-16 02:11:24+00:00
- **Updated**: 2019-12-16 02:11:24+00:00
- **Authors**: Sizhe Chen, Xiaolin Huang, Zhengbao He, Chengjin Sun
- **Comment**: None
- **Journal**: None
- **Summary**: It is now well known that deep neural networks (DNNs) are vulnerable to adversarial attack. Adversarial samples are similar to the clean ones, but are able to cheat the attacked DNN to produce incorrect predictions in high confidence. But most of the existing adversarial attacks have high success rate only when the information of the attacked DNN is well-known or could be estimated by massive queries. A promising way is to generate adversarial samples with high transferability. By this way, we generate 96020 transferable adversarial samples from original ones in ImageNet. The average difference, measured by root means squared deviation, is only around 3.8 on average. However, the adversarial samples are misclassified by various models with an error rate up to 90\%. Since the images are generated independently with the attacked DNNs, this is essentially zero-query adversarial attack. We call the dataset \emph{DAmageNet}, which is the first universal adversarial dataset that beats many models trained in ImageNet. By finding the drawbacks, DAmageNet could serve as a benchmark to study and improve robustness of DNNs. DAmageNet could be downloaded in http://www.pami.sjtu.edu.cn/Show/56/122.



### Transductive Zero-Shot Learning for 3D Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.07161v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07161v2)
- **Published**: 2019-12-16 02:24:10+00:00
- **Updated**: 2019-12-23 01:18:30+00:00
- **Authors**: Ali Cheraghian, Shafin Rahman, Dylan Campbell, Lars Petersson
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: Zero-shot learning, the task of learning to recognize new classes not seen during training, has received considerable attention in the case of 2D image classification. However despite the increasing ubiquity of 3D sensors, the corresponding 3D point cloud classification problem has not been meaningfully explored and introduces new challenges. This paper extends, for the first time, transductive Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) approaches to the domain of 3D point cloud classification. To this end, a novel triplet loss is developed that takes advantage of unlabeled test data. While designed for the task of 3D point cloud classification, the method is also shown to be applicable to the more common use-case of 2D image classification. An extensive set of experiments is carried out, establishing state-of-the-art for ZSL and GZSL in the 3D point cloud domain, as well as demonstrating the applicability of the approach to the image domain.



### Internal-transfer Weighting of Multi-task Learning for Lung Cancer Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.07167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07167v1)
- **Published**: 2019-12-16 02:46:27+00:00
- **Updated**: 2019-12-16 02:46:27+00:00
- **Authors**: Yiyuan Yang, Riqiang Gao, Yucheng Tang, Sanja L. Antic, Steve Deppen, Yuankai Huo, Kim L. Sandler, Pierre P. Massion, Bennett A. Landman
- **Comment**: Accepted by Medical Imaging, SPIE2020
- **Journal**: None
- **Summary**: Recently, multi-task networks have shown to both offer additional estimation capabilities, and, perhaps more importantly, increased performance over single-task networks on a "main/primary" task. However, balancing the optimization criteria of multi-task networks across different tasks is an area of active exploration. Here, we extend a previously proposed 3D attention-based network with four additional multi-task subnetworks for the detection of lung cancer and four auxiliary tasks (diagnosis of asthma, chronic bronchitis, chronic obstructive pulmonary disease, and emphysema). We introduce and evaluate a learning policy, Periodic Focusing Learning Policy (PFLP), that alternates the dominance of tasks throughout the training. To improve performance on the primary task, we propose an Internal-Transfer Weighting (ITW) strategy to suppress the loss functions on auxiliary tasks for the final stages of training. To evaluate this approach, we examined 3386 patients (single scan per patient) from the National Lung Screening Trial (NLST) and de-identified data from the Vanderbilt Lung Screening Program, with a 2517/277/592 (scans) split for training, validation, and testing. Baseline networks include a single-task strategy and a multi-task strategy without adaptive weights (PFLP/ITW), while primary experiments are multi-task trials with either PFLP or ITW or both. On the test set for lung cancer prediction, the baseline single-task network achieved prediction AUC of 0.8080 and the multi-task baseline failed to converge (AUC 0.6720). However, applying PFLP helped multi-task network clarify and achieved test set lung cancer prediction AUC of 0.8402. Furthermore, our ITW technique boosted the PFLP enabled multi-task network and achieved an AUC of 0.8462 (McNemar test, p < 0.01).



### MTRNet++: One-stage Mask-based Scene Text Eraser
- **Arxiv ID**: http://arxiv.org/abs/1912.07183v2
- **DOI**: 10.1016/j.cviu.2020.103066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07183v2)
- **Published**: 2019-12-16 04:11:55+00:00
- **Updated**: 2020-06-04 06:44:45+00:00
- **Authors**: Osman Tursun, Simon Denman, Rui Zeng, Sabesan Sivapalan, Sridha Sridharan, Clinton Fookes
- **Comment**: This paper is under CVIU review (after major revision)
- **Journal**: None
- **Summary**: A precise, controllable, interpretable and easily trainable text removal approach is necessary for both user-specific and large-scale text removal applications. To achieve this, we propose a one-stage mask-based text inpainting network, MTRNet++. It has a novel architecture that includes mask-refine, coarse-inpainting and fine-inpainting branches, and attention blocks. With this architecture, MTRNet++ can remove text either with or without an external mask. It achieves state-of-the-art results on both the Oxford and SCUT datasets without using external ground-truth masks. The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effective and essential. It also demonstrates controllability and interpretability.



### PixelRL: Fully Convolutional Network with Reinforcement Learning for Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1912.07190v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07190v1)
- **Published**: 2019-12-16 04:42:37+00:00
- **Updated**: 2019-12-16 04:42:37+00:00
- **Authors**: Ryosuke Furuta, Naoto Inoue, Toshihiko Yamasaki
- **Comment**: To appear in IEEE Transactions on Multimedia (TMM), Special Issue on
  Multimedia Computing with Interpretable Machine Learning. Extended version of
  our paper in AAAI 2019 (arXiv:1811.04323)
- **Journal**: None
- **Summary**: This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep reinforcement learning (RL) for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. Besides, it is possible to visualize what kind of operation is employed for each pixel at each iteration, which would help us understand why and how such an operation is chosen. We also believe that our technology can enhance the explainability and interpretability of the deep neural networks. In addition, because the operations executed at each pixels are visualized, we can change or modify the operations if necessary. We apply the proposed method to a variety of image processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven image editing. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning. The source code is available on https://github.com/rfuruta/pixelRL.



### Subjective Quality Assessment of Ground-based Camera Images
- **Arxiv ID**: http://arxiv.org/abs/1912.07192v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07192v1)
- **Published**: 2019-12-16 04:49:32+00:00
- **Updated**: 2019-12-16 04:49:32+00:00
- **Authors**: Lucie Lévêque, Soumyabrata Dev, Murhaf Hossari, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in Proc. Progress In Electromagnetics Research Symposium
  (PIERS), 2019
- **Journal**: None
- **Summary**: Image quality assessment is critical to control and maintain the perceived quality of visual content. Both subjective and objective evaluations can be utilised, however, subjective image quality assessment is currently considered the most reliable approach. Databases containing distorted images and mean opinion scores are needed in the field of atmospheric research with a view to improve the current state-of-the-art methodologies. In this paper, we focus on using ground-based sky camera images to understand the atmospheric events. We present a new image quality assessment dataset containing original and distorted nighttime images of sky/cloud from SWINSEG database. Subjective quality assessment was carried out in controlled conditions, as recommended by the ITU. Statistical analyses of the subjective scores showed the impact of noise type and distortion level on the perceived quality.



### Fingerprint Synthesis: Search with 100 Million Prints
- **Arxiv ID**: http://arxiv.org/abs/1912.07195v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07195v3)
- **Published**: 2019-12-16 05:09:52+00:00
- **Updated**: 2020-06-12 18:10:12+00:00
- **Authors**: Vishesh Mistry, Joshua J. Engelsma, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Evaluation of large-scale fingerprint search algorithms has been limited due to lack of publicly available datasets. To address this problem, we utilize a Generative Adversarial Network (GAN) to synthesize a fingerprint dataset consisting of 100 million fingerprint images. In contrast to existing fingerprint synthesis algorithms, we incorporate an identity loss which guides the generator to synthesize fingerprints corresponding to more distinct identities. The characteristics of our synthesized fingerprints are shown to be more similar to real fingerprints than existing methods via eight different metrics (minutiae count - block and template, minutiae direction - block and template, minutiae convex hull area, minutiae spatial distribution, block minutiae quality distribution, and NFIQ 2.0 scores). Additionally, the synthetic fingerprints based on our approach are shown to be more distinct than synthetic fingerprints based on published methods through search results and imposter distribution statistics. Finally, we report for the first time in open literature, search accuracy against a gallery of 100 million fingerprint images (NIST SD4 Rank-1 accuracy of 89.7%).



### Dense Recurrent Neural Networks for Accelerated MRI: History-Cognizant Unrolling of Optimization Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1912.07197v2
- **DOI**: 10.1109/JSTSP.2020.3003170
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.07197v2)
- **Published**: 2019-12-16 05:20:19+00:00
- **Updated**: 2020-07-08 05:00:51+00:00
- **Authors**: Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, Mingyi Hong, Mehmet Akçakaya
- **Comment**: None
- **Journal**: IEEE Journal of Selected Topics in Signal Processing, 2020
- **Summary**: Inverse problems for accelerated MRI typically incorporate domain-specific knowledge about the forward encoding operator in a regularized reconstruction framework. Recently physics-driven deep learning (DL) methods have been proposed to use neural networks for data-driven regularization. These methods unroll iterative optimization algorithms to solve the inverse problem objective function, by alternating between domain-specific data consistency and data-driven regularization via neural networks. The whole unrolled network is then trained end-to-end to learn the parameters of the network. Due to simplicity of data consistency updates with gradient descent steps, proximal gradient descent (PGD) is a common approach to unroll physics-driven DL reconstruction methods. However, PGD methods have slow convergence rates, necessitating a higher number of unrolled iterations, leading to memory issues in training and slower reconstruction times in testing. Inspired by efficient variants of PGD methods that use a history of the previous iterates, we propose a history-cognizant unrolling of the optimization algorithm with dense connections across iterations for improved performance. In our approach, the gradient descent steps are calculated at a trainable combination of the outputs of all the previous regularization units. We also apply this idea to unrolling variable splitting methods with quadratic relaxation. Our results in reconstruction of the fastMRI knee dataset show that the proposed history-cognizant approach reduces residual aliasing artifacts compared to its conventional unrolled counterpart without requiring extra computational power or increasing reconstruction time.



### A Broader Study of Cross-Domain Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.07200v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07200v3)
- **Published**: 2019-12-16 05:29:07+00:00
- **Updated**: 2020-07-17 21:33:27+00:00
- **Authors**: Yunhui Guo, Noel C. Codella, Leonid Karlinsky, James V. Codella, John R. Smith, Kate Saenko, Tajana Rosing, Rogerio Feris
- **Comment**: ECCV 2020. Website: https://www.learning-with-limited-labels.com/
- **Journal**: None
- **Summary**: Recent progress on few-shot learning largely relies on annotated data for meta-learning: base classes sampled from the same domain as the novel classes. However, in many applications, collecting data for meta-learning is infeasible or impossible. This leads to the cross-domain few-shot learning problem, where there is a large shift between base and novel class domains. While investigations of the cross-domain few-shot scenario exist, these works are limited to natural images that still contain a high degree of visual similarity. No work yet exists that examines few-shot learning across different imaging methods seen in real world scenarios, such as aerial and medical imaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, consisting of image data from a diverse assortment of image acquisition methods. This includes natural images, such as crop disease images, but additionally those that present with an increasing dissimilarity to natural images, such as satellite images, dermatology images, and radiology images. Extensive experiments on the proposed benchmark are performed to evaluate state-of-art meta-learning approaches, transfer learning approaches, and newer methods for cross-domain few-shot learning. The results demonstrate that state-of-art meta-learning methods are surprisingly outperformed by earlier meta-learning approaches, and all meta-learning methods underperform in relation to simple fine-tuning by 12.8% average accuracy. Performance gains previously observed with methods specialized for cross-domain few-shot learning vanish in this more challenging benchmark. Finally, accuracy of all methods tend to correlate with dataset similarity to natural images, verifying the value of the benchmark to better represent the diversity of data seen in practice and guiding future research.



### Automated Thalamic Nuclei Segmentation Using Multi-Planar Cascaded Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.07209v2
- **DOI**: 10.1016/j.mri.2020.08.005
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07209v2)
- **Published**: 2019-12-16 05:57:33+00:00
- **Updated**: 2020-06-17 21:06:00+00:00
- **Authors**: Mohammad S Majdi, Mahesh B Keerthivasan, Brian K Rutt, Natalie M Zahr, Jeffrey J Rodriguez, Manojkumar Saranathan
- **Comment**: Submitted to Magnetic Resonance Imaging. 34 pages, 6 figures , 2
  tables, 1 supporting figures, 2 supporting tables. Magnetic Resonance Imaging
  (2020)
- **Journal**: None
- **Summary**: A cascaded multi-planar scheme with a modified residual U-Net architecture was used to segment thalamic nuclei on conventional and white-matter-nulled (WMn) magnetization prepared rapid gradient echo (MPRAGE) data. A single network was optimized to work with images from healthy controls and patients with multiple sclerosis (MS) and essential tremor (ET), acquired at both 3T and 7T field strengths. Dice similarity coefficient and volume similarity index (VSI) were used to evaluate performance. Clinical utility was demonstrated by applying this method to study the effect of MS on thalamic nuclei atrophy. Segmentation of each thalamus into twelve nuclei was achieved in under a minute. For 7T WMn-MPRAGE, the proposed method outperforms current state-of-the-art on patients with ET with statistically significant improvements in Dice for five nuclei (increase in the range of 0.05-0.18) and VSI for four nuclei (increase in the range of 0.05-0.19), while performing comparably for healthy and MS subjects. Dice and VSI achieved using 7T WMn-MPRAGE data are comparable to those using 3T WMn-MPRAGE data. For conventional MPRAGE, the proposed method shows a statistically significant Dice improvement in the range of 0.14-0.63 over FreeSurfer for all nuclei and disease types. Effect of noise on network performance shows robustness to images with SNR as low as half the baseline SNR. Atrophy of four thalamic nuclei and whole thalamus was observed for MS patients compared to healthy control subjects, after controlling for the effect of parallel imaging, intracranial volume, gender, and age (p<0.004). The proposed segmentation method is fast, accurate, performs well across disease types and field strengths, and shows great potential for improving our understanding of thalamic nuclei involvement in neurological diseases.



### FISR: Deep Joint Frame Interpolation and Super-Resolution with a Multi-scale Temporal Loss
- **Arxiv ID**: http://arxiv.org/abs/1912.07213v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07213v2)
- **Published**: 2019-12-16 06:11:44+00:00
- **Updated**: 2022-02-07 02:16:58+00:00
- **Authors**: Soo Ye Kim, Jihyong Oh, Munchurl Kim
- **Comment**: The first two authors contributed equally to this work. Accepted to
  AAAI 2020 (camera-ready version)
- **Journal**: None
- **Summary**: Super-resolution (SR) has been widely used to convert low-resolution legacy videos to high-resolution (HR) ones, to suit the increasing resolution of displays (e.g. UHD TVs). However, it becomes easier for humans to notice motion artifacts (e.g. motion judder) in HR videos being rendered on larger-sized display devices. Thus, broadcasting standards support higher frame rates for UHD (Ultra High Definition) videos (4K@60 fps, 8K@120 fps), meaning that applying SR only is insufficient to produce genuine high quality videos. Hence, to up-convert legacy videos for realistic applications, not only SR but also video frame interpolation (VFI) is necessitated. In this paper, we first propose a joint VFI-SR framework for up-scaling the spatio-temporal resolution of videos from 2K 30 fps to 4K 60 fps. For this, we propose a novel training scheme with a multi-scale temporal loss that imposes temporal regularization on the input video sequence, which can be applied to any general video-related task. The proposed structure is analyzed in depth with extensive experiments.



### Domain Knowledge Based Brain Tumor Segmentation and Overall Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.07224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07224v1)
- **Published**: 2019-12-16 07:21:57+00:00
- **Updated**: 2019-12-16 07:21:57+00:00
- **Authors**: Xiaoqing Guo, Chen Yang, Pak Lun Lam, Peter Y. M. Woo, Yixuan Yuan
- **Comment**: 11 pages, 5 figures, BrainLes 2019
- **Journal**: None
- **Summary**: Automatically segmenting sub-regions of gliomas (necrosis, edema and enhancing tumor) and accurately predicting overall survival (OS) time from multimodal MRI sequences have important clinical significance in diagnosis, prognosis and treatment of gliomas. However, due to the high degree variations of heterogeneous appearance and individual physical state, the segmentation of sub-regions and OS prediction are very challenging. To deal with these challenges, we utilize a 3D dilated multi-fiber network (DMFNet) with weighted dice loss for brain tumor segmentation, which incorporates prior volume statistic knowledge and obtains a balance between small and large objects in MRI scans. For OS prediction, we propose a DenseNet based 3D neural network with position encoding convolutional layer (PECL) to extract meaningful features from T1 contrast MRI, T2 MRI and previously segmented subregions. Both labeled data and unlabeled data are utilized to prevent over-fitting for semi-supervised learning. Those learned deep features along with handcrafted features (such as ages, volume of tumor) and position encoding segmentation features are fed to a Gradient Boosting Decision Tree (GBDT) to predict a specific OS day



### Towards Omni-Supervised Face Alignment for Large Scale Unlabeled Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.07243v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07243v1)
- **Published**: 2019-12-16 08:34:10+00:00
- **Updated**: 2019-12-16 08:34:10+00:00
- **Authors**: Congcong Zhu, Hao Liu, Zhenhua Yu, Xuehong Sun
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: In this paper, we propose a spatial-temporal relational reasoning networks (STRRN) approach to investigate the problem of omni-supervised face alignment in videos. Unlike existing fully supervised methods which rely on numerous annotations by hand, our learner exploits large scale unlabeled videos plus available labeled data to generate auxiliary plausible training annotations. Motivated by the fact that neighbouring facial landmarks are usually correlated and coherent across consecutive frames, our approach automatically reasons about discriminative spatial-temporal relationships among landmarks for stable face tracking. Specifically, we carefully develop an interpretable and efficient network module, which disentangles facial geometry relationship for every static frame and simultaneously enforces the bi-directional cycle-consistency across adjacent frames, thus allowing the modeling of intrinsic spatial-temporal relations from raw face sequences. Extensive experimental results demonstrate that our approach surpasses the performance of most fully supervised state-of-the-arts.



### Mimetics: Towards Understanding Human Actions Out of Context
- **Arxiv ID**: http://arxiv.org/abs/1912.07249v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07249v3)
- **Published**: 2019-12-16 08:49:39+00:00
- **Updated**: 2021-02-02 14:20:11+00:00
- **Authors**: Philippe Weinzaepfel, Grégory Rogez
- **Comment**: None
- **Journal**: None
- **Summary**: Recent methods for video action recognition have reached outstanding performances on existing benchmarks. However, they tend to leverage context such as scenes or objects instead of focusing on understanding the human action itself. For instance, a tennis field leads to the prediction playing tennis irrespectively of the actions performed in the video. In contrast, humans have a more complete understanding of actions and can recognize them without context. The best example of out-of-context actions are mimes, that people can typically recognize despite missing relevant objects and scenes. In this paper, we propose to benchmark action recognition methods in such absence of context and introduce a novel dataset, Mimetics, consisting of mimed actions for a subset of 50 classes from the Kinetics benchmark. Our experiments show that (a) state-of-the-art 3D convolutional neural networks obtain disappointing results on such videos, highlighting the lack of true understanding of the human actions and (b) models leveraging body language via human pose are less prone to context biases. In particular, we show that applying a shallow neural network with a single temporal convolution over body pose features transferred to the action recognition problem performs surprisingly well compared to 3D action recognition methods.



### PolSF: PolSAR image dataset on San Francisco
- **Arxiv ID**: http://arxiv.org/abs/1912.07259v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07259v1)
- **Published**: 2019-12-16 09:33:05+00:00
- **Updated**: 2019-12-16 09:33:05+00:00
- **Authors**: Xu Liu, Licheng Jiao, Fang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Polarimetric SAR data has the characteristics of all-weather, all-time and so on, which is widely used in many fields. However, the data of annotation is relatively small, which is not conducive to our research. In this paper, we have collected five open polarimetric SAR images, which are images of the San Francisco area. These five images come from different satellites at different times, which has great scientific research value. We annotate the collected images at the pixel level for image classification and segmentation. For the convenience of researchers, the annotated data is open source https://github.com/liuxuvip/PolSF.



### A Hybrid Approach and Unified Framework for Bibliographic Reference Extraction
- **Arxiv ID**: http://arxiv.org/abs/1912.07266v2
- **DOI**: 10.1109/ACCESS.2020.3042455
- **Categories**: **cs.CV**, cs.DL
- **Links**: [PDF](http://arxiv.org/pdf/1912.07266v2)
- **Published**: 2019-12-16 09:47:50+00:00
- **Updated**: 2020-10-08 14:28:26+00:00
- **Authors**: Syed Tahseen Raza Rizvi, Andreas Dengel, Sheraz Ahmed
- **Comment**: None
- **Journal**: None
- **Summary**: Publications are an integral part in a scientific community. Bibliographic reference extraction from scientific publication is a challenging task due to diversity in referencing styles and document layout. Existing methods perform sufficiently on one dataset however, applying these solutions to a different dataset proves to be challenging. Therefore, a generic solution was anticipated which could overcome the limitations of the previous approaches. The contribution of this paper is three-fold. First, it presents a novel approach called DeepBiRD which is inspired by human visual perception and exploits layout features to identify individual references in a scientific publication. Second, we release a large dataset for image-based reference detection with 2401 scans containing 38863 references, all manually annotated for individual reference. Third, we present a unified and highly configurable end-to-end automatic bibliographic reference extraction framework called BRExSys which employs DeepBiRD along with state-of-the-art text-based models to detect and visualize references from a bibliographic document. Our proposed approach pre-processes the images in which a hybrid representation is obtained by processing the given image using different computer vision techniques. Then, it performs layout driven reference detection using Mask R-CNN on a given scientific publication. DeepBiRD was evaluated on two different datasets to demonstrate the generalization of this approach. The proposed system achieved an AP50 of 98.56% on our dataset. DeepBiRD significantly outperformed the current state-of-the-art approach on their dataset. Therefore, suggesting that DeepBiRD is significantly superior in performance, generalized, and independent of any domain or referencing style.



### A Sparse Representation Based Joint Demosaicing Method for Single-Chip Polarized Color Sensor
- **Arxiv ID**: http://arxiv.org/abs/1912.07308v2
- **DOI**: 10.1109/TIP.2021.3069190
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07308v2)
- **Published**: 2019-12-16 12:01:24+00:00
- **Updated**: 2021-04-08 02:33:20+00:00
- **Authors**: Sijia Wen, Yinqiang Zheng, Feng Lu
- **Comment**: 10 pages, 7 figures
- **Journal**: IEEE Transactions on Image Processing 2021
- **Summary**: The emergence of the single-chip polarized color sensor now allows for simultaneously capturing chromatic and polarimetric information of the scene on a monochromatic image plane. However, unlike the usual camera with an embedded demosaicing method, the latest polarized color camera is not delivered with an in-built demosaicing tool. For demosaicing, the users have to down-sample the captured images or to use traditional interpolation techniques. Neither of them can perform well since the polarization and color are interdependent. Therefore, joint chromatic and polarimetric demosaicing is the key to obtaining high-quality polarized color images. In this paper, we propose a joint chromatic and polarimetric demosaicing model to address this challenging problem. Instead of mechanically demosaicing for the multi-channel polarized color image, we further present a sparse representation-based optimization strategy that utilizes chromatic information and polarimetric information to jointly optimize the model. To avoid the interaction between color and polarization during demosaicing, we separately construct the corresponding dictionaries. We also build an optical data acquisition system to collect a dataset, which contains various sources of polarization, such as illumination, reflectance and birefringence. Results of both qualitative and quantitative experiments have shown that our method is capable of faithfully recovering full RGB information of four polarization angles for each pixel from a single mosaic input image. Moreover, the proposed method can perform well not only on the synthetic data but the real captured data.



### Evaluating Usage of Images for App Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.12144v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/1912.12144v1)
- **Published**: 2019-12-16 12:27:02+00:00
- **Updated**: 2019-12-16 12:27:02+00:00
- **Authors**: Kushal Singla, Niloy Mukherjee, Hari Manassery Koduvely, Joy Bose
- **Comment**: 5 pages, 3 figures, 3 tables, INDICON conference
- **Journal**: None
- **Summary**: App classification is useful in a number of applications such as adding apps to an app store or building a user model based on the installed apps. Presently there are a number of existing methods to classify apps based on a given taxonomy on the basis of their text metadata. However, text based methods for app classification may not work in all cases, such as when the text descriptions are in a different language, or missing, or inadequate to classify the app. One solution in such cases is to utilize the app images to supplement the text description. In this paper, we evaluate a number of approaches in which app images can be used to classify the apps. In one approach, we use Optical character recognition (OCR) to extract text from images, which is then used to supplement the text description of the app. In another, we use pic2vec to convert the app images into vectors, then train an SVM to classify the vectors to the correct app label. In another, we use the captionbot.ai tool to generate natural language descriptions from the app images. Finally, we use a method to detect and label objects in the app images and use a voting technique to determine the category of the app based on all the images. We compare the performance of our image-based techniques to classify a number of apps in our dataset. We use a text based SVM app classifier as our base and obtained an improved classification accuracy of 96% for some classes when app images are added.



### Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax
- **Arxiv ID**: http://arxiv.org/abs/1912.07329v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07329v4)
- **Published**: 2019-12-16 13:00:32+00:00
- **Updated**: 2021-04-06 11:42:47+00:00
- **Authors**: Karan Jakhar, Avneet Kaur, Dr. Meenu Gupta
- **Comment**: Will be updated later on. Somethings need to be updated and corrected
- **Journal**: None
- **Summary**: Computer vision has shown promising results in medical image processing. Pneumothorax is a deadly condition and if not diagnosed and treated at time then it causes death. It can be diagnosed with chest X-ray images. We need an expert and experienced radiologist to predict whether a person is suffering from pneumothorax or not by looking at the chest X-ray images. Everyone does not have access to such a facility. Moreover, in some cases, we need quick diagnoses. So we propose an image segmentation model to predict and give the output a mask that will assist the doctor in taking this crucial decision. Deep Learning has proved their worth in many areas and outperformed man state-of-the-art models. We want to use the power of these deep learning model to solve this problem. We have used U-net [13] architecture with ResNet [17] as a backbone and achieved promising results. U-net [13] performs very well in medical image processing and semantic segmentation. Our problem falls in the semantic segmentation category.



### ConvPoseCNN: Dense Convolutional 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.07333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07333v1)
- **Published**: 2019-12-16 13:12:23+00:00
- **Updated**: 2019-12-16 13:12:23+00:00
- **Authors**: Catherine Capellen, Max Schwarz, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: 6D object pose estimation is a prerequisite for many applications. In recent years, monocular pose estimation has attracted much research interest because it does not need depth measurements. In this work, we introduce ConvPoseCNN, a fully convolutional architecture that avoids cutting out individual objects. Instead we propose pixel-wise, dense prediction of both translation and orientation components of the object pose, where the dense orientation is represented in Quaternion form. We present different approaches for aggregation of the dense orientation predictions, including averaging and clustering schemes. We evaluate ConvPoseCNN on the challenging YCB-Video Dataset, where we show that the approach has far fewer parameters and trains faster than comparable methods without sacrificing accuracy. Furthermore, our results indicate that the dense orientation prediction implicitly learns to attend to trustworthy, occlusion-free, and feature-rich object regions.



### Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision
- **Arxiv ID**: http://arxiv.org/abs/1912.07372v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07372v2)
- **Published**: 2019-12-16 13:59:33+00:00
- **Updated**: 2020-03-23 14:10:27+00:00
- **Authors**: Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.



### Efficient Error-Tolerant Quantized Neural Network Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1912.07394v1
- **DOI**: 10.1109/DFT.2019.8875314
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07394v1)
- **Published**: 2019-12-16 14:25:13+00:00
- **Updated**: 2019-12-16 14:25:13+00:00
- **Authors**: Giulio Gambardella, Johannes Kappauf, Michaela Blott, Christoph Doehring, Martin Kumm, Peter Zipf, Kees Vissers
- **Comment**: 6 pages, 5 figures
- **Journal**: 2019 IEEE International Symposium on Defect and Fault Tolerance in
  VLSI and Nanotechnology Systems (DFT)
- **Summary**: Neural Networks are currently one of the most widely deployed machine learning algorithms. In particular, Convolutional Neural Networks (CNNs), are gaining popularity and are evaluated for deployment in safety critical applications such as self driving vehicles. Modern CNNs feature enormous memory bandwidth and high computational needs, challenging existing hardware platforms to meet throughput, latency and power requirements. Functional safety and error tolerance need to be considered as additional requirement in safety critical systems. In general, fault tolerant operation can be achieved by adding redundancy to the system, which is further exacerbating the computational demands. Furthermore, the question arises whether pruning and quantization methods for performance scaling turn out to be counterproductive with regards to fail safety requirements. In this work we present a methodology to evaluate the impact of permanent faults affecting Quantized Neural Networks (QNNs) and how to effectively decrease their effects in hardware accelerators. We use FPGA-based hardware accelerated error injection, in order to enable the fast evaluation. A detailed analysis is presented showing that QNNs containing convolutional layers are by far not as robust to faults as commonly believed and can lead to accuracy drops of up to 10%. To circumvent that, we propose two different methods to increase their robustness: 1) selective channel replication which adds significantly less redundancy than used by the common triple modular redundancy and 2) a fault-aware scheduling of processing elements for folded implementations



### Accuracy comparison across face recognition algorithms: Where are we on measuring race bias?
- **Arxiv ID**: http://arxiv.org/abs/1912.07398v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07398v2)
- **Published**: 2019-12-16 14:27:10+00:00
- **Updated**: 2020-06-04 15:50:26+00:00
- **Authors**: Jacqueline G. Cavazos, P. Jonathon Phillips, Carlos D. Castillo, Alice J. O'Toole
- **Comment**: None
- **Journal**: None
- **Summary**: Previous generations of face recognition algorithms differ in accuracy for images of different races (race bias). Here, we present the possible underlying factors (data-driven and scenario modeling) and methodological considerations for assessing race bias in algorithms. We discuss data driven factors (e.g., image quality, image population statistics, and algorithm architecture), and scenario modeling factors that consider the role of the "user" of the algorithm (e.g., threshold decisions and demographic constraints). To illustrate how these issues apply, we present data from four face recognition algorithms (a previous-generation algorithm and three deep convolutional neural networks, DCNNs) for East Asian and Caucasian faces. First, dataset difficulty affected both overall recognition accuracy and race bias, such that race bias increased with item difficulty. Second, for all four algorithms, the degree of bias varied depending on the identification decision threshold. To achieve equal false accept rates (FARs), East Asian faces required higher identification thresholds than Caucasian faces, for all algorithms. Third, demographic constraints on the formulation of the distributions used in the test, impacted estimates of algorithm accuracy. We conclude that race bias needs to be measured for individual applications and we provide a checklist for measuring this bias in face recognition algorithms.



### Formula Derivation and Analysis of the VINS-Mono
- **Arxiv ID**: http://arxiv.org/abs/1912.11986v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11986v2)
- **Published**: 2019-12-16 14:34:02+00:00
- **Updated**: 2020-09-19 05:16:30+00:00
- **Authors**: Yibin Wu
- **Comment**: None
- **Journal**: None
- **Summary**: The VINS-Mono is a monocular visual-inertial 6 DOF state estimator proposed by Aerial Robotics Group of HKUST in 2017. It can be performed on MAVs, smartphones and many other intelligent platforms. Because of the excellent robustness, accuracy and scalability, it has gained extensive attention worldwide. In this manuscript, the main equations including IMU pre-integration, visual-inertial co-initialization and tightly-coupled nonlinear optimization are derived and analyzed.



### Learning Canonical Representations for Scene Graph to Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1912.07414v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07414v5)
- **Published**: 2019-12-16 14:39:45+00:00
- **Updated**: 2020-08-24 12:29:45+00:00
- **Authors**: Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, Amir Globerson
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images. Previous approaches showed that scenes with few entities can be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases. In this work, we show that one limitation of current methods is their inability to capture semantic equivalence in graphs. We present a novel model that addresses these issues by learning canonical graph representations from the data, resulting in improved image generation for complex visual scenes. Our model demonstrates improved empirical performance on large scene graphs, robustness to noise in the input scene graph, and generalization on semantically equivalent graphs. Finally, we show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.



### MetaFusion: Controlled False-Negative Reduction of Minority Classes in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.07420v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07420v1)
- **Published**: 2019-12-16 14:44:36+00:00
- **Updated**: 2019-12-16 14:44:36+00:00
- **Authors**: Robin Chan, Matthias Rottmann, Fabian Hüger, Peter Schlicht, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: In semantic segmentation datasets, classes of high importance are oftentimes underrepresented, e.g., humans in street scenes. Neural networks are usually trained to reduce the overall number of errors, attaching identical loss to errors of all kinds. However, this is not necessarily aligned with human intuition. For instance, an overlooked pedestrian seems more severe than an incorrectly detected one. One possible remedy is to deploy different decision rules by introducing class priors which assigns larger weight to underrepresented classes. While reducing the false-negatives of the underrepresented class, at the same time this leads to a considerable increase of false-positive indications. In this work, we combine decision rules with methods for false-positive detection. We therefore fuse false-negative detection with uncertainty based false-positive meta classification. We present proof-of-concept results for CIFAR-10, and prove the efficiency of our method for the semantic segmentation of street scenes on the Cityscapes dataset based on predicted instances of the 'human' class. In the latter we employ an advanced false-positive detection method using uncertainty measures aggregated over instances. We thereby achieve improved trade-offs between false-negative and false-positive samples of the underrepresented classes.



### Comparisonal study of Deep Learning approaches on Retinal OCT Image
- **Arxiv ID**: http://arxiv.org/abs/1912.07783v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07783v1)
- **Published**: 2019-12-16 15:09:11+00:00
- **Updated**: 2019-12-16 15:09:11+00:00
- **Authors**: Nowshin Tasnim, Mahmudul Hasan, Ishrak Islam
- **Comment**: Poster in International Conference on Innovation in Engineering and
  Technology (ICIET) 23-24 December, 2019
- **Journal**: None
- **Summary**: In medical science, the use of computer science in disease detection and diagnosis is gaining popularity. Previously, the detection of disease used to take a significant amount of time and was less reliable. Machine learning (ML) techniques employed in recent biomedical researches are making revolutionary changes by gaining higher accuracy with more concise timing. At present, it is even possible to automatically detect diseases from the scanned images with the help of ML. In this research, we have taken such an attempt to detect retinal diseases from optical coherence tomography (OCT) X-ray images. Here, we propose a deep learning (DL) based approach in detecting retinal diseases from OCT images which can identify three conditions of the retina. Four different models used in this approach are compared with each other. On the test set, the detection accuracy is 98.00\% for a vanilla convolutional neural network (CNN) model, 99.07\% for Xception model, 97.00\% for ResNet50 model, and 99.17\% for MobileNetV2 model. The MobileNetV2 model acquires the highest accuracy, and the closest to the highest is the Xception model. The proposed approach has a potential impact on creating a tool for automatically detecting retinal diseases.



### Progressive Learning Algorithm for Efficient Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.07447v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07447v2)
- **Published**: 2019-12-16 15:32:01+00:00
- **Updated**: 2020-11-23 22:08:16+00:00
- **Authors**: Zhen Li, Hanyang Shao, Nian Xue, Liang Niu, LiangLiang Cao
- **Comment**: ICPR2020
- **Journal**: None
- **Summary**: This paper studies the problem of Person Re-Identification (ReID)for large-scale applications. Recent research efforts have been devoted to building complicated part models, which introduce considerably high computational cost and memory consumption, inhibiting its practicability in large-scale applications. This paper aims to develop a novel learning strategy to find efficient feature embeddings while maintaining the balance of accuracy and model complexity. More specifically, we find by enhancing the classical triplet loss together with cross-entropy loss, our method can explore the hard examples and build a discriminant feature embedding yet compact enough for large-scale applications. Our method is carried out progressively using Bayesian optimization, and we call it the Progressive Learning Algorithm (PLA). Extensive experiments on three large-scale datasets show that our PLA is comparable or better than the-state-of-the-arts. Especially, on the challenging Market-1501 dataset, we achieve Rank-1=94.7\%/mAP=89.4\% while saving at least 30\% parameters than strong part models.



### Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08061v2
- **DOI**: 10.1016/j.cmpb.2021.106225
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08061v2)
- **Published**: 2019-12-16 16:04:29+00:00
- **Updated**: 2021-06-17 15:58:15+00:00
- **Authors**: Gourav Modanwal, Adithya Vellal, Maciej A. Mazurowski
- **Comment**: Final accepted draft in Computer Methods and Programs in Biomedicine
- **Journal**: Computer Methods and Programs in Biomedicine, 2021
- **Summary**: Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used to complement ultrasound examinations and x-ray mammography during the early detection and diagnosis of breast cancer. However, images generated by various MRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise distribution, preventing algorithms trained on MRIs from one scanner to generalize to data from other scanners successfully. We propose a method for image normalization to solve this problem. MRI normalization is challenging because it requires both normalizing intensity values and mapping between the noise distributions of different scanners. We utilize a cycle-consistent generative adversarial network to learn a bidirectional mapping between MRIs produced by GE Healthcare and Siemens scanners. This allows us learning the mapping between two different scanner types without matched data, which is not commonly available. To ensure the preservation of breast shape and structures within the breast, we propose two technical innovations. First, we incorporate a mutual information loss with the CycleGAN architecture to ensure that the structure of the breast is maintained. Second, we propose a modified discriminator architecture which utilizes a smaller field-of-view to ensure the preservation of finer details in the breast tissue. Quantitative and qualitative evaluations show that the second proposed method was able to consistently preserve a high level of detail in the breast structure while also performing the proper intensity normalization and noise mapping. Our results demonstrate that the proposed model can successfully learn a bidirectional mapping between MRIs produced by different vendors, potentially enabling improved accuracy of downstream computational algorithms for diagnosis and detection of breast cancer. All the data used in this study are publicly available.



### Image Manipulation with Natural Language using Two-sidedAttentive Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1912.07478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07478v1)
- **Published**: 2019-12-16 16:21:13+00:00
- **Updated**: 2019-12-16 16:21:13+00:00
- **Authors**: Dawei Zhu, Aditya Mogadala, Dietrich Klakow
- **Comment**: Submitted to Journal
- **Journal**: None
- **Summary**: Altering the content of an image with photo editing tools is a tedious task for an inexperienced user. Especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. Therefore, in this paper, we address the challenge of manipulating images using natural language description. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images while preserving other contents such as background intact. TEA-cGAN uses fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB and Oxford-102 datasets both quantitatively and qualitatively.



### Lung and Colon Cancer Histopathological Image Dataset (LC25000)
- **Arxiv ID**: http://arxiv.org/abs/1912.12142v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.12142v1)
- **Published**: 2019-12-16 16:28:00+00:00
- **Updated**: 2019-12-16 16:28:00+00:00
- **Authors**: Andrew A. Borkowski, Marilyn M. Bui, L. Brannon Thomas, Catherine P. Wilson, Lauren A. DeLand, Stephen M. Mastorides
- **Comment**: 2 pages
- **Journal**: None
- **Summary**: The field of Machine Learning, a subset of Artificial Intelligence, has led to remarkable advancements in many areas, including medicine. Machine Learning algorithms require large datasets to train computer models successfully. Although there are medical image datasets available, more image datasets are needed from a variety of medical entities, especially cancer pathology. Even more scarce are ML-ready image datasets. To address this need, we created an image dataset (LC25000) with 25,000 color images in 5 classes. Each class contains 5,000 images of the following histologic entities: colon adenocarcinoma, benign colonic tissue, lung adenocarcinoma, lung squamous cell carcinoma, and benign lung tissue. All images are de-identified, HIPAA compliant, validated, and freely available for download to AI researchers.



### Learning a Neural Solver for Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.07515v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07515v2)
- **Published**: 2019-12-16 17:27:55+00:00
- **Updated**: 2020-04-18 10:08:23+00:00
- **Authors**: Guillem Brasó, Laura Leal-Taixé
- **Comment**: Accepted to CVPR 2020 (oral)
- **Journal**: None
- **Summary**: Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such \textit{structured domain} is not trivial. As a consequence, most learning-based work has been devoted to learning better features for MOT, and then using these with well-established optimization frameworks. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks (MPNs). By operating directly on the graph domain, our method can reason globally over an entire set of detections and predict final solutions. Hence, we show that learning in MOT does not need to be restricted to feature extraction, but it can also be applied to the data association step. We show a significant improvement in both MOTA and IDF1 on three publicly available benchmarks. Our code is available at https://bit.ly/motsolv .



### Zoom in to where it matters: a hierarchical graph based model for mammogram analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.07517v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07517v1)
- **Published**: 2019-12-16 17:28:17+00:00
- **Updated**: 2019-12-16 17:28:17+00:00
- **Authors**: Hao Du, Jiashi Feng, Mengling Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In clinical practice, human radiologists actually review medical images with high resolution monitors and zoom into region of interests (ROIs) for a close-up examination. Inspired by this observation, we propose a hierarchical graph neural network to detect abnormal lesions from medical images by automatically zooming into ROIs. We focus on mammogram analysis for breast cancer diagnosis for this study. Our proposed network consist of two graph attention networks performing two tasks: (1) node classification to predict whether to zoom into next level; (2) graph classification to classify whether a mammogram is normal/benign or malignant. The model is trained and evaluated on INbreast dataset and we obtain comparable AUC with state-of-the-art methods.



### Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing
- **Arxiv ID**: http://arxiv.org/abs/1912.07538v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07538v3)
- **Published**: 2019-12-16 17:45:01+00:00
- **Updated**: 2020-05-29 08:58:11+00:00
- **Authors**: Vedika Agarwal, Rakshith Shetty, Mario Fritz
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Despite significant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deficiencies in models and datasets, today's models often rely on correlations rather than predictions that are causal w.r.t. data. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made significantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance.



### A Transferable Adaptive Domain Adversarial Neural Network for Virtual Reality Augmented EMG-Based Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.09380v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09380v2)
- **Published**: 2019-12-16 18:41:56+00:00
- **Updated**: 2021-02-14 14:55:11+00:00
- **Authors**: Ulysse Côté-Allard, Gabriel Gagnon-Turcotte, Angkoon Phinyomark, Kyrre Glette, Erik Scheme, François Laviolette, Benoit Gosselin
- **Comment**: 10 Pages. The last three authors shared senior authorship
- **Journal**: None
- **Summary**: Within the field of electromyography-based (EMG) gesture recognition, disparities exist between the offline accuracy reported in the literature and the real-time usability of a classifier. This gap mainly stems from two factors: 1) The absence of a controller, making the data collected dissimilar to actual control. 2) The difficulty of including the four main dynamic factors (gesture intensity, limb position, electrode shift, and transient changes in the signal), as including their permutations drastically increases the amount of data to be recorded. Contrarily, online datasets are limited to the exact EMG-based controller used to record them, necessitating the recording of a new dataset for each control method or variant to be tested. Consequently, this paper proposes a new type of dataset to serve as an intermediate between offline and online datasets, by recording the data using a real-time experimental protocol. The protocol, performed in virtual reality, includes the four main dynamic factors and uses an EMG-independent controller to guide movements. This EMG-independent feedback ensures that the user is in-the-loop during recording, while enabling the resulting dynamic dataset to be used as an EMG-based benchmark. The dataset is comprised of 20 able-bodied participants completing three to four sessions over a period of 14 to 21 days. The ability of the dynamic dataset to serve as a benchmark is leveraged to evaluate the impact of different recalibration techniques for long-term (across-day) gesture recognition, including a novel algorithm, named TADANN. TADANN consistently and significantly (p<0.05) outperforms using fine-tuning as the recalibration technique.



### Rethinking Medical Image Reconstruction via Shape Prior, Going Deeper and Faster: Deep Joint Indirect Registration and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.07648v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07648v1)
- **Published**: 2019-12-16 19:28:52+00:00
- **Updated**: 2019-12-16 19:28:52+00:00
- **Authors**: Jiulong Liu, Angelica I. Aviles-Rivero, Hui Ji, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: Indirect image registration is a promising technique to improve image reconstruction quality by providing a shape prior for the reconstruction task. In this paper, we propose a novel hybrid method that seeks to reconstruct high quality images from few measurements whilst requiring low computational cost. With this purpose, our framework intertwines indirect registration and reconstruction tasks is a single functional. It is based on two major novelties. Firstly, we introduce a model based on deep nets to solve the indirect registration problem, in which the inversion and registration mappings are recurrently connected through a fixed-point interaction based sparse optimisation. Secondly, we introduce specific inversion blocks, that use the explicit physical forward operator, to map the acquired measurements to the image reconstruction. We also introduce registration blocks based deep nets to predict the registration parameters and warp transformation accurately and efficiently. We demonstrate, through extensive numerical and visual experiments, that our framework outperforms significantly classic reconstruction schemes and other bi-task method; this in terms of both image quality and computational time. Finally, we show generalisation capabilities of our approach by demonstrating their performance on fast Magnetic Resonance Imaging (MRI), sparse view computed tomography (CT) and low dose CT with measurements much below the Nyquist limit.



### UNAS: Differentiable Architecture Search Meets Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.07651v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07651v2)
- **Published**: 2019-12-16 19:31:39+00:00
- **Updated**: 2020-08-27 21:48:42+00:00
- **Authors**: Arash Vahdat, Arun Mallya, Ming-Yu Liu, Jan Kautz
- **Comment**: Accepted to CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Neural architecture search (NAS) aims to discover network architectures with desired properties such as high accuracy or low latency. Recently, differentiable NAS (DNAS) has demonstrated promising results while maintaining a search cost orders of magnitude lower than reinforcement learning (RL) based NAS. However, DNAS models can only optimize differentiable loss functions in search, and they require an accurate differentiable approximation of non-differentiable criteria. In this work, we present UNAS, a unified framework for NAS, that encapsulates recent DNAS and RL-based approaches under one framework. Our framework brings the best of both worlds, and it enables us to search for architectures with both differentiable and non-differentiable criteria in one unified framework while maintaining a low search cost. Further, we introduce a new objective function for search based on the generalization gap that prevents the selection of architectures prone to overfitting. We present extensive experiments on the CIFAR-10, CIFAR-100, and ImageNet datasets and we perform search in two fundamentally different search spaces. We show that UNAS obtains the state-of-the-art average accuracy on all three datasets when compared to the architectures searched in the DARTS space. Moreover, we show that UNAS can find an efficient and accurate architecture in the ProxylessNAS search space, that outperforms existing MobileNetV2 based architectures. The source code is available at https://github.com/NVlabs/unas .



### Self-Supervised Learning of Physics-Guided Reconstruction Neural Networks without Fully-Sampled Reference Data
- **Arxiv ID**: http://arxiv.org/abs/1912.07669v2
- **DOI**: 10.1002/mrm.28378
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.07669v2)
- **Published**: 2019-12-16 20:04:02+00:00
- **Updated**: 2020-04-14 19:13:15+00:00
- **Authors**: Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta Ellermann, Kâmil Uğurbil, Mehmet Akçakaya
- **Comment**: This work is an extension of our previous work arXiv:1910.09116
- **Journal**: Magnetic Resonance in Medicine, 2020
- **Summary**: Purpose: To develop a strategy for training a physics-guided MRI reconstruction neural network without a database of fully-sampled datasets. Theory and Methods: Self-supervised learning via data under-sampling (SSDU) for physics-guided deep learning (DL) reconstruction partitions available measurements into two disjoint sets, one of which is used in the data consistency units in the unrolled network and the other is used to define the loss for training. The proposed training without fully-sampled data is compared to fully-supervised training with ground-truth data, as well as conventional compressed sensing and parallel imaging methods using the publicly available fastMRI knee database. The same physics-guided neural network is used for both proposed SSDU and supervised training. The SSDU training is also applied to prospectively 2-fold accelerated high-resolution brain datasets at different acceleration rates, and compared to parallel imaging. Results: Results on five different knee sequences at acceleration rate of 4 shows that proposed self-supervised approach performs closely with supervised learning, while significantly outperforming conventional compressed sensing and parallel imaging, as characterized by quantitative metrics and a clinical reader study. The results on prospectively sub-sampled brain datasets, where supervised learning cannot be employed due to lack of ground-truth reference, show that the proposed self-supervised approach successfully perform reconstruction at high acceleration rates (4, 6 and 8). Image readings indicate improved visual reconstruction quality with the proposed approach compared to parallel imaging at acquisition acceleration. Conclusion: The proposed SSDU approach allows training of physics-guided DL-MRI reconstruction without fully-sampled data, while achieving comparable results with supervised DL-MRI trained on fully-sampled data.



### Automating Vitiligo Skin Lesion Segmentation Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08350v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08350v1)
- **Published**: 2019-12-16 20:15:44+00:00
- **Updated**: 2019-12-16 20:15:44+00:00
- **Authors**: Makena Low, Priyanka Raina
- **Comment**: None
- **Journal**: None
- **Summary**: For several skin conditions such as vitiligo, accurate segmentation of lesions from skin images is the primary measure of disease progression and severity. Existing methods for vitiligo lesion segmentation require manual intervention. Unfortunately, manual segmentation is time and labor-intensive, as well as irreproducible between physicians. We introduce a convolutional neural network (CNN) that quickly and robustly performs vitiligo skin lesion segmentation. Our CNN has a U-Net architecture with a modified contracting path. We use the CNN to generate an initial segmentation of the lesion, then refine it by running the watershed algorithm on high-confidence pixels. We train the network on 247 images with a variety of lesion sizes, complexity, and anatomical sites. The network with our modifications noticeably outperforms the state-of-the-art U-Net, with a Jaccard Index (JI) score of 73.6% (compared to 36.7%). Moreover, our method requires only a few seconds for segmentation, in contrast with the previously proposed semi-autonomous watershed approach, which requires 2-29 minutes per image.



### Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy
- **Arxiv ID**: http://arxiv.org/abs/1912.07726v1
- **DOI**: 10.1145/3351095.3375709
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07726v1)
- **Published**: 2019-12-16 22:03:05+00:00
- **Updated**: 2019-12-16 22:03:05+00:00
- **Authors**: Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, Olga Russakovsky
- **Comment**: Accepted to FAT* 2020
- **Journal**: None
- **Summary**: Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the "person" subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.



### Evolution of Robust High Speed Optical-Flow-Based Landing for Autonomous MAVs
- **Arxiv ID**: http://arxiv.org/abs/1912.07735v1
- **DOI**: 10.1016/j.robot.2019.103380
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07735v1)
- **Published**: 2019-12-16 22:22:21+00:00
- **Updated**: 2019-12-16 22:22:21+00:00
- **Authors**: Kirk Y. W. Scheper, Guido C. H. E. de Croon
- **Comment**: This is an accepted manuscript preprint of published work available
  at https://doi.org/10.1016/j.robot.2019.103380. Please cite with Kirk Y.W
  Scheper and Guido C.H.E. de Croon, Evolution of robust high speed
  optical-flow-based landing for autonomous MAVs, Robotics and Autonomous
  Systems 124, 2020
- **Journal**: Robotics and Autonomous Systems, Volume 124, 2020, 103380, ISSN
  0921-8890
- **Summary**: Automatic optimization of robotic behavior has been the long-standing goal of Evolutionary Robotics. Allowing the problem at hand to be solved by automation often leads to novel approaches and new insights. A common problem encountered with this approach is that when this optimization occurs in a simulated environment, the optimized policies are subject to the reality gap when implemented in the real world. This often results in sub-optimal behavior, if it works at all. This paper investigates the automatic optimization of neurocontrollers to perform quick but safe landing maneuvers for a quadrotor micro air vehicle using the divergence of the optical flow field of a downward looking camera. The optimized policies showed that a piece-wise linear control scheme is more effective than the simple linear scheme commonly used, something not yet considered by human designers. Additionally, we show the utility in using abstraction on the input and output of the controller as a tool to improve the robustness of the optimized policies to the reality gap by testing our policies optimized in simulation on real world vehicles. We tested the neurocontrollers using two different methods to generate and process the visual input, one using a conventional CMOS camera and one a dynamic vision sensor, both of which perform significantly differently than the simulated sensor. The use of the abstracted input resulted in near seamless transfer to the real world with the controllers showing high robustness to a clear reality gap.



### CAG: A Real-time Low-cost Enhanced-robustness High-transferability Content-aware Adversarial Attack Generator
- **Arxiv ID**: http://arxiv.org/abs/1912.07742v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1912.07742v1)
- **Published**: 2019-12-16 22:48:38+00:00
- **Updated**: 2019-12-16 22:48:38+00:00
- **Authors**: Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial attack despite their tremendous success in many AI fields. Adversarial attack is a method that causes the intended misclassfication by adding imperceptible perturbations to legitimate inputs. Researchers have developed numerous types of adversarial attack methods. However, from the perspective of practical deployment, these methods suffer from several drawbacks such as long attack generating time, high memory cost, insufficient robustness and low transferability. We propose a Content-aware Adversarial Attack Generator (CAG) to achieve real-time, low-cost, enhanced-robustness and high-transferability adversarial attack. First, as a type of generative model-based attack, CAG shows significant speedup (at least 500 times) in generating adversarial examples compared to the state-of-the-art attacks such as PGD and C\&W. CAG only needs a single generative model to perform targeted attack to any targeted class. Because CAG encodes the label information into a trainable embedding layer, it differs from prior generative model-based adversarial attacks that use $n$ different copies of generative models for $n$ different targeted classes. As a result, CAG significantly reduces the required memory cost for generating adversarial examples. CAG can generate adversarial perturbations that focus on the critical areas of input by integrating the class activation maps information in the training process, and hence improve the robustness of CAG attack against the state-of-art adversarial defenses. In addition, CAG exhibits high transferability across different DNN classifier models in black-box attack scenario by introducing random dropout in the process of generating perturbations. Extensive experiments on different datasets and DNN models have verified the real-time, low-cost, enhanced-robustness, and high-transferability benefits of CAG.



### A hierarchical approach to deep learning and its application to tomographic reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.07743v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07743v1)
- **Published**: 2019-12-16 22:53:14+00:00
- **Updated**: 2019-12-16 22:53:14+00:00
- **Authors**: Lin Fu, Bruno De Man
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) has shown unprecedented performance for many image analysis and image enhancement tasks. Yet, solving large-scale inverse problems like tomographic reconstruction remains challenging for DL. These problems involve non-local and space-variant integral transforms between the input and output domains, for which no efficient neural network models have been found. A prior attempt to solve such problems with supervised learning relied on a brute-force fully connected network and applied it to reconstruction for a $128^4$ system matrix size. This cannot practically scale to realistic data sizes such as $512^4$ and $512^6$ for three-dimensional data sets. Here we present a novel framework to solve such problems with deep learning by casting the original problem as a continuum of intermediate representations between the input and output data. The original problem is broken down into a sequence of simpler transformations that can be well mapped onto an efficient hierarchical network architecture, with exponentially fewer parameters than a generic network would need. We applied the approach to computed tomography (CT) image reconstruction for a $512^4$ system matrix size. To our knowledge, this enabled the first data-driven DL solver for full-size CT reconstruction without relying on the structure of direct (analytical) or iterative (numerical) inversion techniques. The proposed approach is applicable to other imaging problems such as emission and magnetic resonance reconstruction. More broadly, hierarchical DL opens the door to a new class of solvers for general inverse problems, which could potentially lead to improved signal-to-noise ratio, spatial resolution and computational efficiency in various areas.



### PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points
- **Arxiv ID**: http://arxiv.org/abs/1912.07744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07744v1)
- **Published**: 2019-12-16 22:58:53+00:00
- **Updated**: 2019-12-16 22:58:53+00:00
- **Authors**: Siyuan Huang, Yixin Chen, Tao Yuan, Siyuan Qi, Yixin Zhu, Song-Chun Zhu
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Detecting 3D objects from a single RGB image is intrinsically ambiguous, thus requiring appropriate prior knowledge and intermediate representations as constraints to reduce the uncertainties and improve the consistencies between the 2D image plane and the 3D world coordinate. To address this challenge, we propose to adopt perspective points as a new intermediate representation for 3D object detection, defined as the 2D projections of local Manhattan 3D keypoints to locate an object; these perspective points satisfy geometric constraints imposed by the perspective projection. We further devise PerspectiveNet, an end-to-end trainable model that simultaneously detects the 2D bounding box, 2D perspective points, and 3D object bounding box for each object from a single RGB image. PerspectiveNet yields three unique advantages: (i) 3D object bounding boxes are estimated based on perspective points, bridging the gap between 2D and 3D bounding boxes without the need of category-specific 3D shape priors. (ii) It predicts the perspective points by a template-based method, and a perspective loss is formulated to maintain the perspective constraints. (iii) It maintains the consistency between the 2D perspective points and 3D bounding boxes via a differentiable projective function. Experiments on SUN RGB-D dataset show that the proposed method significantly outperforms existing RGB-based approaches for 3D object detection.



### PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1912.07745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1912.07745v1)
- **Published**: 2019-12-16 23:00:09+00:00
- **Updated**: 2019-12-16 23:00:09+00:00
- **Authors**: Janis Dalins, Campbell Wilson, Douglas Boudry
- **Comment**: Submitted to Journal of Digital Investigation 08 SEP 2019. Under
  review as at 13 December 2019
- **Journal**: None
- **Summary**: Efficient and reliable automated detection of modified image and multimedia files has long been a challenge for law enforcement, compounded by the harm caused by repeated exposure to psychologically harmful materials. In August 2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and video similarity measurement, respectively. In this report, we review the algorithms' performance on detecting commonly encountered transformations on real-world case data, sourced from contemporary investigations. We also provide a reference implementation to demonstrate the potential application and integration of such algorithms within existing law enforcement systems.



### MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking
- **Arxiv ID**: http://arxiv.org/abs/1912.07748v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07748v3)
- **Published**: 2019-12-16 23:14:56+00:00
- **Updated**: 2020-04-30 17:21:50+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan, Bhavya Kailkhura, Timo Bremer
- **Comment**: International Journal on Computer Vision's (IJCV) Special Issue on
  GANs
- **Journal**: None
- **Summary**: In the past few years, Generative Adversarial Networks (GANs) have dramatically advanced our ability to represent and parameterize high-dimensional, non-linear image manifolds. As a result, they have been widely adopted across a variety of applications, ranging from challenging inverse problems like image completion, to problems such as anomaly detection and adversarial defense. A recurring theme in many of these applications is the notion of projecting an image observation onto the manifold that is inferred by the generator. In this context, Projected Gradient Descent (PGD) has been the most popular approach, which essentially optimizes for a latent vector that minimizes the discrepancy between a generated image and the given observation. However, PGD is a brittle optimization technique that fails to identify the right projection (or latent vector) when the observation is corrupted, or perturbed even by a small amount. Such corruptions are common in the real world, for example images in the wild come with unknown crops, rotations, missing pixels, or other kinds of non-linear distributional shifts which break current encoding methods, rendering downstream applications unusable. To address this, we propose corruption mimicking -- a new robust projection technique, that utilizes a surrogate network to approximate the unknown corruption directly at test time, without the need for additional supervision or data augmentation. The proposed method is significantly more robust than PGD and other competing methods under a wide variety of corruptions, thereby enabling a more effective use of GANs in real-world applications. More importantly, we show that our approach produces state-of-the-art performance in several GAN-based applications -- anomaly detection, domain adaptation, and adversarial defense, that benefit from an accurate projection.



