# Arxiv Papers in cs.CV on 2019-12-20
### Non-congruent non-degenerate curves with identical signatures
- **Arxiv ID**: http://arxiv.org/abs/1912.09597v4
- **DOI**: 10.1007/s10851-020-01015-x
- **Categories**: **math.DG**, cs.CV, 53A04, 53A55, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.09597v4)
- **Published**: 2019-12-20 00:37:11+00:00
- **Updated**: 2021-07-22 04:01:20+00:00
- **Authors**: Eric Geiger, Irina A. Kogan
- **Comment**: 33 pages, 22 figures. Page 20: In the proof of Corollary 31 the
  notation for the length, $L_W$, of a reconstructed curve $\Gamma_W$ is
  introduced and defined. Page 23: The upper bound on the integral in equation
  (35) is updated to use $L_W$ instead of $L$ and the definition of $L_W$ is
  referred to. Page 23: The assumption "$m$ and $\xi$ are relatively prime" is
  added to Proposition 36
- **Journal**: Journal of Mathematical Imaging and Vision, 63, 601-625 (2021)
- **Summary**: While the equality of differential signatures (Calabi et al, Int. J. Comput. Vis. 26: 107-135, 1998) is known to be a necessary condition for congruence, it is not sufficient (Musso and Nicolodi, J. Math Imaging Vis. 35: 68-85, 2009). Hickman (J. Math Imaging Vis. 43: 206-213, 2012, Theorem 2) claimed that for non-degenerate planar curves, equality of Euclidean signatures implies congruence. We prove that while Hickman's claim holds for simple, closed curves with simple signatures, it fails for curves with non-simple signatures. In the later case, we associate a directed graph with the signature and show how various paths along the graph give rise to a family of non-congruent, non-degenerate curves with identical signatures. Using this additional structure, we formulate congruence criteria for non-degenerate, closed, simple curves and show how the paths reflect the global and local symmetries of the corresponding curve.



### Divide and Conquer: an Accurate Machine Learning Algorithm to Process Split Videos on a Parallel Processing Infrastructure
- **Arxiv ID**: http://arxiv.org/abs/1912.09601v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1912.09601v1)
- **Published**: 2019-12-20 00:51:32+00:00
- **Updated**: 2019-12-20 00:51:32+00:00
- **Authors**: Walter M. Mayor Toro, Juan C. Perafan Villota, Oscar H. Mondragon, Johan S. Obando Ceron
- **Comment**: 3 pages, 2 figures, LatinX in AI Research at NeurIPS 2019
- **Journal**: None
- **Summary**: Every day the number of traffic cameras in cities rapidly increase and huge amount of video data are generated. Parallel processing infrastruture, such as Hadoop, and programming models, such as MapReduce, are being used to promptly process that amount of data. The common approach for video processing by using Hadoop MapReduce is to process an entire video on only one node, however, in order to avoid parallelization problems, such as load imbalance, we propose to process videos by splitting it into equal parts and processing each resulting chunk on a different node. We used some machine learning techniques to detect and track the vehicles. However, video division may produce inaccurate results. To solve this problem we proposed a heuristic algorithm to avoid process a vehicle in more than one chunk.



### Understanding Deep Neural Network Predictions for Medical Imaging Applications
- **Arxiv ID**: http://arxiv.org/abs/1912.09621v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09621v1)
- **Published**: 2019-12-20 02:57:05+00:00
- **Updated**: 2019-12-20 02:57:05+00:00
- **Authors**: Barath Narayanan Narayanan, Manawaduge Supun De Silva, Russell C. Hardie, Nathan K. Kueterman, Redha Ali
- **Comment**: 20 pages, 28 Figures and 9 Tables
- **Journal**: None
- **Summary**: Computer-aided detection has been a research area attracting great interest in the past decade. Machine learning algorithms have been utilized extensively for this application as they provide a valuable second opinion to the doctors. Despite several machine learning models being available for medical imaging applications, not many have been implemented in the real-world due to the uninterpretable nature of the decisions made by the network. In this paper, we investigate the results provided by deep neural networks for the detection of malaria, diabetic retinopathy, brain tumor, and tuberculosis in different imaging modalities. We visualize the class activation mappings for all the applications in order to enhance the understanding of these networks. This type of visualization, along with the corresponding network performance metrics, would aid the data science experts in better understanding of their models as well as assisting doctors in their decision-making process.



### Learning Semantic Neural Tree for Human Parsing
- **Arxiv ID**: http://arxiv.org/abs/1912.09622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09622v1)
- **Published**: 2019-12-20 03:05:48+00:00
- **Updated**: 2019-12-20 03:05:48+00:00
- **Authors**: Ruyi Ji, Dawei Du, Libo Zhang, Longyin Wen, Yanjun Wu, Chen Zhao, Feiyue Huang, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: The majority of existing human parsing methods formulate the task as semantic segmentation, which regard each semantic category equally and fail to exploit the intrinsic physiological structure of human body, resulting in inaccurate results. In this paper, we design a novel semantic neural tree for human parsing, which uses a tree architecture to encode physiological structure of human body, and designs a coarse to fine process in a cascade manner to generate accurate results. Specifically, the semantic neural tree is designed to segment human regions into multiple semantic subregions (e.g., face, arms, and legs) in a hierarchical way using a new designed attention routing module. Meanwhile, we introduce the semantic aggregation module to combine multiple hierarchical features to exploit more context information for better performance. Our semantic neural tree can be trained in an end-to-end fashion by standard stochastic gradient descent (SGD) with back-propagation. Several experiments conducted on four challenging datasets for both single and multiple human parsing, i.e., LIP, PASCAL-Person-Part, CIHP and MHP-v2, demonstrate the effectiveness of the proposed method. Code can be found at https://isrc.iscas.ac.cn/gitlab/research/sematree.



### C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.09628v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09628v2)
- **Published**: 2019-12-20 03:39:50+00:00
- **Updated**: 2020-04-20 03:34:33+00:00
- **Authors**: Qihang Yu, Dong Yang, Holger Roth, Yutong Bai, Yixiao Zhang, Alan L. Yuille, Daguang Xu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: 3D convolution neural networks (CNN) have been proved very successful in parsing organs or tumours in 3D medical images, but it remains sophisticated and time-consuming to choose or design proper 3D networks given different task contexts. Recently, Neural Architecture Search (NAS) is proposed to solve this problem by searching for the best network architecture automatically. However, the inconsistency between search stage and deployment stage often exists in NAS algorithms due to memory constraints and large search space, which could become more serious when applying NAS to some memory and time consuming tasks, such as 3D medical image segmentation. In this paper, we propose coarse-to-fine neural architecture search (C2FNAS) to automatically search a 3D segmentation network from scratch without inconsistency on network size or input size. Specifically, we divide the search procedure into two stages: 1) the coarse stage, where we search the macro-level topology of the network, i.e. how each convolution module is connected to other modules; 2) the fine stage, where we search at micro-level for operations in each cell based on previous searched macro-level topology. The coarse-to-fine manner divides the search procedure into two consecutive stages and meanwhile resolves the inconsistency. We evaluate our method on 10 public datasets from Medical Segmentation Decalthon (MSD) challenge, and achieve state-of-the-art performance with the network searched using one dataset, which demonstrates the effectiveness and generalization of our searched models.



### Exploring the Capacity of an Orderless Box Discretization Network for Multi-orientation Scene Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.09629v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09629v3)
- **Published**: 2019-12-20 03:43:48+00:00
- **Updated**: 2021-01-16 03:47:29+00:00
- **Authors**: Yuliang Liu, Tong He, Hao Chen, Xinyu Wang, Canjie Luo, Shuaitao Zhang, Chunhua Shen, Lianwen Jin
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build state-of-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at https://git.io/TextDet.



### AutoScale: Learning to Scale for Crowd Counting and Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.09632v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09632v4)
- **Published**: 2019-12-20 03:54:17+00:00
- **Updated**: 2021-10-19 03:52:18+00:00
- **Authors**: Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, Masayoshi Tomizuka
- **Comment**: This work is accepted by IJCV. Code is available at
  \url{https://github.com/dk-liang/AutoScale.git}
- **Journal**: None
- **Summary**: Recent works on crowd counting mainly leverage CNNs to count by regressing density maps, and have achieved great progress. In the density map, each person is represented by a Gaussian blob, and the final count is obtained from the integration of the whole map. However, it is difficult to accurately predict the density map on dense regions. A major issue is that the density map on dense regions usually accumulates density values from a number of nearby Gaussian blobs, yielding different large density values on a small set of pixels. This makes the density map present variant patterns with significant pattern shifts and brings a long-tailed distribution of pixel-wise density values. We propose a simple and effective Learning to Scale (L2S) module, which automatically scales dense regions into reasonable closeness levels (reflecting image-plane distance between neighboring people). L2S directly normalizes the closeness in different patches such that it dynamically separates the overlapped blobs, decomposes the accumulated values in the ground-truth density map, and thus alleviates the pattern shifts and long-tailed distribution of density values. This helps the model to better learn the density map. We also explore the effectiveness of L2S in localizing people by finding the local minima of the quantized distance (w.r.t. person location map). To the best of our knowledge, such a localization method is also novel in localization-based crowd counting. We further introduce a customized dynamic cross-entropy loss, significantly improving the localization-based model optimization. Extensive experiments demonstrate that the proposed framework termed AutoScale improves upon some state-of-the-art methods in both regression and localization benchmarks on three crowded datasets and achieves very competitive performance on two sparse datasets.



### AtomNAS: Fine-Grained End-to-End Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.09640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09640v2)
- **Published**: 2019-12-20 04:42:43+00:00
- **Updated**: 2020-02-23 13:08:53+00:00
- **Authors**: Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille, Jianchao Yang
- **Comment**: ICLR 2020 camera ready version
- **Journal**: None
- **Summary**: Search space design is very critical to neural architecture search (NAS) algorithms. We propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, we propose a resource-aware architecture search framework which automatically assigns the computational resources (e.g., output channel numbers) for each operation by jointly considering the performance and the computational cost. In addition, to accelerate the search process, we propose a dynamic network shrinkage technique which prunes the atomic blocks with negligible influence on outputs on the fly. Instead of a search-and-retrain two-stage paradigm, our method simultaneously searches and trains the target architecture. Our method achieves state-of-the-art performance under several FLOPs configurations on ImageNet with a small searching cost. We open our entire codebase at: https://github.com/meijieru/AtomNAS.



### ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard
- **Arxiv ID**: http://arxiv.org/abs/1912.09641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09641v1)
- **Published**: 2019-12-20 04:50:17+00:00
- **Updated**: 2019-12-20 04:50:17+00:00
- **Authors**: Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, Xiang Bai, Baoguang Shi, Dimosthenis Karatzas, Shijian Lu, C. V. Jawahar
- **Comment**: International Conference on Document Analysis and Recognition, 2019
- **Journal**: None
- **Summary**: Chinese scene text reading is one of the most challenging problems in computer vision and has attracted great interest. Different from English text, Chinese has more than 6000 commonly used characters and Chinesecharacters can be arranged in various layouts with numerous fonts. The Chinese signboards in street view are a good choice for Chinese scene text images since they have different backgrounds, fonts and layouts. We organized a competition called ICDAR2019-ReCTS, which mainly focuses on reading Chinese text on signboard. This report presents the final results of the competition. A large-scale dataset of 25,000 annotated signboard images, in which all the text lines and characters are annotated with locations and transcriptions, were released. Four tasks, namely character recognition, text line recognition, text line detection and end-to-end recognition were set up. Besides, considering the Chinese text ambiguity issue, we proposed a multi ground truth (multi-GT) evaluation method to make evaluation fairer. The competition started on March 1, 2019 and ended on April 30, 2019. 262 submissions from 46 teams are received. Most of the participants come from universities, research institutes, and tech companies in China. There are also some participants from the United States, Australia, Singapore, and Korea. 21 teams submit results for Task 1, 23 teams submit results for Task 2, 24 teams submit results for Task 3, and 13 teams submit results for Task 4. The official website for the competition is http://rrc.cvc.uab.es/?ch=12.



### JSNet: Joint Instance and Semantic Segmentation of 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.09654v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09654v1)
- **Published**: 2019-12-20 06:30:08+00:00
- **Updated**: 2019-12-20 06:30:08+00:00
- **Authors**: Lin Zhao, Wenbing Tao
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: In this paper, we propose a novel joint instance and semantic segmentation approach, which is called JSNet, in order to address the instance and semantic segmentation of 3D point clouds simultaneously. Firstly, we build an effective backbone network to extract robust features from the raw point clouds. Secondly, to obtain more discriminative features, a point cloud feature fusion module is proposed to fuse the different layer features of the backbone network. Furthermore, a joint instance semantic segmentation module is developed to transform semantic features into instance embedding space, and then the transformed features are further fused with instance features to facilitate instance segmentation. Meanwhile, this module also aggregates instance features into semantic feature space to promote semantic segmentation. Finally, the instance predictions are generated by applying a simple mean-shift clustering on instance embeddings. As a result, we evaluate the proposed JSNet on a large-scale 3D indoor point cloud dataset S3DIS and a part dataset ShapeNet, and compare it with existing approaches. Experimental results demonstrate our approach outperforms the state-of-the-art method in 3D instance segmentation with a significant improvement in 3D semantic prediction and our method is also beneficial for part segmentation. The source code for this work is available at https://github.com/dlinzhao/JSNet.



### Random CapsNet Forest Model for Imbalanced Malware Type Classification Task
- **Arxiv ID**: http://arxiv.org/abs/1912.10836v4
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10836v4)
- **Published**: 2019-12-20 06:40:40+00:00
- **Updated**: 2020-08-23 20:21:04+00:00
- **Authors**: Aykut Çayır, Uğur Ünal, Hasan Dağ
- **Comment**: 30 pages, 10 figures, typos are corrected, references are added
- **Journal**: None
- **Summary**: Behavior of a malware varies with respect to malware types. Therefore,knowing type of a malware affects strategies of system protection softwares. Many malware type classification models empowered by machine and deep learning achieve superior accuracies to predict malware types.Machine learning based models need to do heavy feature engineering and feature engineering is dominantly effecting performance of models.On the other hand, deep learning based models require less feature engineering than machine learning based models. However, traditional deep learning architectures and components cause very complex and data sensitive models. Capsule network architecture minimizes this complexity and data sensitivity unlike classical convolutional neural network architectures. This paper proposes an ensemble capsule network model based on bootstrap aggregating technique. The proposed method are tested on two malware datasets, whose the-state-of-the-art results are well-known.



### AdaBits: Neural Network Quantization with Adaptive Bit-Widths
- **Arxiv ID**: http://arxiv.org/abs/1912.09666v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.09666v2)
- **Published**: 2019-12-20 07:10:23+00:00
- **Updated**: 2020-03-15 19:42:05+00:00
- **Authors**: Qing Jin, Linjie Yang, Zhenyu Liao
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Deep neural networks with adaptive configurations have gained increasing attention due to the instant and flexible deployment of these models on platforms with different resource budgets. In this paper, we investigate a novel option to achieve this goal by enabling adaptive bit-widths of weights and activations in the model. We first examine the benefits and challenges of training quantized model with adaptive bit-widths, and then experiment with several approaches including direct adaptation, progressive training and joint training. We discover that joint training is able to produce comparable performance on the adaptive model as individual models. We further propose a new technique named Switchable Clipping Level (S-CL) to further improve quantized models at the lowest bit-width. With our proposed techniques applied on a bunch of models including MobileNet-V1/V2 and ResNet-50, we demonstrate that bit-width of weights and activations is a new option for adaptively executable deep neural networks, offering a distinct opportunity for improved accuracy-efficiency trade-off as well as instant adaptation according to the platform constraints in real-world applications.



### Adversarial symmetric GANs: bridging adversarial samples and adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09670v5
- **DOI**: 10.1016/j.neunet.2020.10.016
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09670v5)
- **Published**: 2019-12-20 07:20:13+00:00
- **Updated**: 2020-11-14 01:05:21+00:00
- **Authors**: Faqiang Liu, Mingkun Xu, Guoqi Li, Jing Pei, Luping Shi, Rong Zhao
- **Comment**: None
- **Journal**: Neural networks,2020
- **Summary**: Generative adversarial networks have achieved remarkable performance on various tasks but suffer from training instability. Despite many training strategies proposed to improve training stability, this issue remains as a challenge. In this paper, we investigate the training instability from the perspective of adversarial samples and reveal that adversarial training on fake samples is implemented in vanilla GANs, but adversarial training on real samples has long been overlooked. Consequently, the discriminator is extremely vulnerable to adversarial perturbation and the gradient given by the discriminator contains non-informative adversarial noises, which hinders the generator from catching the pattern of real samples. Here, we develop adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of the discriminator on real samples into vanilla GANs, making adversarial training symmetrical. The discriminator is therefore more robust and provides more informative gradient with less adversarial noise, thereby stabilizing training and accelerating convergence. The effectiveness of the AS-GANs is verified on image generation on CIFAR-10 , CelebA, and LSUN with varied network architectures. Not only the training is more stabilized, but the FID scores of generated samples are consistently improved by a large margin compared to the baseline. The bridging of adversarial samples and adversarial networks provides a new approach to further develop adversarial networks.



### A Comprehensive Study and Comparison of Core Technologies for MPEG 3D Point Cloud Compression
- **Arxiv ID**: http://arxiv.org/abs/1912.09674v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09674v1)
- **Published**: 2019-12-20 07:23:20+00:00
- **Updated**: 2019-12-20 07:23:20+00:00
- **Authors**: Hao Liu, Hui Yuan, Qi Liu, Junhui Hou, Ju Liu
- **Comment**: 17pages, has been accepted by IEEE Transactions on Boradcasting
- **Journal**: None
- **Summary**: Point cloud based 3D visual representation is becoming popular due to its ability to exhibit the real world in a more comprehensive and immersive way. However, under a limited network bandwidth, it is very challenging to communicate this kind of media due to its huge data volume. Therefore, the MPEG have launched the standardization for point cloud compression (PCC), and proposed three model categories, i.e., TMC1, TMC2, and TMC3. Because the 3D geometry compression methods of TMC1 and TMC3 are similar, TMC1 and TMC3 are further merged into a new platform namely TMC13. In this paper, we first introduce some basic technologies that are usually used in 3D point cloud compression, then review the encoder architectures of these test models in detail, and finally analyze their rate distortion performance as well as complexity quantitatively for different cases (i.e., lossless geometry and lossless color, lossless geometry and lossy color, lossy geometry and lossy color) by using 16 benchmark 3D point clouds that are recommended by MPEG. Experimental results demonstrate that the coding efficiency of TMC2 is the best on average (especially for lossy geometry and lossy color compression) for dense point clouds while TMC13 achieves the optimal coding performance for sparse and noisy point clouds with lower time complexity.



### Spatial and Temporal Consistency-Aware Dynamic Adaptive Streaming for 360-Degree Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.09675v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09675v1)
- **Published**: 2019-12-20 07:41:02+00:00
- **Updated**: 2019-12-20 07:41:02+00:00
- **Authors**: Hui Yuan, Shiyun Zhao, Junhui Hou, Xuekai Wei, Sam Kwong
- **Comment**: 16 pages, This paper has been accepted by the IEEE Journal of
  Selected Topics in Signal Processing
- **Journal**: None
- **Summary**: The 360-degree video allows users to enjoy the whole scene by interactively switching viewports. However, the huge data volume of the 360-degree video limits its remote applications via network. To provide high quality of experience (QoE) for remote web users, this paper presents a tile-based adaptive streaming method for 360-degree videos. First, we propose a simple yet effective rate adaptation algorithm to determine the requested bitrate for downloading the current video segment by considering the balance between the buffer length and video quality. Then, we propose to use a Gaussian model to predict the field of view at the beginning of each requested video segment. To deal with the circumstance that the view angle is switched during the display of a video segment, we propose to download all the tiles in the 360-degree video with different priorities based on a Zipf model. Finally, in order to allocate bitrates for all the tiles, a two-stage optimization algorithm is proposed to preserve the quality of tiles in FoV and guarantee the spatial and temporal smoothness. Experimental results demonstrate the effectiveness and advantage of the proposed method compared with the state-of-the-art methods. That is, our method preserves both the quality and the smoothness of tiles in FoV, thus providing the best QoE for users.



### IRS: A Large Naturalistic Indoor Robotics Stereo Dataset to Train Deep Models for Disparity and Surface Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.09678v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.09678v2)
- **Published**: 2019-12-20 07:55:09+00:00
- **Updated**: 2021-03-26 13:58:20+00:00
- **Authors**: Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, Xiaowen Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Indoor robotics localization, navigation, and interaction heavily rely on scene understanding and reconstruction. Compared to the monocular vision which usually does not explicitly introduce any geometrical constraint, stereo vision-based schemes are more promising and robust to produce accurate geometrical information, such as surface normal and depth/disparity. Besides, deep learning models trained with large-scale datasets have shown their superior performance in many stereo vision tasks. However, existing stereo datasets rarely contain the high-quality surface normal and disparity ground truth, which hardly satisfies the demand of training a prospective deep model for indoor scenes. To this end, we introduce a large-scale synthetic but naturalistic indoor robotics stereo (IRS) dataset with over 100K stereo RGB images and high-quality surface normal and disparity maps. Leveraging the advanced rendering techniques of our customized rendering engine, the dataset is considerably close to the real-world captured images and covers several visual effects, such as brightness changes, light reflection/transmission, lens flare, vivid shadow, etc. We compare the data distribution of IRS with existing stereo datasets to illustrate the typical visual attributes of indoor scenes. Besides, we present DTN-Net, a two-stage deep model for surface normal estimation. Extensive experiments show the advantages and effectiveness of IRS in training deep models for disparity estimation, and DTN-Net provides state-of-the-art results for normal estimation compared to existing methods.



### Segmentations-Leak: Membership Inference Attacks and Defenses in Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.09685v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09685v2)
- **Published**: 2019-12-20 08:11:47+00:00
- **Updated**: 2020-09-19 22:06:44+00:00
- **Authors**: Yang He, Shadi Rahimian, Bernt Schiele, Mario Fritz
- **Comment**: Accepted to ECCV 2020. Code at:
  https://github.com/SSAW14/segmentation_membership_inference
- **Journal**: None
- **Summary**: Today's success of state of the art methods for semantic segmentation is driven by large datasets. Data is considered an important asset that needs to be protected, as the collection and annotation of such datasets comes at significant efforts and associated costs. In addition, visual data might contain private or sensitive information, that makes it equally unsuited for public release. Unfortunately, recent work on membership inference in the broader area of adversarial machine learning and inference attacks on machine learning models has shown that even black box classifiers leak information on the dataset that they were trained on. We show that such membership inference attacks can be successfully carried out on complex, state of the art models for semantic segmentation. In order to mitigate the associated risks, we also study a series of defenses against such membership inference attacks and find effective counter measures against the existing risks with little effect on the utility of the segmentation method. Finally, we extensively evaluate our attacks and defenses on a range of relevant real-world datasets: Cityscapes, BDD100K, and Mapillary Vistas.



### An Unsupervised Deep Learning Method for Multi-coil Cine MRI
- **Arxiv ID**: http://arxiv.org/abs/1912.12177v3
- **DOI**: 10.1088/1361-6560/abaffa
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12177v3)
- **Published**: 2019-12-20 08:19:41+00:00
- **Updated**: 2020-08-21 03:42:59+00:00
- **Authors**: Ziwen Ke, Jing Cheng, Leslie Ying, Hairong Zheng, Yanjie Zhu, Dong Liang
- **Comment**: 12 pages, 10 figures, submitted to PMB
- **Journal**: None
- **Summary**: Deep learning has achieved good success in cardiac magnetic resonance imaging (MRI) reconstruction, in which convolutional neural networks (CNNs) learn a mapping from the undersampled k-space to the fully sampled images. Although these deep learning methods can improve the reconstruction quality compared with iterative methods without requiring complex parameter selection or lengthy reconstruction time, the following issues still need to be addressed: 1) all these methods are based on big data and require a large amount of fully sampled MRI data, which is always difficult to obtain for cardiac MRI; 2) the effect of coil correlation on reconstruction in deep learning methods for dynamic MR imaging has never been studied. In this paper, we propose an unsupervised deep learning method for multi-coil cine MRI via a time-interleaved sampling strategy. Specifically, a time-interleaved acquisition scheme is utilized to build a set of fully encoded reference data by directly merging the k-space data of adjacent time frames. Then these fully encoded data can be used to train a parallel network for reconstructing images of each coil separately. Finally, the images from each coil are combined via a CNN to implicitly explore the correlations between coils. The comparisons with classic k-t FOCUSS, k-t SLR, L+S and KLR methods on in vivo datasets show that our method can achieve improved reconstruction results in an extremely short amount of time.



### Controllable Face Aging
- **Arxiv ID**: http://arxiv.org/abs/1912.09694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09694v1)
- **Published**: 2019-12-20 08:38:31+00:00
- **Updated**: 2019-12-20 08:38:31+00:00
- **Authors**: Haien Zeng, Hanjiang Lai, Jian Yin
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Motivated by the following two observations: 1) people are aging differently under different conditions for changeable facial attributes, e.g., skin color may become darker when working outside, and 2) it needs to keep some unchanged facial attributes during the aging process, e.g., race and gender, we propose a controllable face aging method via attribute disentanglement generative adversarial network. To offer fine control over the synthesized face images, first, an individual embedding of the face is directly learned from an image that contains the desired facial attribute. Second, since the image may contain other unwanted attributes, an attribute disentanglement network is used to separate the individual embedding and learn the common embedding that contains information about the face attribute (e.g., race). With the common embedding, we can manipulate the generated face image with the desired attribute in an explicit manner. Experimental results on two common benchmarks demonstrate that our proposed generator achieves comparable performance on the aging effect with state-of-the-art baselines while gaining more flexibility for attribute control. Code is available at supplementary material.



### DeepSFM: Structure From Motion Via Deep Bundle Adjustment
- **Arxiv ID**: http://arxiv.org/abs/1912.09697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09697v2)
- **Published**: 2019-12-20 08:47:41+00:00
- **Updated**: 2020-08-10 07:31:28+00:00
- **Authors**: Xingkui Wei, Yinda Zhang, Zhuwen Li, Yanwei Fu, Xiangyang Xue
- **Comment**: Accepted by ECCV2020(Oral)
- **Journal**: None
- **Summary**: Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. However, existing methods usually assume accurate camera poses either from GT or other methods, which is unrealistic in practice. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.



### Adversarial Representation Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.09720v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09720v1)
- **Published**: 2019-12-20 09:42:06+00:00
- **Updated**: 2019-12-20 09:42:06+00:00
- **Authors**: Ali Mottaghi, Serena Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Active learning aims to develop label-efficient algorithms by querying the most informative samples to be labeled by an oracle. The design of efficient training methods that require fewer labels is an important research direction that allows more effective use of computational and human resources for labeling and training deep neural networks. In this work, we demonstrate how we can use recent advances in deep generative models, to outperform the state-of-the-art in achieving the highest classification accuracy using as few labels as possible. Unlike previous approaches, our approach uses not only labeled images to train the classifier but also unlabeled images and generated images for co-training the whole model. Our experiments show that the proposed method significantly outperforms existing approaches in active learning on a wide range of datasets (MNIST, CIFAR-10, SVHN, CelebA, and ImageNet).



### Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.09745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09745v1)
- **Published**: 2019-12-20 10:39:07+00:00
- **Updated**: 2019-12-20 10:39:07+00:00
- **Authors**: Konstantinos Papadopoulos, Enjie Ghorbel, Djamila Aouada, Björn Ottersten
- **Comment**: None
- **Journal**: None
- **Summary**: This paper extends the Spatial-Temporal Graph Convolutional Network (ST-GCN) for skeleton-based action recognition by introducing two novel modules, namely, the Graph Vertex Feature Encoder (GVFE) and the Dilated Hierarchical Temporal Convolutional Network (DH-TCN). On the one hand, the GVFE module learns appropriate vertex features for action recognition by encoding raw skeleton data into a new feature space. On the other hand, the DH-TCN module is capable of capturing both short-term and long-term temporal dependencies using a hierarchical dilated convolutional network. Experiments have been conducted on the challenging NTU RGB-D-60 and NTU RGB-D 120 datasets. The obtained results show that our method competes with state-of-the-art approaches while using a smaller number of layers and parameters; thus reducing the required training time and memory.



### MFPN: A Novel Mixture Feature Pyramid Network of Multiple Architectures for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.09748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09748v1)
- **Published**: 2019-12-20 10:44:27+00:00
- **Updated**: 2019-12-20 10:44:27+00:00
- **Authors**: Tingting Liang, Yongtao Wang, Qijie Zhao, huan zhang, Zhi Tang, Haibin Ling
- **Comment**: 7 pages, 3figures
- **Journal**: None
- **Summary**: Feature pyramids are widely exploited in many detectors to solve the scale variation problem for object detection. In this paper, we first investigate the Feature Pyramid Network (FPN) architectures and briefly categorize them into three typical fashions: top-down, bottom-up and fusing-splitting, which have their own merits for detecting small objects, large objects, and medium-sized objects, respectively. Further, we design three FPNs of different architectures and propose a novel Mixture Feature Pyramid Network (MFPN) which inherits the merits of all these three kinds of FPNs, by assembling the three kinds of FPNs in a parallel multi-branch architecture and mixing the features. MFPN can significantly enhance both one-stage and two-stage FPN-based detectors with about 2 percent Average Precision(AP) increment on the MS-COCO benchmark, at little sacrifice in running time latency. By simply assembling MFPN with the one-stage and two-stage baseline detectors, we achieve competitive single-model detection results on the COCO detection benchmark without bells and whistles.



### Triple Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09784v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09784v2)
- **Published**: 2019-12-20 12:17:27+00:00
- **Updated**: 2020-09-14 10:09:32+00:00
- **Authors**: Chongxuan Li, Kun Xu, Jiashuo Liu, Jun Zhu, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a unified game-theoretical framework to perform classification and conditional image generation given limited supervision. It is formulated as a three-player minimax game consisting of a generator, a classifier and a discriminator, and therefore is referred to as Triple Generative Adversarial Network (Triple-GAN). The generator and the classifier characterize the conditional distributions between images and labels to perform conditional generation and classification, respectively. The discriminator solely focuses on identifying fake image-label pairs. Under a nonparametric assumption, we prove the unique equilibrium of the game is that the distributions characterized by the generator and the classifier converge to the data distribution. As a byproduct of the three-player mechanism, Triple-GAN is flexible to incorporate different semi-supervised classifiers and GAN architectures. We evaluate Triple-GAN in two challenging settings, namely, semi-supervised learning and the extreme low data regime. In both settings, Triple-GAN can achieve excellent classification results and generate meaningful samples in a specific class simultaneously. In particular, using a commonly adopted 13-layer CNN classifier, Triple-GAN outperforms extensive semi-supervised learning methods substantially on more than 10 benchmarks no matter data augmentation is applied or not.



### Taxonomy and Evaluation of Structured Compression of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09802v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09802v1)
- **Published**: 2019-12-20 13:11:44+00:00
- **Updated**: 2019-12-20 13:11:44+00:00
- **Authors**: Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen Blankevoort, Max Welling
- **Comment**: None
- **Journal**: None
- **Summary**: The success of deep neural networks in many real-world applications is leading to new challenges in building more efficient architectures. One effective way of making networks more efficient is neural network compression. We provide an overview of existing neural network compression methods that can be used to make neural networks more efficient by changing the architecture of the network. First, we introduce a new way to categorize all published compression methods, based on the amount of data and compute needed to make the methods work in practice. These are three 'levels of compression solutions'. Second, we provide a taxonomy of tensor factorization based and probabilistic compression methods. Finally, we perform an extensive evaluation of different compression techniques from the literature for models trained on ImageNet. We show that SVD and probabilistic compression or pruning methods are complementary and give the best results of all the considered methods. We also provide practical ways to combine them.



### Axial Attention in Multidimensional Transformers
- **Arxiv ID**: http://arxiv.org/abs/1912.12180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12180v1)
- **Published**: 2019-12-20 13:27:27+00:00
- **Updated**: 2019-12-20 13:27:27+00:00
- **Authors**: Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose Axial Transformers, a self-attention-based autoregressive model for images and other data organized as high dimensional tensors. Existing autoregressive models either suffer from excessively large computational resource requirements for high dimensional data, or make compromises in terms of distribution expressiveness or ease of implementation in order to decrease resource requirements. Our architecture, by contrast, maintains both full expressiveness over joint distributions over data and ease of implementation with standard deep learning frameworks, while requiring reasonable memory and computation and achieving state-of-the-art results on standard generative modeling benchmarks. Our models are based on axial attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.



### Unsupervised Few-shot Learning via Self-supervised Training
- **Arxiv ID**: http://arxiv.org/abs/1912.12178v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12178v1)
- **Published**: 2019-12-20 14:09:57+00:00
- **Updated**: 2019-12-20 14:09:57+00:00
- **Authors**: Zilong Ji, Xiaolong Zou, Tiejun Huang, Si Wu
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Learning from limited exemplars (few-shot learning) is a fundamental, unsolved problem that has been laboriously explored in the machine learning community. However, current few-shot learners are mostly supervised and rely heavily on a large amount of labeled examples. Unsupervised learning is a more natural procedure for cognitive mammals and has produced promising results in many machine learning tasks. In the current study, we develop a method to learn an unsupervised few-shot learner via self-supervised training (UFLST), which can effectively generalize to novel but related classes. The proposed model consists of two alternate processes, progressive clustering and episodic training. The former generates pseudo-labeled training examples for constructing episodic tasks; and the later trains the few-shot learner using the generated episodic tasks which further optimizes the feature representations of data. The two processes facilitate with each other, and eventually produce a high quality few-shot learner. Using the benchmark dataset Omniglot and Mini-ImageNet, we show that our model outperforms other unsupervised few-shot learning methods. Using the benchmark dataset Market1501, we further demonstrate the feasibility of our model to a real-world application on person re-identification.



### Transfer Learning with Edge Attention for Prostate MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.09847v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09847v1)
- **Published**: 2019-12-20 14:32:16+00:00
- **Updated**: 2019-12-20 14:32:16+00:00
- **Authors**: Xiangxiang Qin
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer is one of the common diseases in men, and it is the most common malignant tumor in developed countries. Studies have shown that the male prostate incidence rate is as high as 2.5% to 16%, Currently, the inci-dence of prostate cancer in Asia is lower than that in the West, but it is increas-ing rapidly. If prostate cancer can be found as early as possible and treated in time, it will have a high survival rate. Therefore, it is of great significance for the diagnosis and treatment of prostate cancer. In this paper, we propose a trans-fer learning method based on deep neural network for prostate MRI segmenta-tion. In addition, we design a multi-level edge attention module using wavelet decomposition to overcome the difficulty of ambiguous boundary in prostate MRI segmentation tasks. The prostate images were provided by MICCAI Grand Challenge-Prostate MR Image Segmentation 2012 (PROMISE 12) challenge dataset.



### Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.09857v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09857v1)
- **Published**: 2019-12-20 14:51:35+00:00
- **Updated**: 2019-12-20 14:51:35+00:00
- **Authors**: Bennet Breier, Arno Onken
- **Comment**: 18 pages incl. references and appendix, 16 figures, ICLR 2020
  Conference
- **Journal**: None
- **Summary**: Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.



### Locality and compositionality in zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1912.12179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12179v1)
- **Published**: 2019-12-20 15:50:57+00:00
- **Updated**: 2019-12-20 15:50:57+00:00
- **Authors**: Tristan Sylvain, Linda Petrini, Devon Hjelm
- **Comment**: Published at ICLR 2020
- **Journal**: None
- **Summary**: In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.



### Heterogeneous tissue characterization using ultrasound: a comparison of fractal analysis backscatter models on liver tumors
- **Arxiv ID**: http://arxiv.org/abs/1912.09903v1
- **DOI**: 10.1016/j.ultrasmedbio.2016.02.007
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09903v1)
- **Published**: 2019-12-20 16:03:11+00:00
- **Updated**: 2019-12-20 16:03:11+00:00
- **Authors**: Omar S. Al-Kadi, Daniel Y. F. Chung, Constantin C. Coussios, J. Alison Noble
- **Comment**: 31 pages, 7 figures, 3 tables, journal article
- **Journal**: Ultrasound in Medicine & Biology, vol. 42(7), pp. 1612-1626, 2016
- **Summary**: Assessing tumor tissue heterogeneity via ultrasound has recently been suggested for predicting early response to treatment. The ultrasound backscattering characteristics can assist in better understanding the tumor texture by highlighting local concentration and spatial arrangement of tissue scatterers. However, it is challenging to quantify the various tissue heterogeneities ranging from fine-to-coarse of the echo envelope peaks in tumor texture. Local parametric fractal features extracted via maximum likelihood estimation from five well-known statistical model families are evaluated for the purpose of ultrasound tissue characterization. The fractal dimension (self-similarity measure) was used to characterize the spatial distribution of scatterers, while the Lacunarity (sparsity measure) was applied to determine scatterer number density. Performance was assessed based on 608 cross-sectional clinical ultrasound RF images of liver tumors (230 and 378 demonstrating respondent and non-respondent cases, respectively). Crossvalidation via leave-one-tumor-out and with different k-folds methodologies using a Bayesian classifier were employed for validation. The fractal properties of the backscattered echoes based on the Nakagami model (Nkg) and its extend four-parameter Nakagami-generalized inverse Gaussian (NIG) distribution achieved best results - with nearly similar performance - for characterizing liver tumor tissue. Accuracy, sensitivity and specificity for the Nkg/NIG were: 85.6%/86.3%, 94.0%/96.0%, and 73.0%/71.0%, respectively. Other statistical models, such as the Rician, Rayleigh, and K-distribution were found to not be as effective in characterizing the subtle changes in tissue texture as an indication of response to treatment. Employing the most relevant and practical statistical model could have potential consequences for the design of an early and effective clinical therapy.



### A Calibration Scheme for Non-Line-of-Sight Imaging Setups
- **Arxiv ID**: http://arxiv.org/abs/1912.09923v1
- **DOI**: 10.1364/OE.398647
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09923v1)
- **Published**: 2019-12-20 16:34:56+00:00
- **Updated**: 2019-12-20 16:34:56+00:00
- **Authors**: Jonathan Klein, Martin Laurenzis, Matthias B. Hullin, Julian Iseringhausen
- **Comment**: None
- **Journal**: None
- **Summary**: The recent years have given rise to a large number of techniques for "looking around corners", i.e., for reconstructing occluded objects from time-resolved measurements of indirect light reflections off a wall. While the direct view of cameras is routinely calibrated in computer vision applications, the calibration of non-line-of-sight setups has so far relied on manual measurement of the most important dimensions (device positions, wall position and orientation, etc.). In this paper, we propose a semi-automatic method for calibrating such systems that relies on mirrors as known targets. A roughly determined initialization is refined in order to optimize a spatio-temporal consistency. Our system is general enough to be applicable to a variety of sensing scenarios ranging from single sources/detectors via scanning arrangements to large-scale arrays. It is robust towards bad initialization and the achieved accuracy is proportional to the depth resolution of the camera system. We demonstrate this capability with a real-world setup and despite a large number of dead pixels and very low temporal resolution achieve a result that outperforms a manual calibration.



### Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.09930v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09930v3)
- **Published**: 2019-12-20 16:44:14+00:00
- **Updated**: 2020-09-12 08:24:58+00:00
- **Authors**: Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Human action is naturally compositional: humans can easily recognize and perform actions with objects that are different from those used in training demonstrations. In this paper, we study the compositionality of action by looking into the dynamics of subject-object interactions. We propose a novel model which can explicitly reason about the geometric relations between constituent objects and an agent performing an action. To train our model, we collect dense object box annotations on the Something-Something dataset. We propose a novel compositional action recognition task where the training combinations of verbs and nouns do not overlap with the test set. The novel aspects of our model are applicable to activities with prominent object interaction dynamics and to objects which can be tracked using state-of-the-art approaches; for activities without clearly defined spatial object-agent interactions, we rely on baseline scene-level spatio-temporal representations. We show the effectiveness of our approach not only on the proposed compositional action recognition task, but also in a few-shot compositional setting which requires the model to generalize across both object appearance and action category.



### HiLLoC: Lossless Image Compression with Hierarchical Latent Variable Models
- **Arxiv ID**: http://arxiv.org/abs/1912.09953v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.09953v1)
- **Published**: 2019-12-20 17:20:38+00:00
- **Updated**: 2019-12-20 17:20:38+00:00
- **Authors**: James Townsend, Thomas Bird, Julius Kunze, David Barber
- **Comment**: None
- **Journal**: None
- **Summary**: We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.



### Attributed Relational SIFT-based Regions Graph (ARSRG): concepts and applications
- **Arxiv ID**: http://arxiv.org/abs/1912.09972v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 65D18 (Primary) 68T10 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1912.09972v1)
- **Published**: 2019-12-20 17:52:29+00:00
- **Updated**: 2019-12-20 17:52:29+00:00
- **Authors**: Mario Manzo
- **Comment**: 28 pages, 7 figures, submitted to Journal of Artificial Intelligence
  Research (https://www.jair.org/)
- **Journal**: None
- **Summary**: Graphs are widely adopted tools for encoding information. Generally, they are applied to disparate research fields where data needs to be represented in terms of local and spatial connections. In this context, a structure for ditigal image representation, called Attributed Relational SIFT-based Regions Graph (ARSRG), previously introduced, is presented. ARSRG has not been explored in detail in previous works and for this reason the goal is to investigate unknown aspects. The study is divided into two parts. A first, theoretical, introducing formal definitions, not yet specified previously, with purpose to clarify its structural configuration. A second, experimental, which provides fundamental elements about its adaptability and flexibility regarding different applications. The theoretical vision combined with the experimental one shows how the structure is adaptable to image representation including contents of different nature.



### Automated Segmentation of Optical Coherence Tomography Angiography Images: Benchmark Data and Clinically Relevant Metrics
- **Arxiv ID**: http://arxiv.org/abs/1912.09978v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.09978v2)
- **Published**: 2019-12-20 18:03:09+00:00
- **Updated**: 2020-05-29 18:16:20+00:00
- **Authors**: Ylenia Giarratano, Eleonora Bianchi, Calum Gray, Andrew Morris, Tom MacGillivray, Baljean Dhillon, Miguel O. Bernabeu
- **Comment**: None
- **Journal**: None
- **Summary**: Optical coherence tomography angiography (OCTA) is a novel non-invasive imaging modality for the visualisation of microvasculature in vivo that has encountered broad adoption in retinal research. OCTA potential in the assessment of pathological conditions and the reproducibility of studies relies on the quality of the image analysis. However, automated segmentation of parafoveal OCTA images is still an open problem. In this study, we generate the first open dataset of retinal parafoveal OCTA images with associated ground truth manual segmentations. Furthermore, we establish a standard for OCTA image segmentation by surveying a broad range of state-of-the-art vessel enhancement and binarisation procedures. We provide the most comprehensive comparison of these methods under a unified framework to date. Our results show that, for the set of images considered, deep learning architectures (U-Net and CS-Net) achieve the best performance. For applications where manually segmented data is not available to retrain these approaches, our findings suggest that optimal oriented flux is the best handcrafted filter from those considered. Furthermore, we report on the importance of preserving network structure in the segmentation to enable deep vascular phenotyping. We introduce new metrics for network structure evaluation in segmented angiograms. Our results demonstrate that segmentation methods with equal Dice score perform very differently in terms of network structure preservation. Moreover, we compare the error in the computation of clinically relevant vascular network metrics (e.g. foveal avascular zone area and vessel density) across segmentation methods. Our results show up to 25% differences in vessel density accuracy depending on the segmentation method employed. These findings should be taken into account when comparing the results of clinical studies and performing meta-analyses.



### secml: A Python Library for Secure and Explainable Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.10013v2
- **DOI**: 10.1016/j.softx.2022.101095
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.GT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10013v2)
- **Published**: 2019-12-20 18:41:37+00:00
- **Updated**: 2022-05-13 16:15:10+00:00
- **Authors**: Maura Pintor, Luca Demetrio, Angelo Sotgiu, Marco Melis, Ambra Demontis, Battista Biggio
- **Comment**: Accepted for publication to SoftwareX. Published version can be found
  at: https://doi.org/10.1016/j.softx.2022.101095
- **Journal**: SoftwareX 18 (2022)
- **Summary**: We present \texttt{secml}, an open-source Python library for secure and explainable machine learning. It implements the most popular attacks against machine learning, including test-time evasion attacks to generate adversarial examples against deep neural networks and training-time poisoning attacks against support vector machines and many other algorithms. These attacks enable evaluating the security of learning algorithms and the corresponding defenses under both white-box and black-box threat models. To this end, \texttt{secml} provides built-in functions to compute security evaluation curves, showing how quickly classification performance decreases against increasing adversarial perturbations of the input data. \texttt{secml} also includes explainability methods to help understand why adversarial attacks succeed against a given model, by visualizing the most influential features and training prototypes contributing to each decision. It is distributed under the Apache License 2.0 and hosted at \url{https://github.com/pralab/secml}.



### A Neural Model for Text Localization, Transcription and Named Entity Recognition in Full Pages
- **Arxiv ID**: http://arxiv.org/abs/1912.10016v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10016v2)
- **Published**: 2019-12-20 18:45:19+00:00
- **Updated**: 2020-05-04 17:34:13+00:00
- **Authors**: Manuel Carbonell, Alicia Fornés, Mauricio Villegas, Josep Lladós
- **Comment**: To be published in Pattern Recognition Letters
- **Journal**: None
- **Summary**: In the last years, the consolidation of deep neural network architectures for information extraction in document images has brought big improvements in the performance of each of the tasks involved in this process, consisting of text localization, transcription, and named entity recognition. However, this process is traditionally performed with separate methods for each task. In this work we propose an end-to-end model that combines a one stage object detection network with branches for the recognition of text and named entities respectively in a way that shared features can be learned simultaneously from the training error of each of the tasks. By doing so the model jointly performs handwritten text detection, transcription, and named entity recognition at page level with a single feed forward step. We exhaustively evaluate our approach on different datasets, discussing its advantages and limitations compared to sequential approaches. The results show that the model is capable of benefiting from shared features for simultaneously solving interdependent tasks.



### Identity Document to Selfie Face Matching Across Adolescence
- **Arxiv ID**: http://arxiv.org/abs/1912.10021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10021v1)
- **Published**: 2019-12-20 18:58:33+00:00
- **Updated**: 2019-12-20 18:58:33+00:00
- **Authors**: Vítor Albiero, Nisha Srinivas, Esteban Villalobos, Jorge Perez-Facuse, Roberto Rosenthal, Domingo Mery, Karl Ricanek, Kevin W. Bowyer
- **Comment**: None
- **Journal**: None
- **Summary**: Matching live images (``selfies'') to images from ID documents is a problem that can arise in various applications. A challenging instance of the problem arises when the face image on the ID document is from early adolescence and the live image is from later adolescence. We explore this problem using a private dataset called Chilean Young Adult (CHIYA) dataset, where we match live face images taken at age 18-19 to face images on ID documents created at ages 9 to 18. State-of-the-art deep learning face matchers (e.g., ArcFace) have relatively poor accuracy for document-to-selfie face matching. To achieve higher accuracy, we fine-tune the best available open-source model with triplet loss for a few-shot learning. Experiments show that our approach achieves higher accuracy than the DocFace+ model recently developed for this problem. Our fine-tuned model was able to improve the true acceptance rate for the most difficult (largest age span) subset from 62.92% to 96.67% at a false acceptance rate of 0.01%. Our fine-tuned model is available for use by other researchers.



### Saliency Based Fire Detection Using Texture and Color Features
- **Arxiv ID**: http://arxiv.org/abs/1912.10059v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10059v1)
- **Published**: 2019-12-20 19:05:19+00:00
- **Updated**: 2019-12-20 19:05:19+00:00
- **Authors**: Maedeh Jamali, Nader Karimi, Shadrokh Samavi
- **Comment**: 5 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: Due to industry deployment and extension of urban areas, early warning systems have an essential role in giving emergency. Fire is an event that can rapidly spread and cause injury, death, and damage. Early detection of fire could significantly reduce these injuries. Video-based fire detection is a low cost and fast method in comparison with conventional fire detectors. Most available fire detection methods have a high false-positive rate and low accuracy. In this paper, we increase accuracy by using spatial and temporal features. Captured video sequences are divided into Spatio-temporal blocks. Then a saliency map and combination of color and texture features are used for detecting fire regions. We use the HSV color model as a spatial feature and LBP-TOP for temporal processing of fire texture. Fire detection tests on publicly available datasets have shown the accuracy and robustness of the algorithm.



### EAST: Encoding-Aware Sparse Training for Deep Memory Compression of ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1912.10087v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10087v1)
- **Published**: 2019-12-20 20:20:48+00:00
- **Updated**: 2019-12-20 20:20:48+00:00
- **Authors**: Matteo Grimaldi, Valentino Peluso, Andrea Calimera
- **Comment**: None
- **Journal**: None
- **Summary**: The implementation of Deep Convolutional Neural Networks (ConvNets) on tiny end-nodes with limited non-volatile memory space calls for smart compression strategies capable of shrinking the footprint yet preserving predictive accuracy. There exist a number of strategies for this purpose, from those that play with the topology of the model or the arithmetic precision, e.g. pruning and quantization, to those that operate a model agnostic compression, e.g. weight encoding. The tighter the memory constraint, the higher the probability that these techniques alone cannot meet the requirement, hence more awareness and cooperation across different optimizations become mandatory. This work addresses the issue by introducing EAST, Encoding-Aware Sparse Training, a novel memory-constrained training procedure that leads quantized ConvNets towards deep memory compression. EAST implements an adaptive group pruning designed to maximize the compression rate of the weight encoding scheme (the LZ4 algorithm in this work). If compared to existing methods, EAST meets the memory constraint with lower sparsity, hence ensuring higher accuracy. Results conducted on a state-of-the-art ConvNet (ResNet-9) deployed on a low-power microcontroller (ARM Cortex-M4) validate the proposal.



### From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality
- **Arxiv ID**: http://arxiv.org/abs/1912.10088v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10088v1)
- **Published**: 2019-12-20 20:22:55+00:00
- **Updated**: 2019-12-20 20:22:55+00:00
- **Authors**: Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, Alan Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: Blind or no-reference (NR) perceptual picture quality prediction is a difficult, unsolved problem of great consequence to the social and streaming media industries that impacts billions of viewers daily. Unfortunately, popular NR prediction models perform poorly on real-world distorted pictures. To advance progress on this problem, we introduce the largest (by far) subjective picture quality database, containing about 40000 real-world distorted pictures and 120000 patches, on which we collected about 4M human judgments of picture quality. Using these picture and patch quality labels, we built deep region-based architectures that learn to produce state-of-the-art global picture quality predictions as well as useful local picture quality maps. Our innovations include picture quality prediction architectures that produce global-to-local inferences as well as local-to-global inferences (via feedback).



### TentacleNet: A Pseudo-Ensemble Template for Accurate Binary Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.10103v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10103v2)
- **Published**: 2019-12-20 21:18:16+00:00
- **Updated**: 2019-12-26 12:37:23+00:00
- **Authors**: Luca Mocerino, Andrea Calimera
- **Comment**: None
- **Journal**: None
- **Summary**: Binarization is an attractive strategy for implementing lightweight Deep Convolutional Neural Networks (CNNs). Despite the unquestionable savings offered, memory footprint above all, it may induce an excessive accuracy loss that prevents a widespread use. This work elaborates on this aspect introducing TentacleNet, a new template designed to improve the predictive performance of binarized CNNs via parallelization. Inspired by the ensemble learning theory, it consists of a compact topology that is end-to-end trainable and organized to minimize memory utilization. Experimental results collected over three realistic benchmarks show TentacleNet fills the gap left by classical binary models, ensuring substantial memory savings w.r.t. state-of-the-art binary ensemble methods.



### Assessing Data Quality of Annotations with Krippendorff Alpha For Applications in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1912.10107v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10107v1)
- **Published**: 2019-12-20 21:23:42+00:00
- **Updated**: 2019-12-20 21:23:42+00:00
- **Authors**: Joseph Nassar, Viveca Pavon-Harr, Marc Bosch, Ian McCulloh
- **Comment**: Accepted to AAAI Symposium 2019
- **Journal**: None
- **Summary**: Current supervised deep learning frameworks rely on annotated data for modeling the underlying data distribution of a given task. In particular for computer vision algorithms powered by deep learning, the quality of annotated data is the most critical factor in achieving the desired algorithm performance. Data annotation is, typically, a manual process where the annotator follows guidelines and operates in a best-guess manner. Labeling criteria among annotators can show discrepancies in labeling results. This may impact the algorithm inference performance. Given the popularity and widespread use of deep learning among computer vision, more and more custom datasets are needed to train neural networks to tackle different kinds of tasks. Unfortunately, there is no full understanding of the factors that affect annotated data quality, and how it translates into algorithm performance. In this paper we studied this problem for object detection and recognition.We conducted several data annotation experiments to measure inter annotator agreement and consistency, as well as how the selection of ground truth impacts the perceived algorithm performance.We propose a methodology to monitor the quality of annotations during the labeling of images and how it can be used to measure performance. We also show that neglecting to monitor the annotation process can result in significant loss in algorithm precision. Through these experiments, we observe that knowledge of the labeling process, training data, and ground truth data used for algorithm evaluation are fundamental components to accurately assess trustworthiness of an AI system.



### Generating Robust Supervision for Learning-Based Visual Navigation Using Hamilton-Jacobi Reachability
- **Arxiv ID**: http://arxiv.org/abs/1912.10120v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1912.10120v2)
- **Published**: 2019-12-20 22:11:11+00:00
- **Updated**: 2020-05-15 18:47:10+00:00
- **Authors**: Anjian Li, Somil Bansal, Georgios Giovanis, Varun Tolani, Claire Tomlin, Mo Chen
- **Comment**: Learning for Dynamics and Control (L4DC) 2020
- **Journal**: None
- **Summary**: In Bansal et al. (2019), a novel visual navigation framework that combines learning-based and model-based approaches has been proposed. Specifically, a Convolutional Neural Network (CNN) predicts a waypoint that is used by the dynamics model for planning and tracking a trajectory to the waypoint. However, the CNN inevitably makes prediction errors which often lead to collisions in cluttered and tight spaces. In this paper, we present a novel Hamilton-Jacobi (HJ) reachability-based method to generate supervision for the CNN for waypoint prediction in an unseen environment. By modeling CNN prediction error as "disturbances" in robot's dynamics, our generated waypoints are robust to these disturbances, and consequently to the prediction errors. Moreover, using globally optimal HJ reachability analysis leads to predicting waypoints that are time-efficient and avoid greedy behavior. Through simulations and hardware experiments, we demonstrate the advantages of the proposed approach on navigating through cluttered, narrow indoor environments.



### A Region-based Randers Geodesic Approach for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.10122v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10122v3)
- **Published**: 2019-12-20 22:17:50+00:00
- **Updated**: 2023-08-31 00:53:12+00:00
- **Authors**: Da Chen, Jean-Marie Mirebeau, Huazhong Shu, Laurent D. Cohen
- **Comment**: To Appear in International Journal of Computer Vision
- **Journal**: None
- **Summary**: The geodesic model based on the eikonal partial differential equation (PDE) has served as a fundamental tool for the applications of image segmentation and boundary detection in the past two decades. However, the existing approaches commonly only exploit the image edge-based features for computing minimal geodesic paths, potentially limiting their performance in complicated segmentation situations. In this paper, we introduce a new variational image segmentation model based on the minimal geodesic path framework and the eikonal PDE, where the region-based appearance term that defines then regional homogeneity features can be taken into account for estimating the associated minimal geodesic paths. This is done by constructing a Randers geodesic metric interpretation of the region-based active contour energy functional. As a result, the minimization of the active contour energy functional is transformed into finding the solution to the Randers eikonal PDE.   We also suggest a practical interactive image segmentation strategy, where the target boundary can be delineated by the concatenation of several piecewise geodesic paths. We invoke the Finsler variant of the fast marching method to estimate the geodesic distance map, yielding an efficient implementation of the proposed region-based Randers geodesic model for image segmentation. Experimental results on both synthetic and real images exhibit that our model indeed achieves encouraging segmentation performance.



