# Arxiv Papers in cs.CV on 2019-12-01
### A Programmatic and Semantic Approach to Explaining and DebuggingNeural Network Based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1912.00289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00289v2)
- **Published**: 2019-12-01 00:07:59+00:00
- **Updated**: 2020-06-16 07:48:40+00:00
- **Authors**: Edward Kim, Divya Gopinath, Corina Pasareanu, Sanjit Seshia
- **Comment**: None
- **Journal**: CVPR (2020)
- **Summary**: Even as deep neural networks have become very effective for tasks in vision and perception, it remains difficult to explain and debug their behavior. In this paper, we present a programmatic and semantic approach to explaining, understanding, and debugging the correct and incorrect behaviors of a neural network-based perception system. Our approach is semantic in that it employs a high-level representation of the distribution of environment scenarios that the detector is intended to work on. It is programmatic in that scenario representation is a program in a domain-specific probabilistic programming language which can be used to generate synthetic data to test a given perception module. Our framework assesses the performance of a perception module to identify correct and incorrect detections, extracts rules from those results that semantically characterizes the correct and incorrect scenarios, and then specializes the probabilistic program with those rules in order to more precisely characterize the scenarios in which the perception module operates correctly or not. We demonstrate our results using the SCENIC probabilistic programming language and a neural network-based object detector. Our experiments show that it is possible to automatically generate compact rules that significantly increase the correct detection rate (or conversely the incorrect detection rate) of the network and can thus help with understanding and debugging its behavior.



### Image Based Identification of Ghanaian Timbers Using the XyloTron: Opportunities, Risks and Challenges
- **Arxiv ID**: http://arxiv.org/abs/1912.00296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00296v1)
- **Published**: 2019-12-01 01:05:39+00:00
- **Updated**: 2019-12-01 01:05:39+00:00
- **Authors**: Prabu Ravindran, Emmanuel Ebanyenle, Alberta Asi Ebeheakey, Kofi Bonsu Abban, Ophilious Lambog, Richard Soares, Adriana Costa, Alex C. Wiedenhoeft
- **Comment**: Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: Computer vision systems for wood identification have the potential to empower both producer and consumer countries to combat illegal logging if they can be deployed effectively in the field. In this work, carried out as part of an active international partnership with the support of UNIDO, we constructed and curated a field-relevant image data set to train a classifier for wood identification of $15$ commercial Ghanaian woods using the XyloTron system. We tested model performance in the laboratory, and then collected real-world field performance data across multiple sites using multiple XyloTron devices. We present efficacies of the trained model in the laboratory and in the field, discuss practical implications and challenges of deploying machine learning wood identification models, and conclude that field testing is a necessary step - and should be considered the gold-standard - for validating computer vision wood identification systems.



### Exploiting Motion Information from Unlabeled Videos for Static Image Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.00308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00308v1)
- **Published**: 2019-12-01 03:01:10+00:00
- **Updated**: 2019-12-01 03:01:10+00:00
- **Authors**: Yiyi Zhang, Li Niu, Ziqi Pan, Meichao Luo, Jianfu Zhang, Dawei Cheng, Liqing Zhang
- **Comment**: None
- **Journal**: AAAI 2020
- **Summary**: Static image action recognition, which aims to recognize action based on a single image, usually relies on expensive human labeling effort such as adequate labeled action images and large-scale labeled image dataset. In contrast, abundant unlabeled videos can be economically obtained. Therefore, several works have explored using unlabeled videos to facilitate image action recognition, which can be categorized into the following two groups: (a) enhance visual representations of action images with a designed proxy task on unlabeled videos, which falls into the scope of self-supervised learning; (b) generate auxiliary representations for action images with the generator learned from unlabeled videos. In this paper, we integrate the above two strategies in a unified framework, which consists of Visual Representation Enhancement (VRE) module and Motion Representation Augmentation (MRA) module. Specifically, the VRE module includes a proxy task which imposes pseudo motion label constraint and temporal coherence constraint on unlabeled videos, while the MRA module could predict the motion information of a static action image by exploiting unlabeled videos. We demonstrate the superiority of our framework based on four benchmark human action datasets with limited labeled data.



### Learning to Relate from Captions and Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1912.00311v1
- **DOI**: 10.18653/v1/P19-1660
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00311v1)
- **Published**: 2019-12-01 03:30:00+00:00
- **Updated**: 2019-12-01 03:30:00+00:00
- **Authors**: Sarthak Garg, Joel Ruben Antony Moniz, Anshu Aviral, Priyatham Bollimpalli
- **Comment**: ACL 2019
- **Journal**: None
- **Summary**: In this work, we propose a novel approach that predicts the relationships between various entities in an image in a weakly supervised manner by relying on image captions and object bounding box annotations as the sole source of supervision. Our proposed approach uses a top-down attention mechanism to align entities in captions to objects in the image, and then leverage the syntactic structure of the captions to align the relations. We use these alignments to train a relation classification network, thereby obtaining both grounded captions and dense relationships. We demonstrate the effectiveness of our model on the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of 25% on the relationships present in the image. We also show that the model successfully predicts relations that are not present in the corresponding captions.



### Discriminative Joint Probability Maximum Mean Discrepancy (DJP-MMD) for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1912.00320v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00320v4)
- **Published**: 2019-12-01 04:52:41+00:00
- **Updated**: 2020-04-10 15:13:57+00:00
- **Authors**: Wen Zhang, Dongrui Wu
- **Comment**: Int'l Joint Conf. on Neural Networks (IJCNN), Glasgow, UK, July 2020
- **Journal**: None
- **Summary**: Maximum mean discrepancy (MMD) has been widely adopted in domain adaptation to measure the discrepancy between the source and target domain distributions. Many existing domain adaptation approaches are based on the joint MMD, which is computed as the (weighted) sum of the marginal distribution discrepancy and the conditional distribution discrepancy; however, a more natural metric may be their joint probability distribution discrepancy. Additionally, most metrics only aim to increase the transferability between domains, but ignores the discriminability between different classes, which may result in insufficient classification performance. To address these issues, discriminative joint probability MMD (DJP-MMD) is proposed in this paper to replace the frequently-used joint MMD in domain adaptation. It has two desirable properties: 1) it provides a new theoretical basis for computing the distribution discrepancy, which is simpler and more accurate; 2) it increases the transferability and discriminability simultaneously. We validate its performance by embedding it into a joint probability domain adaptation framework. Experiments on six image classification datasets demonstrated that the proposed DJP-MMD can outperform traditional MMDs.



### Semi-supervised Visual Feature Integration for Pre-trained Language Models
- **Arxiv ID**: http://arxiv.org/abs/1912.00336v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00336v2)
- **Published**: 2019-12-01 06:53:23+00:00
- **Updated**: 2020-08-13 02:59:54+00:00
- **Authors**: Lisai Zhang, Qingcai Chen, Dongfang Li, Buzhou Tang
- **Comment**: 12 pages, 6 figures, 5 tables
- **Journal**: None
- **Summary**: Integrating visual features has been proved useful for natural language understanding tasks. Nevertheless, in most existing multimodal language models, the alignment of visual and textual data is expensive. In this paper, we propose a novel semi-supervised visual integration framework for pre-trained language models. In the framework, the visual features are obtained through a visualization and fusion mechanism. The uniqueness includes: 1) the integration is conducted via a semi-supervised approach, which does not require aligned images for every sentences 2) the visual features are integrated as an external component and can be directly used by pre-trained language models. To verify the efficacy of the proposed framework, we conduct the experiments on both natural language inference and reading comprehension tasks. The results demonstrate that our mechanism brings improvement to two strong baseline models. Considering that our framework only requires an image database, and no not requires further alignments, it provides an efficient and feasible way for multimodal language learning.



### End to End Trainable Active Contours via Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/1912.00367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00367v1)
- **Published**: 2019-12-01 09:27:22+00:00
- **Updated**: 2019-12-01 09:27:22+00:00
- **Authors**: Shir Gur, Tal Shaharabany, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images. Our code is available at https://github.com/shirgur/ACDRNet.



### Dynamic Graph Representation for Partially Occluded Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1912.00377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00377v2)
- **Published**: 2019-12-01 10:27:11+00:00
- **Updated**: 2020-09-12 11:06:59+00:00
- **Authors**: Min Ren, Yunlong Wang, Zhenan Sun, Tieniu Tan
- **Comment**: Accepted by AAAI2020, 9 pages, 15 figures
- **Journal**: None
- **Summary**: The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.The code is avaliable at https://github.com/RenMin1991/Dyamic\_Graph\_Representation



### Gate-Shift Networks for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.00381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00381v2)
- **Published**: 2019-12-01 10:49:11+00:00
- **Updated**: 2020-03-21 19:23:27+00:00
- **Authors**: Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz
- **Comment**: CVPR20 camera ready version. Code and models available at
  https://github.com/swathikirans/GSM
- **Journal**: None
- **Summary**: Deep 3D CNNs for video action recognition are designed to learn powerful representations in the joint spatio-temporal feature space. In practice however, because of the large number of parameters and computations involved, they may under-perform in the lack of sufficiently large datasets for training them at scale. In this paper we introduce spatial gating in spatial-temporal decomposition of 3D kernels. We implement this concept with Gate-Shift Module (GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient spatio-temporal feature extractor. With GSM plugged in, a 2D-CNN learns to adaptively route features through time and combine them, at almost no additional parameters and computational overhead. We perform an extensive evaluation of the proposed module to study its effectiveness in video action recognition, achieving state-of-the-art results on Something Something-V1 and Diving48 datasets, and obtaining competitive results on EPIC-Kitchens with far less model complexity.



### Alignment Free and Distortion Robust Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.00382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00382v1)
- **Published**: 2019-12-01 11:03:09+00:00
- **Updated**: 2019-12-01 11:03:09+00:00
- **Authors**: Min Ren, Caiyong Wang, Yunlong Wang, Zhenan Sun, Tieniu Tan
- **Comment**: Accepted by ICB2019, 7 pages, 5 figures
- **Journal**: None
- **Summary**: Iris recognition is a reliable personal identification method but there is still much room to improve its accuracy especially in less-constrained situations. For example, free movement of head pose may cause large rotation difference between iris images. And illumination variations may cause irregular distortion of iris texture. To match intra-class iris images with head rotation robustly, the existing solutions usually need a precise alignment operation by exhaustive search within a determined range in iris image preprosessing or brute force searching the minimum Hamming distance in iris feature matching. In the wild, iris rotation is of much greater uncertainty than that in constrained situations and exhaustive search within a determined range is impracticable. This paper presents a unified feature-level solution to both alignment free and distortion robust iris recognition in the wild. A new deep learning based method named Alignment Free Iris Network (AFINet) is proposed, which uses a trainable VLAD (Vector of Locally Aggregated Descriptors) encoder called NetVLAD to decouple the correlations between local representations and their spatial positions. And deformable convolution is used to overcome iris texture distortion by dense adaptive sampling. The results of extensive experiments on three public iris image databases and the simulated degradation databases show that AFINet significantly outperforms state-of-art iris recognition methods.



### Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images
- **Arxiv ID**: http://arxiv.org/abs/1912.00384v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00384v6)
- **Published**: 2019-12-01 11:09:48+00:00
- **Updated**: 2021-07-21 01:36:48+00:00
- **Authors**: Zhaohui Yang, Miaojing Shi, Chao Xu, Vittorio Ferrari, Yannis Avrithis
- **Comment**: Accepted by Pattern Recognition
- **Journal**: None
- **Summary**: Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL, our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions.



### The Group Loss for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.00385v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00385v4)
- **Published**: 2019-12-01 11:09:57+00:00
- **Updated**: 2020-07-20 17:28:44+00:00
- **Authors**: Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello Pelillo, Laura Leal-Taixe
- **Comment**: Accepted to European Conference on Computer Vision (ECCV) 2020,
  includes non-archival supplementary material
- **Journal**: None
- **Summary**: Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that "similar objects should belong to the same group", the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We show state-of-the-art results on clustering and image retrieval on several datasets, and show the potential of our method when combined with other techniques such as ensembles



### Modeling Affect-based Intrinsic Rewards for Exploration and Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.00403v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00403v7)
- **Published**: 2019-12-01 13:17:39+00:00
- **Updated**: 2021-04-04 09:37:03+00:00
- **Authors**: Dean Zadok, Daniel McDuff, Ashish Kapoor
- **Comment**: None
- **Journal**: None
- **Summary**: Positive affect has been linked to increased interest, curiosity and satisfaction in human learning. In reinforcement learning, extrinsic rewards are often sparse and difficult to define, intrinsically motivated learning can help address these challenges. We argue that positive affect is an important intrinsic reward that effectively helps drive exploration that is useful in gathering experiences. We present a novel approach leveraging a task-independent reward function trained on spontaneous smile behavior that reflects the intrinsic reward of positive affect. To evaluate our approach we trained several downstream computer vision tasks on data collected with our policy and several baseline methods. We show that the policy based on our affective rewards successfully increases the duration of episodes, the area explored and reduces collisions. The impact is the increased speed of learning for several downstream computer vision tasks.



### Hepatocellular Carcinoma Intra-arterial Treatment Response Prediction for Improved Therapeutic Decision-Making
- **Arxiv ID**: http://arxiv.org/abs/1912.00411v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.00411v1)
- **Published**: 2019-12-01 14:00:37+00:00
- **Updated**: 2019-12-01 14:00:37+00:00
- **Authors**: Junlin Yang, Nicha C. Dvornek, Fan Zhang, Julius Chapiro, MingDe Lin, Aaron Abajian, James S. Duncan
- **Comment**: Accepted by NeurIPS workshop MED-NeurIPS 2019
- **Journal**: None
- **Summary**: This work proposes a pipeline to predict treatment response to intra-arterial therapy of patients with Hepatocellular Carcinoma (HCC) for improved therapeutic decision-making. Our graph neural network model seamlessly combines heterogeneous inputs of baseline MR scans, pre-treatment clinical information, and planned treatment characteristics and has been validated on patients with HCC treated by transarterial chemoembolization (TACE). It achieves Accuracy of $0.713 \pm 0.075$, F1 of $0.702 \pm 0.082$ and AUC of $0.710 \pm 0.108$. In addition, the pipeline incorporates uncertainty estimation to select hard cases and most align with the misclassified cases. The proposed pipeline arrives at more informed intra-arterial therapeutic decisions for patients with HCC via improving model accuracy and incorporating uncertainty estimation.



### MetAdapt: Meta-Learned Task-Adaptive Architecture for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.00412v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.00412v3)
- **Published**: 2019-12-01 14:04:34+00:00
- **Updated**: 2020-03-09 11:44:12+00:00
- **Authors**: Sivan Doveh, Eli Schwartz, Chao Xue, Rogerio Feris, Alex Bronstein, Raja Giryes, Leonid Karlinsky
- **Comment**: None
- **Journal**: None
- **Summary**: Few-Shot Learning (FSL) is a topic of rapidly growing interest. Typically, in FSL a model is trained on a dataset consisting of many small tasks (meta-tasks) and learns to adapt to novel tasks that it will encounter during test time. This is also referred to as meta-learning. Another topic closely related to meta-learning with a lot of interest in the community is Neural Architecture Search (NAS), automatically finding optimal architecture instead of engineering it manually. In this work, we combine these two aspects of meta-learning. So far, meta-learning FSL methods have focused on optimizing parameters of pre-defined network architectures, in order to make them easily adaptable to novel tasks. Moreover, it was observed that, in general, larger architectures perform better than smaller ones up to a certain saturation point (where they start to degrade due to over-fitting). However, little attention has been given to explicitly optimizing the architectures for FSL, nor to an adaptation of the architecture at test time to particular novel tasks. In this work, we propose to employ tools inspired by the Differentiable Neural Architecture Search (D-NAS) literature in order to optimize the architecture for FSL without over-fitting. Additionally, to make the architecture task adaptive, we propose the concept of `MetAdapt Controller' modules. These modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task. Using the proposed approach we observe state-of-the-art results on two popular few-shot benchmarks: miniImageNet and FC100.



### LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.00416v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.00416v3)
- **Published**: 2019-12-01 14:32:58+00:00
- **Updated**: 2020-06-12 02:56:30+00:00
- **Authors**: Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox
- **Comment**: CVPR 2020, Project Page:
  https://keunhong.com/publications/latentfusion/ , Video:
  https://youtu.be/tlzcq1KYXd8 , Code: https://github.com/NVlabs/latentfusion .
  We have added experiments for LINEMOD and have updated the experiments on
  MOPED. We've also added more technical and implementation details to the
  methods section
- **Journal**: None
- **Summary**: Current 6D object pose estimation methods usually require a 3D model for each object. These methods also require additional training in order to incorporate new objects. As a result, they are difficult to scale to a large number of objects and cannot be directly applied to unseen objects.   We propose a novel framework for 6D pose estimation of unseen objects. We present a network that reconstructs a latent 3D representation of an object using a small number of reference views at inference time. Our network is able to render the latent 3D representation from arbitrary views. Using this neural renderer, we directly optimize for pose given an input image. By training our network with a large number of 3D shapes for reconstruction and rendering, our network generalizes well to unseen objects. We present a new dataset for unseen object pose estimation--MOPED. We evaluate the performance of our method for unseen object pose estimation on MOPED as well as the ModelNet and LINEMOD datasets. Our method performs competitively to supervised methods that are trained on those objects. Code and data is available at https://keunhong.com/publications/latentfusion/.



### Diversifying Inference Path Selection: Moving-Mobile-Network for Landmark Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.00418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00418v1)
- **Published**: 2019-12-01 14:40:38+00:00
- **Updated**: 2019-12-01 14:40:38+00:00
- **Authors**: Biao Qian, Yang Wang, Zhao Zhang, Richang Hong, Meng Wang, Ling Shao
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: Deep convolutional neural networks have largely benefited computer vision tasks. However, the high computational complexity limits their real-world applications. To this end, many methods have been proposed for efficient network learning, and applications in portable mobile devices. In this paper, we propose a novel \underline{M}oving-\underline{M}obile-\underline{Net}work, named M$^2$Net, for landmark recognition, equipped each landmark image with located geographic information. We intuitively find that M$^2$Net can essentially promote the diversity of the inference path (selected blocks subset) selection, so as to enhance the recognition accuracy. The above intuition is achieved by our proposed reward function with the input of geo-location and landmarks. We also find that the performance of other portable networks can be improved via our architecture. We construct two landmark image datasets, with each landmark associated with geographic information, over which we conduct extensive experiments to demonstrate that M$^2$Net achieves improved recognition accuracy with comparable complexity.



### Stochastic tissue window normalization of deep learning on computed tomography
- **Arxiv ID**: http://arxiv.org/abs/1912.00420v1
- **DOI**: 10.1117/1.JMI.6.4.044005
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00420v1)
- **Published**: 2019-12-01 14:48:15+00:00
- **Updated**: 2019-12-01 14:48:15+00:00
- **Authors**: Yuankai Huo, Yucheng Tang, Yunqiang Chen, Dashan Gao, Shizhong Han, Shunxing Bao, Smita De, James G. Terry, Jeffrey J. Carr, Richard G. Abramson, Bennett A. Landman
- **Comment**: None
- **Journal**: Journal of Medical Imaging 6.4 (2019): 044005
- **Summary**: Tissue window filtering has been widely used in deep learning for computed tomography (CT) image analyses to improve training performance (e.g., soft tissue windows for abdominal CT). However, the effectiveness of tissue window normalization is questionable since the generalizability of the trained model might be further harmed, especially when such models are applied to new cohorts with different CT reconstruction kernels, contrast mechanisms, dynamic variations in the acquisition, and physiological changes. We evaluate the effectiveness of both with and without using soft tissue window normalization on multisite CT cohorts. Moreover, we propose a stochastic tissue window normalization (SWN) method to improve the generalizability of tissue window normalization. Different from the random sampling, the SWN method centers the randomization around the soft tissue window to maintain the specificity for abdominal organs. To evaluate the performance of different strategies, 80 training and 453 validation and testing scans from six datasets are employed to perform multi-organ segmentation using standard 2D U-Net. The six datasets cover the scenarios, where the training and testing scans are from (1) same scanner and same population, (2) same CT contrast but different pathology, and (3) different CT contrast and pathology. The traditional soft tissue window and nonwindowed approaches achieved better performance on (1). The proposed SWN achieved general superior performance on (2) and (3) with statistical analyses, which offers better generalizability for a trained model.



### RST-MODNet: Real-time Spatio-temporal Moving Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1912.00438v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00438v1)
- **Published**: 2019-12-01 16:14:59+00:00
- **Updated**: 2019-12-01 16:14:59+00:00
- **Authors**: Mohamed Ramzy, Hazem Rashed, Ahmad El Sallab, Senthil Yogamani
- **Comment**: Accepted for presentation at NeurIPS 2019 Workshop on Machine
  Learning for Autonomous Driving
- **Journal**: None
- **Summary**: Moving Object Detection (MOD) is a critical task for autonomous vehicles as moving objects represent higher collision risk than static ones. The trajectory of the ego-vehicle is planned based on the future states of detected moving objects. It is quite challenging as the ego-motion has to be modelled and compensated to be able to understand the motion of the surrounding objects. In this work, we propose a real-time end-to-end CNN architecture for MOD utilizing spatio-temporal context to improve robustness. We construct a novel time-aware architecture exploiting temporal motion information embedded within sequential images in addition to explicit motion maps using optical flow images.We demonstrate the impact of our algorithm on KITTI dataset where we obtain an improvement of 8% relative to the baselines. We compare our algorithm with state-of-the-art methods and achieve competitive results on KITTI-Motion dataset in terms of accuracy at three times better run-time. The proposed algorithm runs at 23 fps on a standard desktop GPU targeting deployment on embedded platforms.



### DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.00439v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.00439v3)
- **Published**: 2019-12-01 16:50:07+00:00
- **Updated**: 2020-08-13 14:47:16+00:00
- **Authors**: Andreas Kuhn, Christian Sormann, Mattia Rossi, Oliver Erdler, Friedrich Fraundorfer
- **Comment**: changes in V3: re-worked confidence prediction scheme, re-organized
  text, updated experiments; changes in V2: a reference was updated
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) have the potential to improve the quality of image-based 3D reconstructions. However, the use of DNNs in the context of 3D reconstruction from large and high-resolution image datasets is still an open challenge, due to memory and computational constraints. We propose a pipeline which takes advantage of DNNs to improve the quality of 3D reconstructions while being able to handle large and high-resolution datasets. In particular, we propose a confidence prediction network explicitly tailored for Multi-View Stereo (MVS) and we use it for both depth map outlier filtering and depth map refinement within our pipeline, in order to improve the quality of the final 3D reconstructions. We train our confidence prediction network on (semi-)dense ground truth depth maps from publicly available real world MVS datasets. With extensive experiments on popular benchmarks, we show that our overall pipeline can produce state-of-the-art 3D reconstructions, both qualitatively and quantitatively.



### AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.00461v2
- **DOI**: 10.1007/978-3-030-58610-2_15
- **Categories**: **cs.CV**, cs.CR, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1912.00461v2)
- **Published**: 2019-12-01 18:13:23+00:00
- **Updated**: 2020-07-16 12:16:06+00:00
- **Authors**: Abdullah Hamdi, Sara Rojas, Ali Thabet, Bernard Ghanem
- **Comment**: Presented at European conference on computer vision (ECCV), 2020. The
  code is available at https://github.com/ajhamdi/AdvPC
- **Journal**: ECCV 2020
- **Summary**: Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple statistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset.



### A Method for Computing Class-wise Universal Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1912.00466v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.00466v1)
- **Published**: 2019-12-01 18:22:14+00:00
- **Updated**: 2019-12-01 18:22:14+00:00
- **Authors**: Tejus Gupta, Abhishek Sinha, Nupur Kumari, Mayank Singh, Balaji Krishnamurthy
- **Comment**: None
- **Journal**: None
- **Summary**: We present an algorithm for computing class-specific universal adversarial perturbations for deep neural networks. Such perturbations can induce misclassification in a large fraction of images of a specific class. Unlike previous methods that use iterative optimization for computing a universal perturbation, the proposed method employs a perturbation that is a linear function of weights of the neural network and hence can be computed much faster. The method does not require any training data and has no hyper-parameters. The attack obtains 34% to 51% fooling rate on state-of-the-art deep neural networks on ImageNet and transfers across models. We also study the characteristics of the decision boundaries learned by standard and adversarially trained models to understand the universal adversarial perturbations.



### Just Go with the Flow: Self-Supervised Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.00497v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00497v2)
- **Published**: 2019-12-01 20:32:54+00:00
- **Updated**: 2020-04-13 19:10:57+00:00
- **Authors**: Himangi Mittal, Brian Okorn, David Held
- **Comment**: Accepted at CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state-of-the-art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.



### Interpreting Context of Images using Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1912.00501v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00501v1)
- **Published**: 2019-12-01 21:32:11+00:00
- **Updated**: 2019-12-01 21:32:11+00:00
- **Authors**: Himangi Mittal, Ajith Abraham, Anuja Arora
- **Comment**: To appear in International Conference on Big Data Analytics (BDA2019)
  (Accepted)
- **Journal**: None
- **Summary**: Understanding a visual scene incorporates objects, relationships, and context. Traditional methods working on an image mostly focus on object detection and fail to capture the relationship between the objects. Relationships can give rich semantic information about the objects in a scene. The context can be conducive to comprehending an image since it will help us to perceive the relation between the objects and thus, give us a deeper insight into the image. Through this idea, our project delivers a model that focuses on finding the context present in an image by representing the image as a graph, where the nodes will the objects and edges will be the relation between them. The context is found using the visual and semantic cues which are further concatenated and given to the Support Vector Machines (SVM) to detect the relation between two objects. This presents us with the context of the image which can be further used in applications such as similar image retrieval, image captioning, or story generation.



### Texture Hallucination for Large-Factor Painting Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1912.00515v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.00515v4)
- **Published**: 2019-12-01 22:46:44+00:00
- **Updated**: 2020-07-30 04:03:53+00:00
- **Authors**: Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, Yun Fu
- **Comment**: Accepted to ECCV 2020. Supplementary material contains more visual
  results and is available at
  http://yulunzhang.com/papers/PaintingSR_supp_arXiv.pdf
- **Journal**: None
- **Summary**: We aim to super-resolve digital paintings, synthesizing realistic details from high-resolution reference painting materials for very large scaling factors (e.g., 8X, 16X). However, previous single image super-resolution (SISR) methods would either lose textural details or introduce unpleasing artifacts. On the other hand, reference-based SR (Ref-SR) methods can transfer textures to some extent, but is still impractical to handle very large factors and keep fidelity with original input. To solve these problems, we propose an efficient high-resolution hallucination network for very large scaling factors with an efficient network structure and feature transferring. To transfer more detailed textures, we design a wavelet texture loss, which helps to enhance more high-frequency components. At the same time, to reduce the smoothing effect brought by the image reconstruction loss, we further relax the reconstruction constraint with a degradation loss which ensures the consistency between downscaled super-resolution results and low-resolution inputs. We also collected a high-resolution (e.g., 4K resolution) painting dataset PaintHD by considering both physical size and image resolution. We demonstrate the effectiveness of our method with extensive experiments on PaintHD by comparing with SISR and Ref-SR state-of-the-art methods.



