# Arxiv Papers in cs.CV on 2019-12-14
### RGait-NET: An Effective Network for Recovering Missing Information from Occluded Gait Cycles
- **Arxiv ID**: http://arxiv.org/abs/1912.06765v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06765v3)
- **Published**: 2019-12-14 01:53:10+00:00
- **Updated**: 2020-02-11 13:51:17+00:00
- **Authors**: Dhritimaan Das, Ayush Agarwal, Pratik Chattopadhyay, Lipo Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Gait of a person refers to his/her walking pattern, and according to medical studies gait of every individual is unique. Over the past decade, several computer vision-based gait recognition approaches have been proposed in which walking information corresponding to a complete gait cycle has been used to construct gait features for person identification. These methods compute gait features with the inherent assumption that a complete gait cycle is always available. However, in most public places occlusion is an inevitable occurrence, and due to this, only a fraction of a gait cycle gets captured by the monitoring camera. Unavailability of complete gait cycle information drastically affects the accuracy of the extracted features, and till date, only a few occlusion handling strategies to gait recognition have been proposed. But none of these performs reliably and robustly in the presence of a single cycle with incomplete information, and because of this practical application of gait recognition is quite limited. In this work, we develop deep learning-based algorithm to accurately identify the affected frames as well as predict the missing frames to reconstruct a complete gait cycle. While occlusion detection has been carried out by employing a VGG-16 model, the model for frame reconstruction is based on Long-Short Term Memory network that has been trained to optimize a multi-objective function based on dice coefficient and cross-entropy loss. The effectiveness of the proposed occlusion reconstruction algorithm is evaluated by computing the accuracy of the popular Gait Energy Feature on the reconstructed sequence. Experimental evaluation on public data sets and comparative analysis with other occlusion handling methods verify the effectiveness of our approach.



### Deep Context Maps: Agent Trajectory Prediction using Location-specific Latent Maps
- **Arxiv ID**: http://arxiv.org/abs/1912.06785v2
- **DOI**: 10.1109/LRA.2020.3004800
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06785v2)
- **Published**: 2019-12-14 05:16:59+00:00
- **Updated**: 2020-06-19 22:18:03+00:00
- **Authors**: Igor Gilitschenski, Guy Rosman, Arjun Gupta, Sertac Karaman, Daniela Rus
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach for agent motion prediction in cluttered environments. One of the main challenges in predicting agent motion is accounting for location and context-specific information. Our main contribution is the concept of learning context maps to improve the prediction task. Context maps are a set of location-specific latent maps that are trained alongside the predictor. Thus, the proposed maps are capable of capturing location context beyond visual context cues (e.g. usual average speeds and typical trajectories) or predefined map primitives (such as lanes and stop lines). We pose context map learning as a multi-task training problem and describe our map model and its incorporation into a state-of-the-art trajectory predictor. In extensive experiments, it is shown that use of learned maps can significantly improve predictor accuracy. Furthermore, the performance can be additionally boosted by providing partial knowledge of map semantics.



### Calorimetry with Deep Learning: Particle Simulation and Reconstruction for Collider Physics
- **Arxiv ID**: http://arxiv.org/abs/1912.06794v3
- **DOI**: 10.1140/epjc/s10052-020-8251-9
- **Categories**: **physics.ins-det**, cs.CV, cs.LG, hep-ex
- **Links**: [PDF](http://arxiv.org/pdf/1912.06794v3)
- **Published**: 2019-12-14 06:19:04+00:00
- **Updated**: 2020-01-08 22:41:43+00:00
- **Authors**: Dawit Belayneh, Federico Carminati, Amir Farbin, Benjamin Hooberman, Gulrukh Khattak, Miaoyuan Liu, Junze Liu, Dominick Olivito, Vit√≥ria Barin Pacela, Maurizio Pierini, Alexander Schwing, Maria Spiropulu, Sofia Vallecorsa, Jean-Roch Vlimant, Wei Wei, Matt Zhang
- **Comment**: 26 pages, 38 figures. Corrected typos and added additional references
  in v2. Extended Acknowledgements section in v3
- **Journal**: None
- **Summary**: Using detailed simulations of calorimeter showers as training data, we investigate the use of deep learning algorithms for the simulation and reconstruction of particles produced in high-energy physics collisions. We train neural networks on shower data at the calorimeter-cell level, and show significant improvements for simulation and reconstruction when using these networks compared to methods which rely on currently-used state-of-the-art algorithms. We define two models: an end-to-end reconstruction network which performs simultaneous particle identification and energy regression of particles when given calorimeter shower data, and a generative network which can provide reasonable modeling of calorimeter showers for different particle types at specified angles and energies. We investigate the optimization of our models with hyperparameter scans. Furthermore, we demonstrate the applicability of the reconstruction model to shower inputs from other detector geometries, specifically ATLAS-like and CMS-like geometries. These networks can serve as fast and computationally light methods for particle shower simulation and reconstruction for current and future experiments at particle colliders.



### Symmetric block-low-rank layers for fully reversible multilevel neural networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12137v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1912.12137v1)
- **Published**: 2019-12-14 06:29:15+00:00
- **Updated**: 2019-12-14 06:29:15+00:00
- **Authors**: Bas Peters, Eldad Haber, Keegan Lensink
- **Comment**: None
- **Journal**: None
- **Summary**: Factors that limit the size of the input and output of a neural network include memory requirements for the network states/activations to compute gradients, as well as memory for the convolutional kernels or other weights. The memory restriction is especially limiting for applications where we want to learn how to map volumetric data to the desired output, such as video-to-video. Recently developed fully reversible neural networks enable gradient computations using storage of the network states for a couple of layers only. While this saves a tremendous amount of memory, it is the convolutional kernels that take up most memory if fully reversible networks contain multiple invertible pooling/coarsening layers. Invertible coarsening operators such as the orthogonal wavelet transform cause the number of channels to grow explosively. We address this issue by combining fully reversible networks with layers that contain the convolutional kernels in a compressed form directly. Specifically, we introduce a layer that has a symmetric block-low-rank structure. In spirit, this layer is similar to bottleneck and squeeze-and-expand structures. We contribute symmetry by construction, and a combination of notation and flattening of tensors allows us to interpret these network structures in linear algebraic fashion as a block-low-rank matrix in factorized form and observe various properties. A video segmentation example shows that we can train a network to segment the entire video in one go, which would not be possible, in terms of memory requirements, using non-reversible networks and previously proposed reversible networks.



### A Novel Automation-Assisted Cervical Cancer Reading Method Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1912.06649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06649v1)
- **Published**: 2019-12-14 07:26:42+00:00
- **Updated**: 2019-12-14 07:26:42+00:00
- **Authors**: Yao Xiang, Wanxin Sun, Changli Pan, Meng Yan, Zhihua Yin, Yixiong Liang
- **Comment**: None
- **Journal**: None
- **Summary**: While most previous automation-assisted reading methods can improve efficiency, their performance often relies on the success of accurate cell segmentation and hand-craft feature extraction. This paper presents an efficient and totally segmentation-free method for automated cervical cell screening that utilizes modern object detector to directly detect cervical cells or clumps, without the design of specific hand-crafted feature. Specifically, we use the state-of-the-art CNN-based object detection methods, YOLOv3, as our baseline model. In order to improve the classification performance of hard examples which are four highly similar categories, we cascade an additional task-specific classifier. We also investigate the presence of unreliable annotations and cope with them by smoothing the distribution of noisy labels. We comprehensively evaluate our methods on test set which is consisted of 1,014 annotated cervical cell images with size of 4000*3000 and complex cellular situation corresponding to 10 categories. Our model achieves 97.5% sensitivity (Sens) and 67.8% specificity (Spec) on cervical cell image-level screening. Moreover, we obtain a mean Average Precision (mAP) of 63.4% on cervical cell-level diagnosis, and improve the Average Precision (AP) of hard examples which are valuable but difficult to distinguish. Our automation-assisted cervical cell reading method not only achieves cervical cell image-level classification but also provides more detailed location and category information of abnormal cells. The results indicate feasible performance of our method, together with the efficiency and robustness, providing a new idea for future development of computer-assisted reading system in clinical cervical screening.



### Cross-Batch Memory for Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.06798v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06798v3)
- **Published**: 2019-12-14 07:38:53+00:00
- **Updated**: 2020-04-21 02:54:52+00:00
- **Authors**: Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott
- **Comment**: CVPR 2020 Oral
- **Journal**: None
- **Summary**: Mining informative negative instances are of central importance to deep metric learning (DML), however this task is intrinsically limited by mini-batch training, where only a mini-batch of instances is accessible at each iteration. In this paper, we identify a "slow drift" phenomena by observing that the embedding features drift exceptionally slow even as the model parameters are updating throughout the training process. This suggests that the features of instances computed at preceding iterations can be used to considerably approximate their features extracted by the current model. We propose a cross-batch memory (XBM) mechanism that memorizes the embeddings of past iterations, allowing the model to collect sufficient hard negative pairs across multiple mini-batches - even over the whole dataset. Our XBM can be directly integrated into a general pair-based DML framework, where the XBM augmented DML can boost performance considerably. In particular, without bells and whistles, a simple contrastive loss with our XBM can have large R@1 improvements of 12%-22.5% on three large-scale image retrieval datasets, surpassing the most sophisticated state-of-the-art methods, by a large margin. Our XBM is conceptually simple, easy to implement - using several lines of codes, and is memory efficient - with a negligible 0.2 GB extra GPU memory. Code is available at: https://github.com/MalongTech/research-xbm.



### Cloud Removal in Satellite Images Using Spatiotemporal Generative Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.06838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06838v1)
- **Published**: 2019-12-14 13:03:31+00:00
- **Updated**: 2019-12-14 13:03:31+00:00
- **Authors**: Vishnu Sarukkai, Anirudh Jain, Burak Uzkent, Stefano Ermon
- **Comment**: Accepted to WACV 2020
- **Journal**: None
- **Summary**: Satellite images hold great promise for continuous environmental monitoring and earth observation. Occlusions cast by clouds, however, can severely limit coverage, making ground information extraction more difficult. Existing pipelines typically perform cloud removal with simple temporal composites and hand-crafted filters. In contrast, we cast the problem of cloud removal as a conditional image synthesis challenge, and we propose a trainable spatiotemporal generator network (STGAN) to remove clouds. We train our model on a new large-scale spatiotemporal dataset that we construct, containing 97640 image pairs covering all continents. We demonstrate experimentally that the proposed STGAN model outperforms standard models and can generate realistic cloud-free images with high PSNR and SSIM values across a variety of atmospheric conditions, leading to improved performance in downstream tasks such as land cover classification.



### Region and Object based Panoptic Image Synthesis through Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1912.06840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06840v1)
- **Published**: 2019-12-14 13:26:14+00:00
- **Updated**: 2019-12-14 13:26:14+00:00
- **Authors**: Heng Wang, Donghao Zhang, Yang Song, Heng Huang, Mei Chen, Weidong Cai
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Image-to-image translation is significant to many computer vision and machine learning tasks such as image synthesis and video synthesis. It has primary applications in the graphics editing and animation industries. With the development of generative adversarial networks, a lot of attention has been drawn to image-to-image translation tasks. In this paper, we propose and investigate a novel task named as panoptic-level image-to-image translation and a naive baseline of solving this task. Panoptic-level image translation extends the current image translation task to two separate objectives of semantic style translation (adjust the style of objects to that of different domains) and instance transfiguration (swap between different types of objects). The proposed task generates an image from a complete and detailed panoptic perspective which can enrich the context of real-world vision synthesis. Our contribution consists of the proposal of a significant task worth investigating and a naive baseline of solving it. The proposed baseline consists of the multiple instances sequential translation and semantic-level translation with domain-invariant content code.



### Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes
- **Arxiv ID**: http://arxiv.org/abs/1912.06842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06842v1)
- **Published**: 2019-12-14 13:33:20+00:00
- **Updated**: 2019-12-14 13:33:20+00:00
- **Authors**: Guolei Sun, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan, Ling Shao
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: The main requisite for fine-grained recognition task is to focus on subtle discriminative details that make the subordinate classes different from each other. We note that existing methods implicitly address this requirement and leave it to a data-driven pipeline to figure out what makes a subordinate class different from the others. This results in two major limitations: First, the network focuses on the most obvious distinctions between classes and overlooks more subtle inter-class variations. Second, the chance of misclassifying a given sample in any of the negative classes is considered equal, while in fact, confusions generally occur among only the most similar classes. Here, we propose to explicitly force the network to find the subtle differences among closely related classes. In this pursuit, we introduce two key novelties that can be easily plugged into existing end-to-end deep learning pipelines. On one hand, we introduce diversification block which masks the most salient features for an input to force the network to use more subtle cues for its correct classification. Concurrently, we introduce a gradient-boosting loss function that focuses only on the confusing classes for each sample and therefore moves swiftly along the direction on the loss surface that seeks to resolve these ambiguities. The synergy between these two blocks helps the network to learn more effective feature representations. Comprehensive experiments are performed on five challenging datasets. Our approach outperforms existing methods using similar experimental setting on all five datasets.



### The Liar's Walk: Detecting Deception with Gait and Gesture
- **Arxiv ID**: http://arxiv.org/abs/1912.06874v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06874v3)
- **Published**: 2019-12-14 16:18:56+00:00
- **Updated**: 2020-03-29 14:24:30+00:00
- **Authors**: Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray, Aniket Bera, Dinesh Manocha
- **Comment**: 10 pages, 6 Figures
- **Journal**: None
- **Summary**: We present a data-driven deep neural algorithm for detecting deceptive walking behavior using nonverbal cues like gaits and gestures. We conducted an elaborate user study, where we recorded many participants performing tasks involving deceptive walking. We extract the participants' walking gaits as series of 3D poses. We annotate various gestures performed by participants during their tasks. Based on the gait and gesture data, we train an LSTM-based deep neural network to obtain deep features. Finally, we use a combination of psychology-based gait, gesture, and deep features to detect deceptive walking with an accuracy of 88.41%. This is an improvement of 10.6% over handcrafted gait and gesture features and an improvement of 4.7% and 9.2% over classifiers based on the state-of-the-art emotion and action classification algorithms, respectively. Additionally, we present a novel dataset, DeceptiveWalk, that contains gaits and gestures with their associated deception labels. To the best of our knowledge, ours is the first algorithm to detect deceptive behavior using non-verbal cues of gait and gesture.



### Sensor-Independent Illumination Estimation for DNN Models
- **Arxiv ID**: http://arxiv.org/abs/1912.06888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06888v1)
- **Published**: 2019-12-14 17:36:36+00:00
- **Updated**: 2019-12-14 17:36:36+00:00
- **Authors**: Mahmoud Afifi, Michael S. Brown
- **Comment**: None
- **Journal**: BMVC 2019
- **Summary**: While modern deep neural networks (DNNs) achieve state-of-the-art results for illuminant estimation, it is currently necessary to train a separate DNN for each type of camera sensor. This means when a camera manufacturer uses a new sensor, it is necessary to retrain an existing DNN model with training images captured by the new sensor. This paper addresses this problem by introducing a novel sensor-independent illuminant estimation framework. Our method learns a sensor-independent working space that can be used to canonicalize the RGB values of any arbitrary camera sensor. Our learned space retains the linear property of the original sensor raw-RGB space and allows unseen camera sensors to be used on a single DNN model trained on this working space. We demonstrate the effectiveness of this approach on several different camera sensors and show it provides performance on par with state-of-the-art methods that were trained per sensor.



### Deep Poisoning: Towards Robust Image Data Sharing against Visual Disclosure
- **Arxiv ID**: http://arxiv.org/abs/1912.06895v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06895v2)
- **Published**: 2019-12-14 18:02:53+00:00
- **Updated**: 2020-11-09 02:56:26+00:00
- **Authors**: Hao Guo, Brian Dolhansky, Eric Hsin, Phong Dinh, Cristian Canton Ferrer, Song Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to respectively limited training data, different entities addressing the same vision task based on certain sensitive images may not train a robust deep network. This paper introduces a new vision task where various entities share task-specific image data to enlarge each other's training data volume without visually disclosing sensitive contents (e.g. illegal images). Then, we present a new structure-based training regime to enable different entities learn task-specific and reconstruction-proof image representations for image data sharing. Specifically, each entity learns a private Deep Poisoning Module (DPM) and insert it to a pre-trained deep network, which is designed to perform the specific vision task. The DPM deliberately poisons convolutional image features to prevent image reconstructions, while ensuring that the altered image data is functionally equivalent to the non-poisoned data for the specific vision task. Given this equivalence, the poisoned features shared from one entity could be used by another entity for further model refinement. Experimental results on image classification prove the efficacy of the proposed method.



### Asymmetric Generative Adversarial Networks for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1912.06931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06931v1)
- **Published**: 2019-12-14 21:24:41+00:00
- **Updated**: 2019-12-14 21:24:41+00:00
- **Authors**: Hao Tang, Dan Xu, Hong Liu, Nicu Sebe
- **Comment**: An extended version of a paper published in ACCV2018. arXiv admin
  note: substantial text overlap with arXiv:1901.04604
- **Journal**: None
- **Summary**: State-of-the-art models for unpaired image-to-image translation with Generative Adversarial Networks (GANs) can learn the mapping from the source domain to the target domain using a cycle-consistency loss. The intuition behind these models is that if we translate from one domain to the other and back again we should arrive at where we started. However, existing methods always adopt a symmetric network architecture to learn both forward and backward cycles. Because of the task complexity and cycle input difference between the source and target image domains, the inequality in bidirectional forward-backward cycle translations is significant and the amount of information between two domains is different. In this paper, we analyze the limitation of the existing symmetric GAN models in asymmetric translation tasks, and propose an AsymmetricGAN model with both translation and reconstruction generators of unequal sizes and different parameter-sharing strategy to adapt to the asymmetric need in both unsupervised and supervised image-to-image translation tasks. Moreover, the training stage of existing methods has the common problem of model collapse that degrades the quality of the generated images, thus we explore different optimization losses for better training of AsymmetricGAN, and thus make image-to-image translation with higher consistency and better stability. Extensive experiments on both supervised and unsupervised generative tasks with several publicly available datasets demonstrate that the proposed AsymmetricGAN achieves superior model capacity and better generation performance compared with existing GAN models. To the best of our knowledge, we are the first to investigate the asymmetric GAN framework on both unsupervised and supervised image-to-image translation tasks. The source code, data and trained models are available at https://github.com/Ha0Tang/AsymmetricGAN.



