# Arxiv Papers in cs.CV on 2019-12-18
### Analysing Deep Reinforcement Learning Agents Trained with Domain Randomisation
- **Arxiv ID**: http://arxiv.org/abs/1912.08324v2
- **DOI**: 10.1016/j.neucom.2022.04.005
- **Categories**: **cs.LG**, cs.CV, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.08324v2)
- **Published**: 2019-12-18 00:18:17+00:00
- **Updated**: 2020-02-17 20:53:30+00:00
- **Authors**: Tianhong Dai, Kai Arulkumaran, Tamara Gerbert, Samyakh Tukra, Feryal Behbahani, Anil Anthony Bharath
- **Comment**: None
- **Journal**: None
- **Summary**: Deep reinforcement learning has the potential to train robots to perform complex tasks in the real world without requiring accurate models of the robot or its environment. A practical approach is to train agents in simulation, and then transfer them to the real world. One popular method for achieving transferability is to use domain randomisation, which involves randomly perturbing various aspects of a simulated environment in order to make trained agents robust to the reality gap. However, less work has gone into understanding such agents - which are deployed in the real world - beyond task performance. In this work we examine such agents, through qualitative and quantitative comparisons between agents trained with and without visual domain randomisation. We train agents for Fetch and Jaco robots on a visuomotor control task and evaluate how well they generalise using different testing conditions. Finally, we investigate the internals of the trained agents by using a suite of interpretability techniques. Our results show that the primary outcome of domain randomisation is more robust, entangled representations, accompanied with larger weights with greater spatial structure; moreover, the types of changes are heavily influenced by the task setup and presence of additional proprioceptive inputs. Additionally, we demonstrate that our domain randomised agents require higher sample complexity, can overfit and more heavily rely on recurrent processing. Furthermore, even with an improved saliency method introduced in this work, we show that qualitative studies may not always correspond with quantitative measures, necessitating the combination of inspection tools in order to provide sufficient insights into the behaviour of trained agents.



### Cost Volume Pyramid Based Depth Inference for Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/1912.08329v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08329v3)
- **Published**: 2019-12-18 00:48:00+00:00
- **Updated**: 2020-03-30 03:32:51+00:00
- **Authors**: Jiayu Yang, Wei Mao, Jose M. Alvarez, Miaomiao Liu
- **Comment**: CVPR 2020 oral
- **Journal**: None
- **Summary**: We propose a cost volume-based neural network for depth inference from multi-view images. We demonstrate that building a cost volume pyramid in a coarse-to-fine manner instead of constructing a cost volume at a fixed resolution leads to a compact, lightweight network and allows us inferring high resolution depth maps to achieve better reconstruction results. To this end, we first build a cost volume based on uniform sampling of fronto-parallel planes across the entire depth range at the coarsest resolution of an image. Then, given current depth estimate, we construct new cost volumes iteratively on the pixelwise depth residual to perform depth map refinement. While sharing similar insight with Point-MVSNet as predicting and refining depth iteratively, we show that working on cost volume pyramid can lead to a more compact, yet efficient network structure compared with the Point-MVSNet on 3D points. We further provide detailed analyses of the relation between (residual) depth sampling and image resolution, which serves as a principle for building compact cost volume pyramid. Experimental results on benchmark datasets show that our model can perform 6x faster and has similar performance as state-of-the-art methods. Code is available at https://github.com/JiayuYANG/CVP-MVSNet



### Learning Regional Attraction for Line Segment Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.09344v1
- **DOI**: 10.1109/TPAMI.2019.2958642
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.09344v1)
- **Published**: 2019-12-18 01:05:23+00:00
- **Updated**: 2019-12-18 01:05:23+00:00
- **Authors**: Nan Xue, Song Bai, Fu-Dong Wang, Gui-Song Xia, Tianfu Wu, Liangpei Zhang, Philip H. S. Torr
- **Comment**: Accepted to IEEE TPAMI. arXiv admin note: text overlap with
  arXiv:1812.02122
- **Journal**: None
- **Summary**: This paper presents regional attraction of line segment maps, and hereby poses the problem of line segment detection (LSD) as a problem of region coloring. Given a line segment map, the proposed regional attraction first establishes the relationship between line segments and regions in the image lattice. Based on this, the line segment map is equivalently transformed to an attraction field map (AFM), which can be remapped to a set of line segments without loss of information. Accordingly, we develop an end-to-end framework to learn attraction field maps for raw input images, followed by a squeeze module to detect line segments. Apart from existing works, the proposed detector properly handles the local ambiguity and does not rely on the accurate identification of edge pixels. Comprehensive experiments on the Wireframe dataset and the YorkUrban dataset demonstrate the superiority of our method. In particular, we achieve an F-measure of 0.831 on the Wireframe dataset, advancing the state-of-the-art performance by 10.3 percent.



### Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data
- **Arxiv ID**: http://arxiv.org/abs/1912.08364v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08364v1)
- **Published**: 2019-12-18 03:29:53+00:00
- **Updated**: 2019-12-18 03:29:53+00:00
- **Authors**: Jeya Maria Jose V., Rajeev Yasarla, Puyang Wang, Ilker Hacihaliloglu, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of anatomical landmarks from ultrasound (US) plays an important role in the management of preterm neonates with a very low birth weight due to the increased risk of developing intraventricular hemorrhage (IVH) or other complications. One major problem in developing an automatic segmentation method for this task is the limited availability of annotated data. To tackle this issue, we propose a novel image synthesis method using multi-scale self attention generator to synthesize US images from various segmentation masks. We show that our method can synthesize high-quality US images for every manipulated segmentation label with qualitative and quantitative improvements over the recent state-of-the-art synthesis methods. Furthermore, for the segmentation task, we propose a novel method, called Confidence-guided Brain Anatomy Segmentation (CBAS) network, where segmentation and corresponding confidence maps are estimated at different scales. In addition, we introduce a technique which guides CBAS to learn the weights based on the confidence measure about the estimate. Extensive experiments demonstrate that the proposed method for both synthesis and segmentation tasks achieve significant improvements over the recent state-of-the-art methods. In particular, we show that the new synthesis framework can be used to generate realistic US images which can be used to improve the performance of a segmentation algorithm.



### P-CapsNets: a General Form of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08367v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08367v1)
- **Published**: 2019-12-18 03:45:17+00:00
- **Updated**: 2019-12-18 03:45:17+00:00
- **Authors**: Zhenhua Chen, Xiwen Li, Chuhua Wang, David Crandall
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Pure CapsNets (P-CapsNets) which is a generation of normal CNNs structurally. Specifically, we make three modifications to current CapsNets. First, we remove routing procedures from CapsNets based on the observation that the coupling coefficients can be learned implicitly. Second, we replace the convolutional layers in CapsNets to improve efficiency. Third, we package the capsules into rank-3 tensors to further improve efficiency. The experiment shows that P-CapsNets achieve better performance than CapsNets with varied routing procedures by using significantly fewer parameters on MNIST\&CIFAR10. The high efficiency of P-CapsNets is even comparable to some deep compressing models. For example, we achieve more than 99\% percent accuracy on MNIST by using only 3888 parameters. We visualize the capsules as well as the corresponding correlation matrix to show a possible way of initializing CapsNets in the future. We also explore the adversarial robustness of P-CapsNets compared to CNNs.



### The CNN-based Coronary Occlusion Site Localization with Effective Preprocessing Method
- **Arxiv ID**: http://arxiv.org/abs/1912.08375v2
- **DOI**: 10.1002/tee.23225
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08375v2)
- **Published**: 2019-12-18 04:44:39+00:00
- **Updated**: 2019-12-19 04:30:03+00:00
- **Authors**: YeongHyeon Park, Il Dong Yun, Si-Hyuck Kang
- **Comment**: None
- **Journal**: None
- **Summary**: The Coronary Artery Occlusion (CAO) acutely comes to human, and it highly threats the human's life. When CAO detected, Percutaneous Coronary Intervention (PCI) should be conducted timely. Before PCI, localizing the CAO is needed firstly, because the heart is covered with various arteries. We handle the three kinds of CAO in this paper and our purpose is not only localization of CAO but also improving the localizing performance via preprocessing method. We improve localization performance from a minimum of 0.150 to a maximum of 0.372 via our noise reduction and pulse extraction based method.



### Iterative and Adaptive Sampling with Spatial Attention for Black-Box Model Explanations
- **Arxiv ID**: http://arxiv.org/abs/1912.08387v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08387v1)
- **Published**: 2019-12-18 05:37:30+00:00
- **Updated**: 2019-12-18 05:37:30+00:00
- **Authors**: Bhavan Vasu, Chengjiang Long
- **Comment**: The paper was accepted to the IEEE Winter Conference on Applications
  of Computer Vision (WACV'2020)
- **Journal**: None
- **Summary**: Deep neural networks have achieved great success in many real-world applications, yet it remains unclear and difficult to explain their decision-making process to an end-user. In this paper, we address the explainable AI problem for deep neural networks with our proposed framework, named IASSA, which generates an importance map indicating how salient each pixel is for the model's prediction with an iterative and adaptive sampling module. We employ an affinity matrix calculated on multi-level deep learning features to explore long-range pixel-to-pixel correlation, which can shift the saliency values guided by our long-range and parameter-free spatial attention. Extensive experiments on the MS-COCO dataset show that our proposed approach matches or exceeds the performance of state-of-the-art black-box explanation methods.



### Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss
- **Arxiv ID**: http://arxiv.org/abs/1912.08393v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08393v2)
- **Published**: 2019-12-18 05:49:37+00:00
- **Updated**: 2021-07-19 10:19:50+00:00
- **Authors**: Jia Li, Jinming Su, Changqun Xia, Mingcan Ma, Yonghong Tian
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: By the aid of attention mechanisms to weight the image features adaptively, recent advanced deep learning-based models encourage the predicted results to approximate the ground-truth masks with as large predictable areas as possible, thus achieving the state-of-the-art performance. However, these methods do not pay enough attention to small areas prone to misprediction. In this way, it is still tough to accurately locate salient objects due to the existence of regions with indistinguishable foreground and background and regions with complex or fine structures. To address these problems, we propose a novel convolutional neural network with purificatory mechanism and structural similarity loss. Specifically, in order to better locate preliminary salient objects, we first introduce the promotion attention, which is based on spatial and channel attention mechanisms to promote attention to salient regions. Subsequently, for the purpose of restoring the indistinguishable regions that can be regarded as error-prone regions of one model, we propose the rectification attention, which is learned from the areas of wrong prediction and guide the network to focus on error-prone regions thus rectifying errors. Through these two attentions, we use the Purificatory Mechanism to impose strict weights with different regions of the whole salient objects and purify results from hard-to-distinguish regions, thus accurately predicting the locations and details of salient objects. In addition to paying different attention to these hard-to-distinguish regions, we also consider the structural constraints on complex regions and propose the Structural Similarity Loss. In experiments, the proposed approach outperforms 19 state-of-the-art methods on six datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.



### Feature engineering workflow for activity recognition from synchronized inertial measurement units
- **Arxiv ID**: http://arxiv.org/abs/1912.08394v1
- **DOI**: 10.1007/978-981-15-3651-9_20
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08394v1)
- **Published**: 2019-12-18 05:51:42+00:00
- **Updated**: 2019-12-18 05:51:42+00:00
- **Authors**: Andreas W. Kempa-Liehr, Jonty Oram, Andrew Wong, Mark Finch, Thor Besier
- **Comment**: Multi-Sensor for Action and Gesture Recognition (MAGR), ACPR 2019
  Workshop, Auckland, New Zealand
- **Journal**: None
- **Summary**: The ubiquitous availability of wearable sensors is responsible for driving the Internet-of-Things but is also making an impact on sport sciences and precision medicine. While human activity recognition from smartphone data or other types of inertial measurement units (IMU) has evolved to one of the most prominent daily life examples of machine learning, the underlying process of time-series feature engineering still seems to be time-consuming. This lengthy process inhibits the development of IMU-based machine learning applications in sport science and precision medicine. This contribution discusses a feature engineering workflow, which automates the extraction of time-series feature on based on the FRESH algorithm (FeatuRe Extraction based on Scalable Hypothesis tests) to identify statistically significant features from synchronized IMU sensors (IMeasureU Ltd, NZ). The feature engineering workflow has five main steps: time-series engineering, automated time-series feature extraction, optimized feature extraction, fitting of a specialized classifier, and deployment of optimized machine learning pipeline. The workflow is discussed for the case of a user-specific running-walking classification, and the generalization to a multi-user multi-activity classification is demonstrated.



### Semantic Regularization: Improve Few-shot Image Classification by Reducing Meta Shift
- **Arxiv ID**: http://arxiv.org/abs/1912.08395v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08395v2)
- **Published**: 2019-12-18 05:54:49+00:00
- **Updated**: 2020-03-09 15:37:41+00:00
- **Authors**: Da Chen, Yongliang Yang, Zunlei Feng, Xiang Wu, Mingli Song, Wenbin Li, Yuan He, Hui Xue, Feng Mao
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image classification requires the classifier to robustly cope with unseen classes even if there are only a few samples for each class. Recent advances benefit from the meta-learning process where episodic tasks are formed to train a model that can adapt to class change. However, these task sare independent to each other and existing works mainly rely on limited samples of individual support set in a single meta task. This strategy leads to severe meta shift issues across multiple tasks, meaning the learned prototypes or class descriptors are not stable as each task only involves their own support set. To avoid this problem, we propose a concise Semantic RegularizationNetwork to learn a common semantic space under the framework of meta-learning. In this space, all class descriptors can be regularized by the learned semantic basis, which can effectively solve the meta shift problem. The key is to train a class encoder and decoder structure that can encode the sample embedding features into the semantic domain with trained semantic basis, and generate a more stable and general class descriptor from the decoder. We evaluate our work by extensive comparisons with previous methods on three benchmark datasets (MiniImageNet, TieredImageNet, and CUB). The results show that the semantic regularization module improves performance by 4%-7% over the baseline method, and achieves competitive results over the current state-of-the-art models.



### Spotting Macro- and Micro-expression Intervals in Long Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1912.11985v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11985v3)
- **Published**: 2019-12-18 06:04:22+00:00
- **Updated**: 2020-03-20 09:41:20+00:00
- **Authors**: Ying He, Su-Jing Wang, Jingting Li, Moi Hoon Yap
- **Comment**: 7 pages, 4 figures and 3 tables
- **Journal**: None
- **Summary**: This paper presents baseline results for the Third Facial Micro-Expression Grand Challenge (MEGC 2020). Both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos are spotted by employing the method of Main Directional Maximal Difference Analysis (MDMD). The MDMD method uses the magnitude maximal difference in the main direction of optical flow features to spot facial movements. The single-frame prediction results of the original MDMD method are post-processed into reasonable video intervals. The metric F1-scores of baseline results are evaluated: for CAS(ME)$^2$, the F1-scores are 0.1196 and 0.0082 for macro- and micro-expressions respectively, and the overall F1-score is 0.0376; for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions respectively, and the overall F1-score is 0.0445. The baseline project codes are publicly available at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.



### Self-Attention Network for Skeleton-based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.08435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08435v1)
- **Published**: 2019-12-18 08:07:07+00:00
- **Updated**: 2019-12-18 08:07:07+00:00
- **Authors**: Sangwoo Cho, Muhammad Hasan Maqbool, Fei Liu, Hassan Foroosh
- **Comment**: WACV 2020 Paper
- **Journal**: None
- **Summary**: Skeleton-based action recognition has recently attracted a lot of attention. Researchers are coming up with new approaches for extracting spatio-temporal relations and making considerable progress on large-scale skeleton-based datasets. Most of the architectures being proposed are based upon recurrent neural networks (RNNs), convolutional neural networks (CNNs) and graph-based CNNs. When it comes to skeleton-based action recognition, the importance of long term contextual information is central which is not captured by the current architectures. In order to come up with a better representation and capturing of long term spatio-temporal relationships, we propose three variants of Self-Attention Network (SAN), namely, SAN-V1, SAN-V2 and SAN-V3. Our SAN variants has the impressive capability of extracting high-level semantics by capturing long-range correlations. We have also integrated the Temporal Segment Network (TSN) with our SAN variants which resulted in improved overall performance. Different configurations of Self-Attention Network (SAN) variants and Temporal Segment Network (TSN) are explored with extensive experiments. Our chosen configuration outperforms state-of-the-art Top-1 and Top-5 by 4.4% and 7.9% respectively on Kinetics and shows consistently better performance than state-of-the-art methods on NTU RGB+D.



### Relational Mimic for Visual Adversarial Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.08444v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08444v1)
- **Published**: 2019-12-18 08:19:39+00:00
- **Updated**: 2019-12-18 08:19:39+00:00
- **Authors**: Lionel Blondé, Yichuan Charlie Tang, Jian Zhang, Russ Webb
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a new method for imitation learning from video demonstrations. Our method, Relational Mimic (RM), improves on previous visual imitation learning methods by combining generative adversarial networks and relational learning. RM is flexible and can be used in conjunction with other recent advances in generative adversarial imitation learning to better address the need for more robust and sample-efficient approaches. In addition, we introduce a new neural network architecture that improves upon the previous state-of-the-art in reinforcement learning and illustrate how increasing the relational reasoning capabilities of the agent enables the latter to achieve increasingly higher performance in a challenging locomotion task with pixel inputs. Finally, we study the effects and contributions of relational learning in policy evaluation, policy improvement and reward learning through ablation studies.



### DADA: Driver Attention Prediction in Driving Accident Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1912.12148v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12148v2)
- **Published**: 2019-12-18 09:41:06+00:00
- **Updated**: 2023-01-05 01:52:56+00:00
- **Authors**: Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, Hongkai Yu
- **Comment**: published by IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Driver attention prediction is becoming an essential research problem in human-like driving systems. This work makes an attempt to predict the driver attention in driving accident scenarios (DADA). However, challenges tread on the heels of that because of the dynamic traffic scene, intricate and imbalanced accident categories. In this work, we design a semantic context induced attentive fusion network (SCAFNet). We first segment the RGB video frames into the images with different semantic regions (i.e., semantic images), where each region denotes one kind of semantic categories of the scene (e.g., road, trees, etc.), and learn the spatio-temporal features of RGB frames and semantic images in two parallel paths simultaneously. Then, the learned features are fused by an attentive fusion network to find the semantic-induced scene variation in driver attention prediction. The contributions are three folds. 1) With the semantic images, we introduce their semantic context features and verified the manifest promotion effect for helping the driver attention prediction, where the semantic context features are modeled by a graph convolution network (GCN) on semantic images; 2) We fuse the semantic context features of semantic images and the features of RGB frames in an attentive strategy, and the fused details are transferred over frames by a convolutional LSTM module to obtain the attention map of each video frame with the consideration of historical scene variation in driving situations; 3) The superiority of the proposed method is evaluated on our previously collected dataset (named as DADA-2000) and two other challenging datasets with state-of-the-art methods. DADA-2000 is available at https://github.com/JWFangit/LOTVS-DADA.



### FuseSeg: LiDAR Point Cloud Segmentation Fusing Multi-Modal Data
- **Arxiv ID**: http://arxiv.org/abs/1912.08487v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, I.4.6; I.4.8; I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.08487v2)
- **Published**: 2019-12-18 09:51:13+00:00
- **Updated**: 2019-12-19 14:52:46+00:00
- **Authors**: Georg Krispel, Michael Opitz, Georg Waltner, Horst Possegger, Horst Bischof
- **Comment**: Accepted for publication in WACV 2020
- **Journal**: None
- **Summary**: We introduce a simple yet effective fusion method of LiDAR and RGB data to segment LiDAR point clouds. Utilizing the dense native range representation of a LiDAR sensor and the setup calibration, we establish point correspondences between the two input modalities. Subsequently, we are able to warp and fuse the features from one domain into the other. Therefore, we can jointly exploit information from both data sources within one single network. To show the merit of our method, we extend SqueezeSeg, a point cloud segmentation network, with an RGB feature branch and fuse it into the original structure. Our extension called FuseSeg leads to an improvement of up to 18% IoU on the KITTI benchmark. In addition to the improved accuracy, we also achieve real-time performance at 50 fps, five times as fast as the KITTI LiDAR data recording speed.



### Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware
- **Arxiv ID**: http://arxiv.org/abs/1912.08519v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08519v3)
- **Published**: 2019-12-18 11:07:12+00:00
- **Updated**: 2021-04-18 04:46:31+00:00
- **Authors**: Yeshwanth Ravi Theja Bethi, Sathyaprakash Narayanan, Venkat Rangan, Chetan Singh Thakur
- **Comment**: None
- **Journal**: None
- **Summary**: Every day around the world, interminable terabytes of data are being captured for surveillance purposes. A typical 1-2MP CCTV camera generates around 7-12GB of data per day. Frame-by-frame processing of such enormous amount of data requires hefty computational resources. In recent years, compressive sensing approaches have shown impressive results in signal processing by reducing the sampling bandwidth. Different sampling mechanisms were developed to incorporate compressive sensing in image and video acquisition. Pixel-wise coded exposure is one among the promising sensing paradigms for capturing videos in the compressed domain, which was also realized into an all-CMOS sensor \cite{Xiong2017}. Though cameras that perform compressive sensing save a lot of bandwidth at the time of sampling and minimize the memory required to store videos, we cannot do much in terms of processing until the videos are reconstructed to the original frames. But, the reconstruction of compressive-sensed (CS) videos still takes a lot of time and is also computationally expensive. In this work, we show that object detection and localization can be possible directly on the CS frames (easily upto 20x compression). To our knowledge, this is the first time that the problem of object detection and localization on CS frames has been attempted. Hence, we also created a dataset for training in the CS domain. We were able to achieve a good accuracy of 46.27\% mAP(Mean Average Precision) with the proposed model with an inference time of 23ms directly on the compressed frames(approx. 20 original domain frames), this facilitated for real-time inference which was verified on NVIDIA TX2 embedded board. Our framework will significantly reduce the communication bandwidth, and thus reduction in power as the video compression will be done at the image sensor processing core.



### Contextually Plausible and Diverse 3D Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.08521v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08521v4)
- **Published**: 2019-12-18 11:13:44+00:00
- **Updated**: 2020-12-05 08:59:14+00:00
- **Authors**: Sadegh Aliakbarian, Fatemeh Sadat Saleh, Lars Petersson, Stephen Gould, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the task of diverse 3D human motion prediction, that is, forecasting multiple plausible future 3D poses given a sequence of observed 3D poses. In this context, a popular approach consists of using a Conditional Variational Autoencoder (CVAE). However, existing approaches that do so either fail to capture the diversity in human motion, or generate diverse but semantically implausible continuations of the observed motion. In this paper, we address both of these problems by developing a new variational framework that accounts for both diversity and context of the generated future motion. To this end, and in contrast to existing approaches, we condition the sampling of the latent variable that acts as source of diversity on the representation of the past observation, thus encouraging it to carry relevant information. Our experiments demonstrate that our approach yields motions not only of higher quality while retaining diversity, but also that preserve the contextual information contained in the observed 3D pose sequence.



### GlobalTrack: A Simple and Strong Baseline for Long-term Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.08531v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08531v1)
- **Published**: 2019-12-18 11:31:19+00:00
- **Updated**: 2019-12-18 11:31:19+00:00
- **Authors**: Lianghua Huang, Xin Zhao, Kaiqi Huang
- **Comment**: Accepted in AAAI2020
- **Journal**: None
- **Summary**: A key capability of a long-term tracker is to search for targets in very large areas (typically the entire image) to handle possible target absences or tracking failures. However, currently there is a lack of such a strong baseline for global instance search. In this work, we aim to bridge this gap. Specifically, we propose GlobalTrack, a pure global instance search based tracker that makes no assumption on the temporal consistency of the target's positions and scales. GlobalTrack is developed based on two-stage object detectors, and it is able to perform full-image and multi-scale search of arbitrary instances with only a single query as the guide. We further propose a cross-query loss to improve the robustness of our approach against distractors. With no online learning, no punishment on position or scale changes, no scale smoothing and no trajectory refinement, our pure global instance search based tracker achieves comparable, sometimes much better performance on four large-scale tracking benchmarks (i.e., 52.1% AUC on LaSOT, 63.8% success rate on TLP, 60.3% MaxGM on OxUvA and 75.4% normalized precision on TrackingNet), compared to state-of-the-art approaches that typically require complex post-processing. More importantly, our tracker runs without cumulative errors, i.e., any type of temporary tracking failures will not affect its performance on future frames, making it ideal for long-term tracking. We hope this work will be a strong baseline for long-term tracking and will stimulate future works in this area. Code is available at https://github.com/huanglianghua/GlobalTrack.



### s-DRN: Stabilized Developmental Resonance Network
- **Arxiv ID**: http://arxiv.org/abs/1912.08541v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.08541v2)
- **Published**: 2019-12-18 11:47:44+00:00
- **Updated**: 2020-07-15 05:06:52+00:00
- **Authors**: In-Ug Yoon, Ue-Hwan Kim, Jong-Hwan
- **Comment**: Under review
- **Journal**: None
- **Summary**: Online incremental clustering of sequentially incoming data without prior knowledge suffers from changing cluster numbers and tends to fall into local extrema according to given data order. To overcome these limitations, we propose a stabilized developmental resonance network (s-DRN). First, we analyze the instability of the conventional choice function during the node activation process and design a scalable activation function to make clustering performance stable over all input data scales. Next, we devise three criteria for the node grouping algorithm: distance, intersection over union (IoU) and size criteria. The proposed node grouping algorithm effectively excludes unnecessary clusters from incrementally created clusters, diminishes the performance dependency on vigilance parameters and makes the clustering process robust. To verify the performance of the proposed s-DRN model, comparative studies are conducted on six real-world datasets whose statistical characteristics are distinctive. The comparative studies demonstrate the proposed s-DRN outperforms baselines in terms of stability and accuracy.



### Lightweight and Robust Representation of Economic Scales from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1912.08197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1912.08197v1)
- **Published**: 2019-12-18 11:53:01+00:00
- **Updated**: 2019-12-18 11:53:01+00:00
- **Authors**: Sungwon Han, Donghyun Ahn, Hyunji Cha, Jeasurk Yang, Sungwon Park, Meeyoung Cha
- **Comment**: Accepted for oral presentation at AAAI 2020
- **Journal**: None
- **Summary**: Satellite imagery has long been an attractive data source that provides a wealth of information on human-inhabited areas. While super resolution satellite images are rapidly becoming available, little study has focused on how to extract meaningful information about human habitation patterns and economic scales from such data. We present READ, a new approach for obtaining essential spatial representation for any given district from high-resolution satellite imagery based on deep neural networks. Our method combines transfer learning and embedded statistics to efficiently learn critical spatial characteristics of arbitrary size areas and represent them into a fixed-length vector with minimal information loss. Even with a small set of labels, READ can distinguish subtle differences between rural and urban areas and infer the degree of urbanization. An extensive evaluation demonstrates the model outperforms the state-of-the-art in predicting economic scales, such as population density for South Korea (R^2=0.9617), and shows a high potential use for developing countries where district-level economic scales are not known.



### Cooperative Perception for 3D Object Detection in Driving Scenarios using Infrastructure Sensors
- **Arxiv ID**: http://arxiv.org/abs/1912.12147v2
- **DOI**: 10.1109/TITS.2020.3028424
- **Categories**: **cs.CV**, cs.LG, cs.MA, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12147v2)
- **Published**: 2019-12-18 12:19:27+00:00
- **Updated**: 2020-10-30 08:59:08+00:00
- **Authors**: Eduardo Arnold, Mehrdad Dianati, Robert de Temple, Saber Fallah
- **Comment**: 13 pages, 4 tables, 7 figures. Published in IEEE Transactions on
  Intelligent Transportation Systems
- **Journal**: None
- **Summary**: 3D object detection is a common function within the perception system of an autonomous vehicle and outputs a list of 3D bounding boxes around objects of interest. Various 3D object detection methods have relied on fusion of different sensor modalities to overcome limitations of individual sensors. However, occlusion, limited field-of-view and low-point density of the sensor data cannot be reliably and cost-effectively addressed by multi-modal sensing from a single point of view. Alternatively, cooperative perception incorporates information from spatially diverse sensors distributed around the environment as a way to mitigate these limitations. This article proposes two schemes for cooperative 3D object detection using single modality sensors. The early fusion scheme combines point clouds from multiple spatially diverse sensing points of view before detection. In contrast, the late fusion scheme fuses the independently detected bounding boxes from multiple spatially diverse sensors. We evaluate the performance of both schemes, and their hybrid combination, using a synthetic cooperative dataset created in two complex driving scenarios, a T-junction and a roundabout. The evaluation shows that the early fusion approach outperforms late fusion by a significant margin at the cost of higher communication bandwidth. The results demonstrate that cooperative perception can recall more than 95% of the objects as opposed to 30% for single-point sensing in the most challenging scenario. To provide practical insights into the deployment of such system, we report how the number of sensors and their configuration impact the detection performance of the system.



### CPGAN: Full-Spectrum Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1912.08562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08562v2)
- **Published**: 2019-12-18 12:31:42+00:00
- **Updated**: 2020-07-12 10:20:02+00:00
- **Authors**: Jiadong Liang, Wenjie Pei, Feng Lu
- **Comment**: 18 pages,8 figures
- **Journal**: None
- **Summary**: Typical methods for text-to-image synthesis seek to design effective generative architecture to model the text-to-image mapping directly. It is fairly arduous due to the cross-modality translation. In this paper we circumvent this problem by focusing on parsing the content of both the input text and the synthesized image thoroughly to model the text-to-image consistency in the semantic level. Particularly, we design a memory structure to parse the textual content by exploring semantic correspondence between each word in the vocabulary to its various visual contexts across relevant images during text encoding. Meanwhile, the synthesized image is parsed to learn its semantics in an object-aware manner. Moreover, we customize a conditional discriminator to model the fine-grained correlations between words and image sub-regions to push for the text-image semantic alignment. Extensive experiments on COCO dataset manifest that our model advances the state-of-the-art performance significantly (from 35.69 to 52.73 in Inception Score).



### ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/1912.08568v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.08568v2)
- **Published**: 2019-12-18 12:44:31+00:00
- **Updated**: 2020-06-18 07:45:16+00:00
- **Authors**: Sena Kiciroglu, Helge Rhodin, Sudipta N. Sinha, Mathieu Salzmann, Pascal Fua
- **Comment**: For associated video, see https://youtu.be/i58Bu-hbZHs Published in
  CVPR 2020
- **Journal**: None
- **Summary**: The accuracy of monocular 3D human pose estimation depends on the viewpoint from which the image is captured. While freely moving cameras, such as on drones, provide control over this viewpoint, automatically positioning them at the location which will yield the highest accuracy remains an open problem. This is the problem that we address in this paper. Specifically, given a short video sequence, we introduce an algorithm that predicts which viewpoints should be chosen to capture future frames so as to maximize 3D human pose estimation accuracy. The key idea underlying our approach is a method to estimate the uncertainty of the 3D body pose estimates. We integrate several sources of uncertainty, originating from deep learning based regressors and temporal smoothness. Our motion planner yields improved 3D body pose estimates and outperforms or matches existing ones that are based on person following and orbiting.



### Image Restoration using Plug-and-Play CNN MAP Denoisers
- **Arxiv ID**: http://arxiv.org/abs/1912.09299v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09299v2)
- **Published**: 2019-12-18 12:57:27+00:00
- **Updated**: 2019-12-20 08:52:18+00:00
- **Authors**: Siavash Bigdeli, David Honzátko, Sabine Süsstrunk, L. Andrea Dunbar
- **Comment**: Code and models available at
  https://github.com/DawyD/cnn-map-denoiser . Accepted for publication in
  VISAPP 2020
- **Journal**: None
- **Summary**: Plug-and-play denoisers can be used to perform generic image restoration tasks independent of the degradation type. These methods build on the fact that the Maximum a Posteriori (MAP) optimization can be solved using smaller sub-problems, including a MAP denoising optimization. We present the first end-to-end approach to MAP estimation for image denoising using deep neural networks. We show that our method is guaranteed to minimize the MAP denoising objective, which is then used in an optimization algorithm for generic image restoration. We provide theoretical analysis of our approach and show the quantitative performance of our method in several experiments. Our experimental results show that the proposed method can achieve 70x faster performance compared to the state-of-the-art, while maintaining the theoretical perspective of MAP.



### A Cross-Modal Image Fusion Method Guided by Human Visual Characteristics
- **Arxiv ID**: http://arxiv.org/abs/1912.08577v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1912.08577v4)
- **Published**: 2019-12-18 13:07:20+00:00
- **Updated**: 2020-06-20 09:43:38+00:00
- **Authors**: Aiqing Fang, Xinbo Zhao, Jiaqi Yang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The characteristics of feature selection, nonlinear combination and multi-task auxiliary learning mechanism of the human visual perception system play an important role in real-world scenarios, but the research of image fusion theory based on the characteristics of human visual perception is less. Inspired by the characteristics of human visual perception, we propose a robust multi-task auxiliary learning optimization image fusion theory. Firstly, we combine channel attention model with nonlinear convolutional neural network to select features and fuse nonlinear features. Then, we analyze the impact of the existing image fusion loss on the image fusion quality, and establish the multi-loss function model of unsupervised learning network. Secondly, aiming at the multi-task auxiliary learning mechanism of human visual perception system, we study the influence of multi-task auxiliary learning mechanism on image fusion task on the basis of single task multi-loss network model. By simulating the three characteristics of human visual perception system, the fused image is more consistent with the mechanism of human brain image fusion. Finally, in order to verify the superiority of our algorithm, we carried out experiments on the combined vision system image data set, and extended our algorithm to the infrared and visible image and the multi-focus image public data set for experimental verification. The experimental results demonstrate the superiority of our fusion theory over state-of-arts in generality and robustness.



### Unsupervised Change Detection in Multi-temporal VHR Images Based on Deep Kernel PCA Convolutional Mapping Network
- **Arxiv ID**: http://arxiv.org/abs/1912.08628v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08628v1)
- **Published**: 2019-12-18 14:20:11+00:00
- **Updated**: 2019-12-18 14:20:11+00:00
- **Authors**: Chen Wu, Hongruixuan Chen, Bo Do, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of Earth observation technology, very-high-resolution (VHR) image has become an important data source of change detection. Nowadays, deep learning methods have achieved conspicuous performance in the change detection of VHR images. Nonetheless, most of the existing change detection models based on deep learning require annotated training samples. In this paper, a novel unsupervised model called kernel principal component analysis (KPCA) convolution is proposed for extracting representative features from multi-temporal VHR images. Based on the KPCA convolution, an unsupervised deep siamese KPCA convolutional mapping network (KPCA-MNet) is designed for binary and multi-class change detection. In the KPCA-MNet, the high-level spatial-spectral feature maps are extracted by a deep siamese network consisting of weight-shared PCA convolution layers. Then, the change information in the feature difference map is mapped into a 2-D polar domain. Finally, the change detection results are generated by threshold segmentation and clustering algorithms. All procedures of KPCA-MNet does not require labeled data. The theoretical analysis and experimental results demonstrate the validity, robustness, and potential of the proposed method in two binary change detection data sets and one multi-class change detection data set.



### Detecting Adversarial Attacks On Audiovisual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.08639v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.08639v2)
- **Published**: 2019-12-18 14:43:43+00:00
- **Updated**: 2021-02-12 17:44:49+00:00
- **Authors**: Pingchuan Ma, Stavros Petridis, Maja Pantic
- **Comment**: Accepted to ICASSP 2021
- **Journal**: None
- **Summary**: Adversarial attacks pose a threat to deep learning models. However, research on adversarial detection methods, especially in the multi-modal domain, is very limited. In this work, we propose an efficient and straightforward detection method based on the temporal correlation between audio and video streams. The main idea is that the correlation between audio and video in adversarial examples will be lower than benign examples due to added adversarial noise. We use the synchronisation confidence score as a proxy for audiovisual correlation and based on it we can detect adversarial attacks. To the best of our knowledge, this is the first work on detection of adversarial attacks on audiovisual speech recognition models. We apply recent adversarial attacks on two audiovisual speech recognition models trained on the GRID and LRW datasets. The experimental results demonstrate that the proposed approach is an effective way for detecting such attacks.



### One-Stage Inpainting with Bilateral Attention and Pyramid Filling Block
- **Arxiv ID**: http://arxiv.org/abs/1912.08642v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08642v1)
- **Published**: 2019-12-18 14:46:06+00:00
- **Updated**: 2019-12-18 14:46:06+00:00
- **Authors**: Hongyu Liu, Bin Jiang, Wei Huang, Chao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based image inpainting methods which utilize contextual information and two-stage architecture have exhibited remarkable performance. However, the two-stage architecture is time-consuming, the contextual information lack high-level semantics and ignores both the semantic relevance and distance information of hole's feature patches, these limitations result in blurry textures and distorted structures of final result. Motivated by these observations, we propose a new deep generative model-based approach, which trains a shared network twice with different targets and utilizes a single network during the testing phase, so that we can effectively save inference time. Specifically, the targets of two training steps are structure reconstruction and texture generation respectively. During the second training, we first propose a Pyramid Filling Block (PF-block) to utilize the high-level features that the hole regions has been filled to guide the filling process of low-level features progressively, the missing content can be filled from deep to shallow in a pyramid fashion. Then, inspired by the classical bilateral filter [30], we propose the Bilateral Attention layer (BA-layer) to optimize filled feature map, which synthesizes feature patches at each position by computing weighted sums of the surrounding feature patches, these weights are derived by considering both distance and value relationships between feature patches, thus making the visually plausible inpainting results. Finally, experiments on multiple publicly available datasets show the superior performance of our approach.



### A Web Page Classifier Library Based on Random Image Content Analysis Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.08644v1
- **DOI**: 10.1145/3197768.3201525
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1912.08644v1)
- **Published**: 2019-12-18 14:50:18+00:00
- **Updated**: 2019-12-18 14:50:18+00:00
- **Authors**: Leonardo Espinosa Leal, Kaj-Mikael Björk, Amaury Lendasse, Anton Akusok
- **Comment**: 4 pages, 3 figures. Proceedings of the 11th PErvasive Technologies
  Related to Assistive Environments Conference. ACM, 2018
- **Journal**: None
- **Summary**: In this paper, we present a methodology and the corresponding Python library 1 for the classification of webpages. Our method retrieves a fixed number of images from a given webpage, and based on them classifies the webpage into a set of established classes with a given probability. The library trains a random forest model build upon the features extracted from images by a pre-trained deep network. The implementation is tested by recognizing weapon class webpages in a curated list of 3859 websites. The results show that the best method of classifying a webpage into the studies classes is to assign the class according to the maximum probability of any image belonging to this (weapon) class being above the threshold, across all the retrieved images. Further research explores the possibilities for the developed methodology to also apply in image classification for healthcare applications.



### Research Frontiers in Transfer Learning -- a systematic and bibliometric review
- **Arxiv ID**: http://arxiv.org/abs/1912.08812v1
- **DOI**: None
- **Categories**: **cs.DL**, cs.CV, cs.LG, 68T05, I.5
- **Links**: [PDF](http://arxiv.org/pdf/1912.08812v1)
- **Published**: 2019-12-18 15:08:19+00:00
- **Updated**: 2019-12-18 15:08:19+00:00
- **Authors**: Frederico Guth, Teofilo Emidio de-Campos
- **Comment**: 19 pages, 9 figures
- **Journal**: None
- **Summary**: Humans can learn from very few samples, demonstrating an outstanding generalization ability that learning algorithms are still far from reaching. Currently, the most successful models demand enormous amounts of well-labeled data, which are expensive and difficult to obtain, becoming one of the biggest obstacles to the use of machine learning in practice. This scenario shows the massive potential for Transfer Learning, which aims to harness previously acquired knowledge to the learning of new tasks more effectively and efficiently. In this systematic review, we apply a quantitative method to select the main contributions to the field and make use of bibliographic coupling metrics to identify research frontiers. We further analyze the linguistic variation between the classics of the field and the frontier and map promising research directions.



### Coupled Network for Robust Pedestrian Detection with Gated Multi-Layer Feature Extraction and Deformable Occlusion Handling
- **Arxiv ID**: http://arxiv.org/abs/1912.08661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08661v1)
- **Published**: 2019-12-18 15:28:03+00:00
- **Updated**: 2019-12-18 15:28:03+00:00
- **Authors**: Tianrui Liu, Wenhan Luo, Lin Ma, Jun-Jie Huang, Tania Stathaki, Tianhong Dai
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2020
- **Summary**: Pedestrian detection methods have been significantly improved with the development of deep convolutional neural networks. Nevertheless, detecting small-scaled pedestrians and occluded pedestrians remains a challenging problem. In this paper, we propose a pedestrian detection method with a couple-network to simultaneously address these two issues. One of the sub-networks, the gated multi-layer feature extraction sub-network, aims to adaptively generate discriminative features for pedestrian candidates in order to robustly detect pedestrians with large variations on scales. The second sub-network targets in handling the occlusion problem of pedestrian detection by using deformable regional RoI-pooling. We investigate two different gate units for the gated sub-network, namely, the channel-wise gate unit and the spatio-wise gate unit, which can enhance the representation ability of the regional convolutional features among the channel dimensions or across the spatial domain, repetitively. Ablation studies have validated the effectiveness of both the proposed gated multi-layer feature extraction sub-network and the deformable occlusion handling sub-network. With the coupled framework, our proposed pedestrian detector achieves state-of-the-art results on the Caltech and the CityPersons pedestrian detection benchmarks.



### Integration of Convolutional Neural Networks for Pulmonary Nodule Malignancy Assessment in a Lung Cancer Classification Pipeline
- **Arxiv ID**: http://arxiv.org/abs/1912.08679v1
- **DOI**: 10.1016/j.cmpb.2019.105172
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.08679v1)
- **Published**: 2019-12-18 15:56:29+00:00
- **Updated**: 2019-12-18 15:56:29+00:00
- **Authors**: Ilaria Bonavita, Xavier Rafael-Palou, Mario Ceresa, Gemma Piella, Vicent Ribas, Miguel A. González Ballester
- **Comment**: 26 pages, 5 figures
- **Journal**: Computer Methods and Programs in Biomedicine,
  volume=185,number=105172, pages=1-9, year=2019, publisher=Elsevier
- **Summary**: The early identification of malignant pulmonary nodules is critical for better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules. In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset. Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score. Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer.



### ResNetX: a more disordered and deeper network architecture
- **Arxiv ID**: http://arxiv.org/abs/1912.12165v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12165v1)
- **Published**: 2019-12-18 16:05:45+00:00
- **Updated**: 2019-12-18 16:05:45+00:00
- **Authors**: Wenfeng Feng, Xin Zhang, Guangpeng Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Designing efficient network structures has always been the core content of neural network research.   ResNet and its variants have proved to be efficient in architecture.   However, how to theoretically character the influence of network structure on performance is still vague.   With the help of techniques in complex networks, We here provide a natural yet efficient extension to ResNet by folding its backbone chain.   Our architecture has two structural features when being mapped to directed acyclic graphs:   First is a higher degree of the disorder compared with ResNet, which let ResNetX explore a larger number of feature maps with different sizes of receptive fields.   Second is a larger proportion of shorter paths compared to ResNet, which improves the direct flow of information through the entire network.   Our architecture exposes a new dimension, namely "fold depth", in addition to existing dimensions of depth, width, and cardinality.   Our architecture is a natural extension to ResNet, and can be integrated with existing state-of-the-art methods with little effort. Image classification results on CIFAR-10 and CIFAR-100 benchmarks suggested that our new network architecture performs better than ResNet.



### Unsupervised Adversarial Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1912.12164v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12164v1)
- **Published**: 2019-12-18 16:06:34+00:00
- **Updated**: 2019-12-18 16:06:34+00:00
- **Authors**: Arthur Pajot, Emmanuel de Bezenac, Patrick Gallinari
- **Comment**: None
- **Journal**: None
- **Summary**: We consider inpainting in an unsupervised setting where there is neither access to paired nor unpaired training data. The only available information is provided by the uncomplete observations and the inpainting process statistics. In this context, an observation should give rise to several plausible reconstructions which amounts at learning a distribution over the space of reconstructed images. We model the reconstruction process by using a conditional GAN with constraints on the stochastic component that introduce an explicit dependency between this component and the generated output. This allows us sampling from the latent component in order to generate a distribution of images associated to an observation. We demonstrate the capacity of our model on several image datasets: faces (CelebA), food images (Recipe-1M) and bedrooms (LSUN Bedrooms) with different types of imputation masks. The approach yields comparable performance to model variants trained with additional supervision.



### $Σ$-net: Systematic Evaluation of Iterative Deep Neural Networks for Fast Parallel MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.09278v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.09278v1)
- **Published**: 2019-12-18 16:52:39+00:00
- **Updated**: 2019-12-18 16:52:39+00:00
- **Authors**: Kerstin Hammernik, Jo Schlemper, Chen Qin, Jinming Duan, Ronald M. Summers, Daniel Rueckert
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: To systematically investigate the influence of various data consistency layers, (semi-)supervised learning and ensembling strategies, defined in a $\Sigma$-net, for accelerated parallel MR image reconstruction using deep learning.   Theory and Methods: MR image reconstruction is formulated as learned unrolled optimization scheme with a Down-Up network as regularization and varying data consistency layers. The different architectures are split into sensitivity networks, which rely on explicit coil sensitivity maps, and parallel coil networks, which learn the combination of coils implicitly. Different content and adversarial losses, a semi-supervised fine-tuning scheme and model ensembling are investigated.   Results: Evaluated on the fastMRI multicoil validation set, architectures involving raw k-space data outperform image enhancement methods significantly. Semi-supervised fine-tuning adapts to new k-space data and provides, together with reconstructions based on adversarial training, the visually most appealing results although quantitative quality metrics are reduced. The $\Sigma$-net ensembles the benefits from different models and achieves similar scores compared to the single state-of-the-art approaches.   Conclusion: This work provides an open-source framework to perform a systematic wide-range comparison of state-of-the-art reconstruction approaches for parallel MR image reconstruction on the fastMRI knee dataset and explores the importance of data consistency. A suitable trade-off between perceptual image quality and quantitative scores are achieved with the ensembled $\Sigma$-net.



### Towards Robust Learning with Different Label Noise Distributions
- **Arxiv ID**: http://arxiv.org/abs/1912.08741v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08741v3)
- **Published**: 2019-12-18 17:12:29+00:00
- **Updated**: 2020-07-27 11:32:45+00:00
- **Authors**: Diego Ortego, Eric Arazo, Paul Albert, Noel E. O'Connor, Kevin McGuinness
- **Comment**: None
- **Journal**: None
- **Summary**: Noisy labels are an unavoidable consequence of labeling processes and detecting them is an important step towards preventing performance degradations in Convolutional Neural Networks. Discarding noisy labels avoids a harmful memorization, while the associated image content can still be exploited in a semi-supervised learning (SSL) setup. Clean samples are usually identified using the small loss trick, i.e. they exhibit a low loss. However, we show that different noise distributions make the application of this trick less straightforward and propose to continuously relabel all images to reveal a discriminative loss against multiple distributions. SSL is then applied twice, once to improve the clean-noisy detection and again for training the final model. We design an experimental setup based on ImageNet32/64 for better understanding the consequences of representation learning with differing label noise distributions and find that non-uniform out-of-distribution noise better resembles real-world noise and that in most cases intermediate features are not affected by label noise corruption. Experiments in CIFAR-10/100, ImageNet32/64 and WebVision (real-world noise) demonstrate that the proposed label noise Distribution Robust Pseudo-Labeling (DRPL) approach gives substantial improvements over recent state-of-the-art. Code is available at https://git.io/JJ0PV.



### Ambient Lighting Generation for Flash Images with Guided Conditional Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08813v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08813v5)
- **Published**: 2019-12-18 17:58:29+00:00
- **Updated**: 2020-02-20 22:12:30+00:00
- **Authors**: José Chávez, Rensso Mora, Edward Cayllahua-Cahuina
- **Comment**: VISAPP 2020
- **Journal**: None
- **Summary**: To cope with the challenges that low light conditions produce in images, photographers tend to use the light provided by the camera flash to get better illumination. Nevertheless, harsh shadows and non-uniform illumination can arise from using a camera flash, especially in low light conditions. Previous studies have focused on normalizing the lighting on flash images; however, to the best of our knowledge, no prior studies have examined the sideways shadows removal, reconstruction of overexposed areas, and the generation of synthetic ambient shadows or natural tone of scene objects. To provide more natural illumination on flash images and ensure high-frequency details, we propose a generative adversarial network in a guided conditional mode. We show that this approach not only generates natural illumination but also attenuates harsh shadows, simultaneously generating synthetic ambient shadows. Our approach achieves promising results on a custom FAID dataset, outperforming our baseline studies. We also analyze the components of our proposal and how they affect the overall performance and discuss the opportunities for future work.



### RealMix: Towards Realistic Semi-Supervised Deep Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1912.08766v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08766v1)
- **Published**: 2019-12-18 18:03:28+00:00
- **Updated**: 2019-12-18 18:03:28+00:00
- **Authors**: Varun Nair, Javier Fuentes Alonso, Tony Beltramelli
- **Comment**: Code available at https://github.com/uizard-technologies/realmix
- **Journal**: None
- **Summary**: Semi-Supervised Learning (SSL) algorithms have shown great potential in training regimes when access to labeled data is scarce but access to unlabeled data is plentiful. However, our experiments illustrate several shortcomings that prior SSL algorithms suffer from. In particular, poor performance when unlabeled and labeled data distributions differ. To address these observations, we develop RealMix, which achieves state-of-the-art results on standard benchmark datasets across different labeled and unlabeled set sizes while overcoming the aforementioned challenges. Notably, RealMix achieves an error rate of 9.79% on CIFAR10 with 250 labels and is the only SSL method tested able to surpass baseline performance when there is significant mismatch in the labeled and unlabeled data distributions. RealMix demonstrates how SSL can be used in real world situations with limited access to both data and compute and guides further research in SSL with practical applicability in mind.



### MRI Pulse Sequence Integration for Deep-Learning Based Brain Metastasis Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.08775v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08775v1)
- **Published**: 2019-12-18 18:12:20+00:00
- **Updated**: 2019-12-18 18:12:20+00:00
- **Authors**: Darvin Yi, Endre Grøvik, Michael Iv, Elizabeth Tong, Kyrre Eeg Emblem, Line Brennhaug Nilsen, Cathrine Saxhaug, Anna Latysheva, Kari Dolven Jacobsen, Åslaug Helland, Greg Zaharchuk, Daniel Rubin
- **Comment**: In the IEEE transactions format for submission to IEEE-TMI
- **Journal**: None
- **Summary**: Magnetic resonance (MR) imaging is an essential diagnostic tool in clinical medicine. Recently, a variety of deep learning methods have been applied to segmentation tasks in medical images, with promising results for computer-aided diagnosis. For MR images, effectively integrating different pulse sequences is important to optimize performance. However, the best way to integrate different pulse sequences remains unclear. In this study, we evaluate multiple architectural features and characterize their effects in the task of metastasis segmentation. Specifically, we consider (1) different pulse sequence integration schemas, (2) different modes of weight sharing for parallel network branches, and (3) a new approach for enabling robustness to missing pulse sequences. We find that levels of integration and modes of weight sharing that favor low variance work best in our regime of small data (n = 100). By adding an input-level dropout layer, we could preserve the overall performance of these networks while allowing for inference on inputs with missing pulse sequence. We illustrate not only the generalizability of the network but also the utility of this robustness when applying the trained model to data from a different center, which does not use the same pulse sequences. Finally, we apply network visualization methods to better understand which input features are most important for network performance. Together, these results provide a framework for building networks with enhanced robustness to missing data while maintaining comparable performance in medical imaging applications.



### Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion
- **Arxiv ID**: http://arxiv.org/abs/1912.08795v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08795v2)
- **Published**: 2019-12-18 18:50:10+00:00
- **Updated**: 2020-06-16 03:30:21+00:00
- **Authors**: Hongxu Yin, Pavlo Molchanov, Zhizhong Li, Jose M. Alvarez, Arun Mallya, Derek Hoiem, Niraj K. Jha, Jan Kautz
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning. Code is available at https://github.com/NVlabs/DeepInversion



### SynSin: End-to-end View Synthesis from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1912.08804v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08804v2)
- **Published**: 2019-12-18 18:59:04+00:00
- **Updated**: 2020-04-18 16:14:02+00:00
- **Authors**: Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson
- **Comment**: Project page: www.robots.ox.ac.uk/~ow/synsin.html
- **Journal**: None
- **Summary**: Single image view synthesis allows for the generation of new views of a scene given a single input image. This is challenging, as it requires comprehensively understanding the 3D scene from a single image. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Unlike prior work, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.



### ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1912.08830v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08830v3)
- **Published**: 2019-12-18 19:00:49+00:00
- **Updated**: 2020-11-11 09:33:31+00:00
- **Authors**: Dave Zhenyu Chen, Angel X. Chang, Matthias Nießner
- **Comment**: Project page: https://daveredrum.github.io/ScanRefer/
- **Journal**: None
- **Summary**: We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.



### Learning Shared Cross-modality Representation Using Multispectral-LiDAR and Hyperspectral Data
- **Arxiv ID**: http://arxiv.org/abs/1912.08837v2
- **DOI**: 10.1109/LGRS.2019.2944599
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08837v2)
- **Published**: 2019-12-18 19:06:50+00:00
- **Updated**: 2020-06-08 09:19:25+00:00
- **Authors**: Danfeng Hong, Jocelyn Chanussot, Naoto Yokoya, Jian Kang, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: IEEE Geoscience and Remote Sensing Letters, 2020
- **Summary**: Due to the ever-growing diversity of the data source, multi-modality feature learning has attracted more and more attention. However, most of these methods are designed by jointly learning feature representation from multi-modalities that exist in both training and test sets, yet they are less investigated in absence of certain modality in the test phase. To this end, in this letter, we propose to learn a shared feature space across multi-modalities in the training process. By this way, the out-of-sample from any of multi-modalities can be directly projected onto the learned space for a more effective cross-modality representation. More significantly, the shared space is regarded as a latent subspace in our proposed method, which connects the original multi-modal samples with label information to further improve the feature discrimination. Experiments are conducted on the multispectral-Lidar and hyperspectral dataset provided by the 2018 IEEE GRSS Data Fusion Contest to demonstrate the effectiveness and superiority of the proposed method in comparison with several popular baselines.



### Design Considerations for Efficient Deep Neural Networks on Processing-in-Memory Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1912.12167v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET
- **Links**: [PDF](http://arxiv.org/pdf/1912.12167v1)
- **Published**: 2019-12-18 19:23:29+00:00
- **Updated**: 2019-12-18 19:23:29+00:00
- **Authors**: Tien-Ju Yang, Vivienne Sze
- **Comment**: Accepted by IEDM 2019
- **Journal**: None
- **Summary**: This paper describes various design considerations for deep neural networks that enable them to operate efficiently and accurately on processing-in-memory accelerators. We highlight important properties of these accelerators and the resulting design considerations using experiments conducted on various state-of-the-art deep neural networks with the large-scale ImageNet dataset.



### Invariant Attribute Profiles: A Spatial-Frequency Joint Feature Extractor for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.08847v1
- **DOI**: 10.1109/TGRS.2019.2957251
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08847v1)
- **Published**: 2019-12-18 19:27:07+00:00
- **Updated**: 2019-12-18 19:27:07+00:00
- **Authors**: Danfeng Hong, Xin Wu, Pedram Ghamisi, Jocelyn Chanussot, Naoto Yokoya, Xiao Xiang Zhu
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2020
- **Summary**: Up to the present, an enormous number of advanced techniques have been developed to enhance and extract the spatially semantic information in hyperspectral image processing and analysis. However, locally semantic change, such as scene composition, relative position between objects, spectral variability caused by illumination, atmospheric effects, and material mixture, has been less frequently investigated in modeling spatial information. As a consequence, identifying the same materials from spatially different scenes or positions can be difficult. In this paper, we propose a solution to address this issue by locally extracting invariant features from hyperspectral imagery (HSI) in both spatial and frequency domains, using a method called invariant attribute profiles (IAPs). IAPs extract the spatial invariant features by exploiting isotropic filter banks or convolutional kernels on HSI and spatial aggregation techniques (e.g., superpixel segmentation) in the Cartesian coordinate system. Furthermore, they model invariant behaviors (e.g., shift, rotation) by the means of a continuous histogram of oriented gradients constructed in a Fourier polar coordinate. This yields a combinatorial representation of spatial-frequency invariant features with application to HSI classification. Extensive experiments conducted on three promising hyperspectral datasets (Houston2013 and Houston2018) demonstrate the superiority and effectiveness of the proposed IAP method in comparison with several state-of-the-art profile-related techniques. The codes will be available from the website: https://sites.google.com/view/danfeng-hong/data-code.



### Surface HOF: Surface Reconstruction from a Single Image Using Higher Order Function Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08852v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08852v1)
- **Published**: 2019-12-18 19:30:24+00:00
- **Updated**: 2019-12-18 19:30:24+00:00
- **Authors**: Ziyun Wang, Volkan Isler, Daniel D. Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of generating a high-resolution surface reconstruction from a single image. Our approach is to learn a Higher Order Function (HOF) which takes an image of an object as input and generates a mapping function. The mapping function takes samples from a canonical domain (e.g. the unit sphere) and maps each sample to a local tangent plane on the 3D reconstruction of the object. Each tangent plane is represented as an origin point and a normal vector at that point. By efficiently learning a continuous mapping function, the surface can be generated at arbitrary resolution in contrast to other methods which generate fixed resolution outputs. We present the Surface HOF in which both the higher order function and the mapping function are represented as neural networks, and train the networks to generate reconstructions of PointNet objects. Experiments show that Surface HOF is more accurate and uses more efficient representations than other state of the art methods for surface reconstruction. Surface HOF is also easier to train: it requires minimal input pre-processing and output post-processing and generates surface representations that are more parameter efficient. Its accuracy and convenience make Surface HOF an appealing method for single image reconstruction.



### Simulating Content Consistent Vehicle Datasets with Attribute Descent
- **Arxiv ID**: http://arxiv.org/abs/1912.08855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08855v2)
- **Published**: 2019-12-18 19:40:31+00:00
- **Updated**: 2020-07-21 19:00:03+00:00
- **Authors**: Yue Yao, Liang Zheng, Xiaodong Yang, Milind Naphade, Tom Gedeon
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: This paper uses a graphic engine to simulate a large amount of training data with free annotations. Between synthetic and real data, there is a two-level domain gap, i.e., content level and appearance level. While the latter has been widely studied, we focus on reducing the content gap in attributes like illumination and viewpoint. To reduce the problem complexity, we choose a smaller and more controllable application, vehicle re-identification (re-ID). We introduce a large-scale synthetic dataset VehicleX. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes. We propose an attribute descent approach to let VehicleX approximate the attributes in real-world datasets. Specifically, we manipulate each attribute in VehicleX, aiming to minimize the discrepancy between VehicleX and real data in terms of the Fr\'echet Inception Distance (FID). This attribute descent algorithm allows content domain adaptation (DA) orthogonal to existing appearance DA methods. We mix the optimized VehicleX data with real-world vehicle re-ID datasets, and observe consistent improvement. With the augmented datasets, we report competitive accuracy. We make the dataset, engine and our codes available at https://github.com/yorkeyao/VehicleX.



### Lower Dimensional Kernels for Video Discriminators
- **Arxiv ID**: http://arxiv.org/abs/1912.08860v1
- **DOI**: 10.1016/j.neunet.2020.09.016
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08860v1)
- **Published**: 2019-12-18 19:54:02+00:00
- **Updated**: 2019-12-18 19:54:02+00:00
- **Authors**: Emmanuel Kahembwe, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: Neural.Networks 132 (2020) 506-520
- **Summary**: This work presents an analysis of the discriminators used in Generative Adversarial Networks (GANs) for Video. We show that unconstrained video discriminator architectures induce a loss surface with high curvature which make optimisation difficult. We also show that this curvature becomes more extreme as the maximal kernel dimension of video discriminators increases. With these observations in hand, we propose a family of efficient Lower-Dimensional Video Discriminators for GANs (LDVD GANs). The proposed family of discriminators improve the performance of video GAN models they are applied to and demonstrate good performance on complex and diverse datasets such as UCF-101. In particular, we show that they can double the performance of Temporal-GANs and provide for state-of-the-art performance on a single GPU.



### Continuous Meta-Learning without Tasks
- **Arxiv ID**: http://arxiv.org/abs/1912.08866v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08866v2)
- **Published**: 2019-12-18 20:10:40+00:00
- **Updated**: 2020-10-21 00:14:30+00:00
- **Authors**: James Harrison, Apoorva Sharma, Chelsea Finn, Marco Pavone
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.



### Attention-Based Face AntiSpoofing of RGB Images, using a Minimal End-2-End Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1912.08870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08870v1)
- **Published**: 2019-12-18 20:15:27+00:00
- **Updated**: 2019-12-18 20:15:27+00:00
- **Authors**: Ali Ghofrani, Rahil Mahdian Toroghi, Seyed Mojtaba Tabatabaie
- **Comment**: 6 pages , 14 figures
- **Journal**: None
- **Summary**: Face anti-spoofing aims at identifying the real face, as well as the fake one, and gains a high attention in security-sensitive applications, liveness detection, fingerprinting, and so on. In this paper, we address the anti-spoofing problem by proposing two end-to-end systems of convolutional neural networks. One model is developed based on the EfficientNet B0 network which has been modified in the final dense layers. The second one, is a very light model of the MobileNet V2, which has been contracted, modified and retrained efficiently on the data being created based on the Rose-Youtu dataset, for this purpose. The experiments show that, both of the proposed architectures achieve remarkable results on detecting the real and fake images of the face input data. The experiments clearly show that the heavy-weight model could be efficiently employed in server-side implementations, whereas the low-weight model could be easily implemented on the hand-held devices and both perform perfectly well using merely RGB input images.



### Adaptive Loss-aware Quantization for Multi-bit Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.08883v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08883v4)
- **Published**: 2019-12-18 20:48:29+00:00
- **Updated**: 2020-07-04 20:24:41+00:00
- **Authors**: Zhongnan Qu, Zimu Zhou, Yun Cheng, Lothar Thiele
- **Comment**: To appear in CVPR 2020; Code available at
  https://github.com/zqu1992/ALQ
- **Journal**: None
- **Summary**: We investigate the compression of deep neural networks by quantizing their weights and activations into multiple binary bases, known as multi-bit networks (MBNs), which accelerate the inference and reduce the storage for the deployment on low-resource mobile and embedded platforms. We propose Adaptive Loss-aware Quantization (ALQ), a new MBN quantization pipeline that is able to achieve an average bitwidth below one-bit without notable loss in inference accuracy. Unlike previous MBN quantization solutions that train a quantizer by minimizing the error to reconstruct full precision weights, ALQ directly minimizes the quantization-induced error on the loss function involving neither gradient approximation nor full precision maintenance. ALQ also exploits strategies including adaptive bitwidth, smooth bitwidth reduction, and iterative trained quantization to allow a smaller network size without loss in accuracy. Experiment results on popular image datasets show that ALQ outperforms state-of-the-art compressed networks in terms of both storage and accuracy. Code is available at https://github.com/zqu1992/ALQ



### The Spectral Bias of the Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1912.08905v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08905v1)
- **Published**: 2019-12-18 21:51:58+00:00
- **Updated**: 2019-12-18 21:51:58+00:00
- **Authors**: Prithvijit Chakrabarty, Subhransu Maji
- **Comment**: Bayesian Deep Learning Workshop, NeurIPS 2019
- **Journal**: None
- **Summary**: The "deep image prior" proposed by Ulyanov et al. is an intriguing property of neural nets: a convolutional encoder-decoder network can be used as a prior for natural images. The network architecture implicitly introduces a bias; If we train the model to map white noise to a corrupted image, this bias guides the model to fit the true image before fitting the corrupted regions.   This paper explores why the deep image prior helps in denoising natural images. We present a novel method to analyze trajectories generated by the deep image prior optimization and demonstrate:   (i) convolution layers of the an encoder-decoder decouple the frequency components of the image, learning each at different rates   (ii) the model fits lower frequencies first, making early stopping behave as a low pass filter.   The experiments study an extension of Cheng et al which showed that at initialization, the deep image prior is equivalent to a stationary Gaussian process.



### On the Metrics and Adaptation Methods for Domain Divergences of sEMG-based Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.08914v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08914v1)
- **Published**: 2019-12-18 22:12:53+00:00
- **Updated**: 2019-12-18 22:12:53+00:00
- **Authors**: István Ketykó, Ferenc Kovács
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new metric to measure domain divergence and a new domain adaptation method for time-series classification. The metric belongs to the class of probability distributions-based metrics, is transductive, and does not assume the presence of source data samples. The 2-stage method utilizes an improved autoregressive, RNN-based architecture with deep/non-linear transformation. We assess our metric and the performance of our model in the context of sEMG/EMG-based gesture recognition under inter-session and inter-subject domain shifts.



### One-Shot Weakly Supervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.08936v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08936v1)
- **Published**: 2019-12-18 22:54:29+00:00
- **Updated**: 2019-12-18 22:54:29+00:00
- **Authors**: Mennatullah Siam, Naren Doraiswamy, Boris N. Oreshkin, Hengshuai Yao, Martin Jagersand
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional few-shot object segmentation methods learn object segmentation from a few labelled support images with strongly labelled segmentation masks. Recent work has shown to perform on par with weaker levels of supervision in terms of scribbles and bounding boxes. However, there has been limited attention given to the problem of few-shot object segmentation with image-level supervision. We propose a novel multi-modal interaction module for few-shot object segmentation that utilizes a co-attention mechanism using both visual and word embeddings. It enables our model to achieve 5.1% improvement over previously proposed image-level few-shot object segmentation. Our method compares relatively close to the state of the art methods that use strong supervision, while ours use the least possible supervision. We further propose a novel setup for few-shot weakly supervised video object segmentation(VOS) that relies on image-level labels for the first frame. The proposed setup uses weak annotation unlike semi-supervised VOS setting that utilizes strongly labelled segmentation masks. The setup evaluates the effectiveness of generalizing to novel classes in the VOS setting. The setup splits the VOS data into multiple folds with different categories per fold. It provides a potential setup to evaluate how few-shot object segmentation methods can benefit from additional object poses, or object interactions that is not available in static frames as in PASCAL-5i benchmark.



### Pathomic Fusion: An Integrated Framework for Fusing Histopathology and Genomic Features for Cancer Diagnosis and Prognosis
- **Arxiv ID**: http://arxiv.org/abs/1912.08937v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.GN, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1912.08937v3)
- **Published**: 2019-12-18 23:01:26+00:00
- **Updated**: 2020-09-03 16:04:39+00:00
- **Authors**: Richard J. Chen, Ming Y. Lu, Jingwen Wang, Drew F. K. Williamson, Scott J. Rodig, Neal I. Lindeman, Faisal Mahmood
- **Comment**: Code and trained models are made available at:
  https://github.com/mahmoodlab/PathomicFusion
- **Journal**: None
- **Summary**: Cancer diagnosis, prognosis, and therapeutic response predictions are based on morphological information from histology slides and molecular profiles from genomic data. However, most deep learning-based objective outcome prediction and grading paradigms are based on histology or genomics alone and do not make use of the complementary information in an intuitive manner. In this work, we propose Pathomic Fusion, an interpretable strategy for end-to-end multimodal fusion of histology image and genomic (mutations, CNV, RNA-Seq) features for survival outcome prediction. Our approach models pairwise feature interactions across modalities by taking the Kronecker product of unimodal feature representations and controls the expressiveness of each representation via a gating-based attention mechanism. Following supervised learning, we are able to interpret and saliently localize features across each modality, and understand how feature importance shifts when conditioning on multimodal input. We validate our approach using glioma and clear cell renal cell carcinoma datasets from the Cancer Genome Atlas (TCGA), which contains paired whole-slide image, genotype, and transcriptome data with ground truth survival and histologic grade labels. In a 15-fold cross-validation, our results demonstrate that the proposed multimodal fusion paradigm improves prognostic determinations from ground truth grading and molecular subtyping, as well as unimodal deep networks trained on histology and genomic data alone. The proposed method establishes insight and theory on how to train deep networks on multimodal biomedical data in an intuitive manner, which will be useful for other problems in medicine that seek to combine heterogeneous data streams for understanding diseases and predicting response and resistance to treatment.



### An Adversarial Perturbation Oriented Domain Adaptation Approach for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.08954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08954v1)
- **Published**: 2019-12-18 23:59:24+00:00
- **Updated**: 2019-12-18 23:59:24+00:00
- **Authors**: Jihan Yang, Ruijia Xu, Ruiyu Li, Xiaojuan Qi, Xiaoyong Shen, Guanbin Li, Liang Lin
- **Comment**: To Appear in AAAI2020
- **Journal**: None
- **Summary**: We focus on Unsupervised Domain Adaptation (UDA) for the task of semantic segmentation. Recently, adversarial alignment has been widely adopted to match the marginal distribution of feature representations across two domains globally. However, this strategy fails in adapting the representations of the tail classes or small objects for semantic segmentation since the alignment objective is dominated by head categories or large objects. In contrast to adversarial alignment, we propose to explicitly train a domain-invariant classifier by generating and defensing against pointwise feature space adversarial perturbations. Specifically, we firstly perturb the intermediate feature maps with several attack objectives (i.e., discriminator and classifier) on each individual position for both domains, and then the classifier is trained to be invariant to the perturbations. By perturbing each position individually, our model treats each location evenly regardless of the category or object size and thus circumvents the aforementioned issue. Moreover, the domain gap in feature space is reduced by extrapolating source and target perturbed features towards each other with attack on the domain discriminator. Our approach achieves the state-of-the-art performance on two challenging domain adaptation tasks for semantic segmentation: GTA5 -> Cityscapes and SYNTHIA -> Cityscapes.



