# Arxiv Papers in cs.CV on 2019-12-12
### Semantic segmentation of trajectories with improved agent models for pedestrian behavior analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.05727v1
- **DOI**: 10.1080/01691864.2018.1554508
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05727v1)
- **Published**: 2019-12-12 02:04:43+00:00
- **Updated**: 2019-12-12 02:04:43+00:00
- **Authors**: Toru Tamaki, Daisuke Ogawa, Bisser Raytchev, Kazufumi Kaneda
- **Comment**: None
- **Journal**: Advanced Robotics, Volume 33, 2019 - Issue 3-4: Special Issue on
  Systems Science of Bio-navigation, Pages 153-168
- **Summary**: In this paper, we propose a method for semantic segmentation of pedestrian trajectories based on pedestrian behavior models, or agents. The agents model the dynamics of pedestrian movements in two-dimensional space using a linear dynamics model and common start and goal locations of trajectories. First, agent models are estimated from the trajectories obtained from image sequences. Our method is built on top of the Mixture model of Dynamic pedestrian Agents (MDA); however, the MDA's trajectory modeling and estimation are improved. Then, the trajectories are divided into semantically meaningful segments. The subsegments of a trajectory are modeled by applying a hidden Markov model using the estimated agent models. Experimental results with a real trajectory dataset show the effectiveness of the proposed method as compared to the well-known classical Ramer-Douglas-Peucker algorithm and also to the original MDA model.



### Improved Activity Forecasting for Generating Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1912.05729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05729v1)
- **Published**: 2019-12-12 02:05:24+00:00
- **Updated**: 2019-12-12 02:05:24+00:00
- **Authors**: Daisuke Ogawa, Toru Tamaki, Tsubasa Hirakawa, Bisser Raytchev, Kazufumi Kaneda, Ken Yoda
- **Comment**: The 2019 International Workshop on Frontiers of Computer Vision
  (IW-FCV2019)
- **Journal**: None
- **Summary**: An efficient inverse reinforcement learning for generating trajectories is proposed based of 2D and 3D activity forecasting. We modify reward function with $L_p$ norm and propose convolution into value iteration steps, which is called convolutional value iteration. Experimental results with seabird trajectories (43 for training and 10 for test), our method is best in terms of MHD error and performs fastest. Generated trajectories for interpolating missing parts of trajectories look much similar to real seabird trajectories than those by the previous works.



### Meaning guided video captioning
- **Arxiv ID**: http://arxiv.org/abs/1912.05730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05730v1)
- **Published**: 2019-12-12 02:05:45+00:00
- **Updated**: 2019-12-12 02:05:45+00:00
- **Authors**: Rushi J. Babariya, Toru Tamaki
- **Comment**: The 5th Asian Conference on Pattern Recognition (ACPR 2019)
- **Journal**: None
- **Summary**: Current video captioning approaches often suffer from problems of missing objects in the video to be described, while generating captions semantically similar with ground truth sentences. In this paper, we propose a new approach to video captioning that can describe objects detected by object detection, and generate captions having similar meaning with correct captions. Our model relies on S2VT, a sequence-to-sequence model for video captioning. Given a sequence of video frames, the encoding RNN takes a frame as well as detected objects in the frame in order to incorporate the information of the objects in the scene. The following decoding RNN outputs are then fed into an attention layer and then to a decoder for generating captions. The caption is compared with the ground truth by learning metric so that vector representations of generated captions are semantically similar to those of ground truth. Experimental results with the MSDV dataset demonstrate that the performance of the proposed approach is much better than the model without the proposed meaning-guided framework, showing the effectiveness of the proposed model. Code are publicly available at https://github.com/captanlevi/Meaning-guided-video-captioning-.



### Estimating 3D Camera Pose from 2D Pedestrian Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1912.05758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05758v2)
- **Published**: 2019-12-12 03:41:03+00:00
- **Updated**: 2020-01-02 03:24:30+00:00
- **Authors**: Yan Xu, Vivek Roy, Kris Kitani
- **Comment**: Accepted in WACV 2020
- **Journal**: None
- **Summary**: We consider the task of re-calibrating the 3D pose of a static surveillance camera, whose pose may change due to external forces, such as birds, wind, falling objects or earthquakes. Conventionally, camera pose estimation can be solved with a PnP (Perspective-n-Point) method using 2D-to-3D feature correspondences, when 3D points are known. However, 3D point annotations are not always available or practical to obtain in real-world applications. We propose an alternative strategy for extracting 3D information to solve for camera pose by using pedestrian trajectories. We observe that 2D pedestrian trajectories indirectly contain useful 3D information that can be used for inferring camera pose. To leverage this information, we propose a data-driven approach by training a neural network (NN) regressor to model a direct mapping from 2D pedestrian trajectories projected on the image plane to 3D camera pose. We demonstrate that our regressor trained only on synthetic data can be directly applied to real data, thus eliminating the need to label any real data. We evaluate our method across six different scenes from the Town Centre Street and DUKEMTMC datasets. Our method achieves an improvement of $\sim50\%$ on both position and orientation prediction accuracy when compared to other SOTA methods.



### GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining
- **Arxiv ID**: http://arxiv.org/abs/1912.05759v3
- **DOI**: 10.1109/TGRS.2020.3046454
- **Categories**: **cs.CV**, cs.LG, eess.IV, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.05759v3)
- **Published**: 2019-12-12 03:43:09+00:00
- **Updated**: 2021-09-26 08:15:44+00:00
- **Authors**: Bin Liu, Yuxiao Ren, Hanchi Liu, Hui Xu, Zhengfang Wang, Anthony G. Cohn, Peng Jiang
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 59, no.
  10, pp. 8305-8325, Oct. 2021
- **Summary**: A DNN architecture referred to as GPRInvNet was proposed to tackle the challenges of mapping the ground-penetrating radar (GPR) B-Scan data to complex permittivity maps of subsurface structures. The GPRInvNet consisted of a trace-to-trace encoder and a decoder. It was specially designed to take into account the characteristics of GPR inversion when faced with complex GPR B-Scan data, as well as addressing the spatial alignment issues between time-series B-Scan data and spatial permittivity maps. It displayed the ability to fuse features from several adjacent traces on the B-Scan data to enhance each trace, and then further condense the features of each trace separately. As a result, the sensitive zones on the permittivity maps spatially aligned to the enhanced trace could be reconstructed accurately. The GPRInvNet has been utilized to reconstruct the permittivity map of tunnel linings. A diverse range of dielectric models of tunnel linings containing complex defects has been reconstructed using GPRInvNet. The results have demonstrated that the GPRInvNet is capable of effectively reconstructing complex tunnel lining defects with clear boundaries. Comparative results with existing baseline methods also demonstrated the superiority of the GPRInvNet. For the purpose of generalizing the GPRInvNet to real GPR data, some background noise patches recorded from practical model testing were integrated into the synthetic GPR data to retrain the GPRInvNet. The model testing has been conducted for validation, and experimental results revealed that the GPRInvNet had also achieved satisfactory results with regard to the real data.



### IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.05763v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05763v1)
- **Published**: 2019-12-12 04:03:57+00:00
- **Updated**: 2019-12-12 04:03:57+00:00
- **Authors**: Liangzhi Li, Manisha Verma, Yuta Nakashima, Hajime Nagahara, Ryo Kawasaki
- **Comment**: Accepted in 2020 Winter Conference on Applications of Computer Vision
  (WACV 20)
- **Journal**: None
- **Summary**: Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet, with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4$\times$ deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10$\sim$20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available.



### CCCNet: An Attention Based Deep Learning Framework for Categorized Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1912.05765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05765v1)
- **Published**: 2019-12-12 04:13:54+00:00
- **Updated**: 2019-12-12 04:13:54+00:00
- **Authors**: Sarkar Snigdha Sarathi Das, Syed Md. Mukit Rashid, Mohammed Eunus Ali
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Crowd counting problem that counts the number of people in an image has been extensively studied in recent years. In this paper, we introduce a new variant of crowd counting problem, namely "Categorized Crowd Counting", that counts the number of people sitting and standing in a given image. Categorized crowd counting has many real-world applications such as crowd monitoring, customer service, and resource management. The major challenges in categorized crowd counting come from high occlusion, perspective distortion and the seemingly identical upper body posture of sitting and standing persons. Existing density map based approaches perform well to approximate a large crowd, but lose important local information necessary for categorization. On the other hand, traditional detection-based approaches perform poorly in occluded environments, especially when the crowd size gets bigger. Hence, to solve the categorized crowd counting problem, we develop a novel attention-based deep learning framework that addresses the above limitations. In particular, our approach works in three phases: i) We first generate basic detection based sitting and standing density maps to capture the local information; ii) Then, we generate a crowd counting based density map as global counting feature; iii) Finally, we have a cross-branch segregating refinement phase that splits the crowd density map into final sitting and standing density maps using attention mechanism. Extensive experiments show the efficacy of our approach in solving the categorized crowd counting problem.



### One Framework to Register Them All: PointNet Encoding for Point Cloud Alignment
- **Arxiv ID**: http://arxiv.org/abs/1912.05766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05766v1)
- **Published**: 2019-12-12 04:16:47+00:00
- **Updated**: 2019-12-12 04:16:47+00:00
- **Authors**: Vinit Sarode, Xueqian Li, Hunter Goforth, Yasuhiro Aoki, Animesh Dhagat, Rangaprasad Arun Srivatsan, Simon Lucey, Howie Choset
- **Comment**: 10 pages. arXiv admin note: substantial text overlap with
  arXiv:1908.07906
- **Journal**: None
- **Summary**: PointNet has recently emerged as a popular representation for unstructured point cloud data, allowing application of deep learning to tasks such as object detection, segmentation and shape completion. However, recent works in literature have shown the sensitivity of the PointNet representation to pose misalignment. This paper presents a novel framework that uses PointNet encoding to align point clouds and perform registration for applications such as 3D reconstruction, tracking and pose estimation. We develop a framework that compares PointNet features of template and source point clouds to find the transformation that aligns them accurately. In doing so, we avoid computationally expensive correspondence finding steps, that are central to popular registration methods such as ICP and its variants. Depending on the prior information about the shape of the object formed by the point clouds, our framework can produce approaches that are shape specific or general to unseen shapes. Our framework produces approaches that are robust to noise and initial misalignment in data and work robustly with sparse as well as partial point clouds. We perform extensive simulation and real-world experiments to validate the efficacy of our approach and compare the performance with state-of-art approaches. Code is available at https://github.com/vinits5/pointnet-registrationframework.



### To See in the Dark: N2DGAN for Background Modeling in Nighttime Scene
- **Arxiv ID**: http://arxiv.org/abs/1912.06556v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06556v2)
- **Published**: 2019-12-12 04:41:38+00:00
- **Updated**: 2020-04-12 06:53:00+00:00
- **Authors**: Zhenfeng Zhu, Yingying Meng, Deqiang Kong, Xingxing Zhang, Yandong Guo, Yao Zhao
- **Comment**: 11 pages, 11 figures. The paper has been accepted by IEEE
  Transactions on Circuits and Systems for Video Technology(TCSVT)
- **Journal**: None
- **Summary**: Due to the deteriorated conditions of \mbox{illumination} lack and uneven lighting, nighttime images have lower contrast and higher noise than their daytime counterparts of the same scene, which limits seriously the performances of conventional background modeling methods. For such a challenging problem of background modeling under nighttime scene, an innovative and reasonable solution is proposed in this paper, which paves a new way completely different from the existing ones. To make background modeling under nighttime scene performs as well as in daytime condition, we put forward a promising generation-based background modeling framework for foreground surveillance. With a pre-specified daytime reference image as background frame, the {\bfseries GAN} based generation model, called {\bfseries N2DGAN}, is trained to transfer each frame of {\bfseries n}ighttime video {\bfseries to} a virtual {\bfseries d}aytime image with the same scene to the reference image except for the foreground region. Specifically, to balance the preservation of background scene and the foreground object(s) in generating the virtual daytime image, we present a two-pathway generation model, in which the global and local sub-networks are well combined with spatial and temporal consistency constraints. For the sequence of generated virtual daytime images, a multi-scale Bayes model is further proposed to characterize pertinently the temporal variation of background. We evaluate on collected datasets with manually labeled ground truth, which provides a valuable resource for related research community. The impressive results illustrated in both the main paper and supplementary show efficacy of our proposed approach.



### Zooming into Face Forensics: A Pixel-level Analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.05790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05790v1)
- **Published**: 2019-12-12 06:17:37+00:00
- **Updated**: 2019-12-12 06:17:37+00:00
- **Authors**: Jia Li, Tong Shen, Wei Zhang, Hui Ren, Dan Zeng, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: The stunning progress in face manipulation methods has made it possible to synthesize realistic fake face images, which poses potential threats to our society. It is urgent to have face forensics techniques to distinguish those tampered images. A large scale dataset "FaceForensics++" has provided enormous training data generated from prominent face manipulation methods to facilitate anti-fake research. However, previous works focus more on casting it as a classification problem by only considering a global prediction. Through investigation to the problem, we find that training a classification network often fails to capture high quality features, which might lead to sub-optimal solutions. In this paper, we zoom in on the problem by conducting a pixel-level analysis, i.e. formulating it as a pixel-level segmentation task. By evaluating multiple architectures on both segmentation and classification tasks, We show the superiority of viewing the problem from a segmentation perspective. Different ablation studies are also performed to investigate what makes an effective and efficient anti-fake model. Strong baselines are also established, which, we hope, could shed some light on the field of face forensics.



### L3DOC: Lifelong 3D Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.06135v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06135v2)
- **Published**: 2019-12-12 06:41:19+00:00
- **Updated**: 2020-01-06 14:51:50+00:00
- **Authors**: Yuyang Liu, Yang Cong, Gan Sun
- **Comment**: 10 pages,17 figures, CVPR 2020 underreview
- **Journal**: None
- **Summary**: 3D object classification has been widely-applied into both academic and industrial scenarios. However, most state-of-the-art algorithms are facing with a fixed 3D object classification task set, which cannot well tackle the new coming data with incremental tasks as human ourselves. Meanwhile, the performance of most state-of-the-art lifelong learning models can be deteriorated easily on previously learned classification tasks, due to the existing of unordered, large-scale, and irregular 3D geometry data. To address this challenge, in this paper, we propose a Lifelong 3D Object Classification (i.e., L3DOC) framewor, which can consecutively learn new 3D object classification tasks via imitating 'human learning'. Specifically, the core idea of our proposed L3DOC model is to factorize PointNet in a perspective of lifelong learning, while capturing and storing the shared point-knowledge in a perspective of layer-wise tensor factorization architecture. To further transfer the task-specific knowledge from previous tasks to the new coming classification task, a memory attention mechanism is proposed to connect the current task with relevant previously tasks, which can effectively prevent catastrophic forgetting via soft-transferring previous knowledge. To our best knowledge, this is the first work about using lifelong learning to handle 3D object classification task without model fine-tuning or retraining. Furthermore, our L3DOC model can also be extended to other backbone network (e.g., PointNet++). To the end, comparisons on several point cloud datasets validate that our L3DOC model can reduce averaged 1.68~3.36 times parameters for the overall model, without sacrificing classification accuracy of each task.



### STEERAGE: Synthesis of Neural Networks Using Architecture Search and Grow-and-Prune Methods
- **Arxiv ID**: http://arxiv.org/abs/1912.05831v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05831v1)
- **Published**: 2019-12-12 08:42:13+00:00
- **Updated**: 2019-12-12 08:42:13+00:00
- **Authors**: Shayan Hassantabar, Xiaoliang Dai, Niraj K. Jha
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: Neural networks (NNs) have been successfully deployed in many applications. However, architectural design of these models is still a challenging problem. Moreover, neural networks are known to have a lot of redundancy. This increases the computational cost of inference and poses an obstacle to deployment on Internet-of-Thing sensors and edge devices. To address these challenges, we propose the STEERAGE synthesis methodology. It consists of two complementary approaches: efficient architecture search, and grow-and-prune NN synthesis. The first step, covered in a global search module, uses an accuracy predictor to efficiently navigate the architectural search space. The predictor is built using boosted decision tree regression, iterative sampling, and efficient evolutionary search. The second step involves local search. By using various grow-and-prune methodologies for synthesizing convolutional and feed-forward NNs, it reduces the network redundancy, while boosting its performance. We have evaluated STEERAGE performance on various datasets, including MNIST and CIFAR-10. On MNIST dataset, our CNN architecture achieves an error rate of 0.66%, with 8.6x fewer parameters compared to the LeNet-5 baseline. For the CIFAR-10 dataset, we used the ResNet architectures as the baseline. Our STEERAGE-synthesized ResNet-18 has a 2.52% accuracy improvement over the original ResNet-18, 1.74% over ResNet-101, and 0.16% over ResNet-1001, while having comparable number of parameters and FLOPs to the original ResNet-18. This shows that instead of just increasing the number of layers to increase accuracy, an alternative is to use a better NN architecture with fewer layers. In addition, STEERAGE achieves an error rate of just 3.86% with a variant of ResNet architecture with 40 layers. To the best of our knowledge, this is the highest accuracy obtained by ResNet-based architectures on the CIFAR-10 dataset.



### Local Context Normalization: Revisiting Local Normalization
- **Arxiv ID**: http://arxiv.org/abs/1912.05845v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05845v3)
- **Published**: 2019-12-12 09:28:24+00:00
- **Updated**: 2020-05-09 09:27:12+00:00
- **Authors**: Anthony Ortiz, Caleb Robinson, Dan Morris, Olac Fuentes, Christopher Kiekintveld, Md Mahmudulla Hassan, Nebojsa Jojic
- **Comment**: Accepted as a CVPR 2020 oral paper. arXiv admin note: text overlap
  with arXiv:1803.08494 by other authors
- **Journal**: CVPR 2020
- **Summary**: Normalization layers have been shown to improve convergence in deep neural networks, and even add useful inductive biases. In many vision applications the local spatial context of the features is important, but most common normalization schemes including Group Normalization (GN), Instance Normalization (IN), and Layer Normalization (LN) normalize over the entire spatial dimension of a feature. This can wash out important signals and degrade performance. For example, in applications that use satellite imagery, input images can be arbitrarily large; consequently, it is nonsensical to normalize over the entire area. Positional Normalization (PN), on the other hand, only normalizes over a single spatial position at a time. A natural compromise is to normalize features by local context, while also taking into account group level information. In this paper, we propose Local Context Normalization (LCN): a normalization layer where every feature is normalized based on a window around it and the filters in its group. We propose an algorithmic solution to make LCN efficient for arbitrary window sizes, even if every point in the image has a unique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN counterparts for object detection, semantic segmentation, and instance segmentation applications in several benchmark datasets, while keeping performance independent of the batch size and facilitating transfer learning.



### The Benefits of Close-Domain Fine-Tuning for Table Detection in Document Images
- **Arxiv ID**: http://arxiv.org/abs/1912.05846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05846v1)
- **Published**: 2019-12-12 09:30:02+00:00
- **Updated**: 2019-12-12 09:30:02+00:00
- **Authors**: Ángela Casado-García, César Domínguez, Jónathan Heras, Eloy Mata, Vico Pascual
- **Comment**: None
- **Journal**: None
- **Summary**: A correct localisation of tables in a document is instrumental for determining their structure and extracting their contents; therefore, table detection is a key step in table understanding. Nowadays, the most successful methods for table detection in document images employ deep learning algorithms; and, particularly, a technique known as fine-tuning. In this context, such a technique exports the knowledge acquired to detect objects in natural images to detect tables in document images. However, there is only a vague relation between natural and document images, and fine-tuning works better when there is a close relation between the source and target task. In this paper, we show that it is more beneficial to employ fine-tuning from a closer domain. To this aim, we train different object detection algorithms (namely, Mask R-CNN, RetinaNet, SSD and YOLO) using the TableBank dataset (a dataset of images of academic documents designed for table detection and recognition), and fine-tune them for several heterogeneous table detection datasets. Using this approach, we considerably improve the accuracy of the detection models fine-tuned from natural images (in mean a 17%, and, in the best case, up to a 60%).



### Totally Deep Support Vector Machines
- **Arxiv ID**: http://arxiv.org/abs/1912.05864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05864v1)
- **Published**: 2019-12-12 10:18:17+00:00
- **Updated**: 2019-12-12 10:18:17+00:00
- **Authors**: Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Support vector machines (SVMs) have been successful in solving many computer vision tasks including image and video category recognition especially for small and mid-scale training problems. The principle of these non-parametric models is to learn hyperplanes that separate data belonging to different classes while maximizing their margins. However, SVMs constrain the learned hyperplanes to lie in the span of support vectors, fixed/taken from training data, and this reduces their representational power and may lead to limited generalization performances. In this paper, we relax this constraint and allow the support vectors to be learned (instead of being fixed/taken from training data) in order to better fit a given classification task. Our approach, referred to as deep total variation support vector machines, is parametric and relies on a novel deep architecture that learns not only the SVM and the kernel parameters but also the support vectors, resulting into highly effective classifiers. We also show (under a particular setting of the activation functions in this deep architecture) that a large class of kernels and their combinations can be learned. Experiments conducted on the challenging task of skeleton-based action recognition show the outperformance of our deep total variation SVMs w.r.t different baselines as well as the related work.



### Variational Coupling Revisited: Simpler Models, Theoretical Connections, and Novel Applications
- **Arxiv ID**: http://arxiv.org/abs/1912.05888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05888v1)
- **Published**: 2019-12-12 11:44:53+00:00
- **Updated**: 2019-12-12 11:44:53+00:00
- **Authors**: Aaron Wewior, Joachim Weickert
- **Comment**: None
- **Journal**: None
- **Summary**: Variational models with coupling terms are becoming increasingly popular in image analysis. They involve auxiliary variables, such that their energy minimisation splits into multiple fractional steps that can be solved easier and more efficiently. In our paper we show that coupling models offer a number of interesting properties that go far beyond their obvious numerical benefits. We demonstrate that discontinuity-preserving denoising can be achieved even with quadratic data and smoothness terms, provided that the coupling term involves the $L^1$ norm. We show that such an $L^1$ coupling term provides additional information as a powerful edge detector that has remained unexplored so far. While coupling models in the literature approximate higher order regularisation, we argue that already first order coupling models can be useful. As a specific example, we present a first order coupling model that outperforms classical TV regularisation. It also establishes a theoretical connection between TV regularisation and the Mumford-Shah segmentation approach. Unlike other Mumford-Shah algorithms, it is a strictly convex approximation, for which we can guarantee convergence of a split Bregman algorithm.



### LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices
- **Arxiv ID**: http://arxiv.org/abs/1912.05905v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05905v3)
- **Published**: 2019-12-12 13:01:36+00:00
- **Updated**: 2020-08-16 16:33:59+00:00
- **Authors**: Radu Alexandru Rosu, Peer Schütt, Jan Quenzel, Sven Behnke
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.



### SegTHOR: Segmentation of Thoracic Organs at Risk in CT images
- **Arxiv ID**: http://arxiv.org/abs/1912.05950v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05950v1)
- **Published**: 2019-12-12 13:46:15+00:00
- **Updated**: 2019-12-12 13:46:15+00:00
- **Authors**: Z. Lambert, C. Petitjean, B. Dubray, S. Ruan
- **Comment**: Submitted to a journal in december 2019
- **Journal**: None
- **Summary**: In the era of open science, public datasets, along with common experimental protocol, help in the process of designing and validating data science algorithms; they also contribute to ease reproductibility and fair comparison between methods. Many datasets for image segmentation are available, each presenting its own challenges; however just a very few exist for radiotherapy planning. This paper is the presentation of a new dataset dedicated to the segmentation of organs at risk (OARs) in the thorax, i.e. the organs surrounding the tumour that must be preserved from irradiations during radiotherapy. This dataset is called SegTHOR (Segmentation of THoracic Organs at Risk). In this dataset, the OARs are the heart, the trachea, the aorta and the esophagus, which have varying spatial and appearance characteristics. The dataset includes 60 3D CT scans, divided into a training set of 40 and a test set of 20 patients, where the OARs have been contoured manually by an experienced radiotherapist. Along with the dataset, we present some baseline results, obtained using both the original, state-of-the-art architecture U-Net and a simplified version. We investigate different configurations of this baseline architecture that will serve as comparison for future studies on the SegTHOR dataset. Preliminary results show that room for improvement is left, especially for smallest organs.



### Toward Better Understanding of Saliency Prediction in Augmented 360 Degree Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.05971v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1912.05971v2)
- **Published**: 2019-12-12 14:16:05+00:00
- **Updated**: 2020-07-20 13:54:08+00:00
- **Authors**: Yucheng Zhu, Xiongkuo Min, DanDan Zhu, Ke Gu, Jiantao Zhou, Guangtao Zhai, Xiaokang Yang, Wenjun Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality (AR) overlays digital content onto the reality. In AR system, correct and precise estimations of user's visual fixations and head movements can enhance the quality of experience by allocating more computation resources on the areas of interest. However, there is inadequate research about understanding the visual exploration of users when using an AR system or modeling AR visual attention. To bridge the gap between the saliency prediction on real-world scene and on scene augmented by virtual information, we construct the ARVR saliency dataset with 12 diverse videos viewed by 20 people. The virtual reality (VR) technique is employed to simulate the real-world. Annotations of object recognition and tracking as augmented contents are blended into the omnidirectional videos. The saliency annotations of head and eye movements for both original and augmented videos are collected and together constitute the ARVR dataset. We also design a model which is capable of solving the saliency prediction problem in AR. Local block images are extracted to simulate the viewport and offset the projection distortion. Conspicuous visual cues in local viewports are extracted to constitute the spatial features. The optical flow information is estimated as the important temporal feature. We also consider the interplay between virtual information and reality. The composition of the augmentation information is distinguished, and the joint effects of adversarial augmentation and complementary augmentation are estimated. We generate a graph by taking each block image as one node. Both the visual saliency mechanism and the characteristics of viewing behaviors are considered in the computation of edge weights on the graph which are interpreted as Markov chains. The fraction of the visual attention that is diverted to each block image is estimated through equilibrium distribution on of this chain.



### IoU-aware Single-stage Object Detector for Accurate Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.05992v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05992v4)
- **Published**: 2019-12-12 14:38:34+00:00
- **Updated**: 2020-04-15 14:26:40+00:00
- **Authors**: Shengkai Wu, Xiaoping Li, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the simpleness and high efficiency, single-stage object detectors have been widely applied in many computer vision applications . However, the low correlation between the classification score and localization accuracy of the predicted detections has severely hurt the localization accuracy of models. In this paper, IoU-aware single-stage object detector is proposed to solve this problem. Specifically, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the classification score and predicted IoU are multiplied to compute the final detection confidence, which is more correlated with the localization accuracy. The detection confidence is then used as the input of the subsequent NMS and COCO AP computation, which will substantially improve the localization accuracy of models. Sufficient experiments on COCO and PASCAL VOC datasets demonstrate the effectiveness of IoU-aware single-stage object detector on improving model's localization accuracy. Without whistles and bells, the proposed method can substantially improve AP by $1.7\%\sim1.9\%$ and AP75 by $2.2\%\sim2.5\%$ on COCO \textit{test-dev}. On PASCAL VOC, the proposed method can substantially improve AP by $2.9\%\sim4.4\%$ and AP80, AP90 by $4.6\%\sim10.2\%$. Code is available here: {https://github.com/ShengkaiWu/IoU-aware-single-stage-object-detector}.



### Automated Analysis of Femoral Artery Calcification Using Machine Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/1912.06010v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06010v1)
- **Published**: 2019-12-12 15:02:24+00:00
- **Updated**: 2019-12-12 15:02:24+00:00
- **Authors**: Liang Zhao, Brendan Odigwe, Susan Lessner, Daniel G. Clair, Firas Mussa, Homayoun Valafar
- **Comment**: 6 pages, submitted for consideration to CSCI 2016
- **Journal**: None
- **Summary**: We report an object tracking algorithm that combines geometrical constraints, thresholding, and motion detection for tracking of the descending aorta and the network of major arteries that branch from the aorta including the iliac and femoral arteries. Using our automated identification and analysis, arterial system was identified with more than 85% success when compared to human annotation. Furthermore, the reported automated system is capable of producing a stenosis profile, and a calcification score similar to the Agatston score. The use of stenosis and calcification profiles will lead to the development of better-informed diagnostic and prognostic tools.



### An Approach to Super-Resolution of Sentinel-2 Images Based on Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.06013v2
- **DOI**: 10.1109/M2GARSS47143.2020.9105165
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06013v2)
- **Published**: 2019-12-12 15:04:25+00:00
- **Updated**: 2020-02-07 17:01:17+00:00
- **Authors**: Kexin Zhang, Gencer Sumbul, Begüm Demir
- **Comment**: Accepted at IEEE Mediterranean and Middle-East Geoscience and Remote
  Sensing Symposium (M2GARSS) 2020
- **Journal**: None
- **Summary**: This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches.



### Efficient Per-Example Gradient Computations in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.06015v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.06015v1)
- **Published**: 2019-12-12 15:10:14+00:00
- **Updated**: 2019-12-12 15:10:14+00:00
- **Authors**: Gaspar Rochette, Andre Manoel, Eric W. Tramel
- **Comment**: None
- **Journal**: Theory and Practice of Differential Privacy (TPDP) workshop at CCS
  2020
- **Summary**: Deep learning frameworks leverage GPUs to perform massively-parallel computations over batches of many training examples efficiently. However, for certain tasks, one may be interested in performing per-example computations, for instance using per-example gradients to evaluate a quantity of interest unique to each example. One notable application comes from the field of differential privacy, where per-example gradients must be norm-bounded in order to limit the impact of each example on the aggregated batch gradient. In this work, we discuss how per-example gradients can be efficiently computed in convolutional neural networks (CNNs). We compare existing strategies by performing a few steps of differentially-private training on CNNs of varying sizes. We also introduce a new strategy for per-example gradient calculation, which is shown to be advantageous depending on the model architecture and how the model is trained. This is a first step in making differentially-private training of CNNs practical.



### Hue-Net: Intensity-based Image-to-Image Translation with Differentiable Histogram Loss Functions
- **Arxiv ID**: http://arxiv.org/abs/1912.06044v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06044v1)
- **Published**: 2019-12-12 15:48:55+00:00
- **Updated**: 2019-12-12 15:48:55+00:00
- **Authors**: Mor Avi-Aharon, Assaf Arbelle, Tammy Riklin Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We present the Hue-Net - a novel Deep Learning framework for Intensity-based Image-to-Image Translation. The key idea is a new technique termed network augmentation which allows a differentiable construction of intensity histograms from images. We further introduce differentiable representations of (1D) cyclic and joint (2D) histograms and use them for defining loss functions based on cyclic Earth Mover's Distance (EMD) and Mutual Information (MI). While the Hue-Net can be applied to several image-to-image translation tasks, we choose to demonstrate its strength on color transfer problems, where the aim is to paint a source image with the colors of a different target image. Note that the desired output image does not exist and therefore cannot be used for supervised pixel-to-pixel learning. This is accomplished by using the HSV color-space and defining an intensity-based loss that is built on the EMD between the cyclic hue histograms of the output and the target images. To enforce color-free similarity between the source and the output images, we define a semantic-based loss by a differentiable approximation of the MI of these images. The incorporation of histogram loss functions in addition to an adversarial loss enables the construction of semantically meaningful and realistic images. Promising results are presented for different datasets.



### Coronary Artery Plaque Characterization from CCTA Scans using Deep Learning and Radiomics
- **Arxiv ID**: http://arxiv.org/abs/1912.06075v2
- **DOI**: 10.1007/978-3-030-32251-9_65
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.06075v2)
- **Published**: 2019-12-12 16:51:09+00:00
- **Updated**: 2019-12-13 11:34:47+00:00
- **Authors**: Felix Denzinger, Michael Wels, Nishant Ravikumar, Katharina Breininger, Anika Reidelshöfer, Joachim Eckert, Michael Sühling, Axel Schmermund, Andreas Maier
- **Comment**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention. Springer, Cham, 2019
- **Journal**: None
- **Summary**: Assessing coronary artery plaque segments in coronary CT angiography scans is an important task to improve patient management and clinical outcomes, as it can help to decide whether invasive investigation and treatment are necessary. In this work, we present three machine learning approaches capable of performing this task. The first approach is based on radiomics, where a plaque segmentation is used to calculate various shape-, intensity- and texture-based features under different image transformations. A second approach is based on deep learning and relies on centerline extraction as sole prerequisite. In the third approach, we fuse the deep learning approach with radiomic features. On our data the methods reached similar scores as simulated fractional flow reserve (FFR) measurements, which - in contrast to our methods - requires an exact segmentation of the whole coronary tree and often time-consuming manual interaction. In literature, the performance of simulated FFR reaches an AUC between 0.79-0.93 predicting an abnormal invasive FFR that demands revascularization. The radiomics approach achieves an AUC of 0.86, the deep learning approach 0.84 and the combined method 0.88 for predicting the revascularization decision directly. While all three proposed methods can be determined within seconds, the FFR simulation typically takes several minutes. Provided representative training data in sufficient quantities, we believe that the presented methods can be used to create systems for fully automatic non-invasive risk assessment for a variety of adverse cardiac events.



### Human Motion Anticipation with Symbolic Label
- **Arxiv ID**: http://arxiv.org/abs/1912.06079v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06079v2)
- **Published**: 2019-12-12 16:56:32+00:00
- **Updated**: 2019-12-13 09:38:48+00:00
- **Authors**: Julian Tanke, Andreas Weber, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: Anticipating human motion depends on two factors: the past motion and the person's intention. While the first factor has been extensively utilized to forecast short sequences of human motion, the second one remains elusive. In this work we approximate a person's intention via a symbolic representation, for example fine-grained action labels such as walking or sitting down. Forecasting a symbolic representation is much easier than forecasting the full body pose with its complex inter-dependencies. However, knowing the future actions makes forecasting human motion easier. We exploit this connection by first anticipating symbolic labels and then generate human motion, conditioned on the human motion input sequence as well as on the forecast labels. This allows the model to anticipate motion changes many steps ahead and adapt the poses accordingly. We achieve state-of-the-art results on short-term as well as on long-term human motion forecasting.



### Unified Generative Adversarial Networks for Controllable Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1912.06112v2
- **DOI**: 10.1109/TIP.2020.3021789
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06112v2)
- **Published**: 2019-12-12 18:21:30+00:00
- **Updated**: 2020-09-02 11:00:42+00:00
- **Authors**: Hao Tang, Hong Liu, Nicu Sebe
- **Comment**: Accepted to TIP, an extended version of a paper published in ACM MM
  2018. arXiv admin note: substantial text overlap with arXiv:1808.04859
- **Journal**: None
- **Summary**: We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fr\'echet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN.



### Local Deep Implicit Functions for 3D Shape
- **Arxiv ID**: http://arxiv.org/abs/1912.06126v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1912.06126v2)
- **Published**: 2019-12-12 18:50:46+00:00
- **Updated**: 2020-06-12 03:26:47+00:00
- **Authors**: Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser
- **Comment**: Camera ready version for CVPR 2020 Oral. Prior to review, this paper
  was referred to as DSIF, "Deep Structured Implicit Functions." 11 pages, 9
  figures. Project video at https://youtu.be/3RAITzNWVJs
- **Journal**: None
- **Summary**: The goal of this project is to learn a 3D shape representation that enables accurate surface reconstruction, compact storage, efficient computation, consistency for similar shapes, generalization across diverse shape categories, and inference from depth camera observations. Towards this end, we introduce Local Deep Implicit Functions (LDIF), a 3D shape representation that decomposes space into a structured set of learned implicit functions. We provide networks that infer the space decomposition and local deep implicit functions from a 3D mesh or posed depth image. During experiments, we find that it provides 10.3 points higher surface reconstruction accuracy (F-Score) than the state-of-the-art (OccNet), while requiring fewer than 1 percent of the network parameters. Experiments on posed depth image completion and generalization to unseen classes show 15.8 and 17.8 point improvements over the state-of-the-art, while producing a structured 3D representation for each input with consistency across diverse shape collections.



### Learning Effective Visual Relationship Detector on 1 GPU
- **Arxiv ID**: http://arxiv.org/abs/1912.06185v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06185v1)
- **Published**: 2019-12-12 19:59:41+00:00
- **Updated**: 2019-12-12 19:59:41+00:00
- **Authors**: Yichao Lu, Cheng Chang, Himanshu Rai, Guangwei Yu, Maksims Volkovs
- **Comment**: None
- **Journal**: None
- **Summary**: We present our winning solution to the Open Images 2019 Visual Relationship challenge. This is the largest challenge of its kind to date with nearly 9 million training images. Challenge task consists of detecting objects and identifying relationships between them in complex scenes. Our solution has three stages, first object detection model is fine-tuned for the challenge classes using a novel weight transfer approach. Then, spatio-semantic and visual relationship models are trained on candidate object pairs. Finally, features and model predictions are combined to generate the final relationship prediction. Throughout the challenge we focused on minimizing the hardware requirements of our architecture. Specifically, our weight transfer approach enables much faster optimization, allowing the entire architecture to be trained on a single GPU in under two days. In addition to efficient optimization, our approach also achieves superior accuracy winning first place out of over 200 teams, and outperforming the second place team by over $5\%$ on the held-out private leaderboard.



### Greenery Segmentation In Urban Images By Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.06199v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4.6, I.5.4, I.4.6; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1912.06199v1)
- **Published**: 2019-12-12 20:35:15+00:00
- **Updated**: 2019-12-12 20:35:15+00:00
- **Authors**: Artur A. M. Oliveira, Nina S. T. Hirata, Roberto Hirata Jr
- **Comment**: Supplemental material can be found at
  http://greenery_data.arturao.org/
- **Journal**: None
- **Summary**: Vegetation is a relevant feature in the urban scenery and its awareness can be measured in an image by the Green View Index (GVI). Previous approaches to estimate the GVI were based upon heuristics image processing approaches and recently by deep learning networks (DLN). By leveraging some recent DLN architectures tuned to the image segmentation problem and exploiting a weighting strategy in the loss function (LF) we improved previously reported results in similar datasets.



### ManiGAN: Text-Guided Image Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1912.06203v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06203v2)
- **Published**: 2019-12-12 20:48:52+00:00
- **Updated**: 2020-03-30 19:42:35+00:00
- **Authors**: Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: The goal of our paper is to semantically edit parts of an image matching a given text that describes desired attributes (e.g., texture, colour, and background), while preserving other contents that are irrelevant to the text. To achieve this, we propose a novel generative adversarial network (ManiGAN), which contains two key components: text-image affine combination module (ACM) and detail correction module (DCM). The ACM selects image regions relevant to the given text and then correlates the regions with corresponding semantic words for effective manipulation. Meanwhile, it encodes original image features to help reconstruct text-irrelevant contents. The DCM rectifies mismatched attributes and completes missing contents of the synthetic image. Finally, we suggest a new metric for evaluating image manipulation results, in terms of both the generation of new attributes and the reconstruction of text-irrelevant contents. Extensive experiments on the CUB and COCO datasets demonstrate the superior performance of the proposed method. Code is available at https://github.com/mrlibw/ManiGAN.



### Theme-Matters: Fashion Compatibility Learning via Theme Attention
- **Arxiv ID**: http://arxiv.org/abs/1912.06227v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06227v3)
- **Published**: 2019-12-12 21:32:28+00:00
- **Updated**: 2020-10-05 03:30:08+00:00
- **Authors**: Jui-Hsin Lai, Bo Wu, Xin Wang, Dan Zeng, Tao Mei, Jingen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion compatibility learning is important to many fashion markets such as outfit composition and online fashion recommendation. Unlike previous work, we argue that fashion compatibility is not only a visual appearance compatible problem but also a theme-matters problem. An outfit, which consists of a set of fashion items (e.g., shirt, suit, shoes, etc.), is considered to be compatible for a "dating" event, yet maybe not for a "business" occasion. In this paper, we aim at solving the fashion compatibility problem given specific themes. To this end, we built the first real-world theme-aware fashion dataset comprising 14K around outfits labeled with 32 themes. In this dataset, there are more than 40K fashion items labeled with 152 fine-grained categories. We also propose an attention model learning fashion compatibility given a specific theme. It starts with a category-specific subspace learning, which projects compatible outfit items in certain categories to be close in the subspace. Thanks to strong connections between fashion themes and categories, we then build a theme-attention model over the category-specific embedding space. This model associates themes with the pairwise compatibility with attention, and thus compute the outfit-wise compatibility. To the best of our knowledge, this is the first attempt to estimate outfit compatibility conditional on a theme. We conduct extensive qualitative and quantitative experiments on our new dataset. Our method outperforms the state-of-the-art approaches.



### Unconstrained Facial Expression Transfer using Style-based Generator
- **Arxiv ID**: http://arxiv.org/abs/1912.06253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06253v1)
- **Published**: 2019-12-12 23:01:15+00:00
- **Updated**: 2019-12-12 23:01:15+00:00
- **Authors**: Chao Yang, Ser-Nam Lim
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression transfer and reenactment has been an important research problem given its applications in face editing, image manipulation, and fabricated videos generation. We present a novel method for image-based facial expression transfer, leveraging the recent style-based GAN shown to be very effective for creating realistic looking images. Given two face images, our method can create plausible results that combine the appearance of one image and the expression of the other. To achieve this, we first propose an optimization procedure based on StyleGAN to infer hierarchical style vector from an image that disentangle different attributes of the face. We further introduce a linear combination scheme that fuses the style vectors of the two given images and generate a new face that combines the expression and appearance of the inputs. Our method can create high-quality synthesis with accurate facial reenactment. Unlike many existing methods, we do not rely on geometry annotations, and can be applied to unconstrained facial images of any identities without the need for retraining, making it feasible to generate large-scale expression-transferred results.



### Mcity Data Collection for Automated Vehicles Study
- **Arxiv ID**: http://arxiv.org/abs/1912.06258v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1912.06258v1)
- **Published**: 2019-12-12 23:13:59+00:00
- **Updated**: 2019-12-12 23:13:59+00:00
- **Authors**: Yiqun Dong, Yuanxin Zhong, Wenbo Yu, Minghan Zhu, Pingping Lu, Yeyang Fang, Jiajun Hong, Huei Peng
- **Comment**: None
- **Journal**: None
- **Summary**: The main goal of this paper is to introduce the data collection effort at Mcity targeting automated vehicle development. We captured a comprehensive set of data from a set of perception sensors (Lidars, Radars, Cameras) as well as vehicle steering/brake/throttle inputs and an RTK unit. Two in-cabin cameras record the human driver's behaviors for possible future use. The naturalistic driving on selected open roads is recorded at different time of day and weather conditions. We also perform designed choreography data collection inside the Mcity test facility focusing on vehicle to vehicle, and vehicle to vulnerable road user interactions which is quite unique among existing open-source datasets. The vehicle platform, data content, tags/labels, and selected analysis results are shown in this paper.



### Towards Disentangled Representations for Human Retargeting by Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.06265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06265v1)
- **Published**: 2019-12-12 23:35:25+00:00
- **Updated**: 2019-12-12 23:35:25+00:00
- **Authors**: Chao Yang, Xiaofeng Liu, Qingming Tang, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of learning disentangled representations for data across multiple domains and its applications in human retargeting. Our goal is to map an input image to an identity-invariant latent representation that captures intrinsic factors such as expressions and poses. To this end, we present a novel multi-view learning approach that leverages various data sources such as images, keypoints, and poses. Our model consists of multiple id-conditioned VAEs for different views of the data. During training, we encourage the latent embeddings to be consistent across these views. Our observation is that auxiliary data like keypoints and poses contain critical, id-agnostic semantic information, and it is easier to train a disentangling CVAE on these simpler views to separate such semantics from other id-specific attributes. We show that training multi-view CVAEs and encourage latent-consistency guides the image encoding to preserve the semantics of expressions and poses, leading to improved disentangled representations and better human retargeting results.



### Inferring Distributions Over Depth from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1912.06268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.06268v1)
- **Published**: 2019-12-12 23:56:57+00:00
- **Updated**: 2019-12-12 23:56:57+00:00
- **Authors**: Gengshan Yang, Peiyun Hu, Deva Ramanan
- **Comment**: IROS 2019
- **Journal**: None
- **Summary**: When building a geometric scene understanding system for autonomous vehicles, it is crucial to know when the system might fail. Most contemporary approaches cast the problem as depth regression, whose output is a depth value for each pixel. Such approaches cannot diagnose when failures might occur. One attractive alternative is a deep Bayesian network, which captures uncertainty in both model parameters and ambiguous sensor measurements. However, estimating uncertainties is often slow and the distributions are often limited to be uni-modal. In this paper, we recast the continuous problem of depth regression as discrete binary classification, whose output is an un-normalized distribution over possible depths for each pixel. Such output allows one to reliably and efficiently capture multi-modal depth distributions in ambiguous cases, such as depth discontinuities and reflective surfaces. Results on standard benchmarks show that our method produces accurate depth predictions and significantly better uncertainty estimations than prior art while running near real-time. Finally, by making use of uncertainties of the predicted distribution, we significantly reduce streak-like artifacts and improves accuracy as well as memory efficiency in 3D map reconstruction.



