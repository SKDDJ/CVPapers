# Arxiv Papers in cs.CV on 2019-12-13
### Joint Viewpoint and Keypoint Estimation with Real and Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1912.06274v1
- **DOI**: 10.1007/978-3-030-33676-9_8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06274v1)
- **Published**: 2019-12-13 00:46:19+00:00
- **Updated**: 2019-12-13 00:46:19+00:00
- **Authors**: Pau Panareda Busto, Juergen Gall
- **Comment**: 11 pages, 4 figures
- **Journal**: German Conference on Pattern Recognition. Lecture Notes in
  Computer Science, Volume 11824, Year 2019, Pages 107-121
- **Summary**: The estimation of viewpoints and keypoints effectively enhance object detection methods by extracting valuable traits of the object instances. While the output of both processes differ, i.e., angles vs. list of characteristic points, they indeed share the same focus on how the object is placed in the scene, inducing that there is a certain level of correlation between them. Therefore, we propose a convolutional neural network that jointly computes the viewpoint and keypoints for different object categories. By training both tasks together, each task improves the accuracy of the other. Since the labelling of object keypoints is very time consuming for human annotators, we also introduce a new synthetic dataset with automatically generated viewpoint and keypoints annotations. Our proposed network can also be trained on datasets that contain viewpoint and keypoints annotations or only one of them. The experiments show that the proposed approach successfully exploits this implicit correlation between the tasks and outperforms previous techniques that are trained independently.



### Deep-learning-based classification and retrieval of components of a process plant from segmented point clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.12135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1912.12135v1)
- **Published**: 2019-12-13 01:34:28+00:00
- **Updated**: 2019-12-13 01:34:28+00:00
- **Authors**: Hyungki Kim, Duhwan Mun
- **Comment**: 31 pages, 7305 words, 16 figures, 4 tables
- **Journal**: None
- **Summary**: Technology to recognize the type of component represented by a point cloud is required in the reconstruction process of an as-built model of a process plant based on laser scanning. The reconstruction process of a process plant through laser scanning is divided into point cloud registration, point cloud segmentation, and component type recognition and placement. Loss of shape data or imbalance of point cloud density problems generally occur in the point cloud data collected from large-scale facilities. In this study, we experimented with the possibility of applying object recognition technology based on 3D deep learning networks, which have been showing high performance recently, and analyzed the results. For training data, we used a segmented point cloud repository about components that we constructed by scanning a process plant. For networks, we selected the multi-view convolutional neural network (MVCNN), which is a view-based method, and PointNet, which is designed to allow the direct input of point cloud data. In the case of the MVCNN, we also performed an experiment on the generation method for two types of multi-view images that can complement the shape occlusion of the segmented point cloud. In this experiment, the MVCNN showed the highest retrieval accuracy of approximately 87%, whereas PointNet showed the highest retrieval mean average precision of approximately 84%. Furthermore, both networks showed high recognition performance for the segmented point cloud of plant components when there was sufficient training data.



### Meta-Learning Initializations for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.06290v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.06290v4)
- **Published**: 2019-12-13 01:58:36+00:00
- **Updated**: 2020-05-07 23:33:07+00:00
- **Authors**: Sean M. Hendryx, Andrew B. Leach, Paul D. Hein, Clayton T. Morrison
- **Comment**: None
- **Journal**: None
- **Summary**: We extend first-order model agnostic meta-learning algorithms (including FOMAML and Reptile) to image segmentation, present a novel neural network architecture built for fast learning which we call EfficientLab, and leverage a formal definition of the test error of meta-learning algorithms to decrease error on out of distribution tasks. We show state of the art results on the FSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian optimization to infer the optimal test-time adaptation routine hyperparameters. We also construct a small benchmark dataset, FP-k, for the empirical study of how meta-learning systems perform in both few- and many-shot settings. On the FP-k dataset, we show that meta-learned initializations provide value for canonical few-shot image segmentation but their performance is quickly matched by conventional transfer learning with performance being equal beyond 10 labeled examples. Our code, meta-learned model, and the FP-k dataset are available at https://github.com/ml4ai/mliis .



### A Practical Solution for SAR Despeckling With Adversarial Learning Generated Speckled-to-Speckled Images
- **Arxiv ID**: http://arxiv.org/abs/1912.06295v2
- **DOI**: 10.1109/LGRS.2020.3034470
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1912.06295v2)
- **Published**: 2019-12-13 02:16:29+00:00
- **Updated**: 2021-01-17 20:48:56+00:00
- **Authors**: Ye Yuan, Jian Guan, Pengming Feng, Yanxia Wu
- **Comment**: 5 pages, 4 figures
- **Journal**: IEEE Geoscience and Remote Sensing Letters,(2020)1-5
- **Summary**: In this letter, we aim to address a synthetic aperture radar (SAR) despeckling problem with the necessity of neither clean (speckle-free) SAR images nor independent speckled image pairs from the same scene, and a practical solution for SAR despeckling (PSD) is proposed. First, an adversarial learning framework is designed to generate speckled-to-speckled (S2S) image pairs from the same scene in the situation where only single speckled SAR images are available. Then, the S2S SAR image pairs are employed to train a modified despeckling Nested-UNet model using the Noise2Noise (N2N) strategy. Moreover, an iterative version of the PSD method (PSDi) is also presented. Experiments are conducted on both synthetic speckled and real SAR data to demonstrate the superiority of the proposed methods compared with several state-of-the-art methods. The results show that our methods can reach a good tradeoff between feature preservation and speckle suppression.



### Identity Preserve Transform: Understand What Activity Classification Models Have Learnt
- **Arxiv ID**: http://arxiv.org/abs/1912.06314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06314v1)
- **Published**: 2019-12-13 03:55:07+00:00
- **Updated**: 2019-12-13 03:55:07+00:00
- **Authors**: Jialing Lyu, Weichao Qiu, Xinyue Wei, Yi Zhang, Alan Yuille, Zheng-Jun Zha
- **Comment**: None
- **Journal**: None
- **Summary**: Activity classification has observed great success recently. The performance on small dataset is almost saturated and people are moving towards larger datasets. What leads to the performance gain on the model and what the model has learnt? In this paper we propose identity preserve transform (IPT) to study this problem. IPT manipulates the nuisance factors (background, viewpoint, etc.) of the data while keeping those factors related to the task (human motion) unchanged. To our surprise, we found popular models are using highly correlated information (background, object) to achieve high classification accuracy, rather than using the essential information (human motion). This can explain why an activity classification model usually fails to generalize to datasets it is not trained on. We implement IPT in two forms, i.e. image-space transform and 3D transform, using synthetic images. The tool will be made open-source to help study model and dataset design.



### Grounding-Tracking-Integration
- **Arxiv ID**: http://arxiv.org/abs/1912.06316v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06316v2)
- **Published**: 2019-12-13 04:07:23+00:00
- **Updated**: 2020-11-14 17:09:44+00:00
- **Authors**: Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jinsong Su, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study Tracking by Language that localizes the target box sequence in a video based on a language query. We propose a framework called GTI that decomposes the problem into three sub-tasks: Grounding, Tracking, and Integration. The three sub-task modules operate simultaneously and predict the box sequence frame-by-frame. "Grounding" predicts the referred region directly from the language query. "Tracking" localizes the target based on the history of the grounded regions in previous frames. "Integration" generates final predictions by synergistically combining grounding and tracking. With the "integration" task as the key, we explore how to indicate the quality of the grounded regions in each frame and achieve the desired mutually beneficial combination. To this end, we propose an "RT-integration" method that defines and predicts two scores to guide the integration: 1) R-score represents the Region correctness whether the grounding prediction accurately covers the target, and 2) T-score represents the Template quality whether the region provides informative visual cues to improve tracking in future frames. We present our real-time GTI implementation with the proposed RT-integration, and benchmark the framework on LaSOT and Lingual OTB99 with highly promising results. Moreover, we produce a disambiguated version of LaSOT queries to facilitate future tracking by language studies.



### Small Object Detection using Context and Attention
- **Arxiv ID**: http://arxiv.org/abs/1912.06319v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06319v2)
- **Published**: 2019-12-13 04:19:48+00:00
- **Updated**: 2019-12-16 04:39:29+00:00
- **Authors**: Jeong-Seon Lim, Marcella Astrid, Hyun-Jin Yoon, Seung-Ik Lee
- **Comment**: None
- **Journal**: None
- **Summary**: There are many limitations applying object detection algorithm on various environments. Especially detecting small objects is still challenging because they have low resolution and limited information. We propose an object detection method using context for improving accuracy of detecting small objects. The proposed method uses additional features from different layers as context by concatenating multi-scale features. We also propose object detection with attention mechanism which can focus on the object in image, and it can include contextual information from target layer. Experimental results shows that proposed method also has higher accuracy than conventional SSD on detecting small objects. Also, for 300$\times$300 input, we achieved 78.1% Mean Average Precision (mAP) on the PASCAL VOC2007 test set.



### Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?
- **Arxiv ID**: http://arxiv.org/abs/1912.06321v2
- **DOI**: 10.1109/LRA.2020.3013848
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.06321v2)
- **Published**: 2019-12-13 04:29:38+00:00
- **Updated**: 2020-08-17 03:26:55+00:00
- **Authors**: Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (RA-L) 2020
- **Summary**: Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation, developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots, transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections, abusing collision dynamics to 'slide' along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving $SRCC_{Succ}$ from 0.18 to 0.844), increasing confidence that in-simulation comparisons will translate to deployed systems in reality.



### Down to the Last Detail: Virtual Try-on with Detail Carving
- **Arxiv ID**: http://arxiv.org/abs/1912.06324v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06324v3)
- **Published**: 2019-12-13 04:55:19+00:00
- **Updated**: 2020-10-27 14:44:48+00:00
- **Authors**: Jiahang Wang, Wei Zhang, Weizhong Liu, Tao Mei
- **Comment**: Insufficient results
- **Journal**: None
- **Summary**: Virtual try-on under arbitrary poses has attracted lots of research attention due to its huge potential applications. However, existing methods can hardly preserve the details in clothing texture and facial identity (face, hair) while fitting novel clothes and poses onto a person. In this paper, we propose a novel multi-stage framework to synthesize person images, where rich details in salient regions can be well preserved. Specifically, a multi-stage framework is proposed to decompose the generation into spatial alignment followed by a coarse-to-fine generation. To better preserve the details in salient areas such as clothing and facial areas, we propose a Tree-Block (tree dilated fusion block) to harness multi-scale features in the generator networks. With end-to-end training of multiple stages, the whole framework can be jointly optimized for results with significantly better visual fidelity and richer details. Extensive experiments on standard datasets demonstrate that our proposed framework achieves the state-of-the-art performance, especially in preserving the visual details in clothing texture and facial identity. Our implementation will be publicly available soon.



### Toward Automatic Threat Recognition for Airport X-ray Baggage Screening with Deep Convolutional Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.06329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06329v1)
- **Published**: 2019-12-13 05:26:25+00:00
- **Updated**: 2019-12-13 05:26:25+00:00
- **Authors**: Kevin J Liang, John B. Sigman, Gregory P. Spell, Dan Strellis, William Chang, Felix Liu, Tejas Mehta, Lawrence Carin
- **Comment**: None
- **Journal**: None
- **Summary**: For the safety of the traveling public, the Transportation Security Administration (TSA) operates security checkpoints at airports in the United States, seeking to keep dangerous items off airplanes. At these checkpoints, the TSA employs a fleet of X-ray scanners, such as the Rapiscan 620DV, so Transportation Security Officers (TSOs) can inspect the contents of carry-on possessions. However, identifying and locating all potential threats can be a challenging task. As a result, the TSA has taken a recent interest in deep learning-based automated detection algorithms that can assist TSOs. In a collaboration funded by the TSA, we collected a sizable new dataset of X-ray scans with a diverse set of threats in a wide array of contexts, trained several deep convolutional object detection models, and integrated such models into the Rapiscan 620DV, resulting in functional prototypes capable of operating in real time. We show performance of our models on held-out evaluation sets, analyze several design parameters, and demonstrate the potential of such systems for automated detection of threats that can be found in airports.



### A Quantum Computational Approach to Correspondence Problems on Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1912.12296v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.ET, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.12296v2)
- **Published**: 2019-12-13 05:48:33+00:00
- **Updated**: 2020-03-29 18:01:59+00:00
- **Authors**: Vladislav Golyanik, Christian Theobalt
- **Comment**: 11 pages, 5 figures, 2 tables, CVPR
- **Journal**: None
- **Summary**: Modern adiabatic quantum computers (AQC) are already used to solve difficult combinatorial optimisation problems in various domains of science. Currently, only a few applications of AQC in computer vision have been demonstrated. We review AQC and derive a new algorithm for correspondence problems on point sets suitable for execution on AQC. Our algorithm has a subquadratic computational complexity of the state preparation. Examples of successful transformation estimation and point set alignment by simulated sampling are shown in the systematic experimental evaluation. Finally, we analyse the differences in the solutions and the corresponding energy values.



### A Method for Arbitrary Instance Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1912.06347v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06347v1)
- **Published**: 2019-12-13 07:42:35+00:00
- **Updated**: 2019-12-13 07:42:35+00:00
- **Authors**: Zhifeng Yu, Yusheng Wu, Tianyou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to synthesize style and content of different images to form a visually coherent image holds great promise in various applications such as stylistic painting, design prototyping, image editing, and augmented reality. However, the majority of works in image style transfer have focused on transferring the style of an image to the entirety of another image, and only a very small number of works have experimented on methods to transfer style to an instance of another image. Researchers have proposed methods to circumvent the difficulty of transferring style to an instance in an arbitrary shape. In this paper, we propose a topologically inspired algorithm called Forward Stretching to tackle this problem by transforming an instance into a tensor representation, which allows us to transfer style to this instance itself directly. Forward Stretching maps pixels to specific positions and interpolate values between pixels to transform an instance to a tensor. This algorithm allows us to introduce a method to transfer arbitrary style to an instance in an arbitrary shape. We showcase the results of our method in this paper.



### Learned Video Compression via Joint Spatial-Temporal Correlation Exploration
- **Arxiv ID**: http://arxiv.org/abs/1912.06348v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1912.06348v1)
- **Published**: 2019-12-13 07:45:44+00:00
- **Updated**: 2019-12-13 07:45:44+00:00
- **Authors**: Haojie Liu, Han shen, Lichao Huang, Ming Lu, Tong Chen, Zhan Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional video compression technologies have been developed over decades in pursuit of higher coding efficiency. Efficient temporal information representation plays a key role in video coding. Thus, in this paper, we propose to exploit the temporal correlation using both first-order optical flow and second-order flow prediction. We suggest an one-stage learning approach to encapsulate flow as quantized features from consecutive frames which is then entropy coded with adaptive contexts conditioned on joint spatial-temporal priors to exploit second-order correlations. Joint priors are embedded in autoregressive spatial neighbors, co-located hyper elements and temporal neighbors using ConvLSTM recurrently. We evaluate our approach for the low-delay scenario with High-Efficiency Video Coding (H.265/HEVC), H.264/AVC and another learned video compression method, following the common test settings. Our work offers the state-of-the-art performance, with consistent gains across all popular test sequences.



### Elastic registration based on compliance analysis and biomechanical graph matching
- **Arxiv ID**: http://arxiv.org/abs/1912.06353v1
- **DOI**: 10.1007/s10439-019-02364-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06353v1)
- **Published**: 2019-12-13 08:05:58+00:00
- **Updated**: 2019-12-13 08:05:58+00:00
- **Authors**: Jaime Garcia Guevara, Igor Peterlik, Marie-Odile Berger, Stéphane Cotin
- **Comment**: Annals of Biomedical Engineering, Springer Verlag, 2019
- **Journal**: None
- **Summary**: An automatic elastic registration method suited for vascularized organs is proposed. The vasculature in both the preoperative and intra-operative images is represented as a graph. A typical application of this method is the fusion of pre-operative information onto the organ during surgery, to compensate for the limited details provided by the intra-operative imaging modality (e.g. CBCT) and to cope with changes in the shape of the organ. Due to image modalities differences and organ deformation, each graph has a different topology and shape. The Adaptive Compliance Graph Matching (ACGM) method presented does not require any manual initialization, handles intra-operative nonrigid deformations of up to 65 mm and computes a complete displacement field over the organ from only the matched vasculature. ACGM is better than the previous Biomechanical Graph Matching method 3 (BGM) because it uses an efficient biomechanical vascularized liver model to compute the organ's transformation and the vessels bifurcations compliance. This allows to efficiently find the best graph matches with a novel compliance-based adaptive search. These contributions are evaluated on ten realistic synthetic and two real porcine automatically segmented datasets. ACGM obtains better target registration error (TRE) than BGM, with an average TRE in the real datasets of 4.2 mm compared to 6.5 mm, respectively. It also is up to one order of magnitude faster, less dependent on the parameters used and more robust to noise.



### Bonn Activity Maps: Dataset Description
- **Arxiv ID**: http://arxiv.org/abs/1912.06354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06354v1)
- **Published**: 2019-12-13 08:09:57+00:00
- **Updated**: 2019-12-13 08:09:57+00:00
- **Authors**: Julian Tanke, Oh-Hun Kwon, Patrick Stotko, Radu Alexandru Rosu, Michael Weinmann, Hassan Errami, Sven Behnke, Maren Bennewitz, Reinhard Klein, Andreas Weber, Angela Yao, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: The key prerequisite for accessing the huge potential of current machine learning techniques is the availability of large databases that capture the complex relations of interest. Previous datasets are focused on either 3D scene representations with semantic information, tracking of multiple persons and recognition of their actions, or activity recognition of a single person in captured 3D environments. We present Bonn Activity Maps, a large-scale dataset for human tracking, activity recognition and anticipation of multiple persons. Our dataset comprises four different scenes that have been recorded by time-synchronized cameras each only capturing the scene partially, the reconstructed 3D models with semantic annotations, motion trajectories for individual people including 3D human poses as well as human activity annotations. We utilize the annotations to generate activity likelihoods on the 3D models called activity maps.



### Fast Image Caption Generation with Position Alignment
- **Arxiv ID**: http://arxiv.org/abs/1912.06365v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06365v1)
- **Published**: 2019-12-13 09:06:46+00:00
- **Updated**: 2019-12-13 09:06:46+00:00
- **Authors**: Zheng-cong Fei
- **Comment**: AAAI2020 oral presentation on workshop
- **Journal**: None
- **Summary**: Recent neural network models for image captioning usually employ an encoder-decoder architecture, where the decoder adopts a recursive sequence decoding way. However, such autoregressive decoding may result in sequential error accumulation and slow generation which limit the applications in practice. Non-autoregressive (NA) decoding has been proposed to cover these issues but suffers from language quality problem due to the indirect modeling of the target distribution. Towards that end, we propose an improved NA prediction framework to accelerate image captioning. Our decoding part consists of a position alignment to order the words that describe the content detected in the given image, and a fine non-autoregressive decoder to generate elegant descriptions. Furthermore, we introduce an inference strategy that regards position information as a latent variable to guide the further sentence generation. The Experimental results on public datasets show that our proposed model achieves better performance compared to general NA captioning models, while achieves comparable performance as autoregressive image captioning models with a significant speedup.



### Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1912.06378v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06378v3)
- **Published**: 2019-12-13 09:57:07+00:00
- **Updated**: 2020-06-05 02:18:55+00:00
- **Authors**: Xiaodong Gu, Zhiwen Fan, Zuozhuo Dai, Siyu Zhu, Feitong Tan, Ping Tan
- **Comment**: Accepted by CVPR2020 Oral
- **Journal**: None
- **Summary**: The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the output depth or disparity. These methods are limited when high-resolution outputs are needed since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a both memory and time efficient cost volume formulation that is complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a standard feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the depth (or disparity) map from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner.   We apply the cascade cost volume to the representative MVS-Net, and obtain a 23.1% improvement on DTU benchmark (1st place), with 50.6% and 74.2% reduction in GPU memory and run-time. It is also the state-of-the-art learning-based method on Tanks and Temples benchmark. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method.



### Neural Cages for Detail-Preserving 3D Deformations
- **Arxiv ID**: http://arxiv.org/abs/1912.06395v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06395v2)
- **Published**: 2019-12-13 10:25:00+00:00
- **Updated**: 2020-03-18 13:33:27+00:00
- **Authors**: Wang Yifan, Noam Aigerman, Vladimir G. Kim, Siddhartha Chaudhuri, Olga Sorkine-Hornung
- **Comment**: accepted for oral presentation at CVPR 2020, code available at
  https://github.com/yifita/deep_cage
- **Journal**: None
- **Summary**: We propose a novel learnable representation for detail-preserving shape deformation. The goal of our method is to warp a source shape to match the general structure of a target shape, while preserving the surface details of the source. Our method extends a traditional cage-based deformation technique, where the source shape is enclosed by a coarse control mesh termed \emph{cage}, and translations prescribed on the cage vertices are interpolated to any point on the source mesh via special weight functions. The use of this sparse cage scaffolding enables preserving surface details regardless of the shape's intricacy and topology. Our key contribution is a novel neural network architecture for predicting deformations by controlling the cage. We incorporate a differentiable cage-based deformation module in our architecture, and train our network end-to-end. Our method can be trained with common collections of 3D models in an unsupervised fashion, without any cage-specific annotations. We demonstrate the utility of our method for synthesizing shape variations and deformation transfer.



### Real-time texturing for 6D object instance detection from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1912.06404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06404v1)
- **Published**: 2019-12-13 10:54:25+00:00
- **Updated**: 2019-12-13 10:54:25+00:00
- **Authors**: Pavel Rojtberg, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: For objected detection, the availability of color cues strongly influences detection rates and is even a prerequisite for many methods. However, when training on synthetic CAD data, this information is not available. We therefore present a method for generating a texture-map from image sequences in real-time. The method relies on 6 degree-of-freedom poses and a 3D-model being available. In contrast to previous works this allows interleaving detection and texturing for upgrading the detector on-the-fly. Our evaluation shows that the acquired texture-map significantly improves detection rates using the LINEMOD detector on RGB images only. Additionally, we use the texture-map to differentiate instances of the same object by surface color.



### Multi-level Similarity Learning for Low-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.06418v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06418v1)
- **Published**: 2019-12-13 11:31:21+00:00
- **Updated**: 2019-12-13 11:31:21+00:00
- **Authors**: Hongwei Xv, Xin Sun, Junyu Dong, Shu Zhang, Qiong Li
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Low-shot learning indicates the ability to recognize unseen objects based on very limited labeled training samples, which simulates human visual intelligence. According to this concept, we propose a multi-level similarity model (MLSM) to capture the deep encoded distance metric between the support and query samples. Our approach is achieved based on the fact that the image similarity learning can be decomposed into image-level, global-level, and object-level. Once the similarity function is established, MLSM will be able to classify images for unseen classes by computing the similarity scores between a limited number of labeled samples and the target images. Furthermore, we conduct 5-way experiments with both 1-shot and 5-shot setting on Caltech-UCSD datasets. It is demonstrated that the proposed model can achieve promising results compared with the existing methods in practical applications.



### End-to-End Learning of Visual Representations from Uncurated Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.06430v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06430v4)
- **Published**: 2019-12-13 11:59:58+00:00
- **Updated**: 2020-08-23 12:20:59+00:00
- **Authors**: Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, Andrew Zisserman
- **Comment**: CVPR'2020 Oral
- **Journal**: None
- **Summary**: Annotating videos is cumbersome, expensive and not scalable. Yet, many strong video models still rely on manually annotated data. With the recent introduction of the HowTo100M dataset, narrated videos now offer the possibility of learning video representations without manual supervision. In this work we propose a new learning approach, MIL-NCE, capable of addressing misalignments inherent to narrated videos. With this approach we are able to learn strong video representations from scratch, without the need for any manual annotation. We evaluate our representations on a wide range of four downstream tasks over eight datasets: action recognition (HMDB-51, UCF-101, Kinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization (YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method outperforms all published self-supervised approaches for these tasks as well as several fully supervised baselines.



### Learning to Observe: Approximating Human Perceptual Thresholds for Detection of Suprathreshold Image Transformations
- **Arxiv ID**: http://arxiv.org/abs/1912.06433v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06433v3)
- **Published**: 2019-12-13 12:08:00+00:00
- **Updated**: 2020-06-15 12:33:21+00:00
- **Authors**: Alan Dolhasz, Carlo Harvey, Ian Williams
- **Comment**: 8 pages + references
- **Journal**: None
- **Summary**: Many tasks in computer vision are often calibrated and evaluated relative to human perception. In this paper, we propose to directly approximate the perceptual function performed by human observers completing a visual detection task. Specifically, we present a novel methodology for learning to detect image transformations visible to human observers through approximating perceptual thresholds. To do this, we carry out a subjective two-alternative forced-choice study to estimate perceptual thresholds of human observers detecting local exposure shifts in images. We then leverage transformation equivariant representation learning to overcome issues of limited perceptual data. This representation is then used to train a dense convolutional classifier capable of detecting local suprathreshold exposure shifts - a distortion common to image composites. In this context, our model can approximate perceptual thresholds with an average error of 0.1148 exposure stops between empirical and predicted thresholds. It can also be trained to detect a range of different local transformations.



### Crack Detection Using Enhanced Hierarchical Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12139v1)
- **Published**: 2019-12-13 12:35:00+00:00
- **Updated**: 2019-12-13 12:35:00+00:00
- **Authors**: Qiuchen Zhu, Manh Duong Phung, Quang Ha
- **Comment**: In Proceedings of Australasian Conference on Robotics and Automation
  2019 (ACRA), Adelaide, Australia
- **Journal**: None
- **Summary**: Unmanned aerial vehicles (UAV) are expected to replace human in hazardous tasks of surface inspection due to their flexibility in operating space and capability of collecting high quality visual data. In this study, we propose enhanced hierarchical convolutional neural networks (HCNN) to detect cracks from image data collected by UAVs. Unlike traditional HCNN, here a set of branch networks is utilised to reduce the obscuration in the down-sampling process. Moreover, the feature preserving blocks combine the current and previous terms from the convolutional blocks to provide input to the loss functions. As a result, the weights of resized images can be reduced to minimise the information loss. Experiments on images of different crack datasets have been carried out to demonstrate the effectiveness of proposed HCNN.



### PreVIous: A Methodology for Prediction of Visual Inference Performance on IoT Devices
- **Arxiv ID**: http://arxiv.org/abs/1912.06442v2
- **DOI**: 10.1109/JIOT.2020.2981684
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06442v2)
- **Published**: 2019-12-13 12:41:40+00:00
- **Updated**: 2020-03-17 08:08:46+00:00
- **Authors**: Delia Velasco-Montero, Jorge Fernández-Berni, Ricardo Carmona-Galán, Ángel Rodríguez-Vázquez
- **Comment**: 18 pages. 7 figures
- **Journal**: None
- **Summary**: This paper presents PreVIous, a methodology to predict the performance of convolutional neural networks (CNNs) in terms of throughput and energy consumption on vision-enabled devices for the Internet of Things. CNNs typically constitute a massive computational load for such devices, which are characterized by scarce hardware resources to be shared among multiple concurrent tasks. Therefore, it is critical to select the optimal CNN architecture for a particular hardware platform according to prescribed application requirements. However, the zoo of CNN models is already vast and rapidly growing. To facilitate a suitable selection, we introduce a prediction framework that allows to evaluate the performance of CNNs prior to their actual implementation. The proposed methodology is based on PreVIousNet, a neural network specifically designed to build accurate per-layer performance predictive models. PreVIousNet incorporates the most usual parameters found in state-of-the-art network architectures. The resulting predictive models for inference time and energy have been tested against comprehensive characterizations of seven well-known CNN models running on two different software frameworks and two different embedded platforms. To the best of our knowledge, this is the most extensive study in the literature concerning CNN performance prediction on low-power low-cost devices. The average deviation between predictions and real measurements is remarkably low, ranging from 3% to 10%. This means state-of-the-art modeling accuracy. As an additional asset, the fine-grained a priori analysis provided by PreVIous could also be exploited by neural architecture search engines.



### The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.06445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06445v3)
- **Published**: 2019-12-13 12:52:50+00:00
- **Updated**: 2020-03-28 14:44:12+00:00
- **Authors**: Junwei Liang, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann
- **Comment**: CVPR 2020. Code, models and dataset are available at:
  https://next.cs.cmu.edu/multiverse/index.html
- **Journal**: None
- **Summary**: This paper studies the problem of predicting the distribution over multiple possible future paths of people as they move through various visual scenes. We make two main contributions. The first contribution is a new dataset, created in a realistic 3D simulator, which is based on real world trajectory data, and then extrapolated by human annotators to achieve different latent goals. This provides the first benchmark for quantitative evaluation of the models to predict multi-future trajectories. The second contribution is a new model to generate multiple plausible future trajectories, which contains novel designs of using multi-scale location encodings and convolutional RNNs over graphs. We refer to our model as Multiverse. We show that our model achieves the best results on our dataset, as well as on the real-world VIRAT/ActEV dataset (which just contains one possible future).



### Fully-Convolutional Intensive Feature Flow Neural Network for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.06446v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06446v2)
- **Published**: 2019-12-13 12:54:19+00:00
- **Updated**: 2020-01-15 12:14:32+00:00
- **Authors**: Zhao Zhang, Zemin Tang, Zheng Zhang, Yang Wang, Jie Qin, Meng Wang
- **Comment**: Accepted by the 24th European Conference on Artificial Intelligence
  (ECAI 2020). arXiv admin note: text overlap with arXiv:1912.07016
- **Journal**: None
- **Summary**: The Deep Convolutional Neural Networks (CNNs) have obtained a great success for pattern recognition, such as recognizing the texts in images. But existing CNNs based frameworks still have several drawbacks: 1) the traditaional pooling operation may lose important feature information and is unlearnable; 2) the tradi-tional convolution operation optimizes slowly and the hierar-chical features from different layers are not fully utilized. In this work, we address these problems by developing a novel deep network model called Fully-Convolutional Intensive Feature Flow Neural Network (IntensiveNet). Specifically, we design a further dense block called intensive block to extract the feature information, where the original inputs and two dense blocks are connected tightly. To encode data appropriately, we present the concepts of dense fusion block and further dense fusion opera-tions for our new intensive block. By adding short connections to different layers, the feature flow and coupling between layers are enhanced. We also replace the traditional convolution by depthwise separable convolution to make the operation efficient. To prevent important feature information being lost to a certain extent, we use a convolution operation with stride 2 to replace the original pooling operation in the customary transition layers. The recognition results on large-scale Chinese string and MNIST datasets show that our IntensiveNet can deliver enhanced recog-nition results, compared with other related deep models.



### Towards Partial Supervision for Generic Object Counting in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1912.06448v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06448v2)
- **Published**: 2019-12-13 12:57:04+00:00
- **Updated**: 2020-09-03 09:07:48+00:00
- **Authors**: Hisham Cholakkal, Guolei Sun, Salman Khan, Fahad Shahbaz Khan, Ling Shao, Luc Van Gool
- **Comment**: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI), 2020. First two authors contributed equally.
  Corresponding author: Guolei Sun. This work is a journal extension of our
  CVPR 2019 paper arXiv:1903.02494
- **Journal**: None
- **Summary**: Generic object counting in natural scenes is a challenging computer vision problem. Existing approaches either rely on instance-level supervision or absolute count information to train a generic object counter. We introduce a partially supervised setting that significantly reduces the supervision level required for generic object counting. We propose two novel frameworks, named lower-count (LC) and reduced lower-count (RLC), to enable object counting under this setting. Our frameworks are built on a novel dual-branch architecture that has an image classification and a density branch. Our LC framework reduces the annotation cost due to multiple instances in an image by using only lower-count supervision for all object categories. Our RLC framework further reduces the annotation cost arising from large numbers of object categories in a dataset by only using lower-count supervision for a subset of categories and class-labels for the remaining ones. The RLC framework extends our dual-branch LC framework with a novel weight modulation layer and a category-independent density map prediction. Experiments are performed on COCO, Visual Genome and PASCAL 2007 datasets. Our frameworks perform on par with state-of-the-art approaches using higher levels of supervision. Additionally, we demonstrate the applicability of our LC supervised density map for image-level supervised instance segmentation.



### Solving Visual Object Ambiguities when Pointing: An Unsupervised Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1912.06449v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.06449v1)
- **Published**: 2019-12-13 12:57:33+00:00
- **Updated**: 2019-12-13 12:57:33+00:00
- **Authors**: Doreen Jirak, David Biertimpel, Matthias Kerzel, Stefan Wermter
- **Comment**: None
- **Journal**: None
- **Summary**: Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Especially pointing gestures are necessary to dissolve ambiguities in a scene and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domestic domains, the development of gesture interfaces complementing human-robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the corresponding object have to be correctly recognized in real-time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be captured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects, use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised learning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define so-called ambiguity classes. Our implementation for the hand and object detection is independent of any markers or skeleton models, thus it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities resulting from close object proximity.



### Multilayer Collaborative Low-Rank Coding Network for Robust Deep Subspace Discovery
- **Arxiv ID**: http://arxiv.org/abs/1912.06450v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06450v3)
- **Published**: 2019-12-13 12:57:57+00:00
- **Updated**: 2020-01-15 12:12:44+00:00
- **Authors**: Xianzhen Li, Zhao Zhang, Yang Wang, Guangcan Liu, Shuicheng Yan, Meng Wang
- **Comment**: Accepted by the 24th European Conference on Artificial Intelligence
  (ECAI 2020)
- **Journal**: None
- **Summary**: For subspace recovery, most existing low-rank representation (LRR) models performs in the original space in single-layer mode. As such, the deep hierarchical information cannot be learned, which may result in inaccurate recoveries for complex real data. In this paper, we explore the deep multi-subspace recovery problem by designing a multilayer architecture for latent LRR. Technically, we propose a new Multilayer Collabora-tive Low-Rank Representation Network model termed DeepLRR to discover deep features and deep subspaces. In each layer (>2), DeepLRR bilinearly reconstructs the data matrix by the collabo-rative representation with low-rank coefficients and projection matrices in the previous layer. The bilinear low-rank reconstruc-tion of previous layer is directly fed into the next layer as the input and low-rank dictionary for representation learning, and is further decomposed into a deep principal feature part, a deep salient feature part and a deep sparse error. As such, the coher-ence issue can be also resolved due to the low-rank dictionary, and the robustness against noise can also be enhanced in the feature subspace. To recover the sparse errors in layers accurately, a dynamic growing strategy is used, as the noise level will be-come smaller for the increase of layers. Besides, a neighborhood reconstruction error is also included to encode the locality of deep salient features by deep coefficients adaptively in each layer. Extensive results on public databases show that our DeepLRR outperforms other related models for subspace discovery and clustering.



### Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras
- **Arxiv ID**: http://arxiv.org/abs/1912.06464v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06464v1)
- **Published**: 2019-12-13 13:26:27+00:00
- **Updated**: 2019-12-13 13:26:27+00:00
- **Authors**: Levente Hajder, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: A new closed-form solver is proposed minimizing the algebraic error optimally, in the least-squares sense, to estimate the relative planar motion of two calibrated cameras. The main objective is to solve the over-determined case, i.e., when a larger-than-minimal sample of point correspondences is given - thus, estimating the motion from at least three correspondences. The algorithm requires the camera movement to be constrained to a plane, e.g. mounted to a vehicle, and the image plane to be orthogonal to the ground. The solver obtains the motion parameters as the roots of a 6-th degree polynomial. It is validated both in synthetic experiments and on publicly available real-world datasets that using the proposed solver leads to results superior to the state-of-the-art in terms of geometric accuracy with no noticeable deterioration in the processing time.



### Relative planar motion for vehicle-mounted cameras from a single affine correspondence
- **Arxiv ID**: http://arxiv.org/abs/1912.06465v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06465v1)
- **Published**: 2019-12-13 13:30:10+00:00
- **Updated**: 2019-12-13 13:30:10+00:00
- **Authors**: Levente Hajder, Daniel Barath
- **Comment**: None
- **Journal**: None
- **Summary**: Two solvers are proposed for estimating the extrinsic camera parameters from a single affine correspondence assuming general planar motion. In this case, the camera movement is constrained to a plane and the image plane is orthogonal to the ground. The algorithms do not assume other constraints, e.g.\ the non-holonomic one, to hold. A new minimal solver is proposed for the semi-calibrated case, i.e. the camera parameters are known except a common focal length. Another method is proposed for the fully calibrated case. Due to requiring a single correspondence, robust estimation, e.g. histogram voting, leads to a fast and accurate procedure. The proposed methods are tested in our synthetic environment and on publicly available real datasets consisting of videos through tens of kilometres. They are superior to the state-of-the-art both in terms of accuracy and processing time.



### Latent-Space Laplacian Pyramids for Adversarial Representation Learning with 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.06466v1
- **DOI**: 10.5220/0009102604210428
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06466v1)
- **Published**: 2019-12-13 13:32:28+00:00
- **Updated**: 2019-12-13 13:32:28+00:00
- **Authors**: Vage Egiazarian, Savva Ignatyev, Alexey Artemov, Oleg Voynov, Andrey Kravchenko, Youyi Zheng, Luiz Velho, Evgeny Burnaev
- **Comment**: None
- **Journal**: None
- **Summary**: Constructing high-quality generative models for 3D shapes is a fundamental task in computer vision with diverse applications in geometry processing, engineering, and design. Despite the recent progress in deep generative modelling, synthesis of finely detailed 3D surfaces, such as high-resolution point clouds, from scratch has not been achieved with existing approaches. In this work, we propose to employ the latent-space Laplacian pyramid representation within a hierarchical generative model for 3D point clouds. We combine the recently proposed latent-space GAN and Laplacian GAN architectures to form a multi-scale model capable of generating 3D point clouds at increasing levels of detail. Our evaluation demonstrates that our model outperforms the existing generative models for 3D point clouds.



### Inferring Super-Resolution Depth from a Moving Light-Source Enhanced RGB-D Sensor: A Variational Approach
- **Arxiv ID**: http://arxiv.org/abs/1912.06501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06501v1)
- **Published**: 2019-12-13 14:07:30+00:00
- **Updated**: 2019-12-13 14:07:30+00:00
- **Authors**: Lu Sang, Bjoern Haefner, Daniel Cremers
- **Comment**: WACV2020 conference paper
- **Journal**: None
- **Summary**: A novel approach towards depth map super-resolution using multi-view uncalibrated photometric stereo is presented. Practically, an LED light source is attached to a commodity RGB-D sensor and is used to capture objects from multiple viewpoints with unknown motion. This non-static camera-to-object setup is described with a nonconvex variational approach such that no calibration on lighting or camera motion is required due to the formulation of an end-to-end joint optimization problem. Solving the proposed variational model results in high resolution depth, reflectance and camera pose estimates, as we show on challenging synthetic and real-world datasets.



### That and There: Judging the Intent of Pointing Actions with Robotic Arms
- **Arxiv ID**: http://arxiv.org/abs/1912.06602v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1912.06602v1)
- **Published**: 2019-12-13 16:54:38+00:00
- **Updated**: 2019-12-13 16:54:38+00:00
- **Authors**: Malihe Alikhani, Baber Khalid, Rahul Shome, Chaitanya Mitash, Kostas Bekris, Matthew Stone
- **Comment**: Accepted to AAAI 2020, New York City
- **Journal**: None
- **Summary**: Collaborative robotics requires effective communication between a robot and a human partner. This work proposes a set of interpretive principles for how a robotic arm can use pointing actions to communicate task information to people by extending existing models from the related literature. These principles are evaluated through studies where English-speaking human subjects view animations of simulated robots instructing pick-and-place tasks. The evaluation distinguishes two classes of pointing actions that arise in pick-and-place tasks: referential pointing (identifying objects) and locating pointing (identifying locations). The study indicates that human subjects show greater flexibility in interpreting the intent of referential pointing compared to locating pointing, which needs to be more deliberate. The results also demonstrate the effects of variation in the environment and task context on the interpretation of pointing. Our corpus, experiments and design principles advance models of context, common sense reasoning and communication in embodied communication.



### Music-oriented Dance Video Synthesis with Pose Perceptual Loss
- **Arxiv ID**: http://arxiv.org/abs/1912.06606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06606v1)
- **Published**: 2019-12-13 17:01:21+00:00
- **Updated**: 2019-12-13 17:01:21+00:00
- **Authors**: Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based approach with pose perceptual loss for automatic music video generation. Our method can produce a realistic dance video that conforms to the beats and rhymes of almost any given music. To achieve this, we firstly generate a human skeleton sequence from music and then apply the learned pose-to-appearance mapping to generate the final video. In the stage of generating skeleton sequences, we utilize two discriminators to capture different aspects of the sequence and propose a novel pose perceptual loss to produce natural dances. Besides, we also provide a new cross-modal evaluation to evaluate the dance quality, which is able to estimate the similarity between two modalities of music and dance. Finally, a user study is conducted to demonstrate that dance video synthesized by the presented approach produces surprisingly realistic results. The results are shown in the supplementary video at https://youtu.be/0rMuFMZa_K4



### Seeing Around Street Corners: Non-Line-of-Sight Detection and Tracking In-the-Wild Using Doppler Radar
- **Arxiv ID**: http://arxiv.org/abs/1912.06613v2
- **DOI**: 10.1109/CVPR42600.2020.00214
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06613v2)
- **Published**: 2019-12-13 17:23:29+00:00
- **Updated**: 2020-03-31 09:25:11+00:00
- **Authors**: Nicolas Scheiner, Florian Kraus, Fangyin Wei, Buu Phan, Fahim Mannan, Nils Appenrodt, Werner Ritter, Jürgen Dickmann, Klaus Dietmayer, Bernhard Sick, Felix Heide
- **Comment**: First three authors contributed equally; Accepted at CVPR 2020
- **Journal**: Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR), 2020, pp. 2068-2077
- **Summary**: Conventional sensor systems record information about directly visible objects, whereas occluded scene components are considered lost in the measurement process. Non-line-of-sight (NLOS) methods try to recover such hidden objects from their indirect reflections - faint signal components, traditionally treated as measurement noise. Existing NLOS approaches struggle to record these low-signal components outside the lab, and do not scale to large-scale outdoor scenes and high-speed motion, typical in automotive scenarios. In particular, optical NLOS capture is fundamentally limited by the quartic intensity falloff of diffuse indirect reflections. In this work, we depart from visible-wavelength approaches and demonstrate detection, classification, and tracking of hidden objects in large-scale dynamic environments using Doppler radars that can be manufactured at low-cost in series production. To untangle noisy indirect and direct reflections, we learn from temporal sequences of Doppler velocity and position measurements, which we fuse in a joint NLOS detection and tracking network over time. We validate the approach on in-the-wild automotive scenes, including sequences of parked cars or house facades as relay surfaces, and demonstrate low-cost, real-time NLOS in dynamic automotive environments.



### Action Modifiers: Learning from Adverbs in Instructional Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.06617v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06617v3)
- **Published**: 2019-12-13 17:27:25+00:00
- **Updated**: 2020-03-24 16:40:21+00:00
- **Authors**: Hazel Doughty, Ivan Laptev, Walterio Mayol-Cuevas, Dima Damen
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We present a method to learn a representation for adverbs from instructional videos using weak supervision from the accompanying narrations. Key to our method is the fact that the visual representation of the adverb is highly dependant on the action to which it applies, although the same adverb will modify multiple actions in a similar way. For instance, while 'spread quickly' and 'mix quickly' will look dissimilar, we can learn a common representation that allows us to recognize both, among other actions. We formulate this as an embedding problem, and use scaled dot-product attention to learn from weakly-supervised video narrations. We jointly learn adverbs as invertible transformations operating on the embedding space, so as to add or remove the effect of the adverb. As there is no prior work on weakly supervised learning from adverbs, we gather paired action-adverb annotations from a subset of the HowTo100M dataset for 6 adverbs: quickly/slowly, finely/coarsely, and partially/completely. Our method outperforms all baselines for video-to-adverb retrieval with a performance of 0.719 mAP. We also demonstrate our model's ability to attend to the relevant video parts in order to determine the adverb for a given action.



### SPIN: A High Speed, High Resolution Vision Dataset for Tracking and Action Recognition in Ping Pong
- **Arxiv ID**: http://arxiv.org/abs/1912.06640v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.06640v1)
- **Published**: 2019-12-13 18:30:29+00:00
- **Updated**: 2019-12-13 18:30:29+00:00
- **Authors**: Steven Schwarcz, Peng Xu, David D'Ambrosio, Juhana Kangaspunta, Anelia Angelova, Huong Phan, Navdeep Jaitly
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new high resolution, high frame rate stereo video dataset, which we call SPIN, for tracking and action recognition in the game of ping pong. The corpus consists of ping pong play with three main annotation streams that can be used to learn tracking and action recognition models -- tracking of the ping pong ball and poses of humans in the videos and the spin of the ball being hit by humans. The training corpus consists of 53 hours of data with labels derived from previous models in a semi-supervised method. The testing corpus contains 1 hour of data with the same information, except that crowd compute was used to obtain human annotations of the ball position, from which ball spin has been derived. Along with the dataset we introduce several baseline models that were trained on this data. The models were specifically chosen to be able to perform inference at the same rate as the images are generated -- specifically 150 fps. We explore the advantages of multi-task training on this data, and also show interesting properties of ping pong ball trajectories that are derived from our observational data, rather than from prior physics models. To our knowledge this is the first large scale dataset of ping pong; we offer it to the community as a rich dataset that can be used for a large variety of machine learning and vision tasks such as tracking, pose estimation, semi-supervised and unsupervised learning and generative modeling.



### Towards Contextual Learning in Few-shot Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.06679v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06679v3)
- **Published**: 2019-12-13 19:56:39+00:00
- **Updated**: 2021-01-04 18:11:31+00:00
- **Authors**: Mathieu Pagé Fortin, Brahim Chaib-draa
- **Comment**: Accepted to WACV 2021
- **Journal**: None
- **Summary**: Few-shot Learning (FSL) aims to classify new concepts from a small number of examples. While there have been an increasing amount of work on few-shot object classification in the last few years, most current approaches are limited to images with only one centered object. On the opposite, humans are able to leverage prior knowledge to quickly learn new concepts, such as semantic relations with contextual elements. Inspired by the concept of contextual learning in educational sciences, we propose to make a step towards adopting this principle in FSL by studying the contribution that context can have in object classification in a low-data regime. To this end, we first propose an approach to perform FSL on images of complex scenes. We develop two plug-and-play modules that can be incorporated into existing FSL methods to enable them to leverage contextual learning. More specifically, these modules are trained to weight the most important context elements while learning a particular concept, and then use this knowledge to ground visual class representations in context semantics. Extensive experiments on Visual Genome and Open Images show the superiority of contextual learning over learning individual objects in isolation.



### LiteSeg: A Novel Lightweight ConvNet for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.06683v1
- **DOI**: 10.1109/DICTA47822.2019.8945975
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06683v1)
- **Published**: 2019-12-13 20:05:52+00:00
- **Updated**: 2019-12-13 20:05:52+00:00
- **Authors**: Taha Emara, Hossam E. Abd El Munim, Hazem M. Abbas
- **Comment**: Accepted, DICTA 2019
- **Journal**: None
- **Summary**: Semantic image segmentation plays a pivotal role in many vision applications including autonomous driving and medical image analysis. Most of the former approaches move towards enhancing the performance in terms of accuracy with a little awareness of computational efficiency. In this paper, we introduce LiteSeg, a lightweight architecture for semantic image segmentation. In this work, we explore a new deeper version of Atrous Spatial Pyramid Pooling module (ASPP) and apply short and long residual connections, and depthwise separable convolution, resulting in a faster and efficient model. LiteSeg architecture is introduced and tested with multiple backbone networks as Darknet19, MobileNet, and ShuffleNet to provide multiple trade-offs between accuracy and computational cost. The proposed model LiteSeg, with MobileNetV2 as a backbone network, achieves an accuracy of 67.81% mean intersection over union at 161 frames per second with $640 \times 360$ resolution on the Cityscapes dataset.



### Systematic Misestimation of Machine Learning Performance in Neuroimaging Studies of Depression
- **Arxiv ID**: http://arxiv.org/abs/1912.06686v2
- **DOI**: 10.1038/s41386-021-01020-7
- **Categories**: **q-bio.NC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06686v2)
- **Published**: 2019-12-13 20:12:52+00:00
- **Updated**: 2021-05-03 15:10:35+00:00
- **Authors**: Claas Flint, Micah Cearns, Nils Opel, Ronny Redlich, David M. A. Mehler, Daniel Emden, Nils R. Winter, Ramona Leenings, Simon B. Eickhoff, Tilo Kircher, Axel Krug, Igor Nenadic, Volker Arolt, Scott Clark, Bernhard T. Baune, Xiaoyi Jiang, Udo Dannlowski, Tim Hahn
- **Comment**: None
- **Journal**: Neuropsychopharmacology 46 (2021) 1510-1517
- **Summary**: We currently observe a disconcerting phenomenon in machine learning studies in psychiatry: While we would expect larger samples to yield better results due to the availability of more data, larger machine learning studies consistently show much weaker performance than the numerous small-scale studies. Here, we systematically investigated this effect focusing on one of the most heavily studied questions in the field, namely the classification of patients suffering from major depressive disorder (MDD) and healthy control (HC) based on neuroimaging data. Drawing upon structural magnetic resonance imaging (MRI) data from a balanced sample of $N = 1,868$ MDD patients and HC from our recent international Predictive Analytics Competition (PAC), we first trained and tested a classification model on the full dataset which yielded an accuracy of $61\,\%$. Next, we mimicked the process by which researchers would draw samples of various sizes ($N = 4$ to $N = 150$) from the population and showed a strong risk of misestimation. Specifically, for small sample sizes ($N = 20$), we observe accuracies of up to $95\,\%$. For medium sample sizes ($N = 100$) accuracies up to $75\,\%$ were found. Importantly, further investigation showed that sufficiently large test sets effectively protect against performance misestimation whereas larger datasets per se do not. While these results question the validity of a substantial part of the current literature, we outline the relatively low-cost remedy of larger test sets, which is readily available in most cases.



### Unsupervised and Generic Short-Term Anticipation of Human Body Motions
- **Arxiv ID**: http://arxiv.org/abs/1912.06688v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.06688v1)
- **Published**: 2019-12-13 20:13:36+00:00
- **Updated**: 2019-12-13 20:13:36+00:00
- **Authors**: Kristina Enes, Hassan Errami, Moritz Wolter, Tim Krake, Bernhard Eberhardt, Andreas Weber, Jörg Zimmermann
- **Comment**: None
- **Journal**: None
- **Summary**: Various neural network based methods are capable of anticipating human body motions from data for a short period of time. What these methods lack are the interpretability and explainability of the network and its results. We propose to use Dynamic Mode Decomposition with delays to represent and anticipate human body motions. Exploring the influence of the number of delays on the reconstruction and prediction of various motion classes, we show that the anticipation errors in our results are comparable or even better for very short anticipation times ($<0.4$ sec) to a recurrent neural network based method. We perceive our method as a first step towards the interpretability of the results by representing human body motions as linear combinations of ``factors''. In addition, compared to the neural network based methods large training times are not needed. Actually, our methods do not even regress to any other motions than the one to be anticipated and hence is of a generic nature.



### ViBE: Dressing for Diverse Body Shapes
- **Arxiv ID**: http://arxiv.org/abs/1912.06697v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.06697v2)
- **Published**: 2019-12-13 20:39:22+00:00
- **Updated**: 2020-03-28 23:25:45+00:00
- **Authors**: Wei-Lin Hsiao, Kristen Grauman
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Body shape plays an important role in determining what garments will best suit a given person, yet today's clothing recommendation methods take a "one shape fits all" approach. These body-agnostic vision methods and datasets are a barrier to inclusion, ill-equipped to provide good suggestions for diverse body shapes. We introduce ViBE, a VIsual Body-aware Embedding that captures clothing's affinity with different body shapes. Given an image of a person, the proposed embedding identifies garments that will flatter her specific body shape. We show how to learn the embedding from an online catalog displaying fashion models of various shapes and sizes wearing the products, and we devise a method to explain the algorithm's suggestions for well-fitting garments. We apply our approach to a dataset of diverse subjects, and demonstrate its strong advantages over the status quo body-agnostic recommendation, both according to automated metrics and human opinion.



### Hierarchical Deep Stereo Matching on High-resolution Images
- **Arxiv ID**: http://arxiv.org/abs/1912.06704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.06704v1)
- **Published**: 2019-12-13 20:57:15+00:00
- **Updated**: 2019-12-13 20:57:15+00:00
- **Authors**: Gengshan Yang, Joshua Manela, Michael Happold, Deva Ramanan
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We explore the problem of real-time stereo matching on high-res imagery. Many state-of-the-art (SOTA) methods struggle to process high-res imagery because of memory constraints or speed limitations. To address this issue, we propose an end-to-end framework that searches for correspondences incrementally over a coarse-to-fine hierarchy. Because high-res stereo datasets are relatively rare, we introduce a dataset with high-res stereo pairs for both training and evaluation. Our approach achieved SOTA performance on Middlebury-v3 and KITTI-15 while running significantly faster than its competitors. The hierarchical design also naturally allows for anytime on-demand reports of disparity by capping intermediate coarse results, allowing us to accurately predict disparity for near-range structures with low latency (30ms). We demonstrate that the performance-vs-speed trade-off afforded by on-demand hierarchies may address sensing needs for time-critical applications such as autonomous driving.



### Keyhole Imaging: Non-Line-of-Sight Imaging and Tracking of Moving Objects Along a Single Optical Path
- **Arxiv ID**: http://arxiv.org/abs/1912.06727v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1912.06727v3)
- **Published**: 2019-12-13 22:05:18+00:00
- **Updated**: 2021-01-05 18:22:14+00:00
- **Authors**: Christopher A. Metzler, David B. Lindell, Gordon Wetzstein
- **Comment**: None
- **Journal**: None
- **Summary**: Non-line-of-sight (NLOS) imaging and tracking is an emerging technology that allows the shape or position of objects around corners or behind diffusers to be recovered from transient, time-of-flight measurements. However, existing NLOS approaches require the imaging system to scan a large area on a visible surface, where the indirect light paths of hidden objects are sampled. In many applications, such as robotic vision or autonomous driving, optical access to a large scanning area may not be available, which severely limits the practicality of existing NLOS techniques. Here, we propose a new approach, dubbed keyhole imaging, that captures a sequence of transient measurements along a single optical path, for example, through a keyhole. Assuming that the hidden object of interest moves during the acquisition time, we effectively capture a series of time-resolved projections of the object's shape from unknown viewpoints. We derive inverse methods based on expectation-maximization to recover the object's shape and location using these measurements. Then, with the help of long exposure times and retroreflective tape, we demonstrate successful experimental results with a prototype keyhole imaging system.



### Laguerre-Gauss Preprocessing: Line Profiles as Image Features for Aerial Images Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.06729v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06729v1)
- **Published**: 2019-12-13 22:21:26+00:00
- **Updated**: 2019-12-13 22:21:26+00:00
- **Authors**: Alejandro Murillo-González, José David Ortega Pabón, Juan Guillermo Paniagua, Olga Lucía Quintero Montoya
- **Comment**: None
- **Journal**: None
- **Summary**: An image preprocessing methodology based on Fourier analysis together with the Laguerre-Gauss Spatial Filter is proposed. This is an alternative to obtain features from aerial images that reduces the feature space significantly, preserving enough information for classification tasks. Experiments on a challenging data set of aerial images show that it is possible to learn a robust classifier from this transformed and smaller feature space using simple models, with similar performance to the complete feature space and more complex models.



