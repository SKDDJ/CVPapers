# Arxiv Papers in cs.CV on 2019-12-09
### Patch Aggregator for Scene Text Script Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.03818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03818v1)
- **Published**: 2019-12-09 02:17:04+00:00
- **Updated**: 2019-12-09 02:17:04+00:00
- **Authors**: Changxu Cheng, Qiuhui Huang, Xiang Bai, Bin Feng, Wenyu Liu
- **Comment**: Accepted as ICDAR2019 oral
- **Journal**: None
- **Summary**: Script identification in the wild is of great importance in a multi-lingual robust-reading system. The scripts deriving from the same language family share a large set of characters, which makes script identification a fine-grained classification problem. Most existing methods make efforts to learn a single representation that combines the local features by making a weighted average or other clustering methods, which may reduce the discriminatory power of some important parts in each script for the interference of redundant features. In this paper, we present a novel module named Patch Aggregator (PA), which learns a more discriminative representation for script identification by taking into account the prediction scores of local patches. Specifically, we design a CNN-based method consisting of a standard CNN classifier and a PA module. Experiments demonstrate that the proposed PA module brings significant performance improvements over the baseline CNN model, achieving the state-of-the-art results on three benchmark datasets for script identification: SIW-13, CVSI 2015 and RRC-MLT 2017.



### Deep Efficient End-to-end Reconstruction (DEER) Network for Few-view Breast CT Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.04278v3
- **DOI**: 10.1109/ACCESS.2020.3033795
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04278v3)
- **Published**: 2019-12-09 02:44:24+00:00
- **Updated**: 2020-11-03 21:00:05+00:00
- **Authors**: Huidong Xie, Hongming Shan, Wenxiang Cong, Chi Liu, Xiaohua Zhang, Shaohua Liu, Ruola Ning, Ge Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Breast CT provides image volumes with isotropic resolution in high contrast, enabling detection of small calcification (down to a few hundred microns in size) and subtle density differences. Since breast is sensitive to x-ray radiation, dose reduction of breast CT is an important topic, and for this purpose, few-view scanning is a main approach. In this article, we propose a Deep Efficient End-to-end Reconstruction (DEER) network for few-view breast CT image reconstruction. The major merits of our network include high dose efficiency, excellent image quality, and low model complexity. By the design, the proposed network can learn the reconstruction process with as few as O(N) parameters, where N is the side length of an image to be reconstructed, which represents orders of magnitude improvements relative to the state-of-the-art deep-learning-based reconstruction methods that map raw data to tomographic images directly. Also, validated on a cone-beam breast CT dataset prepared by Koning Corporation on a commercial scanner, our method demonstrates a competitive performance over the state-of-the-art reconstruction networks in terms of image quality. The source code of this paper is available at: https://github.com/HuidongXie/DEER.



### LiDAR Iris for Loop-Closure Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.03825v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03825v3)
- **Published**: 2019-12-09 03:04:00+00:00
- **Updated**: 2020-07-02 11:26:42+00:00
- **Authors**: Ying Wang, Zezhou Sun, Cheng-Zhong Xu, Sanjay Sarma, Jian Yang, Hui Kong
- **Comment**: IROS 2020
- **Journal**: None
- **Summary**: In this paper, a global descriptor for a LiDAR point cloud, called LiDAR Iris, is proposed for fast and accurate loop-closure detection. A binary signature image can be obtained for each point cloud after several LoG-Gabor filtering and thresholding operations on the LiDAR-Iris image representation. Given two point clouds, their similarities can be calculated as the Hamming distance of two corresponding binary signature images extracted from the two point clouds, respectively. Our LiDAR-Iris method can achieve a pose-invariant loop-closure detection at a descriptor level with the Fourier transform of the LiDAR-Iris representation if assuming a 3D (x,y,yaw) pose space, although our method can generally be applied to a 6D pose space by re-aligning point clouds with an additional IMU sensor. Experimental results on five road-scene sequences demonstrate its excellent performance in loop-closure detection.



### Amora: Black-box Adversarial Morphing Attack
- **Arxiv ID**: http://arxiv.org/abs/1912.03829v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1912.03829v5)
- **Published**: 2019-12-09 03:23:36+00:00
- **Updated**: 2020-08-15 13:29:53+00:00
- **Authors**: Run Wang, Felix Juefei-Xu, Qing Guo, Yihao Huang, Xiaofei Xie, Lei Ma, Yang Liu
- **Comment**: Accepted by ACM MM'20
- **Journal**: None
- **Summary**: Nowadays, digital facial content manipulation has become ubiquitous and realistic with the success of generative adversarial networks (GANs), making face recognition (FR) systems suffer from unprecedented security concerns. In this paper, we investigate and introduce a new type of adversarial attack to evade FR systems by manipulating facial content, called \textbf{\underline{a}dversarial \underline{mor}phing \underline{a}ttack} (a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel intensity values by adding human-imperceptible noise, our proposed adversarial morphing attack works at the semantic level that perturbs pixels spatially in a coherent manner. To tackle the black-box attack problem, we devise a simple yet effective joint dictionary learning pipeline to obtain a proprietary optical flow field for each attack. Our extensive evaluation on two popular FR systems demonstrates the effectiveness of our adversarial morphing attack at various levels of morphing intensity with smiling facial expression manipulations. Both open-set and closed-set experimental results indicate that a novel black-box adversarial attack based on local deformation is possible, and is vastly different from additive noise attacks. The findings of this work potentially pave a new research direction towards a more thorough understanding and investigation of image-based adversarial attacks and defenses.



### Selective Synthetic Augmentation with Quality Assurance
- **Arxiv ID**: http://arxiv.org/abs/1912.03837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03837v1)
- **Published**: 2019-12-09 04:08:14+00:00
- **Updated**: 2019-12-09 04:08:14+00:00
- **Authors**: Yuan Xue, Jiarong Ye, Rodney Long, Sameer Antani, Zhiyun Xue, Xiaolei Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised training of an automated medical image analysis system often requires a large amount of expert annotations that are hard to collect. Moreover, the proportions of data available across different classes may be highly imbalanced for rare diseases. To mitigate these issues, we investigate a novel data augmentation pipeline that selectively adds new synthetic images generated by conditional Adversarial Networks (cGANs), rather than extending directly the training set with synthetic images. The selection mechanisms that we introduce to the synthetic augmentation pipeline are motivated by the observation that, although cGAN-generated images can be visually appealing, they are not guaranteed to contain essential features for classification performance improvement. By selecting synthetic images based on the confidence of their assigned labels and their feature similarity to real labeled images, our framework provides quality assurance to synthetic augmentation by ensuring that adding the selected synthetic images to the training set will improve performance. We evaluate our model on a medical histopathology dataset, and two natural image classification benchmarks, CIFAR10 and SVHN. Results on these datasets show significant and consistent improvements in classification performance (with 6.8%, 3.9%, 1.6% higher accuracy, respectively) by leveraging cGAN generated images with selective augmentation.



### Neural Wireframe Renderer: Learning Wireframe to Image Translations
- **Arxiv ID**: http://arxiv.org/abs/1912.03840v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03840v2)
- **Published**: 2019-12-09 04:17:37+00:00
- **Updated**: 2020-07-17 04:57:26+00:00
- **Authors**: Yuan Xue, Zihan Zhou, Xiaolei Huang
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: In architecture and computer-aided design, wireframes (i.e., line-based models) are widely used as basic 3D models for design evaluation and fast design iterations. However, unlike a full design file, a wireframe model lacks critical information, such as detailed shape, texture, and materials, needed by a conventional renderer to produce 2D renderings of the objects or scenes. In this paper, we bridge the information gap by generating photo-realistic rendering of indoor scenes from wireframe models in an image translation framework. While existing image synthesis methods can generate visually pleasing images for common objects such as faces and birds, these methods do not explicitly model and preserve essential structural constraints in a wireframe model, such as junctions, parallel lines, and planar surfaces. To this end, we propose a novel model based on a structure-appearance joint representation learned from both images and wireframes. In our model, structural constraints are explicitly enforced by learning a joint representation in a shared encoder network that must support the generation of both images and wireframes. Experiments on a wireframe-scene dataset show that our wireframe-to-image translation model significantly outperforms the state-of-the-art methods in both visual quality and structural integrity of generated images.



### Shape-Aware Organ Segmentation by Predicting Signed Distance Maps
- **Arxiv ID**: http://arxiv.org/abs/1912.03849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03849v1)
- **Published**: 2019-12-09 04:52:24+00:00
- **Updated**: 2019-12-09 04:52:24+00:00
- **Authors**: Yuan Xue, Hui Tang, Zhi Qiao, Guanzhong Gong, Yong Yin, Zhen Qian, Chao Huang, Wei Fan, Xiaolei Huang
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: In this work, we propose to resolve the issue existing in current deep learning based organ segmentation systems that they often produce results that do not capture the overall shape of the target organ and often lack smoothness. Since there is a rigorous mapping between the Signed Distance Map (SDM) calculated from object boundary contours and the binary segmentation map, we exploit the feasibility of learning the SDM directly from medical scans. By converting the segmentation task into predicting an SDM, we show that our proposed method retains superior segmentation performance and has better smoothness and continuity in shape. To leverage the complementary information in traditional segmentation training, we introduce an approximated Heaviside function to train the model by predicting SDMs and segmentation maps simultaneously. We validate our proposed models by conducting extensive experiments on a hippocampus segmentation dataset and the public MICCAI 2015 Head and Neck Auto Segmentation Challenge dataset with multiple organs. While our carefully designed backbone 3D segmentation network improves the Dice coefficient by more than 5% compared to current state-of-the-arts, the proposed model with SDM learning produces smoother segmentation results with smaller Hausdorff distance and average surface distance, thus proving the effectiveness of our method.



### Bundle Adjustment Revisited
- **Arxiv ID**: http://arxiv.org/abs/1912.03858v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03858v1)
- **Published**: 2019-12-09 05:47:19+00:00
- **Updated**: 2019-12-09 05:47:19+00:00
- **Authors**: Yu Chen, Yisong Chen, Guoping Wang
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: 3D reconstruction has been developing all these two decades, from moderate to medium size and to large scale. It's well known that bundle adjustment plays an important role in 3D reconstruction, mainly in Structure from Motion(SfM) and Simultaneously Localization and Mapping(SLAM). While bundle adjustment optimizes camera parameters and 3D points as a non-negligible final step, it suffers from memory and efficiency requirements in very large scale reconstruction. In this paper, we study the development of bundle adjustment elaborately in both conventional and distributed approaches. The detailed derivation and pseudo code are also given in this paper.



### Learning a Layout Transfer Network for Context Aware Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.03865v1
- **DOI**: 10.1109/TITS.2019.2939213
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03865v1)
- **Published**: 2019-12-09 06:07:44+00:00
- **Updated**: 2019-12-09 06:07:44+00:00
- **Authors**: Tao Wang, Xuming He, Yuanzheng Cai, Guobao Xiao
- **Comment**: Paper accepted by the IEEE Transactions on Intelligent Transportation
  Systems
- **Journal**: None
- **Summary**: We present a context aware object detection method based on a retrieve-and-transform scene layout model. Given an input image, our approach first retrieves a coarse scene layout from a codebook of typical layout templates. In order to handle large layout variations, we use a variant of the spatial transformer network to transform and refine the retrieved layout, resulting in a set of interpretable and semantically meaningful feature maps of object locations and scales. The above steps are implemented as a Layout Transfer Network which we integrate into Faster RCNN to allow for joint reasoning of object detection and scene layout estimation. Extensive experiments on three public datasets verified that our approach provides consistent performance improvements to the state-of-the-art object detection baselines on a variety of challenging tasks in the traffic surveillance and the autonomous driving domains.



### CNN-based Lidar Point Cloud De-Noising in Adverse Weather
- **Arxiv ID**: http://arxiv.org/abs/1912.03874v2
- **DOI**: 10.1109/LRA.2020.2972865
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.03874v2)
- **Published**: 2019-12-09 07:00:33+00:00
- **Updated**: 2020-02-12 05:32:07+00:00
- **Authors**: Robin Heinzler, Florian Piewak, Philipp Schindler, Wilhelm Stork
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters (2020)
- **Summary**: Lidar sensors are frequently used in environment perception for autonomous vehicles and mobile robotics to complement camera, radar, and ultrasonic sensors. Adverse weather conditions are significantly impacting the performance of lidar-based scene understanding by causing undesired measurement points that in turn effect missing detections and false positives. In heavy rain or dense fog, water drops could be misinterpreted as objects in front of the vehicle which brings a mobile robot to a full stop. In this paper, we present the first CNN-based approach to understand and filter out such adverse weather effects in point cloud data. Using a large data set obtained in controlled weather environments, we demonstrate a significant performance improvement of our method over state-of-the-art involving geometric filtering. Data is available at https://github.com/rheinzler/PointCloudDeNoising.



### Bi-Semantic Reconstructing Generative Network for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03877v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03877v3)
- **Published**: 2019-12-09 07:12:18+00:00
- **Updated**: 2020-01-05 16:47:28+00:00
- **Authors**: Shibing Xu, Zishu Gao, Guojun Xie
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Many recent methods of zero-shot learning (ZSL) attempt to utilize generative model to generate the unseen visual samples from semantic descriptions and random noise. Therefore, the ZSL problem becomes a traditional supervised classification problem. However, most of the existing methods based on the generative model only focus on the quality of synthesized samples at the training stage, and ignore the importance of the zero-shot recognition stage. In this paper, we consider both the above two points and propose a novel approach. Specially, we select the Generative Adversarial Network (GAN) as our generative model. In order to improve the quality of synthesized samples, considering the internal relation of the semantic description in the semantic space as well as the fact that the seen and unseen visual information belong to different domains, we propose a bi-semantic reconstructing (BSR) component which contain two different semantic reconstructing regressors to lead the training of GAN. Since the semantic descriptions are available during the training stage, to further improve the ability of classifier, we combine the visual samples and semantic descriptions to train a classifier. At the recognition stage, we naturally utilize the BSR component to transfer the visual features and semantic descriptions, and concatenate them for classification. Experimental results show that our method outperforms the state of the art on several ZSL benchmark datasets with significant improvements.



### AI2D-RST: A multimodal corpus of 1000 primary school science diagrams
- **Arxiv ID**: http://arxiv.org/abs/1912.03879v2
- **DOI**: 10.1007/s10579-020-09517-1
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03879v2)
- **Published**: 2019-12-09 07:22:54+00:00
- **Updated**: 2020-03-20 10:03:17+00:00
- **Authors**: Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, John A. Bateman
- **Comment**: 24 pages; revised version submitted to Language Resources &
  Evaluation
- **Journal**: Language Resources and Evaluation 55(3), 2021, pp. 661-688
- **Summary**: This article introduces AI2D-RST, a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelligence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. Building on the segmentation of diagram layouts in AI2D, the AI2D-RST corpus presents a new multi-layer annotation schema that provides a rich description of their multimodal structure. Annotated by trained experts, the layers describe (1) the grouping of diagram elements into perceptual units, (2) the connections set up by diagrammatic elements such as arrows and lines, and (3) the discourse relations between diagram elements, which are described using Rhetorical Structure Theory (RST). Each annotation layer in AI2D-RST is represented using a graph. The corpus is freely available for research and teaching.



### Video Motion Capture from the Part Confidence Maps of Multi-Camera Images by Spatiotemporal Filtering Using the Human Skeletal Model
- **Arxiv ID**: http://arxiv.org/abs/1912.03880v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03880v2)
- **Published**: 2019-12-09 07:25:33+00:00
- **Updated**: 2019-12-10 07:56:50+00:00
- **Authors**: Takuya Ohashi, Yosuke Ikegami, Kazuki Yamamoto, Wataru Takano, Yoshihiko Nakamura
- **Comment**: International Conference on Intelligent Robots and Systems (IROS),
  2018
- **Journal**: None
- **Summary**: This paper discusses video motion capture, namely, 3D reconstruction of human motion from multi-camera images. After the Part Confidence Maps are computed from each camera image, the proposed spatiotemporal filter is applied to deliver the human motion data with accuracy and smoothness for human motion analysis. The spatiotemporal filter uses the human skeleton and mixes temporal smoothing in two-time inverse kinematics computations. The experimental results show that the mean per joint position error was 26.1mm for regular motions and 38.8mm for inverted motions.



### Efficient Object Detection in Large Images using Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03966v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03966v2)
- **Published**: 2019-12-09 11:10:55+00:00
- **Updated**: 2020-04-06 21:01:37+00:00
- **Authors**: Burak Uzkent, Christopher Yeh, Stefano Ermon
- **Comment**: Published in WACV 2020
- **Journal**: None
- **Summary**: Traditionally, an object detector is applied to every part of the scene of interest, and its accuracy and computational cost increases with higher resolution images. However, in some application domains such as remote sensing, purchasing high spatial resolution images is expensive. To reduce the large computational and monetary cost associated with using high spatial resolution images, we propose a reinforcement learning agent that adaptively selects the spatial resolution of each image that is provided to the detector. In particular, we train the agent in a dual reward setting to choose low spatial resolution images to be run through a coarse level detector when the image is dominated by large objects, and high spatial resolution images to be run through a fine level detector when it is dominated by small objects. This reduces the dependency on high spatial resolution images for building a robust detector and increases run-time efficiency. We perform experiments on the xView dataset, consisting of large images, where we increase run-time efficiency by 50% and use high resolution images only 30% of the time while maintaining similar accuracy as a detector that uses only high resolution images.



### InfoCNF: An Efficient Conditional Continuous Normalizing Flow with Adaptive Solvers
- **Arxiv ID**: http://arxiv.org/abs/1912.03978v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03978v1)
- **Published**: 2019-12-09 11:37:22+00:00
- **Updated**: 2019-12-09 11:37:22+00:00
- **Authors**: Tan M. Nguyen, Animesh Garg, Richard G. Baraniuk, Anima Anandkumar
- **Comment**: 17 pages, 14 figures, 2 tables
- **Journal**: None
- **Summary**: Continuous Normalizing Flows (CNFs) have emerged as promising deep generative models for a wide range of tasks thanks to their invertibility and exact likelihood estimation. However, conditioning CNFs on signals of interest for conditional image generation and downstream predictive tasks is inefficient due to the high-dimensional latent code generated by the model, which needs to be of the same size as the input data. In this paper, we propose InfoCNF, an efficient conditional CNF that partitions the latent space into a class-specific supervised code and an unsupervised code that shared among all classes for efficient use of labeled information. Since the partitioning strategy (slightly) increases the number of function evaluations (NFEs), InfoCNF also employs gating networks to learn the error tolerances of its ordinary differential equation (ODE) solvers for better speed and performance. We show empirically that InfoCNF improves the test accuracy over the baseline while yielding comparable likelihood scores and reducing the NFEs on CIFAR10. Furthermore, applying the same partitioning strategy in InfoCNF on time-series data helps improve extrapolation performance.



### Naive Gabor Networks for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.03991v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1912.03991v2)
- **Published**: 2019-12-09 12:16:48+00:00
- **Updated**: 2020-02-24 06:21:41+00:00
- **Authors**: Chenying Liu, Jun Li, Lin He, Antonio J. Plaza, Shutao Li, Bo Li
- **Comment**: This paper has been accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Recently, many convolutional neural network (CNN) methods have been designed for hyperspectral image (HSI) classification since CNNs are able to produce good representations of data, which greatly benefits from a huge number of parameters. However, solving such a high-dimensional optimization problem often requires a large amount of training samples in order to avoid overfitting. Additionally, it is a typical non-convex problem affected by many local minima and flat regions. To address these problems, in this paper, we introduce naive Gabor Networks or Gabor-Nets which, for the first time in the literature, design and learn CNN kernels strictly in the form of Gabor filters, aiming to reduce the number of involved parameters and constrain the solution space, and hence improve the performances of CNNs. Specifically, we develop an innovative phase-induced Gabor kernel, which is trickily designed to perform the Gabor feature learning via a linear combination of local low-frequency and high-frequency components of data controlled by the kernel phase. With the phase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to automatically adapt to the local harmonic characteristics of the HSI data and thus yields more representative harmonic features. Also, this kernel can fulfill the traditional complex-valued Gabor filtering in a real-valued manner, hence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our newly developed Gabor-Nets on three well-known HSIs, suggesting that our proposed Gabor-Nets can significantly improve the performance of CNNs, particularly with a small training set.



### Environment reconstruction on depth images using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.03992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.03992v1)
- **Published**: 2019-12-09 12:18:45+00:00
- **Updated**: 2019-12-09 12:18:45+00:00
- **Authors**: Lucas P. N. Matias, Jefferson R. Souza, Denis F. Wolf
- **Comment**: 12 pages; 10 figures; open sourced; code and demo available in
  https://github.com/nuneslu/VeIGAN
- **Journal**: None
- **Summary**: Robust perception systems are essential for autonomous vehicle safety. To navigate in a complex urban environment, it is necessary precise sensors with reliable data. The task of understanding the surroundings is hard by itself; for intelligent vehicles, it is even more critical due to the high speed in which the vehicle navigates. To successfully navigate in an urban environment, the perception system must quickly receive, process, and execute an action to guarantee both passenger and pedestrian safety. Stereo cameras collect environment information at many levels, e.g., depth, color, texture, shape, which guarantee ample knowledge about the surroundings. Even so, when compared to human, computational methods lack the ability to deal with missing information, i.e., occlusions. For many perception tasks, this lack of data can be a hindrance due to the environment incomplete information. In this paper, we address this problem and discuss recent methods to deal with occluded areas inference. We then introduce a loss function focused on disparity and environment depth data reconstruction, and a Generative Adversarial Network (GAN) architecture able to deal with occluded information inference. Our results present a coherent reconstruction on depth maps, estimating regions occluded by different obstacles. Our final contribution is a loss function focused on disparity data and a GAN able to extract depth features and estimate depth data by inpainting disparity images.



### Deep Neural Network for Fast and Accurate Single Image Super-Resolution via Channel-Attention-based Fusion of Orientation-aware Features
- **Arxiv ID**: http://arxiv.org/abs/1912.04016v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04016v1)
- **Published**: 2019-12-09 13:18:58+00:00
- **Updated**: 2019-12-09 13:18:58+00:00
- **Authors**: Du Chen, Zewei He, Yanpeng Cao, Jiangxin Yang, Yanlong Cao, Michael Ying Yang, Siliang Tang, Yueting Zhuang
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Recently, Convolutional Neural Networks (CNNs) have been successfully adopted to solve the ill-posed single image super-resolution (SISR) problem. A commonly used strategy to boost the performance of CNN-based SISR models is deploying very deep networks, which inevitably incurs many obvious drawbacks (e.g., a large number of network parameters, heavy computational loads, and difficult model training). In this paper, we aim to build more accurate and faster SISR models via developing better-performing feature extraction and fusion techniques. Firstly, we proposed a novel Orientation-Aware feature extraction and fusion Module (OAM), which contains a mixture of 1D and 2D convolutional kernels (i.e., 5 x 1, 1 x 5, and 3 x 3) for extracting orientation-aware features. Secondly, we adopt the channel attention mechanism as an effective technique to adaptively fuse features extracted in different directions and in hierarchically stacked convolutional stages. Based on these two important improvements, we present a compact but powerful CNN-based model for high-quality SISR via Channel Attention-based fusion of Orientation-Aware features (SISR-CA-OA). Extensive experimental results verify the superiority of the proposed SISR-CA-OA model, performing favorably against the state-of-the-art SISR models in terms of both restoration accuracy and computational efficiency. The source codes will be made publicly available.



### Parallel Total Variation Distance Estimation with Neural Networks for Merging Over-Clusterings
- **Arxiv ID**: http://arxiv.org/abs/1912.04022v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04022v1)
- **Published**: 2019-12-09 13:25:14+00:00
- **Updated**: 2019-12-09 13:25:14+00:00
- **Authors**: Christian Reiser, Jörg Schlötterer, Michael Granitzer
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the initial situation where a dataset has been over-partitioned into $k$ clusters and seek a domain independent way to merge those initial clusters. We identify the total variation distance (TVD) as suitable for this goal. By exploiting the relation of the TVD to the Bayes accuracy we show how neural networks can be used to estimate TVDs between all pairs of clusters in parallel. Crucially, the needed memory space is decreased by reducing the required number of output neurons from $k^2$ to $k$. On realistically obtained over-clusterings of ImageNet subsets it is demonstrated that our TVD estimates lead to better merge decisions than those obtained by relying on state-of-the-art unsupervised representations. Further the generality of the approach is verified by evaluating it on a a point cloud dataset.



### ShadingNet: Image Intrinsics by Fine-Grained Shading Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1912.04023v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04023v3)
- **Published**: 2019-12-09 13:30:11+00:00
- **Updated**: 2021-01-21 16:50:00+00:00
- **Authors**: Anil S. Baslamisli, Partha Das, Hoang-An Le, Sezer Karaoglu, Theo Gevers
- **Comment**: Submitted to International Journal of Computer Vision (IJCV)
- **Journal**: None
- **Summary**: In general, intrinsic image decomposition algorithms interpret shading as one unified component including all photometric effects. As shading transitions are generally smoother than reflectance (albedo) changes, these methods may fail in distinguishing strong photometric effects from reflectance variations. Therefore, in this paper, we propose to decompose the shading component into direct (illumination) and indirect shading (ambient light and shadows) subcomponents. The aim is to distinguish strong photometric effects from reflectance variations. An end-to-end deep convolutional neural network (ShadingNet) is proposed that operates in a fine-to-coarse manner with a specialized fusion and refinement unit exploiting the fine-grained shading model. It is designed to learn specific reflectance cues separated from specific photometric effects to analyze the disentanglement capability. A large-scale dataset of scene-level synthetic images of outdoor natural environments is provided with fine-grained intrinsic image ground-truths. Large scale experiments show that our approach using fine-grained shading decompositions outperforms state-of-the-art algorithms utilizing unified shading on NED, MPI Sintel, GTA V, IIW, MIT Intrinsic Images, 3DRMS and SRD datasets.



### Synthetic Humans for Action Recognition from Unseen Viewpoints
- **Arxiv ID**: http://arxiv.org/abs/1912.04070v3
- **DOI**: 10.1007/s11263-021-01467-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04070v3)
- **Published**: 2019-12-09 14:17:03+00:00
- **Updated**: 2021-05-23 14:08:35+00:00
- **Authors**: Gül Varol, Ivan Laptev, Cordelia Schmid, Andrew Zisserman
- **Comment**: 21 pages
- **Journal**: International Journal of Computer Vision (2021)
- **Summary**: Although synthetic training data has been shown to be beneficial for tasks such as human pose estimation, its use for RGB human action recognition is relatively unexplored. Our goal in this work is to answer the question whether synthetic humans can improve the performance of human action recognition, with a particular focus on generalization to unseen viewpoints. We make use of the recent advances in monocular 3D human body reconstruction from real action sequences to automatically render synthetic training videos for the action labels. We make the following contributions: (i) we investigate the extent of variations and augmentations that are beneficial to improving performance at new viewpoints. We consider changes in body shape and clothing for individuals, as well as more action relevant augmentations such as non-uniform frame sampling, and interpolating between the motion of individuals performing the same action; (ii) We introduce a new data generation methodology, SURREACT, that allows training of spatio-temporal CNNs for action classification; (iii) We substantially improve the state-of-the-art action recognition performance on the NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally, (iv) we extend the augmentation approach to in-the-wild videos from a subset of the Kinetics dataset to investigate the case when only one-shot training data is available, and demonstrate improvements in this case as well.



### DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image
- **Arxiv ID**: http://arxiv.org/abs/1912.04071v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04071v1)
- **Published**: 2019-12-09 14:17:04+00:00
- **Updated**: 2019-12-09 14:17:04+00:00
- **Authors**: Fuyang Huang, Ailing Zeng, Minhao Liu, Qiuxia Lai, Qiang Xu
- **Comment**: None
- **Journal**: WACV 2020
- **Summary**: In this paper, we propose a two-stage fully 3D network, namely \textbf{DeepFuse}, to estimate human pose in 3D space by fusing body-worn Inertial Measurement Unit (IMU) data and multi-view images deeply. The first stage is designed for pure vision estimation. To preserve data primitiveness of multi-view inputs, the vision stage uses multi-channel volume as data representation and 3D soft-argmax as activation layer. The second one is the IMU refinement stage which introduces an IMU-bone layer to fuse the IMU and vision data earlier at data level. without requiring a given skeleton model a priori, we can achieve a mean joint error of $28.9$mm on TotalCapture dataset and $13.4$mm on Human3.6M dataset under protocol 1, improving the SOTA result by a large margin. Finally, we discuss the effectiveness of a fully 3D network for 3D pose estimation experimentally which may benefit future research.



### Estimation of Muscle Fascicle Orientation in Ultrasonic Images
- **Arxiv ID**: http://arxiv.org/abs/1912.04134v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04134v1)
- **Published**: 2019-12-09 15:46:38+00:00
- **Updated**: 2019-12-09 15:46:38+00:00
- **Authors**: Regina Pohle-Fröhlich, Christoph Dalitz, Charlotte Richter, Benjamin Stäudle, Kirsten Albracht
- **Comment**: 7 pages, 7 figures, accepted for VISAPP 2020
- **Journal**: International Conference on Computer Vision Theory and
  Applications (VISAPP), pp. 79-86 (2020)
- **Summary**: We compare four different algorithms for automatically estimating the muscle fascicle angle from ultrasonic images: the vesselness filter, the Radon transform, the projection profile method and the gray level cooccurence matrix (GLCM). The algorithm results are compared to ground truth data generated by three different experts on 425 image frames from two videos recorded during different types of motion. The best agreement with the ground truth data was achieved by a combination of pre-processing with a vesselness filter and measuring the angle with the projection profile method. The robustness of the estimation is increased by applying the algorithms to subregions with high gradients and performing a LOESS fit through these estimates.



### Learning a Neural 3D Texture Space from 2D Exemplars
- **Arxiv ID**: http://arxiv.org/abs/1912.04158v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04158v2)
- **Published**: 2019-12-09 16:35:17+00:00
- **Updated**: 2020-04-02 18:26:48+00:00
- **Authors**: Philipp Henzler, Niloy J. Mitra, Tobias Ritschel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a generative model of 2D and 3D natural textures with diversity, visual fidelity and at high computational efficiency. This is enabled by a family of methods that extend ideas from classic stochastic procedural texturing (Perlin noise) to learned, deep, non-linearities. The key idea is a hard-coded, tunable and differentiable step that feeds multiple transformed random 2D or 3D fields into an MLP that can be sampled over infinite domains. Our model encodes all exemplars from a diverse set of textures without a need to be re-trained for each exemplar. Applications include texture interpolation, and learning 3D textures from 2D exemplars.



### cGANs with Multi-Hinge Loss
- **Arxiv ID**: http://arxiv.org/abs/1912.04216v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04216v2)
- **Published**: 2019-12-09 17:51:50+00:00
- **Updated**: 2020-11-21 21:01:27+00:00
- **Authors**: Ilya Kavalerov, Wojciech Czaja, Rama Chellappa
- **Comment**: Accepted to Winter Conference on Applications of Computer Vision
  (WACV) 2021
- **Journal**: None
- **Summary**: We propose a new algorithm to incorporate class conditional information into the critic of GANs via a multi-class generalization of the commonly used Hinge loss that is compatible with both supervised and semi-supervised settings. We study the compromise between training a state of the art generator and an accurate classifier simultaneously, and propose a way to use our algorithm to measure the degree to which a generator and critic are class conditional. We show the trade-off between a generator-critic pair respecting class conditioning inputs and generating the highest quality images. With our multi-hinge loss modification we are able to improve Inception Scores and Frechet Inception Distance on the Imagenet dataset. We make our tensorflow code available at https://github.com/ilyakava/gan.



### DCIL: Deep Contextual Internal Learning for Image Restoration and Image Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1912.04229v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04229v1)
- **Published**: 2019-12-09 18:12:49+00:00
- **Updated**: 2019-12-09 18:12:49+00:00
- **Authors**: Indra Deep Mastan, Shanmuganathan Raman
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, there is a vast interest in developing methods which are independent of the training samples such as deep image prior, zero-shot learning, and internal learning. The methods above are based on the common goal of maximizing image features learning from a single image despite inherent technical diversity. In this work, we bridge the gap between the various unsupervised approaches above and propose a general framework for image restoration and image retargeting. We use contextual feature learning and internal learning to improvise the structure similarity between the source and the target images. We perform image resize application in the following setups: classical image resize using super-resolution, a challenging image resize where the low-resolution image contains noise, and content-aware image resize using image retargeting. We also provide comparisons to the relevant state-of-the-art methods.



### Self-supervised Object Motion and Depth Estimation from Video
- **Arxiv ID**: http://arxiv.org/abs/1912.04250v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04250v2)
- **Published**: 2019-12-09 18:40:47+00:00
- **Updated**: 2020-05-13 16:20:11+00:00
- **Authors**: Qi Dai, Vaishakh Patil, Simon Hecker, Dengxin Dai, Luc Van Gool, Konrad Schindler
- **Comment**: Camera Ready Version for CVPRW, 2020
- **Journal**: None
- **Summary**: We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict dense optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Our system eliminates the scale ambiguity of motion prediction through imposing a novel geometric constraint loss term. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction, and contribute to the disparity prediction in dynamic area.



### Cascaded Structure Tensor Framework for Robust Identification of Heavily Occluded Baggage Items from Multi-Vendor X-ray Scans
- **Arxiv ID**: http://arxiv.org/abs/1912.04251v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04251v2)
- **Published**: 2019-12-09 18:40:47+00:00
- **Updated**: 2020-01-21 05:28:20+00:00
- **Authors**: Taimur Hassan, Salman H. Khan, Samet Akcay, Mohammed Bennamoun, Naoufel Werghi
- **Comment**: None
- **Journal**: None
- **Summary**: In the last two decades, luggage scanning has globally become one of the prime aviation security concerns. Manual screening of the baggage items is a cumbersome, subjective and inefficient process. Hence, many researchers have developed Xray imagery-based autonomous systems to address these shortcomings. However, to the best of our knowledge, there is no framework, up to now, that can recognize heavily occluded and cluttered baggage items from multi-vendor X-ray scans. This paper presents a cascaded structure tensor framework which can automatically extract and recognize suspicious items irrespective of their position and orientation in the multi-vendor X-ray scans. The proposed framework is unique, as it intelligently extracts each object by iteratively picking contour based transitional information from different orientations and uses only a single feedforward convolutional neural network for the recognition. The proposed framework has been rigorously tested on publicly available GDXray and SIXray datasets containing a total of 1,067,381 X-ray scans where it significantly outperformed the state-of-the-art solutions by achieving the mean average precision score of 0.9343 and 0.9595 for extracting and recognizing suspicious items from GDXray and SIXray scans, respectively. Furthermore, the proposed framework has achieved 15.78% better time



### An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.04259v3
- **DOI**: 10.1109/ICSPIS48872.2019.9066113
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04259v3)
- **Published**: 2019-12-09 18:52:30+00:00
- **Updated**: 2020-04-23 05:00:20+00:00
- **Authors**: Moein Hasani, Hassan Khotanlou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we have studied how the training of the convolutional neural networks (CNNs) can be affected by changing the position of the batch normalization (BN) layer. Three different convolutional neural networks have been chosen for our experiments. These networks are AlexNet, VGG-16, and ResNet- 20. We show that the speed up in training provided by the BN algorithm can be improved by using other positions for the BN layer than the one suggested by its original paper. Also, we discuss how the BN layer in a certain position can aid the training of one network but not the other. Three different positions for the BN layer have been studied in this research. These positions are: the BN layer between the convolution layer and the non-linear activation function, the BN layer after the non-linear activation function and finally, the BN layer before each of the convolutional layers.



### Side-Aware Boundary Localization for More Precise Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04260v2)
- **Published**: 2019-12-09 18:54:05+00:00
- **Updated**: 2020-08-26 13:03:16+00:00
- **Authors**: Jiaqi Wang, Wenwei Zhang, Yuhang Cao, Kai Chen, Jiangmiao Pang, Tao Gong, Jianping Shi, Chen Change Loy, Dahua Lin
- **Comment**: ECCV 2020 Spotlight
- **Journal**: None
- **Summary**: Current object detection frameworks mainly rely on bounding box regression to localize objects. Despite the remarkable progress in recent years, the precision of bounding box regression remains unsatisfactory, hence limiting performance in object detection. We observe that precise localization requires careful placement of each side of the bounding box. However, the mainstream approach, which focuses on predicting centers and sizes, is not the most effective way to accomplish this task, especially when there exists displacements with large variance between the anchors and the targets. In this paper, we propose an alternative approach, named as Side-Aware Boundary Localization (SABL), where each side of the bounding box is respectively localized with a dedicated network branch. To tackle the difficulty of precise localization in the presence of displacements with large variance, we further propose a two-step localization scheme, which first predicts a range of movement through bucket prediction and then pinpoints the precise position within the predicted bucket. We test the proposed method on both two-stage and single-stage detection frameworks. Replacing the standard bounding box regression branch with the proposed design leads to significant improvements on Faster R-CNN, RetinaNet, and Cascade R-CNN, by 3.0%, 1.7%, and 0.9%, respectively. Code is available at https://github.com/open-mmlab/mmdetection.



### DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised Data
- **Arxiv ID**: http://arxiv.org/abs/1912.04302v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1912.04302v2)
- **Published**: 2019-12-09 19:00:04+00:00
- **Updated**: 2020-03-24 19:00:02+00:00
- **Authors**: Aljaž Božič, Michael Zollhöfer, Christian Theobalt, Matthias Nießner
- **Comment**: Video: https://youtu.be/OrHLacCDZVQ
- **Journal**: None
- **Summary**: Applying data-driven approaches to non-rigid 3D reconstruction has been difficult, which we believe can be attributed to the lack of a large-scale training corpus. Unfortunately, this method fails for important cases such as highly non-rigid deformations. We first address this problem of lack of data by introducing a novel semi-supervised strategy to obtain dense inter-frame correspondences from a sparse set of annotations. This way, we obtain a large dataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned frame pairs; in addition, we provide a test set along with several metrics for evaluation. Based on this corpus, we introduce a data-driven non-rigid feature matching approach, which we integrate into an optimization-based reconstruction pipeline. Here, we propose a new neural network that operates on RGB-D frames, while maintaining robustness under large non-rigid deformations and producing accurate predictions. Our approach significantly outperforms existing non-rigid reconstruction methods that do not use learned data terms, as well as learning-based approaches that only use self-supervision.



### Video action detection by learning graph-based spatio-temporal interactions
- **Arxiv ID**: http://arxiv.org/abs/1912.04316v3
- **DOI**: 10.1016/j.cviu.2021.103187
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04316v3)
- **Published**: 2019-12-09 19:01:46+00:00
- **Updated**: 2021-03-01 10:37:54+00:00
- **Authors**: Matteo Tomei, Lorenzo Baraldi, Simone Calderara, Simone Bronzin, Rita Cucchiara
- **Comment**: This is the authors version of an article accepted for publication in
  Computer Vision and Image Understanding (CVIU), available online February
  2021
- **Journal**: Computer Vision and Image Understanding (CVIU), 2021
- **Summary**: Action Detection is a complex task that aims to detect and classify human actions in video clips. Typically, it has been addressed by processing fine-grained features extracted from a video classification backbone. Recently, thanks to the robustness of object and people detectors, a deeper focus has been added on relationship modelling. Following this line, we propose a graph-based framework to learn high-level interactions between people and objects, in both space and time. In our formulation, spatio-temporal relationships are learned through self-attention on a multi-layer graph structure which can connect entities from consecutive clips, thus considering long-range spatial and temporal dependencies. The proposed module is backbone independent by design and does not require end-to-end training. Extensive experiments are conducted on the AVA dataset, where our model demonstrates state-of-the-art results and consistent improvements over baselines built with different backbones. Code is publicly available at https://github.com/aimagelab/STAGE_action_detection.



### 3D Particle Positions from Computer Stereo Vision in PK-4
- **Arxiv ID**: http://arxiv.org/abs/1912.04333v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.plasm-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.04333v1)
- **Published**: 2019-12-09 19:21:19+00:00
- **Updated**: 2019-12-09 19:21:19+00:00
- **Authors**: Daniel P. Mohr, Peter Huber, Mierk Schwabe, Christina A. Knapek
- **Comment**: None
- **Journal**: None
- **Summary**: Complex plasmas consist of microparticles embedded in a low-temperature plasma containing ions, electrons and neutral particles. The microparticles form a dynamical system that can be used to study a multitude of effects on the level of the constituent particles. The microparticles are usually illuminated with a sheet of laser light, and the scattered light can be observed with digital cameras. Some complex plasma microgravity research facilities use two cameras with an overlapping field of view.   An overlapping field of view can be used to combine the resulting images into one and trace the particles in the larger field of view. In previous work this was discussed for the images recorded by the PK-4 Laboratory on board the International Space Station. In that work the width of the laser sheet was, however, not taken into account. In this paper, we will discuss how to improve the transformation of the features into a joint coordinate system, and possibly extract information on the 3D position of particles in the overlap region.



### Grasping in the Wild:Learning 6DoF Closed-Loop Grasping from Low-Cost Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/1912.04344v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.04344v2)
- **Published**: 2019-12-09 19:47:08+00:00
- **Updated**: 2020-06-17 20:05:35+00:00
- **Authors**: Shuran Song, Andy Zeng, Johnny Lee, Thomas Funkhouser
- **Comment**: Project Webpage https://graspinwild.cs.columbia.edu/
- **Journal**: None
- **Summary**: Intelligent manipulation benefits from the capacity to flexibly control an end-effector with high degrees of freedom (DoF) and dynamically react to the environment. However, due to the challenges of collecting effective training data and learning efficiently, most grasping algorithms today are limited to top-down movements and open-loop execution. In this work, we propose a new low-cost hardware interface for collecting grasping demonstrations by people in diverse environments. Leveraging this data, we show that it is possible to train a robust end-to-end 6DoF closed-loop grasping model with reinforcement learning that transfers to real robots. A key aspect of our grasping model is that it uses "action-view" based rendering to simulate future states with respect to different possible actions. By evaluating these states using a learned value function (Q-function), our method is able to better select corresponding actions that maximize total rewards (i.e., grasping success). Our final grasping system is able to achieve reliable 6DoF closed-loop grasping of novel objects across various scene configurations, as well as dynamic scenes with moving objects.



### Car Pose in Context: Accurate Pose Estimation with Ground Plane Constraints
- **Arxiv ID**: http://arxiv.org/abs/1912.04363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04363v1)
- **Published**: 2019-12-09 20:36:17+00:00
- **Updated**: 2019-12-09 20:36:17+00:00
- **Authors**: Pengfei Li, Weichao Qiu, Michael Peven, Gregory D. Hager, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Scene context is a powerful constraint on the geometry of objects within the scene in cases, such as surveillance, where the camera geometry is unknown and image quality may be poor. In this paper, we describe a method for estimating the pose of cars in a scene jointly with the ground plane that supports them. We formulate this as a joint optimization that accounts for varying car shape using a statistical atlas, and which simultaneously computes geometry and internal camera parameters. We demonstrate that this method produces significant improvements for car pose estimation, and we show that the resulting 3D geometry, when computed over a video sequence, makes it possible to improve on state of the art classification of car behavior. We also show that introducing the planar constraint allows us to estimate camera focal length in a reliable manner.



### Modular Multimodal Architecture for Document Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.04376v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04376v1)
- **Published**: 2019-12-09 21:06:15+00:00
- **Updated**: 2019-12-09 21:06:15+00:00
- **Authors**: Tyler Dauphinee, Nikunj Patel, Mohammad Rashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Page classification is a crucial component to any document analysis system, allowing for complex branching control flows for different components of a given document. Utilizing both the visual and textual content of a page, the proposed method exceeds the current state-of-the-art performance on the RVL-CDIP benchmark at 93.03% test accuracy.



### Training Deep Neural Networks to Detect Repeatable 2D Features Using Large Amounts of 3D World Capture Data
- **Arxiv ID**: http://arxiv.org/abs/1912.04384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04384v1)
- **Published**: 2019-12-09 21:28:50+00:00
- **Updated**: 2019-12-09 21:28:50+00:00
- **Authors**: Alexander Mai, Joseph Menke, Allen Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Image space feature detection is the act of selecting points or parts of an image that are easy to distinguish from the surrounding image region. By combining a repeatable point detection with a descriptor, parts of an image can be matched with one another, which is useful in applications like estimating pose from camera input or rectifying images. Recently, precise indoor tracking has started to become important for Augmented and Virtual reality as it is necessary to allow positioning of a headset in 3D space without the need for external tracking devices. Several modern feature detectors use homographies to simulate different viewpoints, not only to train feature detection and description, but test them as well. The problem is that, often, views of indoor spaces contain high depth disparity. This makes the approximation that a homography applied to an image represents a viewpoint change inaccurate. We claim that in order to train detectors to work well in indoor environments, they must be robust to this type of geometry, and repeatable under true viewpoint change instead of homographies. Here we focus on the problem of detecting repeatable feature locations under true viewpoint change. To this end, we generate labeled 2D images from a photo-realistic 3D dataset. These images are used for training a neural network based feature detector. We further present an algorithm for automatically generating labels of repeatable 2D features, and present a fast, easy to use test algorithm for evaluating a detector in an 3D environment.



### Self Organizing Nebulous Growths for Robust and Incremental Data Visualization
- **Arxiv ID**: http://arxiv.org/abs/1912.04896v3
- **DOI**: 10.1109/TNNLS.2020.3023941.
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04896v3)
- **Published**: 2019-12-09 22:11:51+00:00
- **Updated**: 2020-10-02 01:18:23+00:00
- **Authors**: Damith Senanayake, Wei Wang, Shalin H. Naik, Saman Halgamuge
- **Comment**: in IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Non-parametric dimensionality reduction techniques, such as t-SNE and UMAP, are proficient in providing visualizations for datasets of fixed sizes. However, they cannot incrementally map and insert new data points into an already provided data visualization. We present Self-Organizing Nebulous Growths (SONG), a parametric nonlinear dimensionality reduction technique that supports incremental data visualization, i.e., incremental addition of new data while preserving the structure of the existing visualization. In addition, SONG is capable of handling new data increments, no matter whether they are similar or heterogeneous to the already observed data distribution. We test SONG on a variety of real and simulated datasets. The results show that SONG is superior to Parametric t-SNE, t-SNE and UMAP in incremental data visualization. Specifically, for heterogeneous increments, SONG improves over Parametric t-SNE by 14.98 % on the Fashion MNIST dataset and 49.73% on the MNIST dataset regarding the cluster quality measured by the Adjusted Mutual Information scores. On similar or homogeneous increments, the improvements are 8.36% and 42.26% respectively. Furthermore, even when the above datasets are presented all at once, SONG performs better or comparable to UMAP, and superior to t-SNE. We also demonstrate that the algorithmic foundations of SONG render it more tolerant to noise compared to UMAP and t-SNE, thus providing greater utility for data with high variance, high mixing of clusters, or noise.



### Deep Autoencoders with Value-at-Risk Thresholding for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.04418v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.04418v1)
- **Published**: 2019-12-09 23:14:05+00:00
- **Updated**: 2019-12-09 23:14:05+00:00
- **Authors**: Albert Akhriev, Jakub Marecek
- **Comment**: None
- **Journal**: IEEE International Symposium on Multimedia 2019
- **Summary**: Many real-world monitoring and surveillance applications require non-trivial anomaly detection to be run in the streaming model. We consider an incremental-learning approach, wherein a deep-autoencoding (DAE) model of what is normal is trained and used to detect anomalies at the same time. In the detection of anomalies, we utilise a novel thresholding mechanism, based on value at risk (VaR). We compare the resulting convolutional neural network (CNN) against a number of subspace methods, and present results on changedetection net.



### Basis Prediction Networks for Effective Burst Denoising with Large Kernels
- **Arxiv ID**: http://arxiv.org/abs/1912.04421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.04421v2)
- **Published**: 2019-12-09 23:28:30+00:00
- **Updated**: 2020-12-03 00:19:45+00:00
- **Authors**: Zhihao Xia, Federico Perazzi, Michaël Gharbi, Kalyan Sunkavalli, Ayan Chakrabarti
- **Comment**: CVPR 2020. Project website at
  https://www.cse.wustl.edu/~zhihao.xia/bpn/
- **Journal**: None
- **Summary**: Bursts of images exhibit significant self-similarity across both time and space. This motivates a representation of the kernels as linear combinations of a small set of basis elements. To this end, we introduce a novel basis prediction network that, given an input burst, predicts a set of global basis kernels -- shared within the image -- and the corresponding mixing coefficients -- which are specific to individual pixels. Compared to state-of-the-art techniques that output a large tensor of per-pixel spatiotemporal kernels, our formulation substantially reduces the dimensionality of the network output. This allows us to effectively exploit comparatively larger denoising kernels, achieving both significant quality improvements (over 1dB PSNR) and faster run-times over state-of-the-art methods.



### Robust, Extensible, and Fast: Teamed Classifiers for Vehicle Tracking and Vehicle Re-ID in Multi-Camera Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.04423v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.04423v2)
- **Published**: 2019-12-09 23:34:33+00:00
- **Updated**: 2020-01-07 13:54:53+00:00
- **Authors**: Abhijit Suprem, Rodrigo Alves Lima, Bruno Padilha, Joao Eduardo Ferreira, Calton Pu
- **Comment**: None
- **Journal**: 2019 IEEE Conference on Cognitive Machine Intelligence
- **Summary**: As camera networks have become more ubiquitous over the past decade, the research interest in video management has shifted to analytics on multi-camera networks. This includes performing tasks such as object detection, attribute identification, and vehicle/person tracking across different cameras without overlap. Current frameworks for management are designed for multi-camera networks in a closed dataset environment where there is limited variability in cameras and characteristics of the surveillance environment are well known. Furthermore, current frameworks are designed for offline analytics with guidance from human operators for forensic applications. This paper presents a teamed classifier framework for video analytics in heterogeneous many-camera networks with adversarial conditions such as multi-scale, multi-resolution cameras capturing the environment with varying occlusion, blur, and orientations. We describe an implementation for vehicle tracking and vehicle re-identification (re-id), where we implement a zero-shot learning (ZSL) system that performs automated tracking of all vehicles all the time. Our evaluations on VeRi-776 and Cars196 show the teamed classifier framework is robust to adversarial conditions, extensible to changing video characteristics such as new vehicle types/brands and new cameras, and offers real-time performance compared to current offline video analytics approaches.



