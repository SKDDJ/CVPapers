# Arxiv Papers in cs.CV on 2019-12-08
### Deep Learning Methods for Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1912.05435v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05435v1)
- **Published**: 2019-12-08 00:37:47+00:00
- **Updated**: 2019-12-08 00:37:47+00:00
- **Authors**: Zihan Zeng, Jing Tian
- **Comment**: Submit to ICMR. arXiv admin note: text overlap with arXiv:1907.11845
  by other authors
- **Journal**: None
- **Summary**: Signature is widely used in human daily lives, and serves as a supplementary characteristic for verifying human identity. However, there is rare work of verifying signature. In this paper, we propose a few deep learning architectures to tackle this task, ranging from CNN, RNN to CNN-RNN compact model. We also improve Path Signature Features by encoding temporal information in order to enlarge the discrepancy between genuine and forgery signatures. Our numerical experiments demonstrate the effectiveness of our constructed models and features representations.



### Deep Learning-Based Feature-Aware Data Modeling for Complex Physics Simulations
- **Arxiv ID**: http://arxiv.org/abs/1912.03587v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03587v1)
- **Published**: 2019-12-08 01:14:47+00:00
- **Updated**: 2019-12-08 01:14:47+00:00
- **Authors**: Qun Liu, Subhashis Hazarika, John M. Patchett, James Paul Ahrens, Ayan Biswas
- **Comment**: Accepted as a research poster at the International Conference for
  High Performance Computing, Networking, Storage, and Analysis (SC19)
- **Journal**: None
- **Summary**: Data modeling and reduction for in situ is important. Feature-driven methods for in situ data analysis and reduction are a priority for future exascale machines as there are currently very few such methods. We investigate a deep-learning based workflow that targets in situ data processing using autoencoders. We propose a Residual Autoencoder integrated Residual in Residual Dense Block (RRDB) to obtain better performance. Our proposed framework compressed our test data into 66 KB from 2.1 MB per 3D volume timestep.



### Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language
- **Arxiv ID**: http://arxiv.org/abs/1912.03590v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1912.03590v3)
- **Published**: 2019-12-08 01:34:39+00:00
- **Updated**: 2020-12-26 15:27:54+00:00
- **Authors**: Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo
- **Comment**: This paper is accepted by AAAI 2020
- **Journal**: None
- **Summary**: We address the problem of retrieving a specific moment from an untrimmed video by a query sentence. This is a challenging problem because a target moment may take place in relations to other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they consider temporal moments individually and neglect the temporal dependencies. In this paper, we model the temporal relations between video moments by a two-dimensional map, where one dimension indicates the starting time of a moment and the other indicates the end time. This 2D temporal map can cover diverse video moments with different lengths, while representing their adjacent relations. Based on the 2D map, we propose a Temporal Adjacent Network (2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal relation, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed 2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.



### Neural Network Generalization: The impact of camera parameters
- **Arxiv ID**: http://arxiv.org/abs/1912.03604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03604v1)
- **Published**: 2019-12-08 03:14:32+00:00
- **Updated**: 2019-12-08 03:14:32+00:00
- **Authors**: Zhenyi Liu, Trisha Lian, Joyce Farrell, Brian Wandell
- **Comment**: 11 pages, 11 figures, in preparation for submission
- **Journal**: None
- **Summary**: We quantify the generalization of a convolutional neural network (CNN) trained to identify cars. First, we perform a series of experiments to train the network using one image dataset - either synthetic or from a camera - and then test on a different image dataset. We show that generalization between images obtained with different cameras is roughly the same as generalization between images from a camera and ray-traced multispectral synthetic images. Second, we use ISETAuto, a soft prototyping tool that creates ray-traced multispectral simulations of camera images, to simulate sensor images with a range of pixel sizes, color filters, acquisition and post-acquisition processing. These experiments reveal how variations in specific camera parameters and image processing operations impact CNN generalization. We find that (a) pixel size impacts generalization, (b) demosaicking substantially impacts performance and generalization for shallow (8-bit) bit-depths but not deeper ones (10-bit), and (c) the network performs well using raw (not demosaicked) sensor data for 10-bit pixels.



### Individual predictions matter: Assessing the effect of data ordering in training fine-tuned CNNs for medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.03606v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03606v1)
- **Published**: 2019-12-08 03:18:18+00:00
- **Updated**: 2019-12-08 03:18:18+00:00
- **Authors**: John R. Zech, Jessica Zosa Forde, Michael L. Littman
- **Comment**: J.Z. and J.F. contributed equally to this work
- **Journal**: None
- **Summary**: We reproduced the results of CheXNet with fixed hyperparameters and 50 different random seeds to identify 14 finding in chest radiographs (x-rays). Because CheXNet fine-tunes a pre-trained DenseNet, the random seed affects the ordering of the batches of training data but not the initialized model weights. We found substantial variability in predictions for the same radiograph across model runs (mean ln[(maximum probability)/(minimum probability)] 2.45, coefficient of variation 0.543). This individual radiograph-level variability was not fully reflected in the variability of AUC on a large test set. Averaging predictions from 10 models reduced variability by nearly 70% (mean coefficient of variation from 0.543 to 0.169, t-test 15.96, p-value < 0.0001). We encourage researchers to be aware of the potential variability of CNNs and ensemble predictions from multiple models to minimize the effect this variability may have on the care of individual patients when these models are deployed clinically.



### Learning Sparse 2D Temporal Adjacent Networks for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.03612v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03612v1)
- **Published**: 2019-12-08 04:16:28+00:00
- **Updated**: 2019-12-08 04:16:28+00:00
- **Authors**: Songyang Zhang, Houwen Peng, Le Yang, Jianlong Fu, Jiebo Luo
- **Comment**: This is our winner solution for the HACS Temporal Action Localization
  Challenge at ICCV 2019
- **Journal**: None
- **Summary**: In this report, we introduce the Winner method for HACS Temporal Action Localization Challenge 2019. Temporal action localization is challenging since a target proposal may be related to several other candidate proposals in an untrimmed video. Existing methods cannot tackle this challenge well since temporal proposals are considered individually and their temporal dependencies are neglected. To address this issue, we propose sparse 2D temporal adjacent networks to model the temporal relationship between candidate proposals. This method is built upon the recent proposed 2D-TAN approach. The sampling strategy in 2D-TAN introduces the unbalanced context problem, where short proposals can perceive more context than long proposals. Therefore, we further propose a Sparse 2D Temporal Adjacent Network (S-2D-TAN). It is capable of involving more context information for long proposals and further learning discriminative features from them. By combining our S-2D-TAN with a simple action classifier, our method achieves a mAP of 23.49 on the test set, which win the first place in the HACS challenge.



### DASZL: Dynamic Action Signatures for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03613v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03613v3)
- **Published**: 2019-12-08 04:30:59+00:00
- **Updated**: 2020-11-18 03:53:54+00:00
- **Authors**: Tae Soo Kim, Jonathan D. Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi Zhang, Weichao Qiu, Alan Yuille, Gregory D. Hager
- **Comment**: 10 pages, 4 figures, 3 tables, AAAI2021 submission
- **Journal**: None
- **Summary**: There are many realistic applications of activity recognition where the set of potential activity descriptions is combinatorially large. This makes end-to-end supervised training of a recognition system impractical as no training set is practically able to encompass the entire label set. In this paper, we present an approach to fine-grained recognition that models activities as compositions of dynamic action signatures. This compositional approach allows us to reframe fine-grained recognition as zero-shot activity recognition, where a detector is composed "on the fly" from simple first-principles state machines supported by deep-learned components. We evaluate our method on the Olympic Sports and UCF101 datasets, where our model establishes a new state of the art under multiple experimental paradigms. We also extend this method to form a unique framework for zero-shot joint segmentation and classification of activities in video and demonstrate the first results in zero-shot decoding of complex action sequences on a widely-used surgical dataset. Lastly, we show that we can use off-the-shelf object detectors to recognize activities in completely de-novo settings with no additional training.



### Single image reflection removal via learning with multi-image constraints
- **Arxiv ID**: http://arxiv.org/abs/1912.03623v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03623v3)
- **Published**: 2019-12-08 06:10:49+00:00
- **Updated**: 2023-08-27 13:50:10+00:00
- **Authors**: Yingda Yin, Qingnan Fan, Dongdong Chen, Yujie Wang, Angelica Aviles-Rivero, Ruoteng Li, Carola-Bibiane Schnlieb, Baoquan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Reflections are very common phenomena in our daily photography, which distract people's attention from the scene behind the glass. The problem of removing reflection artifacts is important but challenging due to its ill-posed nature. The traditional approaches solve an optimization problem over the constraints induced from multiple images, at the expense of large computation costs. Recent learning-based approaches have demonstrated a significant improvement in both performance and running time for single image reflection removal, but are limited as they require a large number of synthetic reflection/clean image pairs for direct supervision to approximate the ground truth, at the risk of overfitting in the synthetic image domain and degrading in the real image domain. In this paper, we propose a novel learning-based solution that combines the advantages of the aforementioned approaches and overcomes their drawbacks. Our algorithm works by learning a deep neural network to optimize the target with joint constraints enhanced among multiple input images during the training phase, but is able to eliminate reflections only from a single input for evaluation. Our algorithm runs in real-time and achieves state-of-the-art reflection removal performance on real images. We further propose a strong network backbone that disentangles the background and reflection information into separate latent codes, which are embedded into a shared one-branch deep neural network for both background and reflection predictions. The proposed backbone experimentally performs better than the other common network implementations, and provides insightful knowledge to understand the reflection removal task.



### Classification of Brainwave Signals Based on Hybrid Deep Learning and an Evolutionary Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1912.07361v1
- **DOI**: 10.17656/jzs.10755
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07361v1)
- **Published**: 2019-12-08 07:03:14+00:00
- **Updated**: 2019-12-08 07:03:14+00:00
- **Authors**: Zhyar Rzgar K. Rostam, Sozan Abdullah Mahmood
- **Comment**: 10 pages, 6 tables, and 5 figures
- **Journal**: JZS (2019) 21_2 35_44
- **Summary**: Brainwave signals are read through Electroencephalogram (EEG) devices. These signals are generated from an active brain based on brain activities and thoughts. The classification of brainwave signals is a challenging task due to its non-stationary nature. To address the issue, this paper proposes a Convolutional Neural Network (CNN) model to classify brainwave signals. In order to evaluate the performance of the proposed model a dataset is developed by recording brainwave signals for two conditions, which are visible and invisible. In the visible mode, the human subjects focus on the color and shape presented. Meanwhile, in the invisible mode, the subjects think about specific colors or shapes with closed eyes. A comparison has been provided between the original CNN and the proposed CNN architecture on the same dataset. The results show that the proposed CNN model achieves higher classification accuracy as compared to the standard CNN. The best accuracy rate achieved when the proposed CNN is applied on the visible color mode is 92%. In the future, improvements on the proposed CNN will be able to classify raw EEG signals in an efficient way.



### 6-DOF Grasping for Target-driven Object Manipulation in Clutter
- **Arxiv ID**: http://arxiv.org/abs/1912.03628v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03628v2)
- **Published**: 2019-12-08 07:09:53+00:00
- **Updated**: 2020-05-21 03:23:05+00:00
- **Authors**: Adithyavairavan Murali, Arsalan Mousavian, Clemens Eppner, Chris Paxton, Dieter Fox
- **Comment**: Accepted to the International Conference on Robotics and Automation
  (ICRA) 2020
- **Journal**: None
- **Summary**: Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3%, outperforming baseline approaches by 17.6% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found at https://youtu.be/w0B5S-gCsJk.



### VoronoiNet: General Functional Approximators with Local Support
- **Arxiv ID**: http://arxiv.org/abs/1912.03629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03629v1)
- **Published**: 2019-12-08 07:12:34+00:00
- **Updated**: 2019-12-08 07:12:34+00:00
- **Authors**: Francis Williams, Daniele Panozzo, Kwang Moo Yi, Andrea Tagliasacchi
- **Comment**: None
- **Journal**: None
- **Summary**: Voronoi diagrams are highly compact representations that are used in various Graphics applications. In this work, we show how to embed a differentiable version of it -- via a novel deep architecture -- into a generative deep network. By doing so, we achieve a highly compact latent embedding that is able to provide much more detailed reconstructions, both in 2D and 3D, for various shapes. In this tech report, we introduce our representation and present a set of preliminary results comparing it with recently proposed implicit occupancy networks.



### Face Beautification: Beyond Makeup Transfer
- **Arxiv ID**: http://arxiv.org/abs/1912.03630v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03630v2)
- **Published**: 2019-12-08 07:16:53+00:00
- **Updated**: 2019-12-11 18:29:08+00:00
- **Authors**: Xudong Liu, Ruizhe Wang, Chih-Fan Chen, Minglei Yin, Hao Peng, Shukhan Ng, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Facial appearance plays an important role in our social lives. Subjective perception of women's beauty depends on various face-related (e.g., skin, shape, hair) and environmental (e.g., makeup, lighting, angle) factors. Similar to cosmetic surgery in the physical world, virtual face beautification is an emerging field with many open issues to be addressed. Inspired by the latest advances in style-based synthesis and face beauty prediction, we propose a novel framework of face beautification. For a given reference face with a high beauty score, our GAN-based architecture is capable of translating an inquiry face into a sequence of beautified face images with referenced beauty style and targeted beauty score values. To achieve this objective, we propose to integrate both style-based beauty representation (extracted from the reference face) and beauty score prediction (trained on SCUT-FBP database) into the process of beautification. Unlike makeup transfer, our approach targets at many-to-many (instead of one-to-one) translation where multiple outputs can be defined by either different references or varying beauty scores. Extensive experimental results are reported to demonstrate the effectiveness and flexibility of the proposed face beautification framework.



### View-invariant Deep Architecture for Human Action Recognition using late fusion
- **Arxiv ID**: http://arxiv.org/abs/1912.03632v1
- **DOI**: 10.1109/TIP.2020.2965299
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03632v1)
- **Published**: 2019-12-08 07:30:48+00:00
- **Updated**: 2019-12-08 07:30:48+00:00
- **Authors**: Chhavi Dhiman, Dinesh Kumar Vishwakarma
- **Comment**: 10 pages, 7 figures
- **Journal**: 2019
- **Summary**: Human action Recognition for unknown views is a challenging task. We propose a view-invariant deep human action recognition framework, which is a novel integration of two important action cues: motion and shape temporal dynamics (STD). The motion stream encapsulates the motion content of action as RGB Dynamic Images (RGB-DIs) which are processed by the fine-tuned InceptionV3 model. The STD stream learns long-term view-invariant shape dynamics of action using human pose model (HPM) based view-invariant features mined from structural similarity index matrix (SSIM) based key depth human pose frames. To predict the score of the test sample, three types of late fusion (maximum, average and product) techniques are applied on individual stream scores. To validate the performance of the proposed novel framework the experiments are performed using both cross subject and cross-view validation schemes on three publically available benchmarks- NUCLA multi-view dataset, UWA3D-II Activity dataset and NTU RGB-D Activity dataset. Our algorithm outperforms with existing state-of-the-arts significantly that is reported in terms of accuracy, receiver operating characteristic (ROC) curve and area under the curve (AUC).



### Capsule-Based Persian/Arabic Robust Handwritten Digit Recognition Using EM Routing
- **Arxiv ID**: http://arxiv.org/abs/1912.03634v2
- **DOI**: 10.1109/PRIA.2019.8785981
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03634v2)
- **Published**: 2019-12-08 07:58:26+00:00
- **Updated**: 2019-12-18 20:00:22+00:00
- **Authors**: Ali Ghofrani, Rahil Mahdian Toroghi
- **Comment**: 5 pages, 10 figures, 4th International Conference on Pattern
  Recognition and Image Analysis (IPRIA2019), IEEE
- **Journal**: None
- **Summary**: In this paper, the problem of handwritten digit recognition has been addressed. However, the underlying language is Persian/Arabic, and the system with which this task is a capsule network (CapsNet) has recently emerged as a more advanced architecture than its ancestor, namely CNN (Convolutional Neural Network). The training of the architecture is performed using the Hoda dataset, which has been provided for Persian/Arabic handwritten digits. The output of the system clearly outperforms the results achieved by its ancestors, as well as other previously presented recognition algorithms.



### SaLite : A light-weight model for salient object detection
- **Arxiv ID**: http://arxiv.org/abs/1912.03641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03641v1)
- **Published**: 2019-12-08 08:45:08+00:00
- **Updated**: 2019-12-08 08:45:08+00:00
- **Authors**: Kitty Varghese, Sauradip Nag
- **Comment**: This was submitted to NCVPRIPG 2019
- **Journal**: None
- **Summary**: Salient object detection is a prevalent computer vision task that has applications ranging from abnormality detection to abnormality processing. Context modelling is an important criterion in the domain of saliency detection. A global context helps in determining the salient object in a given image by contrasting away other objects in the global view of the scene. However, the local context features detects the boundaries of the salient object with higher accuracy in a given region. To incorporate the best of both worlds, our proposed SaLite model uses both global and local contextual features. It is an encoder-decoder based architecture in which the encoder uses a lightweight SqueezeNet and decoder is modelled using convolution layers. Modern deep based models entitled for saliency detection use a large number of parameters, which is difficult to deploy on embedded systems. This paper attempts to solve the above problem using SaLite which is a lighter process for salient object detection without compromising on performance. Our approach is extensively evaluated on three publicly available datasets namely DUTS, MSRA10K, and SOC. Experimental results show that our proposed SaLite has significant and consistent improvements over the state-of-the-art methods.



### Compressing 3DCNNs Based on Tensor Train Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1912.03647v2
- **DOI**: 10.1016/j.neunet.2020.07.028
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03647v2)
- **Published**: 2019-12-08 09:51:08+00:00
- **Updated**: 2020-08-11 03:42:21+00:00
- **Authors**: Dingheng Wang, Guangshe Zhao, Guoqi Li, Lei Deng, Yang Wu
- **Comment**: Accepted by Neural Networks. Please see the final version by the DOI
  below
- **Journal**: None
- **Summary**: Three dimensional convolutional neural networks (3DCNNs) have been applied in many tasks, e.g., video and 3D point cloud recognition. However, due to the higher dimension of convolutional kernels, the space complexity of 3DCNNs is generally larger than that of traditional two dimensional convolutional neural networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining environments such as embedded devices, neural network compression is a promising approach. In this work, we adopt the tensor train (TT) decomposition, a straightforward and simple in situ training compression method, to shrink the 3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT format, we investigate how to select appropriate TT ranks for achieving higher compression ratio. We have also discussed the redundancy of 3D convolutional kernels for compression, core significance and future directions of this work, as well as the theoretical computation complexity versus practical executing time of convolution in TT. In the light of multiple contrast experiments based on VIVA challenge, UCF11, and UCF101 datasets, we conclude that TT decomposition can compress 3DCNNs by around one hundred times without significant accuracy loss, which will enable its applications in extensive real world scenarios.



### ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1912.03650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03650v1)
- **Published**: 2019-12-08 10:30:53+00:00
- **Updated**: 2019-12-08 10:30:53+00:00
- **Authors**: Yair Shemer, Daniel Rotman, Nahum Shimkin
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there has been an increasing interest in building video summarization tools, where the goal is to automatically create a short summary of an input video that properly represents the original content. We consider shot-based video summarization where the summary consists of a subset of the video shots which can be of various lengths. A straightforward approach to maximize the representativeness of a subset of shots is by minimizing the total distance between shots and their nearest selected shots. We formulate the task of video summarization as an optimization problem with a knapsack-like constraint on the total summary duration. Previous studies have proposed greedy algorithms to solve this problem approximately, but no experiments were presented to measure the ability of these methods to obtain solutions with low total distance. Indeed, our experiments on video summarization datasets show that the success of current methods in obtaining results with low total distance still has much room for improvement. In this paper, we develop ILS-SUMM, a novel video summarization algorithm to solve the subset selection problem under the knapsack constraint. Our algorithm is based on the well-known metaheuristic optimization framework -- Iterated Local Search (ILS), known for its ability to avoid weak local minima and obtain a good near-global minimum. Extensive experiments show that our method finds solutions with significantly better total distance than previous methods. Moreover, to indicate the high scalability of ILS-SUMM, we introduce a new dataset consisting of videos of various lengths.



### Bidirectional Scene Text Recognition with a Single Decoder
- **Arxiv ID**: http://arxiv.org/abs/1912.03656v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03656v2)
- **Published**: 2019-12-08 11:20:35+00:00
- **Updated**: 2020-03-02 14:44:34+00:00
- **Authors**: Maurits Bleeker, Maarten de Rijke
- **Comment**: 8 pages. In 24th European Conference on Artificial Intelligence
- **Journal**: None
- **Summary**: Scene Text Recognition (STR) is the problem of recognizing the correct word or character sequence in a cropped word image. To obtain more robust output sequences, the notion of bidirectional STR has been introduced. So far, bidirectional STRs have been implemented by using two separate decoders; one for left-to-right decoding and one for right-to-left. Having two separate decoders for almost the same task with the same output space is undesirable from a computational and optimization point of view. We introduce the bidirectional Scene Text Transformer (Bi-STET), a novel bidirectional STR method with a single decoder for bidirectional text decoding. With its single decoder, Bi-STET outperforms methods that apply bidirectional decoding by using two separate decoders while also being more efficient than those methods, Furthermore, we achieve or beat state-of-the-art (SOTA) methods on all STR benchmarks with Bi-STET. Finally, we provide analyses and insights into the performance of Bi-STET.



### SampleNet: Differentiable Point Cloud Sampling
- **Arxiv ID**: http://arxiv.org/abs/1912.03663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03663v2)
- **Published**: 2019-12-08 12:06:20+00:00
- **Updated**: 2020-04-04 19:39:54+00:00
- **Authors**: Itai Lang, Asaf Manor, Shai Avidan
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: There is a growing number of tasks that work directly on point clouds. As the size of the point cloud grows, so do the computational demands of these tasks. A possible solution is to sample the point cloud first. Classic sampling approaches, such as farthest point sampling (FPS), do not consider the downstream task. A recent work showed that learning a task-specific sampling can improve results significantly. However, the proposed technique did not deal with the non-differentiability of the sampling operation and offered a workaround instead. We introduce a novel differentiable relaxation for point cloud sampling that approximates sampled points as a mixture of points in the primary input cloud. Our approximation scheme leads to consistently good results on classification and geometry reconstruction applications. We also show that the proposed sampling method can be used as a front to a point cloud registration network. This is a challenging task since sampling must be consistent across two different point clouds for a shared downstream task. In all cases, our approach outperforms existing non-learned and learned sampling alternatives. Our code is publicly available at https://github.com/itailang/SampleNet.



### Feature-aware Adaptation and Density Alignment for Crowd Counting in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1912.03672v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03672v2)
- **Published**: 2019-12-08 12:40:51+00:00
- **Updated**: 2020-10-27 08:13:50+00:00
- **Authors**: Junyu Gao, Yuan Yuan, Qi Wang
- **Comment**: accepted by IEEE T-CYB
- **Journal**: None
- **Summary**: With the development of deep neural networks, the performance of crowd counting and pixel-wise density estimation are continually being refreshed. Despite this, there are still two challenging problems in this field: 1) current supervised learning needs a large amount of training data, but collecting and annotating them is difficult; 2) existing methods can not generalize well to the unseen domain. A recently released synthetic crowd dataset alleviates these two problems. However, the domain gap between the real-world data and synthetic images decreases the models' performance. To reduce the gap, in this paper, we propose a domain-adaptation-style crowd counting method, which can effectively adapt the model from synthetic data to the specific real-world scenes. It consists of Multi-level Featureaware Adaptation (MFA) and Structured Density map Alignment (SDA). To be specific, MFA boosts the model to extract domain-invariant features from multiple layers. SDA guarantees the network outputs fine density maps with a reasonable distribution on the real domain. Finally, we evaluate the proposed method on four mainstream surveillance crowd datasets, Shanghai Tech Part B, WorldExpo'10, Mall and UCSD. Extensive experiments evidence that our approach outperforms the state-of-the-art methods for the same cross-domain counting problem.



### Detection of False Positive and False Negative Samples in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.03673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, 62-07
- **Links**: [PDF](http://arxiv.org/pdf/1912.03673v1)
- **Published**: 2019-12-08 13:04:06+00:00
- **Updated**: 2019-12-08 13:04:06+00:00
- **Authors**: Matthias Rottmann, Kira Maag, Robin Chan, Fabian Hüger, Peter Schlicht, Hanno Gottschalk
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning methods have outperformed other methods in image recognition. This has fostered imagination of potential application of deep learning technology including safety relevant applications like the interpretation of medical images or autonomous driving. The passage from assistance of a human decision maker to ever more automated systems however increases the need to properly handle the failure modes of deep learning modules. In this contribution, we review a set of techniques for the self-monitoring of machine-learning algorithms based on uncertainty quantification. In particular, we apply this to the task of semantic segmentation, where the machine learning algorithm decomposes an image according to semantic categories. We discuss false positive and false negative error modes at instance-level and review techniques for the detection of such errors that have been recently proposed by the authors. We also give an outlook on future research directions.



### Domain-adaptive Crowd Counting via High-quality Image Translation and Density Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.03677v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03677v3)
- **Published**: 2019-12-08 13:38:48+00:00
- **Updated**: 2021-10-28 07:48:40+00:00
- **Authors**: Junyu Gao, Tao Han, Qi Wang, Yuan Yuan
- **Comment**: Accepted by IEEE T-NNLS
- **Journal**: None
- **Summary**: Recently, crowd counting using supervised learning achieves a remarkable improvement. Nevertheless, most counters rely on a large amount of manually labeled data. With the release of synthetic crowd data, a potential alternative is transferring knowledge from them to real data without any manual label. However, there is no method to effectively suppress domain gaps and output elaborate density maps during the transferring. To remedy the above problems, this paper proposes a Domain-Adaptive Crowd Counting (DACC) framework, which consists of a high-quality image translation and density map reconstruction. To be specific, the former focuses on translating synthetic data to realistic images, which prompts the translation quality by segregating domain-shared/independent features and designing content-aware consistency loss. The latter aims at generating pseudo labels on real scenes to improve the prediction quality. Next, we retrain a final counter using these pseudo labels. Adaptation experiments on six real-world datasets demonstrate that the proposed method outperforms the state-of-the-art methods.



### Voxel2Mesh: 3D Mesh Model Generation from Volumetric Data
- **Arxiv ID**: http://arxiv.org/abs/1912.03681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03681v2)
- **Published**: 2019-12-08 13:53:22+00:00
- **Updated**: 2020-04-01 09:07:58+00:00
- **Authors**: Udaranga Wickramasinghe, Edoardo Remelli, Graham Knott, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: CNN-based volumetric methods that label individual voxels now dominate the field of biomedical segmentation. However, 3D surface representations are often required for proper analysis. They can be obtained by post-processing the labeled volumes which typically introduces artifacts and prevents end-to-end training. In this paper, we therefore introduce a novel architecture that goes directly from 3D image volumes to 3D surfaces without post-processing and with better accuracy than current methods. We evaluate it on Electron Microscopy and MRI brain images as well as CT liver scans. We will show that it outperforms state-of-the-art segmentation methods.



### SolarNet: A Deep Learning Framework to Map Solar Power Plants In China From Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1912.03685v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03685v2)
- **Published**: 2019-12-08 14:19:47+00:00
- **Updated**: 2019-12-10 15:27:33+00:00
- **Authors**: Xin Hou, Biao Wang, Wanqi Hu, Lei Yin, Haishan Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Renewable energy such as solar power is critical to fight the ever more serious climate change. China is the world leading installer of solar panel and numerous solar power plants were built. In this paper, we proposed a deep learning framework named SolarNet which is designed to perform semantic segmentation on large scale satellite imagery data to detect solar farms. SolarNet has successfully mapped 439 solar farms in China, covering near 2000 square kilometers, equivalent to the size of whole Shenzhen city or two and a half of New York city. To the best of our knowledge, it is the first time that we used deep learning to reveal the locations and sizes of solar farms in China, which could provide insights for solar power companies, market analysts and the government.



### Minimum Class Confusion for Versatile Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1912.03699v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03699v3)
- **Published**: 2019-12-08 15:31:14+00:00
- **Updated**: 2020-08-10 15:49:25+00:00
- **Authors**: Ying Jin, Ximei Wang, Mingsheng Long, Jianmin Wang
- **Comment**: Accepted by ECCV2020
- **Journal**: None
- **Summary**: There are a variety of Domain Adaptation (DA) scenarios subject to label sets and domain configurations, including closed-set and partial-set DA, as well as multi-source and multi-target DA. It is notable that existing DA methods are generally designed only for a specific scenario, and may underperform for scenarios they are not tailored to. To this end, this paper studies Versatile Domain Adaptation (VDA), where one method can handle several different DA scenarios without any modification. Towards this goal, a more general inductive bias other than the domain alignment should be explored. We delve into a missing piece of existing methods: class confusion, the tendency that a classifier confuses the predictions between the correct and ambiguous classes for target examples, which is common in different DA scenarios. We uncover that reducing such pairwise class confusion leads to significant transfer gains. With this insight, we propose a general loss function: Minimum Class Confusion (MCC). It can be characterized as (1) a non-adversarial DA method without explicitly deploying domain alignment, enjoying faster convergence speed; (2) a versatile approach that can handle four existing scenarios: Closed-Set, Partial-Set, Multi-Source, and Multi-Target DA, outperforming the state-of-the-art methods in these scenarios, especially on one of the largest and hardest datasets to date (7.3% on DomainNet). Its versatility is further justified by two scenarios proposed in this paper: Multi-Source Partial DA and Multi-Target Partial DA. In addition, it can also be used as a general regularizer that is orthogonal and complementary to a variety of existing DA methods, accelerating convergence and pushing these readily competitive methods to stronger ones. Code is available at https://github.com/thuml/Versatile-Domain-Adaptation.



### ICDAR 2019 Competition on Image Retrieval for Historical Handwritten Documents
- **Arxiv ID**: http://arxiv.org/abs/1912.03713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03713v1)
- **Published**: 2019-12-08 16:48:27+00:00
- **Updated**: 2019-12-08 16:48:27+00:00
- **Authors**: Vincent Christlein, Anguelos Nicolaou, Mathias Seuret, Dominique Stutzmann, Andreas Maier
- **Comment**: None
- **Journal**: None
- **Summary**: This competition investigates the performance of large-scale retrieval of historical document images based on writing style. Based on large image data sets provided by cultural heritage institutions and digital libraries, providing a total of 20 000 document images representing about 10 000 writers, divided in three types: writers of (i) manuscript books, (ii) letters, (iii) charters and legal documents. We focus on the task of automatic image retrieval to simulate common scenarios of humanities research, such as writer retrieval. The most teams submitted traditional methods not using deep learning techniques. The competition results show that a combination of methods is outperforming single methods. Furthermore, letters are much more difficult to retrieve than manuscripts.



### VideoDG: Generalizing Temporal Relations in Videos to Novel Domains
- **Arxiv ID**: http://arxiv.org/abs/1912.03716v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03716v2)
- **Published**: 2019-12-08 17:13:51+00:00
- **Updated**: 2021-09-17 01:57:23+00:00
- **Authors**: Zhiyu Yao, Yunbo Wang, Jianmin Wang, Philip S. Yu, Mingsheng Long
- **Comment**: Accepted by IEEE TPAMI, 2021. Code: https://github.com/thuml/VideoDG
- **Journal**: None
- **Summary**: This paper introduces video domain generalization where most video classification networks degenerate due to the lack of exposure to the target domains of divergent distributions. We observe that the global temporal features are less generalizable, due to the temporal domain shift that videos from other unseen domains may have an unexpected absence or misalignment of the temporal relations. This finding has motivated us to solve video domain generalization by effectively learning the local-relation features of different timescales that are more generalizable, and exploiting them along with the global-relation features to maintain the discriminability. This paper presents the VideoDG framework with two technical contributions. The first is a new deep architecture named the Adversarial Pyramid Network, which improves the generalizability of video features by capturing the local-relation, global-relation, and cross-relation features progressively. On the basis of pyramid features, the second contribution is a new and robust approach of adversarial data augmentation that can bridge different video domains by improving the diversity and quality of augmented data. We construct three video domain generalization benchmarks in which domains are divided according to different datasets, different consequences of actions, or different camera views, respectively. VideoDG consistently outperforms the combinations of previous video classification models and existing domain generalization methods on all benchmarks.



### Dually Supervised Feature Pyramid for Object Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.03730v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03730v2)
- **Published**: 2019-12-08 18:06:06+00:00
- **Updated**: 2019-12-13 22:28:51+00:00
- **Authors**: Fan Yang, Cheng Lu, Yandong Guo, Longin Jan Latecki, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Feature pyramid architecture has been broadly adopted in object detection and segmentation to deal with multi-scale problem. However, in this paper we show that the capacity of the architecture has not been fully explored due to the inadequate utilization of the supervision information. Such insufficient utilization is caused by the supervision signal degradation in back propagation. Thus inspired, we propose a dually supervised method, named dually supervised FPN (DSFPN), to enhance the supervision signal when training the feature pyramid network (FPN). In particular, DSFPN is constructed by attaching extra prediction (i.e., detection or segmentation) heads to the bottom-up subnet of FPN. Hence, the features can be optimized by the additional heads before being forwarded to subsequent networks. Further, the auxiliary heads can serve as a regularization term to facilitate the model training. In addition, to strengthen the capability of the detection heads in DSFPN for handling two inhomogeneous tasks, i.e., classification and regression, the originally shared hidden feature space is separated by decoupling classification and regression subnets. To demonstrate the generalizability, effectiveness, and efficiency of the proposed method, DSFPN is integrated into four representative detectors (Faster RCNN, Mask RCNN, Cascade RCNN, and Cascade Mask RCNN) and assessed on the MS COCO dataset. Promising precision improvement, state-of-the-art performance, and negligible additional computational cost are demonstrated through extensive experiments. Code will be provided.



### Universal Material Translator: Towards Spoof Fingerprint Generalization
- **Arxiv ID**: http://arxiv.org/abs/1912.03737v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03737v1)
- **Published**: 2019-12-08 18:32:05+00:00
- **Updated**: 2019-12-08 18:32:05+00:00
- **Authors**: Rohit Gajawada, Additya Popli, Tarang Chugh, Anoop Namboodiri, Anil K. Jain
- **Comment**: 8 pages, 6 figures, conference
- **Journal**: IAPR International Conference on Biometrics (ICB), 2019
- **Summary**: Spoof detectors are classifiers that are trained to distinguish spoof fingerprints from bonafide ones. However, state of the art spoof detectors do not generalize well on unseen spoof materials. This study proposes a style transfer based augmentation wrapper that can be used on any existing spoof detector and can dynamically improve the robustness of the spoof detection system on spoof materials for which we have very low data. Our method is an approach for synthesizing new spoof images from a few spoof examples that transfers the style or material properties of the spoof examples to the content of bonafide fingerprints to generate a larger number of examples to train the classifier on. We demonstrate the effectiveness of our approach on materials in the publicly available LivDet 2015 dataset and show that the proposed approach leads to robustness to fingerprint spoofs of the target material.



