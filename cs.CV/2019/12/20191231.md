# Arxiv Papers in cs.CV on 2019-12-31
### Segmentation-Aware and Adaptive Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.00989v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00989v1)
- **Published**: 2019-12-31 04:31:37+00:00
- **Updated**: 2019-12-31 04:31:37+00:00
- **Authors**: Kuo Wang, Ajay Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Iris recognition has emerged as one of the most accurate and convenient biometric for the human identification and has been increasingly employed in a wide range of e-security applications. The quality of iris images acquired at-a-distance or under less constrained imaging environments is known to degrade the iris matching accuracy. The periocular information is inherently embedded in such iris images and can be exploited to assist in the iris recognition under such non-ideal scenarios. Our analysis of such iris templates also indicates significant degradation and reduction in the region of interest, where the iris recognition can benefit from a similarity distance that can consider importance of different binary bits, instead of the direct use of Hamming distance in the literature. Periocular information can be dynamically reinforced, by incorporating the differences in the effective area of available iris regions, for more accurate iris recognition. This paper presents such a segmentation-assisted adaptive framework for more accurate less-constrained iris recognition. The effectiveness of this framework is evaluated on three publicly available iris databases using within-dataset and cross-dataset performance evaluation and validates the merit of the proposed iris recognition framework.



### Deep Learning on Image Denoising: An overview
- **Arxiv ID**: http://arxiv.org/abs/1912.13171v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13171v4)
- **Published**: 2019-12-31 05:03:57+00:00
- **Updated**: 2020-08-03 06:55:36+00:00
- **Authors**: Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, Chia-Wen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques have received much attention in the area of image denoising. However, there are substantial differences in the various types of deep learning methods dealing with image denoising. Specifically, discriminative learning based on deep learning can ably address the issue of Gaussian noise. Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising. In this paper, we offer a comparative study of deep techniques in image denoising. We first classify the deep convolutional neural networks (CNNs) for additive white noisy images; the deep CNNs for real noisy images; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analysis. Finally, we point out some potential challenges and directions of future research.



### Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1912.13179v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.13179v1)
- **Published**: 2019-12-31 05:32:02+00:00
- **Updated**: 2019-12-31 05:32:02+00:00
- **Authors**: Sajjad Abbasi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi
- **Comment**: six pages, 6 figures
- **Journal**: None
- **Summary**: Knowledge distillation (KD) is a new method for transferring knowledge of a structure under training to another one. The typical application of KD is in the form of learning a small model (named as a student) by soft labels produced by a complex model (named as a teacher). Due to the novel idea introduced in KD, recently, its notion is used in different methods such as compression and processes that are going to enhance the model accuracy. Although different techniques are proposed in the area of KD, there is a lack of a model to generalize KD techniques. In this paper, various studies in the scope of KD are investigated and analyzed to build a general model for KD. All the methods and techniques in KD can be summarized through the proposed model. By utilizing the proposed model, different methods in KD are better investigated and explored. The advantages and disadvantages of different approaches in KD can be better understood and develop a new strategy for KD can be possible. Using the proposed model, different KD methods are represented in an abstract view.



### Diversity Transfer Network for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.13182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13182v1)
- **Published**: 2019-12-31 05:44:38+00:00
- **Updated**: 2019-12-31 05:44:38+00:00
- **Authors**: Mengting Chen, Yuxin Fang, Xinggang Wang, Heng Luo, Yifeng Geng, Xinyu Zhang, Chang Huang, Wenyu Liu, Bo Wang
- **Comment**: 9 pages, 3 figures, AAAI 2020
- **Journal**: None
- **Summary**: Few-shot learning is a challenging task that aims at training a classifier for unseen classes with only a few training examples. The main difficulty of few-shot learning lies in the lack of intra-class diversity within insufficient training samples. To alleviate this problem, we propose a novel generative framework, Diversity Transfer Network (DTN), that learns to transfer latent diversities from known categories and composite them with support features to generate diverse samples for novel categories in feature space. The learning problem of the sample generation (i.e., diversity transfer) is solved via minimizing an effective meta-classification loss in a single-stage network, instead of the generative loss in previous works.   Besides, an organized auxiliary task co-training over known categories is proposed to stabilize the meta-training process of DTN. We perform extensive experiments and ablation studies on three datasets, i.e., \emph{mini}ImageNet, CIFAR100 and CUB. The results show that DTN, with single-stage training and faster convergence speed, obtains the state-of-the-art results among the feature generation based few-shot learning methods. Code and supplementary material are available at: \texttt{https://github.com/Yuxin-CV/DTN}



### Modeling Neural Architecture Search Methods for Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.13183v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13183v1)
- **Published**: 2019-12-31 05:51:03+00:00
- **Updated**: 2019-12-31 05:51:03+00:00
- **Authors**: Emad Malekhosseini, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: There are many research works on the designing of architectures for the deep neural networks (DNN), which are named neural architecture search (NAS) methods. Although there are many automatic and manual techniques for NAS problems, there is no unifying model in which these NAS methods can be explored and compared. In this paper, we propose a general abstraction model for NAS methods. By using the proposed framework, it is possible to compare different design approaches for categorizing and identifying critical areas of interest in designing DNN architectures. Also, under this framework, different methods in the NAS area are summarized; hence a better view of their advantages and disadvantages is possible.



### PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.13192v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13192v2)
- **Published**: 2019-12-31 06:34:10+00:00
- **Updated**: 2021-04-09 06:37:15+00:00
- **Authors**: Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by CVPR 2020. arXiv admin note: substantial text overlap
  with arXiv:2102.00463
- **Journal**: None
- **Summary**: We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Specifically, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the high-quality 3D proposals generated by the voxel CNN, the RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive fields. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object confidences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at https://github.com/open-mmlab/OpenPCDet.



### Comparison of object detection methods for crop damage assessment using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1912.13199v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13199v3)
- **Published**: 2019-12-31 06:54:48+00:00
- **Updated**: 2020-04-22 00:32:32+00:00
- **Authors**: Ali HamidiSepehr, Seyed Vahid Mirnezami, Jason K. Ward
- **Comment**: None
- **Journal**: None
- **Summary**: Severe weather events can cause large financial losses to farmers. Detailed information on the location and severity of damage will assist farmers, insurance companies, and disaster response agencies in making wise post-damage decisions. The goal of this study was a proof-of-concept to detect damaged crop areas from aerial imagery using computer vision and deep learning techniques. A specific objective was to compare existing object detection algorithms to determine which was best suited for crop damage detection. Two modes of crop damage common in maize (corn) production were simulated: stalk lodging at the lowest ear and stalk lodging at ground level. Simulated damage was used to create a training and analysis data set. An unmanned aerial system (UAS) equipped with a RGB camera was used for image acquisition. Three popular object detectors (Faster R-CNN, YOLOv2, and RetinaNet) were assessed for their ability to detect damaged regions in a field. Average precision was used to compare object detectors. YOLOv2 and RetinaNet were able to detect crop damage across multiple late-season growth stages. Faster R-CNN was not successful as the other two advanced detectors. Detecting crop damage at later growth stages was more difficult for all tested object detectors. Weed pressure in simulated damage plots and increased target density added additional complexity.



### AdderNet: Do We Really Need Multiplications in Deep Learning?
- **Arxiv ID**: http://arxiv.org/abs/1912.13200v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13200v6)
- **Published**: 2019-12-31 06:56:47+00:00
- **Updated**: 2021-07-01 04:46:58+00:00
- **Authors**: Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu
- **Comment**: New version in arXiv:2105.14202
- **Journal**: CVPR 2020
- **Summary**: Compared with cheap addition operation, multiplication operation is of much higher computation complexity. The widely-used convolutions in deep neural networks are exactly cross-correlation to measure the similarity between input feature and convolution filters, which involves massive multiplications between float values. In this paper, we present adder networks (AdderNets) to trade these massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs. In AdderNets, we take the $\ell_1$-norm distance between filters and input feature as the output response. The influence of this new similarity measure on the optimization of neural network have been thoroughly analyzed. To achieve a better performance, we develop a special back-propagation approach for AdderNets by investigating the full-precision gradient. We then propose an adaptive learning rate strategy to enhance the training procedure of AdderNets according to the magnitude of each neuron's gradient. As a result, the proposed AdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50 on the ImageNet dataset without any multiplication in convolution layer. The codes are publicly available at: https://github.com/huaweinoah/AdderNet.



### Image Seam-Carving by Controlling Positional Distribution of Seams
- **Arxiv ID**: http://arxiv.org/abs/1912.13214v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13214v1)
- **Published**: 2019-12-31 08:17:55+00:00
- **Updated**: 2019-12-31 08:17:55+00:00
- **Authors**: Mahdi Ahmadi, Nader Karimi, Shadrokh Samavi
- **Comment**: Five pages, 4 figures
- **Journal**: None
- **Summary**: Image retargeting is a new image processing task that renders the change of aspect ratio in images. One of the most famous image-retargeting algorithms is seam-carving. Although seam-carving is fast and straightforward, it usually distorts the images. In this paper, we introduce a new seam-carving algorithm that not only has the simplicity of the original seam-carving but also lacks the usual unwanted distortion existed in the original method. The positional distribution of seams is introduced. We show that the proposed method outperforms the original seam-carving in terms of retargeted image quality assessment and seam coagulation measures.



### Learning to Infer User Interface Attributes from Images
- **Arxiv ID**: http://arxiv.org/abs/1912.13243v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.13243v1)
- **Published**: 2019-12-31 09:45:59+00:00
- **Updated**: 2019-12-31 09:45:59+00:00
- **Authors**: Philippe Schlattner, Pavol Bielik, Martin Vechev
- **Comment**: None
- **Journal**: None
- **Summary**: We explore a new domain of learning to infer user interface attributes that helps developers automate the process of user interface implementation. Concretely, given an input image created by a designer, we learn to infer its implementation which when rendered, looks visually the same as the input image. To achieve this, we take a black box rendering engine and a set of attributes it supports (e.g., colors, border radius, shadow or text properties), use it to generate a suitable synthetic training dataset, and then train specialized neural models to predict each of the attribute values. To improve pixel-level accuracy, we additionally use imitation learning to train a neural policy that refines the predicted attribute values by learning to compute the similarity of the original and rendered images in their attribute space, rather than based on the difference of pixel values. We instantiate our approach to the task of inferring Android Button attribute values and achieve 92.5% accuracy on a dataset consisting of real-world Google Play Store applications.



### Scalable NAS with Factorizable Architectural Parameters
- **Arxiv ID**: http://arxiv.org/abs/1912.13256v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.13256v2)
- **Published**: 2019-12-31 10:26:56+00:00
- **Updated**: 2020-09-22 18:47:42+00:00
- **Authors**: Lanfei Wang, Lingxi Xie, Tianyi Zhang, Jun Guo, Qi Tian
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) is an emerging topic in machine learning and computer vision. The fundamental ideology of NAS is using an automatic mechanism to replace manual designs for exploring powerful network architectures. One of the key factors of NAS is to scale-up the search space, e.g., increasing the number of operators, so that more possibilities are covered, but existing search algorithms often get lost in a large number of operators. For avoiding huge computing and competition among similar operators in the same pool, this paper presents a scalable algorithm by factorizing a large set of candidate operators into smaller subspaces. As a practical example, this allows us to search for effective activation functions along with the regular operators including convolution, pooling, skip-connect, etc. With a small increase in search costs and no extra costs in re-training, we find interesting architectures that were not explored before, and achieve state-of-the-art performance on CIFAR10 and ImageNet, two standard image classification benchmarks.



### Unseen Face Presentation Attack Detection Using Class-Specific Sparse One-Class Multiple Kernel Fusion Regression
- **Arxiv ID**: http://arxiv.org/abs/1912.13276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13276v1)
- **Published**: 2019-12-31 11:53:20+00:00
- **Updated**: 2019-12-31 11:53:20+00:00
- **Authors**: Shervin Rahimzadeh Arashloo
- **Comment**: None
- **Journal**: None
- **Summary**: The paper addresses face presentation attack detection in the challenging conditions of an unseen attack scenario where the system is exposed to novel presentation attacks that were not present in the training step. For this purpose, a pure one-class face presentation attack detection approach based on kernel regression is developed which only utilises bona fide (genuine) samples for training. In the context of the proposed approach, a number of innovations, including multiple kernel fusion, client-specific modelling, sparse regularisation and probabilistic modelling of score distributions are introduced to improve the efficacy of the method. The results of experimental evaluations conducted on the OULU-NPU, Replay-Mobile, Replay-Attack and MSU-MFSD datasets illustrate that the proposed method compares very favourably with other methods operating in an unseen attack detection scenario while achieving very competitive performance to multi-class methods (benefiting from presentation attack data for training) despite using only bona fide samples for training.



### Automatic segmentation and determining radiodensity of the liver in a large-scale CT database
- **Arxiv ID**: http://arxiv.org/abs/1912.13290v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1912.13290v1)
- **Published**: 2019-12-31 12:41:05+00:00
- **Updated**: 2019-12-31 12:41:05+00:00
- **Authors**: N. S. Kulberg, A. B. Elizarov, V. P. Novik, V. A. Gombolevsky, A. P. Gonchar, A. L. Alliua, V. Yu. Bosin, A. V. Vladzymyrsky, S. P. Morozov
- **Comment**: 16 pages, 8 figures
- **Journal**: None
- **Summary**: This study proposes an automatic technique for liver segmentation in computed tomography (CT) images. Localization of the liver volume is based on the correlation with an optimized set of liver templates developed by the authors that allows clear geometric interpretation. Radiodensity values are calculated based on the boundaries of the segmented liver, which allows identifying liver abnormalities. The performance of the technique was evaluated on 700 CT images from dataset of the Unified Radiological Information System (URIS) of Moscow. Despite the decrease in accuracy, the technique is applicable to CT volumes with a partially visible region of the liver. The technique can be used to process CT images obtained in various patient positions in a wide range of exposition parameters. It is capable in dealing with low dose CT scans in real large-scale medical database with over 1 million of studies.



### Microlens array grid estimation, light field decoding, and calibration
- **Arxiv ID**: http://arxiv.org/abs/1912.13298v1
- **DOI**: 10.1109/tci.2020.2964257
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13298v1)
- **Published**: 2019-12-31 13:27:13+00:00
- **Updated**: 2019-12-31 13:27:13+00:00
- **Authors**: Maximilian Schambach, Fernando Puente León
- **Comment**: \copyright 2020 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: IEEE Transactions on Computational Imaging, vol. 6, pp. 591-603,
  2020
- **Summary**: We quantitatively investigate multiple algorithms for microlens array grid estimation for microlens array-based light field cameras. Explicitly taking into account natural and mechanical vignetting effects, we propose a new method for microlens array grid estimation that outperforms the ones previously discussed in the literature. To quantify the performance of the algorithms, we propose an evaluation pipeline utilizing application-specific ray-traced white images with known microlens positions. Using a large dataset of synthesized white images, we thoroughly compare the performance of the different estimation algorithms. As an example, we apply our results to the decoding and calibration of light fields taken with a Lytro Illum camera. We observe that decoding as well as calibration benefit from a more accurate, vignetting-aware grid estimation, especially in peripheral subapertures of the light field.



### Volumetric Lung Nodule Segmentation using Adaptive ROI with Multi-View Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.13335v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.13335v2)
- **Published**: 2019-12-31 15:03:18+00:00
- **Updated**: 2020-02-03 10:57:24+00:00
- **Authors**: Muhammad Usman, Byoung-Dai Lee, Shi Sub Byon, Sung Hyun Kim, Byung-ilLee
- **Comment**: The manuscript is currently under review and copyright shall be
  transferred to the publisher upon acceptance
- **Journal**: None
- **Summary**: Accurate quantification of pulmonary nodules can greatly assist the early diagnosis of lung cancer, which can enhance patient survival possibilities. A number of nodule segmentation techniques have been proposed, however, all of the existing techniques rely on radiologist 3-D volume of interest (VOI) input or use the constant region of interest (ROI) and only investigate the presence of nodule voxels within the given VOI. Such approaches restrain the solutions to investigate the nodule presence outside the given VOI and also include the redundant structures into VOI, which may lead to inaccurate nodule segmentation. In this work, a novel semi-automated approach for 3-D segmentation of nodule in volumetric computerized tomography (CT) lung scans has been proposed. The proposed technique can be segregated into two stages, at the first stage, it takes a 2-D ROI containing the nodule as input and it performs patch-wise investigation along the axial axis with a novel adaptive ROI strategy. The adaptive ROI algorithm enables the solution to dynamically select the ROI for the surrounding slices to investigate the presence of nodule using deep residual U-Net architecture. The first stage provides the initial estimation of nodule which is further utilized to extract the VOI. At the second stage, the extracted VOI is further investigated along the coronal and sagittal axis with two different networks and finally, all the estimated masks are fed into the consensus module to produce the final volumetric segmentation of nodule. The proposed approach has been rigorously evaluated on the LIDC dataset, which is the largest publicly available dataset. The result suggests that the approach is significantly robust and accurate as compared to the previous state of the art techniques.



### Learning 3D Human Shape and Pose from Dense Body Parts
- **Arxiv ID**: http://arxiv.org/abs/1912.13344v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13344v2)
- **Published**: 2019-12-31 15:09:51+00:00
- **Updated**: 2020-12-06 10:46:53+00:00
- **Authors**: Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, Zhenan Sun
- **Comment**: Journal article accepted by IEEE TPAMI. Project page:
  https://hongwenzhang.github.io/dense2mesh
- **Journal**: None
- **Summary**: Reconstructing 3D human shape and pose from monocular images is challenging despite the promising results achieved by the most recent learning-based methods. The commonly occurred misalignment comes from the facts that the mapping from images to the model space is highly non-linear and the rotation-based pose representation of body models is prone to result in the drift of joint positions. In this work, we investigate learning 3D human shape and pose from dense correspondences of body parts and propose a Decompose-and-aggregate Network (DaNet) to address these issues. DaNet adopts the dense correspondence maps, which densely build a bridge between 2D pixels and 3D vertices, as intermediate representations to facilitate the learning of 2D-to-3D mapping. The prediction modules of DaNet are decomposed into one global stream and multiple local streams to enable global and fine-grained perceptions for the shape and pose predictions, respectively. Messages from local streams are further aggregated to enhance the robust prediction of the rotation-based poses, where a position-aided rotation feature refinement strategy is proposed to exploit spatial relationships between body joints. Moreover, a Part-based Dropout (PartDrop) strategy is introduced to drop out dense information from intermediate representations during training, encouraging the network to focus on more complementary body parts as well as neighboring position features. The efficacy of the proposed method is validated on both indoor and real-world datasets including Human3.6M, UP3D, COCO, and 3DPW, showing that our method could significantly improve the reconstruction performance in comparison with previous state-of-the-art methods. Our code is publicly available at https://hongwenzhang.github.io/dense2mesh .



### Morphology-Agnostic Visual Robotic Control
- **Arxiv ID**: http://arxiv.org/abs/1912.13360v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13360v1)
- **Published**: 2019-12-31 15:45:10+00:00
- **Updated**: 2019-12-31 15:45:10+00:00
- **Authors**: Brian Yang, Dinesh Jayaraman, Glen Berseth, Alexei Efros, Sergey Levine
- **Comment**: None
- **Journal**: None
- **Summary**: Existing approaches for visuomotor robotic control typically require characterizing the robot in advance by calibrating the camera or performing system identification. We propose MAVRIC, an approach that works with minimal prior knowledge of the robot's morphology, and requires only a camera view containing the robot and its environment and an unknown control interface. MAVRIC revolves around a mutual information-based method for self-recognition, which discovers visual "control points" on the robot body within a few seconds of exploratory interaction, and these control points in turn are then used for visual servoing. MAVRIC can control robots with imprecise actuation, no proprioceptive feedback, unknown morphologies including novel tools, unknown camera poses, and even unsteady handheld cameras. We demonstrate our method on visually-guided 3D point reaching, trajectory following, and robot-to-robot imitation.



### Learning Wavefront Coding for Extended Depth of Field Imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.13423v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1912.13423v2)
- **Published**: 2019-12-31 17:00:09+00:00
- **Updated**: 2020-05-25 18:59:13+00:00
- **Authors**: Ugur Akpinar, Erdem Sahin, Monjurul Meem, Rajesh Menon, Atanas Gotchev
- **Comment**: None
- **Journal**: None
- **Summary**: Depth of field is an important factor of imaging systems that highly affects the quality of the acquired spatial information. Extended depth of field (EDoF) imaging is a challenging ill-posed problem and has been extensively addressed in the literature. We propose a computational imaging approach for EDoF, where we employ wavefront coding via a diffractive optical element (DOE) and we achieve deblurring through a convolutional neural network. Thanks to the end-to-end differentiable modeling of optical image formation and computational post-processing, we jointly optimize the optical design, i.e., DOE, and the deblurring through standard gradient descent methods. Based on the properties of the underlying refractive lens and the desired EDoF range, we provide an analytical expression for the search space of the DOE, which is instrumental in the convergence of the end-to-end network. We achieve superior EDoF imaging performance compared to the state of the art, where we demonstrate results with minimal artifacts in various scenarios, including deep 3D scenes and broadband imaging.



### BIRL: Benchmark on Image Registration methods with Landmark validation
- **Arxiv ID**: http://arxiv.org/abs/1912.13452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.13452v2)
- **Published**: 2019-12-31 17:50:12+00:00
- **Updated**: 2020-01-21 09:56:17+00:00
- **Authors**: Jiri Borovec
- **Comment**: None
- **Journal**: None
- **Summary**: This report presents a generic image registration benchmark with automatic evaluation using landmark annotations. The key features of the BIRL framework are: easily extendable, performance evaluation, parallel experimentation, simple visualisations, experiment's time-out limit, resuming unfinished experiments. From the research practice, we identified and focused on these two main use-cases: (a) comparison of user's (newly developed) method with some State-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA methods on user's custom dataset (which should contain landmark annotation). Moreover, we present an integration of several standard image registration methods aiming at biomedical imaging into the BIRL framework. This report also contains experimental results of these SOTA methods on the CIMA dataset, which is a dataset of Whole Slice Imaging (WSI) from histology/pathology containing several multi-stain tissue samples from three tissue kinds. Source and results: https://borda.github.io/BIRL



### FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping
- **Arxiv ID**: http://arxiv.org/abs/1912.13457v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13457v3)
- **Published**: 2019-12-31 17:57:46+00:00
- **Updated**: 2020-09-15 07:43:58+00:00
- **Authors**: Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen
- **Comment**: Accepted to CVPR 2020 (Oral), generated dataset and project webpage:
  lingzhili.com/FaceShifterPage/
- **Journal**: None
- **Summary**: In this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. Unlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, our framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. We propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. To address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. Extensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods.



### Face X-ray for More General Face Forgery Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.13458v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13458v2)
- **Published**: 2019-12-31 17:57:56+00:00
- **Updated**: 2020-04-19 02:22:40+00:00
- **Authors**: Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, Baining Guo
- **Comment**: Accepted to CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: In this paper we propose a novel image representation called face X-ray for detecting forgery in face images. The face X-ray of an input face image is a greyscale image that reveals whether the input image can be decomposed into the blending of two images from different sources. It does so by showing the blending boundary for a forged image and the absence of blending for a real image. We observe that most existing face manipulation methods share a common step: blending the altered face into an existing background image. For this reason, face X-ray provides an effective way for detecting forgery generated by most existing face manipulation algorithms. Face X-ray is general in the sense that it only assumes the existence of a blending step and does not rely on any knowledge of the artifacts associated with a specific face manipulation technique. Indeed, the algorithm for computing face X-ray can be trained without fake images generated by any of the state-of-the-art face manipulation methods. Extensive experiments show that face X-ray remains effective when applied to forgery generated by unseen face manipulation techniques, while most existing face forgery detection or deepfake detection algorithms experience a significant performance drop.



### GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for Object Grasping
- **Arxiv ID**: http://arxiv.org/abs/1912.13470v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.13470v2)
- **Published**: 2019-12-31 18:15:11+00:00
- **Updated**: 2020-01-01 03:49:58+00:00
- **Authors**: Hao-Shu Fang, Chenxi Wang, Minghao Gou, Cewu Lu
- **Comment**: Report for our recent work
- **Journal**: None
- **Summary**: Object grasping is critical for many applications, which is also a challenging computer vision problem. However, for the clustered scene, current researches suffer from the problems of insufficient training data and the lacking of evaluation benchmarks. In this work, we contribute a large-scale grasp pose detection dataset with a unified evaluation system. Our dataset contains 87,040 RGBD images with over 370 million grasp poses. Meanwhile, our evaluation system directly reports whether a grasping is successful or not by analytic computation, which is able to evaluate any kind of grasp poses without exhausted labeling pose ground-truth. We conduct extensive experiments to show that our dataset and evaluation system can align well with real-world experiments. Our dataset, source code and models will be made publicly available.



### OneGAN: Simultaneous Unsupervised Learning of Conditional Image Generation, Foreground Segmentation, and Fine-Grained Clustering
- **Arxiv ID**: http://arxiv.org/abs/1912.13471v2
- **DOI**: 10.1007/978-3-030-58574-7_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.13471v2)
- **Published**: 2019-12-31 18:15:58+00:00
- **Updated**: 2020-07-12 12:29:00+00:00
- **Authors**: Yaniv Benny, Lior Wolf
- **Comment**: To be published in the European Conference on Computer Vision (ECCV)
  2020
- **Journal**: None
- **Summary**: We present a method for simultaneously learning, in an unsupervised manner, (i) a conditional image generator, (ii) foreground extraction and segmentation, (iii) clustering into a two-level class hierarchy, and (iv) object removal and background completion, all done without any use of annotation. The method combines a Generative Adversarial Network and a Variational Auto-Encoder, with multiple encoders, generators and discriminators, and benefits from solving all tasks at once. The input to the training scheme is a varied collection of unlabeled images from the same domain, as well as a set of background images without a foreground object. In addition, the image generator can mix the background from one image, with a foreground that is conditioned either on that of a second image or on the index of a desired cluster. The method obtains state of the art results in comparison to the literature methods, when compared to the current state of the art in each of the tasks.



### Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.13503v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.13503v4)
- **Published**: 2019-12-31 18:52:32+00:00
- **Updated**: 2020-07-31 00:44:06+00:00
- **Authors**: Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, Jitendra Malik
- **Comment**: In ECCV 2020 (Spotlight). For more, see project website and code at
  http://sidetuning.berkeley.edu
- **Journal**: None
- **Summary**: When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than starting from randomly initialized weights. Adaptation can be useful in cases when training data is scarce, when a single learner needs to perform multiple tasks, or when one wishes to encode priors in the network. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others.   In this paper, we propose a straightforward alternative: side-tuning. Side-tuning adapts a pre-trained network by training a lightweight "side" network that is fused with the (unchanged) pre-trained network via summation. This simple method works as well as or better than existing solutions and it resolves some of the basic issues with fine-tuning, fixed features, and other common approaches. In particular, side-tuning is less prone to overfitting, is asymptotically consistent, and does not suffer from catastrophic forgetting in incremental learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.



### Quantum Adversarial Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/2001.00030v1
- **DOI**: 10.1103/PhysRevResearch.2.033212
- **Categories**: **quant-ph**, cond-mat.dis-nn, cond-mat.str-el, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00030v1)
- **Published**: 2019-12-31 19:00:12+00:00
- **Updated**: 2019-12-31 19:00:12+00:00
- **Authors**: Sirui Lu, Lu-Ming Duan, Dong-Ling Deng
- **Comment**: 22 pages, 16 figures, 5 tables
- **Journal**: Phys. Rev. Research 2, 033212 (2020)
- **Summary**: Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.



### Non-rigid Registration Method between 3D CT Liver Data and 2D Ultrasonic Images based on Demons Model
- **Arxiv ID**: http://arxiv.org/abs/2001.00035v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2001.00035v1)
- **Published**: 2019-12-31 19:01:04+00:00
- **Updated**: 2019-12-31 19:01:04+00:00
- **Authors**: Shuo Huang, Ke wu, Xiaolin Meng, Cheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: The non-rigid registration between CT data and ultrasonic images of liver can facilitate the diagnosis and treatment, which has been widely studied in recent years. To improve the registration accuracy of the Demons model on the non-rigid registration between 3D CT liver data and 2D ultrasonic images, a novel boundary extraction and enhancement method based on radial directional local intuitionistic fuzzy entropy in the polar coordinates has been put forward, and a new registration workflow has been provided. Experiments show that our method can acquire high-accuracy registration results. Experiments also show that the accuracy of the results of our method is higher than that of the original Demons method and the Demons method using simulated ultrasonic image by Field II. The operation time of our registration workflow is about 30 seconds, and it can be used in the surgery.



### HMM-guided frame querying for bandwidth-constrained video search
- **Arxiv ID**: http://arxiv.org/abs/2001.00057v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.00057v1)
- **Published**: 2019-12-31 19:54:35+00:00
- **Updated**: 2019-12-31 19:54:35+00:00
- **Authors**: Bhairav Chidambaram, Mason McGill, Pietro Perona
- **Comment**: 4 pages, 5 figures
- **Journal**: None
- **Summary**: We design an agent to search for frames of interest in video stored on a remote server, under bandwidth constraints. Using a convolutional neural network to score individual frames and a hidden Markov model to propagate predictions across frames, our agent accurately identifies temporal regions of interest based on sparse, strategically sampled frames. On a subset of the ImageNet-VID dataset, we demonstrate that using a hidden Markov model to interpolate between frame scores allows requests of 98% of frames to be omitted, without compromising frame-of-interest classification accuracy.



### privGAN: Protecting GANs from membership inference attacks at low cost
- **Arxiv ID**: http://arxiv.org/abs/2001.00071v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/2001.00071v4)
- **Published**: 2019-12-31 20:47:21+00:00
- **Updated**: 2020-12-13 18:27:26+00:00
- **Authors**: Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Juan Lavista Ferres
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have made releasing of synthetic images a viable approach to share data without releasing the original dataset. It has been shown that such synthetic data can be used for a variety of downstream tasks such as training classifiers that would otherwise require the original dataset to be shared. However, recent work has shown that the GAN models and their synthetically generated data can be used to infer the training set membership by an adversary who has access to the entire dataset and some auxiliary information. Current approaches to mitigate this problem (such as DPGAN) lead to dramatically poorer generated sample quality than the original non--private GANs. Here we develop a new GAN architecture (privGAN), where the generator is trained not only to cheat the discriminator but also to defend membership inference attacks. The new mechanism provides protection against this mode of attack while leading to negligible loss in downstream performances. In addition, our algorithm has been shown to explicitly prevent overfitting to the training set, which explains why our protection is so effective. The main contributions of this paper are: i) we propose a novel GAN architecture that can generate synthetic data in a privacy preserving manner without additional hyperparameter tuning and architecture selection, ii) we provide a theoretical understanding of the optimal solution of the privGAN loss function, iii) we demonstrate the effectiveness of our model against several white and black--box attacks on several benchmark datasets, iv) we demonstrate on three common benchmark datasets that synthetic images generated by privGAN lead to negligible loss in downstream performance when compared against non--private GANs.



### Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition
- **Arxiv ID**: http://arxiv.org/abs/2001.05833v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05833v1)
- **Published**: 2019-12-31 23:30:27+00:00
- **Updated**: 2019-12-31 23:30:27+00:00
- **Authors**: Yi Zhang, Chong Wang, Ye Zheng, Jieyu Zhao, Yuqi Li, Xijiong Xie
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of gesture recognition is to recognize meaningful movements of human bodies, and gesture recognition is an important issue in computer vision. In this paper, we present a multimodal gesture recognition method based on 3D densely convolutional networks (3D-DenseNets) and improved temporal convolutional networks (TCNs). The key idea of our approach is to find a compact and effective representation of spatial and temporal features, which orderly and separately divide task of gesture video analysis into two parts: spatial analysis and temporal analysis. In spatial analysis, we adopt 3D-DenseNets to learn short-term spatio-temporal features effectively. Subsequently, in temporal analysis, we use TCNs to extract temporal features and employ improved Squeeze-and-Excitation Networks (SENets) to strengthen the representational power of temporal features from each TCNs' layers. The method has been evaluated on the VIVA and the NVIDIA Gesture Dynamic Hand Gesture Datasets. Our approach obtains very competitive performance on VIVA benchmarks with the classification accuracies of 91.54%, and achieve state-of-the art performance with 86.37% accuracy on NVIDIA benchmark.



