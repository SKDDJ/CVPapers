# Arxiv Papers in cs.CV on 2019-12-06
### Geometric Capsule Autoencoders for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.03310v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03310v1)
- **Published**: 2019-12-06 00:10:14+00:00
- **Updated**: 2019-12-06 00:10:14+00:00
- **Authors**: Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to learn object representations from 3D point clouds using bundles of geometrically interpretable hidden units, which we call geometric capsules. Each geometric capsule represents a visual entity, such as an object or a part, and consists of two components: a pose and a feature. The pose encodes where the entity is, while the feature encodes what it is. We use these capsules to construct a Geometric Capsule Autoencoder that learns to group 3D points into parts (small local surfaces), and these parts into the whole object, in an unsupervised manner. Our novel Multi-View Agreement voting mechanism is used to discover an object's canonical pose and its pose-invariant feature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the properties of the learned representations and show the benefits of having multiple votes agree. We perform alignment and retrieval of arbitrarily rotated objects -- tasks that evaluate our model's object identification and canonical pose recovery capabilities -- and obtained insightful results.



### End-to-end Training of CNN-CRF via Differentiable Dual-Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1912.02937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02937v1)
- **Published**: 2019-12-06 00:49:48+00:00
- **Updated**: 2019-12-06 00:49:48+00:00
- **Authors**: Shaofei Wang, Vishnu Lokhande, Maneesh Singh, Konrad Kording, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: Modern computer vision (CV) is often based on convolutional neural networks (CNNs) that excel at hierarchical feature extraction. The previous generation of CV approaches was often based on conditional random fields (CRFs) that excel at modeling flexible higher order interactions. As their benefits are complementary they are often combined. However, these approaches generally use mean-field approximations and thus, arguably, did not directly optimize the real problem. Here we revisit dual-decomposition-based approaches to CRF optimization, an alternative to the mean-field approximation. These algorithms can efficiently and exactly solve sub-problems and directly optimize a convex upper bound of the real problem, providing optimality certificates on the way. Our approach uses a novel fixed-point iteration algorithm which enjoys dual-monotonicity, dual-differentiability and high parallelism. The whole system, CRF and CNN can thus be efficiently trained using back-propagation. We demonstrate the effectiveness of our system on semantic image segmentation, showing consistent improvement over baseline models.



### Generating Anthropomorphic Phantoms Using Fully Unsupervised Deformable Image Registration with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.02942v3
- **DOI**: 10.1002/mp.14545
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02942v3)
- **Published**: 2019-12-06 01:31:34+00:00
- **Updated**: 2020-04-26 20:41:37+00:00
- **Authors**: Junyu Chen, Ye Li, Yong Du, Eric C. Frey
- **Comment**: None
- **Journal**: Med. Phys., 47: 6366-6380 (2020)
- **Summary**: Objectives: Computerized phantoms play an essential role in various applications of medical imaging research. Although the existing computerized phantoms can model anatomical variations through organ and phantom scaling, this does not provide a way to fully reproduce anatomical variations seen in humans. However, having a population of phantoms that models the variations in patient anatomy and, in nuclear medicine, uptake realization is essential for comprehensive validation and training. In this work, we present a novel image registration method for creating highly anatomically detailed anthropomorphic phantoms from a single digital phantom. Methods: We propose a deep-learning-based registration algorithm to predict deformation parameters for warping an XCAT phantom to a patient CT scan. This proposed algorithm optimizes a novel SSIM-based objective function for a given image pair independently of the training data and thus is truly and fully unsupervised. We evaluate the proposed method on a publicly available low-dose CT dataset from TCIA. Results: The performance of the proposed model was compared with that of several state-of-the-art methods, and outperformed them by more than 8%, measured by the SSIM and less than 30%, by the MSE. Conclusion: A deep-learning-based unsupervised registration method was developed to create anthropomorphic phantoms while providing "gold-standard" anatomies that can be used as the basis for modeling organ properties. Significance: Experimental results demonstrate the effectiveness of the proposed method. The resulting anthropomorphic phantom is highly realistic. Combined with realistic simulations of the image formation process, the generated phantoms could serve in many applications of medical imaging research.



### cFineGAN: Unsupervised multi-conditional fine-grained image generation
- **Arxiv ID**: http://arxiv.org/abs/1912.05028v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05028v1)
- **Published**: 2019-12-06 04:16:08+00:00
- **Updated**: 2019-12-06 04:16:08+00:00
- **Authors**: Gunjan Aggarwal, Abhishek Sinha
- **Comment**: Accepted at NeurIPS Workshop on Machine Learning for Creativity and
  Design 3.0
- **Journal**: None
- **Summary**: We propose an unsupervised multi-conditional image generation pipeline: cFineGAN, that can generate an image conditioned on two input images such that the generated image preserves the texture of one and the shape of the other input. To achieve this goal, we extend upon the recently proposed work of FineGAN \citep{singh2018finegan} and make use of standard as well as shape-biased pre-trained ImageNet models. We demonstrate both qualitatively as well as quantitatively the benefit of using the shape-biased network. We present our image generation result across three benchmark datasets- CUB-200-2011, Stanford Dogs and UT Zappos50k.



### SAM: Squeeze-and-Mimic Networks for Conditional Visual Driving Policy Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.02973v2)
- **Published**: 2019-12-06 04:41:51+00:00
- **Updated**: 2020-11-19 07:50:13+00:00
- **Authors**: Albert Zhao, Tong He, Yitao Liang, Haibin Huang, Guy Van den Broeck, Stefano Soatto
- **Comment**: Conference on Robot Learning (CoRL) 2020
- **Journal**: None
- **Summary**: We describe a policy learning approach to map visual inputs to driving controls conditioned on turning command that leverages side tasks on semantics and object affordances via a learned representation trained for driving. To learn this representation, we train a squeeze network to drive using annotations for the side task as input. This representation encodes the driving-relevant information associated with the side task while ideally throwing out side task-relevant but driving-irrelevant nuisances. We then train a mimic network to drive using only images as input and use the squeeze network's latent representation to supervise the mimic network via a mimicking loss. Notably, we do not aim to achieve the side task nor to learn features for it; instead, we aim to learn, via the mimicking loss, a representation of the side task annotations directly useful for driving. We test our approach using the CARLA simulator. In addition, we introduce a more challenging but realistic evaluation protocol that considers a run that reaches the destination successful only if it does not violate common traffic rules. A video summarizing this work is available at https://youtu.be/ipKAMzmJpMs , and code is available at https://github.com/twsq/sam-driving .



### DeepEthnic: Multi-Label Ethnic Classification from Face Images
- **Arxiv ID**: http://arxiv.org/abs/1912.02983v1
- **DOI**: 10.1007/978-3-030-01424-7_59
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.02983v1)
- **Published**: 2019-12-06 05:59:16+00:00
- **Updated**: 2019-12-06 05:59:16+00:00
- **Authors**: Katia Huri, Eli David, Nathan S. Netanyahu
- **Comment**: None
- **Journal**: International Conference on Artificial Neural Networks (ICANN),
  Springer LNCS, Vol. 11141, pp. 604-612, Rhodes, Greece, October 2018
- **Summary**: Ethnic group classification is a well-researched problem, which has been pursued mainly during the past two decades via traditional approaches of image processing and machine learning. In this paper, we propose a method of classifying an image face into an ethnic group by applying transfer learning from a previously trained classification network for large-scale data recognition. Our proposed method yields state-of-the-art success rates of 99.02%, 99.76%, 99.2%, and 96.7%, respectively, for the four ethnic groups: African, Asian, Caucasian, and Indian.



### Grid-GCN for Fast and Scalable Point Cloud Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.02984v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.02984v5)
- **Published**: 2019-12-06 05:59:40+00:00
- **Updated**: 2021-04-13 03:37:00+00:00
- **Authors**: Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, Ulrich Neumann
- **Comment**: None
- **Journal**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2020)
- **Summary**: Due to the sparsity and irregularity of the point cloud data, methods that directly consume points have become popular. Among all point-based models, graph convolutional networks (GCN) lead to notable performance by fully preserving the data granularity and exploiting point interrelation. However, point-based networks spend a significant amount of time on data structuring (e.g., Farthest Point Sampling (FPS) and neighbor points querying), which limit the speed and scalability. In this paper, we present a method, named Grid-GCN, for fast and scalable point cloud learning. Grid-GCN uses a novel data structuring strategy, Coverage-Aware Grid Query (CAGQ). By leveraging the efficiency of grid space, CAGQ improves spatial coverage while reducing the theoretical time complexity. Compared with popular sampling methods such as Farthest Point Sampling (FPS) and Ball Query, CAGQ achieves up to 50X speed-up. With a Grid Context Aggregation (GCA) module, Grid-GCN achieves state-of-the-art performance on major point cloud classification and segmentation benchmarks with significantly faster runtime than previous studies. Remarkably, Grid-GCN achieves the inference speed of 50fps on ScanNet using 81920 points per scene as input.



### 3D CNN with Localized Residual Connections for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.03000v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03000v1)
- **Published**: 2019-12-06 06:46:01+00:00
- **Updated**: 2019-12-06 06:46:01+00:00
- **Authors**: Shivangi Dwivedi, Murari Mandal, Shekhar Yadav, Santosh Kumar Vipparthi
- **Comment**: 4th International Conference on Computer Vision and Image Processing
  (CVIP-2019)
- **Journal**: None
- **Summary**: In this paper we propose a novel 3D CNN network with localized residual connections for hyperspectral image classification. Our work chalks a comparative study with the existing methods employed for abstracting deeper features and propose a model which incorporates residual features from multiple stages in the network. The proposed architecture processes individual spatiospectral feature rich cubes from hyperspectral images through 3D convolutional layers. The residual connections result in improved performance due to assimilation of both low-level and high-level features. We conduct experiments over Pavia University and Pavia Center dataset for performance analysis. We compare our method with two recent state-of-the-art methods for hyperspectral image classification method. The proposed network outperforms the existing approaches by a good margin.



### Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1912.03001v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03001v2)
- **Published**: 2019-12-06 07:06:27+00:00
- **Updated**: 2020-07-21 14:48:01+00:00
- **Authors**: Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, Yu-Wing Tai
- **Comment**: Accepted by ECCV2020 as a Poster
- **Journal**: ECCV2020
- **Summary**: n this paper, we propose an effective and efficient pyramid multi-view stereo (MVS) net with self-adaptive view aggregation for accurate and complete dense point cloud reconstruction. Different from using mean square variance to generate cost volume in previous deep-learning based MVS methods, our \textbf{VA-MVSNet} incorporates the cost variances in different views with small extra memory consumption by introducing two novel self-adaptive view aggregations: pixel-wise view aggregation and voxel-wise view aggregation. To further boost the robustness and completeness of 3D point cloud reconstruction, we extend VA-MVSNet with pyramid multi-scale images input as \textbf{PVA-MVSNet}, where multi-metric constraints are leveraged to aggregate the reliable depth estimation at the coarser scale to fill in the mismatched regions at the finer scale. Experimental results show that our approach establishes a new state-of-the-art on the \textsl{\textbf{DTU}} dataset with significant improvements in the completeness and overall quality, and has strong generalization by achieving a comparable performance as the state-of-the-art methods on the \textsl{\textbf{Tanks and Temples}} benchmark. Our codebase is at \hyperlink{https://github.com/yhw-yhw/PVAMVSNet}{https://github.com/yhw-yhw/PVAMVSNet}



### Perspective-consistent multifocus multiview 3D reconstruction of small objects
- **Arxiv ID**: http://arxiv.org/abs/1912.03005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03005v1)
- **Published**: 2019-12-06 07:16:27+00:00
- **Updated**: 2019-12-06 07:16:27+00:00
- **Authors**: Hengjia Li, Chuong Nguyen
- **Comment**: Accepted to DICTA 2019, best student scientific paper award
- **Journal**: None
- **Summary**: Image-based 3D reconstruction or 3D photogrammetry of small-scale objects including insects and biological specimens is challenging due to the use of high magnification lens with inherent limited depth of field, and the object's fine structures and complex surface properties. Due to these challenges, traditional 3D reconstruction techniques cannot be applied without suitable image pre-processings. One such preprocessing technique is multifocus stacking that combines a set of partially focused images captured from the same viewing angle to create a single in-focus image. Traditional multifocus image capture uses a camera on a macro rail. Furthermore, the scale and shift are not properly considered by multifocus stacking techniques. As a consequence, the resulting in-focus images contain artifacts that violate perspective image formation. A 3D reconstruction using such images will fail to produce an accurate 3D model of the object. This paper shows how this problem can be solved effectively by a new multifocus stacking procedure which includes a new Fixed-Lens Multifocus Capture and camera calibration for image scale and shift. Initial experimental results are presented to confirm our expectation and show that the camera poses of fixed-lens images are at least 3-times less noisy than those of conventional moving lens images.



### Performing Arithmetic Using a Neural Network Trained on Digit Permutation Pairs
- **Arxiv ID**: http://arxiv.org/abs/1912.03035v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03035v1)
- **Published**: 2019-12-06 09:23:25+00:00
- **Updated**: 2019-12-06 09:23:25+00:00
- **Authors**: Marcus D. Bloice, Peter M. Roth, Andreas Holzinger
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper a neural network is trained to perform simple arithmetic using images of concatenated handwritten digit pairs. A convolutional neural network was trained with images consisting of two side-by-side handwritten digits, where the image's label is the summation of the two digits contained in the combined image. Crucially, the network was tested on permutation pairs that were not present during training in an effort to see if the network could learn the task of addition, as opposed to simply mapping images to labels. A dataset was generated for all possible permutation pairs of length 2 for the digits 0-9 using MNIST as a basis for the images, with one thousand samples generated for each permutation pair. For testing the network, samples generated from previously unseen permutation pairs were fed into the trained network, and its predictions measured. Results were encouraging, with the network achieving an accuracy of over 90% on some permutation train/test splits. This suggests that the network learned at first digit recognition, and subsequently the further task of addition based on the two recognised digits. As far as the authors are aware, no previous work has concentrated on learning a mathematical operation in this way.



### Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/1912.03063v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.03063v1)
- **Published**: 2019-12-06 11:04:08+00:00
- **Updated**: 2019-12-06 11:04:08+00:00
- **Authors**: Corentin Kervadec, Grigory Antipov, Moez Baccouche, Christian Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The large adoption of the self-attention (i.e. transformer model) and BERT-like training principles has recently resulted in a number of high performing models on a large panoply of vision-and-language problems (such as Visual Question Answering (VQA), image retrieval, etc.). In this paper we claim that these State-Of-The-Art (SOTA) approaches perform reasonably well in structuring information inside a single modality but, despite their impressive performances , they tend to struggle to identify fine-grained inter-modality relationships. Indeed, such relations are frequently assumed to be implicitly learned during training from application-specific losses, mostly cross-entropy for classification. While most recent works provide inductive bias for inter-modality relationships via cross attention modules, in this work, we demonstrate (1) that the latter assumption does not hold, i.e. modality alignment does not necessarily emerge automatically, and (2) that adding weak supervision for alignment between visual objects and words improves the quality of the learned models on tasks requiring reasoning. In particular , we integrate an object-word alignment loss into SOTA vision-language reasoning models and evaluate it on two tasks VQA and Language-driven Comparison of Images. We show that the proposed fine-grained inter-modality supervision significantly improves performance on both tasks. In particular, this new learning signal allows obtaining SOTA-level performances on GQA dataset (VQA task) with pre-trained models without finetuning on the task, and a new SOTA on NLVR2 dataset (Language-driven Comparison of Images). Finally, we also illustrate the impact of the contribution on the models reasoning by visualizing attention distributions.



### Continual egocentric object recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.05029v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05029v2)
- **Published**: 2019-12-06 12:10:59+00:00
- **Updated**: 2020-02-18 15:22:01+00:00
- **Authors**: Luca Erculiani, Fausto Giunchiglia, Andrea Passerini
- **Comment**: None
- **Journal**: None
- **Summary**: We present a framework capable of tackilng the problem of continual object recognition in a setting which resembles that under whichhumans see and learn. This setting has a set of unique characteristics:it assumes an egocentric point-of-view bound to the needs of a singleperson, which implies a relatively low diversity of data and a coldstart with no data; it requires to operate in an open world, where newobjects can be encounteredat any time; supervision is scarce and hasto be solicited to the user, and completelyunsupervised recognitionof new objects should be possible. Note that this setting differs fromthe one addressed in the open world recognition literature, where supervised feedback is always requested to be able to incorporate newobjects. We propose a first solution to this problem in the form ofa memory-based incremental framework that is capable of storinginformation of each and any object it encounters, while using the supervision of the user to learn to discriminate between known and unknown objects. Our approach is based on four main features: the useof time and space persistence (i.e., the appearance of objects changesrelatively slowly), the use of similarity as the main driving principlefor object recognition and novelty detection, the progressive introduction of new objects in a developmental fashion and the selectiveelicitation of user feedback in an online active learning fashion. Experimental results show the feasibility of open world, generic objectrecognition, the ability to recognize, memorize and re-identify newobjects even in complete absence of user supervision, and the utilityof persistence and incrementality in boosting performance.



### Visual-Textual Association with Hardest and Semi-Hard Negative Pairs Mining for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1912.03083v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03083v1)
- **Published**: 2019-12-06 12:21:06+00:00
- **Updated**: 2019-12-06 12:21:06+00:00
- **Authors**: Jing Ge, Guangyu Gao, Zhen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Searching persons in large-scale image databases with the query of natural language description is a more practical important applications in video surveillance. Intuitively, for person search, the core issue should be visual-textual association, which is still an extremely challenging task, due to the contradiction between the high abstraction of textual description and the intuitive expression of visual images. However, for this task, while positive image-text pairs are always well provided, most existing methods doesn't tackle this problem effectively by mining more reasonable negative pairs. In this paper, we proposed a novel visual-textual association approach with visual and textual attention, and cross-modality hardest and semi-hard negative pair mining. In order to evaluate the effectiveness and feasibility of the proposed approach, we conduct extensive experiments on typical person search datasdet: CUHK-PEDES, in which our approach achieves the top1 score of 55.32% as a new state-of-the-art. Besides, we also evaluate the semi-hard pair mining approach in COCO caption dataset, and validate the effectiveness and complementarity of the methods.



### Exploring Unlabeled Faces for Novel Attribute Discovery
- **Arxiv ID**: http://arxiv.org/abs/1912.03085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03085v1)
- **Published**: 2019-12-06 12:27:37+00:00
- **Updated**: 2019-12-06 12:27:37+00:00
- **Authors**: Hyojin Bahng, Sunghyo Chung, Seungjoo Yoo, Jaegul Choo
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Despite remarkable success in unpaired image-to-image translation, existing systems still require a large amount of labeled images. This is a bottleneck for their real-world applications; in practice, a model trained on labeled CelebA dataset does not work well for test images from a different distribution -- greatly limiting their application to unlabeled images of a much larger quantity. In this paper, we attempt to alleviate this necessity for labeled data in the facial image translation domain. We aim to explore the degree to which you can discover novel attributes from unlabeled faces and perform high-quality translation. To this end, we use prior knowledge about the visual world as guidance to discover novel attributes and transfer them via a novel normalization method. Experiments show that our method trained on unlabeled data produces high-quality translations, preserves identity, and be perceptually realistic as good as, or better than, state-of-the-art methods trained on labeled data.



### Video to Events: Recycling Video Datasets for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1912.03095v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03095v2)
- **Published**: 2019-12-06 13:10:59+00:00
- **Updated**: 2020-04-01 16:25:30+00:00
- **Authors**: Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carrió, Davide Scaramuzza
- **Comment**: None
- **Journal**: None
- **Summary**: Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous "events" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high dynamic range (HDR), high temporal resolution, and no motion blur. Recently, novel learning approaches operating on event data have achieved impressive results. Yet, these methods require a large amount of event data for training, which is hardly available due the novelty of event sensors in computer vision research. In this paper, we present a method that addresses these needs by converting any existing video dataset recorded with conventional cameras to synthetic event data. This unlocks the use of a virtually unlimited number of existing video datasets for training networks designed for real event data. We evaluate our method on two relevant vision tasks, i.e., object recognition and semantic segmentation, and show that models trained on synthetic events have several benefits: (i) they generalize well to real event data, even in scenarios where standard-camera images are blurry or overexposed, by inheriting the outstanding properties of event cameras; (ii) they can be used for fine-tuning on real data to improve over state-of-the-art for both classification and semantic segmentation.



### Connecting Vision and Language with Localized Narratives
- **Arxiv ID**: http://arxiv.org/abs/1912.03098v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03098v4)
- **Published**: 2019-12-06 13:21:16+00:00
- **Updated**: 2020-07-20 17:18:38+00:00
- **Authors**: Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio Ferrari
- **Comment**: ECCV 2020 Camera Ready
- **Journal**: None
- **Summary**: We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.



### Face Recognition via Locality Constrained Low Rank Representation and Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03145v1)
- **Published**: 2019-12-06 14:24:52+00:00
- **Updated**: 2019-12-06 14:24:52+00:00
- **Authors**: He-Feng Yin, Xiao-Jun Wu, Josef Kittler
- **Comment**: 6 pages, 2 figures
- **Journal**: None
- **Summary**: Face recognition has been widely studied due to its importance in smart cities applications. However, the case when both training and test images are corrupted is not well solved. To address such a problem, this paper proposes a locality constrained low rank representation and dictionary learning (LCLRRDL) algorithm for robust face recognition. In particular, we present three contributions in the proposed formulation. First, a low-rank representation is introduced to handle the possible contamination of the training as well as test data. Second, a locality constraint is incorporated to acknowledge the intrinsic manifold structure of training data. With the locality constraint term, our scheme induces similar samples to have similar representations. Third, a compact dictionary is learned to handle the problem of corrupted data. The experimental results on two public databases demonstrate the effectiveness of the proposed approach. Matlab code of our proposed LCLRRDL can be downloaded from https://github.com/yinhefeng/LCLRRDL.



### NASNet: A Neuron Attention Stage-by-Stage Net for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1912.03151v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03151v2)
- **Published**: 2019-12-06 14:28:20+00:00
- **Updated**: 2020-05-28 03:08:43+00:00
- **Authors**: Xu Qin, Zhilin Wang
- **Comment**: underreviewed by conference
- **Journal**: None
- **Summary**: Images captured under complicated rain conditions often suffer from noticeable degradation of visibility. The rain models generally introduce diversity visibility degradation, which includes rain streak, rain drop as well as rain mist. Numerous existing single image deraining methods focus on the only one type rain model, which does not have strong generalization ability. In this paper, we propose a novel end-to-end Neuron Attention Stage-by-Stage Net (NASNet), which can solve all types of rain model tasks efficiently. For one thing, we pay more attention on the Neuron relationship and propose a lightweight Neuron Attention (NA) architectural mechanism. It can adaptively recalibrate neuron-wise feature responses by modelling interdependencies and mutual influence between neurons. Our NA architecture consists of Depthwise Conv and Pointwise Conv, which has slight computation cost and higher performance than SE block by our contrasted experiments. For another, we propose a stage-by-stage unified pattern network architecture, the stage-by-stage strategy guides the later stage by incorporating the useful information in previous stage. We concatenate and fuse stage-level information dynamically by NA module. Extensive experiments demonstrate that our proposed NASNet significantly outperforms the state-of-theart methods by a large margin in terms of both quantitative and qualitative measures on all six public large-scale datasets for three rain model tasks.



### Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking
- **Arxiv ID**: http://arxiv.org/abs/1912.12125v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.12125v1)
- **Published**: 2019-12-06 14:32:04+00:00
- **Updated**: 2019-12-06 14:32:04+00:00
- **Authors**: Kilian Kleeberger, Christian Landgraf, Marco F. Huber
- **Comment**: Accepted at 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: In this paper, we introduce a new public dataset for 6D object pose estimation and instance segmentation for industrial bin-picking. The dataset comprises both synthetic and real-world scenes. For both, point clouds, depth images, and annotations comprising the 6D pose (position and orientation), a visibility score, and a segmentation mask for each object are provided. Along with the raw data, a method for precisely annotating real-world scenes is proposed. To the best of our knowledge, this is the first public dataset for 6D object pose estimation and instance segmentation for bin-picking containing sufficiently annotated data for learning-based approaches. Furthermore, it is one of the largest public datasets for object pose estimation in general. The dataset is publicly available at http://www.bin-picking.ai/en/dataset.html.



### 300 GHz Radar Object Recognition based on Deep Neural Networks and Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03157v1)
- **Published**: 2019-12-06 14:42:48+00:00
- **Updated**: 2019-12-06 14:42:48+00:00
- **Authors**: Marcel Sheeny, Andrew Wallace, Sen Wang
- **Comment**: This paper is a preprint of a paper submitted to IET Radar, Sonar and
  Navigation. If accepted, the copy of record will be available at the IET
  Digital Library
- **Journal**: None
- **Summary**: For high resolution scene mapping and object recognition, optical technologies such as cameras and LiDAR are the sensors of choice. However, for robust future vehicle autonomy and driver assistance in adverse weather conditions, improvements in automotive radar technology, and the development of algorithms and machine learning for robust mapping and recognition are essential. In this paper, we describe a methodology based on deep neural networks to recognise objects in 300GHz radar images, investigating robustness to changes in range, orientation and different receivers in a laboratory environment. As the training data is limited, we have also investigated the effects of transfer learning. As a necessary first step before road trials, we have also considered detection and classification in multiple object scenes.



### Controlling Style and Semantics in Weakly-Supervised Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1912.03161v2
- **DOI**: 10.1007/978-3-030-58539-6_29
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03161v2)
- **Published**: 2019-12-06 14:47:50+00:00
- **Updated**: 2020-07-21 16:18:18+00:00
- **Authors**: Dario Pavllo, Aurelien Lucchi, Thomas Hofmann
- **Comment**: European Conference on Computer Vision (ECCV) 2020, Spotlight. Code
  at https://github.com/dariopavllo/style-semantics
- **Journal**: None
- **Summary**: We propose a weakly-supervised approach for conditional image generation of complex scenes where a user has fine control over objects appearing in the scene. We exploit sparse semantic maps to control object shapes and classes, as well as textual descriptions or attributes to control both local and global style. In order to condition our model on textual descriptions, we introduce a semantic attention module whose computational cost is independent of the image resolution. To further augment the controllability of the scene, we propose a two-step generation scheme that decomposes background and foreground. The label maps used to train our model are produced by a large-vocabulary object detector, which enables access to unlabeled data and provides structured instance information. In such a setting, we report better FID scores compared to fully-supervised settings where the model is trained on ground-truth semantic maps. We also showcase the ability of our model to manipulate a scene on complex datasets such as COCO and Visual Genome.



### Waterfall Atrous Spatial Pooling Architecture for Efficient Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.03183v1
- **DOI**: 10.3390/s19245361
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03183v1)
- **Published**: 2019-12-06 15:29:39+00:00
- **Updated**: 2019-12-06 15:29:39+00:00
- **Authors**: Bruno Artacho, Andreas Savakis
- **Comment**: 17 pages, 11 figures
- **Journal**: Sensors, 19(24), 2019
- **Summary**: We propose a new efficient architecture for semantic segmentation, based on a "Waterfall" Atrous Spatial Pooling architecture, that achieves a considerable accuracy increase while decreasing the number of network parameters and memory footprint. The proposed Waterfall architecture leverages the efficiency of progressive filtering in the cascade architecture while maintaining multiscale fields-of-view comparable to spatial pyramid configurations. Additionally, our method does not rely on a postprocessing stage with Conditional Random Fields, which further reduces complexity and required training time. We demonstrate that the Waterfall approach with a ResNet backbone is a robust and efficient architecture for semantic segmentation obtaining state-of-the-art results with significant reduction in the number of parameters for the Pascal VOC dataset and the Cityscapes dataset.



### Achieving Robustness in the Wild via Adversarial Mixing with Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1912.03192v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03192v2)
- **Published**: 2019-12-06 15:56:53+00:00
- **Updated**: 2020-03-25 09:33:57+00:00
- **Authors**: Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann, Pushmeet Kohli
- **Comment**: Accepted at CVPR 2020
- **Journal**: None
- **Summary**: Recent research has made the surprising finding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically defined transformations like $\ell_p$-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efficacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a "smile" detector by 21% for example).



### A Neural Spiking Approach Compared to Deep Feedforward Networks on Stepwise Pixel Erasement
- **Arxiv ID**: http://arxiv.org/abs/1912.03201v1
- **DOI**: 10.1007/978-3-030-01418-6_25
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03201v1)
- **Published**: 2019-12-06 16:08:45+00:00
- **Updated**: 2019-12-06 16:08:45+00:00
- **Authors**: René Larisch, Michael Teichmann, Fred H. Hamker
- **Comment**: Published in ICANN 2018: Artificial Neural Networks and Machine
  Learning - ICANN 2018
  https://link.springer.com/chapter/10.1007/978-3-030-01418-6_25 The final
  authenticated publication is available online at
  https://doi.org/10.1007/978-3-030-01418-6_25
- **Journal**: None
- **Summary**: In real world scenarios, objects are often partially occluded. This requires a robustness for object recognition against these perturbations. Convolutional networks have shown good performances in classification tasks. The learned convolutional filters seem similar to receptive fields of simple cells found in the primary visual cortex. Alternatively, spiking neural networks are more biological plausible. We developed a two layer spiking network, trained on natural scenes with a biologically plausible learning rule. It is compared to two deep convolutional neural networks using a classification task of stepwise pixel erasement on MNIST. In comparison to these networks the spiking approach achieves good accuracy and robustness.



### Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference
- **Arxiv ID**: http://arxiv.org/abs/1912.03203v3
- **DOI**: 10.1109/CVPR42600.2020.00239
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03203v3)
- **Published**: 2019-12-06 16:11:16+00:00
- **Updated**: 2022-08-05 15:59:50+00:00
- **Authors**: Thomas Verelst, Tinne Tuytelaars
- **Comment**: CVPR 2020 (poster)
- **Journal**: None
- **Summary**: Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed with MobileNetV2 residual blocks. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60% with no loss in accuracy.



### NASA: Neural Articulated Shape Approximation
- **Arxiv ID**: http://arxiv.org/abs/1912.03207v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03207v5)
- **Published**: 2019-12-06 16:18:35+00:00
- **Updated**: 2022-07-21 19:48:00+00:00
- **Authors**: Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, Andrea Tagliasacchi
- **Comment**: ECCV 2020; Project Page: https://nasa-eccv20.github.io/
- **Journal**: None
- **Summary**: Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.



### Tree bark re-identification using a deep-learning feature descriptor
- **Arxiv ID**: http://arxiv.org/abs/1912.03221v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03221v2)
- **Published**: 2019-12-06 16:43:02+00:00
- **Updated**: 2020-04-01 15:14:40+00:00
- **Authors**: Martin Robert, Patrick Dallaire, Philippe Giguère
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to visually re-identify objects is a fundamental capability in vision systems. Oftentimes, it relies on collections of visual signatures based on descriptors, such as SIFT or SURF. However, these traditional descriptors were designed for a certain domain of surface appearances and geometries (limited relief). Consequently, highly-textured surfaces such as tree bark pose a challenge to them. In turn, this makes it more difficult to use trees as identifiable landmarks for navigational purposes (robotics) or to track felled lumber along a supply chain (logistics). We thus propose to use data-driven descriptors trained on bark images for tree surface re-identification. To this effect, we collected a large dataset containing 2,400 bark images with strong illumination changes, annotated by surface and with the ability to pixel-align them. We used this dataset to sample from more than 2 million 64x64 pixel patches to train our novel local descriptors DeepBark and SqueezeBark. Our DeepBark method has shown a clear advantage against the hand-crafted descriptors SIFT and SURF. For instance, we demonstrated that DeepBark can reach a mAP of 87.2% when retrieving 11 relevant bark images, i.e. corresponding to the same physical surface, to a bark query against 7,900 images. Our work thus suggests that re-identifying tree surfaces in a challenging illuminations context is possible. We also make public our dataset, which can be used to benchmark surface re-identification techniques.



### Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03227v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03227v1)
- **Published**: 2019-12-06 16:54:10+00:00
- **Updated**: 2019-12-06 16:54:10+00:00
- **Authors**: Jannik Zürn, Wolfram Burgard, Abhinav Valada
- **Comment**: 14 pages, 12 figures
- **Journal**: IEEE Transactions on Robotics (T-RO), vol. 37, no. 2, pp. 466-481,
  2021
- **Summary**: Mobile robots operating in unknown urban environments encounter a wide range of complex terrains to which they must adapt their planned trajectory for safe and efficient navigation. Most existing approaches utilize supervised learning to classify terrains from either an exteroceptive or a proprioceptive sensor modality. However, this requires a tremendous amount of manual labeling effort for each newly encountered terrain as well as for variations of terrains caused by changing environmental conditions. In this work, we propose a novel terrain classification framework leveraging an unsupervised proprioceptive classifier that learns from vehicle-terrain interaction sounds to self-supervise an exteroceptive classifier for pixel-wise semantic segmentation of images. To this end, we first learn a discriminative embedding space for vehicle-terrain interaction sounds from triplets of audio clips formed using visual features of the corresponding terrain patches and cluster the resulting embeddings. We subsequently use these clusters to label the visual terrain patches by projecting the traversed tracks of the robot into the camera images. Finally, we use the sparsely labeled images to train our semantic segmentation network in a weakly supervised manner. We present extensive quantitative and qualitative results that demonstrate that our proprioceptive terrain classifier exceeds the state-of-the-art among unsupervised methods and our self-supervised exteroceptive semantic segmentation model achieves a comparable performance to supervised learning with manually labeled data.



### Benchmarking Image Sensors Under Adverse Weather Conditions for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1912.03238v1
- **DOI**: 10.1109/IVS.2018.8500659
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03238v1)
- **Published**: 2019-12-06 17:26:25+00:00
- **Updated**: 2019-12-06 17:26:25+00:00
- **Authors**: Mario Bijelic, Tobias Gruber, Werner Ritter
- **Comment**: None
- **Journal**: Published in: 2018 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Adverse weather conditions are very challenging for autonomous driving because most of the state-of-the-art sensors stop working reliably under these conditions. In order to develop robust sensors and algorithms, tests with current sensors in defined weather conditions are crucial for determining the impact of bad weather for each sensor. This work describes a testing and evaluation methodology that helps to benchmark novel sensor technologies and compare them to state-of-the-art sensors. As an example, gated imaging is compared to standard imaging under foggy conditions. It is shown that gated imaging outperforms state-of-the-art standard passive imaging due to time-synchronized active illumination.



### Gaussian Process Priors for View-Aware Inference
- **Arxiv ID**: http://arxiv.org/abs/1912.03249v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03249v2)
- **Published**: 2019-12-06 17:41:37+00:00
- **Updated**: 2021-03-03 20:02:15+00:00
- **Authors**: Yuxin Hou, Ari Heljakka, Arno Solin
- **Comment**: Appearing in AAAI 2021
- **Journal**: None
- **Summary**: While frame-independent predictions with deep neural networks have become the prominent solutions to many computer vision tasks, the potential benefits of utilizing correlations between frames have received less attention. Even though probabilistic machine learning provides the ability to encode correlation as prior knowledge for inference, there is a tangible gap between the theory and practice of applying probabilistic methods to modern vision problems. For this, we derive a principled framework to combine information coupling between camera poses (translation and orientation) with deep models. We proposed a novel view kernel that generalizes the standard periodic kernel in $\mathrm{SO}(3)$. We show how this soft-prior knowledge can aid several pose-related vision tasks like novel view synthesis and predict arbitrary points in the latent space of generative models, pointing towards a range of new applications for inter-frame reasoning.



### A Benchmark for Lidar Sensors in Fog: Is Detection Breaking Down?
- **Arxiv ID**: http://arxiv.org/abs/1912.03251v1
- **DOI**: 10.1109/IVS.2018.8500543
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03251v1)
- **Published**: 2019-12-06 17:49:10+00:00
- **Updated**: 2019-12-06 17:49:10+00:00
- **Authors**: Mario Bijelic, Tobias Gruber, Werner Ritter
- **Comment**: None
- **Journal**: Published in: 2018 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Autonomous driving at level five does not only means self-driving in the sunshine. Adverse weather is especially critical because fog, rain, and snow degrade the perception of the environment. In this work, current state of the art light detection and ranging (lidar) sensors are tested in controlled conditions in a fog chamber. We present current problems and disturbance patterns for four different state of the art lidar systems. Moreover, we investigate how tuning internal parameters can improve their performance in bad weather situations. This is of great importance because most state of the art detection algorithms are based on undisturbed lidar data.



### Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One
- **Arxiv ID**: http://arxiv.org/abs/1912.03263v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03263v3)
- **Published**: 2019-12-06 18:00:36+00:00
- **Updated**: 2020-09-15 15:40:19+00:00
- **Authors**: Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x,y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may beused and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, andout-of-distribution detection while also enabling our models to generate samplesrivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and presentan approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-artin both generative and discriminative learning within one hybrid model.



### Recent advances in deep learning applied to skin cancer detection
- **Arxiv ID**: http://arxiv.org/abs/1912.03280v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.03280v1)
- **Published**: 2019-12-06 18:23:30+00:00
- **Updated**: 2019-12-06 18:23:30+00:00
- **Authors**: Andre G. C. Pacheco, Renato A. Krohling
- **Comment**: Paper accepted in the Retrospectives Workshop @ NeurIPS 2019
- **Journal**: None
- **Summary**: Skin cancer is a major public health problem around the world. Its early detection is very important to increase patient prognostics. However, the lack of qualified professionals and medical instruments are significant issues in this field. In this context, over the past few years, deep learning models applied to automated skin cancer detection have become a trend. In this paper, we present an overview of the recent advances reported in this field as well as a discussion about the challenges and opportunities for improvement in the current models. In addition, we also present some important aspects regarding the use of these models in smartphones and indicate future directions we believe the field will take.



### ClusterFit: Improving Generalization of Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/1912.03330v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.03330v1)
- **Published**: 2019-12-06 19:56:42+00:00
- **Updated**: 2019-12-06 19:56:42+00:00
- **Authors**: Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, Dhruv Mahajan
- **Comment**: None
- **Journal**: None
- **Summary**: Pre-training convolutional neural networks with weakly-supervised and self-supervised strategies is becoming increasingly popular for several computer vision tasks. However, due to the lack of strong discriminative signals, these learned representations may overfit to the pre-training objective (e.g., hashtag prediction) and not generalize well to downstream tasks. In this work, we present a simple strategy - ClusterFit (CF) to improve the robustness of the visual representations learned during pre-training. Given a dataset, we (a) cluster its features extracted from a pre-trained network using k-means and (b) re-train a new network from scratch on this dataset using cluster assignments as pseudo-labels. We empirically show that clustering helps reduce the pre-training task-specific information from the extracted features thereby minimizing overfitting to the same. Our approach is extensible to different pre-training frameworks -- weak- and self-supervised, modalities -- images and videos, and pre-training tasks -- object and action classification. Through extensive transfer learning experiments on 11 different target datasets of varied vocabularies and granularities, we show that ClusterFit significantly improves the representation quality compared to the state-of-the-art large-scale (millions / billions) weakly-supervised image and video models and self-supervised image models.



### Bilinear Models for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.03354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03354v1)
- **Published**: 2019-12-06 21:59:59+00:00
- **Updated**: 2019-12-06 21:59:59+00:00
- **Authors**: Tayssir Doghri, Leszek Szczecinski, Jacob Benesty, Amar Mitiche
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we define and analyze the bilinear models which replace the conventional linear operation used in many building blocks of machine learning (ML). The main idea is to devise the ML algorithms which are adapted to the objects they treat. In the case of monochromatic images, we show that the bilinear operation exploits better the structure of the image than the conventional linear operation which ignores the spatial relationship between the pixels. This translates into significantly smaller number of parameters required to yield the same performance. We show numerical examples of classification in the MNIST data set.



### Sparse and redundant signal representations for x-ray computed tomography
- **Arxiv ID**: http://arxiv.org/abs/1912.03379v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.03379v1)
- **Published**: 2019-12-06 22:50:09+00:00
- **Updated**: 2019-12-06 22:50:09+00:00
- **Authors**: Davood Karimi
- **Comment**: None
- **Journal**: None
- **Summary**: Image models are central to all image processing tasks. The great advancements in digital image processing would not have been made possible without powerful models which, themselves, have evolved over time. In the past decade, patch-based models have emerged as one of the most effective models for natural images. Patch-based methods have outperformed other competing methods in many image processing tasks. These developments have come at a time when greater availability of powerful computational resources and growing concerns over the health risks of the ionizing radiation encourage research on image processing algorithms for computed tomography (CT). The goal of this paper is to explain the principles of patch-based methods and to review some of their recent applications in CT. We review the central concepts in patch-based image processing and explain some of the state-of-the-art algorithms, with a focus on aspects that are more relevant to CT. Then, we review some of the recent application of patch-based methods in CT.



### Deep Distance Transform for Tubular Structure Segmentation in CT Scans
- **Arxiv ID**: http://arxiv.org/abs/1912.03383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.03383v1)
- **Published**: 2019-12-06 23:04:51+00:00
- **Updated**: 2019-12-06 23:04:51+00:00
- **Authors**: Yan Wang, Xu Wei, Fengze Liu, Jieneng Chen, Yuyin Zhou, Wei Shen, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Tubular structure segmentation in medical images, e.g., segmenting vessels in CT scans, serves as a vital step in the use of computers to aid in screening early stages of related diseases. But automatic tubular structure segmentation in CT scans is a challenging problem, due to issues such as poor contrast, noise and complicated background. A tubular structure usually has a cylinder-like shape which can be well represented by its skeleton and cross-sectional radii (scales). Inspired by this, we propose a geometry-aware tubular structure segmentation method, Deep Distance Transform (DDT), which combines intuitions from the classical distance transform for skeletonization and modern deep segmentation networks. DDT first learns a multi-task network to predict a segmentation mask for a tubular structure and a distance map. Each value in the map represents the distance from each tubular structure voxel to the tubular structure surface. Then the segmentation mask is refined by leveraging the shape prior reconstructed from the distance map. We apply our DDT on six medical image datasets. The experiments show that (1) DDT can boost tubular structure segmentation performance significantly (e.g., over 13% improvement measured by DSC for pancreatic duct segmentation), and (2) DDT additionally provides a geometrical measurement for a tubular structure, which is important for clinical diagnosis (e.g., the cross-sectional scale of a pancreatic duct can be an indicator for pancreatic cancer).



