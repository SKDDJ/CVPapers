# Arxiv Papers in cs.CV on 2019-12-27
### Apricot variety classification using image processing and machine learning approaches
- **Arxiv ID**: http://arxiv.org/abs/1912.11953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11953v1)
- **Published**: 2019-12-27 00:51:53+00:00
- **Updated**: 2019-12-27 00:51:53+00:00
- **Authors**: Seyed Vahid Mirnezami, Ali HamidiSepehr, Mahdi Ghaebi
- **Comment**: None
- **Journal**: None
- **Summary**: Apricot which is a cultivated type of Zerdali (wild apricot) has an important place in human nutrition and its medical properties are essential for human health. The objective of this research was to obtain a model for apricot mass and separate apricot variety with image processing technology using external features of apricot fruit. In this study, five verities of apricot were used. In order to determine the size of the fruits, three mutually perpendicular axes were defined, length, width, and thickness. Measurements show that the effect of variety on all properties was statistically significant at the 1% probability level. Furthermore, there is no significant difference between the estimated dimensions by image processing approach and the actual dimensions. The developed system consists of a digital camera, a light diffusion chamber, a distance adjustment pedestal, and a personal computer. Images taken by the digital camera were stored as (RGB) for further analysis. The images were taken for a number of 49 samples of each cultivar in three directions. A linear equation is recommended to calculate the apricot mass based on the length and the width with R 2 = 0.97. In addition, ANFIS model with C-means was the best model for classifying the apricot varieties based on the physical features including length, width, thickness, mass, and projected area of three perpendicular surfaces. The accuracy of the model was 87.7.



### Non-Cooperative Game Theory Based Rate Adaptation for Dynamic Video Streaming over HTTP
- **Arxiv ID**: http://arxiv.org/abs/1912.11954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1912.11954v1)
- **Published**: 2019-12-27 01:19:14+00:00
- **Updated**: 2019-12-27 01:19:14+00:00
- **Authors**: Hui Yuan, Huayong Fu, Ju Liu, Junhui Hou, Sam Kwong
- **Comment**: This paper has been published on IEEE Transactions on Mobile
  Computing. H. Yuan, H. Fu, J. Liu, J. Hou, and S. Kwong, "Non-Cooperative
  Game Theory Based Rate Adaptation for Dynamic Video Streaming over HTTP,"
  IEEE Transactions on Mobile Computing, vol.17, no.10, pp. 2334-2348, Oct.
  2018
- **Journal**: IEEE Transactions on Mobile Computing, vol.17, no.10, pp.
  2334-2348, Oct. 2018
- **Summary**: Dynamic Adaptive Streaming over HTTP (DASH) has demonstrated to be an emerging and promising multimedia streaming technique, owing to its capability of dealing with the variability of networks. Rate adaptation mechanism, a challenging and open issue, plays an important role in DASH based systems since it affects Quality of Experience (QoE) of users, network utilization, etc. In this paper, based on non-cooperative game theory, we propose a novel algorithm to optimally allocate the limited export bandwidth of the server to multi-users to maximize their QoE with fairness guaranteed. The proposed algorithm is proxy-free. Specifically, a novel user QoE model is derived by taking a variety of factors into account, like the received video quality, the reference buffer length, and user accumulated buffer lengths, etc. Then, the bandwidth competing problem is formulated as a non-cooperation game with the existence of Nash Equilibrium that is theoretically proven. Finally, a distributed iterative algorithm with stability analysis is proposed to find the Nash Equilibrium. Compared with state-of-the-art methods, extensive experimental results in terms of both simulated and realistic networking scenarios demonstrate that the proposed algorithm can produce higher QoE, and the actual buffer lengths of all users keep nearly optimal states, i.e., moving around the reference buffer all the time. Besides, the proposed algorithm produces no playback interruption.



### DeGAN : Data-Enriching GAN for Retrieving Representative Samples from a Trained Classifier
- **Arxiv ID**: http://arxiv.org/abs/1912.11960v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11960v1)
- **Published**: 2019-12-27 02:05:45+00:00
- **Updated**: 2019-12-27 02:05:45+00:00
- **Authors**: Sravanti Addepalli, Gaurav Kumar Nayak, Anirban Chakraborty, R. Venkatesh Babu
- **Comment**: Accepted at AAAI-2020
- **Journal**: None
- **Summary**: In this era of digital information explosion, an abundance of data from numerous modalities is being generated as well as archived everyday. However, most problems associated with training Deep Neural Networks still revolve around lack of data that is rich enough for a given task. Data is required not only for training an initial model, but also for future learning tasks such as Model Compression and Incremental Learning. A diverse dataset may be used for training an initial model, but it may not be feasible to store it throughout the product life cycle due to data privacy issues or memory constraints. We propose to bridge the gap between the abundance of available data and lack of relevant data, for the future learning tasks of a given trained network. We use the available data, that may be an imbalanced subset of the original training dataset, or a related domain dataset, to retrieve representative samples from a trained classifier, using a novel Data-enriching GAN (DeGAN) framework. We demonstrate that data from a related domain can be leveraged to achieve state-of-the-art performance for the tasks of Data-free Knowledge Distillation and Incremental Learning on benchmark datasets. We further demonstrate that our proposed framework can enrich any data, even from unrelated domains, to make it more useful for the future learning tasks of a given network.



### Handling Missing MRI Input Data in Deep Learning Segmentation of Brain Metastases: A Multi-Center Study
- **Arxiv ID**: http://arxiv.org/abs/1912.11966v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11966v1)
- **Published**: 2019-12-27 02:49:45+00:00
- **Updated**: 2019-12-27 02:49:45+00:00
- **Authors**: Endre Grøvik, Darvin Yi, Michael Iv, Elizabeth Tong, Line Brennhaug Nilsen, Anna Latysheva, Cathrine Saxhaug, Kari Dolven Jacobsen, Åslaug Helland, Kyrre Eeg Emblem, Daniel Rubin, Greg Zaharchuk
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose was to assess the clinical value of a novel DropOut model for detecting and segmenting brain metastases, in which a neural network is trained on four distinct MRI sequences using an input dropout layer, thus simulating the scenario of missing MRI data by training on the full set and all possible subsets of the input data. This retrospective, multi-center study, evaluated 165 patients with brain metastases. A deep learning based segmentation model for automatic segmentation of brain metastases, named DropOut, was trained on multi-sequence MRI from 100 patients, and validated/tested on 10/55 patients. The segmentation results were compared with the performance of a state-of-the-art DeepLabV3 model. The MR sequences in the training set included pre- and post-gadolinium (Gd) T1-weighted 3D fast spin echo, post-Gd T1-weighted inversion recovery (IR) prepped fast spoiled gradient echo, and 3D fluid attenuated inversion recovery (FLAIR), whereas the test set did not include the IR prepped image-series. The ground truth were established by experienced neuroradiologists. The results were evaluated using precision, recall, Dice score, and receiver operating characteristics (ROC) curve statistics, while the Wilcoxon rank sum test was used to compare the performance of the two neural networks. The area under the ROC curve (AUC), averaged across all test cases, was 0.989+-0.029 for the DropOut model and 0.989+-0.023 for the DeepLabV3 model (p=0.62). The DropOut model showed a significantly higher Dice score compared to the DeepLabV3 model (0.795+-0.105 vs. 0.774+-0.104, p=0.017), and a significantly lower average false positive rate of 3.6/patient vs. 7.0/patient (p<0.001) using a 10mm3 lesion-size limit. The DropOut model may facilitate accurate detection and segmentation of brain metastases on a multi-center basis, even when the test cohort is missing MRI input data.



### A single target tracking algorithm based on Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.11967v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11967v1)
- **Published**: 2019-12-27 02:55:48+00:00
- **Updated**: 2019-12-27 02:55:48+00:00
- **Authors**: Zhaofu Diao
- **Comment**: None
- **Journal**: None
- **Summary**: In the single target tracking field, occlusion leads to the loss of tracking targets is a ubiquitous and arduous problem. To solve this problem, we propose a single target tracking algorithm with anti-occlusion capability. The main content of our algorithm is to use the Region Proposal Network to obtain the tracked target and potential interferences, and use the occlusion awareness module to judge whether the interfering object occludes the target. If no occlusion occurs, continue tracking. If occlusion occurs, the prediction module is started, and the motion trajectory of the target in subsequent frames is predicted according to the motion trajectory before occlusion. The result obtained by the prediction module is used to replace the target position feature obtained by the original tracking algorithm. So we solve the problem that the occlusion causes the tracking algorithm to lose the target. In actual performance, our algorithm can successfully track the target in the occluded dataset. On the VOT2018 dataset, our algorithm has an EAO of 0.421, an Accuracy of 0.67, and a Robustness of 0.186. Compared with SiamRPN ++, they increased by 1.69%, 11.67% and 9.3%, respectively.



### Efficient Adversarial Training with Transferable Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1912.11969v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11969v2)
- **Published**: 2019-12-27 03:05:05+00:00
- **Updated**: 2020-07-02 16:48:22+00:00
- **Authors**: Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, Atul Prakash
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition 2020 (CVPR 2020)
- **Journal**: None
- **Summary**: Adversarial training is an effective defense method to protect classification models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we first show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efficiency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and requires 12~14x less training time on MNIST and CIFAR10 datasets with comparable model robustness.



### HoMM: Higher-order Moment Matching for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1912.11976v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11976v1)
- **Published**: 2019-12-27 03:53:03+00:00
- **Updated**: 2019-12-27 03:53:03+00:00
- **Authors**: Chao Chen, Zhihang Fu, Zhihong Chen, Sheng Jin, Zhaowei Cheng, Xinyu Jin, Xian-Sheng Hua
- **Comment**: Accept by AAAI-2020, codes are available at
  https://github.com/chenchao666/HoMM-Master
- **Journal**: None
- **Summary**: Minimizing the discrepancy of feature distributions between different domains is one of the most promising directions in unsupervised domain adaptation. From the perspective of distribution matching, most existing discrepancy-based methods are designed to match the second-order or lower statistics, which however, have limited expression of statistical characteristic for non-Gaussian distributions. In this work, we explore the benefits of using higher-order statistics (mainly refer to third-order and fourth-order statistics) for domain matching. We propose a Higher-order Moment Matching (HoMM) method, and further extend the HoMM into reproducing kernel Hilbert spaces (RKHS). In particular, our proposed HoMM can perform arbitrary-order moment tensor matching, we show that the first-order HoMM is equivalent to Maximum Mean Discrepancy (MMD) and the second-order HoMM is equivalent to Correlation Alignment (CORAL). Moreover, the third-order and the fourth-order moment tensor matching are expected to perform comprehensive domain alignment as higher-order statistics can approximate more complex, non-Gaussian distributions. Besides, we also exploit the pseudo-labeled target samples to learn discriminative representations in the target domain, which further improves the transfer performance. Extensive experiments are conducted, showing that our proposed HoMM consistently outperforms the existing moment matching methods by a large margin. Codes are available at \url{https://github.com/chenchao666/HoMM-Master}



### TBC-Net: A real-time detector for infrared small target detection using semantic constraint
- **Arxiv ID**: http://arxiv.org/abs/2001.05852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2001.05852v1)
- **Published**: 2019-12-27 05:25:39+00:00
- **Updated**: 2019-12-27 05:25:39+00:00
- **Authors**: Mingxin Zhao, Li Cheng, Xu Yang, Peng Feng, Liyuan Liu, Nanjian Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Infrared small target detection is a key technique in infrared search and tracking (IRST) systems. Although deep learning has been widely used in the vision tasks of visible light images recently, it is rarely used in infrared small target detection due to the difficulty in learning small target features. In this paper, we propose a novel lightweight convolutional neural network TBC-Net for infrared small target detection. The TBCNet consists of a target extraction module (TEM) and a semantic constraint module (SCM), which are used to extract small targets from infrared images and to classify the extracted target images during the training, respectively. Meanwhile, we propose a joint loss function and a training method. The SCM imposes a semantic constraint on TEM by combining the high-level classification task and solve the problem of the difficulty to learn features caused by class imbalance problem. During the training, the targets are extracted from the input image and then be classified by SCM. During the inference, only the TEM is used to detect the small targets. We also propose a data synthesis method to generate training data. The experimental results show that compared with the traditional methods, TBC-Net can better reduce the false alarm caused by complicated background, the proposed network structure and joint loss have a significant improvement on small target feature learning. Besides, TBC-Net can achieve real-time detection on the NVIDIA Jetson AGX Xavier development board, which is suitable for applications such as field research with drones equipped with infrared sensors.



### An Abstraction Model for Semantic Segmentation Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1912.11995v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11995v2)
- **Published**: 2019-12-27 05:39:24+00:00
- **Updated**: 2022-12-01 23:55:21+00:00
- **Authors**: Reihaneh Teymoori, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi
- **Comment**: This is the corrected version of the previously submitted paper. Many
  grammatical and spelling errors are now corrected. The technical content of
  the paper is unchanged
- **Journal**: None
- **Summary**: Semantic segmentation classifies each pixel in the image. Due to its advantages, semantic segmentation is used in many tasks, such as cancer detection, robot-assisted surgery, satellite image analysis, and self-driving cars. Accuracy and efficiency are the two crucial goals for this purpose, and several state-of-the-art neural networks exist. By employing different techniques, new solutions have been presented in each method to increase efficiency and accuracy and reduce costs. However, the diversity of the implemented approaches for semantic segmentation makes it difficult for researchers to achieve a comprehensive view of the field. In this paper, an abstraction model for semantic segmentation offers a comprehensive view of the field. The proposed framework consists of four general blocks that cover the operation of the majority of semantic segmentation methods. We also compare different approaches and analyze each of the four abstraction blocks' importance in each method's operation.



### Visual Agreement Regularized Training for Multi-Modal Machine Translation
- **Arxiv ID**: http://arxiv.org/abs/1912.12014v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12014v1)
- **Published**: 2019-12-27 07:46:29+00:00
- **Updated**: 2019-12-27 07:46:29+00:00
- **Authors**: Pengcheng Yang, Boxing Chen, Pei Zhang, Xu Sun
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Multi-modal machine translation aims at translating the source sentence into a different language in the presence of the paired image. Previous work suggests that additional visual information only provides dispensable help to translation, which is needed in several very special cases such as translating ambiguous words. To make better use of visual information, this work presents visual agreement regularized training. The proposed approach jointly trains the source-to-target and target-to-source translation models and encourages them to share the same focus on the visual information when generating semantically equivalent visual words (e.g. "ball" in English and "ballon" in French). Besides, a simple yet effective multi-head co-attention model is also introduced to capture interactions between visual and textual features. The results show that our approaches can outperform competitive baselines by a large margin on the Multi30k dataset. Further analysis demonstrates that the proposed regularized training can effectively improve the agreement of attention on the image, leading to better use of visual information.



### A General Framework for Saliency Detection Methods
- **Arxiv ID**: http://arxiv.org/abs/1912.12027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12027v2)
- **Published**: 2019-12-27 08:49:53+00:00
- **Updated**: 2022-12-02 00:01:42+00:00
- **Authors**: Fateme Mostafaie, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi
- **Comment**: Many grammatical and spelling errors are corrected. The technical
  content is not changed
- **Journal**: None
- **Summary**: Saliency detection is one of the most challenging problems in image analysis and computer vision. Many approaches propose different architectures based on the psychological and biological properties of the human visual attention system. However, there is still no abstract framework that summarizes the existing methods. In this paper, we offered a general framework for saliency models, which consists of five main steps: pre-processing, feature extraction, saliency map generation, saliency map combination, and post-processing. Also, we study different saliency models containing each level and compare their performance. This framework helps researchers to have a comprehensive view of studying new methods.



### Deep Learning for 3D Point Clouds: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1912.12033v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12033v2)
- **Published**: 2019-12-27 09:15:54+00:00
- **Updated**: 2020-06-23 10:54:36+00:00
- **Authors**: Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, Mohammed Bennamoun
- **Comment**: Accepted by IEEE TPAMI. Project page:
  https://github.com/QingyongHu/SoTA-Point-Cloud
- **Journal**: None
- **Summary**: Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.



### A sparsity augmented probabilistic collaborative representation based classification method
- **Arxiv ID**: http://arxiv.org/abs/1912.12044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12044v1)
- **Published**: 2019-12-27 10:06:20+00:00
- **Updated**: 2019-12-27 10:06:20+00:00
- **Authors**: Xiao-Yun Cai, He-Feng Yin
- **Comment**: This manuscript has been accepted for publication by Journal of
  Algorithms and Computational Technology (JACT)
- **Journal**: None
- **Summary**: In order to enhance the performance of image recognition, a sparsity augmented probabilistic collaborative representation based classification (SA-ProCRC) method is presented. The proposed method obtains the dense coefficient through ProCRC, then augments the dense coefficient with a sparse one, and the sparse coefficient is attained by the orthogonal matching pursuit (OMP) algorithm. In contrast to conventional methods which require explicit computation of the reconstruction residuals for each class, the proposed method employs the augmented coefficient and the label matrix of the training samples to classify the test sample. Experimental results indicate that the proposed method can achieve promising results for face and scene images. The source code of our proposed SA-ProCRC is accessible at https://github.com/yinhefeng/SAProCRC.



### 3D Sensing of a Moving Object with a Nodding 2D LIDAR and Reconfigurable Mirrors
- **Arxiv ID**: http://arxiv.org/abs/1912.13461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.13461v1)
- **Published**: 2019-12-27 10:22:26+00:00
- **Updated**: 2019-12-27 10:22:26+00:00
- **Authors**: Anindya Harchowdhury, Lindsay Kleeman, Leena Vachhani
- **Comment**: None
- **Journal**: None
- **Summary**: Perception in 3D has become standard practice for a large part of robotics applications. High quality 3D perception is costly. Our previous work on a nodding 2D Lidar provides high quality 3D depth information with low cost, but the sparse data generated by this sensor poses challenges in understanding the characteristics of moving objects within an uncertain environment. This paper proposes a novel design of the nodding Lidar but provides dynamic reconfigurability in terms of limiting the field of view of the sensor using a set of optical mirrors. It not only provides denser scans, but it also achieves a three times higher scan update rate. Additionally, we propose a novel calibration mechanism for this sensor and prove its effectiveness for dynamic object detection and tracking.



### Embedding of FRPN in CNN architecture
- **Arxiv ID**: http://arxiv.org/abs/2001.05851v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2001.05851v1)
- **Published**: 2019-12-27 12:41:15+00:00
- **Updated**: 2019-12-27 12:41:15+00:00
- **Authors**: Alberto Rossi, Markus Hagenbuchner, Franco Scarselli, Ah Chung Tsoi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper extends the fully recursive perceptron network (FRPN) model for vectorial inputs to include deep convolutional neural networks (CNNs) which can accept multi-dimensional inputs. A FRPN consists of a recursive layer, which, given a fixed input, iteratively computes an equilibrium state. The unfolding realized with this kind of iterative mechanism allows to simulate a deep neural network with any number of layers. The extension of the FRPN to CNN results in an architecture, which we call convolutional-FRPN (C-FRPN), where the convolutional layers are recursive. The method is evaluated on several image classification benchmarks. It is shown that the C-FRPN consistently outperforms standard CNNs having the same number of parameters. The gap in performance is particularly large for small networks, showing that the C-FRPN is a very powerful architecture, since it allows to obtain equivalent performance with fewer parameters when compared with deep CNNs.



### Pointwise Attention-Based Atrous Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12082v1)
- **Published**: 2019-12-27 13:12:58+00:00
- **Updated**: 2019-12-27 13:12:58+00:00
- **Authors**: Mobina Mahdavi, Fahimeh Fooladgar, Shohreh Kasaei
- **Comment**: 7 pages, 6 figures. Author one and author two contributed equally
- **Journal**: None
- **Summary**: With the rapid progress of deep convolutional neural networks, in almost all robotic applications, the availability of 3D point clouds improves the accuracy of 3D semantic segmentation methods. Rendering of these irregular, unstructured, and unordered 3D points to 2D images from multiple viewpoints imposes some issues such as loss of information due to 3D to 2D projection, discretizing artifacts, and high computational costs. To efficiently deal with a large number of points and incorporate more context of each point, a pointwise attention-based atrous convolutional neural network architecture is proposed. It focuses on salient 3D feature points among all feature maps while considering outstanding contextual information via spatial channel-wise attention modules. The proposed model has been evaluated on the two most important 3D point cloud datasets for the 3D semantic segmentation task. It achieves a reasonable performance compared to state-of-the-art models in terms of accuracy, with a much smaller number of parameters.



### One Point, One Object: Simultaneous 3D Object Segmentation and 6-DOF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.12095v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12095v1)
- **Published**: 2019-12-27 13:48:03+00:00
- **Updated**: 2019-12-27 13:48:03+00:00
- **Authors**: Hongsen Liu, Yang Cong, Yandong Tang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a single-shot method for simultaneous 3D object segmentation and 6-DOF pose estimation in pure 3D point clouds scenes based on a consensus that \emph{one point only belongs to one object}, i.e., each point has the potential power to predict the 6-DOF pose of its corresponding object. Unlike the recently proposed methods of the similar task, which rely on 2D detectors to predict the projection of 3D corners of the 3D bounding boxes and the 6-DOF pose must be estimated by a PnP like spatial transformation method, ours is concise enough not to require additional spatial transformation between different dimensions. Due to the lack of training data for many objects, the recently proposed 2D detection methods try to generate training data by using rendering engine and achieve good results. However, rendering in 3D space along with 6-DOF is relatively difficult. Therefore, we propose an augmented reality technology to generate the training data in semi-virtual reality 3D space. The key component of our method is a multi-task CNN architecture that can simultaneously predicts the 3D object segmentation and 6-DOF pose estimation in pure 3D point clouds.   For experimental evaluation, we generate expanded training data for two state-of-the-arts 3D object datasets \cite{PLCHF}\cite{TLINEMOD} by using Augmented Reality technology (AR). We evaluate our proposed method on the two datasets. The results show that our method can be well generalized into multiple scenarios and provide performance comparable to or better than the state-of-the-arts.



### Quaternion Equivariant Capsule Networks for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.12098v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12098v3)
- **Published**: 2019-12-27 13:51:17+00:00
- **Updated**: 2020-08-23 13:12:46+00:00
- **Authors**: Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, Federico Tombari
- **Comment**: Oral Presentation at ECCV 2020. Find our video under:
  https://youtu.be/LHh56snwhTA. We release our sources at:
  http://tolgabirdal.github.io/qecnetworks
- **Journal**: None
- **Summary**: We present a 3D capsule module for processing point clouds that is equivariant to 3D rotations and translations, as well as invariant to permutations of the input points. The operator receives a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end transformation equivariance through a novel dynamic routing procedure on quaternions. Further, we theoretically connect dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving \emph{iterative re-weighted least squares} (IRLS) problems with provable convergence properties. It is shown that such group dynamic routing can be interpreted as robust IRLS rotation averaging on capsule votes, where information is routed based on the final inlier scores. Based on our operator, we build a capsule network that disentangles geometry from pose, paving the way for more informative descriptors and a structured latent space. Our architecture allows joint object classification and orientation estimation without explicit supervision of rotations. We validate our algorithm empirically on common benchmark datasets.



### A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1912.12101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.12101v1)
- **Published**: 2019-12-27 13:56:13+00:00
- **Updated**: 2019-12-27 13:56:13+00:00
- **Authors**: Linh Kästner, Vlad Catalin Frasineanu, Jens Lambrecht
- **Comment**: 7 Pages, 7 Figures
- **Journal**: None
- **Summary**: Augmented Reality and mobile robots are gaining much attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate the Augmented Reality device using 3D depth sensor data. We use the depth camera of a cutting edge Augmented Reality Device - the Microsoft Hololens for deep learning based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on the point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.



### Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation
- **Arxiv ID**: http://arxiv.org/abs/1912.12215v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12215v3)
- **Published**: 2019-12-27 16:14:53+00:00
- **Updated**: 2020-03-31 01:31:12+00:00
- **Authors**: Hao Tang, Dan Xu, Yan Yan, Philip H. S. Torr, Nicu Sebe
- **Comment**: Accepted to CVPR 2020, camera ready (10 pages) + supplementary (18
  pages)
- **Journal**: None
- **Summary**: In this paper, we address the task of semantic-guided scene generation. One open challenge in scene generation is the difficulty of the generation of small objects and detailed local texture, which has been widely observed in global image-level generation methods. To tackle this issue, in this work we consider learning the scene generation in a local context, and correspondingly design a local class-specific generative network with semantic maps as a guidance, which separately constructs and learns sub-generators concentrating on the generation of different classes, and is able to provide more scene details. To learn more discriminative class-specific feature representations for the local generation, a novel classification module is also proposed. To combine the advantage of both the global image-level and the local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Extensive experiments on two scene image generation tasks show superior generation performance of the proposed model. The state-of-the-art results are established by large margins on both tasks and on challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.



### Combining Deep Learning and Verification for Precise Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.12270v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12270v4)
- **Published**: 2019-12-27 18:11:20+00:00
- **Updated**: 2020-06-29 15:18:52+00:00
- **Authors**: Siddharth Ancha, Junyu Nan, David Held
- **Comment**: 9 pages main paper, 2 pages references, 10 pages supplementary
  material
- **Journal**: Conference on Robot Learning (CoRL), 2019
- **Summary**: Deep learning object detectors often return false positives with very high confidence. Although they optimize generic detection performance, such as mean average precision (mAP), they are not designed for reliability. For a reliable detection system, if a high confidence detection is made, we would want high certainty that the object has indeed been detected. To achieve this, we have developed a set of verification tests which a proposed detection must pass to be accepted. We develop a theoretical framework which proves that, under certain assumptions, our verification tests will not accept any false positives. Based on an approximation to this framework, we present a practical detection system that can verify, with high precision, whether each detection of a machine-learning based object detector is correct. We show that these tests can improve the overall accuracy of a base detector and that accepted examples are highly likely to be correct. This allows the detector to operate in a high precision regime and can thus be used for robotic perception systems as a reliable instance detection method. Code is available at https://github.com/siddancha/FlowVerify.



### Seeing without Looking: Contextual Rescoring of Object Detections for AP Maximization
- **Arxiv ID**: http://arxiv.org/abs/1912.12290v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12290v2)
- **Published**: 2019-12-27 18:56:29+00:00
- **Updated**: 2020-03-30 16:09:20+00:00
- **Authors**: Lourenço V. Pato, Renato Negrinho, Pedro M. Q. Aguiar
- **Comment**: 14 pages, 12 figures
- **Journal**: None
- **Summary**: The majority of current object detectors lack context: class predictions are made independently from other detections. We propose to incorporate context in object detection by post-processing the output of an arbitrary detector to rescore the confidences of its detections. Rescoring is done by conditioning on contextual information from the entire set of detections: their confidences, predicted classes, and positions. We show that AP can be improved by simply reassigning the detection confidence values such that true positives that survive longer (i.e., those with the correct class and large IoU) are scored higher than false positives or detections with small IoU. In this setting, we use a bidirectional RNN with attention for contextual rescoring and introduce a training target that uses the IoU with ground truth to maximize AP for the given set of detections. The fact that our approach does not require access to visual features makes it computationally inexpensive and agnostic to the detection architecture. In spite of this simplicity, our model consistently improves AP over strong pre-trained baselines (Cascade R-CNN and Faster R-CNN with several backbones), particularly by reducing the confidence of duplicate detections (a learned form of non-maximum suppression) and removing out-of-context objects by conditioning on the confidences, classes, positions, and sizes of the co-occurrent detections. Code is available at https://github.com/LourencoVazPato/seeing-without-looking/



### Learning by Cheating
- **Arxiv ID**: http://arxiv.org/abs/1912.12294v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12294v1)
- **Published**: 2019-12-27 18:59:04+00:00
- **Updated**: 2019-12-27 18:59:04+00:00
- **Authors**: Dian Chen, Brady Zhou, Vladlen Koltun, Philipp Krähenbühl
- **Comment**: Paper published in CoRL2019
- **Journal**: None
- **Summary**: Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art. For the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw



### Deep Learning in Medical Image Registration: A Review
- **Arxiv ID**: http://arxiv.org/abs/1912.12318v1
- **DOI**: 10.1088/1361-6560/ab843e
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12318v1)
- **Published**: 2019-12-27 19:32:32+00:00
- **Updated**: 2019-12-27 19:32:32+00:00
- **Authors**: Yabo Fu, Yang Lei, Tonghe Wang, Walter J. Curran, Tian Liu, Xiaofeng Yang
- **Comment**: 32 pages, 4 figures, 9 tables
- **Journal**: None
- **Summary**: This paper presents a review of deep learning (DL) based medical image registration methods. We summarized the latest developments and applications of DL-based registration methods in the medical field. These methods were classified into seven categories according to their methods, functions and popularity. A detailed review of each category was presented, highlighting important contributions and identifying specific challenges. A short assessment was presented following the detailed review of each category to summarize its achievements and future potentials. We provided a comprehensive comparison among DL-based methods for lung and brain deformable registration using benchmark datasets. Lastly, we analyzed the statistics of all the cited works from various aspects, revealing the popularity and future trend of development in medical image registration using deep learning.



### ODE-based Deep Network for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.12325v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12325v1)
- **Published**: 2019-12-27 20:13:30+00:00
- **Updated**: 2019-12-27 20:13:30+00:00
- **Authors**: Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield
- **Comment**: None
- **Journal**: None
- **Summary**: Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand and scan time directly depends on the number of acquired k-space samples. The data-driven methods based on deep neural networks have resulted in promising improvements, compared to the conventional methods, in image reconstruction algorithms. The connection between deep neural network and Ordinary Differential Equation (ODE) has been observed and studied recently. The studies show that different residual networks can be interpreted as Euler discretization of an ODE. In this paper, we propose an ODE-based deep network for MRI reconstruction to enable the rapid acquisition of MR images with improved image quality. Our results with undersampled data demonstrate that our method can deliver higher quality images in comparison to the reconstruction methods based on the standard UNet network and Residual network.



