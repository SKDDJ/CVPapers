# Arxiv Papers in cs.CV on 2019-12-21
### Learning Diverse Stochastic Human-Action Generators by Learning Smooth Latent Transitions
- **Arxiv ID**: http://arxiv.org/abs/1912.10150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10150v1)
- **Published**: 2019-12-21 00:02:27+00:00
- **Updated**: 2019-12-21 00:02:27+00:00
- **Authors**: Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan Zhou, Junsong Yuan, Changyou Chen
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: Human-motion generation is a long-standing challenging task due to the requirement of accurately modeling complex and diverse dynamic patterns. Most existing methods adopt sequence models such as RNN to directly model transitions in the original action space. Due to high dimensionality and potential noise, such modeling of action transitions is particularly challenging. In this paper, we focus on skeleton-based action generation and propose to model smooth and diverse transitions on a latent space of action sequences with much lower dimensionality. Conditioned on a latent sequence, actions are generated by a frame-wise decoder shared by all latent action-poses. Specifically, an implicit RNN is defined to model smooth latent sequences, whose randomness (diversity) is controlled by noise from the input. Different from standard action-prediction methods, our model can generate action sequences from pure noise without any conditional action poses. Remarkably, it can also generate unseen actions from mixed classes during training. Our model is learned with a bi-directional generative-adversarial-net framework, which not only can generate diverse action sequences of a particular class or mix classes, but also learns to classify action sequences within the same model. Experimental results show the superiority of our method in both diverse action-sequence generation and classification, relative to existing methods.



### Measuring Dataset Granularity
- **Arxiv ID**: http://arxiv.org/abs/1912.10154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10154v1)
- **Published**: 2019-12-21 00:44:52+00:00
- **Updated**: 2019-12-21 00:44:52+00:00
- **Authors**: Yin Cui, Zeqi Gu, Dhruv Mahajan, Laurens van der Maaten, Serge Belongie, Ser-Nam Lim
- **Comment**: Code is available at:
  https://github.com/richardaecn/dataset-granularity
- **Journal**: None
- **Summary**: Despite the increasing visibility of fine-grained recognition in our field, "fine-grained'' has thus far lacked a precise definition. In this work, building upon clustering theory, we pursue a framework for measuring dataset granularity. We argue that dataset granularity should depend not only on the data samples and their labels, but also on the distance function we choose. We propose an axiomatic framework to capture desired properties for a dataset granularity measure and provide examples of measures that satisfy these properties. We assess each measure via experiments on datasets with hierarchical labels of varying granularity. When measuring granularity in commonly used datasets with our measure, we find that certain datasets that are widely considered fine-grained in fact contain subsets of considerable size that are substantially more coarse-grained than datasets generally regarded as coarse-grained. We also investigate the interplay between dataset granularity with a variety of factors and find that fine-grained datasets are more difficult to learn from, more difficult to transfer to, more difficult to perform few-shot learning with, and more vulnerable to adversarial attacks.



### DBP: Discrimination Based Block-Level Pruning for Deep Model Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1912.10178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10178v1)
- **Published**: 2019-12-21 02:11:47+00:00
- **Updated**: 2019-12-21 02:11:47+00:00
- **Authors**: Wenxiao Wang, Shuai Zhao, Minghao Chen, Jinming Hu, Deng Cai, Haifeng Liu
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Neural network pruning is one of the most popular methods of accelerating the inference of deep convolutional neural networks (CNNs). The dominant pruning methods, filter-level pruning methods, evaluate their performance through the reduction ratio of computations and deem that a higher reduction ratio of computations is equivalent to a higher acceleration ratio in terms of inference time. However, we argue that they are not equivalent if parallel computing is considered. Given that filter-level pruning only prunes filters in layers and computations in a layer usually run in parallel, most computations reduced by filter-level pruning usually run in parallel with the un-reduced ones. Thus, the acceleration ratio of filter-level pruning is limited. To get a higher acceleration ratio, it is better to prune redundant layers because computations of different layers cannot run in parallel. In this paper, we propose our Discrimination based Block-level Pruning method (DBP). Specifically, DBP takes a sequence of consecutive layers (e.g., Conv-BN-ReLu) as a block and removes redundant blocks according to the discrimination of their output features. As a result, DBP achieves a considerable acceleration ratio by reducing the depth of CNNs. Extensive experiments show that DBP has surpassed state-of-the-art filter-level pruning methods in both accuracy and acceleration ratio. Our code will be made available soon.



### Jacobian Adversarially Regularized Networks for Robustness
- **Arxiv ID**: http://arxiv.org/abs/1912.10185v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.10185v2)
- **Published**: 2019-12-21 02:46:50+00:00
- **Updated**: 2020-01-29 08:06:12+00:00
- **Authors**: Alvin Chan, Yi Tay, Yew Soon Ong, Jie Fu
- **Comment**: ICLR 2020 Camera Ready
- **Journal**: None
- **Summary**: Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.



### Eliminating cross-camera bias for vehicle re-identification
- **Arxiv ID**: http://arxiv.org/abs/1912.10193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10193v1)
- **Published**: 2019-12-21 04:14:36+00:00
- **Updated**: 2019-12-21 04:14:36+00:00
- **Authors**: Jinjia Peng, Guangqi Jiang, Dongyan Chen, Tongtong Zhao, Huibing Wang, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (reID) often requires recognize a target vehicle in large datasets captured from multi-cameras. It plays an important role in the automatic analysis of the increasing urban surveillance videos, which has become a hot topic in recent years. However, the appearance of vehicle images is easily affected by the environment that various illuminations, different backgrounds and viewpoints, which leads to the large bias between different cameras. To address this problem, this paper proposes a cross-camera adaptation framework (CCA), which smooths the bias by exploiting the common space between cameras for all samples. CCA first transfers images from multi-cameras into one camera to reduce the impact of the illumination and resolution, which generates the samples with the similar distribution. Then, to eliminate the influence of background and focus on the valuable parts, we propose an attention alignment network (AANet) to learn powerful features for vehicle reID. Specially, in AANet, the spatial transfer network with attention module is introduced to locate a series of the most discriminative regions with high-attention weights and suppress the background. Moreover, comprehensive experimental results have demonstrated that our proposed CCA can achieve excellent performances on benchmark datasets VehicleID and VeRi-776.



### Convolutional Neural Networks: A Binocular Vision Perspective
- **Arxiv ID**: http://arxiv.org/abs/1912.10201v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10201v1)
- **Published**: 2019-12-21 05:17:12+00:00
- **Updated**: 2019-12-21 05:17:12+00:00
- **Authors**: Yigit Oktar, Diclehan Karakaya, Oguzhan Ulucan, Mehmet Turkan
- **Comment**: None
- **Journal**: None
- **Summary**: It is arguable that whether the single camera captured (monocular) image datasets are sufficient enough to train and test convolutional neural networks (CNNs) for imitating the biological neural network structures of the human brain. As human visual system works in binocular, the collaboration of the eyes with the two brain lobes needs more investigation for improvements in such CNN-based visual imagery analysis applications. It is indeed questionable that if respective visual fields of each eye and the associated brain lobes are responsible for different learning abilities of the same scene. There are such open questions in this field of research which need rigorous investigation in order to further understand the nature of the human visual system, hence improve the currently available deep learning applications. This position paper analyses a binocular CNNs architecture that is more analogous to the biological structure of the human visual system than the conventional deep learning techniques. While taking a structure called optic chiasma into account, this architecture consists of basically two parallel CNN structures associated with each visual field and the brain lobe, fully connected later possibly as in the primary visual cortex (V1). Experimental results demonstrate that binocular learning of two different visual fields leads to better classification rates on average, when compared to classical CNN architectures.



### Decoupled Attention Network for Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.10205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10205v1)
- **Published**: 2019-12-21 05:51:58+00:00
- **Updated**: 2019-12-21 05:51:58+00:00
- **Authors**: Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xiaoxue Chen, Yaqiang Wu, Qianying Wang, Mingxiang Cai
- **Comment**: 9 pages, 8 figures, 6 tables, accepted by AAAI-2020
- **Journal**: None
- **Summary**: Text recognition has attracted considerable research interests because of its various applications. The cutting-edge text recognition methods are based on attention mechanisms. However, most of attention methods usually suffer from serious alignment problem due to its recurrency alignment operation, where the alignment relies on historical decoding results. To remedy this issue, we propose a decoupled attention network (DAN), which decouples the alignment operation from using historical decoding results. DAN is an effective, flexible and robust end-to-end text recognizer, which consists of three components: 1) a feature encoder that extracts visual features from the input image; 2) a convolutional alignment module that performs the alignment operation based on visual features from the encoder; and 3) a decoupled text decoder that makes final prediction by jointly using the feature map and attention maps. Experimental results show that DAN achieves state-of-the-art performance on multiple text recognition tasks, including offline handwritten text recognition and regular/irregular scene text recognition.



### Towards Efficient Training for Neural Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1912.10207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10207v1)
- **Published**: 2019-12-21 05:57:30+00:00
- **Updated**: 2019-12-21 05:57:30+00:00
- **Authors**: Qing Jin, Linjie Yang, Zhenyu Liao
- **Comment**: 23 pages, 8 figures
- **Journal**: None
- **Summary**: Quantization reduces computation costs of neural networks but suffers from performance degeneration. Is this accuracy drop due to the reduced capacity, or inefficient training during the quantization procedure? After looking into the gradient propagation process of neural networks by viewing the weights and intermediate activations as random variables, we discover two critical rules for efficient training. Recent quantization approaches violates the two rules and results in degenerated convergence. To deal with this problem, we propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitates efficient training. We also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models including MobileNet-V1/V2 and PreResNet-50.



### Spatio-Temporal Segmentation in 3D Echocardiographic Sequences using Fractional Brownian Motion
- **Arxiv ID**: http://arxiv.org/abs/1912.10220v1
- **DOI**: 10.1109/TBME.2019.2958701
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10220v1)
- **Published**: 2019-12-21 08:22:18+00:00
- **Updated**: 2019-12-21 08:22:18+00:00
- **Authors**: Omar S. Al-Kadi
- **Comment**: 11 pages, 10 figures, 2 tables, journal article
- **Journal**: IEEE Transactions on Biomedical Engineering, 2019
- **Summary**: An important aspect for an improved cardiac functional analysis is the accurate segmentation of the left ventricle (LV). A novel approach for fully-automated segmentation of the LV endocardium and epicardium contours is presented. This is mainly based on the natural physical characteristics of the LV shape structure. Both sides of the LV boundaries exhibit natural elliptical curvatures by having details on various scales, i.e. exhibiting fractal-like characteristics. The fractional Brownian motion (fBm), which is a non-stationary stochastic process, integrates well with the stochastic nature of ultrasound echoes. It has the advantage of representing a wide range of non-stationary signals and can quantify statistical local self-similarity throughout the time-sequence ultrasound images. The locally characterized boundaries of the fBm segmented LV were further iteratively refined using global information by means of second-order moments. The method is benchmarked using synthetic 3D+time echocardiographic sequences for normal and different ischemic cardiomyopathy, and results compared with state-of-the-art LV segmentation. Furthermore, the framework was validated against real data from canine cases with expert-defined segmentations and demonstrated improved accuracy. The fBm-based segmentation algorithm is fully automatic and has the potential to be used clinically together with 3D echocardiography for improved cardiovascular disease diagnosis.



### Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.12184v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12184v1)
- **Published**: 2019-12-21 08:32:27+00:00
- **Updated**: 2019-12-21 08:32:27+00:00
- **Authors**: Chia-Mu Yu, Ching-Tang Chang, Yen-Wu Ti
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in AI technology have made the forgery of digital images and videos easier, and it has become significantly more difficult to identify such forgeries. These forgeries, if disseminated with malicious intent, can negatively impact social and political stability, and pose significant ethical and legal challenges as well. Deepfake is a variant of auto-encoders that use deep learning techniques to identify and exchange images of a person's face in a picture or film. Deepfake can result in an erosion of public trust in digital images and videos, which has far-reaching effects on political and social stability. This study therefore proposes a solution for facial forgery detection to determine if a picture or film has ever been processed by Deepfake. The proposed solution reaches detection efficiency by using the recently proposed separable convolutional neural network (CNN) and image segmentation. In addition, this study also examined how different image segmentation methods affect detection results. Finally, the ensemble model is used to improve detection capabilities. Experiment results demonstrated the excellent performance of the proposed solution.



### Exploiting Style and Attention in Real-World Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1912.10227v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10227v2)
- **Published**: 2019-12-21 09:10:53+00:00
- **Updated**: 2020-10-11 11:58:08+00:00
- **Authors**: Xin Ma, Yi Li, Huaibo Huang, Mandi Luo, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world image super-resolution (SR) is a challenging image translation problem. Low-resolution (LR) images are often generated by various unknown transformations rather than by applying simple bilinear down-sampling on high-resolution (HR) images. To address this issue, this paper proposes a novel pipeline which exploits style and attention mechanism in real-world SR. Our pipeline consists of a style Variational Autoencoder (styleVAE) and a SR network incorporated with attention mechanism. To get real-world-like low-quality images paired with the HR images, we design the styleVAE to transfer the complex nuisance factors in real-world LR images to the generated LR images. We also use mutual information estimation (MI) to get better style information. For our SR network, we firstly propose a global attention residual block to learn long-range dependencies in images. Then another local attention residual block is proposed to enforce the attention of SR network moving to local areas of images in which texture detail will be filled. It is worth noticing that styleVAE can be presented in a plug-and-play manner and thus can help to improve the generalization and robustness of our SR method as well as other SR methods. Extensive experiments demonstrate that our method surpasses the state-of-the-art work, both quantitatively and qualitatively.



### A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images
- **Arxiv ID**: http://arxiv.org/abs/1912.10230v5
- **DOI**: 10.1080/08839514.2022.2032924
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10230v5)
- **Published**: 2019-12-21 09:31:09+00:00
- **Updated**: 2022-03-16 10:28:03+00:00
- **Authors**: Irem Ulku, Erdem Akagunduz
- **Comment**: published in the J. of Applied Artificial Intelligence (09 Feb 2022)
- **Journal**: None
- **Summary**: Semantic segmentation is the pixel-wise labelling of an image. Since the problem is defined at the pixel level, determining image class labels only is not acceptable, but localising them at the original image pixel resolution is necessary. Boosted by the extraordinary ability of convolutional neural networks (CNN) in creating semantic, high level and hierarchical image features; several deep learning-based 2D semantic segmentation approaches have been proposed within the last decade. In this survey, we mainly focus on the recent scientific developments in semantic segmentation, specifically on deep learning-based methods using 2D images. We started with an analysis of the public image sets and leaderboards for 2D semantic segmentation, with an overview of the techniques employed in performance evaluation. In examining the evolution of the field, we chronologically categorised the approaches into three main periods, namely pre-and early deep learning era, the fully convolutional era, and the post-FCN era. We technically analysed the solutions put forward in terms of solving the fundamental problems of the field, such as fine-grained localisation and scale invariance. Before drawing our conclusions, we present a table of methods from all mentioned eras, with a summary of each approach that explains their contribution to the field. We conclude the survey by discussing the current challenges of the field and to what extent they have been solved.



### Latent Variables on Spheres for Autoencoders in High Dimensions
- **Arxiv ID**: http://arxiv.org/abs/1912.10233v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10233v2)
- **Published**: 2019-12-21 09:53:53+00:00
- **Updated**: 2020-02-17 02:20:03+00:00
- **Authors**: Deli Zhao, Jiapeng Zhu, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Variational Auto-Encoder (VAE) has been widely applied as a fundamental generative model in machine learning. For complex samples like imagery objects or scenes, however, VAE suffers from the dimensional dilemma between reconstruction precision that needs high-dimensional latent codes and probabilistic inference that favors a low-dimensional latent space. By virtue of high-dimensional geometry, we propose a very simple algorithm, called Spherical Auto-Encoder (SAE), completely different from existing VAEs to address the issue. SAE is in essence the vanilla autoencoder with spherical normalization on the latent space. We analyze the unique characteristics of random variables on spheres in high dimensions and argue that random variables on spheres are agnostic to various prior distributions and data modes when the dimension is sufficiently high. Therefore, SAE can harness a high-dimensional latent space to improve the inference precision of latent codes while maintain the property of stochastic sampling from priors. The experiments on sampling and inference validate our theoretical analysis and the superiority of SAE.



### Seek and You Will Find: A New Optimized Framework for Efficient Detection of Pedestrian
- **Arxiv ID**: http://arxiv.org/abs/1912.10241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10241v1)
- **Published**: 2019-12-21 10:33:56+00:00
- **Updated**: 2019-12-21 10:33:56+00:00
- **Authors**: Sudip Das, Partha Sarathi Mukherjee, Ujjwal Bhattacharya
- **Comment**: None
- **Journal**: None
- **Summary**: Studies of object detection and localization, particularly pedestrian detection have received considerable attention in recent times due to its several prospective applications such as surveillance, driving assistance, autonomous cars, etc. Also, a significant trend of latest research studies in related problem areas is the use of sophisticated Deep Learning based approaches to improve the benchmark performance on various standard datasets. A trade-off between the speed (number of video frames processed per second) and detection accuracy has often been reported in the existing literature. In this article, we present a new but simple deep learning based strategy for pedestrian detection that improves this trade-off. Since training of similar models using publicly available sample datasets failed to improve the detection performance to some significant extent, particularly for the instances of pedestrians of smaller sizes, we have developed a new sample dataset consisting of more than 80K annotated pedestrian figures in videos recorded under varying traffic conditions. Performance of the proposed model on the test samples of the new dataset and two other existing datasets, namely Caltech Pedestrian Dataset (CPD) and CityPerson Dataset (CD) have been obtained. Our proposed system shows nearly 16\% improvement over the existing state-of-the-art result.



### Research on Clustering Performance of Sparse Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1912.10256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10256v1)
- **Published**: 2019-12-21 12:41:08+00:00
- **Updated**: 2019-12-21 12:41:08+00:00
- **Authors**: Wen-Jin Fu, Xiao-Jun Wu, He-Feng Yin, Wen-Bo Hu
- **Comment**: 12 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, sparse subspace clustering has been a valid tool to deal with high-dimensional data. There are two essential steps in the framework of sparse subspace clustering. One is solving the coefficient matrix of data, and the other is constructing the affinity matrix from the coefficient matrix, which is applied to the spectral clustering. This paper investigates the factors which affect clustering performance from both clustering accuracy and stability of the approaches based on existing algorithms. We select four methods to solve the coefficient matrix and use four different ways to construct a similarity matrix for each coefficient matrix. Then we compare the clustering performance of different combinations on three datasets. The experimental results indicate that both the coefficient matrix and affinity matrix have a huge influence on clustering performance and how to develop a stable and valid algorithm still needs to be studied.



### A sparse resultant based method for efficient minimal solvers
- **Arxiv ID**: http://arxiv.org/abs/1912.10268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/1912.10268v1)
- **Published**: 2019-12-21 14:29:44+00:00
- **Updated**: 2019-12-21 14:29:44+00:00
- **Authors**: Snehal Bhayani, Zuzana Kukelova, Janne Heikkilä
- **Comment**: None
- **Journal**: None
- **Summary**: Many computer vision applications require robust and efficient estimation of camera geometry. The robust estimation is usually based on solving camera geometry problems from a minimal number of input data measurements, i.e. solving minimal problems in a RANSAC framework. Minimal problems often result in complex systems of polynomial equations. Many state-of-the-art efficient polynomial solvers to these problems are based on Gr\"obner bases and the action-matrix method that has been automatized and highly optimized in recent years. In this paper we study an alternative algebraic method for solving systems of polynomial equations, i.e., the sparse resultant-based method and propose a novel approach to convert the resultant constraint to an eigenvalue problem. This technique can significantly improve the efficiency and stability of existing resultant-based solvers. We applied our new resultant-based method to a large variety of computer vision problems and show that for most of the considered problems, the new method leads to solvers that are the same size as the the best available Gr\"obner basis solvers and of similar accuracy. For some problems the new sparse-resultant based method leads to even smaller and more stable solvers than the state-of-the-art Gr\"obner basis solvers. Our new method can be fully automatized and incorporated into existing tools for automatic generation of efficient polynomial solvers and as such it represents a competitive alternative to popular Gr\"obner basis methods for minimal problems in computer vision.



### UWGAN: Underwater GAN for Real-world Underwater Color Restoration and Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1912.10269v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10269v2)
- **Published**: 2019-12-21 14:31:35+00:00
- **Updated**: 2021-03-26 08:27:11+00:00
- **Authors**: Nan Wang, Yabin Zhou, Fenglei Han, Haitao Zhu, Jingzheng Yao
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: In real-world underwater environment, exploration of seabed resources, underwater archaeology, and underwater fishing rely on a variety of sensors, vision sensor is the most important one due to its high information content, non-intrusive, and passive nature. However, wavelength-dependent light attenuation and back-scattering result in color distortion and haze effect, which degrade the visibility of images. To address this problem, firstly, we proposed an unsupervised generative adversarial network (GAN) for generating realistic underwater images (color distortion and haze effect) from in-air image and depth map pairs based on improved underwater imaging model. Secondly, U-Net, which is trained efficiently using synthetic underwater dataset, is adopted for color restoration and dehazing. Our model directly reconstructs underwater clear images using end-to-end autoencoder networks, while maintaining scene content structural similarity. The results obtained by our method were compared with existing methods qualitatively and quantitatively. Experimental results obtained by the proposed model demonstrate well performance on open real-world underwater datasets, and the processing speed can reach up to 125FPS running on one NVIDIA 1060 GPU. Source code, sample datasets are made publicly available at https://github.com/infrontofme/UWGAN_UIE.



### \emph{cm}SalGAN: RGB-D Salient Object Detection with Cross-View Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.10280v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10280v2)
- **Published**: 2019-12-21 15:44:51+00:00
- **Updated**: 2020-05-16 05:36:44+00:00
- **Authors**: Bo Jiang, Zitai Zhou, Xiao Wang, Jin Tang, Bin Luo
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Image salient object detection (SOD) is an active research topic in computer vision and multimedia area. Fusing complementary information of RGB and depth has been demonstrated to be effective for image salient object detection which is known as RGB-D salient object detection problem. The main challenge for RGB-D salient object detection is how to exploit the salient cues of both intra-modality (RGB, depth) and cross-modality simultaneously which is known as cross-modality detection problem. In this paper, we tackle this challenge by designing a novel cross-modality Saliency Generative Adversarial Network (\emph{cm}SalGAN). \emph{cm}SalGAN aims to learn an optimal view-invariant and consistent pixel-level representation for RGB and depth images via a novel adversarial learning framework, which thus incorporates both information of intra-view and correlation information of cross-view images simultaneously for RGB-D saliency detection problem. To further improve the detection results, the attention mechanism and edge detection module are also incorporated into \emph{cm}SalGAN. The entire \emph{cm}SalGAN can be trained in an end-to-end manner by using the standard deep neural network framework. Experimental results show that \emph{cm}SalGAN achieves the new state-of-the-art RGB-D saliency detection performance on several benchmark datasets.



### Joint Forward-Backward Visual Odometry for Stereo Cameras
- **Arxiv ID**: http://arxiv.org/abs/1912.10293v1
- **DOI**: 10.1145/3352593.3352651
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10293v1)
- **Published**: 2019-12-21 16:40:01+00:00
- **Updated**: 2019-12-21 16:40:01+00:00
- **Authors**: Raghav Sardana, Rahul Kottath, Vinod Karar, Shashi Poddar
- **Comment**: Accepted to Advances in Robotics 2019
- **Journal**: None
- **Summary**: Visual odometry is a widely used technique in the field of robotics and automation to keep a track on the location of a robot using visual cues alone. In this paper, we propose a joint forward backward visual odometry framework by combining both, the forward motion and backward motion estimated from stereo cameras. The basic framework of LIBVIOS2 is used here for pose estimation as it can run in real-time on standard CPUs. The complementary nature of errors in the forward and backward mode of visual odometry helps in providing a refined motion estimation upon combining these individual estimates. In addition, two reliability measures, that is, forward-backward relative pose error and forward-backward absolute pose error have been proposed for evaluating visual odometry frameworks on its own without the requirement of any ground truth data. The proposed scheme is evaluated on the KITTI visual odometry dataset. The experimental results demonstrate improved accuracy of the proposed scheme over the traditional odometry pipeline without much increase in the system overload.



### Learning Representations by Maximizing Mutual Information in Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1912.13361v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.13361v2)
- **Published**: 2019-12-21 17:44:09+00:00
- **Updated**: 2020-01-07 05:42:39+00:00
- **Authors**: Ali Lotfi Rezaabad, Sriram Vishwanath
- **Comment**: None
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and $\beta$-VAE.



### Candidate Fusion: Integrating Language Modelling into a Sequence-to-Sequence Handwritten Word Recognition Architecture
- **Arxiv ID**: http://arxiv.org/abs/1912.10308v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1912.10308v1)
- **Published**: 2019-12-21 18:14:32+00:00
- **Updated**: 2019-12-21 18:14:32+00:00
- **Authors**: Lei Kang, Pau Riba, Mauricio Villegas, Alicia Fornés, Marçal Rusiñol
- **Comment**: None
- **Journal**: None
- **Summary**: Sequence-to-sequence models have recently become very popular for tackling handwritten word recognition problems. However, how to effectively integrate an external language model into such recognizer is still a challenging problem. The main challenge faced when training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system. Thus, the bias between both word corpora leads to incorrectness on the transcriptions, providing similar or even worse performances on the recognition task. In this work, we introduce Candidate Fusion, a novel way to integrate an external language model to a sequence-to-sequence architecture. Moreover, it provides suggestions from an external language knowledge, as a new input to the sequence-to-sequence recognizer. Hence, Candidate Fusion provides two improvements. On the one hand, the sequence-to-sequence recognizer has the flexibility not only to combine the information from itself and the language model, but also to choose the importance of the information provided by the language model. On the other hand, the external language model has the ability to adapt itself to the training corpus and even learn the most commonly errors produced from the recognizer. Finally, by conducting comprehensive experiments, the Candidate Fusion proves to outperform the state-of-the-art language models for handwritten word recognition tasks.



### Do Facial Expressions Predict Ad Sharing? A Large-Scale Observational Study
- **Arxiv ID**: http://arxiv.org/abs/1912.10311v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1912.10311v1)
- **Published**: 2019-12-21 18:25:57+00:00
- **Updated**: 2019-12-21 18:25:57+00:00
- **Authors**: Daniel McDuff, Jonah Berger
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: People often share news and information with their social connections, but why do some advertisements get shared more than others? A large-scale test examines whether facial responses predict sharing. Facial expressions play a key role in emotional expression. Using scalable automated facial coding algorithms, we quantify the facial expressions of thousands of individuals in response to hundreds of advertisements. Results suggest that not all emotions expressed during viewing increase sharing, and that the relationship between emotion and transmission is more complex than mere valence alone. Facial actions linked to positive emotions (i.e., smiles) were associated with increased sharing. But while some actions associated with negative emotion (e.g., lip depressor, associated with sadness) were linked to decreased sharing, others (i.e., nose wrinkles, associated with disgust) were linked to increased sharing. The ability to quickly collect facial responses at scale in peoples' natural environment has important implications for marketers and opens up a range of avenues for further research.



### Multimodal Prediction based on Graph Representations
- **Arxiv ID**: http://arxiv.org/abs/1912.10314v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10314v4)
- **Published**: 2019-12-21 18:47:35+00:00
- **Updated**: 2020-07-03 14:14:04+00:00
- **Authors**: Icaro Cavalcante Dourado, Salvatore Tabbone, Ricardo da Silva Torres
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a learning model, based on rank-fusion graphs, for general applicability in multimodal prediction tasks, such as multimodal regression and image classification. Rank-fusion graphs encode information from multiple descriptors and retrieval models, thus being able to capture underlying relationships between modalities, samples, and the collection itself. The solution is based on the encoding of multiple ranks for a query (or test sample), defined according to different criteria, into a graph. Later, we project the generated graph into an induced vector space, creating fusion vectors, targeting broader generality and efficiency. A fusion vector estimator is then built to infer whether a multimodal input object refers to a class or not. Our method is capable of promoting a fusion model better than early-fusion and late-fusion alternatives. Performed experiments in the context of multiple multimodal and visual datasets, as well as several descriptors and retrieval models, demonstrate that our learning model is highly effective for different prediction scenarios involving visual, textual, and multimodal features, yielding better effectiveness than state-of-the-art methods.



### Deep Automodulators
- **Arxiv ID**: http://arxiv.org/abs/1912.10321v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.10321v4)
- **Published**: 2019-12-21 19:16:33+00:00
- **Updated**: 2020-10-29 12:58:09+00:00
- **Authors**: Ari Heljakka, Yuxin Hou, Juho Kannala, Arno Solin
- **Comment**: To appear in Advances in Neural Information Processing Systems
  (NeurIPS 2020)
- **Journal**: None
- **Summary**: We introduce a new category of generative autoencoders called automodulators. These networks can faithfully reproduce individual real-world input images like regular autoencoders, but also generate a fused sample from an arbitrary combination of several such images, allowing instantaneous 'style-mixing' and other new applications. An automodulator decouples the data flow of decoder operations from statistical properties thereof and uses the latent vector to modulate the former by the latter, with a principled approach for mutual disentanglement of decoder layers. Prior work has explored similar decoder architecture with GANs, but their focus has been on random sampling. A corresponding autoencoder could operate on real input images. For the first time, we show how to train such a general-purpose model with sharp outputs in high resolution, using novel training techniques, demonstrated on four image data sets. Besides style-mixing, we show state-of-the-art results in autoencoder comparison, and visual image quality nearly indistinguishable from state-of-the-art GANs. We expect the automodulator variants to become a useful building block for image applications and other data domains.



### A Deep Learning Model for Chilean Bills Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.12120v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12120v1)
- **Published**: 2019-12-21 20:29:20+00:00
- **Updated**: 2019-12-21 20:29:20+00:00
- **Authors**: Daniel San Martin, Daniel Manzano
- **Comment**: 3 pages, 3 figures, Posters Content EVIC 2019
- **Journal**: None
- **Summary**: Automatic bill classification is an attractive task with many potential applications such as automated detection and counting in images or videos. To address this purpose we present a Deep Learning Model to classify Chilean Banknotes, because of its successful results in image processing applications. For optimal performance of the proposed model, data augmentation techniques are introduced due to the limited number of image samples. Positive results were achieved in this work, verifying that it could be a stating point to be extended to more complex applications.



### Depth Completion via Deep Basis Fitting
- **Arxiv ID**: http://arxiv.org/abs/1912.10336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10336v1)
- **Published**: 2019-12-21 21:07:12+00:00
- **Updated**: 2019-12-21 21:07:12+00:00
- **Authors**: Chao Qu, Ty Nguyen, Camillo J. Taylor
- **Comment**: WACV2020
- **Journal**: None
- **Summary**: In this paper we consider the task of image-guided depth completion where our system must infer the depth at every pixel of an input image based on the image content and a sparse set of depth measurements. We propose a novel approach that builds upon the strengths of modern deep learning techniques and classical optimization algorithms and significantly improves performance. The proposed method replaces the final $1\times 1$ convolutional layer employed in most depth completion networks with a least squares fitting module which computes weights by fitting the implicit depth bases to the given sparse depth measurements. In addition, we show how our proposed method can be naturally extended to a multi-scale formulation for improved self-supervised training. We demonstrate through extensive experiments on various datasets that our approach achieves consistent improvements over state-of-the-art baseline methods with small computational overhead.



### Tifinagh-IRCAM Handwritten character recognition using Deep learning
- **Arxiv ID**: http://arxiv.org/abs/1912.10338v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10338v1)
- **Published**: 2019-12-21 21:12:56+00:00
- **Updated**: 2019-12-21 21:12:56+00:00
- **Authors**: El Wardani Dadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we exploit the benefits of the deep learning approach to design an efficient system of Amazigh handwritten recognition. Indeed, this approach has proved a greater efficiency in the various domains, especially recognition tasks. However, to take full advantage of this approach it's necessary to construct an adequate dataset of training and testing that represent faithfully the concerned problem. To this end, we have prepared our dataset of 102 writers each one contains 33 characters of IRCAM-Tifinagh. Inspired by the MNIST database, the set of characters is size-normalized and centered in a fixed-size image. The resulting is a grey level image of size 28x28, where the black color is the non-color of the character. The number of images produced after this preprocessing step is 3,366.



