# Arxiv Papers in cs.CV on 2019-12-17
### Dim the Lights! -- Low-Rank Prior Temporal Data for Specular-Free Video Recovery
- **Arxiv ID**: http://arxiv.org/abs/1912.07764v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07764v1)
- **Published**: 2019-12-17 00:19:46+00:00
- **Updated**: 2019-12-17 00:19:46+00:00
- **Authors**: Samar M. Alsaleh, Angelica I. Aviles-Rivero, Noemie Debroux, James K. Hahn
- **Comment**: 22 pages, 6 figures
- **Journal**: None
- **Summary**: The appearance of an object is significantly affected by the illumination conditions in the environment. This is more evident with strong reflective objects as they suffer from more dominant specular reflections, causing information loss and discontinuity in the image domain. In this paper, we present a novel framework for specular-free video recovery with special emphasis on dealing with complex motions coming from objects or camera. Our solution is a twostep approach that allows for both detection and restoration of the damaged regions on video data. We first propose a spatially adaptive detection term that searches for the damage areas. We then introduce a variational solution for specular-free video recovery that allows exploiting spatio-temporal correlations by representing prior data in a low-rank form. We demonstrate that our solution prevents major drawbacks of existing approaches while improving the performance in both detection accuracy and inpainting quality. Finally, we show that our approach can be applied to other problems such as object removal.



### MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.07773v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.07773v4)
- **Published**: 2019-12-17 01:05:26+00:00
- **Updated**: 2021-10-06 13:42:05+00:00
- **Authors**: Sonia Baee, Erfan Pakdamanian, Inki Kim, Lu Feng, Vicente Ordonez, Laura Barnes
- **Comment**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2021
- **Journal**: None
- **Summary**: Inspired by human visual attention, we propose a novel inverse reinforcement learning formulation using Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) for predicting the visual attention of drivers in accident-prone situations. MEDIRL predicts fixation locations that lead to maximal rewards by learning a task-sensitive reward function from eye fixation patterns recorded from attentive drivers. Additionally, we introduce EyeCar, a new driver attention dataset in accident-prone situations. We conduct comprehensive experiments to evaluate our proposed model on three common benchmarks: (DR(eye)VE, BDD-A, DADA-2000), and our EyeCar dataset. Results indicate that MEDIRL outperforms existing models for predicting attention and achieves state-of-the-art performance. We present extensive ablation studies to provide more insights into different features of our proposed model.



### CNN-Based Invertible Wavelet Scattering for the Investigation of Diffusion Properties of the In Vivo Human Heart in Diffusion Tensor Imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.07776v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07776v1)
- **Published**: 2019-12-17 01:26:06+00:00
- **Updated**: 2019-12-17 01:26:06+00:00
- **Authors**: Zeyu Deng, Lihui Wang, Zixiang Kuai, Qijian Chen, Xinyu Cheng, Feng Yang, Jie Yang, Yuemin Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: In vivo diffusion tensor imaging (DTI) is a promising technique to investigate noninvasively the fiber structures of the in vivo human heart. However, signal loss due to motions remains a persistent problem in in vivo cardiac DTI. We propose a novel motion-compensation method for investigating in vivo myocardium structures in DTI with free-breathing acquisitions. The method is based on an invertible Wavelet Scattering achieved by means of Convolutional Neural Network (WSCNN). It consists of first extracting translation-invariant wavelet scattering features from DW images acquired at different trigger delays and then mapping the fused scattering features into motion-compensated spatial DW images by performing an inverse wavelet scattering transform achieved using CNN. The results on both simulated and acquired in vivo cardiac DW images showed that the proposed WSCNN method effectively compensates for motion-induced signal loss and produces in vivo cardiac DW images with better quality and more coherent fiber structures with respect to existing methods, which makes it an interesting method for measuring correctly the diffusion properties of the in vivo human heart in DTI under free breathing.



### Convolutional Dictionary Pair Learning Network for Image Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.12138v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12138v3)
- **Published**: 2019-12-17 01:34:28+00:00
- **Updated**: 2020-01-15 12:12:14+00:00
- **Authors**: Zhao Zhang, Yulin Sun, Yang Wang, Zhengjun Zha, Shuicheng Yan, Meng Wang
- **Comment**: Accepted by the 24th European Conference on Artificial Intelligence
  (ECAI 2020)
- **Journal**: None
- **Summary**: Both the Dictionary Learning (DL) and Convolutional Neural Networks (CNN) are powerful image representation learning systems based on different mechanisms and principles, however whether we can seamlessly integrate them to improve the per-formance is noteworthy exploring. To address this issue, we propose a novel generalized end-to-end representation learning architecture, dubbed Convolutional Dictionary Pair Learning Network (CDPL-Net) in this paper, which integrates the learning schemes of the CNN and dictionary pair learning into a unified framework. Generally, the architecture of CDPL-Net includes two convolutional/pooling layers and two dictionary pair learn-ing (DPL) layers in the representation learning module. Besides, it uses two fully-connected layers as the multi-layer perception layer in the nonlinear classification module. In particular, the DPL layer can jointly formulate the discriminative synthesis and analysis representations driven by minimizing the batch based reconstruction error over the flatted feature maps from the convolution/pooling layer. Moreover, DPL layer uses l1-norm on the analysis dictionary so that sparse representation can be delivered, and the embedding process will also be robust to noise. To speed up the training process of DPL layer, the efficient stochastic gradient descent is used. Extensive simulations on real databases show that our CDPL-Net can deliver enhanced performance over other state-of-the-art methods.



### Collaborative representation-based robust face recognition by discriminative low-rank representation
- **Arxiv ID**: http://arxiv.org/abs/1912.07778v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07778v1)
- **Published**: 2019-12-17 01:40:04+00:00
- **Updated**: 2019-12-17 01:40:04+00:00
- **Authors**: Wen Zhao, Xiao-Jun Wu, He-Feng Yin, Zi-Qi Li
- **Comment**: 28 pages, 5 figures
- **Journal**: None
- **Summary**: We consider the problem of robust face recognition in which both the training and test samples might be corrupted because of disguise and occlusion. Performance of conventional subspace learning methods and recently proposed sparse representation based classification (SRC) might be degraded when corrupted training samples are provided. In addition, sparsity based approaches are time-consuming due to the sparsity constraint. To alleviate the aforementioned problems to some extent, in this paper, we propose a discriminative low-rank representation method for collaborative representation-based (DLRR-CR) robust face recognition. DLRR-CR not only obtains a clean dictionary, it further forces the sub-dictionaries for distinct classes to be as independent as possible by introducing a structural incoherence regularization term. Simultaneously, a low-rank projection matrix can be learned to remove the possible corruptions in the testing samples. Collaborative representation based classification (CRC) method is exploited in our proposed method which has closed-form solution. Experimental results obtained on public face databases verify the effectiveness and robustness of our method.



### Quaternion Product Units for Deep Learning on 3D Rotation Groups
- **Arxiv ID**: http://arxiv.org/abs/1912.07791v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07791v2)
- **Published**: 2019-12-17 02:46:54+00:00
- **Updated**: 2020-04-02 12:47:39+00:00
- **Authors**: Xuan Zhang, Shaofei Qin, Yi Xu, Hongteng Xu
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We propose a novel quaternion product unit (QPU) to represent data on 3D rotation groups. The QPU leverages quaternion algebra and the law of 3D rotation group, representing 3D rotation data as quaternions and merging them via a weighted chain of Hamilton products. We prove that the representations derived by the proposed QPU can be disentangled into "rotation-invariant" features and "rotation-equivariant" features, respectively, which supports the rationality and the efficiency of the QPU in theory. We design quaternion neural networks based on our QPUs and make our models compatible with existing deep learning models. Experiments on both synthetic and real-world data show that the proposed QPU is beneficial for the learning tasks requiring rotation robustness.



### Large-scale Multi-modal Person Identification in Real Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1912.12134v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1912.12134v1)
- **Published**: 2019-12-17 03:16:28+00:00
- **Updated**: 2019-12-17 03:16:28+00:00
- **Authors**: Jiajie Ye, Yisheng Guan, Junfa Liu, Xinghong Huang, Hong Zhang
- **Comment**: 6 pages, IEEE International Conference on Robotics and Biomimetics
  2019
- **Journal**: None
- **Summary**: Person identification (P-ID) under real unconstrained noisy environments is a huge challenge. In multiple-feature learning with Deep Convolutional Neural Networks (DCNNs) or Machine Learning method for large-scale person identification in the wild, the key is to design an appropriate strategy for decision layer fusion or feature layer fusion which can enhance discriminative power. It is necessary to extract different types of valid features and establish a reasonable framework to fuse different types of information. In traditional methods, different persons are identified based on single modal features to identify, such as face feature, audio feature, and head feature. These traditional methods cannot realize a highly accurate level of person identification in real unconstrained environments. The study aims to propose a fusion module to fuse multi-modal features for person identification in real unconstrained environments.



### Joint Architecture and Knowledge Distillation in CNN for Chinese Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.07806v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07806v3)
- **Published**: 2019-12-17 03:48:37+00:00
- **Updated**: 2020-10-26 14:14:47+00:00
- **Authors**: Zi-Rui Wang, Jun Du
- **Comment**: None
- **Journal**: None
- **Summary**: The technique of distillation helps transform cumbersome neural network into compact network so that the model can be deployed on alternative hardware devices. The main advantages of distillation based approaches include simple training process, supported by most off-the-shelf deep learning softwares and no special requirement of hardwares. In this paper, we propose a guideline to distill the architecture and knowledge of pre-trained standard CNNs simultaneously. We first make a quantitative analysis of the baseline network, including computational cost and storage overhead in different components. And then, according to the analysis results, optional strategies can be adopted to the compression of fully-connected layers. For vanilla convolution layers, the proposed parsimonious convolution (ParConv) block only consisting of depthwise separable convolution and pointwise convolution is used as a direct replacement without other adjustments such as the widths and depths in the network. Finally, the knowledge distillation with multiple losses is adopted to improve performance of the compact CNN. The proposed algorithm is first verified on offline handwritten Chinese text recognition (HCTR) where the CNNs are characterized by tens of thousands of output nodes and trained by hundreds of millions of training samples. Compared with the CNN in the state-of-the-art system, our proposed joint architecture and knowledge distillation can reduce the computational cost by >10x and model size by >8x with negligible accuracy loss. And then, by conducting experiments on one of the most popular data sets: MNIST, we demonstrate the proposed approach can also be successfully applied on mainstream backbone networks.



### LAMP-HQ: A Large-Scale Multi-Pose High-Quality Database and Benchmark for NIR-VIS Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.07809v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07809v2)
- **Published**: 2019-12-17 04:01:18+00:00
- **Updated**: 2020-03-29 05:05:36+00:00
- **Authors**: Aijing Yu, Haoxue Wu, Huaibo Huang, Zhen Lei, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: Near-infrared-visible (NIR-VIS) heterogeneous face recognition matches NIR to corresponding VIS face images. However, due to the sensing gap, NIR images often lose some identity information so that the recognition issue is more difficult than conventional VIS face recognition. Recently, NIR-VIS heterogeneous face recognition has attracted considerable attention in the computer vision community because of its convenience and adaptability in practical applications. Various deep learning-based methods have been proposed and substantially increased the recognition performance, but the lack of NIR-VIS training samples leads to the difficulty of the model training process. In this paper, we propose a new Large-Scale Multi-Pose High-Quality NIR-VIS database LAMP-HQ containing 56,788 NIR and 16,828 VIS images of 573 subjects with large diversities in pose, illumination, attribute, scene and accessory. We furnish a benchmark along with the protocol for NIR-VIS face recognition via generation on LAMP-HQ, including Pixel2Pixel, CycleGAN, and ADFL. Furthermore, we propose a novel exemplar-based variational spectral attention network to produce high-fidelity VIS images from NIR data. A spectral conditional attention module is introduced to reduce the domain gap between NIR and VIS data and then improve the performance of NIR-VIS heterogeneous face recognition on various databases including the LAMP-HQ.



### Capsule Attention for Multimodal EEG-EOG Representation Learning with Application to Driver Vigilance Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.07812v4
- **DOI**: 10.1109/TNSRE.2021.3089594
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07812v4)
- **Published**: 2019-12-17 04:20:08+00:00
- **Updated**: 2021-06-13 16:14:03+00:00
- **Authors**: Guangyi Zhang, Ali Etemad
- **Comment**: Accepted by IEEE Transactions on Neural Systems and Rehabilitation
  Engineering
- **Journal**: None
- **Summary**: Driver vigilance estimation is an important task for transportation safety. Wearable and portable brain-computer interface devices provide a powerful means for real-time monitoring of the vigilance level of drivers to help with avoiding distracted or impaired driving. In this paper, we propose a novel multimodal architecture for in-vehicle vigilance estimation from Electroencephalogram and Electrooculogram. To enable the system to focus on the most salient parts of the learned multimodal representations, we propose an architecture composed of a capsule attention mechanism following a deep Long Short-Term Memory (LSTM) network. Our model learns hierarchical dependencies in the data through the LSTM and capsule feature representation layers. To better explore the discriminative ability of the learned representations, we study the effect of the proposed capsule attention mechanism including the number of dynamic routing iterations as well as other parameters. Experiments show the robustness of our method by outperforming other solutions and baseline techniques, setting a new state-of-the-art. We then provide an analysis on different frequency bands and brain regions to evaluate their suitability for driver vigilance estimation. Lastly, an analysis on the role of capsule attention, multimodality, and robustness to noise is performed, highlighting the advantages of our approach.



### Angular Learning: Toward Discriminative Embedded Features
- **Arxiv ID**: http://arxiv.org/abs/1912.07819v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07819v1)
- **Published**: 2019-12-17 05:08:20+00:00
- **Updated**: 2019-12-17 05:08:20+00:00
- **Authors**: JT Wu, L. Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The margin-based softmax loss functions greatly enhance intra-class compactness and perform well on the tasks of face recognition and object classification. Outperformance, however, depends on the careful hyperparameter selection. Moreover, the hard angle restriction also increases the risk of overfitting. In this paper, angular loss suggested by maximizing the angular gradient to promote intra-class compactness avoids overfitting. Besides, our method has only one adjustable constant for intra-class compactness control. We define three metrics to measure inter-class separability and intra-class compactness. In experiments, we test our method, as well as other methods, on many well-known datasets. Experimental results reveal that our method has the superiority of accuracy improvement, discriminative information, and time-consumption.



### Unpaired Image Enhancement Featuring Reinforcement-Learning-Controlled Image Editing Software
- **Arxiv ID**: http://arxiv.org/abs/1912.07833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07833v1)
- **Published**: 2019-12-17 06:03:05+00:00
- **Updated**: 2019-12-17 06:03:05+00:00
- **Authors**: Satoshi Kosugi, Toshihiko Yamasaki
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: This paper tackles unpaired image enhancement, a task of learning a mapping function which transforms input images into enhanced images in the absence of input-output image pairs. Our method is based on generative adversarial networks (GANs), but instead of simply generating images with a neural network, we enhance images utilizing image editing software such as Adobe Photoshop for the following three benefits: enhanced images have no artifacts, the same enhancement can be applied to larger images, and the enhancement is interpretable. To incorporate image editing software into a GAN, we propose a reinforcement learning framework where the generator works as the agent that selects the software's parameters and is rewarded when it fools the discriminator. Our framework can use high-quality non-differentiable filters present in image editing software, which enables image enhancement with high performance. We apply the proposed method to two unpaired image enhancement tasks: photo enhancement and face beautification. Our experimental results demonstrate that the proposed method achieves better performance, compared to the performances of the state-of-the-art methods based on unpaired learning.



### Spatial-Angular Interaction for Light Field Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1912.07849v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07849v3)
- **Published**: 2019-12-17 07:17:52+00:00
- **Updated**: 2020-06-04 09:33:56+00:00
- **Authors**: Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
- **Comment**: In this version, we have revised the paper and compared our
  LF-InterNet to the most recent LF-ATO method (CVPR2020). Codes and
  pre-trained models are available at
  https://github.com/YingqianWang/LF-InterNet
- **Journal**: None
- **Summary**: Light field (LF) cameras record both intensity and directions of light rays, and capture scenes from a number of viewpoints. Both information within each perspective (i.e., spatial information) and among different perspectives (i.e., angular information) is beneficial to image super-resolution (SR). In this paper, we propose a spatial-angular interactive network (namely, LF-InterNet) for LF image SR. Specifically, spatial and angular features are first separately extracted from input LFs, and then repetitively interacted to progressively incorporate spatial and angular information. Finally, the interacted features are fused to superresolve each sub-aperture image. Experimental results demonstrate the superiority of LF-InterNet over the state-of-the-art methods, i.e., our method can achieve high PSNR and SSIM scores with low computational cost, and recover faithful details in the reconstructed images.



### Enhanced Spatially Interleaved Techniques for Multi-View Distributed Video Coding
- **Arxiv ID**: http://arxiv.org/abs/1912.07854v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07854v1)
- **Published**: 2019-12-17 07:35:26+00:00
- **Updated**: 2019-12-17 07:35:26+00:00
- **Authors**: Nantheera Anantrasirichai, Dimitris Agrafiotis
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: This paper presents a multi-view distributed video coding framework for independent camera encoding and centralized decoding. Spatio-temporal-view concealment methods are developed that exploit the interleaved nature of the employed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side information (SI). We study a number of view concealment methods and develop a joint approach that exploits all available correlation for forming the side information. We apply a diversity technique for fusing multiple such predictions thereby achieving more reliable results. We additionally introduce systems enhancements for further improving the rate distortion performance through selective feedback, inter-view bitplane projection and frame subtraction. Results show a significant improvement in performance relative to H.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB increase in PSNR.



### In Defense of the Triplet Loss Again: Learning Robust Person Re-Identification with Fast Approximated Triplet Loss and Label Distillation
- **Arxiv ID**: http://arxiv.org/abs/1912.07863v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07863v2)
- **Published**: 2019-12-17 08:16:45+00:00
- **Updated**: 2019-12-19 18:23:44+00:00
- **Authors**: Ye Yuan, Wuyang Chen, Yang Yang, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The comparative losses (typically, triplet loss) are appealing choices for learning person re-identification (ReID) features. However, the triplet loss is computationally much more expensive than the (practically more popular) classification loss, limiting their wider usage in massive datasets. Moreover, the abundance of label noise and outliers in ReID datasets may also put the margin-based loss in jeopardy. This work addresses the above two shortcomings of triplet loss, extending its effectiveness to large-scale ReID datasets with potentially noisy labels. We propose a fast-approximated triplet (FAT) loss, which provably converts the point-wise triplet loss into its upper bound form, consisting of a point-to-set loss term plus cluster compactness regularization. It preserves the effectiveness of triplet loss, while leading to linear complexity to the training set size. A label distillation strategy is further designed to learn refined soft-labels in place of the potentially noisy labels, from only an identified subset of confident examples, through teacher-student networks. We conduct extensive experiments on three most popular ReID benchmarks (Market-1501, DukeMTMC-reID, and MSMT17), and demonstrate that FAT loss with distilled labels lead to ReID features with remarkable accuracy, efficiency, robustness, and direct transferability to unseen datasets.



### $\ell_0$ Regularized Structured Sparsity Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.07868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07868v1)
- **Published**: 2019-12-17 08:34:22+00:00
- **Updated**: 2019-12-17 08:34:22+00:00
- **Authors**: Kevin Bui, Fredrick Park, Shuai Zhang, Yingyong Qi, Jack Xin
- **Comment**: None
- **Journal**: None
- **Summary**: Deepening and widening convolutional neural networks (CNNs) significantly increases the number of trainable weight parameters by adding more convolutional layers and feature maps per layer, respectively. By imposing inter- and intra-group sparsity onto the weights of the layers during the training process, a compressed network can be obtained with accuracy comparable to a dense one. In this paper, we propose a new variant of sparse group lasso that blends the $\ell_0$ norm onto the individual weight parameters and the $\ell_{2,1}$ norm onto the output channels of a layer. To address the non-differentiability of the $\ell_0$ norm, we apply variable splitting resulting in an algorithm that consists of executing stochastic gradient descent followed by hard thresholding for each iteration. Numerical experiments are demonstrated on LeNet-5 and wide-residual-networks for MNIST and CIFAR 10/100, respectively. They showcase the effectiveness of our proposed method in attaining superior test accuracy with network sparsification on par with the current state of the art.



### Constructing the F-Graph with a Symmetric Constraint for Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1912.07871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07871v1)
- **Published**: 2019-12-17 08:40:01+00:00
- **Updated**: 2019-12-17 08:40:01+00:00
- **Authors**: Kai Xu, Xiao-Jun Wu, Wen-Bo Hu
- **Comment**: 9 pages, 1 figure
- **Journal**: None
- **Summary**: Based on further studying the low-rank subspace clustering (LRSC) and L2-graph subspace clustering algorithms, we propose a F-graph subspace clustering algorithm with a symmetric constraint (FSSC), which constructs a new objective function with a symmetric constraint basing on F-norm, whose the most significant advantage is to obtain a closed-form solution of the coefficient matrix. Then, take the absolute value of each element of the coefficient matrix, and retain the k largest coefficients per column, set the other elements to 0, to get a new coefficient matrix. Finally, FSSC performs spectral clustering over the new coefficient matrix. The experimental results on face clustering and motion segmentation show FSSC algorithm can not only obviously reduce the running time, but also achieve higher accuracy compared with the state-of-the-art representation-based subspace clustering algorithms, which verifies that the FSSC algorithm is efficacious and feasible.



### Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.07872v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07872v2)
- **Published**: 2019-12-17 08:41:01+00:00
- **Updated**: 2020-03-27 08:41:07+00:00
- **Authors**: Renchun You, Zhiyao Guo, Lei Cui, Xiang Long, Yingze Bao, Shilei Wen
- **Comment**: Accepted by AAAI2020
- **Journal**: None
- **Summary**: Multi-label image and video classification are fundamental yet challenging tasks in computer vision. The main challenges lie in capturing spatial or temporal dependencies between labels and discovering the locations of discriminative features for each class. In order to overcome these challenges, we propose to use cross-modality attention with semantic graph embedding for multi label classification. Based on the constructed label graph, we propose an adjacency-based similarity graph embedding method to learn semantic label embeddings, which explicitly exploit label relationships. Then our novel cross-modality attention maps are generated with the guidance of learned label embeddings. Experiments on two multi-label image classification datasets (MS-COCO and NUS-WIDE) show our method outperforms other existing state-of-the-arts. In addition, we validate our method on a large multi-label video classification dataset (YouTube-8M Segments) and the evaluation results demonstrate the generalization capability of our method.



### Deep SCNN-based Real-time Object Detection for Self-driving Vehicles Using LiDAR Temporal Data
- **Arxiv ID**: http://arxiv.org/abs/1912.07906v4
- **DOI**: 10.1109/ACCESS.2020.2990416
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07906v4)
- **Published**: 2019-12-17 09:56:15+00:00
- **Updated**: 2020-04-28 23:24:12+00:00
- **Authors**: Shibo Zhou, Ying Chen, Xiaohua Li, Arindam Sanyal
- **Comment**: None
- **Journal**: IEEE Access, 2020
- **Summary**: Real-time accurate detection of three-dimensional (3D) objects is a fundamental necessity for self-driving vehicles. Most existing computer vision approaches are based on convolutional neural networks (CNNs). Although the CNN-based approaches can achieve high detection accuracy, their high energy consumption is a severe drawback. To resolve this problem, novel energy efficient approaches should be explored. Spiking neural network (SNN) is a promising candidate because it has orders-of-magnitude lower energy consumption than CNN. Unfortunately, the studying of SNN has been limited in small networks only. The application of SNN for large 3D object detection networks has remain largely open. In this paper, we integrate spiking convolutional neural network (SCNN) with temporal coding into the YOLOv2 architecture for real-time object detection. To take the advantage of spiking signals, we develop a novel data preprocessing layer that translates 3D point-cloud data into spike time data. We propose an analog circuit to implement the non-leaky integrate and fire neuron used in our SCNN, from which the energy consumption of each spike is estimated. Moreover, we present a method to calculate the network sparsity and the energy consumption of the overall network. Extensive experiments have been conducted based on the KITTI dataset, which show that the proposed network can reach competitive detection accuracy as existing approaches, yet with much lower average energy consumption. If implemented in dedicated hardware, our network could have a mean sparsity of 56.24% and extremely low total energy consumption of 0.247mJ only. Implemented in NVIDIA GTX 1080i GPU, we can achieve 35.7 fps frame rate, high enough for real-time object detection.



### Pioneer dataset and automatic recognition of Urdu handwritten characters using a deep autoencoder and convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1912.07943v1
- **DOI**: 10.1007/s42452-019-1914-1
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.07943v1)
- **Published**: 2019-12-17 11:49:13+00:00
- **Updated**: 2019-12-17 11:49:13+00:00
- **Authors**: Hazrat Ali, Ahsan Ullah, Talha Iqbal, Shahid Khattak
- **Comment**: SN Applied Sciences, December 2019
- **Journal**: None
- **Summary**: Automatic recognition of Urdu handwritten digits and characters, is a challenging task. It has applications in postal address reading, bank's cheque processing, and digitization and preservation of handwritten manuscripts from old ages. While there exists a significant work for automatic recognition of handwritten English characters and other major languages of the world, the work done for Urdu lan-guage is extremely insufficient. This paper has two goals. Firstly, we introduce a pioneer dataset for handwritten digits and characters of Urdu, containing samples from more than 900 individuals. Secondly, we report results for automatic recog-nition of handwritten digits and characters as achieved by using deep auto-encoder network and convolutional neural network. More specifically, we use a two-layer and a three-layer deep autoencoder network and convolutional neural network and evaluate the two frameworks in terms of recognition accuracy. The proposed framework of deep autoencoder can successfully recognize digits and characters with an accuracy of 97% for digits only, 81% for characters only and 82% for both digits and characters simultaneously. In comparison, the framework of convolutional neural network has accuracy of 96.7% for digits only, 86.5% for characters only and 82.7% for both digits and characters simultaneously. These frameworks can serve as baselines for future research on Urdu handwritten text.



### Multi-focus Image Fusion Based on Similarity Characteristics
- **Arxiv ID**: http://arxiv.org/abs/1912.07959v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07959v2)
- **Published**: 2019-12-17 12:16:04+00:00
- **Updated**: 2022-01-29 05:50:19+00:00
- **Authors**: Ya-Qiong Zhang, Xiao-Jun Wu, Hui Li
- **Comment**: 7 pages, 10 figures, 4 tables
- **Journal**: None
- **Summary**: A novel multi-focus image fusion algorithm performed in spatial domain based on similarity characteristics is proposed incorporating with region segmentation. In this paper, a new similarity measure is developed based on the structural similarity (SSIM) index, which is more suitable for multi-focus image segmentation. Firstly, the SSNSIM map is calculated between two input images. Then we segment the SSNSIM map using watershed method, and merge the small homogeneous regions with fuzzy c-means clustering algorithm (FCM). For three source images, a joint region segmentation method based on segmentation of two images is used to obtain the final segmentation result. Finally, the corresponding segmented regions of the source images are fused according to their average gradient. The performance of the image fusion method is evaluated by several criteria including spatial frequency, average gradient, entropy, edge retention etc. The evaluation results indicate that the proposed method is effective and has good visual perception.



### Nanoscale Microscopy Images Colorization Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.07964v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07964v2)
- **Published**: 2019-12-17 12:21:42+00:00
- **Updated**: 2020-02-22 15:03:34+00:00
- **Authors**: Israel Goytom, Qin Wang, Tianxiang Yu, Kunjie Dai, Kris Sankaran, Xinfei Zhou, Dongdong Lin
- **Comment**: 27 pages, 6 figures
- **Journal**: None
- **Summary**: Microscopy images are powerful tools and widely used in the majority of research areas, such as biology, chemistry, physics and materials fields by various microscopies (scanning electron microscope (SEM), atomic force microscope (AFM) and the optical microscope, et al.). However, most of the microscopy images are colorless due to the unique imaging mechanism. Though investigating on some popular solutions proposed recently about colorizing images, we notice the process of those methods are usually tedious, complicated, and time-consuming. In this paper, inspired by the achievement of machine learning algorithms on different science fields, we introduce two artificial neural networks for gray microscopy image colorization: An end-to-end convolutional neural network (CNN) with a pre-trained model for feature extraction and a pixel-to-pixel neural style transfer convolutional neural network (NST-CNN), which can colorize gray microscopy images with semantic information learned from a user-provided colorful image at inference time. The results demonstrate that our algorithm not only can colorize the microscopy images under complex circumstances precisely but also make the color naturally according to the training of a massive number of nature images with proper hue and saturation.



### KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild
- **Arxiv ID**: http://arxiv.org/abs/1912.07966v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.07966v2)
- **Published**: 2019-12-17 12:26:32+00:00
- **Updated**: 2021-03-01 14:25:17+00:00
- **Authors**: Franz Götz-Hahn, Vlad Hosu, Hanhe Lin, Dietmar Saupe
- **Comment**: None
- **Journal**: None
- **Summary**: Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k, LIVE-VQC, and LIVE-Qualcomm with a 0.83, 0.75, and 0.64 SRCC, respectively. For both KoNViD-1k and LIVE-VQC this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization.



### Conditional Generative ConvNets for Exemplar-based Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1912.07971v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.07971v1)
- **Published**: 2019-12-17 12:40:47+00:00
- **Updated**: 2019-12-17 12:40:47+00:00
- **Authors**: Zi-Ming Wang, Meng-Han Li, Gui-Song Xia
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of exemplar-based texture synthesis is to generate texture images that are visually similar to a given exemplar. Recently, promising results have been reported by methods relying on convolutional neural networks (ConvNets) pretrained on large-scale image datasets. However, these methods have difficulties in synthesizing image textures with non-local structures and extending to dynamic or sound textures. In this paper, we present a conditional generative ConvNet (cgCNN) model which combines deep statistics and the probabilistic framework of generative ConvNet (gCNN) model. Given a texture exemplar, the cgCNN model defines a conditional distribution using deep statistics of a ConvNet, and synthesize new textures by sampling from the conditional distribution. In contrast to previous deep texture models, the proposed cgCNN dose not rely on pre-trained ConvNets but learns the weights of ConvNets for each input exemplar instead. As a result, the cgCNN model can synthesize high quality dynamic, sound and image textures in a unified manner. We also explore the theoretical connections between our model and other texture models. Further investigations show that the cgCNN model can be easily generalized to texture expansion and inpainting. Extensive experiments demonstrate that our model can achieve better or at least comparable results than the state-of-the-art methods.



### Jointly Trained Image and Video Generation using Residual Vectors
- **Arxiv ID**: http://arxiv.org/abs/1912.07991v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.07991v1)
- **Published**: 2019-12-17 13:14:17+00:00
- **Updated**: 2019-12-17 13:14:17+00:00
- **Authors**: Yatin Dandi, Aniket Das, Soumye Singhal, Vinay P. Namboodiri, Piyush Rai
- **Comment**: Accepted in 2020 Winter Conference on Applications of Computer Vision
  (WACV '20)
- **Journal**: None
- **Summary**: In this work, we propose a modeling technique for jointly training image and video generation models by simultaneously learning to map latent variables with a fixed prior onto real images and interpolate over images to generate videos. The proposed approach models the variations in representations using residual vectors encoding the change at each time step over a summary vector for the entire video. We utilize the technique to jointly train an image generation model with a fixed prior along with a video generation model lacking constraints such as disentanglement. The joint training enables the image generator to exploit temporal information while the video generation model learns to flexibly share information across frames. Moreover, experimental results verify our approach's compatibility with pre-training on videos or images and training on datasets containing a mixture of both. A comprehensive set of quantitative and qualitative evaluations reveal the improvements in sample quality and diversity over both video generation and image generation baselines. We further demonstrate the technique's capabilities of exploiting similarity in features across frames by applying it to a model based on decomposing the video into motion and content. The proposed model allows minor variations in content across frames while maintaining the temporal dependence through latent vectors encoding the pose or motion features.



### Adaptive Densely Connected Super-Resolution Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.08002v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08002v2)
- **Published**: 2019-12-17 13:38:51+00:00
- **Updated**: 2019-12-18 09:09:13+00:00
- **Authors**: Tangxin Xie, Xin Yang, Yu Jia, Chen Zhu, Xiaochuan Li
- **Comment**: None
- **Journal**: None
- **Summary**: For a better performance in single image super-resolution(SISR), we present an image super-resolution algorithm based on adaptive dense connection (ADCSR). The algorithm is divided into two parts: BODY and SKIP. BODY improves the utilization of convolution features through adaptive dense connections. Also, we develop an adaptive sub-pixel reconstruction layer (AFSL) to reconstruct the features of the BODY output. We pre-trained SKIP to make BODY focus on high-frequency feature learning. The comparison of PSNR, SSIM, and visual effects verify the superiority of our method to the state-of-the-art algorithms.



### Towards Generalization Across Depth for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.08035v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08035v3)
- **Published**: 2019-12-17 14:34:27+00:00
- **Updated**: 2020-04-02 12:34:40+00:00
- **Authors**: Andrea Simonelli, Samuel Rota Bulò, Lorenzo Porzi, Elisa Ricci, Peter Kontschieder
- **Comment**: None
- **Journal**: None
- **Summary**: While expensive LiDAR and stereo camera rigs have enabled the development of successful 3D object detection methods, monocular RGB-only approaches lag much behind. This work advances the state of the art by introducing MoVi-3D, a novel, single-stage deep architecture for monocular 3D object detection. MoVi-3D builds upon a novel approach which leverages geometrical information to generate, both at training and test time, virtual views where the object appearance is normalized with respect to distance. These virtually generated views facilitate the detection task as they significantly reduce the visual appearance variability associated to objects placed at different distances from the camera. As a consequence, the deep model is relieved from learning depth-specific representations and its complexity can be significantly reduced. In particular, in this work we show that, thanks to our virtual views generation process, a lightweight, single-stage architecture suffices to set new state-of-the-art results on the popular KITTI3D benchmark.



### Feature Fusion Use Unsupervised Prior Knowledge to Let Small Object Represent
- **Arxiv ID**: http://arxiv.org/abs/1912.08059v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08059v1)
- **Published**: 2019-12-17 15:00:56+00:00
- **Updated**: 2019-12-17 15:00:56+00:00
- **Authors**: Tian Liu, Lichun Wang, Shaofan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Fusing low level and high level features is a widely used strategy to provide details that might be missing during convolution and pooling. Different from previous works, we propose a new fusion mechanism called FillIn which takes advantage of prior knowledge described with superpixel segmentation. According to the prior knowledge, the FillIn chooses small region on low level feature map to fill into high level feature map. By using the proposed fusion mechanism, the low level features have equal channels for some tiny region as high level features, which makes the low level features have relatively independent power to decide final semantic label. We demonstrate the effectiveness of our model on PASCAL VOC 2012, it achieves competitive test result based on DeepLabv3+ backbone and visualizations of predictions prove our fusion can let small objects represent and low level features have potential for segmenting small objects.



### Improved Surrogates in Inertial Confinement Fusion with Manifold and Cycle Consistencies
- **Arxiv ID**: http://arxiv.org/abs/1912.08113v1
- **DOI**: 10.1073/pnas.1916634117
- **Categories**: **cs.LG**, cs.CV, physics.comp-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08113v1)
- **Published**: 2019-12-17 16:14:43+00:00
- **Updated**: 2019-12-17 16:14:43+00:00
- **Authors**: Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Brian K. Spears
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Neural networks have become very popular in surrogate modeling because of their ability to characterize arbitrary, high dimensional functions in a data driven fashion. This paper advocates for the training of surrogates that are consistent with the physical manifold -- i.e., predictions are always physically meaningful, and are cyclically consistent -- i.e., when the predictions of the surrogate, when passed through an independently trained inverse model give back the original input parameters. We find that these two consistencies lead to surrogates that are superior in terms of predictive performance, more resilient to sampling artifacts, and tend to be more data efficient. Using Inertial Confinement Fusion (ICF) as a test bed problem, we model a 1D semi-analytic numerical simulator and demonstrate the effectiveness of our approach. Code and data are available at https://github.com/rushilanirudh/macc/



### Direction Concentration Learning: Enhancing Congruency in Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.08136v2
- **DOI**: 10.1109/TPAMI.2019.2963387
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08136v2)
- **Published**: 2019-12-17 16:58:04+00:00
- **Updated**: 2020-01-02 01:28:05+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan S. Kankanhalli, Qi Zhao
- **Comment**: This is a preprint and the formal version has been published in TPAMI
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2019
- **Summary**: One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency.



### On-the-fly Global Embeddings Using Random Projections for Extreme Multi-label Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.08140v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08140v2)
- **Published**: 2019-12-17 17:11:17+00:00
- **Updated**: 2021-10-15 06:54:43+00:00
- **Authors**: Yashaswi Verma
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of eXtreme Multi-label Learning (XML) is to automatically annotate a given data point with the most relevant subset of labels from an extremely large vocabulary of labels (e.g., a million labels). Lately, many attempts have been made to address this problem that achieve reasonable performance on benchmark datasets. In this paper, rather than coming-up with an altogether new method, our objective is to present and validate a simple baseline for this task. Precisely, we investigate an on-the-fly global and structure preserving feature embedding technique using random projections whose learning phase is independent of training samples and label vocabulary. Further, we show how an ensemble of multiple such learners can be used to achieve further boost in prediction accuracy with only linear increase in training and prediction time. Experiments on three public XML benchmarks show that the proposed approach obtains competitive accuracy compared with many existing methods. Additionally, it also provides around 6572x speed-up ratio in terms of training time and around 14.7x reduction in model-size compared to the closest competitors on the largest publicly available dataset.



### Causality matters in medical imaging
- **Arxiv ID**: http://arxiv.org/abs/1912.08142v1
- **DOI**: 10.1038/s41467-020-17478-w
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08142v1)
- **Published**: 2019-12-17 17:15:02+00:00
- **Updated**: 2019-12-17 17:15:02+00:00
- **Authors**: Daniel C. Castro, Ian Walker, Ben Glocker
- **Comment**: 20 pages, 5 figures, 4 tables
- **Journal**: Nature Communications 11 (2020) 3673
- **Summary**: This article discusses how the language of causality can shed new light on the major challenges in machine learning for medical imaging: 1) data scarcity, which is the limited availability of high-quality annotations, and 2) data mismatch, whereby a trained algorithm may fail to generalize in clinical practice. Looking at these challenges through the lens of causality allows decisions about data collection, annotation procedures, and learning strategies to be made (and scrutinized) more transparently. We discuss how causal relationships between images and annotations can not only have profound effects on the performance of predictive models, but may even dictate which learning strategies should be considered in the first place. For example, we conclude that semi-supervision may be unsuitable for image segmentation---one of the possibly surprising insights from our causal analysis, which is illustrated with representative real-world examples of computer-aided diagnosis (skin lesion classification in dermatology) and radiotherapy (automated contouring of tumours). We highlight that being aware of and accounting for the causal relationships in medical imaging data is important for the safe development of machine learning and essential for regulation and responsible reporting. To facilitate this we provide step-by-step recommendations for future studies.



### APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.08166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08166v2)
- **Published**: 2019-12-17 18:08:01+00:00
- **Updated**: 2020-08-20 21:37:23+00:00
- **Authors**: Anneliese Braunegg, Amartya Chakraborty, Michael Krumdick, Nicole Lape, Sara Leary, Keith Manville, Elizabeth Merkhofer, Laura Strickhart, Matthew Walmer
- **Comment**: 23 pages, 14 figures, 3 tables. Updated version as accepted to ECCV
  2020
- **Journal**: None
- **Summary**: Physical adversarial attacks threaten to fool object detection systems, but reproducible research on the real-world effectiveness of physical patches and how to defend against them requires a publicly available benchmark dataset. We present APRICOT, a collection of over 1,000 annotated photographs of printed adversarial patches in public locations. The patches target several object categories for three COCO-trained detection models, and the photos represent natural variation in position, distance, lighting conditions, and viewing angle. Our analysis suggests that maintaining adversarial robustness in uncontrolled settings is highly challenging, but it is still possible to produce targeted detections under white-box and sometimes black-box settings. We establish baselines for defending against adversarial patches through several methods, including a detector supervised with synthetic data and unsupervised methods such as kernel density estimation, Bayesian uncertainty, and reconstruction error. Our results suggest that adversarial patches can be effectively flagged, both in a high-knowledge, attack-specific scenario, and in an unsupervised setting where patches are detected as anomalies in natural images. This dataset and the described experiments provide a benchmark for future research on the effectiveness of and defenses against physical adversarial objects in the wild.



### Towards personalized diagnosis of Glioblastoma in Fluid-attenuated inversion recovery (FLAIR) by topological interpretable machine learning
- **Arxiv ID**: http://arxiv.org/abs/1912.08167v6
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.08167v6)
- **Published**: 2019-12-17 18:09:50+00:00
- **Updated**: 2020-04-18 18:51:57+00:00
- **Authors**: Matteo Rucco, Lorenzo Falsetti, Giovanna Viticchi
- **Comment**: 22 pages; 16 figures
- **Journal**: None
- **Summary**: Glioblastoma multiforme (GBM) is a fast-growing and highly invasive brain tumour, it tends to occur in adults between the ages of 45 and 70 and it accounts for 52 percent of all primary brain tumours. Usually, GBMs are detected by magnetic resonance images (MRI). Among MRI, Fluid-attenuated inversion recovery (FLAIR) sequence produces high quality digital tumour representation. Fast detection and segmentation techniques are needed for overcoming subjective medical doctors (MDs) judgment. In the present investigation, we intend to demonstrate by means of numerical experiments that topological features combined with textural features can be enrolled for GBM analysis and morphological characterization on FLAIR. To this extent, we have performed three numerical experiments. In the first experiment, Topological Data Analysis (TDA) of a simplified 2D tumour growth mathematical model had allowed to understand the bio-chemical conditions that facilitate tumour growth: the higher the concentration of chemical nutrients the more virulent the process. In the second experiment topological data analysis was used for evaluating GBM temporal progression on FLAIR recorded within 90 days following treatment (e.g., chemo-radiation therapy - CRT) completion and at progression. The experiment had confirmed that persistent entropy is a viable statistics for monitoring GBM evolution during the follow-up period. In the third experiment we had developed a novel methodology based on topological and textural features and automatic interpretable machine learning for automatic GBM classification on FLAIR. The algorithm reached a classification accuracy up to the 97%.



### DeepHashing using TripletLoss
- **Arxiv ID**: http://arxiv.org/abs/1912.10822v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10822v1)
- **Published**: 2019-12-17 18:17:48+00:00
- **Updated**: 2019-12-17 18:17:48+00:00
- **Authors**: Jithin James
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing is one of the most efficient techniques for approximate nearest neighbour search for large scale image retrieval. Most of the techniques are based on hand-engineered features and do not give optimal results all the time. Deep Convolutional Neural Networks have proven to generate very effective representation of images that are used for various computer vision tasks and inspired by this there have been several Deep Hashing models like Wang et al. (2016) have been proposed. These models train on the triplet loss function which can be used to train models with superior representation capabilities. Taking the latest advancements in training using the triplet loss I propose new techniques that help the Deep Hash-ing models train more faster and efficiently. Experiment result1show that using the more efficient techniques for training on the triplet loss, we have obtained a 5%percent improvement in our model compared to the original work of Wang et al.(2016). Using a larger model and more training data we can drastically improve the performance using the techniques we propose



### AeroRIT: A New Scene for Hyperspectral Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.08178v3
- **DOI**: 10.1109/TGRS.2020.2987199
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.08178v3)
- **Published**: 2019-12-17 18:31:56+00:00
- **Updated**: 2020-04-07 23:18:04+00:00
- **Authors**: Aneesh Rangnekar, Nilay Mokashi, Emmett Ientilucci, Christopher Kanan, Matthew J. Hoffman
- **Comment**: To appear in IEEE TGRS
- **Journal**: None
- **Summary**: We investigate applying convolutional neural network (CNN) architecture to facilitate aerial hyperspectral scene understanding and present a new hyperspectral dataset-AeroRIT-that is large enough for CNN training. To date the majority of hyperspectral airborne have been confined to various sub-categories of vegetation and roads and this scene introduces two new categories: buildings and cars. To the best of our knowledge, this is the first comprehensive large-scale hyperspectral scene with nearly seven million pixel annotations for identifying cars, roads, and buildings. We compare the performance of three popular architectures - SegNet, U-Net, and Res-U-Net, for scene understanding and object identification via the task of dense semantic segmentation to establish a benchmark for the scene. To further strengthen the network, we add squeeze and excitation blocks for better channel interactions and use self-supervised learning for better encoder initialization. Aerial hyperspectral image analysis has been restricted to small datasets with limited train/test splits capabilities and we believe that AeroRIT will help advance the research in the field with a more complex object distribution to perform well on. The full dataset, with flight lines in radiance and reflectance domain, is available for download at https://github.com/aneesh3108/AeroRIT. This dataset is the first step towards developing robust algorithms for hyperspectral airborne sensing that can robustly perform advanced tasks like vehicle tracking and occlusion handling.



### PointRend: Image Segmentation as Rendering
- **Arxiv ID**: http://arxiv.org/abs/1912.08193v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08193v2)
- **Published**: 2019-12-17 18:57:02+00:00
- **Updated**: 2020-02-16 23:18:43+00:00
- **Authors**: Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.



### Learning Generalizable Visual Representations via Interactive Gameplay
- **Arxiv ID**: http://arxiv.org/abs/1912.08195v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08195v3)
- **Published**: 2019-12-17 18:57:50+00:00
- **Updated**: 2021-02-25 17:51:31+00:00
- **Authors**: Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi
- **Comment**: Replaced with version accepted to ICLR'21
- **Journal**: None
- **Summary**: A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed from the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.



### Meshed-Memory Transformer for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1912.08226v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1912.08226v2)
- **Published**: 2019-12-17 19:03:23+00:00
- **Updated**: 2020-03-20 19:29:14+00:00
- **Authors**: Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M$^2$ - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M$^2$ Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the "Karpathy" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.



### Fingerprint Spoof Detection: Temporal Analysis of Image Sequence
- **Arxiv ID**: http://arxiv.org/abs/1912.08240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08240v1)
- **Published**: 2019-12-17 19:23:21+00:00
- **Updated**: 2019-12-17 19:23:21+00:00
- **Authors**: Tarang Chugh, Anil K. Jain
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We utilize the dynamics involved in the imaging of a fingerprint on a touch-based fingerprint reader, such as perspiration, changes in skin color (blanching), and skin distortion, to differentiate real fingers from spoof (fake) fingers. Specifically, we utilize a deep learning-based architecture (CNN-LSTM) trained end-to-end using sequences of minutiae-centered local patches extracted from ten color frames captured on a COTS fingerprint reader. A time-distributed CNN (MobileNet-v1) extracts spatial features from each local patch, while a bi-directional LSTM layer learns the temporal relationship between the patches in the sequence. Experimental results on a database of 26,650 live frames from 685 subjects (1,333 unique fingers), and 32,910 spoof frames of 7 spoof materials (with 14 variants) shows the superiority of the proposed approach in both known-material and cross-material (generalization) scenarios. For instance, the proposed approach improves the state-of-the-art cross-material performance from TDR of 81.65% to 86.20% @ FDR = 0.2%.



### ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.08263v3
- **DOI**: 10.1109/CVPRW50498.2020.00029
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, I.2.9; I.2.10; I.4.1; I.4.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1912.08263v3)
- **Published**: 2019-12-17 20:29:15+00:00
- **Updated**: 2020-06-24 15:20:45+00:00
- **Authors**: Felix Ott, Tobias Feigl, Christoffer Löffler, Christopher Mutschler
- **Comment**: Conf. on Computer Vision and Pattern Recognition (CVPR): Joint
  Workshop on Long-Term Visual Localization, Visual Odometry and Geometric and
  Learning-based SLAM 2020
- **Journal**: None
- **Summary**: Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.



### Learning from Synthetic Animals
- **Arxiv ID**: http://arxiv.org/abs/1912.08265v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.08265v2)
- **Published**: 2019-12-17 20:45:45+00:00
- **Updated**: 2020-04-05 18:59:56+00:00
- **Authors**: Jiteng Mu, Weichao Qiu, Gregory Hager, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Despite great success in human parsing, progress for parsing other deformable articulated objects, like animals, is still limited by the lack of labeled data. In this paper, we use synthetic images and ground truth generated from CAD animal models to address this challenge. To bridge the domain gap between real and synthetic images, we propose a novel consistency-constrained semi-supervised learning method (CC-SSL). Our method leverages both spatial and temporal consistencies, to bootstrap weak models trained on synthetic data with unlabeled real images. We demonstrate the effectiveness of our method on highly deformable animals, such as horses and tigers. Without using any real image label, our method allows for accurate keypoint prediction on real images. Moreover, we quantitatively show that models using synthetic data achieve better generalization performance than models trained on real images across different domains in the Visual Domain Adaptation Challenge dataset. Our synthetic dataset contains 10+ animals with diverse poses and rich ground truth, which enables us to use the multi-task learning strategy to further boost models' performance.



### A Probabilistic approach for Learning Embeddings without Supervision
- **Arxiv ID**: http://arxiv.org/abs/1912.08275v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.08275v1)
- **Published**: 2019-12-17 21:14:08+00:00
- **Updated**: 2019-12-17 21:14:08+00:00
- **Authors**: Ujjal Kr Dutta, Mehrtash Harandi, Chandra Sekhar Chellu
- **Comment**: None
- **Journal**: None
- **Summary**: For challenging machine learning problems such as zero-shot learning and fine-grained categorization, embedding learning is the machinery of choice because of its ability to learn generic notions of similarity, as opposed to class-specific concepts in standard classification models. Embedding learning aims at learning discriminative representations of data such that similar examples are pulled closer, while pushing away dissimilar ones. Despite their exemplary performances, supervised embedding learning approaches require huge number of annotations for training. This restricts their applicability for large datasets in new applications where obtaining labels require extensive manual efforts and domain knowledge. In this paper, we propose to learn an embedding in a completely unsupervised manner without using any class labels. Using a graph-based clustering approach to obtain pseudo-labels, we form triplet-based constraints following a metric learning paradigm. Our novel embedding learning approach uses a probabilistic notion, that intuitively minimizes the chances of each triplet violating a geometric constraint. Due to nature of the search space, we learn the parameters of our approach using Riemannian geometry. Our proposed approach performs competitive to state-of-the-art approaches.



### Progressive VAE Training on Highly Sparse and Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/1912.08283v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1912.08283v1)
- **Published**: 2019-12-17 21:38:51+00:00
- **Updated**: 2019-12-17 21:38:51+00:00
- **Authors**: Dmitry Utyamishev, Inna Partin-Vaisband
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel approach for training a Variational Autoencoder (VAE) on a highly imbalanced data set. The proposed training of a high-resolution VAE model begins with the training of a low-resolution core model, which can be successfully trained on imbalanced data set. In subsequent training steps, new convolutional, upsampling, deconvolutional, and downsampling layers are iteratively attached to the model. In each iteration, the additional layers are trained based on the intermediate pretrained model - a result of previous training iterations. Thus, the resolution of the model is progressively increased up to the required resolution level. In this paper, the progressive VAE training is exploited for learning a latent representation with imbalanced, highly sparse data sets and, consequently, generating routes in a constrained 2D space. Routing problems (e.g., vehicle routing problem, travelling salesman problem, and arc routing) are of special significance in many modern applications (e.g., route planning, network maintenance, developing high-performance nanoelectronic systems, and others) and typically associated with sparse imbalanced data. In this paper, the critical problem of routing billions of components in nanoelectronic devices is considered. The proposed approach exhibits a significant training speedup as compared with state-of-the-art existing VAE training methods, while generating expected image outputs from unseen input data. Furthermore, the final progressive VAE models exhibit much more precise output representation, than the Generative Adversarial Network (GAN) models trained with comparable training time. The proposed method is expected to be applicable to a wide range of applications, including but not limited image impainting, sentence interpolation, and semi-supervised learning.



### Facial Synthesis from Visual Attributes via Sketch using Multi-Scale Generators
- **Arxiv ID**: http://arxiv.org/abs/1912.10479v1
- **DOI**: 10.1109/TBIOM.2019.2961926
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.10479v1)
- **Published**: 2019-12-17 22:53:23+00:00
- **Updated**: 2019-12-17 22:53:23+00:00
- **Authors**: Xing Di, Vishal M. Patel
- **Comment**: This work is accepted in IEEE Transactions on Biometrics, Behavior,
  and Identity Science (T-BIOM). arXiv admin note: substantial text overlap
  with arXiv:1801.00077
- **Journal**: None
- **Summary**: Automatic synthesis of faces from visual attributes is an important problem in computer vision and has wide applications in law enforcement and entertainment. With the advent of deep generative convolutional neural networks (CNNs), attempts have been made to synthesize face images from attributes and text descriptions. In this paper, we take a different approach, where we formulate the original problem as a stage-wise learning problem. We first synthesize the facial sketch corresponding to the visual attributes and then we generate the face image based on the synthesized sketch. The proposed framework, is based on a combination of two different Generative Adversarial Networks (GANs) - (1) a sketch generator network which synthesizes realistic sketch from the input attributes, and (2) a face generator network which synthesizes facial images from the synthesized sketch images with the help of facial attributes. Extensive experiments and comparison with recent methods are performed to verify the effectiveness of the proposed attribute-based two-stage face synthesis method.



