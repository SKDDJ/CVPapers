# Arxiv Papers in cs.CV on 2019-12-24
### Unsupervised Scene Adaptation with Memory Regularization in vivo
- **Arxiv ID**: http://arxiv.org/abs/1912.11164v3
- **DOI**: 10.24963/ijcai.2020/150
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11164v3)
- **Published**: 2019-12-24 01:12:36+00:00
- **Updated**: 2023-02-07 17:05:26+00:00
- **Authors**: Zhedong Zheng, Yi Yang
- **Comment**: 7 pages, 4 figures, 6 tables (accepted by IJCAI 2020)
- **Journal**: None
- **Summary**: We consider the unsupervised scene adaptation problem of learning from both labeled source data and unlabeled target data. Existing methods focus on minoring the inter-domain gap between the source and target domains. However, the intra-domain knowledge and inherent uncertainty learned by the network are under-explored. In this paper, we propose an orthogonal method, called memory regularization in vivo to exploit the intra-domain knowledge and regularize the model training. Specifically, we refer to the segmentation model itself as the memory module, and minor the discrepancy of the two classifiers, i.e., the primary classifier and the auxiliary classifier, to reduce the prediction inconsistency. Without extra parameters, the proposed method is complementary to the most existing domain adaptation methods and could generally improve the performance of existing methods. Albeit simple, we verify the effectiveness of memory regularization on two synthetic-to-real benchmarks: GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, yielding +11.1% and +11.3% mIoU improvement over the baseline model, respectively. Besides, a similar +12.0% mIoU improvement is observed on the cross-city benchmark: Cityscapes -> Oxford RobotCar.



### Geometry-Aware Generation of Adversarial Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1912.11171v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11171v3)
- **Published**: 2019-12-24 01:52:46+00:00
- **Updated**: 2020-12-13 06:23:06+00:00
- **Authors**: Yuxin Wen, Jiehong Lin, Ke Chen, C. L. Philip Chen, Kui Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models have been shown to be vulnerable to adversarial examples. While most of the existing methods for adversarial attack and defense work on the 2D image domain, a few recent attempts have been made to extend them to 3D point cloud data. However, adversarial results obtained by these methods typically contain point outliers, which are both noticeable and easy to defend against using the simple techniques of outlier removal. Motivated by the different mechanisms by which humans perceive 2D images and 3D shapes, in this paper we propose the new design of \emph{geometry-aware objectives}, whose solutions favor (the discrete versions of) the desired surface properties of smoothness and fairness. To generate adversarial point clouds, we use a targeted attack misclassification loss that supports continuous pursuit of increasingly malicious signals. Regularizing the targeted attack loss with our proposed geometry-aware objectives results in our proposed method, Geometry-Aware Adversarial Attack ($GeoA^3$). The results of $GeoA^3$ tend to be more harmful, arguably harder to defend against, and of the key adversarial characterization of being imperceptible to humans. While the main focus of this paper is to learn to generate adversarial point clouds, we also present a simple but effective algorithm termed $Geo_{+}A^3$-IterNormPro, with Iterative Normal Projection (IterNorPro) that solves a new objective function $Geo_{+}A^3$, towards surface-level adversarial attacks via generation of adversarial point clouds. We quantitatively evaluate our methods on both synthetic and physical objects in terms of attack success rate and geometric regularity. For a qualitative evaluation, we conduct subjective studies by collecting human preferences from Amazon Mechanical Turk. Comparative results in comprehensive experiments confirm the advantages of our proposed methods.



### Self-adaption grey DBSCAN clustering
- **Arxiv ID**: http://arxiv.org/abs/1912.11477v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11477v1)
- **Published**: 2019-12-24 02:46:15+00:00
- **Updated**: 2019-12-24 02:46:15+00:00
- **Authors**: Shizhan Lu
- **Comment**: 8 pages, 4 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:1906.11416
- **Journal**: None
- **Summary**: Clustering analysis, a classical issue in data mining, is widely used in various research areas. This article aims at proposing a self-adaption grey DBSCAN clustering (SAG-DBSCAN) algorithm. First, the grey relational matrix is used to obtain the grey local density indicator, and then this indicator is applied to make self-adapting noise identification for obtaining a dense subset of clustering dataset, finally, the DBSCAN which automatically selects parameters is utilized to cluster the dense subset. Several frequently-used datasets were used to demonstrate the performance and effectiveness of the proposed clustering algorithm and to compare the results with those of other state-of-the-art algorithms. The comprehensive comparisons indicate that our method has advantages over other compared methods.



### Cascading Convolutional Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/1912.11180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11180v1)
- **Published**: 2019-12-24 02:46:36+00:00
- **Updated**: 2019-12-24 02:46:36+00:00
- **Authors**: Huanglin Yu, Ke Chen, Kaiqi Wang, Yanlin Qian, Zhaoxiang Zhang, Kui Jia
- **Comment**: 9 pages, 5 figures, 4 tables, accepted by AAAI-20
- **Journal**: None
- **Summary**: Regressing the illumination of a scene from the representations of object appearances is popularly adopted in computational color constancy. However, it's still challenging due to intrinsic appearance and label ambiguities caused by unknown illuminants, diverse reflection property of materials and extrinsic imaging factors (such as different camera sensors). In this paper, we introduce a novel algorithm by Cascading Convolutional Color Constancy (in short, C4) to improve robustness of regression learning and achieve stable generalization capability across datasets (different cameras and scenes) in a unique framework. The proposed C4 method ensembles a series of dependent illumination hypotheses from each cascade stage via introducing a weighted multiply-accumulate loss function, which can inherently capture different modes of illuminations and explicitly enforce coarse-to-fine network optimization. Experimental results on the public Color Checker and NUS 8-Camera benchmarks demonstrate superior performance of the proposed algorithm in comparison with the state-of-the-art methods, especially for more difficult scenes.



### A Comprehensive Analysis of Weakly-Supervised Semantic Segmentation in Different Image Domains
- **Arxiv ID**: http://arxiv.org/abs/1912.11186v3
- **DOI**: 10.1007/s11263-020-01373-4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11186v3)
- **Published**: 2019-12-24 03:00:34+00:00
- **Updated**: 2020-10-17 20:19:27+00:00
- **Authors**: Lyndon Chan, Mahdi S. Hosseini, Konstantinos N. Plataniotis
- **Comment**: 23 pages; accepted by International Journal of Computer Vision
  (IJCV). Associated code available at
  https://github.com/lyndonchan/wsss-analysis. To view Supplementary Materials,
  please download pdf file listed under "Ancillary files". Int J Comput Vis
  (2020)
- **Journal**: None
- **Summary**: Recently proposed methods for weakly-supervised semantic segmentation have achieved impressive performance in predicting pixel classes despite being trained with only image labels which lack positional information. Because image annotations are cheaper and quicker to generate, weak supervision is more practical than full supervision for training segmentation algorithms. These methods have been predominantly developed to solve the background separation and partial segmentation problems presented by natural scene images and it is unclear whether they can be simply transferred to other domains with different characteristics, such as histopathology and satellite images, and still perform well. This paper evaluates state-of-the-art weakly-supervised semantic segmentation methods on natural scene, histopathology, and satellite image datasets and analyzes how to determine which method is most suitable for a given dataset. Our experiments indicate that histopathology and satellite images present a different set of problems for weakly-supervised semantic segmentation than natural scene images, such as ambiguous boundaries and class co-occurrence. Methods perform well for datasets they were developed on, but tend to perform poorly on other datasets. We present some practical techniques for these methods on unseen datasets and argue that more work is needed for a generalizable approach to weakly-supervised semantic segmentation. Our full code implementation is available on GitHub: https://github.com/lyndonchan/wsss-analysis.



### Adversarial AutoAugment
- **Arxiv ID**: http://arxiv.org/abs/1912.11188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11188v1)
- **Published**: 2019-12-24 03:17:17+00:00
- **Updated**: 2019-12-24 03:17:17+00:00
- **Authors**: Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong
- **Comment**: ICLR2020
- **Journal**: None
- **Summary**: Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.



### BETANAS: BalancEd TrAining and selective drop for Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.11191v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11191v1)
- **Published**: 2019-12-24 03:24:06+00:00
- **Updated**: 2019-12-24 03:24:06+00:00
- **Authors**: Muyuan Fang, Qiang Wang, Zhao Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic neural architecture search techniques are becoming increasingly important in machine learning area. Especially, weight sharing methods have shown remarkable potentials on searching good network architectures with few computational resources. However, existing weight sharing methods mainly suffer limitations on searching strategies: these methods either uniformly train all network paths to convergence which introduces conflicts between branches and wastes a large amount of computation on unpromising candidates, or selectively train branches with different frequency which leads to unfair evaluation and comparison among paths. To address these issues, we propose a novel neural architecture search method with balanced training strategy to ensure fair comparisons and a selective drop mechanism to reduce conflicts among candidate paths. The experimental results show that our proposed method can achieve a leading performance of 79.0% on ImageNet under mobile settings, which outperforms other state-of-the-art methods in both accuracy and efficiency.



### A Simple and Effective Framework for Pairwise Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.11194v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11194v3)
- **Published**: 2019-12-24 03:47:25+00:00
- **Updated**: 2020-06-18 15:44:21+00:00
- **Authors**: Qi Qi, Yan Yan, Xiaoyu Wang, Tianbao Yang
- **Comment**: 16 pages, 5 figures
- **Journal**: None
- **Summary**: Deep metric learning (DML) has received much attention in deep learning due to its wide applications in computer vision. Previous studies have focused on designing complicated losses and hard example mining methods, which are mostly heuristic and lack of theoretical understanding. In this paper, we cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. It identifies the most critical issue in this problem--imbalanced data pairs. To tackle this issue, we propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows us to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate that our simple and effective method outperforms the state-of-the-art results. Codes are available at: https://github.com/qiqi-helloworld/A-Simple-and-Effective-Framework-for-Pairewise-Distance-Metric-Learning



### Computation Reallocation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.11234v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11234v1)
- **Published**: 2019-12-24 07:19:04+00:00
- **Updated**: 2019-12-24 07:19:04+00:00
- **Authors**: Feng Liang, Chen Lin, Ronghao Guo, Ming Sun, Wei Wu, Junjie Yan, Wanli Ouyang
- **Comment**: ICLR2020
- **Journal**: None
- **Summary**: The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9% and 1.7% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.



### Ordered or Orderless: A Revisit for Video based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.11236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11236v1)
- **Published**: 2019-12-24 07:29:14+00:00
- **Updated**: 2019-12-24 07:29:14+00:00
- **Authors**: Le Zhang, Zenglin Shi, Joey Tianyi Zhou, Ming-Ming Cheng, Yun Liu, Jia-Wang Bian, Zeng Zeng, Chunhua Shen
- **Comment**: Under Minor Revision in IEEE TPAMI
- **Journal**: None
- **Summary**: Is recurrent network really necessary for learning a good visual representation for video based person re-identification (VPRe-id)? In this paper, we first show that the common practice of employing recurrent neural networks (RNNs) to aggregate temporal spatial features may not be optimal. Specifically, with a diagnostic analysis, we show that the recurrent structure may not be effective to learn temporal dependencies than what we expected and implicitly yields an orderless representation. Based on this observation, we then present a simple yet surprisingly powerful approach for VPRe-id, where we treat VPRe-id as an efficient orderless ensemble of image based person re-identification problem. More specifically, we divide videos into individual images and re-identify person with ensemble of image based rankers. Under the i.i.d. assumption, we provide an error bound that sheds light upon how could we improve VPRe-id. Our work also presents a promising way to bridge the gap between video and image based person re-identification. Comprehensive experimental evaluations demonstrate that the proposed solution achieves state-of-the-art performances on multiple widely used datasets (iLIDS-VID, PRID 2011, and MARS).



### Multi-Graph Transformer for Free-Hand Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.11258v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11258v3)
- **Published**: 2019-12-24 09:28:10+00:00
- **Updated**: 2021-03-25 04:34:16+00:00
- **Authors**: Peng Xu, Chaitanya K. Joshi, Xavier Bresson
- **Comment**: This paper has been accepted by IEEE TNNLS
- **Journal**: None
- **Summary**: Learning meaningful representations of free-hand sketches remains a challenging task given the signal sparsity and the high-level abstraction of sketches. Existing techniques have focused on exploiting either the static nature of sketches with Convolutional Neural Networks (CNNs) or the temporal sequential property with Recurrent Neural Networks (RNNs). In this work, we propose a new representation of sketches as multiple sparsely connected graphs. We design a novel Graph Neural Network (GNN), the Multi-Graph Transformer (MGT), for learning representations of sketches from multiple graphs which simultaneously capture global and local geometric stroke structures, as well as temporal information. We report extensive numerical experiments on a sketch recognition task to demonstrate the performance of the proposed approach. Particularly, MGT applied on 414k sketches from Google QuickDraw: (i) achieves small recognition gap to the CNN-based performance upper bound (72.80% vs. 74.22%), and (ii) outperforms all RNN-based models by a significant margin. To the best of our knowledge, this is the first work proposing to represent sketches as graphs and apply GNNs for sketch recognition. Code and trained models are available at https://github.com/PengBoXiangShang/multigraph_transformer.



### Deep Manifold Embedding for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.11264v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11264v3)
- **Published**: 2019-12-24 09:41:56+00:00
- **Updated**: 2021-03-27 16:15:13+00:00
- **Authors**: Zhiqiang Gong, Weidong Hu, Xiaoyong Du, Ping Zhong, Panhe Hu
- **Comment**: Accepted by IEEE TCYB
- **Journal**: None
- **Summary**: Deep learning methods have played a more and more important role in hyperspectral image classification. However, the general deep learning methods mainly take advantage of the information of sample itself or the pairwise information between samples while ignore the intrinsic data structure within the whole data. To tackle this problem, this work develops a novel deep manifold embedding method(DMEM) for hyperspectral image classification. First, each class in the image is modelled as a specific nonlinear manifold and the geodesic distance is used to measure the correlation between the samples. Then, based on the hierarchical clustering, the manifold structure of the data can be captured and each nonlinear data manifold can be divided into several sub-classes. Finally, considering the distribution of each sub-class and the correlation between different subclasses, the DMEM is constructed to preserve the estimated geodesic distances on the data manifold between the learned low dimensional features of different samples. Experiments over three real-world hyperspectral image datasets have demonstrated the effectiveness of the proposed method.



### Robustness of Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.11312v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11312v3)
- **Published**: 2019-12-24 12:18:37+00:00
- **Updated**: 2020-12-15 21:56:16+00:00
- **Authors**: Sabine Müller, Joachim Weickert, Norbert Graf
- **Comment**: 23 pages, 12 figures
- **Journal**: None
- **Summary**: Purpose: The segmentation of brain tumors is one of the most active areas of medical image analysis. While current methods perform superhuman on benchmark data sets, their applicability in daily clinical practice has not been evaluated. In our work we investigate the generalization behavior of deep neural networks in this scenario.   Approach: We evaluate the performance of three state-of-the-art methods, a basic U-net architecture and a cascadic Mumford-Shah approach. We also propose two simple modifications (which do not change the topology) to improve generalization performance.   Results: In our experiments we show that a well-trained U-network shows the best generalization behavior and is sufficient to solve this segmentation problem. We illustrate why extensions of this model in a realistic scenario can be not only pointless but even harmful.   Conclusions: We conclude from our experiments that the generalization performance of deep neural networks is severely limited in medical image analysis especially in the area of brain tumor segmentation. In our opinion, current topologies are optimized for the actual benchmark data set, but are not directly applicable in daily clinical practice.



### TRADI: Tracking deep neural network weight distributions for uncertainty estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.11316v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11316v5)
- **Published**: 2019-12-24 12:22:45+00:00
- **Updated**: 2021-03-25 12:27:09+00:00
- **Authors**: Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson, Isabelle Bloch
- **Comment**: Accepted to ECCV2020
- **Journal**: None
- **Summary**: During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by sampling an ensemble of networks from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does not require any changes in the architecture nor on the training procedure. We evaluate our method on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to other popular approaches.



### Adaptive Distraction Context Aware Tracking Based on Correlation Filter
- **Arxiv ID**: http://arxiv.org/abs/1912.11325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11325v1)
- **Published**: 2019-12-24 12:52:53+00:00
- **Updated**: 2019-12-24 12:52:53+00:00
- **Authors**: Fei Feng, Xiao-Jun Wu, Tianyang Xu, Josef Kittler, Xue-Feng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The Discriminative Correlation Filter (CF) uses a circulant convolution operation to provide several training samples for the design of a classifier that can distinguish the target from the background. The filter design may be interfered by objects close to the target during the tracking process, resulting in tracking failure. This paper proposes an adaptive distraction context aware tracking algorithm to solve this problem. In the response map obtained for the previous frame by the CF algorithm, we adaptively find the image blocks that are similar to the target and use them as negative samples. This diminishes the influence of similar image blocks on the classifier in the tracking process and its accuracy is improved. The tracking results on video sequences show that the algorithm can cope with rapid changes such as occlusion and rotation, and can adaptively use the distractive objects around the target as negative samples to improve the accuracy of target tracking.



### Robust Visual Tracking via Implicit Low-Rank Constraints and Structural Color Histograms
- **Arxiv ID**: http://arxiv.org/abs/1912.11343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11343v1)
- **Published**: 2019-12-24 13:27:48+00:00
- **Updated**: 2019-12-24 13:27:48+00:00
- **Authors**: Yi-Xuan Wang, Xiao-Jun Wu, Xue-Feng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: With the guaranteed discrimination and efficiency of spatial appearance model, Discriminative Correlation Filters (DCF-) based tracking methods have achieved outstanding performance recently. However, the construction of effective temporal appearance model is still challenging on account of filter degeneration becomes a significant factor that causes tracking failures in the DCF framework. To encourage temporal continuity and to explore the smooth variation of target appearance, we propose to enhance low-rank structure of the learned filters, which can be realized by constraining the successive filters within a $\ell_2$-norm ball. Moreover, we design a global descriptor, structural color histograms, to provide complementary support to the final response map, improving the stability and robustness to the DCF framework. The experimental results on standard benchmarks demonstrate that our Implicit Low-Rank Constraints and Structural Color Histograms (ILRCSCH) tracker outperforms state-of-the-art methods.



### Big Transfer (BiT): General Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.11370v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.11370v3)
- **Published**: 2019-12-24 14:04:11+00:00
- **Updated**: 2020-05-05 20:48:23+00:00
- **Authors**: Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby
- **Comment**: The first three authors contributed equally. Results on ObjectNet are
  reported in v3
- **Journal**: None
- **Summary**: Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.



### Characterizing the Decision Boundary of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.11460v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.11460v3)
- **Published**: 2019-12-24 18:30:11+00:00
- **Updated**: 2020-06-03 16:18:25+00:00
- **Authors**: Hamid Karimi, Tyler Derr, Jiliang Tang
- **Comment**: Please contact the first author for any issue or the question
  regarding this paper
- **Journal**: None
- **Summary**: Deep neural networks and in particular, deep neural classifiers have become an integral part of many modern applications. Despite their practical success, we still have limited knowledge of how they work and the demand for such an understanding is evergrowing. In this regard, one crucial aspect of deep neural network classifiers that can help us deepen our knowledge about their decision-making behavior is to investigate their decision boundaries. Nevertheless, this is contingent upon having access to samples populating the areas near the decision boundary. To achieve this, we propose a novel approach we call Deep Decision boundary Instance Generation (DeepDIG). DeepDIG utilizes a method based on adversarial example generation as an effective way of generating samples near the decision boundary of any deep neural network model. Then, we introduce a set of important principled characteristics that take advantage of the generated instances near the decision boundary to provide multifaceted understandings of deep neural networks. We have performed extensive experiments on multiple representative datasets across various deep neural network models and characterized their decision boundaries. The code is publicly available at https://github.com/hamidkarimi/DeepDIG/.



### FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network
- **Arxiv ID**: http://arxiv.org/abs/1912.11463v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.11463v1)
- **Published**: 2019-12-24 18:38:40+00:00
- **Updated**: 2019-12-24 18:38:40+00:00
- **Authors**: Zeeshan Khan, Mukul Khanna, Shanmuganathan Raman
- **Comment**: 2019 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP)
- **Journal**: None
- **Summary**: High dynamic range (HDR) image generation from a single exposure low dynamic range (LDR) image has been made possible due to the recent advances in Deep Learning. Various feed-forward Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. To better utilize the power of CNNs, we exploit the idea of feedback, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a single forward pass in a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations. This enables us to create a coarse-to-fine representation, leading to an improved reconstruction at every iteration. Various advantages over standard feed-forward networks include early reconstruction ability and better reconstruction quality with fewer network parameters. We design a dense feedback block and propose an end-to-end feedback network- FHDR for HDR image generation from a single exposure LDR image. Qualitative and quantitative evaluations show the superiority of our approach over the state-of-the-art methods.



### Dense RepPoints: Representing Visual Objects with Dense Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1912.11473v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11473v3)
- **Published**: 2019-12-24 18:59:10+00:00
- **Updated**: 2020-05-18 17:23:20+00:00
- **Authors**: Ze Yang, Yinghao Xu, Han Xue, Zheng Zhang, Raquel Urtasun, Liwei Wang, Stephen Lin, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new object representation, called Dense RepPoints, that utilizes a large set of points to describe an object at multiple levels, including both box level and pixel level. Techniques are proposed to efficiently process these dense points, maintaining near-constant complexity with increasing point numbers. Dense RepPoints is shown to represent and learn object segments well, with the use of a novel distance transform sampling method combined with set-to-set supervision. The distance transform sampling combines the strengths of contour and grid representations, leading to performance that surpasses counterparts based on contours or grids. Code is available at \url{https://github.com/justimyhxu/Dense-RepPoints}.



### SoundSpaces: Audio-Visual Navigation in 3D Environments
- **Arxiv ID**: http://arxiv.org/abs/1912.11474v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.11474v3)
- **Published**: 2019-12-24 18:59:50+00:00
- **Updated**: 2020-08-21 18:00:31+00:00
- **Authors**: Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, Kristen Grauman
- **Comment**: Accepted to ECCV 2020 (Spotlight). Project page:
  http://vision.cs.utexas.edu/projects/audio_visual_navigation/
- **Journal**: None
- **Summary**: Moving around in the world is naturally a multisensory experience, but today's embodied agents are deaf---restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception.



### Parallel optimization of fiber bundle segmentation for massive tractography datasets
- **Arxiv ID**: http://arxiv.org/abs/1912.11494v1
- **DOI**: 10.1109/ISBI.2019.8759208
- **Categories**: **cs.DS**, cs.CV, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1912.11494v1)
- **Published**: 2019-12-24 19:08:51+00:00
- **Updated**: 2019-12-24 19:08:51+00:00
- **Authors**: Andrea Vázquez, Narciso López-López, Nicole Labra, Miguel Figueroa, Cyril Poupon, Jean-François Mangin, Cecilia Hernández, Pamela Guevara
- **Comment**: This research has received funding from the European Union's Horizon
  2020 research and innovation programme under the Marie Sk{\l}odowska-Curie
  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO
  NACIONAL/2016-21160342, CONICYT FONDECYT 1161427, CONICYT PIA/Anillo de
  Investigaci\'on en Ciencia y Tecnolog\'ia ACT172121, CONICYT BASAL FB0008 and
  from CONICYT Basal FB0001
- **Journal**: None
- **Summary**: We present an optimized algorithm that performs automatic classification of white matter fibers based on a multi-subject bundle atlas. We implemented a parallel algorithm that improves upon its previous version in both execution time and memory usage. Our new version uses the local memory of each processor, which leads to a reduction in execution time. Hence, it allows the analysis of bigger subject and/or atlas datasets. As a result, the segmentation of a subject of 4,145,000 fibers is reduced from about 14 minutes in the previous version to about 6 minutes, yielding an acceleration of 2.34. In addition, the new algorithm reduces the memory consumption of the previous version by a factor of 0.79.



### Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.11521v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.11521v1)
- **Published**: 2019-12-24 20:35:57+00:00
- **Updated**: 2019-12-24 20:35:57+00:00
- **Authors**: Jialin Gao, Tong He, Xi Zhou, Shiming Ge
- **Comment**: None
- **Journal**: None
- **Summary**: A collection of approaches based on graph convolutional networks have proven success in skeleton-based action recognition by exploring neighborhood information and dense dependencies between intra-frame joints. However, these approaches usually ignore the spatial-temporal global context as well as the local relation between inter-frame and intra-frame. In this paper, we propose a focusing and diffusion mechanism to enhance graph convolutional networks by paying attention to the kinematic dependence of articulated human pose in a frame and their implicit dependencies over frames. In the focusing process, we introduce an attention module to learn a latent node over the intra-frame joints to convey spatial contextual information. In this way, the sparse connections between joints in a frame can be well captured, while the global context over the entire sequence is further captured by these hidden nodes with a bidirectional LSTM. In the diffusing process, the learned spatial-temporal contextual information is passed back to the spatial joints, leading to a bidirectional attentive graph convolutional network (BAGCN) that can facilitate skeleton-based action recognition. Extensive experiments on the challenging NTU RGB+D and Skeleton-Kinetics benchmarks demonstrate the efficacy of our approach.



### Barycenters of Natural Images -- Constrained Wasserstein Barycenters for Image Morphing
- **Arxiv ID**: http://arxiv.org/abs/1912.11545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11545v1)
- **Published**: 2019-12-24 21:26:40+00:00
- **Updated**: 2019-12-24 21:26:40+00:00
- **Authors**: Dror Simon, Aviad Aberdam
- **Comment**: None
- **Journal**: None
- **Summary**: Image interpolation, or image morphing, refers to a visual transition between two (or more) input images. For such a transition to look visually appealing, its desirable properties are (i) to be smooth; (ii) to apply the minimal required change in the image; and (iii) to seem "real", avoiding unnatural artifacts in each image in the transition. To obtain a smooth and straightforward transition, one may adopt the well-known Wasserstein Barycenter Problem (WBP). While this approach guarantees minimal changes under the Wasserstein metric, the resulting images might seem unnatural. In this work, we propose a novel approach for image morphing that possesses all three desired properties. To this end, we define a constrained variant of the WBP that enforces the intermediate images to satisfy an image prior. We describe an algorithm that solves this problem and demonstrate it using the sparse prior and generative adversarial networks.



### Depth Extraction from Video Using Non-parametric Sampling
- **Arxiv ID**: http://arxiv.org/abs/2002.04479v1
- **DOI**: 10.1007/978-3-642-33715-4_56
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2002.04479v1)
- **Published**: 2019-12-24 23:36:50+00:00
- **Updated**: 2019-12-24 23:36:50+00:00
- **Authors**: Kevin Karsch, Ce Liu, Sing Bing Kang
- **Comment**: arXiv admin note: text overlap with arXiv:2001.00987
- **Journal**: ECCV 2012: Computer Vision ECCV 2012: Lecture Notes in Computer
  Science, vol 7576 pp 775-788
- **Summary**: We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.



### Boundary Cues for 3D Object Shape Recovery
- **Arxiv ID**: http://arxiv.org/abs/1912.11566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.11566v1)
- **Published**: 2019-12-24 23:40:30+00:00
- **Updated**: 2019-12-24 23:40:30+00:00
- **Authors**: Kevin Karsch, Zicheng Liao, Jason Rock, Jonathan T. Barron, Derek Hoiem
- **Comment**: None
- **Journal**: None
- **Summary**: Early work in computer vision considered a host of geometric cues for both shape reconstruction and recognition. However, since then, the vision community has focused heavily on shading cues for reconstruction, and moved towards data-driven approaches for recognition. In this paper, we reconsider these perhaps overlooked "boundary" cues (such as self occlusions and folds in a surface), as well as many other established constraints for shape reconstruction. In a variety of user studies and quantitative tasks, we evaluate how well these cues inform shape reconstruction (relative to each other) in terms of both shape quality and shape recognition. Our findings suggest many new directions for future research in shape reconstruction, such as automatic boundary cue detection and relaxing assumptions in shape from shading (e.g. orthographic projection, Lambertian surfaces).



### DepthTransfer: Depth Extraction from Video Using Non-parametric Sampling
- **Arxiv ID**: http://arxiv.org/abs/2001.00987v1
- **DOI**: 10.1109/TPAMI.2014.2316835
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.00987v1)
- **Published**: 2019-12-24 23:46:16+00:00
- **Updated**: 2019-12-24 23:46:16+00:00
- **Authors**: Kevin Karsch, Ce Liu, Sing Bing Kang
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  Volume: 36 Issue: 11 pgs 2144-2158 (2014)
- **Summary**: We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.



