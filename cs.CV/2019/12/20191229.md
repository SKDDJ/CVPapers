# Arxiv Papers in cs.CV on 2019-12-29
### Visual Perception and Modelling in Unstructured Orchard for Apple Harvesting Robots
- **Arxiv ID**: http://arxiv.org/abs/1912.12555v1
- **DOI**: 10.1109/ACCESS.2020.2984556
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12555v1)
- **Published**: 2019-12-29 00:30:59+00:00
- **Updated**: 2019-12-29 00:30:59+00:00
- **Authors**: Hanwen Kang, Chao Chen
- **Comment**: 21 pages,10 figures
- **Journal**: None
- **Summary**: Vision perception and modelling are the essential tasks of robotic harvesting in the unstructured orchard. This paper develops a framework of visual perception and modelling for robotic harvesting of fruits in the orchard environments. The developed framework includes visual perception, scenarios mapping, and fruit modelling. The Visual perception module utilises a deep-learning model to perform multi-purpose visual perception task within the working scenarios; The scenarios mapping module applies OctoMap to represent the multiple classes of objects or elements within the environment; The fruit modelling module estimates the geometry property of objects and estimates the proper access pose of each fruit. The developed framework is implemented and evaluated in the apple orchards. The experiment results show that visual perception and modelling algorithm can accurately detect and localise the fruits, and modelling working scenarios in real orchard environments. The $F_{1}$ score and mean intersection of union of visual perception module on fruit detection and segmentation are 0.833 and 0.852, respectively. The accuracy of the fruit modelling in terms of centre localisation and pose estimation are 0.955 and 0.923, respectively. Overall, an accurate visual perception and modelling algorithm are presented in this paper.



### Active Learning in Video Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.12557v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12557v3)
- **Published**: 2019-12-29 00:42:06+00:00
- **Updated**: 2020-03-21 00:15:56+00:00
- **Authors**: Sima Behpour
- **Comment**: None
- **Journal**: In International Conference on Machine Learning, pp. 563-572. 2019
- **Summary**: Active learning methods, like uncertainty sampling, combined with probabilistic prediction techniques have achieved success in various problems like image classification and text classification. For more complex multivariate prediction tasks, the relationships between labels play an important role in designing structured classifiers with better performance. However, computational time complexity limits prevalent probabilistic methods from effectively supporting active learning. Specifically, while non-probabilistic methods based on structured support vector machines can be tractably applied to predicting bipartite matchings, conditional random fields are intractable for these structures. We propose an adversarial approach for active learning with structured prediction domains that is tractable for matching. We evaluate this approach algorithmically in an important structured prediction problems: object tracking in videos. We demonstrate better accuracy and computational efficiency for our proposed method.



### Infant brain MRI segmentation with dilated convolution pyramid downsampling and self-attention
- **Arxiv ID**: http://arxiv.org/abs/1912.12570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12570v2)
- **Published**: 2019-12-29 02:33:57+00:00
- **Updated**: 2020-03-28 03:01:46+00:00
- **Authors**: Zhihao Lei, Lin Qi, Ying Wei, Yunlong Zhou
- **Comment**: 5 pages,5 figures
- **Journal**: None
- **Summary**: In this paper, we propose a dual aggregation network to adaptively aggregate different information in infant brain MRI segmentation. More precisely, we added two modules based on 3D-UNet to better model information at different levels and locations. The dilated convolution pyramid downsampling module is mainly to solve the problem of loss of spatial information on the downsampling process, and it can effectively save details while reducing the resolution. The self-attention module can integrate the remote dependence on the feature maps in two dimensions of spatial and channel, effectively improving the representation ability and discriminating ability of the model. Our results are compared to the winners of iseg2017's first evaluation, the DICE ratio of WM and GM increased by 0.7%, and CSF is comparable.In the latest evaluation of the iseg-2019 cross-dataset challenge,we achieve the first place in the DICE of WM and GM, and the DICE of CSF is second.



### A Time-Delay Feedback Neural Network for Discriminating Small, Fast-Moving Targets in Complex Dynamic Environments
- **Arxiv ID**: http://arxiv.org/abs/2001.05846v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05846v5)
- **Published**: 2019-12-29 03:10:36+00:00
- **Updated**: 2021-06-28 02:02:51+00:00
- **Authors**: Hongxin Wang, Huatian Wang, Jiannan Zhao, Cheng Hu, Jigen Peng, Shigang Yue
- **Comment**: 14 pages, 16 figures
- **Journal**: None
- **Summary**: Discriminating small moving objects within complex visual environments is a significant challenge for autonomous micro robots that are generally limited in computational power. By exploiting their highly evolved visual systems, flying insects can effectively detect mates and track prey during rapid pursuits, even though the small targets equate to only a few pixels in their visual field. The high degree of sensitivity to small target movement is supported by a class of specialized neurons called small target motion detectors (STMDs). Existing STMD-based computational models normally comprise four sequentially arranged neural layers interconnected via feedforward loops to extract information on small target motion from raw visual inputs. However, feedback, another important regulatory circuit for motion perception, has not been investigated in the STMD pathway and its functional roles for small target motion detection are not clear. In this paper, we propose an STMD-based neural network with feedback connection (Feedback STMD), where the network output is temporally delayed, then fed back to the lower layers to mediate neural responses. We compare the properties of the model with and without the time-delay feedback loop, and find it shows preference for high-velocity objects. Extensive experiments suggest that the Feedback STMD achieves superior detection performance for fast-moving small targets, while significantly suppressing background false positive movements which display lower velocities. The proposed feedback model provides an effective solution in robotic visual systems for detecting fast-moving small targets that are always salient and potentially threatening.



### Human Correspondence Consensus for 3D Object Semantic Understanding
- **Arxiv ID**: http://arxiv.org/abs/1912.12577v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12577v2)
- **Published**: 2019-12-29 04:24:22+00:00
- **Updated**: 2020-11-26 05:24:05+00:00
- **Authors**: Yujing Lou, Yang You, Chengkun Li, Zhoujun Cheng, Liangwei Li, Lizhuang Ma, Weiming Wang, Cewu Lu
- **Comment**: 18 pages; ECCV 2020
- **Journal**: None
- **Summary**: Semantic understanding of 3D objects is crucial in many applications such as object manipulation. However, it is hard to give a universal definition of point-level semantics that everyone would agree on. We observe that people have a consensus on semantic correspondences between two areas from different objects, but are less certain about the exact semantic meaning of each area. Therefore, we argue that by providing human labeled correspondences between different objects from the same category instead of explicit semantic labels, one can recover rich semantic information of an object. In this paper, we introduce a new dataset named CorresPondenceNet. Based on this dataset, we are able to learn dense semantic embeddings with a novel geodesic consistency loss. Accordingly, several state-of-the-art networks are evaluated on this correspondence benchmark. We further show that CorresPondenceNet could not only boost fine-grained understanding of heterogeneous objects but also cross-object registration and partial object matching.



### Towards Unified INT8 Training for Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1912.12607v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.12607v1)
- **Published**: 2019-12-29 08:37:53+00:00
- **Updated**: 2019-12-29 08:37:53+00:00
- **Authors**: Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, Junjie Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Recently low-bit (e.g., 8-bit) network quantization has been extensively studied to accelerate the inference. Besides inference, low-bit training with quantized gradients can further bring more considerable acceleration, since the backward process is often computation-intensive. Unfortunately, the inappropriate quantization of backward propagation usually makes the training unstable and even crash. There lacks a successful unified low-bit training framework that can support diverse networks on various tasks. In this paper, we give an attempt to build a unified 8-bit (INT8) training framework for common convolutional neural networks from the aspects of both accuracy and speed. First, we empirically find the four distinctive characteristics of gradients, which provide us insightful clues for gradient quantization. Then, we theoretically give an in-depth analysis of the convergence bound and derive two principles for stable INT8 training. Finally, we propose two universal techniques, including Direction Sensitive Gradient Clipping that reduces the direction deviation of gradients and Deviation Counteractive Learning Rate Scaling that avoids illegal gradient update along the wrong direction. The experiments show that our unified solution promises accurate and efficient INT8 training for a variety of networks and tasks, including MobileNetV2, InceptionV3 and object detection that prior studies have never succeeded. Moreover, it enjoys a strong flexibility to run on off-the-shelf hardware, and reduces the training time by 22% on Pascal GPU without too much optimization effort. We believe that this pioneering study will help lead the community towards a fully unified INT8 training for convolutional neural networks.



### Deep learning surrogate models for spatial and visual connectivity
- **Arxiv ID**: http://arxiv.org/abs/1912.12616v1
- **DOI**: 10.1177/1478077119894483
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML, I.6.3; J.6.1
- **Links**: [PDF](http://arxiv.org/pdf/1912.12616v1)
- **Published**: 2019-12-29 09:17:19+00:00
- **Updated**: 2019-12-29 09:17:19+00:00
- **Authors**: Sherif Tarabishy, Stamatios Psarras, Marcin Kosicki, Martha Tsigkari
- **Comment**: Accepted manuscript in the International Journal of Architectural
  Computing (2019)
- **Journal**: None
- **Summary**: Spatial and visual connectivity are important metrics when developing workplace layouts. Calculating those metrics in real-time can be difficult, depending on the size of the floor plan being analysed and the resolution of the analyses. This paper investigates the possibility of considerably speeding up the outcomes of such computationally intensive simulations by using machine learning to create models capable of identifying the spatial and visual connectivity potential of a space. To that end we present the entire process of investigating different machine learning models and a pipeline for training them on such task, from the incorporation of a bespoke spatial and visual connectivity analysis engine through a distributed computation pipeline, to the process of synthesizing training data and evaluating the performance of different neural networks.



### Copy Move Source-Target Disambiguation through Multi-Branch CNNs
- **Arxiv ID**: http://arxiv.org/abs/1912.12640v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1912.12640v2)
- **Published**: 2019-12-29 11:56:33+00:00
- **Updated**: 2021-01-21 10:24:36+00:00
- **Authors**: Mauro Barni, Quoc-Tin Phan, Benedetta Tondi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method to identify the source and target regions of a copy-move forgery so allow a correct localisation of the tampered area. First, we cast the problem into a hypothesis testing framework whose goal is to decide which region between the two nearly-duplicate regions detected by a generic copy-move detector is the original one. Then we design a multi-branch CNN architecture that solves the hypothesis testing problem by learning a set of features capable to reveal the presence of interpolation artefacts and boundary inconsistencies in the copy-moved area. The proposed architecture, trained on a synthetic dataset explicitly built for this purpose, achieves good results on copy-move forgeries from both synthetic and realistic datasets. Based on our tests, the proposed disambiguation method can reliably reveal the target region even in realistic cases where an approximate version of the copy-move localization mask is provided by a state-of-the-art copy-move detection algorithm.



### Personalizing Fast-Forward Videos Based on Visual and Textual Features from Social Network
- **Arxiv ID**: http://arxiv.org/abs/1912.12655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12655v1)
- **Published**: 2019-12-29 14:09:32+00:00
- **Updated**: 2019-12-29 14:09:32+00:00
- **Authors**: Washington L. S. Ramos, Michel M. Silva, Edson R. Araujo, Alan C. Neves, Erickson R. Nascimento
- **Comment**: None
- **Journal**: None
- **Summary**: The growth of Social Networks has fueled the habit of people logging their day-to-day activities, and long First-Person Videos (FPVs) are one of the main tools in this new habit. Semantic-aware fast-forward methods are able to decrease the watch time and select meaningful moments, which is key to increase the chances of these videos being watched. However, these methods can not handle semantics in terms of personalization. In this work, we present a new approach to automatically creating personalized fast-forward videos for FPVs. Our approach explores the availability of text-centric data from the user's social networks such as status updates to infer her/his topics of interest and assigns scores to the input frames according to her/his preferences. Extensive experiments are conducted on three different datasets with simulated and real-world users as input, achieving an average F1 score of up to 12.8 percentage points higher than the best competitors. We also present a user study to demonstrate the effectiveness of our method.



### Mixed-Precision Quantized Neural Network with Progressively Decreasing Bitwidth For Image Classification and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.12656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12656v1)
- **Published**: 2019-12-29 14:11:33+00:00
- **Updated**: 2019-12-29 14:11:33+00:00
- **Authors**: Tianshu Chu, Qin Luo, Jie Yang, Xiaolin Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Efficient model inference is an important and practical issue in the deployment of deep neural network on resource constraint platforms. Network quantization addresses this problem effectively by leveraging low-bit representation and arithmetic that could be conducted on dedicated embedded systems. In the previous works, the parameter bitwidth is set homogeneously and there is a trade-off between superior performance and aggressive compression. Actually the stacked network layers, which are generally regarded as hierarchical feature extractors, contribute diversely to the overall performance. For a well-trained neural network, the feature distributions of different categories differentiate gradually as the network propagates forward. Hence the capability requirement on the subsequent feature extractors is reduced. It indicates that the neurons in posterior layers could be assigned with lower bitwidth for quantized neural networks. Based on this observation, a simple but effective mixed-precision quantized neural network with progressively ecreasing bitwidth is proposed to improve the trade-off between accuracy and compression. Extensive experiments on typical network architectures and benchmark datasets demonstrate that the proposed method could achieve better or comparable results while reducing the memory space for quantized parameters by more than 30\% in comparison with the homogeneous counterparts. In addition, the results also demonstrate that the higher-precision bottom layers could boost the 1-bit network performance appreciably due to a better preservation of the original image information while the lower-precision posterior layers contribute to the regularization of $k-$bit networks.



### FLAT: Few-Shot Learning via Autoencoding Transformation Regularizers
- **Arxiv ID**: http://arxiv.org/abs/1912.12674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12674v1)
- **Published**: 2019-12-29 15:26:28+00:00
- **Updated**: 2019-12-29 15:26:28+00:00
- **Authors**: Haohang Xu, Hongkai Xiong, Guojun Qi
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most significant challenges facing a few-shot learning task is the generalizability of the (meta-)model from the base to the novel categories. Most of existing few-shot learning models attempt to address this challenge by either learning the meta-knowledge from multiple simulated tasks on the base categories, or resorting to data augmentation by applying various transformations to training examples. However, the supervised nature of model training in these approaches limits their ability of exploring variations across different categories, thus restricting their cross-category generalizability in modeling novel concepts. To this end, we present a novel regularization mechanism by learning the change of feature representations induced by a distribution of transformations without using the labels of data examples. We expect this regularizer could expand the semantic space of base categories to cover that of novel categories through the transformation of feature representations. It could minimize the risk of overfitting into base categories by inspecting the transformation-augmented variations at the encoded feature level. This results in the proposed FLAT (Few-shot Learning via Autoencoding Transformations) approach by autoencoding the applied transformations. The experiment results show the superior performances to the current state-of-the-art methods in literature.



### Very Long Natural Scenery Image Prediction by Outpainting
- **Arxiv ID**: http://arxiv.org/abs/1912.12688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12688v1)
- **Published**: 2019-12-29 16:29:01+00:00
- **Updated**: 2019-12-29 16:29:01+00:00
- **Authors**: Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, Shuicheng Yan
- **Comment**: ICCV-19
- **Journal**: None
- **Summary**: Comparing to image inpainting, image outpainting receives less attention due to two challenges in it. The first challenge is how to keep the spatial and content consistency between generated images and original input. The second challenge is how to maintain high quality in generated results, especially for multi-step generations in which generated regions are spatially far away from the initial input. To solve the two problems, we devise some innovative modules, named Skip Horizontal Connection and Recurrent Content Transfer, and integrate them into our designed encoder-decoder structure. By this design, our network can generate highly realistic outpainting prediction effectively and efficiently. Other than that, our method can generate new images with very long sizes while keeping the same style and semantic content as the given input. To test the effectiveness of the proposed architecture, we collect a new scenery dataset with diverse, complicated natural scenes. The experimental results on this dataset have demonstrated the efficacy of our proposed network. The code and dataset are available from https://github.com/z-x-yang/NS-Outpainting.



### An Analytical Workflow for Clustering Forensic Images
- **Arxiv ID**: http://arxiv.org/abs/2001.05845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2001.05845v1)
- **Published**: 2019-12-29 18:20:07+00:00
- **Updated**: 2019-12-29 18:20:07+00:00
- **Authors**: Sara Mousavi, Dylan Lee, Tatianna Griffin, Dawnie Steadman, Audris Mockus
- **Comment**: None
- **Journal**: None
- **Summary**: Large collections of images, if curated, drastically contribute to the quality of research in many domains. Unsupervised clustering is an intuitive, yet effective step towards curating such datasets. In this work, we present a workflow for unsupervisedly clustering a large collection of forensic images. The workflow utilizes classic clustering on deep feature representation of the images in addition to domain-related data to group them together. Our manual evaluation shows a purity of 89\% for the resulted clusters.



### The Semantic Mutex Watershed for Efficient Bottom-Up Semantic Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.12717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12717v1)
- **Published**: 2019-12-29 19:48:39+00:00
- **Updated**: 2019-12-29 19:48:39+00:00
- **Authors**: Steffen Wolf, Yuyan Li, Constantin Pape, Alberto Bailoni, Anna Kreshuk, Fred A. Hamprecht
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic instance segmentation is the task of simultaneously partitioning an image into distinct segments while associating each pixel with a class label. In commonly used pipelines, segmentation and label assignment are solved separately since joint optimization is computationally expensive. We propose a greedy algorithm for joint graph partitioning and labeling derived from the efficient Mutex Watershed partitioning algorithm. It optimizes an objective function closely related to the Symmetric Multiway Cut objective and empirically shows efficient scaling behavior. Due to the algorithm's efficiency it can operate directly on pixels without prior over-segmentation of the image into superpixels. We evaluate the performance on the Cityscapes dataset (2D urban scenes) and on a 3D microscopy volume. In urban scenes, the proposed algorithm combined with current deep neural networks outperforms the strong baseline of `Panoptic Feature Pyramid Networks' by Kirillov et al. (2019). In the 3D electron microscopy images, we show explicitly that our joint formulation outperforms a separate optimization of the partitioning and labeling problems.



### SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory
- **Arxiv ID**: http://arxiv.org/abs/1912.12726v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12726v1)
- **Published**: 2019-12-29 20:38:32+00:00
- **Updated**: 2019-12-29 20:38:32+00:00
- **Authors**: Steven W. Chen, Guilherme V. Nardari, Elijah S. Lee, Chao Qu, Xu Liu, Roseli A. F. Romero, Vijay Kumar
- **Comment**: 8 pages, 5 figures, IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: This paper describes an end-to-end pipeline for tree diameter estimation based on semantic segmentation and lidar odometry and mapping. Accurate mapping of this type of environment is challenging since the ground and the trees are surrounded by leaves, thorns and vines, and the sensor typically experiences extreme motion. We propose a semantic feature based pose optimization that simultaneously refines the tree models while estimating the robot pose. The pipeline utilizes a custom virtual reality tool for labeling 3D scans that is used to train a semantic segmentation network. The masked point cloud is used to compute a trellis graph that identifies individual instances and extracts relevant features that are used by the SLAM module. We show that traditional lidar and image based methods fail in the forest environment on both Unmanned Aerial Vehicle (UAV) and hand-carry systems, while our method is more robust, scalable, and automatically generates tree diameter estimations.



### Deep Context-Aware Kernel Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.12735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.12735v1)
- **Published**: 2019-12-29 21:18:09+00:00
- **Updated**: 2019-12-29 21:18:09+00:00
- **Authors**: Mingyuan Jiu, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Context plays a crucial role in visual recognition as it provides complementary clues for different learning tasks including image classification and annotation. As the performances of these tasks are currently reaching a plateau, any extra knowledge, including context, should be leveraged in order to seek significant leaps in these performances. In the particular scenario of kernel machines, context-aware kernel design aims at learning positive semi-definite similarity functions which return high values not only when data share similar contents, but also similar structures (a.k.a contexts). However, the use of context in kernel design has not been fully explored; indeed, context in these solutions is handcrafted instead of being learned. In this paper, we introduce a novel deep network architecture that learns context in kernel design. This architecture is fully determined by the solution of an objective function mixing a content term that captures the intrinsic similarity between data, a context criterion which models their structure and a regularization term that helps designing smooth kernel network representations. The solution of this objective function defines a particular deep network architecture whose parameters correspond to different variants of learned contexts including layerwise, stationary and classwise; larger values of these parameters correspond to the most influencing contextual relationships between data. Extensive experiments conducted on the challenging ImageCLEF Photo Annotation and Corel5k benchmarks show that our deep context networks are highly effective for image classification and the learned contexts further enhance the performance of image annotation.



### Target-less registration of point clouds: A review
- **Arxiv ID**: http://arxiv.org/abs/1912.12756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.12756v1)
- **Published**: 2019-12-29 23:12:33+00:00
- **Updated**: 2019-12-29 23:12:33+00:00
- **Authors**: Yue Pan
- **Comment**: 9 pages, 14 figures, written as the final report of the geomatics
  seminar at ETH Zurich
- **Journal**: None
- **Summary**: Point cloud registration has been one of the basic steps of point cloud processing, which has a lot of applications in remote sensing and robotics. In this report, we summarized the basic workflow of target-less point cloud registration,namely correspondence determination and transformation estimation. Then we reviewed three commonly used groups of registration approaches, namely the feature matching based methods, the iterative closest points algorithm and the randomly hypothesis and verify based methods. Besides, we analyzed the advantage and disadvantage of these methods are introduced their common application scenarios. At last, we discussed the challenges of current point cloud registration methods and proposed several open questions for the future development of automatic registration approaches.



