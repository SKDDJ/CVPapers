# Arxiv Papers in cs.CV on 2019-12-11
### Wide-Area Land Cover Mapping with Sentinel-1 Imagery using Deep Learning Semantic Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/1912.05067v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05067v4)
- **Published**: 2019-12-11 00:38:37+00:00
- **Updated**: 2021-03-23 14:05:35+00:00
- **Authors**: Sanja Šćepanović, Oleg Antropov, Pekka Laurila, Yrjö Rauste, Vladimir Ignatenko, Jaan Praks
- **Comment**: None
- **Journal**: None
- **Summary**: Land cover mapping is essential to monitoring the environment and understanding the effects of human activities on it. The automatic approaches to land cover mapping (i.e., image segmentation) mostly used traditional machine learning that requires heuristic feature design. On natural images, deep learning has outperformed traditional machine learning approaches for image segmentation. On remote sensing images, recent studies demonstrate successful applications of specific deep learning models to small-scale land cover mapping tasks (e.g., to classify wetland complexes). However, it is not readily clear which of the existing models are the best candidates for which remote sensing task. In this study, we answer that question for mapping the fundamental land cover classes using satellite radar data. We took Sentinel-1 C-band SAR images available at no cost to users as representative data. CORINE land cover map was used as a reference, and the models were trained to distinguish between the 5 major CORINE classes. We selected seven among the state-of-the-art semantic segmentation models so that they cover a diverse set of approaches: U-Net, DeepLabV3+, PSPNet, BiSeNet, SegNet, FC-DenseNet, and FRRN-B. The models were pre-trained on the ImageNet dataset and further fine-tuned in this study. All the models demonstrated solid performance with overall accuracy between 87.9% and 93.1%, and with good to a very good agreement (kappa statistic between 0.75 and 0.86). The two best models were FC-DenseNet and SegNet, with the latter having a much smaller inference time. Overall, our results indicate that the semantic segmentation models are suitable for efficient wide-area mapping using satellite SAR imagery and also provide baseline accuracy against which the newly proposed models should be evaluated.



### RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.05070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05070v1)
- **Published**: 2019-12-11 01:10:33+00:00
- **Updated**: 2019-12-11 01:10:33+00:00
- **Authors**: Shaoru Wang, Yongchao Gong, Junliang Xing, Lichao Huang, Chang Huang, Weiming Hu
- **Comment**: Accepted by AAAI20
- **Journal**: None
- **Summary**: Object detection and instance segmentation are two fundamental computer vision tasks. They are closely correlated but their relations have not yet been fully explored in most previous work. This paper presents RDSNet, a novel deep architecture for reciprocal object detection and instance segmentation. To reciprocate these two tasks, we design a two-stream structure to learn features on both the object level (i.e., bounding boxes) and the pixel level (i.e., instance masks) jointly. Within this structure, information from the two streams is fused alternately, namely information on the object level introduces the awareness of instance and translation variance to the pixel level, and information on the pixel level refines the localization accuracy of objects on the object level in return. Specifically, a correlation module and a cropping module are proposed to yield instance masks, as well as a mask based boundary refinement module for more accurate bounding boxes. Extensive experimental analyses and comparisons on the COCO dataset demonstrate the effectiveness and efficiency of RDSNet. The source code is available at https://github.com/wangsr126/RDSNet.



### UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.05074v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05074v2)
- **Published**: 2019-12-11 01:26:22+00:00
- **Updated**: 2020-01-28 23:15:28+00:00
- **Authors**: Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming Liang
- **Comment**: Journal of IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects -- an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.



### Learning from Noisy Anchors for One-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.05086v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05086v2)
- **Published**: 2019-12-11 02:17:32+00:00
- **Updated**: 2020-05-28 18:35:34+00:00
- **Authors**: Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, Larry S. Davis
- **Comment**: CVPR 2020 camera ready
- **Journal**: None
- **Summary**: State-of-the-art object detectors rely on regressing and classifying an extensive list of possible anchors, which are divided into positive and negative samples based on their intersection-over-union (IoU) with corresponding groundtruth objects. Such a harsh split conditioned on IoU results in binary labels that are potentially noisy and challenging for training. In this paper, we propose to mitigate noise incurred by imperfect label assignment such that the contributions of anchors are dynamically determined by a carefully constructed cleanliness score associated with each anchor. Exploring outputs from both regression and classification branches, the cleanliness scores, estimated without incurring any additional computational overhead, are used not only as soft labels to supervise the training of the classification branch but also sample re-weighting factors for improved localization and classification accuracy. We conduct extensive experiments on COCO, and demonstrate, among other things, the proposed approach steadily improves RetinaNet by ~2% with various backbones.



### BioNet: Infusing Biomarker Prior into Global-to-Local Network for Choroid Segmentation in Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1912.05090v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1912.05090v1)
- **Published**: 2019-12-11 02:57:03+00:00
- **Updated**: 2019-12-11 02:57:03+00:00
- **Authors**: Huihong Zhang, Jianlong Yang, Kang Zhou, Zhenjie Chai, Jun Cheng, Shenghua Gao, Jiang Liu
- **Comment**: This paper has been cast for ISBI 2020
- **Journal**: None
- **Summary**: Choroid is the vascular layer of the eye, which is directly related to the incidence and severity of many ocular diseases. Optical Coherence Tomography (OCT) is capable of imaging both the cross-sectional view of retina and choroid, but the segmentation of the choroid region is challenging because of the fuzzy choroid-sclera interface (CSI). In this paper, we propose a biomarker infused global-to-local network (BioNet) for choroid segmentation, which segments the choroid with higher credibility and robustness. Firstly, our method trains a biomarker prediction network to learn the features of the biomarker. Then a global multi-layers segmentation module is applied to segment the OCT image into 12 layers. Finally, the global multi-layered result and the original OCT image are fed into a local choroid segmentation module to segment the choroid region with the biomarker infused as regularizer. We conducted comparison experiments with the state-of-the-art methods on a dataset (named AROD). The experimental results demonstrate the superiority of our method with $90.77\%$ Dice-index and 6.23 pixels Average-unsigned-surface-detection-error, etc.



### Associative Alignment for Few-shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.05094v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05094v3)
- **Published**: 2019-12-11 03:14:48+00:00
- **Updated**: 2020-08-04 16:09:17+00:00
- **Authors**: Arman Afrasiyabi, Jean-François Lalonde, Christian Gagné
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot image classification aims at training a model from only a few examples for each of the "novel" classes. This paper proposes the idea of associative alignment for leveraging part of the base data by aligning the novel training instances to the closely related ones in the base training set. This expands the size of the effective novel training set by adding extra "related base" instances to the few novel ones, thereby allowing a constructive fine-tuning. We propose two associative alignment strategies: 1) a metric-learning loss for minimizing the distance between related base samples and the centroid of novel instances in the feature space, and 2) a conditional adversarial alignment loss based on the Wasserstein distance. Experiments on four standard datasets and three backbones demonstrate that combining our centroid-based alignment loss results in absolute accuracy improvements of 4.4%, 1.2%, and 6.2% in 5-shot learning over the state of the art for object recognition, fine-grained classification, and cross-domain adaptation, respectively.



### Bottleneck detection by slope difference distribution: a robust approach for separating overlapped cells
- **Arxiv ID**: http://arxiv.org/abs/1912.05096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05096v1)
- **Published**: 2019-12-11 03:16:01+00:00
- **Updated**: 2019-12-11 03:16:01+00:00
- **Authors**: ZhenZhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: To separate the overlapped cells, a bottleneck detection approach is proposed in this paper. The cell image is segmented by slope difference distribution (SDD) threshold selection. For each segmented binary clump, its one-dimensional boundary is computed as the distance distribution between its centroid and each point on the two-dimensional boundary. The bottleneck points of the one-dimensional boundary is detected by SDD and then transformed back into two-dimensional bottleneck points. Two largest concave parts of the binary clump are used to select the valid bottleneck points. Two bottleneck points from different concave parts with the minimum Euclidean distance is connected to separate the binary clump with minimum-cut. The binary clumps are separated iteratively until the number of computed concave parts is smaller than two. We use four types of open-accessible cell datasets to verify the effectiveness of the proposed approach and experimental results showed that the proposed approach is significantly more robust than state of the art methods.



### Deep Direct Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1912.05101v3
- **DOI**: 10.1109/TITS.2021.3071886
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05101v3)
- **Published**: 2019-12-11 03:22:03+00:00
- **Updated**: 2021-04-10 06:53:17+00:00
- **Authors**: Chaoqiang Zhao, Yang Tang, Qiyu Sun, Athanasios V. Vasilakos
- **Comment**: 10 pages,8 figures
- **Journal**: None
- **Summary**: Traditional monocular direct visual odometry (DVO) is one of the most famous methods to estimate the ego-motion of robots and map environments from images simultaneously. However, DVO heavily relies on high-quality images and accurate initial pose estimation during tracking. With the outstanding performance of deep learning, previous works have shown that deep neural networks can effectively learn 6-DoF (Degree of Freedom) poses between frames from monocular image sequences in the unsupervised manner. However, these unsupervised deep learning-based frameworks cannot accurately generate the full trajectory of a long monocular video because of the scale-inconsistency between each pose. To address this problem, we use several geometric constraints to improve the scale-consistency of the pose network, including improving the previous loss function and proposing a novel scale-to-trajectory constraint for unsupervised training. We call the pose network trained by the proposed novel constraint as TrajNet. In addition, a new DVO architecture, called deep direct sparse odometry (DDSO), is proposed to overcome the drawbacks of the previous direct sparse odometry (DSO) framework by embedding TrajNet. Extensive experiments on the KITTI dataset show that the proposed constraints can effectively improve the scale-consistency of TrajNet when compared with previous unsupervised monocular methods, and integration with TrajNet makes the initialization and tracking of DSO more robust and accurate.



### PuckNet: Estimating hockey puck location from broadcast video
- **Arxiv ID**: http://arxiv.org/abs/1912.05107v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05107v3)
- **Published**: 2019-12-11 03:52:37+00:00
- **Updated**: 2021-03-18 03:42:20+00:00
- **Authors**: Kanav Vats, William McNally, Chris Dulhanty, Zhong Qiu Lin, David A. Clausi, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Puck location in ice hockey is essential for hockey analysts for determining the location of play and analyzing game events. However, because of the difficulty involved in obtaining accurate annotations due to the extremely low visibility and commonly occurring occlusions of the puck, the problem is very challenging. The problem becomes even more challenging in broadcast videos with changing camera angles. We introduce a novel methodology for determining puck location from approximate puck location annotations in broadcast video. Our method uniquely leverages the existing puck location information that is publicly available in existing hockey event data and uses the corresponding one-second broadcast video clips as input to the network. The rationale behind using video as input instead of static images is that with video, the temporal information can be utilized to handle puck occlusions. The network outputs a heatmap representing the probability of the puck location using a 3D CNN based architecture. The network is able to regress the puck location from broadcast hockey video clips with varying camera angles. Experimental results demonstrate the capability of the method, achieving 47.07% AUC on the test dataset. The network is also able to estimate the puck location in defensive/offensive zones with an accuracy of greater than 80%.



### DeepMeshFlow: Content Adaptive Mesh Deformation for Robust Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1912.05131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05131v1)
- **Published**: 2019-12-11 06:03:07+00:00
- **Updated**: 2019-12-11 06:03:07+00:00
- **Authors**: Nianjin Ye, Chuan Wang, Shuaicheng Liu, Lanpeng Jia, Jue Wang, Yongqing Cui
- **Comment**: 9 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1909.05983
- **Journal**: None
- **Summary**: Image alignment by mesh warps, such as meshflow, is a fundamental task which has been widely applied in various vision applications(e.g., multi-frame HDR/denoising, video stabilization). Traditional mesh warp methods detect and match image features, where the quality of alignment highly depends on the quality of image features. However, the image features are not robust in occurrence of low-texture and low-light scenes. Deep homography methods, on the other hand, are free from such problem by learning deep features for robust performance. However, a homography is limited to plane motions. In this work, we present a deep meshflow motion model, which takes two images as input and output a sparse motion field with motions located at mesh vertexes. The deep meshflow enjoys the merics of meshflow that can describe nonlinear motions while also shares advantage of deep homography that is robust against challenging textureless scenarios. In particular, a new unsupervised network structure is presented with content-adaptive capability. On one hand, the image content that cannot be aligned under mesh representation are rejected by our learned mask, similar to the RANSAC procedure. On the other hand, we learn multiple mesh resolutions, combining to a non-uniform mesh division. Moreover, a comprehensive dataset is presented, covering various scenes for training and testing. The comparison between both traditional mesh warp methods and deep based methods show the effectiveness of our deep meshflow motion model.



### Callisto: Entropy based test generation and data quality assessment for Machine Learning Systems
- **Arxiv ID**: http://arxiv.org/abs/1912.08920v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.08920v1)
- **Published**: 2019-12-11 06:20:18+00:00
- **Updated**: 2019-12-11 06:20:18+00:00
- **Authors**: Sakshi Udeshi, Xingbin Jiang, Sudipta Chattopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Machine Learning (ML) has seen massive progress in the last decade and as a result, there is a pressing need for validating ML-based systems. To this end, we propose, design and evaluate CALLISTO - a novel test generation and data quality assessment framework. To the best of our knowledge, CALLISTO is the first blackbox framework to leverage the uncertainty in the prediction and systematically generate new test cases for ML classifiers. Our evaluation of CALLISTO on four real world data sets reveals thousands of errors. We also show that leveraging the uncertainty in prediction can increase the number of erroneous test cases up to a factor of 20, as compared to when no such knowledge is used for testing.   CALLISTO has the capability to detect low quality data in the datasets that may contain mislabelled data. We conduct and present an extensive user study to validate the results of CALLISTO on identifying low quality data from four state-of-the-art real world datasets.



### Vectorizing World Buildings: Planar Graph Reconstruction by Primitive Detection and Relationship Inference
- **Arxiv ID**: http://arxiv.org/abs/1912.05135v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05135v3)
- **Published**: 2019-12-11 06:23:26+00:00
- **Updated**: 2020-03-14 22:38:43+00:00
- **Authors**: Nelson Nauata, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: This paper tackles a 2D architecture vectorization problem, whose task is to infer an outdoor building architecture as a 2D planar graph from a single RGB image. We provide a new benchmark with ground-truth annotations for 2,001 complex buildings across the cities of Atlanta, Paris, and Las Vegas. We also propose a novel algorithm utilizing 1) convolutional neural networks (CNNs) that detects geometric primitives and infers their relationships and 2) an integer programming (IP) that assembles the information into a 2D planar graph. While being a trivial task for human vision, the inference of a graph structure with an arbitrary topology is still an open problem for computer vision. Qualitative and quantitative evaluations demonstrate that our algorithm makes significant improvements over the current state-of-the-art, towards an intelligent system at the level of human perception. We will share code and data.



### Lifelong learning for text retrieval and recognition in historical handwritten document collections
- **Arxiv ID**: http://arxiv.org/abs/1912.05156v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05156v1)
- **Published**: 2019-12-11 07:56:31+00:00
- **Updated**: 2019-12-11 07:56:31+00:00
- **Authors**: Lambert Schomaker
- **Comment**: To appear as chapter in book: Handwritten Historical Document
  Analysis, Recognition, and Retrieval -- State of the Art and Future Trends,
  in the book series: Series in Machine Perception and Artificial Intelligence
  World Scientific, ISSN (print): 1793-0839 Original version deposited at
  Zenodo: https://zenodo.org/record/2346885#.XfCfsq5ytpg on December 17, 2018
- **Journal**: None
- **Summary**: This chapter provides an overview of the problems that need to be dealt with when constructing a lifelong-learning retrieval, recognition and indexing engine for large historical document collections in multiple scripts and languages, the Monk system. This application is highly variable over time, since the continuous labeling by end users changes the concept of what a 'ground truth' constitutes. Although current advances in deep learning provide a huge potential in this application domain, the scale of the problem, i.e., more than 520 hugely diverse books, documents and manuscripts precludes the current meticulous and painstaking human effort which is required in designing and developing successful deep-learning systems. The ball-park principle is introduced, which describes the evolution from the sparsely-labeled stage that can only be addressed by traditional methods or nearest-neighbor methods on embedded vectors of pre-trained neural networks, up to the other end of the spectrum where massive labeling allows reliable training of deep-learning methods. Contents: Introduction, Expectation management, Deep learning, The ball-park principle, Technical realization, Work flow, Quality and quantity of material, Industrialization and scalability, Human effort, Algorithms, Object of recognition, Processing pipeline, Performance,Compositionality, Conclusion.



### Graph-based Multi-view Binary Learning for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1912.05159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05159v1)
- **Published**: 2019-12-11 08:04:56+00:00
- **Updated**: 2019-12-11 08:04:56+00:00
- **Authors**: Guangqi Jiang, Huibing Wang, Jinjia Peng, Dongyan Chen, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing techniques, also known as binary code learning, have recently gained increasing attention in large-scale data analysis and storage. Generally, most existing hash clustering methods are single-view ones, which lack complete structure or complementary information from multiple views. For cluster tasks, abundant prior researches mainly focus on learning discrete hash code while few works take original data structure into consideration. To address these problems, we propose a novel binary code algorithm for clustering, which adopts graph embedding to preserve the original data structure, called (Graph-based Multi-view Binary Learning) GMBL in this paper. GMBL mainly focuses on encoding the information of multiple views into a compact binary code, which explores complementary information from multiple views. In particular, in order to maintain the graph-based structure of the original data, we adopt a Laplacian matrix to preserve the local linear relationship of the data and map it to the Hamming space. Considering different views have distinctive contributions to the final clustering results, GMBL adopts a strategy of automatically assign weights for each view to better guide the clustering. Finally, An alternating iterative optimization method is adopted to optimize discrete binary codes directly instead of relaxing the binary constraint in two steps. Experiments on five public datasets demonstrate the superiority of our proposed method compared with previous approaches in terms of clustering performance.



### TANet: Robust 3D Object Detection from Point Clouds with Triple Attention
- **Arxiv ID**: http://arxiv.org/abs/1912.05163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05163v1)
- **Published**: 2019-12-11 08:13:36+00:00
- **Updated**: 2019-12-11 08:13:36+00:00
- **Authors**: Zhe Liu, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, Xiang Bai
- **Comment**: AAAI 2020(Oral)
- **Journal**: None
- **Summary**: In this paper, we focus on exploring the robustness of the 3D object detection in point clouds, which has been rarely discussed in existing approaches. We observe two crucial phenomena: 1) the detection accuracy of the hard objects, e.g., Pedestrians, is unsatisfactory, 2) when adding additional noise points, the performance of existing approaches decreases rapidly. To alleviate these problems, a novel TANet is introduced in this paper, which mainly contains a Triple Attention (TA) module, and a Coarse-to-Fine Regression (CFR) module. By considering the channel-wise, point-wise and voxel-wise attention jointly, the TA module enhances the crucial information of the target while suppresses the unstable cloud points. Besides, the novel stacked TA further exploits the multi-level feature attention. In addition, the CFR module boosts the accuracy of localization without excessive computation cost. Experimental results on the validation set of KITTI dataset demonstrate that, in the challenging noisy cases, i.e., adding additional random noisy points around each object,the presented approach goes far beyond state-of-the-art approaches. Furthermore, for the 3D object detection task of the KITTI benchmark, our approach ranks the first place on Pedestrian class, by using the point clouds as the only input. The running speed is around 29 frames per second.



### Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1912.05170v3
- **DOI**: 10.1016/j.knosys.2021.106771
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05170v3)
- **Published**: 2019-12-11 08:26:57+00:00
- **Updated**: 2021-01-11 08:50:51+00:00
- **Authors**: Görkem Algan, Ilkay Ulusoy
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.



### BINet: a binary inpainting network for deep patch-based image compression
- **Arxiv ID**: http://arxiv.org/abs/1912.05189v2
- **DOI**: 10.1016/j.image.2020.116119
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05189v2)
- **Published**: 2019-12-11 09:11:01+00:00
- **Updated**: 2021-01-13 16:33:53+00:00
- **Authors**: André Nortje, Willie Brink, Herman A. Engelbrecht, Herman Kamper
- **Comment**: Signal Processing: Image Communication
- **Journal**: Signal Processing: Image Communication 92C (2021) 116119
- **Summary**: Recent deep learning models outperform standard lossy image compression codecs. However, applying these models on a patch-by-patch basis requires that each image patch be encoded and decoded independently. The influence from adjacent patches is therefore lost, leading to block artefacts at low bitrates. We propose the Binary Inpainting Network (BINet), an autoencoder framework which incorporates binary inpainting to reinstate interdependencies between adjacent patches, for improved patch-based compression of still images. When decoding a patch, BINet additionally uses the binarised encodings from surrounding patches to guide its reconstruction. In contrast to sequential inpainting methods where patches are decoded based on previons reconstructions, BINet operates directly on the binary codes of surrounding patches without access to the original or reconstructed image data. Encoding and decoding can therefore be performed in parallel. We demonstrate that BINet improves the compression quality of a competitive deep image codec across a range of compression levels.



### IoU-uniform R-CNN: Breaking Through the Limitations of RPN
- **Arxiv ID**: http://arxiv.org/abs/1912.05190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05190v1)
- **Published**: 2019-12-11 09:13:02+00:00
- **Updated**: 2019-12-11 09:13:02+00:00
- **Authors**: Li Zhu, Zihao Xie, Liman Liu, Bo Tao, Wenbing Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Region Proposal Network (RPN) is the cornerstone of two-stage object detectors, it generates a sparse set of object proposals and alleviates the extrem foregroundbackground class imbalance problem during training. However, we find that the potential of the detector has not been fully exploited due to the IoU distribution imbalance and inadequate quantity of the training samples generated by RPN. With the increasing intersection over union (IoU), the exponentially smaller numbers of positive samples would lead to the distribution skewed towards lower IoUs, which hinders the optimization of detector at high IoU levels. In this paper, to break through the limitations of RPN, we propose IoU-Uniform R-CNN, a simple but effective method that directly generates training samples with uniform IoU distribution for the regression branch as well as the IoU prediction branch. Besides, we improve the performance of IoU prediction branch by eliminating the feature offsets of RoIs at inference, which helps the NMS procedure by preserving accurately localized bounding box. Extensive experiments on the PASCAL VOC and MS COCO dataset show the effectiveness of our method, as well as its compatibility and adaptivity to many object detection architectures. The code is made publicly available at https://github.com/zl1994/IoU-Uniform-R-CNN,



### Deep motion estimation for parallel inter-frame prediction in video compression
- **Arxiv ID**: http://arxiv.org/abs/1912.05193v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05193v1)
- **Published**: 2019-12-11 09:16:17+00:00
- **Updated**: 2019-12-11 09:16:17+00:00
- **Authors**: André Nortje, Herman A. Engelbrecht, Herman Kamper
- **Comment**: 25 pages, 11 figures, 5 tables
- **Journal**: None
- **Summary**: Standard video codecs rely on optical flow to guide inter-frame prediction: pixels from reference frames are moved via motion vectors to predict target video frames. We propose to learn binary motion codes that are encoded based on an input video sequence. These codes are not limited to 2D translations, but can capture complex motion (warping, rotation and occlusion). Our motion codes are learned as part of a single neural network which also learns to compress and decode them. This approach supports parallel video frame decoding instead of the sequential motion estimation and compensation of flow-based methods. We also introduce 3D dynamic bit assignment to adapt to object displacements caused by motion, yielding additional bit savings. By replacing the optical flow-based block-motion algorithms found in an existing video codec with our learned inter-frame prediction model, our approach outperforms the standard H.264 and H.265 video codecs across at low bitrates.



### Discriminative Autoencoder for Feature Extraction: Application to Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.12131v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12131v1)
- **Published**: 2019-12-11 10:12:22+00:00
- **Updated**: 2019-12-11 10:12:22+00:00
- **Authors**: Anupriya Gogna, Angshul Majumdar
- **Comment**: The final version has been accepted at Neural Processing Letters
- **Journal**: None
- **Summary**: Conventionally, autoencoders are unsupervised representation learning tools. In this work, we propose a novel discriminative autoencoder. Use of supervised discriminative learning ensures that the learned representation is robust to variations commonly encountered in image datasets. Using the basic discriminating autoencoder as a unit, we build a stacked architecture aimed at extracting relevant representation from the training data. The efficiency of our feature extraction algorithm ensures a high classification accuracy with even simple classification schemes like KNN (K-nearest neighbor). We demonstrate the superiority of our model for representation learning by conducting experiments on standard datasets for character/image recognition and subsequent comparison with existing supervised deep architectures like class sparse stacked autoencoder and discriminative deep belief network.



### Lane Detection For Prototype Autonomous Vehicle
- **Arxiv ID**: http://arxiv.org/abs/1912.05220v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.05220v1)
- **Published**: 2019-12-11 10:27:45+00:00
- **Updated**: 2019-12-11 10:27:45+00:00
- **Authors**: Sertap Kamçı, Dogukan Aksu, Muhammed Ali Aydin
- **Comment**: The paper was presented in the 6th International Conference on
  Signal, Image Processing and Multimedia SPM 2019 in Zurich, Switzerland
- **Journal**: None
- **Summary**: Unmanned vehicle technologies are an area of great interest in theory and practice today. These technologies have advanced considerably after the first applications have been implemented and cause a rapid change in human life. Autonomous vehicles are also a big part of these technologies. The most important action of a driver has to do is to follow the lanes on the way to the destination. By using image processing and artificial intelligence techniques, an autonomous vehicle can move successfully without a driver help. They can go from the initial point to the specified target by applying pre-defined rules. There are also rules for proper tracking of the lanes. Many accidents are caused due to insufficient follow-up of the lanes and non-compliance with these rules. The majority of these accidents also result in injury and death.   In this paper, we present an autonomous vehicle prototype that follows lanes via image processing techniques, which are a major part of autonomous vehicle technology. Autonomous movement capability is provided by using some image processing algorithms such as canny edge detection, Sobel filter, etc. We implemented and tested these algorithms on the vehicle. The vehicle detected and followed the determined lanes. By that way, it went to the destination successfully.



### Automatic Analysis of Sewer Pipes Based on Unrolled Monocular Fisheye Images
- **Arxiv ID**: http://arxiv.org/abs/1912.05222v1
- **DOI**: 10.1109/WACV.2018.00223
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05222v1)
- **Published**: 2019-12-11 10:32:35+00:00
- **Updated**: 2019-12-11 10:32:35+00:00
- **Authors**: Johannes Künzel, Thomas Werner, Ronja Möller, Peter Eisert, Jan Waschnewski, Ralf Hilpert
- **Comment**: Published in: 2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)
- **Journal**: J. Kunzel, T. Werner, P. Eisert and J. Waschnewski, "Automatic
  Analysis of Sewer Pipes Based on Unrolled Monocular Fisheye Images," 2018
  IEEE Winter Conference on Applications of Computer Vision (WACV), Lake Tahoe,
  NV, 2018, pp. 2019-2027
- **Summary**: The task of detecting and classifying damages in sewer pipes offers an important application area for computer vision algorithms. This paper describes a system, which is capable of accomplishing this task solely based on low quality and severely compressed fisheye images from a pipe inspection robot. Relying on robust image features, we estimate camera poses, model the image lighting, and exploit this information to generate high quality cylindrical unwraps of the pipes' surfaces.Based on the generated images, we apply semantic labeling based on deep convolutional neural networks to detect and classify defects as well as structural elements.



### Kernel Transform Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.12129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.12129v1)
- **Published**: 2019-12-11 10:55:38+00:00
- **Updated**: 2019-12-11 10:55:38+00:00
- **Authors**: Jyoti Maggu, Angshul Majumdar
- **Comment**: None
- **Journal**: Pattern Recognition Letters, 98, pp.117-122 (2017)
- **Summary**: This work proposes kernel transform learning. The idea of dictionary learning is well known; it is a synthesis formulation where a basis is learnt along with the coefficients so as to generate or synthesize the data. Transform learning is its analysis equivalent; the transforms operates or analyses on the data to generate the coefficients. The concept of kernel dictionary learning has been introduced in the recent past, where the dictionary is represented as a linear combination of non-linear version of the data. Its success has been showcased in feature extraction. In this work we propose to kernelize transform learning on line similar to kernel dictionary learning. An efficient solution for kernel transform learning has been proposed especially for problems where the number of samples is much larger than the dimensionality of the input samples making the kernel matrix very high dimensional. Kernel transform learning has been compared with other representation learning tools like autoencoder, restricted Boltzmann machine as well as with dictionary learning (and its kernelized version). Our proposed kernel transform learning yields better results than all the aforesaid techniques; experiments have been carried out on benchmark databases.



### Discriminative Robust Deep Dictionary Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.10803v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.10803v1)
- **Published**: 2019-12-11 10:59:25+00:00
- **Updated**: 2019-12-11 10:59:25+00:00
- **Authors**: Vanika Singhal, Hemant K. Aggarwal, Snigdha Tariyal, Angshul Majumdar
- **Comment**: Final version accepted at IEEE Transactions on Geosciences and Remote
  Sensing
- **Journal**: None
- **Summary**: This work proposes a new framework for deep learning that has been particularly tailored for hyperspectral image classification. We learn multiple levels of dictionaries in a robust fashion. The last layer is discriminative that learns a linear classifier. The training proceeds greedily, at a time a single level of dictionary is learnt and the coefficients used to train the next level. The coefficients from the final level are used for classification. Robustness is incorporated by minimizing the absolute deviations instead of the more popular Euclidean norm. The inbuilt robustness helps combat mixed noise (Gaussian and sparse) present in hyperspectral images. Results show that our proposed techniques outperforms all other deep learning methods Deep Belief Network (DBN), Stacked Autoencoder (SAE) and Convolutional Neural Network (CNN). The experiments have been carried out on benchmark hyperspectral imaging datasets.



### HistoNet: Predicting size histograms of object instances
- **Arxiv ID**: http://arxiv.org/abs/1912.05227v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05227v3)
- **Published**: 2019-12-11 11:01:35+00:00
- **Updated**: 2020-06-04 08:32:37+00:00
- **Authors**: Kishan Sharma, Moritz Gold, Christian Zurbruegg, Laura Leal-Taixé, Jan Dirk Wegner
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to predict histograms of object sizes in crowded scenes directly without any explicit object instance segmentation. What makes this task challenging is the high density of objects (of the same category), which makes instance identification hard. Instead of explicitly segmenting object instances, we show that directly learning histograms of object sizes improves accuracy while using drastically less parameters. This is very useful for application scenarios where explicit, pixel-accurate instance segmentation is not needed, but there lies interest in the overall distribution of instance sizes. Our core applications are in biology, where we estimate the size distribution of soldier fly larvae, and medicine, where we estimate the size distribution of cancer cells as an intermediate step to calculate the tumor cellularity score. Given an image with hundreds of small object instances, we output the total count and the size histogram. We also provide a new data set for this task, the FlyLarvae data set, which consists of 11,000 larvae instances labeled pixel-wise. Our method results in an overall improvement in the count and size distribution prediction as compared to state-of-the-art instance segmentation method Mask R-CNN.



### Boundary-Aware Salient Object Detection via Recurrent Two-Stream Guided Refinement Network
- **Arxiv ID**: http://arxiv.org/abs/1912.05236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05236v1)
- **Published**: 2019-12-11 11:19:31+00:00
- **Updated**: 2019-12-11 11:19:31+00:00
- **Authors**: Fangting Lin, Chao Yang, Huizhou Li, Bin Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Recent deep learning based salient object detection methods which utilize both saliency and boundary features have achieved remarkable performance. However, most of them ignore the complementarity between saliency features and boundary features, thus get worse predictions in scenes with low contrast between foreground and background. To address this issue, we propose a novel Recurrent Two-Stream Guided Refinement Network (RTGRNet) that consists of iterating Two-Stream Guided Refinement Modules (TGRMs). TGRM consists of a Guide Block and two feature streams: saliency and boundary, the Guide Block utilizes the refined features after previous TGRM to further improve the performance of two feature streams in current TGRM. Meanwhile, the low-level integrated features are also utilized as a reference to get better details. Finally, we progressively refine these features by recurrently stacking more TGRMs. Extensive experiments on six public datasets show that our proposed RTGRNet achieves the state-of-the-art performance in salient object detection.



### Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1912.05237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05237v2)
- **Published**: 2019-12-11 11:20:36+00:00
- **Updated**: 2020-03-24 12:24:56+00:00
- **Authors**: Yiyi Liao, Katja Schwarz, Lars Mescheder, Andreas Geiger
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.



### Deep Learning-based Denoising of Mammographic Images using Physics-driven Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.05240v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05240v1)
- **Published**: 2019-12-11 11:32:33+00:00
- **Updated**: 2019-12-11 11:32:33+00:00
- **Authors**: Dominik Eckert, Sulaiman Vesal, Ludwig Ritschl, Steffen Kappler, Andreas Maier
- **Comment**: Accepted at BVM 2020
- **Journal**: None
- **Summary**: Mammography is using low-energy X-rays to screen the human breast and is utilized by radiologists to detect breast cancer. Typically radiologists require a mammogram with impeccable image quality for an accurate diagnosis. In this study, we propose a deep learning method based on Convolutional Neural Networks (CNNs) for mammogram denoising to improve the image quality. We first enhance the noise level and employ Anscombe Transformation (AT) to transform Poisson noise to white Gaussian noise. With this data augmentation, a deep residual network is trained to learn the noise map of the noisy images. We show, that the proposed method can remove not only simulated but also real noise. Furthermore, we also compare our results with state-of-the-art denoising methods, such as BM3D and DNCNN. In an early investigation, we achieved qualitatively better mammogram denoising results.



### Automatic quality assessment for 2D fetal sonographic standard plane based on multi-task learning
- **Arxiv ID**: http://arxiv.org/abs/1912.05260v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05260v1)
- **Published**: 2019-12-11 12:29:00+00:00
- **Updated**: 2019-12-11 12:29:00+00:00
- **Authors**: Hong Luo, Han Liu, Kejun Li, Bo Zhang
- **Comment**: 14 pages, 10 figures, this paper is submitted to "Medical Image
  Analysis"
- **Journal**: None
- **Summary**: The quality control of fetal sonographic (FS) images is essential for the correct biometric measurements and fetal anomaly diagnosis. However, quality control requires professional sonographers to perform and is often labor-intensive. To solve this problem, we propose an automatic image quality assessment scheme based on multi-task learning to assist in FS image quality control. An essential criterion for FS image quality control is that all the essential anatomical structures in the section should appear full and remarkable with a clear boundary. Therefore, our scheme aims to identify those essential anatomical structures to judge whether an FS image is the standard image, which is achieved by three convolutional neural networks. The Feature Extraction Network aims to extract deep level features of FS images. Based on the extracted features, the Class Prediction Network determines whether the structure meets the standard and Region Proposal Network identifies its position. The scheme has been applied to three types of fetal sections, which are the head, abdominal, and heart. The experimental results show that our method can make a quality assessment of an FS image within less a second. Also, our method achieves competitive performance in both the detection and classification compared with state-of-the-art methods.



### MineGAN: effective knowledge transfer from GANs to target domains with few images
- **Arxiv ID**: http://arxiv.org/abs/1912.05270v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05270v3)
- **Published**: 2019-12-11 12:43:01+00:00
- **Updated**: 2020-04-02 09:14:10+00:00
- **Authors**: Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, Joost van de Weijer
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: One of the attractive characteristics of deep neural networks is their ability to transfer knowledge obtained in one domain to other related domains. As a result, high-quality networks can be trained in domains with relatively little training data. This property has been extensively studied for discriminative networks but has received significantly less attention for generative models. Given the often enormous effort required to train GANs, both computationally as well as in the dataset collection, the re-use of pretrained GANs is a desirable objective. We propose a novel knowledge transfer method for generative models based on mining the knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs. This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain. Mining effectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods such as mode collapse and lack of flexibility. We perform experiments on several complex datasets using various GAN architectures (BigGAN, Progressive GAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods. In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs. Our code is available at: https://github.com/yaxingwang/MineGAN.



### Interactive Multi-Dimension Modulation with Dynamic Controllable Residual Learning for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1912.05293v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05293v2)
- **Published**: 2019-12-11 13:38:01+00:00
- **Updated**: 2020-09-22 08:16:47+00:00
- **Authors**: Jingwen He, Chao Dong, Yu Qiao
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Interactive image restoration aims to generate restored images by adjusting a controlling coefficient which determines the restoration level. Previous works are restricted in modulating image with a single coefficient. However, real images always contain multiple types of degradation, which cannot be well determined by one coefficient. To make a step forward, this paper presents a new problem setup, called multi-dimension (MD) modulation, which aims at modulating output effects across multiple degradation types and levels. Compared with the previous single-dimension (SD) modulation, the MD is setup to handle multiple degradations adaptively and relief unbalanced learning problem in different degradations. We also propose a deep architecture - CResMD with newly introduced controllable residual connections for multi-dimension modulation. Specifically, we add a controlling variable on the conventional residual connection to allow a weighted summation of input and residual. The values of these weights are generated by another condition network. We further propose a new data sampling strategy based on beta distribution to balance different degradation types and levels. With corrupted image and degradation information as inputs, the network can output the corresponding restored image. By tweaking the condition vector, users can control the output effects in MD space at test time. Extensive experiments demonstrate that the proposed CResMD achieve excellent performance on both SD and MD modulation tasks. Code is available at https://github.com/hejingwenhejingwen/CResMD.



### Bipartite Conditional Random Fields for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.05307v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05307v2)
- **Published**: 2019-12-11 13:50:36+00:00
- **Updated**: 2020-08-22 02:18:27+00:00
- **Authors**: Sadeep Jayasumana, Kanchana Ranasinghe, Mayuka Jayawardhana, Sahan Liyanaarachchi, Harsha Ranasinghe
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the panoptic segmentation problem with a conditional random field (CRF) model. Panoptic segmentation involves assigning a semantic label and an instance label to each pixel of a given image. At each pixel, the semantic label and the instance label should be compatible. Furthermore, a good panoptic segmentation should have a number of other desirable properties such as the spatial and color consistency of the labeling (similar looking neighboring pixels should have the same semantic label and the instance label). To tackle this problem, we propose a CRF model, named Bipartite CRF or BCRF, with two types of random variables for semantic and instance labels. In this formulation, various energies are defined within and across the two types of random variables to encourage a consistent panoptic segmentation. We propose a mean-field-based efficient inference algorithm for solving the CRF and empirically show its convergence properties. This algorithm is fully differentiable, and therefore, BCRF inference can be included as a trainable module in a deep network. In the experimental evaluation, we quantitatively and qualitatively show that the BCRF yields superior panoptic segmentation results in practice.



### A Variational-Sequential Graph Autoencoder for Neural Architecture Performance Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.05317v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05317v2)
- **Published**: 2019-12-11 14:02:07+00:00
- **Updated**: 2020-08-26 09:50:48+00:00
- **Authors**: David Friede, Jovita Lukasik, Heiner Stuckenschmidt, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision research, the process of automating architecture engineering, Neural Architecture Search (NAS), has gained substantial interest. In the past, NAS was hardly accessible to researchers without access to large-scale compute systems, due to very long compute times for the recurrent search and evaluation of new candidate architectures. The NAS-Bench-101 dataset facilitates a paradigm change towards classical methods such as supervised learning to evaluate neural architectures. In this paper, we propose a graph encoder built upon Graph Neural Networks (GNN). We demonstrate the effectiveness of the proposed encoder on NAS performance prediction for seen architecture types as well an unseen ones (i.e., zero shot prediction). We also provide a new variational-sequential graph autoencoder (VS-GAE) based on the proposed graph encoder. The VS-GAE is specialized on encoding and decoding graphs of varying length utilizing GNNs. Experiments on different sampling methods show that the embedding space learned by our VS-GAE increases the stability on the accuracy prediction task.



### An Efficient Approach for Using Expectation Maximization Algorithm in Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.05333v3
- **DOI**: 10.1109/MVIP49855.2020.9116870
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05333v3)
- **Published**: 2019-12-11 14:13:15+00:00
- **Updated**: 2020-07-31 13:10:16+00:00
- **Authors**: Moein Hasani, Amin Nasim Saravi, Hassan Khotanlou
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule Networks (CapsNets) are brand-new architectures that have shown ground-breaking results in certain areas of Computer Vision (CV). In 2017, Hinton and his team introduced CapsNets with routing-by-agreement in "Sabour et al" and in a more recent paper "Matrix Capsules with EM Routing" they proposed a more complete architecture with Expectation-Maximization (EM) algorithm. Unlike the traditional convolutional neural networks (CNNs), this architecture is able to preserve the pose of the objects in the picture. Due to this characteristic, it has been able to beat the previous state-of-theart results on the smallNORB dataset, which includes samples with various view points. Also, this architecture is more robust to white box adversarial attacks. However, CapsNets have two major drawbacks. They can't perform as well as CNNs on complex datasets and, they need a huge amount of time for training. We try to mitigate these shortcomings by finding optimum settings of EM routing iterations for training CapsNets. Unlike the past studies, we use un-equal numbers of EM routing iterations for different stages of the CapsNet. For our research, we use three datasets: Yale face dataset, Belgium Traffic Sign dataset, and Fashion-MNIST dataset.



### Feeding the zombies: Synthesizing brain volumes using a 3D progressive growing GAN
- **Arxiv ID**: http://arxiv.org/abs/1912.05357v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05357v2)
- **Published**: 2019-12-11 14:35:37+00:00
- **Updated**: 2020-01-24 15:59:48+00:00
- **Authors**: Anders Eklund
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning requires large datasets for training (convolutional) networks with millions of parameters. In neuroimaging, there are few open datasets with more than 100 subjects, which makes it difficult to, for example, train a classifier to discriminate controls from diseased persons. Generative adversarial networks (GANs) can be used to synthesize data, but virtually all research is focused on 2D images. In medical imaging, and especially in neuroimaging, most datasets are 3D or 4D. Here we therefore present preliminary results showing that a 3D progressive growing GAN can be used to synthesize MR brain volumes.



### MAGSAC++, a fast, reliable and accurate robust estimator
- **Arxiv ID**: http://arxiv.org/abs/1912.05909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05909v1)
- **Published**: 2019-12-11 14:39:33+00:00
- **Updated**: 2019-12-11 14:39:33+00:00
- **Authors**: Daniel Barath, Jana Noskova, Maksym Ivashechkin, Jiri Matas
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1906.02295
- **Journal**: None
- **Summary**: A new method for robust estimation, MAGSAC++, is proposed. It introduces a new model quality (scoring) function that does not require the inlier-outlier decision, and a novel marginalization procedure formulated as an iteratively re-weighted least-squares approach. We also propose a new sampler, Progressive NAPSAC, for RANSAC-like robust estimators. Exploiting the fact that nearby points often originate from the same model in real-world data, it finds local structures earlier than global samplers. The progressive transition from local to global sampling does not suffer from the weaknesses of purely localized samplers. On six publicly available real-world datasets for homography and fundamental matrix fitting, MAGSAC++ produces results superior to state-of-the-art robust methods. It is faster, more geometrically accurate and fails less often.



### Parting with Illusions about Deep Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1912.05361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05361v1)
- **Published**: 2019-12-11 14:43:18+00:00
- **Updated**: 2019-12-11 14:43:18+00:00
- **Authors**: Sudhanshu Mittal, Maxim Tatarchenko, Özgün Çiçek, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: Active learning aims to reduce the high labeling cost involved in training machine learning models on large datasets by efficiently labeling only the most informative samples. Recently, deep active learning has shown success on various tasks. However, the conventional evaluation scheme used for deep active learning is below par. Current methods disregard some apparent parallel work in the closely related fields. Active learning methods are quite sensitive w.r.t. changes in the training procedure like data augmentation. They improve by a large-margin when integrated with semi-supervised learning, but barely perform better than the random baseline. We re-implement various latest active learning approaches for image classification and evaluate them under more realistic settings. We further validate our findings for semantic segmentation. Based on our observations, we realistically assess the current state of the field and propose a more suitable evaluation protocol.



### AugFPN: Improving Multi-scale Feature Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1912.05384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05384v1)
- **Published**: 2019-12-11 15:19:59+00:00
- **Updated**: 2019-12-11 15:19:59+00:00
- **Authors**: Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art detectors typically exploit feature pyramid to detect objects at different scales. Among them, FPN is one of the representative works that build a feature pyramid by multi-scale features summation. However, the design defects behind prevent the multi-scale features from being fully exploited. In this paper, we begin by first analyzing the design defects of feature pyramid in FPN, and then introduce a new feature pyramid architecture named AugFPN to address these problems. Specifically, AugFPN consists of three components: Consistent Supervision, Residual Feature Augmentation, and Soft RoI Selection. AugFPN narrows the semantic gaps between features of different scales before feature fusion through Consistent Supervision. In feature fusion, ratio-invariant context information is extracted by Residual Feature Augmentation to reduce the information loss of feature map at the highest pyramid level. Finally, Soft RoI Selection is employed to learn a better RoI feature adaptively after feature fusion. By replacing FPN with AugFPN in Faster R-CNN, our models achieve 2.3 and 1.6 points higher Average Precision (AP) when using ResNet50 and MobileNet-v2 as backbone respectively. Furthermore, AugFPN improves RetinaNet by 1.6 points AP and FCOS by 0.9 points AP when using ResNet50 as backbone. Codes will be made available.



### Fine-grained Classification of Rowing teams
- **Arxiv ID**: http://arxiv.org/abs/1912.05393v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05393v1)
- **Published**: 2019-12-11 15:36:25+00:00
- **Updated**: 2019-12-11 15:36:25+00:00
- **Authors**: M. J. A. van Wezel, L. J. Hamburger, Y. Napolean
- **Comment**: 7 pages, NCCV 2019, 6 figures, deep learning, attention learning,
  CNN, rowing boat, team detector, club detector, data set, dataset
- **Journal**: None
- **Summary**: Fine-grained classification tasks such as identifying different breeds of dog are quite challenging as visual differences between categories is quite small and can be easily overwhelmed by external factors such as object pose, lighting, etc. This work focuses on the specific case of classifying rowing teams from various associations. Currently, the photos are taken at rowing competitions and are manually classified by a small set of members, in what is a painstaking process. To alleviate this, Deep learning models can be utilised as a faster method to classify the images. Recent studies show that localising the manually defined parts, and modelling based on these parts, improves on vanilla convolution models, so this work also investigates the detection of clothing attributes. The networks were trained and tested on a partially labelled data set mainly consisting of rowers from multiple associations. This paper resulted in the classification of up to ten rowing associations by using deep learning networks the smaller VGG network achieved 90.1\% accuracy whereas ResNet was limited to 87.20\%. Adding attention to the ResNet resulted into a drop of performance as only 78.10\% was achieved.



### Multimodal Self-Supervised Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1912.05396v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.05396v2)
- **Published**: 2019-12-11 15:44:00+00:00
- **Updated**: 2020-10-25 23:39:07+00:00
- **Authors**: Aiham Taleb, Christoph Lippert, Tassilo Klein, Moin Nabi
- **Comment**: NeurIPS 2019 Workshops
- **Journal**: None
- **Summary**: Self-supervised learning approaches leverage unlabeled samples to acquire generic knowledge about different concepts, hence allowing for annotation-efficient downstream task learning. In this paper, we propose a novel self-supervised method that leverages multiple imaging modalities. We introduce the multimodal puzzle task, which facilitates rich representation learning from multiple image modalities. The learned representations allow for subsequent fine-tuning on different downstream tasks. To achieve that, we learn a modality-agnostic feature embedding by confusing image modalities at the data-level. Together with the Sinkhorn operator, with which we formulate the puzzle solving optimization as permutation matrix inference instead of classification, they allow for efficient solving of multimodal puzzles with varying levels of complexity. In addition, we also propose to utilize cross-modal generation techniques for multimodal data augmentation used for training self-supervised tasks. In other words, we exploit synthetic images for self-supervised pretraining, instead of downstream tasks directly, in order to circumvent quality issues associated with synthetic images, while improving data-efficiency and representations quality. Our experimental results, which assess the gains in downstream performance and data-efficiency, show that solving our multimodal puzzles yields better semantic representations, compared to treating each modality independently. Our results also highlight the benefits of exploiting synthetic images for self-supervised pretraining. We showcase our approach on four downstream tasks: Brain tumor segmentation and survival days prediction using four MRI modalities, Prostate segmentation using two MRI modalities, and Liver segmentation using unregistered CT and MRI modalities. We outperform many previous solutions, and achieve results competitive to state-of-the-art.



### U-Net with spatial pyramid pooling for drusen segmentation in optical coherence tomography
- **Arxiv ID**: http://arxiv.org/abs/1912.05404v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05404v1)
- **Published**: 2019-12-11 16:00:03+00:00
- **Updated**: 2019-12-11 16:00:03+00:00
- **Authors**: Rhona Asgari, Sebastian Waldstein, Ferdinand Schlanitz, Magdalena Baratsits, Ursula Schmidt-Erfurth, Hrvoje Bogunović
- **Comment**: None
- **Journal**: None
- **Summary**: The presence of drusen is the main hallmark of early/intermediate age-related macular degeneration (AMD). Therefore, automated drusen segmentation is an important step in image-guided management of AMD. There are two common approaches to drusen segmentation. In the first, the drusen are segmented directly as a binary classification task. In the second approach, the surrounding retinal layers (outer boundary retinal pigment epithelium (OBRPE) and Bruch's membrane (BM)) are segmented and the remaining space between these two layers is extracted as drusen. In this work, we extend the standard U-Net architecture with spatial pyramid pooling components to introduce global feature context. We apply the model to the task of segmenting drusen together with BM and OBRPE. The proposed network was trained and evaluated on a longitudinal OCT dataset of 425 scans from 38 patients with early/intermediate AMD. This preliminary study showed that the proposed network consistently outperformed the standard U-net model.



### Training Deep SLAM on Single Frames
- **Arxiv ID**: http://arxiv.org/abs/1912.05405v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05405v1)
- **Published**: 2019-12-11 16:02:20+00:00
- **Updated**: 2019-12-11 16:02:20+00:00
- **Authors**: Igor Slinko, Anna Vorontsova, Dmitry Zhukov, Olga Barinova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.



### Photosequencing of Motion Blur using Short and Long Exposures
- **Arxiv ID**: http://arxiv.org/abs/1912.06102v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06102v1)
- **Published**: 2019-12-11 16:23:14+00:00
- **Updated**: 2019-12-11 16:23:14+00:00
- **Authors**: Vijay Rengarajan, Shuo Zhao, Ruiwen Zhen, John Glotzbach, Hamid Sheikh, Aswin C. Sankaranarayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Photosequencing aims to transform a motion blurred image to a sequence of sharp images. This problem is challenging due to the inherent ambiguities in temporal ordering as well as the recovery of lost spatial textures due to blur. Adopting a computational photography approach, we propose to capture two short exposure images, along with the original blurred long exposure image to aid in the aforementioned challenges. Post-capture, we recover the sharp photosequence using a novel blur decomposition strategy that recursively splits the long exposure image into smaller exposure intervals. We validate the approach by capturing a variety of scenes with interesting motions using machine vision cameras programmed to capture short and long exposure sequences. Our experimental results show that the proposed method resolves both fast and fine motions better than prior works.



### Self-Driving Car Steering Angle Prediction Based on Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.05440v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05440v1)
- **Published**: 2019-12-11 16:44:25+00:00
- **Updated**: 2019-12-11 16:44:25+00:00
- **Authors**: Shuyang Du, Haoli Guo, Andrew Simpson
- **Comment**: 9 pages 13 figures. Paper originally from CS231n (Stanford) 2017
- **Journal**: None
- **Summary**: Self-driving vehicles have expanded dramatically over the last few years. Udacity has release a dataset containing, among other data, a set of images with the steering angle captured during driving. The Udacity challenge aimed to predict steering angle based on only the provided images. We explore two different models to perform high quality prediction of steering angles based on images using different deep learning techniques including Transfer Learning, 3D CNN, LSTM and ResNet. If the Udacity challenge was still ongoing, both of our models would have placed in the top ten of all entries.



### $Σ$-net: Ensembled Iterative Deep Neural Networks for Accelerated Parallel MR Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1912.05480v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05480v1)
- **Published**: 2019-12-11 17:23:58+00:00
- **Updated**: 2019-12-11 17:23:58+00:00
- **Authors**: Jo Schlemper, Chen Qin, Jinming Duan, Ronald M. Summers, Kerstin Hammernik
- **Comment**: fastMRI challenge submission (team: holykspace)
- **Journal**: None
- **Summary**: We explore an ensembled $\Sigma$-net for fast parallel MR imaging, including parallel coil networks, which perform implicit coil weighting, and sensitivity networks, involving explicit sensitivity maps. The networks in $\Sigma$-net are trained in a supervised way, including content and GAN losses, and with various ways of data consistency, i.e., proximal mappings, gradient descent and variable splitting. A semi-supervised finetuning scheme allows us to adapt to the k-space data at test time, which, however, decreases the quantitative metrics, although generating the visually most textured and sharp images. For this challenge, we focused on robust and high SSIM scores, which we achieved by ensembling all models to a $\Sigma$-net.



### SiamMan: Siamese Motion-aware Network for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.05515v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05515v2)
- **Published**: 2019-12-11 18:26:44+00:00
- **Updated**: 2020-01-18 18:08:03+00:00
- **Authors**: Wenzhang Zhou, Longyin Wen, Libo Zhang, Dawei Du, Tiejian Luo, Yanjun Wu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel siamese motion-aware network (SiamMan) for visual tracking, which consists of the siamese feature extraction subnetwork, followed by the classification, regression, and localization branches in parallel. The classification branch is used to distinguish the foreground from background, and the regression branch is adopt to regress the bounding box of target. To reduce the impact of manually designed anchor boxes to adapt to different target motion patterns, we design the localization branch, which aims to coarsely localize the target to help the regression branch to generate accurate results. Meanwhile, we introduce the global context module into the localization branch to capture long-range dependency for more robustness in large displacement of target. In addition, we design a multi-scale learnable attention module to guide these three branches to exploit discriminative features for better performance. The whole network is trained offline in an end-to-end fashion with large-scale image pairs using the standard SGD algorithm with back-propagation. Extensive experiments on five challenging benchmarks, i.e., VOT2016, VOT2018, OTB100, UAV123 and LTB35, demonstrate that SiamMan achieves leading accuracy with high efficiency. Code can be found at https://isrc.iscas.ac.cn/gitlab/research/siamman.



### G3AN: Disentangling Appearance and Motion for Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1912.05523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05523v3)
- **Published**: 2019-12-11 18:46:53+00:00
- **Updated**: 2020-06-13 10:58:55+00:00
- **Authors**: Yaohui Wang, Piotr Bilinski, Francois Bremond, Antitza Dantcheva
- **Comment**: CVPR 2020, project link https://wyhsirius.github.io/G3AN/
- **Journal**: None
- **Summary**: Creating realistic human videos entails the challenge of being able to simultaneously generate both appearance, as well as motion. To tackle this challenge, we introduce G$^{3}$AN, a novel spatio-temporal generative model, which seeks to capture the distribution of high dimensional video data and to model appearance and motion in disentangled manner. The latter is achieved by decomposing appearance and motion in a three-stream Generator, where the main stream aims to model spatio-temporal consistency, whereas the two auxiliary streams augment the main stream with multi-scale appearance and motion features, respectively. An extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as well as the Weizmann and UCF101 datasets on human action. Additional analysis on the learned latent representations confirms the successful decomposition of appearance and motion. Source code and pre-trained models are publicly available.



### GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1912.05524v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05524v3)
- **Published**: 2019-12-11 18:47:11+00:00
- **Updated**: 2021-04-05 13:31:38+00:00
- **Authors**: Prune Truong, Martin Danelljan, Radu Timofte
- **Comment**: Website: https://prunetruong.com/research/glu-net. Code:
  https://github.com/PruneTruong/GLU-Net. Video:
  https://www.youtube.com/watch?v=xB2gNx8f8Xc. CVPR 2020 (ORAL)
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) 2020
- **Summary**: Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required.   In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights. Code and trained models are available at https://github.com/PruneTruong/GLU-Net.



### Variable Rate Deep Image Compression with Modulated Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1912.05526v2
- **DOI**: 10.1109/LSP.2020.2970539
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05526v2)
- **Published**: 2019-12-11 18:51:32+00:00
- **Updated**: 2020-07-21 20:11:08+00:00
- **Authors**: Fei Yang, Luis Herranz, Joost van de Weijer, José A. Iglesias Guitián, Antonio López, Mikhail Mozerov
- **Comment**: Published as a journal paper in IEEE Signal Processing Letters
- **Journal**: IEEE SPL,VOL.27(2020),331-335
- **Summary**: Variable rate is a requirement for flexible and adaptable image and video compression. However, deep image compression methods are optimized for a single fixed rate-distortion tradeoff. While this can be addressed by training multiple models for different tradeoffs, the memory requirements increase proportionally to the number of models. Scaling the bottleneck representation of a shared autoencoder can provide variable rate compression with a single shared autoencoder. However, the R-D performance using this simple mechanism degrades in low bitrates, and also shrinks the effective range of bit rates. Addressing these limitations, we formulate the problem of variable rate-distortion optimization for deep image compression, and propose modulated autoencoders (MAEs), where the representations of a shared autoencoder are adapted to the specific rate-distortion tradeoff via a modulation network. Jointly training this modulated autoencoder and modulation network provides an effective way to navigate the R-D operational curve. Our experiments show that the proposed method can achieve almost the same R-D performance of independent models with significantly fewer parameters.



### Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.05534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05534v1)
- **Published**: 2019-12-11 18:59:15+00:00
- **Updated**: 2019-12-11 18:59:15+00:00
- **Authors**: Jinwoo Choi, Chen Gao, Joseph C. E. Messou, Jia-Bin Huang
- **Comment**: NeurIPS 2019. Project webpage: http://chengao.vision/SDN/ Code:
  https://github.com/vt-vl-lab/SDN
- **Journal**: None
- **Summary**: Human activities often occur in specific scene contexts, e.g., playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.



### Neural Voice Puppetry: Audio-driven Facial Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1912.05566v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1912.05566v2)
- **Published**: 2019-12-11 19:00:18+00:00
- **Updated**: 2020-07-29 09:49:45+00:00
- **Authors**: Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, Matthias Nießner
- **Comment**: Video: https://youtu.be/s74_yQiJMXA Project/Demo/Code:
  https://justusthies.github.io/posts/neural-voice-puppetry/
- **Journal**: ECCV2020
- **Summary**: We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study.



### Object Recognition with Human in the Loop Intelligent Frameworks
- **Arxiv ID**: http://arxiv.org/abs/1912.05575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05575v1)
- **Published**: 2019-12-11 19:05:39+00:00
- **Updated**: 2019-12-11 19:05:39+00:00
- **Authors**: Orod Razeghi, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Classifiers embedded within human in the loop visual object recognition frameworks commonly utilise two sources of information: one derived directly from the imagery data of an object, and the other obtained interactively from user interactions. These computer vision frameworks exploit human high-level cognitive power to tackle particularly difficult visual object recognition tasks. In this paper, we present innovative techniques to combine the two sources of information intelligently for the purpose of improving recognition accuracy. We firstly employ standard algorithms to build two classifiers for the two sources independently, and subsequently fuse the outputs from these classifiers to make a conclusive decision. The two fusion techniques proposed are: i) a modified naive Bayes algorithm that adaptively selects an individual classifier's output or combines both to produce a definite answer, and ii) a neural network based algorithm which feeds the outputs of the two classifiers to a 4-layer feedforward network to generate a final output. We present extensive experimental results on 4 challenging visual recognition tasks to illustrate that the new intelligent techniques consistently outperform traditional approaches to fusing the two sources of information.



### Deteção de estruturas permanentes a partir de dados de séries temporais Sentinel 1 e 2
- **Arxiv ID**: http://arxiv.org/abs/1912.10799v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.10799v1)
- **Published**: 2019-12-11 19:08:07+00:00
- **Updated**: 2019-12-11 19:08:07+00:00
- **Authors**: André Neves, Carlos Damásio, João Pires, Fernando Birra
- **Comment**: 12 pages, in Portuguese, 7 figures, conference: INForum 2019
- **Journal**: None
- **Summary**: Mapping structures such as settlements, roads, individual houses and any other types of artificial structures is of great importance for the analysis of urban growth, masking, image alignment and, especially in the studied use case, the definition of Fuel Management Networks (FGC), which protect buildings from forest fires. Current cartography has a low generation frequency and their resolution may not be suitable for extracting small structures such as small settlements or roads, which may lack forest fire protection. In this paper, we use time series data, extracted from Sentinel-1 and 2 constellations, over Santar\'em, Ma\c{c}\~ao, to explore the detection of permanent structures at a resolution of 10 by 10 meters. For this purpose, a XGBoost classification model is trained with 133 attributes extracted from the time series from all the bands, including normalized radiometric indices. The results show that the use of time series data increases the accuracy of the extraction of permanent structures when compared using only static data, using multitemporal data also increases the number of detected roads. In general, the final result has a permanent structure mapping with a higher resolution than state of the art settlement maps, small structures and roads are also more accurately represented. Regarding the use case, by using our final map for the creation of FGC it is possible to simplify and accelerate the process of delimitation of the official FGC.



### Simultaneous Detection and Removal of Dynamic Objects in Multi-view Images
- **Arxiv ID**: http://arxiv.org/abs/1912.05591v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05591v1)
- **Published**: 2019-12-11 19:44:35+00:00
- **Updated**: 2019-12-11 19:44:35+00:00
- **Authors**: Gagan Kanojia, Shanmuganathan Raman
- **Comment**: Accepted at WACV 2020
- **Journal**: None
- **Summary**: Consider a set of images of a scene consisting of moving objects captured using a hand-held camera. In this work, we propose an algorithm which takes this set of multi-view images as input, detects the dynamic objects present in the scene, and replaces them with the static regions which are being occluded by them. The proposed algorithm scans the reference image in the row-major order at the pixel level and classifies each pixel as static or dynamic. During the scan, when a pixel is classified as dynamic, the proposed algorithm replaces that pixel value with the corresponding pixel value of the static region which is being occluded by that dynamic region. We show that we achieve artifact-free removal of dynamic objects in multi-view images of several real-world scenes. To the best of our knowledge, we propose the first method which simultaneously detects and removes the dynamic objects present in multi-view images.



### A Billion Ways to Grasp: An Evaluation of Grasp Sampling Schemes on a Dense, Physics-based Grasp Data Set
- **Arxiv ID**: http://arxiv.org/abs/1912.05604v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05604v1)
- **Published**: 2019-12-11 20:22:52+00:00
- **Updated**: 2019-12-11 20:22:52+00:00
- **Authors**: Clemens Eppner, Arsalan Mousavian, Dieter Fox
- **Comment**: For associated web page, see
  https://sites.google.com/view/abillionwaystograsp . 19th International
  Symposium of Robotics Research (ISRR)
- **Journal**: None
- **Summary**: Robot grasping is often formulated as a learning problem. With the increasing speed and quality of physics simulations, generating large-scale grasping data sets that feed learning algorithms is becoming more and more popular. An often overlooked question is how to generate the grasps that make up these data sets. In this paper, we review, classify, and compare different grasp sampling strategies. Our evaluation is based on a fine-grained discretization of SE(3) and uses physics-based simulation to evaluate the quality and robustness of the corresponding parallel-jaw grasps. Specifically, we consider more than 1 billion grasps for each of the 21 objects from the YCB data set. This dense data set lets us evaluate existing sampling schemes w.r.t. their bias and efficiency. Our experiments show that some popular sampling schemes contain significant bias and do not cover all possible ways an object can be grasped.



### Discriminative Dimension Reduction based on Mutual Information
- **Arxiv ID**: http://arxiv.org/abs/1912.05631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05631v1)
- **Published**: 2019-12-11 21:12:16+00:00
- **Updated**: 2019-12-11 21:12:16+00:00
- **Authors**: Orod Razeghi, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: The "curse of dimensionality" is a well-known problem in pattern recognition. A widely used approach to tackling the problem is a group of subspace methods, where the original features are projected onto a new space. The lower dimensional subspace is then used to approximate the original features for classification. However, most subspace methods were not originally developed for classification. We believe that direct adoption of these subspace methods for pattern classification should not be considered best practice. In this paper, we present a new information theory based algorithm for selecting subspaces, which can always result in superior performance over conventional methods. This paper makes the following main contributions: i) it improves a common practice widely used by practitioners in the field of pattern recognition, ii) it develops an information theory based technique for systematically selecting the subspaces that are discriminative and therefore are suitable for pattern recognition/classification purposes, iii) it presents extensive experimental results on a variety of computer vision and pattern recognition tasks to illustrate that the subspaces selected based on maximum mutual information criterion will always enhance performance regardless of the classification techniques used.



### CineFilter: Unsupervised Filtering for Real Time Autonomous Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/1912.05636v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1912.05636v4)
- **Published**: 2019-12-11 21:23:59+00:00
- **Updated**: 2020-05-27 10:24:37+00:00
- **Authors**: Sudheer Achary, K L Bhanu Moorthy, Syed Ashar Javed, Nikita Shravan, Vineet Gandhi, Anoop Namboodiri
- **Comment**: None
- **Journal**: None
- **Summary**: Autonomous camera systems are often subjected to an optimization/filtering operation to smoothen and stabilize the rough trajectory estimates. Most common filtering techniques do reduce the irregularities in data; however, they fail to mimic the behavior of a human cameraman. Global filtering methods modeling human camera operators have been successful; however, they are limited to offline settings. In this paper, we propose two online filtering methods called Cinefilters, which produce smooth camera trajectories that are motivated by cinematographic principles. The first filter (CineConvex) uses a sliding window-based convex optimization formulation, and the second (CineCNN) is a CNN based encoder-decoder model. We evaluate the proposed filters in two different settings, namely a basketball dataset and a stage performance dataset. Our models outperform previous methods and baselines on both quantitative and qualitative metrics. The CineConvex and CineCNN filters operate at about 250fps and 1000fps, respectively, with a minor latency (half a second), making them apt for a variety of real-time applications.



### deepsing: Generating Sentiment-aware Visual Stories using Cross-modal Music Translation
- **Arxiv ID**: http://arxiv.org/abs/1912.05654v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1912.05654v1)
- **Published**: 2019-12-11 21:46:41+00:00
- **Updated**: 2019-12-11 21:46:41+00:00
- **Authors**: Nikolaos Passalis, Stavros Doropoulos
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a deep learning method for performing attributed-based music-to-image translation. The proposed method is applied for synthesizing visual stories according to the sentiment expressed by songs. The generated images aim to induce the same feelings to the viewers, as the original song does, reinforcing the primary aim of music, i.e., communicating feelings. The process of music-to-image translation poses unique challenges, mainly due to the unstable mapping between the different modalities involved in this process. In this paper, we employ a trainable cross-modal translation method to overcome this limitation, leading to the first, to the best of our knowledge, deep learning method for generating sentiment-aware visual stories. Various aspects of the proposed method are extensively evaluated and discussed using different songs.



### VIBE: Video Inference for Human Body Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.05656v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05656v3)
- **Published**: 2019-12-11 21:47:26+00:00
- **Updated**: 2020-04-29 19:35:34+00:00
- **Authors**: Muhammed Kocabas, Nikos Athanasiou, Michael J. Black
- **Comment**: CVPR-2020 camera ready. Code is available at
  https://github.com/mkocabas/VIBE
- **Journal**: None
- **Summary**: Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methods fail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose Video Inference for Body Pose and Shape Estimation (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape regression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE.



### Gabor Layers Enhance Network Robustness
- **Arxiv ID**: http://arxiv.org/abs/1912.05661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05661v2)
- **Published**: 2019-12-11 21:59:59+00:00
- **Updated**: 2020-03-27 21:52:04+00:00
- **Authors**: Juan C. Pérez, Motasem Alfarra, Guillaume Jeanneret, Adel Bibi, Ali Thabet, Bernard Ghanem, Pablo Arbeláez
- **Comment**: 32 pages, 23 figures, 14 tables
- **Journal**: None
- **Summary**: We revisit the benefits of merging classical vision concepts with deep learning models. In particular, we explore the effect on robustness against adversarial attacks of replacing the first layers of various deep architectures with Gabor layers, i.e. convolutional layers with filters that are based on learnable Gabor parameters. We observe that architectures enhanced with Gabor layers gain a consistent boost in robustness over regular models and preserve high generalizing test performance, even though these layers come at a negligible increase in the number of parameters. We then exploit the closed form expression of Gabor filters to derive an expression for a Lipschitz constant of such filters, and harness this theoretical result to develop a regularizer we use during training to further enhance network robustness. We conduct extensive experiments with various architectures (LeNet, AlexNet, VGG16 and WideResNet) on several datasets (MNIST, SVHN, CIFAR10 and CIFAR100) and demonstrate large empirical robustness gains. Furthermore, we experimentally show how our regularizer provides consistent robustness improvements.



### Machine Learning for Precipitation Nowcasting from Radar Images
- **Arxiv ID**: http://arxiv.org/abs/1912.12132v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.12132v1)
- **Published**: 2019-12-11 22:46:54+00:00
- **Updated**: 2019-12-11 22:46:54+00:00
- **Authors**: Shreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk Gazen, Jason Hickey
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.



### Online Deep Reinforcement Learning for Autonomous UAV Navigation and Exploration of Outdoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1912.05684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05684v1)
- **Published**: 2019-12-11 23:02:24+00:00
- **Updated**: 2019-12-11 23:02:24+00:00
- **Authors**: Bruna G. Maciel-Pearson, Letizia Marchegiani, Samet Akcay, Amir Atapour-Abarghouei, James Garforth, Toby P. Breckon
- **Comment**: Journal Submission
- **Journal**: None
- **Summary**: With the rapidly growing expansion in the use of UAVs, the ability to autonomously navigate in varying environments and weather conditions remains a highly desirable but as-of-yet unsolved challenge. In this work, we use Deep Reinforcement Learning to continuously improve the learning and understanding of a UAV agent while exploring a partially observable environment, which simulates the challenges faced in a real-life scenario. Our innovative approach uses a double state-input strategy that combines the acquired knowledge from the raw image and a map containing positional information. This positional data aids the network understanding of where the UAV has been and how far it is from the target position, while the feature map from the current scene highlights cluttered areas that are to be avoided. Our approach is extensively tested using variants of Deep Q-Network adapted to cope with double state input data. Further, we demonstrate that by altering the reward and the Q-value function, the agent is capable of consistently outperforming the adapted Deep Q-Network, Double Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate that our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of navigating through multiple unseen environments and under severe weather conditions.



### REFINED (REpresentation of Features as Images with NEighborhood Dependencies): A novel feature representation for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.05687v2
- **DOI**: 10.1038/s41467-020-18197-y
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.05687v2)
- **Published**: 2019-12-11 23:18:05+00:00
- **Updated**: 2020-05-15 05:26:15+00:00
- **Authors**: Omid Bazgir, Ruibo Zhang, Saugato Rahman Dhruba, Raziur Rahman, Souparno Ghosh, Ranadip Pal
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning with Convolutional Neural Networks has shown great promise in various areas of image-based classification and enhancement but is often unsuitable for predictive modeling involving non-image based features or features without spatial correlations. We present a novel approach for representation of high dimensional feature vector in a compact image form, termed REFINED (REpresentation of Features as Images with NEighborhood Dependencies), that is conducible for convolutional neural network based deep learning. We consider the correlations between features to generate a compact representation of the features in the form of a two-dimensional image using minimization of pairwise distances similar to multi-dimensional scaling. We hypothesize that this approach enables embedded feature selection and integrated with Convolutional Neural Network based Deep Learning can produce more accurate predictions as compared to Artificial Neural Networks, Random Forests and Support Vector Regression. We illustrate the superior predictive performance of the proposed representation, as compared to existing approaches, using synthetic datasets, cell line efficacy prediction based on drug chemical descriptors for NCI60 dataset and drug sensitivity prediction based on transcriptomic data and chemical descriptors using GDSC dataset. Results illustrated on both synthetic and biological datasets shows the higher prediction accuracy of the proposed framework as compared to existing methodologies while maintaining desirable properties in terms of bias and feature extraction.



### Learned Variable-Rate Image Compression with Residual Divisive Normalization
- **Arxiv ID**: http://arxiv.org/abs/1912.05688v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.05688v1)
- **Published**: 2019-12-11 23:19:58+00:00
- **Updated**: 2019-12-11 23:19:58+00:00
- **Authors**: Mohammad Akbari, Jie Liang, Jingning Han, Chengjie Tu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Recently it has been shown that deep learning-based image compression has shown the potential to outperform traditional codecs. However, most existing methods train multiple networks for multiple bit rates, which increases the implementation complexity. In this paper, we propose a variable-rate image compression framework, which employs more Generalized Divisive Normalization (GDN) layers than previous GDN-based methods. Novel GDN-based residual sub-networks are also developed in the encoder and decoder networks. Our scheme also uses a stochastic rounding-based scalable quantization. To further improve the performance, we encode the residual between the input and the reconstructed image from the decoder network as an enhancement layer. To enable a single model to operate with different bit rates and to learn multi-rate image features, a new objective function is introduced. Experimental results show that the proposed framework trained with variable-rate objective function outperforms all standard codecs such as H.265/HEVC-based BPG and state-of-the-art learning-based variable-rate methods.



### What it Thinks is Important is Important: Robustness Transfers through Input Gradients
- **Arxiv ID**: http://arxiv.org/abs/1912.05699v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.05699v3)
- **Published**: 2019-12-11 23:51:37+00:00
- **Updated**: 2020-10-29 13:45:16+00:00
- **Authors**: Alvin Chan, Yi Tay, Yew-Soon Ong
- **Comment**: Accepted as Oral in CVPR 2020, Camera-Ready Version
- **Journal**: None
- **Summary**: Adversarial perturbations are imperceptible changes to input pixels that can change the prediction of deep learning models. Learned weights of models robust to such perturbations are previously found to be transferable across different tasks but this applies only if the model architecture for the source and target tasks is the same. Input gradients characterize how small changes at each input pixel affect the model output. Using only natural images, we show here that training a student model's input gradients to match those of a robust teacher model can gain robustness close to a strong baseline that is robustly trained from scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and Tiny-ImageNet, we show that our proposed method, input gradient adversarial matching, can transfer robustness across different tasks and even across different model architectures. This demonstrates that directly targeting the semantics of input gradients is a feasible way towards adversarial robustness.



