# Arxiv Papers in cs.CV on 2019-12-03
### Mixing autoencoder with classifier: conceptual data visualization
- **Arxiv ID**: http://arxiv.org/abs/1912.01137v3
- **DOI**: 10.1109/ACCESS.2020.2999155
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01137v3)
- **Published**: 2019-12-03 00:33:26+00:00
- **Updated**: 2020-02-21 10:27:28+00:00
- **Authors**: Pitoyo Hartono
- **Comment**: None
- **Journal**: IEEE Access, vol. 8, no. 1, pp. 105301-105310, 2020
- **Summary**: In this short paper, a neural network that is able to form a low dimensional topological hidden representation is explained. The neural network can be trained as an autoencoder, a classifier or mix of both, and produces different low dimensional topological map for each of them. When it is trained as an autoencoder, the inherent topological structure of the data can be visualized, while when it is trained as a classifier, the topological structure is further constrained by the concept, for example the labels the data, hence the visualization is not only structural but also conceptual. The proposed neural network significantly differ from many dimensional reduction models, primarily in its ability to execute both supervised and unsupervised dimensional reduction. The neural network allows multi perspective visualization of the data, and thus giving more flexibility in data analysis. This paper is supported by preliminary but intuitive visualization experiments.



### A Deep Convolutional Network for Seismic Shot-Gather Image Quality Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.01148v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01148v1)
- **Published**: 2019-12-03 01:48:20+00:00
- **Updated**: 2019-12-03 01:48:20+00:00
- **Authors**: Eduardo Betine Bucker, Antonio José Grandson Busson, Ruy Luiz Milidiú, Sérgio Colcher, Bruno Pereira Dias, André Bulcão
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Learning-based models such as Convolutional Neural Networks, have led to significant advancements in several areas of computing applications. Seismogram quality assurance is a relevant Geophysics task, since in the early stages of seismic processing, we are required to identify and fix noisy sail lines. In this work, we introduce a real-world seismogram quality classification dataset based on 6,613 examples, manually labeled by human experts as good, bad or ugly, according to their noise intensity. This dataset is used to train a CNN classifier for seismic shot-gathers quality prediction. In our empirical evaluation, we observe an F1-score of 93.56% in the test set.



### Noise2Blur: Online Noise Extraction and Denoising
- **Arxiv ID**: http://arxiv.org/abs/1912.01158v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01158v2)
- **Published**: 2019-12-03 02:28:56+00:00
- **Updated**: 2020-05-14 07:33:08+00:00
- **Authors**: Huangxing Lin, Weihong Zeng, Xinghao Ding, Xueyang Fu, Yue Huang, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new framework called Noise2Blur (N2B) for training robust image denoising models without pre-collected paired noisy/clean images. The training of the model requires only some (or even one) noisy images, some random unpaired clean images, and noise-free but blurred labels obtained by predefined filtering of the noisy images. The N2B model consists of two parts: a denoising network and a noise extraction network. First, the noise extraction network learns to output a noise map using the noise information from the denoising network under the guidence of the blurred labels. Then, the noise map is added to a clean image to generate a new "noisy/clean" image pair. Using the new image pair, the denoising network learns to generate clean and high-quality images from noisy observations. These two networks are trained simultaneously and mutually aid each other to learn the mappings of noise to clean/blur. Experiments on several denoising tasks show that the denoising performance of N2B is close to that of other denoising CNNs trained with pre-collected paired data.



### Google street view and deep learning: a new ground truthing approach for crop mapping
- **Arxiv ID**: http://arxiv.org/abs/1912.05024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.05024v1)
- **Published**: 2019-12-03 02:41:31+00:00
- **Updated**: 2019-12-03 02:41:31+00:00
- **Authors**: Yulin Yan, Youngryel Ryu
- **Comment**: None
- **Journal**: None
- **Summary**: Ground referencing is essential for supervised crop mapping. However, conventional ground truthing involves extensive field surveys and post processing, which is costly in terms of time and labor. In this study, we applied a convolutional neural network (CNN) model to explore the efficacy of automatic ground truthing via Google street view (GSV) images in two distinct farming regions: central Illinois and southern California. We demonstrated the feasibility and reliability of the new ground referencing technique further by performing pixel-based crop mapping with vegetation indices as the model input. The results were evaluated using the United States Department of Agriculture (USDA) crop data layer (CDL) products. From 8,514 GSV images, the CNN model screened out 2,645 target crop images. These images were well classified into crop types, including alfalfa, almond, corn, cotton, grape, soybean, and pistachio. The overall GSV image classification accuracy reached 93% in California and 97% in Illinois. We then shifted the image geographic coordinates using fixed empirical coefficients to produce 8,173 crop reference points including 1,764 in Illinois and 6,409 in California. Evaluation of these new reference points with CDL products showed satisfactory coherence, with 94 to 97% agreement. CNN-based mapping also captured the general pattern of crop type distributions. The overall differences between CDL products and our mapping results were 4% in California and 5% in Illinois. Thus, using these deep learning and GSV image techniques, we have provided an efficient and cost-effective alternative method for ground referencing and crop mapping.



### SAIS: Single-stage Anchor-free Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.01176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01176v1)
- **Published**: 2019-12-03 03:22:32+00:00
- **Updated**: 2019-12-03 03:22:32+00:00
- **Authors**: Canqun Xiang, Shishun Tian, Wenbin Zou, Chen Xu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet efficientinstance segmentation approach based on the single-stage anchor-free detector, termed SAIS. In our approach, the instancesegmentation task consists of two parallel subtasks which re-spectively predict the mask coefficients and the mask prototypes.Then, instance masks are generated by linearly combining theprototypes with the mask coefficients. To enhance the quality ofinstance mask, the information from regression and classificationis fused to predict the mask coefficients. In addition, center-aware target is designed to preserve the center coordination ofeach instance, which achieves a stable improvement in instancesegmentation. The experiment on MS COCO shows that SAISachieves the performance of the exiting state-of-the-art single-stage methods with a much less memory footpr



### RSA: Randomized Simulation as Augmentation for Robust Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.01180v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01180v1)
- **Published**: 2019-12-03 03:45:45+00:00
- **Updated**: 2019-12-03 03:45:45+00:00
- **Authors**: Yi Zhang, Xinyue Wei, Weichao Qiu, Zihao Xiao, Gregory D. Hager, Alan Yuille
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Despite the rapid growth in datasets for video activity, stable robust activity recognition with neural networks remains challenging. This is in large part due to the explosion of possible variation in video -- including lighting changes, object variation, movement variation, and changes in surrounding context. An alternative is to make use of simulation data, where all of these factors can be artificially controlled. In this paper, we propose the Randomized Simulation as Augmentation (RSA) framework which augments real-world training data with synthetic data to improve the robustness of action recognition networks. We generate large-scale synthetic datasets with randomized nuisance factors. We show that training with such extra data, when appropriately constrained, can significantly improve the performance of the state-of-the-art I3D networks or, conversely, reduce the number of labeled real videos needed to achieve good performance. Experiments on two real-world datasets NTU RGB+D and VIRAT demonstrate the effectiveness of our method.



### Multi-resolution Graph Neural Network for Identifying Disease-specific Variations in Brain Connectivity
- **Arxiv ID**: http://arxiv.org/abs/1912.01181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01181v1)
- **Published**: 2019-12-03 03:46:14+00:00
- **Updated**: 2019-12-03 03:46:14+00:00
- **Authors**: Xin Ma, Guorong Wu, Won Hwa Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution Neural Network (CNN) recently have been adopted in several neuroimaging studies for diagnosis capturing disease-specific changes in the brain. While many of these methods are designed to work with images in $\mathbb R^n$ exploiting regular structure of the domain, they are not well-suited to analyze data with irregular structure such as brain connectivity. As there is significant interest in understanding the altered interactions between different brain regions that lead to neuro-disorders, it is important to develop data-driven methods that work with a population of graph data for traditional prediction tasks. In this regime, we propose a novel CNN-based framework with adaptive graph transforms to learn the most disease-relevant connectome feature maps which have the highest discrimination power across diagnostic categories. The backbone of our framework is a multi-resolution representation of the graph matrix which is steered by a set of wavelet-like graph transforms. In this context, our supervised graph learning framework outperforms conventional graph methods that predict diagnostic label only based on the underlying individual graph. Our extensive experiments on two real datasets of functional and structural brain networks show that our multi-resolution framework achieves significantly higher accuracy, precision and recall in predicting diagnostic labels and identifying disease-specific brain connectivities that are associated with brain disorders such as Attention-Deficit/Hyperactivity Disorder (ADHD) and Alzheimer's Disease (AD).



### Learning to Super Resolve Intensity Images from Events
- **Arxiv ID**: http://arxiv.org/abs/1912.01196v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01196v3)
- **Published**: 2019-12-03 05:24:06+00:00
- **Updated**: 2020-04-09 23:28:28+00:00
- **Authors**: S. Mohammad Mostafavi I., Jonghyun Choi, Kuk-Jin Yoon
- **Comment**: To appear in CVPR 2020 as an oral presentation
- **Journal**: None
- **Summary**: An event camera detects per-pixel intensity difference and produces asynchronous event stream with low latency, high dynamic range, and low power consumption. As a trade-off, the event camera has low spatial resolution. We propose an end-to-end network to reconstruct high resolution, high dynamic range (HDR) images directly from the event stream. We evaluate our algorithm on both simulated and real-world sequences and verify that it captures fine details of a scene and outperforms the combination of the state-of-the-art event to image algorithms with the state-of-the-art super resolution schemes in many quantitative measures by large margins. We further extend our method by using the active sensor pixel (APS) frames or reconstructing images iteratively.



### Structure Learning with Similarity Preserving
- **Arxiv ID**: http://arxiv.org/abs/1912.01197v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01197v1)
- **Published**: 2019-12-03 05:25:08+00:00
- **Updated**: 2019-12-03 05:25:08+00:00
- **Authors**: Zhao Kang, Xiao Lu, Yiwei Lu, Chong Peng, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Leveraging on the underlying low-dimensional structure of data, low-rank and sparse modeling approaches have achieved great success in a wide range of applications. However, in many applications the data can display structures beyond simply being low-rank or sparse. Fully extracting and exploiting hidden structure information in the data is always desirable and favorable. To reveal more underlying effective manifold structure, in this paper, we explicitly model the data relation. Specifically, we propose a structure learning framework that retains the pairwise similarities between the data points. Rather than just trying to reconstruct the original data based on self-expression, we also manage to reconstruct the kernel matrix, which functions as similarity preserving. Consequently, this technique is particularly suitable for the class of learning problems that are sensitive to sample similarity, e.g., clustering and semisupervised classification. To take advantage of representation power of deep neural network, a deep auto-encoder architecture is further designed to implement our model. Extensive experiments on benchmark data sets demonstrate that our proposed framework can consistently and significantly improve performance on both evaluation tasks. We conclude that the quality of structure learning can be enhanced if similarity information is incorporated.



### Multi-view Subspace Clustering via Partition Fusion
- **Arxiv ID**: http://arxiv.org/abs/1912.01201v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01201v1)
- **Published**: 2019-12-03 05:48:44+00:00
- **Updated**: 2019-12-03 05:48:44+00:00
- **Authors**: Juncheng Lv, Zhao Kang, Boyu Wang, Luping Ji, Zenglin Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-view clustering is an important approach to analyze multi-view data in an unsupervised way. Among various methods, the multi-view subspace clustering approach has gained increasing attention due to its encouraging performance. Basically, it integrates multi-view information into graphs, which are then fed into spectral clustering algorithm for final result. However, its performance may degrade due to noises existing in each individual view or inconsistency between heterogeneous features. Orthogonal to current work, we propose to fuse multi-view information in a partition space, which enhances the robustness of Multi-view clustering. Specifically, we generate multiple partitions and integrate them to find the shared partition. The proposed model unifies graph learning, generation of basic partitions, and view weight learning. These three components co-evolve towards better quality outputs. We have conducted comprehensive experiments on benchmark datasets and our empirical results verify the effectiveness and robustness of our approach.



### Real-Time Panoptic Segmentation from Dense Detections
- **Arxiv ID**: http://arxiv.org/abs/1912.01202v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01202v3)
- **Published**: 2019-12-03 05:50:02+00:00
- **Updated**: 2020-04-03 23:08:01+00:00
- **Authors**: Rui Hou, Jie Li, Arjun Bhargava, Allan Raventos, Vitor Guizilini, Chao Fang, Jerome Lynch, Adrien Gaidon
- **Comment**: 12 pages, 6 figures
- **Journal**: None
- **Summary**: Panoptic segmentation is a complex full scene parsing task requiring simultaneous instance and semantic segmentation at high resolution. Current state-of-the-art approaches cannot run in real-time, and simplifying these architectures to improve efficiency severely degrades their accuracy. In this paper, we propose a new single-shot panoptic segmentation network that leverages dense detections and a global self-attention mechanism to operate in real-time with performance approaching the state of the art. We introduce a novel parameter-free mask construction method that substantially reduces computational complexity by efficiently reusing information from the object detection and semantic segmentation sub-tasks. The resulting network has a simple data flow that does not require feature map re-sampling or clustering post-processing, enabling significant hardware acceleration. Our experiments on the Cityscapes and COCO benchmarks show that our network works at 30 FPS on 1024x2048 resolution, trading a 3% relative performance degradation from the current state of the art for up to 440% faster inference.



### Robust Invisible Hyperlinks in Physical Photographs Based on 3D Rendering Attacks
- **Arxiv ID**: http://arxiv.org/abs/1912.01224v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01224v1)
- **Published**: 2019-12-03 07:30:49+00:00
- **Updated**: 2019-12-03 07:30:49+00:00
- **Authors**: Jun Jia, Zhongpai Gao, Kang Chen, Menghan Hu, Guangtao Zhai, Guodong Guo, Xiaokang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In the era of multimedia and Internet, people are eager to obtain information from offline to online. Quick Response (QR) codes and digital watermarks help us access information quickly. However, QR codes look ugly and invisible watermarks can be easily broken in physical photographs. Therefore, this paper proposes a novel method to embed hyperlinks into natural images, making the hyperlinks invisible for human eyes but detectable for mobile devices. Our method is an end-to-end neural network with an encoder to hide information and a decoder to recover information. From original images to physical photographs, camera imaging process will introduce a series of distortion such as noise, blur, and light. To train a robust decoder against the physical distortion from the real world, a distortion network based on 3D rendering is inserted between the encoder and the decoder to simulate the camera imaging process. Besides, in order to maintain the visual attraction of the image with hyperlinks, we propose a loss function based on just noticeable difference (JND) to supervise the training of encoder. Experimental results show that our approach outperforms the previous method in both simulated and real situations.



### Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.01230v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01230v3)
- **Published**: 2019-12-03 07:46:47+00:00
- **Updated**: 2020-03-14 20:34:26+00:00
- **Authors**: Seokeon Choi, Sumin Lee, Youngeun Kim, Taekyung Kim, Changick Kim
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Visible-infrared person re-identification (VI-ReID) is an important task in night-time surveillance applications, since visible cameras are difficult to capture valid appearance information under poor illumination conditions. Compared to traditional person re-identification that handles only the intra-modality discrepancy, VI-ReID suffers from additional cross-modality discrepancy caused by different types of imaging systems. To reduce both intra- and cross-modality discrepancies, we propose a Hierarchical Cross-Modality Disentanglement (Hi-CMD) method, which automatically disentangles ID-discriminative factors and ID-excluded factors from visible-thermal images. We only use ID-discriminative factors for robust cross-modality matching without ID-excluded factors such as pose or illumination. To implement our approach, we introduce an ID-preserving person image generation network and a hierarchical feature learning module. Our generation network learns the disentangled representation by generating a new cross-modality image with different poses and illuminations while preserving a person's identity. At the same time, the feature learning module enables our model to explicitly extract the common ID-discriminative characteristic between visible-infrared images. Extensive experimental results demonstrate that our method outperforms the state-of-the-art methods on two VI-ReID datasets. The source code is available at: https://github.com/bismex/HiCMD.



### EDAS: Efficient and Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1912.01237v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01237v2)
- **Published**: 2019-12-03 08:13:28+00:00
- **Updated**: 2019-12-04 10:10:04+00:00
- **Authors**: Hyeong Gwon Hong, Pyunghwan Ahn, Junmo Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Transferrable neural architecture search can be viewed as a binary optimization problem where a single optimal path should be selected among candidate paths in each edge within the repeated cell block of the directed a cyclic graph form. Recently, the field of differentiable architecture search attempts to relax the search problem continuously using a one-shot network that combines all the candidate paths in search space. However, when the one-shot network is pruned to the model in the discrete architecture space by the derivation algorithm, performance is significantly degraded to an almost random estimator. To reduce the quantization error from the heavy use of relaxation, we only sample a single edge to relax the corresponding variable and clamp variables in the other edges to zero or one. By this method, there is no performance drop after pruning the one-shot network by derivation algorithm, due to the preservation of the discrete nature of optimization variables during the search. Furthermore, the minimization of relaxation degree allows searching in a deeper network to discover better performance with remarkable search cost reduction (0.125 GPU days) compared to previous methods. By adding several regularization methods that help explore within the search space, we could obtain the network with notable performances on CIFAR-10, CIFAR-100, and ImageNet.



### Cyclic Functional Mapping: Self-supervised correspondence between non-isometric deformable shapes
- **Arxiv ID**: http://arxiv.org/abs/1912.01249v1
- **DOI**: 10.1007/978-3-030-58558-7_3
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01249v1)
- **Published**: 2019-12-03 09:11:25+00:00
- **Updated**: 2019-12-03 09:11:25+00:00
- **Authors**: Dvir Ginzburg, Dan Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We present the first utterly self-supervised network for dense correspondence mapping between non-isometric shapes. The task of alignment in non-Euclidean domains is one of the most fundamental and crucial problems in computer vision. As 3D scanners can generate highly complex and dense models, the mission of finding dense mappings between those models is vital. The novelty of our solution is based on a cyclic mapping between metric spaces, where the distance between a pair of points should remain invariant after the full cycle. As the same learnable rules that generate the point-wise descriptors apply in both directions, the network learns invariant structures without any labels while coping with non-isometric deformations. We show here state-of-the-art-results by a large margin for a variety of tasks compared to known self-supervised and supervised methods.



### The Knowledge Within: Methods for Data-Free Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1912.01274v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01274v2)
- **Published**: 2019-12-03 10:01:51+00:00
- **Updated**: 2020-04-06 19:00:35+00:00
- **Authors**: Matan Haroush, Itay Hubara, Elad Hoffer, Daniel Soudry
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, an extensive amount of research has been focused on compressing and accelerating Deep Neural Networks (DNN). So far, high compression rate algorithms require part of the training dataset for a low precision calibration, or a fine-tuning process. However, this requirement is unacceptable when the data is unavailable or contains sensitive information, as in medical and biometric use-cases. We present three methods for generating synthetic samples from trained models. Then, we demonstrate how these samples can be used to calibrate and fine-tune quantized models without using any real data in the process. Our best performing method has a negligible accuracy degradation compared to the original training set. This method, which leverages intrinsic batch normalization layers' statistics of the trained model, can be used to evaluate data similarity. Our approach opens a path towards genuine data-free model compression, alleviating the need for training data during model deployment.



### Scene recognition based on DNN and game theory with its applications in human-robot interaction
- **Arxiv ID**: http://arxiv.org/abs/1912.01293v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01293v4)
- **Published**: 2019-12-03 10:54:38+00:00
- **Updated**: 2020-01-10 11:03:24+00:00
- **Authors**: R. Q. Wang, W. Z. Wang, D. Z. Zhao, G. H. Chen, D. S. Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Scene recognition model based on the DNN and game theory with its applications in human-robot interaction is proposed in this paper. The use of deep learning methods in the field of scene recognition is still in its infancy, but has become an important trend in the future. As the innovative idea of the paper, we propose the following novelties. (1) In this paper, the image registration problem is transformed into a problem of minimum energy in Markov Random Field to finalize the image pre-processing task. Game theory is used to find the optimal. (2) We select neighboring homogeneous sample features and the neighboring heterogeneous sample features for the extracted sample features to build a triple and modify the traditional neural network to propose the novel DNN for scene understanding. (3) The robot control is well combined to guide the robot vision for multiple tasks. The experiment is then conducted to validate the overall performance.



### Viewpoint-Aware Loss with Angular Regularization for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.01300v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01300v1)
- **Published**: 2019-12-03 11:10:29+00:00
- **Updated**: 2019-12-03 11:10:29+00:00
- **Authors**: Zhihui Zhu, Xinyang Jiang, Feng Zheng, Xiaowei Guo, Feiyue Huang, Weishi Zheng, Xing Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Although great progress in supervised person re-identification (Re-ID) has been made recently, due to the viewpoint variation of a person, Re-ID remains a massive visual challenge. Most existing viewpoint-based person Re-ID methods project images from each viewpoint into separated and unrelated sub-feature spaces. They only model the identity-level distribution inside an individual viewpoint but ignore the underlying relationship between different viewpoints. To address this problem, we propose a novel approach, called \textit{Viewpoint-Aware Loss with Angular Regularization }(\textbf{VA-reID}). Instead of one subspace for each viewpoint, our method projects the feature from different viewpoints into a unified hypersphere and effectively models the feature distribution on both the identity-level and the viewpoint-level. In addition, rather than modeling different viewpoints as hard labels used for conventional viewpoint classification, we introduce viewpoint-aware adaptive label smoothing regularization (VALSR) that assigns the adaptive soft label to feature representation. VALSR can effectively solve the ambiguity of the viewpoint cluster label assignment. Extensive experiments on the Market1501 and DukeMTMC-reID datasets demonstrated that our method outperforms the state-of-the-art supervised Re-ID methods.



### Joint Graph-based Depth Refinement and Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.01306v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01306v2)
- **Published**: 2019-12-03 11:26:32+00:00
- **Updated**: 2020-09-02 09:20:12+00:00
- **Authors**: Mattia Rossi, Mireille El Gheche, Andreas Kuhn, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is an essential component in understanding the 3D geometry of a scene, with numerous applications in urban and indoor settings. These scenes are characterized by a prevalence of human made structures, which in most of the cases, are either inherently piece-wise planar, or can be approximated as such. In these settings, we devise a novel depth refinement framework that aims at recovering the underlying piece-wise planarity of the inverse depth map. We formulate this task as an optimization problem involving a data fidelity term that minimizes the distance to the input inverse depth map, as well as a regularization that enforces a piece-wise planar solution. As for the regularization term, we model the inverse depth map as a weighted graph between pixels. The proposed regularization is designed to estimate a plane automatically at each pixel, without any need for an a priori estimation of the scene planes, and at the same time it encourages similar pixels to be assigned to the same plane. The resulting optimization problem is efficiently solved with ADAM algorithm. Experiments show that our method leads to a significant improvement in depth refinement, both visually and numerically, with respect to state-of-the-art algorithms on Middlebury, KITTI and ETH3D multi-view stereo datasets.



### Analyzing and Improving the Image Quality of StyleGAN
- **Arxiv ID**: http://arxiv.org/abs/1912.04958v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.04958v2)
- **Published**: 2019-12-03 11:44:01+00:00
- **Updated**: 2020-03-23 17:21:07+00:00
- **Authors**: Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila
- **Comment**: None
- **Journal**: None
- **Summary**: The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.



### ATIS + SpiNNaker: a Fully Event-based Visual Tracking Demonstration
- **Arxiv ID**: http://arxiv.org/abs/1912.01320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01320v1)
- **Published**: 2019-12-03 11:46:54+00:00
- **Updated**: 2019-12-03 11:46:54+00:00
- **Authors**: Arren Glover, Alan B. Stokes, Steve Furber, Chiara Bartolozzi
- **Comment**: Presented at the Unconventional Sensing and Processing for Robotic
  Visual Perception workshop at the 2018 IEEE/RSJ International Conference on
  Intelligent Robots and Systems. 2 pages, 2 figures
- **Journal**: None
- **Summary**: The Asynchronous Time-based Image Sensor (ATIS) and the Spiking Neural Network Architecture (SpiNNaker) are both neuromorphic technologies that "unconventionally" use binary spikes to represent information. The ATIS produces spikes to represent the change in light falling on the sensor, and the SpiNNaker is a massively parallel computing platform that asynchronously sends spikes between cores for processing. In this demonstration we show these two hardware used together to perform a visual tracking task. We aim to show the hardware and software architecture that integrates the ATIS and SpiNNaker together in a robot middle-ware that makes processing agnostic to the platform (CPU or SpiNNaker). We also aim to describe the algorithm, why it is suitable for the "unconventional" sensor and processing platform including the advantages as well as challenges faced.



### A Context-Aware Loss Function for Action Spotting in Soccer Videos
- **Arxiv ID**: http://arxiv.org/abs/1912.01326v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01326v3)
- **Published**: 2019-12-03 11:59:55+00:00
- **Updated**: 2020-03-30 13:53:26+00:00
- **Authors**: Anthony Cioppa, Adrien Deliège, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck, Rikke Gade, Thomas B. Moeslund
- **Comment**: Accepted for CVPR2020 main conference. This document contains 8 pages
  + references + supplementary material
- **Journal**: None
- **Summary**: In video understanding, action spotting consists in temporally localizing human-induced events annotated with single timestamps. In this paper, we propose a novel loss function that specifically considers the temporal context naturally present around each action, rather than focusing on the single annotated frame to spot. We benchmark our loss on a large dataset of soccer videos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We show the generalization capability of our loss for generic activity proposals and detection on ActivityNet, by spotting the beginning and the end of each activity. Furthermore, we provide an extended ablation study and display challenging cases for action spotting in soccer videos. Finally, we qualitatively illustrate how our loss induces a precise temporal understanding of actions and show how such semantic knowledge can be used for automatic highlights generation.



### Asymmetric Co-Teaching for Unsupervised Cross Domain Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1912.01349v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01349v1)
- **Published**: 2019-12-03 13:00:34+00:00
- **Updated**: 2019-12-03 13:00:34+00:00
- **Authors**: Fengxiang Yang, Ke Li, Zhun Zhong, Zhiming Luo, Xing Sun, Hao Cheng, Xiaowei Guo, Feiyue Huang, Rongrong Ji, Shaozi Li
- **Comment**: Accepted by AAAi 2020
- **Journal**: None
- **Summary**: Person re-identification (re-ID), is a challenging task due to the high variance within identity samples and imaging conditions. Although recent advances in deep learning have achieved remarkable accuracy in settled scenes, i.e., source domain, few works can generalize well on the unseen target domain. One popular solution is assigning unlabeled target images with pseudo labels by clustering, and then retraining the model. However, clustering methods tend to introduce noisy labels and discard low confidence samples as outliers, which may hinder the retraining process and thus limit the generalization ability. In this study, we argue that by explicitly adding a sample filtering procedure after the clustering, the mined examples can be much more efficiently used. To this end, we design an asymmetric co-teaching framework, which resists noisy labels by cooperating two models to select data with possibly clean labels for each other. Meanwhile, one of the models receives samples as pure as possible, while the other takes in samples as diverse as possible. This procedure encourages that the selected training samples can be both clean and miscellaneous, and that the two models can promote each other iteratively. Extensive experiments show that the proposed framework can consistently benefit most clustering-based methods, and boost the state-of-the-art adaptation accuracy. Our code is available at https://github.com/FlyingRoastDuck/ACT_AAAI20.



### A Step Towards Exposing Bias in Trained Deep Convolutional Neural Network Models
- **Arxiv ID**: http://arxiv.org/abs/1912.02094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.02094v1)
- **Published**: 2019-12-03 13:06:31+00:00
- **Updated**: 2019-12-03 13:06:31+00:00
- **Authors**: Daniel Omeiza
- **Comment**: Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World. arXiv admin note: substantial text overlap with
  arXiv:1908.01224
- **Journal**: None
- **Summary**: We present Smooth Grad-CAM++, a technique which combines two recent techniques: SMOOTHGRAD and Grad-CAM++. Smooth Grad-CAM++ has the capability of either visualizing a layer, subset of feature maps, or subset of neurons within a feature map at each instance. We experimented with few images, and we discovered that Smooth Grad-CAM++ produced more visually sharp maps with larger number of salient pixels highlighted in the given input images when compared with other methods. Smooth Grad-CAM++ will give insight into what our deep CNN models (including models trained on medical scan or imagery) learn. Hence informing decisions on creating a representative training set.



### A deep learning based tool for automatic brain extraction from functional magnetic resonance images in rodents
- **Arxiv ID**: http://arxiv.org/abs/1912.01359v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1912.01359v2)
- **Published**: 2019-12-03 13:35:03+00:00
- **Updated**: 2019-12-06 00:11:09+00:00
- **Authors**: Sidney Pontes-Filho, Annelene Gulden Dahl, Stefano Nichele, Gustavo Borges Moreno e Mello
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Removing skull artifacts from functional magnetic images (fMRI) is a well understood and frequently encountered problem. Because the fMRI field has grown mostly due to human studies, many new tools were developed to handle human data. Nonetheless, these tools are not equally useful to handle the data derived from animal studies, especially from rodents. This represents a major problem to the field because rodent studies generate larger datasets from larger populations, which implies that preprocessing these images manually to remove the skull becomes a bottleneck in the data analysis pipeline. In this study, we address this problem by implementing a neural network based method that uses a U-Net architecture to segment the brain area into a mask and removing the skull and other tissues from the image. We demonstrate several strategies to speed up the process of generating the training dataset using watershedding and several strategies for data augmentation that allowed to train faster the U-Net to perform the segmentation. Finally, we deployed the trained network freely available.



### Multi-Channel Volumetric Neural Network for Knee Cartilage Segmentation in Cone-beam CT
- **Arxiv ID**: http://arxiv.org/abs/1912.01362v1
- **DOI**: 10.1007/978-3-658-29267-6_14
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01362v1)
- **Published**: 2019-12-03 13:43:46+00:00
- **Updated**: 2019-12-03 13:43:46+00:00
- **Authors**: Jennifer Maier, Luis Carlos Rivera Monroy, Christopher Syben, Yejin Jeon, Jang-Hwan Choi, Mary Elizabeth Hall, Marc Levenston, Garry Gold, Rebecca Fahrig, Andreas Maier
- **Comment**: 6 pages, accepted at BVM 2020
- **Journal**: None
- **Summary**: Analyzing knee cartilage thickness and strain under load can help to further the understanding of the effects of diseases like Osteoarthritis. A precise segmentation of the cartilage is a necessary prerequisite for this analysis. This segmentation task has mainly been addressed in Magnetic Resonance Imaging, and was rarely investigated on contrast-enhanced Computed Tomography, where contrast agent visualizes the border between femoral and tibial cartilage. To overcome the main drawback of manual segmentation, namely its high time investment, we propose to use a 3D Convolutional Neural Network for this task. The presented architecture consists of a V-Net with SeLu activation, and a Tversky loss function. Due to the high imbalance between very few cartilage pixels and many background pixels, a high false positive rate is to be expected. To reduce this rate, the two largest segmented point clouds are extracted using a connected component analysis, since they most likely represent the medial and lateral tibial cartilage surfaces. The resulting segmentations are compared to manual segmentations, and achieve on average a recall of 0.69, which confirms the feasibility of this approach.



### Multi-Objective Evolutionary Design of Deep Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1912.01369v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1912.01369v3)
- **Published**: 2019-12-03 13:57:25+00:00
- **Updated**: 2020-09-15 13:35:27+00:00
- **Authors**: Zhichao Lu, Ian Whalen, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, Wolfgang Banzhaf, Vishnu Naresh Boddeti
- **Comment**: Published in IEEE Transactions on Evolutionary Computation, 23 pages
- **Journal**: None
- **Summary**: Early advancements in convolutional neural networks (CNNs) architectures are primarily driven by human expertise and by elaborate design processes. Recently, neural architecture search was proposed with the aim of automating the network design process and generating task-dependent architectures. While existing approaches have achieved competitive performance in image classification, they are not well suited to problems where the computational budget is limited for two reasons: (1) the obtained architectures are either solely optimized for classification performance, or only for one deployment scenario; (2) the search process requires vast computational resources in most approaches. To overcome these limitations, we propose an evolutionary algorithm for searching neural architectures under multiple objectives, such as classification performance and floating-point operations (FLOPs). The proposed method addresses the first shortcoming by populating a set of architectures to approximate the entire Pareto frontier through genetic operations that recombine and modify architectural components progressively. Our approach improves computational efficiency by carefully down-scaling the architectures during the search as well as reinforcing the patterns commonly shared among past successful architectures through Bayesian model learning. The integration of these two main contributions allows an efficient design of architectures that are competitive and in most cases outperform both manually and automatically designed architectures on benchmark image classification datasets: CIFAR, ImageNet, and human chest X-ray. The flexibility provided from simultaneously obtaining multiple architecture choices for different compute requirements further differentiates our approach from other methods in the literature. Code is available at https://github.com/mikelzc1990/nsganetv1



### Robust Morph-Detection at Automated Border Control Gate using Deep Decomposed 3D Shape and Diffuse Reflectance
- **Arxiv ID**: http://arxiv.org/abs/1912.01372v1
- **DOI**: None
- **Categories**: **cs.CV**, I.5.4, I.4.9, I.2.10, I.5.4; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.01372v1)
- **Published**: 2019-12-03 14:02:40+00:00
- **Updated**: 2019-12-03 14:02:40+00:00
- **Authors**: Jag Mohan Singh, Raghavendra Ramachandra, Kiran B. Raja, Christoph Busch
- **Comment**: This work was accepted in The 15th International Conference on SIGNAL
  IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS, 2019
- **Journal**: None
- **Summary**: Face recognition is widely employed in Automated Border Control (ABC) gates, which verify the face image on passport or electronic Machine Readable Travel Document (eMTRD) against the captured image to confirm the identity of the passport holder. In this paper, we present a robust morph detection algorithm that is based on differential morph detection. The proposed method decomposes the bona fide image captured from the ABC gate and the digital face image extracted from the eMRTD into the diffuse reconstructed image and a quantized normal map. The extracted features are further used to learn a linear classifier (SVM) to detect a morphing attack based on the assessment of differences between the bona fide image from the ABC gate and the digital face image extracted from the passport. Owing to the availability of multiple cameras within an ABC gate, we extend the proposed method to fuse the classification scores to generate the final decision on morph-attack-detection. To validate our proposed algorithm, we create a morph attack database with overall 588 images, where bona fide are captured in an indoor lighting environment with a Canon DSLR Camera with one sample per subject and correspondingly images from ABC gates. We benchmark our proposed method with the existing state-of-the-art and can state that the new approach significantly outperforms previous approaches in the ABC gate scenario.



### Automatic Video Object Segmentation via Motion-Appearance-Stream Fusion and Instance-aware Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.01373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01373v1)
- **Published**: 2019-12-03 14:02:59+00:00
- **Updated**: 2019-12-03 14:02:59+00:00
- **Authors**: Sungkwon Choo, Wonkyo Seo, Nam Ik Cho
- **Comment**: 8+1 pages, 5 figures
- **Journal**: None
- **Summary**: This paper presents a method for automatic video object segmentation based on the fusion of motion stream, appearance stream, and instance-aware segmentation. The proposed scheme consists of a two-stream fusion network and an instance segmentation network. The two-stream fusion network again consists of motion and appearance stream networks, which extract long-term temporal and spatial information, respectively. Unlike the existing two-stream fusion methods, the proposed fusion network blends the two streams at the original resolution for obtaining accurate segmentation boundary. We develop a recurrent bidirectional multiscale structure with skip connection for the stream fusion network to extract long-term temporal information. Also, the multiscale structure enables to obtain the original resolution features at the end of the network. As a result of two-stream fusion, we have a pixel-level probabilistic segmentation map, which has higher values at the pixels belonging to the foreground object. By combining the probability of foreground map and objectness score of instance segmentation mask, we finally obtain foreground segmentation results for video sequences without any user intervention, i.e., we achieve successful automatic video segmentation. The proposed structure shows a state-of-the-art performance for automatic video object segmentation task, and also achieves near semi-supervised performance.



### RGPNet: A Real-Time General Purpose Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.01394v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01394v2)
- **Published**: 2019-12-03 14:24:55+00:00
- **Updated**: 2020-12-15 11:19:38+00:00
- **Authors**: Elahe Arani, Shabbir Marzban, Andrei Pata, Bahram Zonooz
- **Comment**: Accepted at IEEE Winter Conference on Applications of Computer Vision
  (WACV, 2021)
- **Journal**: None
- **Summary**: We propose a real-time general purpose semantic segmentation architecture, RGPNet, which achieves significant performance gain in complex environments. RGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor. The adaptor helps preserve and refine the abstract concepts from multiple levels of distributed representations between the encoder and decoder. It also facilitates the gradient flow from deeper layers to shallower layers. Our experiments demonstrate that RGPNet can generate segmentation results in real-time with comparable accuracy to the state-of-the-art non-real-time heavy models. Moreover, towards green AI, we show that using an optimized label-relaxation technique with progressive resizing can reduce the training time by up to 60% while preserving the performance. We conclude that RGPNet obtains a better speed-accuracy trade-off across multiple datasets.



### The Analysis of Projective Transformation Algorithms for Image Recognition on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1912.01401v1
- **DOI**: 10.1117/12.2559732
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01401v1)
- **Published**: 2019-12-03 14:27:38+00:00
- **Updated**: 2019-12-03 14:27:38+00:00
- **Authors**: Anton Trusov, Elena Limonova
- **Comment**: ICMV 2019
- **Journal**: None
- **Summary**: In this work we apply commonly known methods of non-adaptive interpolation (nearest pixel, bilinear, B-spline, bicubic, Hermite spline) and sampling (point sampling, supersampling, mip-map pre-filtering, rip-map pre-filtering and FAST) to the problem of projective image transformation. We compare their computational complexity, describe their artifacts and than experimentally measure their quality and working time on mobile processor with ARM architecture. Those methods were widely developed in the 90s and early 2000s, but were not in an area of active research in resent years due to a lower need in computationally efficient algorithms. However, real-time mobile recognition systems, which collect more and more attention, do not only require fast projective transform methods, but also demand high quality images without artifacts. As a result, in this work we choose methods appropriate for those systems, which allow to avoid artifacts, while preserving low computational complexity. Based on the experimental results for our setting they are bilinear interpolation combined with either mip-map pre-filtering or FAST sampling, but could be modified for specific use cases.



### Detecting Finger-Vein Presentation Attacks Using 3D Shape & Diffuse Reflectance Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1912.01408v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.5.4, I.4.9, I.2.10, I.5.4; I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1912.01408v1)
- **Published**: 2019-12-03 14:31:49+00:00
- **Updated**: 2019-12-03 14:31:49+00:00
- **Authors**: Jag Mohan Singh, Sushma Venkatesh, Kiran B. Raja, Raghavendra Ramachandra, Christoph Busch
- **Comment**: This work was accepted in The 15th International Conference on SIGNAL
  IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS, 2019
- **Journal**: None
- **Summary**: Despite the high biometric performance, finger-vein recognition systems are vulnerable to presentation attacks (aka., spoofing attacks). In this paper, we present a new and robust approach for detecting presentation attacks on finger-vein biometric systems exploiting the 3D Shape (normal-map) and material properties (diffuse-map) of the finger. Observing the normal-map and diffuse-map exhibiting enhanced textural differences in comparison with the original finger-vein image, especially in the presence of varying illumination intensity, we propose to employ textural feature-descriptors on both of them independently. The features are subsequently used to compute a separating hyper-plane using Support Vector Machine (SVM) classifiers for the features computed from normal-maps and diffuse-maps independently. Given the scores from each classifier for normal-map and diffuse-map, we propose sum-rule based score level fusion to make detection of such presentation attack more robust. To this end, we construct a new database of finger-vein images acquired using a custom capture device with three inbuilt illuminations and validate the applicability of the proposed approach. The newly collected database consists of 936 images, which corresponds to 468 bona fide images and 468 artefact images. We establish the superiority of the proposed approach by benchmarking it with classical textural feature-descriptor applied directly on finger-vein images. The proposed approach outperforms the classical approaches by providing the Attack Presentation Classification Error Rate (APCER) & Bona fide Presentation Classification Error Rate (BPCER) of 0% compared to comparable traditional methods.



### FlowNet3D++: Geometric Losses For Deep Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1912.01438v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.01438v3)
- **Published**: 2019-12-03 14:53:56+00:00
- **Updated**: 2021-04-26 14:00:11+00:00
- **Authors**: Zirui Wang, Shuda Li, Henry Howard-Jenkins, Victor Adrian Prisacariu, Min Chen
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: We present FlowNet3D++, a deep scene flow estimation network. Inspired by classical methods, FlowNet3D++ incorporates geometric constraints in the form of point-to-plane distance and angular alignment between individual vectors in the flow field, into FlowNet3D. We demonstrate that the addition of these geometric loss terms improves the previous state-of-art FlowNet3D accuracy from 57.85% to 63.43%. To further demonstrate the effectiveness of our geometric constraints, we propose a benchmark for flow estimation on the task of dynamic 3D reconstruction, thus providing a more holistic and practical measure of performance than the breakdown of individual metrics previously used to evaluate scene flow. This is made possible through the contribution of a novel pipeline to integrate point-based scene flow predictions into a global dense volume. FlowNet3D++ achieves up to a 15.0% reduction in reconstruction error over FlowNet3D, and up to a 35.2% improvement over KillingFusion alone. We will release our scene flow estimation code later.



### Multiscale Self Attentive Convolutions for Vision and Language Modeling
- **Arxiv ID**: http://arxiv.org/abs/1912.01521v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01521v1)
- **Published**: 2019-12-03 16:51:09+00:00
- **Updated**: 2019-12-03 16:51:09+00:00
- **Authors**: Oren Barkan
- **Comment**: None
- **Journal**: None
- **Summary**: Self attention mechanisms have become a key building block in many state-of-the-art language understanding models. In this paper, we show that the self attention operator can be formulated in terms of 1x1 convolution operations. Following this observation, we propose several novel operators: First, we introduce a 2D version of self attention that is applicable for 2D signals such as images. Second, we present the 1D and 2D Self Attentive Convolutions (SAC) operator that generalizes self attention beyond 1x1 convolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self attention operate on individual words and pixels, SAC operates on m-grams and image patches, respectively. Third, we present a multiscale version of SAC (MSAC) which analyzes the input by employing multiple SAC operators that vary by filter size, in parallel. Finally, we explain how MSAC can be utilized for vision and language modeling, and further harness MSAC to form a cross attentive image similarity machinery.



### Convolutional STN for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1912.01522v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01522v2)
- **Published**: 2019-12-03 16:51:11+00:00
- **Updated**: 2020-12-01 23:16:53+00:00
- **Authors**: Akhil Meethal, Marco Pedersoli, Soufiane Belharbi, Eric Granger
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Weakly supervised object localization is a challenging task in which the object of interest should be localized while learning its appearance. State-of-the-art methods recycle the architecture of a standard CNN by using the activation maps of the last layer for localizing the object. While this approach is simple and works relatively well, object localization relies on different features than classification, thus, a specialized localization mechanism is required during training to improve performance. In this paper, we propose a convolutional, multi-scale spatial localization network that provides accurate localization for the object of interest. Experimental results on CUB-200-2011 and ImageNet datasets show that our proposed approach provides competitive performance for weakly supervised localization.



### Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia
- **Arxiv ID**: http://arxiv.org/abs/1912.01526v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01526v5)
- **Published**: 2019-12-03 16:58:14+00:00
- **Updated**: 2021-09-29 11:54:04+00:00
- **Authors**: Daniele Ravi, Stefano B. Blumberg, Silvia Ingala, Frederik Barkhof, Daniel C. Alexander, Neil P. Oxtoby
- **Comment**: Paper Accepted at Medical Image Analysis - Journal - Elsevier
- **Journal**: None
- **Summary**: Accurate and realistic simulation of high-dimensional medical images has become an important research area relevant to many AI-enabled healthcare applications. However, current state-of-the-art approaches lack the ability to produce satisfactory high-resolution and accurate subject-specific images. In this work, we present a deep learning framework, namely 4D-Degenerative Adversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution, longitudinal MRI scans that mimic subject-specific neurodegeneration in ageing and dementia. 4D-DANI-Net is a modular framework based on adversarial training and a set of novel spatiotemporal, biologically-informed constraints. To ensure efficient training and overcome memory limitations affecting such high-dimensional problems, we rely on three key technological advances: i) a new 3D training consistency mechanism called Profile Weight Functions (PWFs), ii) a 3D super-resolution module and iii) a transfer learning strategy to fine-tune the system for a given individual. To evaluate our approach, we trained the framework on 9852 T1-weighted MRI scans from 876 participants in the Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate test set of 1283 MRI scans from 170 participants for quantitative and qualitative assessment of the personalised time series of synthetic images. We performed three evaluations: i) image quality assessment; ii) quantifying the accuracy of regional brain volumes over and above benchmark models; and iii) quantifying visual perception of the synthetic images by medical experts. Overall, both quantitative and qualitative results show that 4D-DANI-Net produces realistic, low-artefact, personalised time series of synthetic T1 MRI that outperforms benchmark models.



### Quantifying Urban Canopy Cover with Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.02109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.02109v1)
- **Published**: 2019-12-03 17:14:53+00:00
- **Updated**: 2019-12-03 17:14:53+00:00
- **Authors**: Bill Cai, Xiaojiang Li, Carlo Ratti
- **Comment**: NeurIPS 2019 Workshop on Climate Change AI at Vancouver, British
  Columbia, Canada. arXiv admin note: text overlap with arXiv:1808.04754
- **Journal**: None
- **Summary**: Urban canopy cover is important to mitigate the impact of climate change. Yet, existing quantification of urban greenery is either manual and not scalable, or use traditional computer vision methods that are inaccurate. We train deep convolutional neural networks (DCNNs) on datasets used for self-driving cars to estimate urban greenery instead, and find that our semantic segmentation and direct end-to-end estimation method are more accurate and scalable, reducing mean absolute error of estimating the Green View Index (GVI) metric from 10.1% to 4.67%. With the revised DCNN methods, the Treepedia project was able to scale and analyze canopy cover in 22 cities internationally, sparking interest and action in public policy and research fields.



### QUEST: Quantized embedding space for transferring knowledge
- **Arxiv ID**: http://arxiv.org/abs/1912.01540v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1912.01540v2)
- **Published**: 2019-12-03 17:38:40+00:00
- **Updated**: 2020-07-17 18:34:19+00:00
- **Authors**: Himalaya Jain, Spyros Gidaris, Nikos Komodakis, Patrick Pérez, Matthieu Cord
- **Comment**: Accepted at ECCV 2020
- **Journal**: None
- **Summary**: Knowledge distillation refers to the process of training a compact student network to achieve better accuracy by learning from a high capacity teacher network. Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher's output, feature maps or their distribution. In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized space. According to our method, the teacher's feature maps are quantized to represent the main visual concepts encompassed in the feature maps. The student is then asked to predict the quantized representation, which thus forms the task that the student uses to learn from the teacher. Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation. To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets.



### Learning Spatially Structured Image Transformations Using Planar Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1912.01553v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01553v2)
- **Published**: 2019-12-03 17:54:35+00:00
- **Updated**: 2020-08-10 00:46:43+00:00
- **Authors**: Joel Michelson, Joshua H. Palmer, Aneesha Dasari, Maithilee Kunda
- **Comment**: None
- **Journal**: None
- **Summary**: Learning image transformations is essential to the idea of mental simulation as a method of cognitive inference. We take a connectionist modeling approach, using planar neural networks to learn fundamental imagery transformations, like translation, rotation, and scaling, from perceptual experiences in the form of image sequences. We investigate how variations in network topology, training data, and image shape, among other factors, affect the efficiency and effectiveness of learning visual imagery transformations, including effectiveness of transfer to operating on new types of data.



### EventGAN: Leveraging Large Scale Image Datasets for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1912.01584v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01584v2)
- **Published**: 2019-12-03 18:29:49+00:00
- **Updated**: 2019-12-19 10:07:36+00:00
- **Authors**: Alex Zihao Zhu, Ziyun Wang, Kaung Khant, Kostas Daniilidis
- **Comment**: 10 pages, 5 figures, 2 tables, Code:
  https://github.com/alexzzhu/EventGAN, Video:
  https://www.youtube.com/watch?v=Vcm4Iox4H2w
- **Journal**: None
- **Summary**: Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both of these networks. Trained fully end to end, our network learns a generative model for events from images without the need for accurate modeling of the motion in the scene, exhibited by modeling based methods, while also implicitly modeling event noise. Using this simulator, we train a pair of downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets, and demonstrate the networks' abilities to generalize to datasets with real events.



### LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition
- **Arxiv ID**: http://arxiv.org/abs/1912.01601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01601v1)
- **Published**: 2019-12-03 18:54:50+00:00
- **Updated**: 2019-12-03 18:54:50+00:00
- **Authors**: Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, Larry S. Davis
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: This paper presents LiteEval, a simple yet effective coarse-to-fine framework for resource efficient video recognition, suitable for both online and offline scenarios. Exploiting decent yet computationally efficient features derived at a coarse scale with a lightweight CNN model, LiteEval dynamically decides on-the-fly whether to compute more powerful features for incoming video frames at a finer scale to obtain more details. This is achieved by a coarse LSTM and a fine LSTM operating cooperatively, as well as a conditional gating module to learn when to allocate more computation. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate LiteEval requires substantially less computation while offering excellent classification accuracy for both online and offline predictions.



### YOLACT++: Better Real-time Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1912.06218v2
- **DOI**: 10.1109/TPAMI.2020.3014297
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1912.06218v2)
- **Published**: 2019-12-03 18:58:03+00:00
- **Updated**: 2020-09-24 03:42:52+00:00
- **Authors**: Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee
- **Comment**: Journal extension of our previous conference paper arXiv:1904.02689
- **Journal**: None
- **Summary**: We present a simple, fully-convolutional model for real-time (>30 fps) instance segmentation that achieves competitive results on MS COCO evaluated on a single Titan Xp, which is significantly faster than any previous state-of-the-art approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. We also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty. Finally, by incorporating deformable convolutions into the backbone network, optimizing the prediction head with better anchor scales and aspect ratios, and adding a novel fast mask re-scoring branch, our YOLACT++ model can achieve 34.1 mAP on MS COCO at 33.5 fps, which is fairly close to the state-of-the-art approaches while still running at real-time.



### Visual Illusions Also Deceive Convolutional Neural Networks: Analysis and Implications
- **Arxiv ID**: http://arxiv.org/abs/1912.01643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01643v1)
- **Published**: 2019-12-03 19:33:30+00:00
- **Updated**: 2019-12-03 19:33:30+00:00
- **Authors**: A. Gomez-Villa, A. Martín, J. Vazquez-Corral, M. Bertalmío, J. Malo
- **Comment**: None
- **Journal**: None
- **Summary**: Visual illusions allow researchers to devise and test new models of visual perception. Here we show that artificial neural networks trained for basic visual tasks in natural images are deceived by brightness and color illusions, having a response that is qualitatively very similar to the human achromatic and chromatic contrast sensitivity functions, and consistent with natural image statistics. We also show that, while these artificial networks are deceived by illusions, their response might be significantly different to that of humans. Our results suggest that low-level illusions appear in any system that has to perform basic visual tasks in natural environments, in line with error minimization explanations of visual function, and they also imply a word of caution on using artificial networks to study human vision, as previously suggested in other contexts in the vision science literature.



### Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1912.01652v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.01652v2)
- **Published**: 2019-12-03 19:47:01+00:00
- **Updated**: 2020-03-04 06:30:01+00:00
- **Authors**: Eric Heiden, Ziang Liu, Ragesh K. Ramachandran, Gaurav S. Sukhatme
- **Comment**: Published at ICRA 2020
- **Journal**: None
- **Summary**: Light Detection and Ranging (LIDAR) sensors play an important role in the perception stack of autonomous robots, supplying mapping and localization pipelines with depth measurements of the environment. While their accuracy outperforms other types of depth sensors, such as stereo or time-of-flight cameras, the accurate modeling of LIDAR sensors requires laborious manual calibration that typically does not take into account the interaction of laser light with different surface types, incidence angles and other phenomena that significantly influence measurements. In this work, we introduce a physically plausible model of a 2D continuous-wave LIDAR that accounts for the surface-light interactions and simulates the measurement process in the Hokuyo URG-04LX LIDAR. Through automatic differentiation, we employ gradient-based optimization to estimate model parameters from real sensor measurements.



### Integrating Motion into Vision Models for Better Visual Prediction
- **Arxiv ID**: http://arxiv.org/abs/1912.01661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01661v1)
- **Published**: 2019-12-03 19:56:05+00:00
- **Updated**: 2019-12-03 19:56:05+00:00
- **Authors**: Michael Hazoglou, Todd Hylton
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate an improved vision system that learns a model of its environment using a self-supervised, predictive learning method. The system includes a pan-tilt camera, a foveated visual input, a saccading reflex to servo the foveated region to areas high prediction error, input frame transformation synced to the camera motion, and a recursive, hierachical machine learning technique based on the Predictive Vision Model. In earlier work, which did not integrate camera motion into the vision model, prediction was impaired and camera movement suffered from undesired feedback effects. Here we detail the integration of camera motion into the predictive learning system and show improved visual prediction and saccadic behavior. From these experiences, we speculate on the integration of additional sensory and motor systems into self-supervised, predictive learning models.



### A Survey of Black-Box Adversarial Attacks on Computer Vision Models
- **Arxiv ID**: http://arxiv.org/abs/1912.01667v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1912.01667v3)
- **Published**: 2019-12-03 20:06:49+00:00
- **Updated**: 2020-02-07 09:17:38+00:00
- **Authors**: Siddhant Bhambri, Sumanyu Muku, Avinash Tulasi, Arun Balaji Buduru
- **Comment**: 33 pages
- **Journal**: None
- **Summary**: Machine learning has seen tremendous advances in the past few years, which has lead to deep learning models being deployed in varied applications of day-to-day life. Attacks on such models using perturbations, particularly in real-life scenarios, pose a severe challenge to their applicability, pushing research into the direction which aims to enhance the robustness of these models. After the introduction of these perturbations by Szegedy et al. [1], significant amount of research has focused on the reliability of such models, primarily in two aspects - white-box, where the adversary has access to the targeted model and related parameters; and the black-box, which resembles a real-life scenario with the adversary having almost no knowledge of the model to be attacked. To provide a comprehensive security cover, it is essential to identify, study, and build defenses against such attacks. Hence, in this paper, we propose to present a comprehensive comparative study of various black-box adversarial attacks and defense techniques.



### Learning to Separate: Detecting Heavily-Occluded Objects in Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1912.01674v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01674v3)
- **Published**: 2019-12-03 20:21:21+00:00
- **Updated**: 2020-07-19 13:41:35+00:00
- **Authors**: Chenhongyi Yang, Vitaly Ablavsky, Kaihong Wang, Qi Feng, Margrit Betke
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: While visual object detection with deep learning has received much attention in the past decade, cases when heavy intra-class occlusions occur have not been studied thoroughly. In this work, we propose a Non-Maximum-Suppression (NMS) algorithm that dramatically improves the detection recall while maintaining high precision in scenes with heavy occlusions. Our NMS algorithm is derived from a novel embedding mechanism, in which the semantic and geometric features of the detected boxes are jointly exploited. The embedding makes it possible to determine whether two heavily-overlapping boxes belong to the same object in the physical world. Our approach is particularly useful for car detection and pedestrian detection in urban scenes where occlusions often happen. We show the effectiveness of our approach by creating a model called SG-Det (short for Semantics and Geometry Detection) and testing SG-Det on two widely-adopted datasets, KITTI and CityPersons for which it achieves state-of-the-art performance.



### Maximum entropy methods for texture synthesis: theory and practice
- **Arxiv ID**: http://arxiv.org/abs/1912.01691v1
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, math.PR, stat.CO, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1912.01691v1)
- **Published**: 2019-12-03 21:36:44+00:00
- **Updated**: 2019-12-03 21:36:44+00:00
- **Authors**: Valentin De Bortoli, Agnes Desolneux, Alain Durmus, Bruno Galerne, Arthur Leclaire
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have seen the rise of convolutional neural network techniques in exemplar-based image synthesis. These methods often rely on the minimization of some variational formulation on the image space for which the minimizers are assumed to be the solutions of the synthesis problem. In this paper we investigate, both theoretically and experimentally, another framework to deal with this problem using an alternate sampling/minimization scheme. First, we use results from information geometry to assess that our method yields a probability measure which has maximum entropy under some constraints in expectation. Then, we turn to the analysis of our method and we show, using recent results from the Markov chain literature, that its error can be explicitly bounded with constants which depend polynomially in the dimension even in the non-convex setting. This includes the case where the constraints are defined via a differentiable neural network. Finally, we present an extensive experimental study of the model, including a comparison with state-of-the-art methods and an extension to style transfer.



### Robustness-Driven Exploration with Probabilistic Metric Temporal Logic
- **Arxiv ID**: http://arxiv.org/abs/1912.01704v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1912.01704v1)
- **Published**: 2019-12-03 22:06:22+00:00
- **Updated**: 2019-12-03 22:06:22+00:00
- **Authors**: Xiaotian Liu, Pengyi Shi, Sarra Alqahtani, Victor Paúl Pauca, Miles Silman
- **Comment**: 10 pages, 8 figures, submitted to AAMAS 2020 for review
- **Journal**: None
- **Summary**: The ability to perform autonomous exploration is essential for unmanned aerial vehicles (UAV) operating in unstructured or unknown environments where it is hard or even impossible to describe the environment beforehand. However, algorithms for autonomous exploration often focus on optimizing time and coverage in a greedy fashion. That type of exploration can collect irrelevant data and wastes time navigating areas with no important information. In this paper, we propose a method for exploiting the discovered knowledge about the environment while exploring it by relying on a theory of robustness based on Probabilistic Metric Temporal Logic (P-MTL) as applied to offline verification and online control of hybrid systems. By maximizing the satisfaction of the predefined P-MTL specifications of the exploration problem, the robustness values guide the UAV towards areas with more interesting information to gain. We use Markov Chain Monte Carlo to solve the P-MTL constraints. We demonstrate the effectiveness of the proposed approach by simulating autonomous exploration over Amazonian rainforest where our approach is used to detect areas occupied by illegal Artisanal Small-scale Gold Mining (ASGM) activities. The results show that our approach outperform a greedy exploration approach (Autonomous Exploration Planner) by 38% in terms of ASGM coverage.



### It GAN DO Better: GAN-based Detection of Objects on Images with Varying Quality
- **Arxiv ID**: http://arxiv.org/abs/1912.01707v1
- **DOI**: 10.1109/TIP.2021.3124155
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01707v1)
- **Published**: 2019-12-03 22:10:07+00:00
- **Updated**: 2019-12-03 22:10:07+00:00
- **Authors**: Charan D. Prakash, Lina J. Karam
- **Comment**: None
- **Journal**: Published in the IEEE Transactions on Image Processing, Vol. 30,
  2021, pages 9220-9230
- **Summary**: In this paper, we propose in our novel generative framework the use of Generative Adversarial Networks (GANs) to generate features that provide robustness for object detection on reduced quality images. The proposed GAN-based Detection of Objects (GAN-DO) framework is not restricted to any particular architecture and can be generalized to several deep neural network (DNN) based architectures. The resulting deep neural network maintains the exact architecture as the selected baseline model without adding to the model parameter complexity or inference speed. We first evaluate the effect of image quality not only on the object classification but also on the object bounding box regression. We then test the models resulting from our proposed GAN-DO framework, using two state-of-the-art object detection architectures as the baseline models. We also evaluate the effect of the number of re-trained parameters in the generator of GAN-DO on the accuracy of the final trained model. Performance results provided using GAN-DO on object detection datasets establish an improved robustness to varying image quality and a higher mAP compared to the existing approaches.



### Deep Learning based Switching Filter for Impulsive Noise Removal in Color Images
- **Arxiv ID**: http://arxiv.org/abs/1912.01721v1
- **DOI**: 10.3390/s20102782
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1912.01721v1)
- **Published**: 2019-12-03 22:23:00+00:00
- **Updated**: 2019-12-03 22:23:00+00:00
- **Authors**: Krystian Radlak, Lukasz Malinski, Bogdan Smolka
- **Comment**: None
- **Journal**: Sensors 20 (2020) 2782
- **Summary**: Noise reduction is one the most important and still active research topic in low-level image processing due to its high impact on object detection and scene understanding for computer vision systems. Recently, we can observe a substantial increase of interest in the application of deep learning algorithms in many computer vision problems due to its impressive capability of automatic feature extraction and classification. These methods have been also successfully applied in image denoising, significantly improving the performance, but most of the proposed approaches were designed for Gaussian noise suppression. In this paper, we present a switching filtering design intended for impulsive noise removal using deep learning. In the proposed method, the impulses are identified using a novel deep neural network architecture and noisy pixels are restored using the fast adaptive mean filter. The performed experiments show that the proposed approach is superior to the state-of-the-art filters designed for impulsive noise removal in digital color images.



### ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks
- **Arxiv ID**: http://arxiv.org/abs/1912.01734v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1912.01734v2)
- **Published**: 2019-12-03 23:18:59+00:00
- **Updated**: 2020-03-31 01:18:33+00:00
- **Authors**: Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox
- **Comment**: Computer Vision and Pattern Recognition (CVPR) 2020 ;
  https://askforalfred.com/
- **Journal**: None
- **Summary**: We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like "Rinse off a mug and place it in the coffee maker." and low-level language instructions like "Walk to the coffee maker on the right." ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.



