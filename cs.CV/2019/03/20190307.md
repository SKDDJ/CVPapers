# Arxiv Papers in cs.CV on 2019-03-07
### Understanding Ancient Coin Images
- **Arxiv ID**: http://arxiv.org/abs/1903.02665v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02665v2)
- **Published**: 2019-03-07 00:07:45+00:00
- **Updated**: 2019-03-13 21:58:09+00:00
- **Authors**: Jessica Cooper, Ognjen Arandjelovic
- **Comment**: 2019
- **Journal**: None
- **Summary**: In recent years, a range of problems within the broad umbrella of automatic, computer vision based analysis of ancient coins has been attracting an increasing amount of attention. Notwithstanding this research effort, the results achieved by the state of the art in the published literature remain poor and far from sufficiently well performing for any practical purpose. In the present paper we present a series of contributions which we believe will benefit the interested community. Firstly, we explain that the approach of visual matching of coins, universally adopted in all existing published papers on the topic, is not of practical interest because the number of ancient coin types exceeds by far the number of those types which have been imaged, be it in digital form (e.g. online) or otherwise (traditional film, in print, etc.). Rather, we argue that the focus should be on the understanding of the semantic content of coins. Hence, we describe a novel method which uses real-world multimodal input to extract and associate semantic concepts with the correct coin images and then using a novel convolutional neural network learn the appearance of these concepts. Empirical evidence on a real-world and by far the largest data set of ancient coins, we demonstrate highly promising results.



### Discovering Visual Patterns in Art Collections with Spatially-consistent Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.02678v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02678v2)
- **Published**: 2019-03-07 01:12:16+00:00
- **Updated**: 2019-03-08 09:16:27+00:00
- **Authors**: Xi Shen, Alexei A. Efros, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: Our goal in this paper is to discover near duplicate patterns in large collections of artworks. This is harder than standard instance mining due to differences in the artistic media (oil, pastel, drawing, etc), and imperfections inherent in the copying process. The key technical insight is to adapt a standard deep feature to this task by fine-tuning it on the specific art collection using self-supervised learning. More specifically, spatial consistency between neighbouring feature matches is used as supervisory fine-tuning signal. The adapted feature leads to more accurate style-invariant matching, and can be used with a standard discovery approach, based on geometric verification, to identify duplicate patterns in the dataset. The approach is evaluated on several different datasets and shows surprisingly good qualitative discovery results. For quantitative evaluation of the method, we annotated 273 near duplicate details in a dataset of 1587 artworks attributed to Jan Brueghel and his workshop. Beyond artwork, we also demonstrate improvement on localization on the Oxford5K photo dataset as well as on historical photograph localization on the Large Time Lags Location (LTLL) dataset.



### Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1903.02679v1
- **DOI**: 10.25919/5c495488b0f4e
- **Categories**: **cs.CV**, I.2.10; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1903.02679v1)
- **Published**: 2019-03-07 01:13:24+00:00
- **Updated**: 2019-03-07 01:13:24+00:00
- **Authors**: Shafeeq Elanattil, Peyman Moghadam
- **Comment**: More information at
  https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/
- **Journal**: None
- **Summary**: We introduce a synthetic dataset for evaluating non-rigid 3D human reconstruction based on conventional RGB-D cameras. The dataset consist of seven motion sequences of a single human model. For each motion sequence per-frame ground truth geometry and ground truth skeleton are given. The dataset also contains skinning weights of the human model. More information about the dataset can be found at: https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/



### Stratified Labeling for Surface Consistent Parallax Correction and Occlusion Completion
- **Arxiv ID**: http://arxiv.org/abs/1903.02688v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02688v2)
- **Published**: 2019-03-07 02:03:56+00:00
- **Updated**: 2019-03-23 04:43:25+00:00
- **Authors**: Jie Chen, Lap-Pui Chau, Junhui Hou
- **Comment**: None
- **Journal**: None
- **Summary**: The light field faithfully records the spatial and angular configurations of the scene, which facilitates a wide range of imaging possibilities. In this work, we propose an LF synthesis algorithm which renders high quality novel LF views far outside the range of angular baselines of the given references. A stratified synthesis strategy is adopted which parses the scene content based on stratified disparity layers and across a varying range of spatial granularities. Such a stratified methodology proves to help preserve scene structures over large perspective shifts, and it provides informative clues for inferring the textures of occluded regions. A generative-adversarial network model is further adopted for parallax correction and occlusion completion conditioned on the stratified synthesis features. Experiments show that our proposed model can provide more reliable novel view synthesis quality at large baseline extension ratios. Over 3dB quality improvement has been achieved against state-of-the-art LF view synthesis algorithms.



### Novel quantitative indicators of digital ophthalmoscopy image quality
- **Arxiv ID**: http://arxiv.org/abs/1903.02695v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML, 68T45, I.4.9; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1903.02695v1)
- **Published**: 2019-03-07 02:21:41+00:00
- **Updated**: 2019-03-07 02:21:41+00:00
- **Authors**: Chris von Csefalvay
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: With the advent of smartphone indirect ophthalmoscopy, teleophthalmology - the use of specialist ophthalmology assets at a distance from the patient - has experienced a breakthrough, promising enormous benefits especially for healthcare in distant, inaccessible or opthalmologically underserved areas, where specialists are either unavailable or too few in number. However, accurate teleophthalmology requires high-quality ophthalmoscopic imagery. This paper considers three feature families - statistical metrics, gradient-based metrics and wavelet transform coefficient derived indicators - as possible metrics to identify unsharp or blurry images. By using standard machine learning techniques, the suitability of these features for image quality assessment is confirmed, albeit on a rather small data set. With the increased availability and decreasing cost of digital ophthalmoscopy on one hand and the increased prevalence of diabetic retinopathy worldwide on the other, creating tools that can determine whether an image is likely to be diagnostically suitable can play a significant role in accelerating and streamlining the teleophthalmology process. This paper highlights the need for more research in this area, including the compilation of a diverse database of ophthalmoscopic imagery, annotated with quality markers, to train the Point of Acquisition error detection algorithms of the future.



### Robust Semantic Segmentation By Dense Fusion Network On Blurred VHR Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1903.02702v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02702v2)
- **Published**: 2019-03-07 02:53:13+00:00
- **Updated**: 2020-12-01 09:26:29+00:00
- **Authors**: Yi Peng, Shihao Sun, Zheng Wang, Yining Pan, Ruirui Li
- **Comment**: Submitted to BigDIA 2020
- **Journal**: None
- **Summary**: Robust semantic segmentation of VHR remote sensing images from UAV sensors is critical for earth observation, land use, land cover or mapping applications. Several factors such as shadows, weather disruption and camera shakes making this problem highly challenging, especially only using RGB images. In this paper, we propose the use of multi-modality data including NIR, RGB and DSM to increase robustness of segmentation in blurred or partially damaged VHR remote sensing images. By proposing a cascaded dense encoder-decoder network and the SELayer based fusion and assembling techniques, the proposed RobustDenseNet achieves steady performance when the image quality is decreasing, compared with the state-of-the-art semantic segmentation model.



### Alternating Phase Projected Gradient Descent with Generative Priors for Solving Compressive Phase Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.02707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02707v1)
- **Published**: 2019-03-07 03:03:14+00:00
- **Updated**: 2019-03-07 03:03:14+00:00
- **Authors**: Rakib Hyder, Viraj Shah, Chinmay Hegde, M. Salman Asif
- **Comment**: Published in ICASSP 2019
- **Journal**: None
- **Summary**: The classical problem of phase retrieval arises in various signal acquisition systems. Due to the ill-posed nature of the problem, the solution requires assumptions on the structure of the signal. In the last several years, sparsity and support-based priors have been leveraged successfully to solve this problem. In this work, we propose replacing the sparsity/support priors with generative priors and propose two algorithms to solve the phase retrieval problem. Our proposed algorithms combine the ideas from AltMin approach for non-convex sparse phase retrieval and projected gradient descent approach for solving linear inverse problems using generative priors. We empirically show that the performance of our method with projected gradient descent is superior to the existing approach for solving phase retrieval under generative priors. We support our method with an analysis of sample complexity with Gaussian measurements.



### Graphical Contrastive Losses for Scene Graph Parsing
- **Arxiv ID**: http://arxiv.org/abs/1903.02728v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02728v5)
- **Published**: 2019-03-07 05:07:43+00:00
- **Updated**: 2019-08-16 21:30:29+00:00
- **Authors**: Ji Zhang, Kevin J. Shih, Ahmed Elgammal, Andrew Tao, Bryan Catanzaro
- **Comment**: None
- **Journal**: None
- **Summary**: Most scene graph parsers use a two-stage pipeline to detect visual relationships: the first stage detects entities, and the second predicts the predicate for each entity pair using a softmax distribution. We find that such pipelines, trained with only a cross entropy loss over predicate classes, suffer from two common errors. The first, Entity Instance Confusion, occurs when the model confuses multiple instances of the same type of entity (e.g. multiple cups). The second, Proximal Relationship Ambiguity, arises when multiple subject-predicate-object triplets appear in close proximity with the same predicate, and the model struggles to infer the correct subject-object pairings (e.g. mis-pairing musicians and their instruments). We propose a set of contrastive loss formulations that specifically target these types of errors within the scene graph parsing problem, collectively termed the Graphical Contrastive Losses. These losses explicitly force the model to disambiguate related and unrelated instances through margin constraints specific to each type of confusion. We further construct a relationship detector, called RelDN, using the aforementioned pipeline to demonstrate the efficacy of our proposed losses. Our model outperforms the winning method of the OpenImages Relationship Detection Challenge by 4.7\% (16.5\% relative) on the test set. We also show improved results over the best previous methods on the Visual Genome and Visual Relationship Detection datasets.



### Integrating neural networks into the blind deblurring framework to compete with the end-to-end learning-based methods
- **Arxiv ID**: http://arxiv.org/abs/1903.02731v2
- **DOI**: 10.1109/TIP.2020.2994413
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02731v2)
- **Published**: 2019-03-07 05:20:25+00:00
- **Updated**: 2019-06-22 02:08:40+00:00
- **Authors**: Junde Wu, Xiaoguang Di, Jiehao Huang, Yu Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, end-to-end learning-based methods based on deep neural network (DNN) have been proven effective for blind deblurring. Without human-made assumptions and numerical algorithms, they are able to restore images with fewer artifacts and better perceptual quality. However, in practice, we also find some of their drawbacks. Without the theoretical guidance, these methods can not perform well when the motion is complex and sometimes generate unreasonable results. In this paper, for overcoming these drawbacks, we integrate deep convolution neural networks into conventional deblurring framework. Specifically, we build Stacked Estimation Residual Net (SEN) to estimate the motion flow map and Recurrent Prior Generative and Adversarial Net (RP-GAN) to learn the implicit image prior in the optimization model. Comparing with state-of-the-art end-to-end learning-based methods, our method restores reasonable details and shows better generalization ability.



### CE-Net: Context Encoder Network for 2D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.02740v1
- **DOI**: 10.1109/TMI.2019.2903562
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02740v1)
- **Published**: 2019-03-07 06:24:27+00:00
- **Updated**: 2019-03-07 06:24:27+00:00
- **Authors**: Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying Hao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, Jiang Liu
- **Comment**: accepted by IEEE transcations on medical imaging, (TMI)
- **Journal**: None
- **Summary**: Medical image segmentation is an important step in medical image analysis. With the rapid development of convolutional neural network in image processing, deep learning has been used for medical image segmentation, such as optic disc segmentation, blood vessel detection, lung segmentation, cell segmentation, etc. Previously, U-net based approaches have been proposed. However, the consecutive pooling and strided convolutional operations lead to the loss of some spatial information. In this paper, we propose a context encoder network (referred to as CE-Net) to capture more high-level information and preserve spatial information for 2D medical image segmentation. CE-Net mainly contains three major components: a feature encoder module, a context extractor and a feature decoder module. We use pretrained ResNet block as the fixed feature extractor. The context extractor module is formed by a newly proposed dense atrous convolution (DAC) block and residual multi-kernel pooling (RMP) block. We applied the proposed CE-Net to different 2D medical image segmentation tasks. Comprehensive results show that the proposed method outperforms the original U-Net method and other state-of-the-art methods for optic disc segmentation, vessel detection, lung segmentation, cell contour segmentation and retinal optical coherence tomography layer segmentation.



### RAVEN: A Dataset for Relational and Analogical Visual rEasoNing
- **Arxiv ID**: http://arxiv.org/abs/1903.02741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.02741v1)
- **Published**: 2019-03-07 06:28:44+00:00
- **Updated**: 2019-03-07 06:28:44+00:00
- **Authors**: Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu
- **Comment**: CVPR 2019 paper. Supplementary:
  http://wellyzhang.github.io/attach/cvpr19zhang_supp.pdf Project:
  http://wellyzhang.github.io/project/raven.html
- **Journal**: None
- **Summary**: Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still an enormous performance gap between artificial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven's Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.



### Using DP Towards A Shortest Path Problem-Related Application
- **Arxiv ID**: http://arxiv.org/abs/1903.02765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.02765v1)
- **Published**: 2019-03-07 08:11:15+00:00
- **Updated**: 2019-03-07 08:11:15+00:00
- **Authors**: Jianhao Jiao, Rui Fan, Han Ma, Ming Liu
- **Comment**: 8 pages, 8 figures, accepted by IEEE International Conference on
  Robotics and Automation (ICRA) 2019
- **Journal**: None
- **Summary**: The detection of curved lanes is still challenging for autonomous driving systems. Although current cutting-edge approaches have performed well in real applications, most of them are based on strict model assumptions. Similar to other visual recognition tasks, lane detection can be formulated as a two-dimensional graph searching problem, which can be solved by finding several optimal paths along with line segments and boundaries. In this paper, we present a directed graph model, in which dynamic programming is used to deal with a specific shortest path problem. This model is particularly suitable to represent objects with long continuous shape structure, e.g., lanes and roads. We apply the designed model and proposed an algorithm for detecting lanes by formulating it as the shortest path problem. To evaluate the performance of our proposed algorithm, we tested five sequences (including 1573 frames) from the KITTI database. The results showed that our method achieves an average successful detection precision of 97.5%.



### Hair Segmentation on Time-of-Flight RGBD Images
- **Arxiv ID**: http://arxiv.org/abs/1903.02775v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02775v3)
- **Published**: 2019-03-07 08:55:35+00:00
- **Updated**: 2020-04-20 14:01:42+00:00
- **Authors**: Yuanxi Ma, Cen Wang, Shiying Li, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Robust segmentation of hair from portrait images remains challenging: hair does not conform to a uniform shape, style or even color; dark hair in particular lacks features. We present a novel computational imaging solution that tackles the problem from both input and processing fronts. We explore using Time-of-Flight (ToF) RGBD sensors on recent mobile devices. We first conduct a comprehensive analysis to show that scattering and inter-reflection cause different noise patterns on hair vs. non-hair regions on ToF images, by changing the light path and/or combining multiple paths. We then develop a deep network based approach that employs both ToF depth map and the RGB gradient maps to produce an initial hair segmentation with labeled hair components. We then refine the result by imposing ToF noise prior under the conditional random field. We collect the first ToF RGBD hair dataset with 20k+ head images captured on 30 human subjects with a variety of hairstyles at different view angles. Comprehensive experiments show that our approach outperforms the RGB based techniques in accuracy and robustness and can handle traditionally challenging cases such as dark hair, similar hair/background, similar hair/foreground, etc.



### SR-LSTM: State Refinement for LSTM towards Pedestrian Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.02793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02793v1)
- **Published**: 2019-03-07 09:49:57+00:00
- **Updated**: 2019-03-07 09:49:57+00:00
- **Authors**: Pu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, Nanning Zheng
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: In crowd scenarios, reliable trajectory prediction of pedestrians requires insightful understanding of their social behaviors. These behaviors have been well investigated by plenty of studies, while it is hard to be fully expressed by hand-craft rules. Recent studies based on LSTM networks have shown great ability to learn social behaviors. However, many of these methods rely on previous neighboring hidden states but ignore the important current intention of the neighbors. In order to address this issue, we propose a data-driven state refinement module for LSTM network (SR-LSTM), which activates the utilization of the current intention of neighbors, and jointly and iteratively refines the current states of all participants in the crowd through a message passing mechanism. To effectively extract the social effect of neighbors, we further introduce a social-aware information selection mechanism consisting of an element-wise motion gate and a pedestrian-wise attention to select useful message from neighboring pedestrians. Experimental results on two public datasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed SR-LSTM and we achieves state-of-the-art results.



### Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification from the Bottom Up
- **Arxiv ID**: http://arxiv.org/abs/1903.02827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02827v1)
- **Published**: 2019-03-07 10:54:16+00:00
- **Updated**: 2019-03-07 10:54:16+00:00
- **Authors**: Weifeng Ge, Xiangru Lin, Yizhou Yu
- **Comment**: Accepted to appear in CVPR 2019
- **Journal**: None
- **Summary**: Given a training dataset composed of images and corresponding category labels, deep convolutional neural networks show a strong ability in mining discriminative parts for image classification. However, deep convolutional neural networks trained with image level labels only tend to focus on the most discriminative parts while missing other object parts, which could provide complementary information. In this paper, we approach this problem from a different perspective. We build complementary parts models in a weakly supervised manner to retrieve information suppressed by dominant object parts detected by convolutional neural networks. Given image level labels only, we first extract rough object instances by performing weakly supervised object detection and instance segmentation using Mask R-CNN and CRF-based segmentation. Then we estimate and search for the best parts model for each object instance under the principle of preserving as much diversity as possible. In the last stage, we build a bi-directional long short-term memory (LSTM) network to fuze and encode the partial information of these complementary parts into a comprehensive feature for image classification. Experimental results indicate that the proposed method not only achieves significant improvement over our baseline models, but also outperforms state-of-the-art algorithms by a large margin (6.7%, 2.8%, 5.2% respectively) on Stanford Dogs 120, Caltech-UCSD Birds 2011-200 and Caltech 256.



### Active Scene Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.02832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02832v1)
- **Published**: 2019-03-07 11:07:54+00:00
- **Updated**: 2019-03-07 11:07:54+00:00
- **Authors**: Erelcan Yanik, Tevfik Metin Sezgin
- **Comment**: To be submitted to the Pattern Recognition Journal
- **Journal**: None
- **Summary**: Sketch recognition allows natural and efficient interaction in pen-based interfaces. A key obstacle to building accurate sketch recognizers has been the difficulty of creating large amounts of annotated training data. Several authors have attempted to address this issue by creating synthetic data, and by building tools that support efficient annotation. Two prominent sets of approaches stand out from the rest of the crowd. They use interim classifiers trained with a small set of labeled data to aid the labeling of the remainder of the data. The first set of approaches uses a classifier trained with a partially labeled dataset to automatically label unlabeled instances. The others, based on active learning, save annotation effort by giving priority to labeling informative data instances. The former is sub-optimal since it doesn't prioritize the order of labeling to favor informative instances, while the latter makes the strong assumption that unlabeled data comes in an already segmented form (i.e. the ink in the training data is already assembled into groups forming isolated object instances). In this paper, we propose an active learning framework that combines the strengths of these methods, while addressing their weaknesses. In particular, we propose two methods for deciding how batches of unsegmented sketch scenes should be labeled. The first method, scene-wise selection, assesses the informativeness of each drawing (sketch scene) as a whole, and asks the user to annotate all objects in the drawing. The latter, segment-wise selection, attempts more precise targeting to locate informative fragments of drawings for user labeling. We show that both selection schemes outperform random selection. Furthermore, we demonstrate that precise targeting yields superior performance. Overall, our approach allows reaching top accuracy figures with up to 30% savings in annotation cost.



### Exploit fully automatic low-level segmented PET data for training high-level deep learning algorithms for the corresponding CT data
- **Arxiv ID**: http://arxiv.org/abs/1903.02871v1
- **DOI**: 10.1371/journal.pone.0212550
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02871v1)
- **Published**: 2019-03-07 12:26:03+00:00
- **Updated**: 2019-03-07 12:26:03+00:00
- **Authors**: Christina Gsaxner, Peter M. Roth, Jürgen Wallner, Jan Egger
- **Comment**: 20 pages
- **Journal**: PLoS ONE 14(3): e0212550 (2019)
- **Summary**: We present an approach for fully automatic urinary bladder segmentation in CT images with artificial neural networks in this study. Automatic medical image analysis has become an invaluable tool in the different treatment stages of diseases. Especially medical image segmentation plays a vital role, since segmentation is often the initial step in an image analysis pipeline. Since deep neural networks have made a large impact on the field of image processing in the past years, we use two different deep learning architectures to segment the urinary bladder. Both of these architectures are based on pre-trained classification networks that are adapted to perform semantic segmentation. Since deep neural networks require a large amount of training data, specifically images and corresponding ground truth labels, we furthermore propose a method to generate such a suitable training data set from Positron Emission Tomography/Computed Tomography image data. This is done by applying thresholding to the Positron Emission Tomography data for obtaining a ground truth and by utilizing data augmentation to enlarge the dataset. In this study, we discuss the influence of data augmentation on the segmentation results, and compare and evaluate the proposed architectures in terms of qualitative and quantitative segmentation performance. The results presented in this study allow concluding that deep neural networks can be considered a promising approach to segment the urinary bladder in CT images.



### COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.02874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02874v1)
- **Published**: 2019-03-07 12:32:32+00:00
- **Updated**: 2019-03-07 12:32:32+00:00
- **Authors**: Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, Jie Zhou
- **Comment**: CVPR2019, project page: https://coin-dataset.github.io/
- **Journal**: None
- **Summary**: There are substantial instructional videos on the Internet, which enables us to acquire knowledge for completing various tasks. However, most existing datasets for instructional video analysis have the limitations in diversity and scale,which makes them far from many real-world applications where more diverse activities occur. Moreover, it still remains a great challenge to organize and harness such data. To address these problems, we introduce a large-scale dataset called "COIN" for COmprehensive INstructional video analysis. Organized with a hierarchical structure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed toolbox, all the videos are annotated effectively with a series of step descriptions and the corresponding temporal boundaries. Furthermore, we propose a simple yet effective method to capture the dependencies among different steps, which can be easily plugged into conventional proposal-based action detection methods for localizing important steps in instructional videos. In order to provide a benchmark for instructional video analysis, we evaluate plenty of approaches on the COIN dataset under different evaluation criteria. We expect the introduction of the COIN dataset will promote the future in-depth research on instructional video analysis for the community.



### Ultrasound Image Representation Learning by Modeling Sonographer Visual Attention
- **Arxiv ID**: http://arxiv.org/abs/1903.02974v2
- **DOI**: 10.1007/978-3-030-20351-1_46
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1903.02974v2)
- **Published**: 2019-03-07 15:05:31+00:00
- **Updated**: 2019-05-27 11:55:31+00:00
- **Authors**: Richard Droste, Yifan Cai, Harshita Sharma, Pierre Chatelain, Lior Drukker, Aris T. Papageorghiou, J. Alison Noble
- **Comment**: Accepted at the international conference on Information Processing in
  Medical Imaging (IPMI) 2019
- **Journal**: None
- **Summary**: Image representations are commonly learned from class labels, which are a simplistic approximation of human image understanding. In this paper we demonstrate that transferable representations of images can be learned without manual annotations by modeling human visual attention. The basis of our analyses is a unique gaze tracking dataset of sonographers performing routine clinical fetal anomaly screenings. Models of sonographer visual attention are learned by training a convolutional neural network (CNN) to predict gaze on ultrasound video frames through visual saliency prediction or gaze-point regression. We evaluate the transferability of the learned representations to the task of ultrasound standard plane detection in two contexts. Firstly, we perform transfer learning by fine-tuning the CNN with a limited number of labeled standard plane images. We find that fine-tuning the saliency predictor is superior to training from random initialization, with an average F1-score improvement of 9.6% overall and 15.3% for the cardiac planes. Secondly, we train a simple softmax regression on the feature activations of each CNN layer in order to evaluate the representations independently of transfer learning hyper-parameters. We find that the attention models derive strong representations, approaching the precision of a fully-supervised baseline model for all but the last layer.



### Correction of Electron Back-scattered Diffraction datasets using an evolutionary algorithm
- **Arxiv ID**: http://arxiv.org/abs/1903.02982v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.ins-det
- **Links**: [PDF](http://arxiv.org/pdf/1903.02982v1)
- **Published**: 2019-03-07 15:21:44+00:00
- **Updated**: 2019-03-07 15:21:44+00:00
- **Authors**: Florian Strub, Marie-Agathe Charpagne, Tresa M. Pollock
- **Comment**: This short paper target an audience working in Machine Learning. A
  long version of this paper exists towards people working in Materials (more
  experiments, more experimental details and analysis), namely arXiv:1903.02988
- **Journal**: None
- **Summary**: In materials science and particularly electron microscopy, Electron Back-scatter Diffraction (EBSD) is a common and powerful mapping technique for collecting local crystallographic data at the sub-micron scale. The quality of the reconstruction of the maps is critical to study the spatial distribution of phases and crystallographic orientation relationships between phases, a key interest in materials science. However, EBSD data is known to suffer from distortions that arise from several instrument and detector artifacts. In this paper, we present an unsupervised method that corrects those distortions, and enables or enhances phase differentiation in EBSD data. The method uses a segmented electron image of the phases of interest (laths, precipitates, voids, inclusions) gathered using detectors that generate less distorted data, of the same area than the EBSD map, and then searches for the best transformation to correct the distortions of the initial EBSD data. To do so, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is implemented to distort the EBSD until it matches the reference electron image. Fast and versatile, this method does not require any human annotation and can be applied to large datasets and wide areas, where the distortions are important. Besides, this method requires very little assumption concerning the shape of the distortion function. Some application examples in multiphase materials with feature sizes down to 1 $\mu$m are presented, including a Titanium alloy and a Nickel-base superalloy.



### Attack Type Agnostic Perceptual Enhancement of Adversarial Images
- **Arxiv ID**: http://arxiv.org/abs/1903.03029v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03029v3)
- **Published**: 2019-03-07 16:35:03+00:00
- **Updated**: 2019-05-10 14:15:47+00:00
- **Authors**: Bilgin Aksoy, Alptekin Temizel
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial images are samples that are intentionally modified to deceive machine learning systems. They are widely used in applications such as CAPTHAs to help distinguish legitimate human users from bots. However, the noise introduced during the adversarial image generation process degrades the perceptual quality and introduces artificial colours; making it also difficult for humans to classify images and recognise objects. In this letter, we propose a method to enhance the perceptual quality of these adversarial images. The proposed method is attack type agnostic and could be used in association with the existing attacks in the literature. Our experiments show that the generated adversarial images have lower Euclidean distance values while maintaining the same adversarial attack performance. Distances are reduced by 5.88% to 41.27% with an average reduction of 22% over the different attack and network types.



### Characterization of Posidonia Oceanica Seagrass Aerenchyma through Whole Slide Imaging: A Pilot Study
- **Arxiv ID**: http://arxiv.org/abs/1903.03044v2
- **DOI**: None
- **Categories**: **q-bio.TO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03044v2)
- **Published**: 2019-03-07 17:01:32+00:00
- **Updated**: 2019-03-11 14:51:28+00:00
- **Authors**: Olivier Debeir, Justine Allard, Christine Decaestecker, Jean-Pierre Hermand
- **Comment**: None
- **Journal**: None
- **Summary**: Characterizing the tissue morphology and anatomy of seagrasses is essential to predicting their acoustic behavior. In this pilot study, we use histology techniques and whole slide imaging (WSI) to describe the composition and topology of the aerenchyma of an entire leaf blade in an automatic way combining the advantages of X-ray microtomography and optical microscopy. Paraffin blocks are prepared in such a way that microtome slices contain an arbitrarily large number of cross sections distributed along the full length of a blade. The sample organization in the paraffin block coupled with whole slide image analysis allows high throughput data extraction and an exhaustive characterization along the whole blade length. The core of the work are image processing algorithms that can identify cells and air lacunae (or void) from fiber strand, epidermis, mesophyll and vascular system. A set of specific features is developed to adequately describe the convexity of cells and voids where standard descriptors fail. The features scrutinize the local curvature of the object borders to allow an accurate discrimination between void and cell through machine learning. The algorithm allows to reconstruct the cells and cell membrane features that are relevant to tissue density, compressibility and rigidity. Size distribution of the different cell types and gas spaces, total biomass and total void volume fraction are then extracted from the high resolution slices to provide a complete characterization of the tissue along the leave from its base to the apex.



### Label Embedded Dictionary Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.03087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03087v2)
- **Published**: 2019-03-07 18:26:36+00:00
- **Updated**: 2019-04-17 07:40:05+00:00
- **Authors**: Shuai Shao, Yan-Jiang Wang, Bao-Di Liu, Weifeng Liu, Rui Xu
- **Comment**: 25pages, 12 figures
- **Journal**: None
- **Summary**: Recently, label consistent k-svd (LC-KSVD) algorithm has been successfully applied in image classification. The objective function of LC-KSVD is consisted of reconstruction error, classification error and discriminative sparse codes error with L0-norm sparse regularization term. The L0-norm, however, leads to NP-hard problem. Despite some methods such as orthogonal matching pursuit can help solve this problem to some extent, it is quite difficult to find the optimum sparse solution. To overcome this limitation, we propose a label embedded dictionary learning (LEDL) method to utilise the L1-norm as the sparse regularization term so that we can avoid the hard-to-optimize problem by solving the convex optimization problem. Alternating direction method of multipliers and blockwise coordinate descent algorithm are then exploited to optimize the corresponding objective function. Extensive experimental results on six benchmark datasets illustrate that the proposed algorithm has achieved superior performance compared to some conventional classification algorithms.



### A Learnable ScatterNet: Locally Invariant Convolutional Layers
- **Arxiv ID**: http://arxiv.org/abs/1903.03137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03137v1)
- **Published**: 2019-03-07 19:30:19+00:00
- **Updated**: 2019-03-07 19:30:19+00:00
- **Authors**: Fergal Cotter, Nick Kingsbury
- **Comment**: 4 pages, 1 Figure, pre-print of paper submitted to ICIP 2019
- **Journal**: None
- **Summary**: In this paper we explore tying together the ideas from Scattering Transforms and Convolutional Neural Networks (CNN) for Image Analysis by proposing a learnable ScatterNet. Previous attempts at tying them together in hybrid networks have tended to keep the two parts separate, with the ScatterNet forming a fixed front end and a CNN forming a learned backend. We instead look at adding learning between scattering orders, as well as adding learned layers before the ScatterNet. We do this by breaking down the scattering orders into single convolutional-like layers we call 'locally invariant' layers, and adding a learned mixing term to this layer. Our experiments show that these locally invariant layers can improve accuracy when added to either a CNN or a ScatterNet. We also discover some surprising results in that the ScatterNet may be best positioned after one or more layers of learning rather than at the front of a neural network.



### Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.03148v1
- **DOI**: 10.1109/CVPR.2018.00968
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03148v1)
- **Published**: 2019-03-07 19:51:03+00:00
- **Updated**: 2019-03-07 19:51:03+00:00
- **Authors**: Adrian V. Dalca, John Guttag, Mert R. Sabuncu
- **Comment**: Presented at CVPR 2018. IEEE CVPR proceedings pp. 9290-9299
- **Journal**: None
- **Summary**: We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior can enable fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code is freely available at http://github.com/adalca/neuron.



### Deep CNN-based Multi-task Learning for Open-Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.03161v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03161v1)
- **Published**: 2019-03-07 20:11:32+00:00
- **Updated**: 2019-03-07 20:11:32+00:00
- **Authors**: Poojan Oza, Vishal M. Patel
- **Comment**: Under Review
- **Journal**: None
- **Summary**: We propose a novel deep convolutional neural network (CNN) based multi-task learning approach for open-set visual recognition. We combine a classifier network and a decoder network with a shared feature extractor network within a multi-task learning framework. We show that this approach results in better open-set recognition accuracy. In our approach, reconstruction errors from the decoder network are utilized for open-set rejection. In addition, we model the tail of the reconstruction error distribution from the known classes using the statistical Extreme Value Theory to improve the overall performance. Experiments on multiple image classification datasets are performed and it is shown that this method can perform significantly better than many competitive open set recognition algorithms available in the literature. The code will be made available at: github.com/otkupjnoz/mlosr.



### CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1903.03166v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03166v2)
- **Published**: 2019-03-07 20:18:39+00:00
- **Updated**: 2019-09-18 18:04:43+00:00
- **Authors**: Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, Marcus Rohrbach
- **Comment**: 13 pages, 11 figures, 3 tables, accepted as a short paper at NAACL
  2019
- **Journal**: None
- **Summary**: Visual Dialog is a multimodal task of answering a sequence of questions grounded in an image, using the conversation history as context. It entails challenges in vision, language, reasoning, and grounding. However, studying these subtasks in isolation on large, real datasets is infeasible as it requires prohibitively-expensive complete annotation of the 'state' of all images and dialogs.   We develop CLEVR-Dialog, a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, we construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.   We use CLEVR-Dialog to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. We hope the findings from CLEVR-Dialog will help inform the development of future models for visual dialog. Our dataset and code are publicly available.



### Fast Video Retargeting Based on Seam Carving with Parental Labeling
- **Arxiv ID**: http://arxiv.org/abs/1903.03180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03180v1)
- **Published**: 2019-03-07 20:52:17+00:00
- **Updated**: 2019-03-07 20:52:17+00:00
- **Authors**: Zhu Chuning
- **Comment**: None
- **Journal**: None
- **Summary**: Seam carving is a state-of-the-art content-aware image resizing technique that effectively preserves the salient areas of an image. However, when applied to video retargeting, not only is it time intensive, but it also creates highly visible frame-wise discontinuities. In this paper, we propose a novel video retargeting method based on seam carving. First, for a single frame, we locate and remove several seams instead of one seam at once. Second, we use a dynamic spatiotemporal buffer of energy maps and a standard deviation operator to carve out the same seams in a temporal cube of frames with low variation in energy. Last but not least, an improved energy function that considers motions detected through difference method is employed. During testing, these enhancements result in a 93 percent reduction in processing time and a higher frame-wise consistency, thus showing superior performance compared to existing video retargeting methods.



### Root Identification in Minirhizotron Imagery with Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.03207v3
- **DOI**: 10.1007/s00138-020-01088-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03207v3)
- **Published**: 2019-03-07 22:20:05+00:00
- **Updated**: 2020-05-18 20:33:52+00:00
- **Authors**: Guohao Yu, Alina Zare, Hudanyun Sheng, Roser Matamala, Joel Reyes-Cabrera, Felix B. Fritschi, Thomas E. Juenger
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, multiple instance learning (MIL) algorithms to automatically perform root detection and segmentation in minirhizotron imagery using only image-level labels are proposed. Root and soil characteristics vary from location to location, thus, supervised machine learning approaches that are trained with local data provide the best ability to identify and segment roots in minirhizotron imagery. However, labeling roots for training data (or otherwise) is an extremely tedious and time-consuming task. This paper aims to address this problem by labeling data at the image level (rather than the individual root or root pixel level) and train algorithms to perform individual root pixel level segmentation using MIL strategies. Three MIL methods (multiple instance adaptive cosine coherence estimator, multiple instance support vector machine, multiple instance learning with randomized trees) were applied to root detection and compared to non-MIL approches. The results show that MIL methods improve root segmentation in challenging minirhizotron imagery and reduce the labeling burden. In our results, multiple instance support vector machine outperformed other methods. The multiple instance adaptive cosine coherence estimator algorithm was a close second with an added advantage that it learned an interpretable root signature which identified the traits used to distinguish roots from soil and did not require parameter selection.



### Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss
- **Arxiv ID**: http://arxiv.org/abs/1903.03215v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03215v2)
- **Published**: 2019-03-07 23:07:15+00:00
- **Updated**: 2020-02-16 17:59:37+00:00
- **Authors**: Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, Elisa Ricci
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: A classifier trained on a dataset seldom works on other datasets obtained under different conditions due to domain shift. This problem is commonly addressed by domain adaptation methods. In this work we introduce a novel deep learning framework which unifies different paradigms in unsupervised domain adaptation. Specifically, we propose domain alignment layers which implement feature whitening for the purpose of matching source and target feature distributions. Additionally, we leverage the unlabeled target data by proposing the Min-Entropy Consensus loss, which regularizes training while avoiding the adoption of many user-defined hyper-parameters. We report results on publicly available datasets, considering both digit classification and object recognition tasks. We show that, in most of our experiments, our approach improves upon previous methods, setting new state-of-the-art performances.



### Pattern Recognition in SAR Images using Fractional Random Fields and its Possible Application to the Problem of the Detection of Oil Spills in Open Sea
- **Arxiv ID**: http://arxiv.org/abs/1903.03221v1
- **DOI**: None
- **Categories**: **cs.CV**, 60G22, 60G35 (Primary), 42C40, 60G20 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1903.03221v1)
- **Published**: 2019-03-07 23:48:29+00:00
- **Updated**: 2019-03-07 23:48:29+00:00
- **Authors**: Agustín Mailing, Segundo A. Molina, José L. Hamkalo, Fernando R. Dobarro, Juan M. Medina, Bruno Cernuschi-Frías, Daniel A. Fernández, Érica Schlaps
- **Comment**: Keywords: Fractional Processes, Wavelets. 3 pages, 4 figures
- **Journal**: None
- **Summary**: In this note we deal with the detection of oil spills in open sea via self similar, long range dependence random fields and wavelet filters. We show some preliminary experimental results of our technique with Sentinel 1 SAR images.



