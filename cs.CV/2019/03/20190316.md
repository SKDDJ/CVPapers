# Arxiv Papers in cs.CV on 2019-03-16
### Directional PointNet: 3D Environmental Classification for Wearable Robotics
- **Arxiv ID**: http://arxiv.org/abs/1903.06846v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06846v2)
- **Published**: 2019-03-16 00:14:20+00:00
- **Updated**: 2019-03-20 18:19:32+00:00
- **Authors**: Kuangen Zhang, Jing Wang, Chenglong Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Environmental information can provide reliable prior information about human motion intent, which can aid the subject with wearable robotics to walk in complex environments. Previous researchers have utilized 1D signal and 2D images to classify environments, but they may face the problems of self-occlusion. Comparatively, 3D point cloud can be more appropriate to depict environments, thus we propose a directional PointNet to classify 3D point cloud directly. By utilizing the orientation information of the point cloud, the directional PointNet can classify daily terrains, including level ground, up stairs, and down stairs, and the classification accuracy achieves 99% for testing set. Moreover, the directional PointNet is more efficient than the previous PointNet because the T-net, which is utilized to estimate the transformation of the point cloud, is removed in this research and the length of the global feature is optimized. The experimental results demonstrate that the directional PointNet can classify the environments robustly and efficiently.



### Learning Super-resolution 3D Segmentation of Plant Root MRI Images from Few Examples
- **Arxiv ID**: http://arxiv.org/abs/1903.06855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06855v1)
- **Published**: 2019-03-16 01:12:04+00:00
- **Updated**: 2019-03-16 01:12:04+00:00
- **Authors**: Ali Oguz Uzman, Jannis Horn, Sven Behnke
- **Comment**: 27th European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Bruges, Belgium, 2019
- **Journal**: None
- **Summary**: Analyzing plant roots is crucial to understand plant performance in different soil environments. While magnetic resonance imaging (MRI) can be used to obtain 3D images of plant roots, extracting the root structural model is challenging due to highly noisy soil environments and low-resolution of MRI images. To improve both contrast and resolution, we adapt the state-of-the-art method RefineNet for 3D segmentation of the plant root MRI images in super-resolution. The networks are trained from few manual segmentations that are augmented by geometric transformations, realistic noise, and other variabilities. The resulting segmentations contain most root structures, including branches not extracted by the human annotator.



### Domain Generalization by Solving Jigsaw Puzzles
- **Arxiv ID**: http://arxiv.org/abs/1903.06864v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06864v2)
- **Published**: 2019-03-16 01:56:59+00:00
- **Updated**: 2019-04-14 11:03:44+00:00
- **Authors**: Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi
- **Comment**: Accepted at CVPR 2019 (oral)
- **Journal**: None
- **Summary**: Human adaptability relies crucially on the ability to learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the task of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals how to solve a jigsaw puzzle on the same images. This secondary task helps the network to learn the concepts of spatial correlation while acting as a regularizer for the classification task. Multiple experiments on the PACS, VLCS, Office-Home and digits datasets confirm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach.



### Fast Interactive Object Annotation with Curve-GCN
- **Arxiv ID**: http://arxiv.org/abs/1903.06874v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06874v1)
- **Published**: 2019-03-16 03:14:41+00:00
- **Updated**: 2019-03-16 03:14:41+00:00
- **Authors**: Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, Sanja Fidler
- **Comment**: In Computer Vision and Pattern Recognition (CVPR), Long Beach, US,
  2019
- **Journal**: None
- **Summary**: Manually labeling objects by tracing their boundaries is a laborious process. In Polygon-RNN++ the authors proposed Polygon-RNN that produces polygonal annotations in a recurrent manner using a CNN-RNN architecture, allowing interactive correction via humans-in-the-loop. We propose a new framework that alleviates the sequential nature of Polygon-RNN, by predicting all vertices simultaneously using a Graph Convolutional Network (GCN). Our model is trained end-to-end. It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. We show that Curve-GCN outperforms all existing approaches in automatic mode, including the powerful PSP-DeepLab and is significantly more efficient in interactive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and 2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++.



### Ontology Based Global and Collective Motion Patterns for Event Classification in Basketball Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.06879v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06879v2)
- **Published**: 2019-03-16 04:52:02+00:00
- **Updated**: 2019-03-19 08:53:50+00:00
- **Authors**: Lifang Wu, Zhou Yang, Jiaoyu He, Meng Jian, Yaowen Xu, Dezhong Xu, Chang Wen Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In multi-person videos, especially team sport videos, a semantic event is usually represented as a confrontation between two teams of players, which can be represented as collective motion. In broadcast basketball videos, specific camera motions are used to present specific events. Therefore, a semantic event in broadcast basketball videos is closely related to both the global motion (camera motion) and the collective motion. A semantic event in basketball videos can be generally divided into three stages: pre-event, event occurrence (event-occ), and post-event. In this paper, we propose an ontology-based global and collective motion pattern (On_GCMP) algorithm for basketball event classification. First, a two-stage GCMP based event classification scheme is proposed. The GCMP is extracted using optical flow. The two-stage scheme progressively combines a five-class event classification algorithm on event-occs and a two-class event classification algorithm on pre-events. Both algorithms utilize sequential convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to extract the spatial and temporal features of GCMP for event classification. Second, we utilize post-event segments to predict success/failure using deep features of images in the video frames (RGB_DF_VF) based algorithms. Finally the event classification results and success/failure classification results are integrated to obtain the final results. To evaluate the proposed scheme, we collected a new dataset called NCAA+, which is automatically obtained from the NCAA dataset by extending the fixed length of video clips forward and backward of the corresponding semantic events. The experimental results demonstrate that the proposed scheme achieves the mean average precision of 58.10% on NCAA+. It is higher by 6.50% than state-of-the-art on NCAA.



### Concatenated Feature Pyramid Network for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00768v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00768v1)
- **Published**: 2019-03-16 07:44:10+00:00
- **Updated**: 2019-03-16 07:44:10+00:00
- **Authors**: Yongqing Sun, Pranav Shenoy K P, Jun Shimamura, Atsushi Sagata
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: Low level features like edges and textures play an important role in accurately localizing instances in neural networks. In this paper, we propose an architecture which improves feature pyramid networks commonly used instance segmentation networks by incorporating low level features in all layers of the pyramid in an optimal and efficient way. Specifically, we introduce a new layer which learns new correlations from feature maps of multiple feature pyramid levels holistically and enhances the semantic information of the feature pyramid to improve accuracy. Our architecture is simple to implement in instance segmentation or object detection frameworks to boost accuracy. Using this method in Mask RCNN, our model achieves consistent improvement in precision on COCO Dataset with the computational overhead compared to the original feature pyramid network.



### HexaShrink, an exact scalable framework for hexahedral meshes with attributes and discontinuities: multiresolution rendering and storage of geoscience models
- **Arxiv ID**: http://arxiv.org/abs/1903.07614v2
- **DOI**: 10.1007/s10596-019-9816-2
- **Categories**: **cs.GR**, cs.CV, cs.DS, physics.data-an, physics.geo-ph, 65M50
- **Links**: [PDF](http://arxiv.org/pdf/1903.07614v2)
- **Published**: 2019-03-16 10:24:22+00:00
- **Updated**: 2019-05-04 13:33:17+00:00
- **Authors**: Jean-Luc Peyrot, Laurent Duval, Frédéric Payan, Lauriane Bouard, Lénaïc Chizat, Sébastien Schneider, Marc Antonini
- **Comment**: None
- **Journal**: None
- **Summary**: With huge data acquisition progresses realized in the past decades and acquisition systems now able to produce high resolution grids and point clouds, the digitization of physical terrains becomes increasingly more precise. Such extreme quantities of generated and modeled data greatly impact computational performances on many levels of high-performance computing (HPC): storage media, memory requirements, transfer capability, and finally simulation interactivity, necessary to exploit this instance of big data. Efficient representations and storage are thus becoming "enabling technologies'' in HPC experimental and simulation science. We propose HexaShrink, an original decomposition scheme for structured hexahedral volume meshes. The latter are used for instance in biomedical engineering, materials science, or geosciences. HexaShrink provides a comprehensive framework allowing efficient mesh visualization and storage. Its exactly reversible multiresolution decomposition yields a hierarchy of meshes of increasing levels of details, in terms of either geometry, continuous or categorical properties of cells. Starting with an overview of volume meshes compression techniques, our contribution blends coherently different multiresolution wavelet schemes in different dimensions. It results in a global framework preserving discontinuities (faults) across scales, implemented as a fully reversible upscaling at different resolutions. Experimental results are provided on meshes of varying size and complexity. They emphasize the consistency of the proposed representation, in terms of visualization, attribute downsampling and distribution at different resolutions. Finally, HexaShrink yields gains in storage space when combined to lossless compression techniques.



### Classification of dry age-related macular degeneration and diabetic macular edema from optical coherence tomography images using dictionary learning
- **Arxiv ID**: http://arxiv.org/abs/1903.06909v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.06909v1)
- **Published**: 2019-03-16 11:17:57+00:00
- **Updated**: 2019-03-16 11:17:57+00:00
- **Authors**: Elahe Mousavi, Rahele Kafieh, Hossein Rabbani
- **Comment**: 9 pages, IET journal
- **Journal**: None
- **Summary**: Age-related Macular Degeneration (AMD) and Diabetic Macular Edema (DME) are the major causes of vision loss in developed countries. Alteration of retinal layer structure and appearance of exudate are the most significant signs of these diseases. With the aim of automatic classification of DME, AMD and normal subjects from Optical Coherence Tomography (OCT) images, we proposed a classification algorithm. The two important issues intended in this approach are, not utilizing retinal layer segmentation which by itself is a challenging task and attempting to identify diseases in their early stages, where the signs of diseases appear in a small fraction of B-Scans. We used a histogram of oriented gradients (HOG) feature descriptor to well characterize the distribution of local intensity gradients and edge directions. In order to capture the structure of extracted features, we employed different dictionary learning-based classifiers. Our dataset consists of 45 subjects: 15 patients with AMD, 15 patients with DME and 15 normal subjects. The proposed classifier leads to an accuracy of 95.13%, 100.00%, and 100.00% for DME, AMD, and normal OCT images, respectively, only by considering the 4% of all B-Scans of a volume which outperforms the state of the art methods.



### A Cross-Season Correspondence Dataset for Robust Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.06916v2
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1903.06916v2)
- **Published**: 2019-03-16 13:01:23+00:00
- **Updated**: 2019-08-16 06:41:21+00:00
- **Authors**: Måns Larsson, Erik Stenborg, Lars Hammarstrand, Torsten Sattler, Mark Pollefeys, Fredrik Kahl
- **Comment**: In Proc. CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present a method to utilize 2D-2D point matches between images taken during different image conditions to train a convolutional neural network for semantic segmentation. Enforcing label consistency across the matches makes the final segmentation algorithm robust to seasonal changes. We describe how these 2D-2D matches can be generated with little human interaction by geometrically matching points from 3D models built from images. Two cross-season correspondence datasets are created providing 2D-2D matches across seasonal changes as well as from day to night. The datasets are made publicly available to facilitate further research. We show that adding the correspondences as extra supervision during training improves the segmentation performance of the convolutional neural network, making it more robust to seasonal changes and weather conditions.



### Robust Super-Resolution GAN, with Manifold-based and Perception Loss
- **Arxiv ID**: http://arxiv.org/abs/1903.06920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06920v1)
- **Published**: 2019-03-16 13:45:09+00:00
- **Updated**: 2019-03-16 13:45:09+00:00
- **Authors**: Uddeshya Upadhyay, Suyash P. Awate
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI)-2019
- **Journal**: None
- **Summary**: Super-resolution using deep neural networks typically relies on highly curated training sets that are often unavailable in clinical deployment scenarios. Using loss functions that assume Gaussian-distributed residuals makes the learning sensitive to corruptions in clinical training sets. We propose novel loss functions that are robust to corruptions in training sets by modeling heavy-tailed non-Gaussian distributions on the residuals. We propose a loss based on an autoencoder-based manifold-distance between the super-resolved and high-resolution images, to reproduce realistic textural content in super-resolved images. We propose to learn to super-resolve images to match human perceptions of structure, luminance, and contrast. Results on a large clinical dataset shows the advantages of each of our contributions, where our framework improves over the state of the art.



### Real time backbone for semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.06922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06922v1)
- **Published**: 2019-03-16 13:51:15+00:00
- **Updated**: 2019-03-16 13:51:15+00:00
- **Authors**: Zhengeng Yang, Hongshan Yu, Qiang Fu, Wei Sun, Wenyan Jia, Mingui Sun, Zhi-Hong Mao
- **Comment**: 6pages
- **Journal**: None
- **Summary**: The rapid development of autonomous driving in recent years presents lots of challenges for scene understanding. As an essential step towards scene understanding, semantic segmentation thus received lots of attention in past few years. Although deep learning based state-of-the-arts have achieved great success in improving the segmentation accuracy, most of them suffer from an inefficiency problem and can hardly applied to practical applications. In this paper, we systematically analyze the computation cost of Convolutional Neural Network(CNN) and found that the inefficiency of CNN is mainly caused by its wide structure rather than the deep structure. In addition, the success of pruning based model compression methods proved that there are many redundant channels in CNN. Thus, we designed a very narrow while deep backbone network to improve the efficiency of semantic segmentation. By casting our network to FCN32 segmentation architecture, the basic structure of most segmentation methods, we achieved 60.6\% mIoU on Cityscape val dataset with 54 frame per seconds(FPS) on $1024\times2048$ inputs, which already outperforms one of the earliest real time deep learning based segmentation methods: ENet.



### Spatiotemporal Feature Learning for Event-Based Vision
- **Arxiv ID**: http://arxiv.org/abs/1903.06923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06923v1)
- **Published**: 2019-03-16 13:52:30+00:00
- **Updated**: 2019-03-16 13:52:30+00:00
- **Authors**: Rohan Ghosh, Anupam Gupta, Siyi Tang, Alcimar Soares, Nitish Thakor
- **Comment**: Submitted to IEEE Transactions in Neural Networks and Learning
  Systems
- **Journal**: None
- **Summary**: Unlike conventional frame-based sensors, event-based visual sensors output information through spikes at a high temporal resolution. By only encoding changes in pixel intensity, they showcase a low-power consuming, low-latency approach to visual information sensing. To use this information for higher sensory tasks like object recognition and tracking, an essential simplification step is the extraction and learning of features. An ideal feature descriptor must be robust to changes involving (i) local transformations and (ii) re-appearances of a local event pattern. To that end, we propose a novel spatiotemporal feature representation learning algorithm based on slow feature analysis (SFA). Using SFA, smoothly changing linear projections are learnt which are robust to local visual transformations. In order to determine if the features can learn to be invariant to various visual transformations, feature point tracking tasks are used for evaluation. Extensive experiments across two datasets demonstrate the adaptability of the spatiotemporal feature learner to translation, scaling and rotational transformations of the feature points. More importantly, we find that the obtained feature representations are able to exploit the high temporal resolution of such event-based cameras in generating better feature tracks.



### Unsupervised Part-Based Disentangling of Object Shape and Appearance
- **Arxiv ID**: http://arxiv.org/abs/1903.06946v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06946v3)
- **Published**: 2019-03-16 15:40:51+00:00
- **Updated**: 2019-06-17 14:51:51+00:00
- **Authors**: Dominik Lorenz, Leonard Bereska, Timo Milbich, Björn Ommer
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: Large intra-class variation is the result of changes in multiple object characteristics. Images, however, only show the superposition of different variable factors such as appearance or shape. Therefore, learning to disentangle and represent these different characteristics poses a great challenge, especially in the unsupervised case. Moreover, large object articulation calls for a flexible part-based model. We present an unsupervised approach for disentangling appearance and shape by learning parts consistently over all instances of a category. Our model for learning an object representation is trained by simultaneously exploiting invariance and equivariance constraints between synthetically transformed images. Since no part annotation or prior information on an object class is required, the approach is applicable to arbitrary classes. We evaluate our approach on a wide range of object categories and diverse tasks including pose prediction, disentangled image synthesis, and video-to-video translation. The approach outperforms the state-of-the-art on unsupervised keypoint prediction and compares favorably even against supervised approaches on the task of shape and appearance transfer.



### Hand range of motion evaluation for Rheumatoid Arthritis patients
- **Arxiv ID**: http://arxiv.org/abs/1903.06949v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1903.06949v1)
- **Published**: 2019-03-16 15:51:23+00:00
- **Updated**: 2019-03-16 15:51:23+00:00
- **Authors**: Luciano Walenty Xavier Cejnog, Roberto Marcondes Cesar Jr., Teofilo Emidio de Campos, Valeria Meirelles Carril Elui
- **Comment**: This preprint has been submitted to 4th IEEE International Conference
  on Automatic Face and Gesture Recognition, FG2019. 5 pages, 7 figures, 1
  table
- **Journal**: None
- **Summary**: We introduce a framework for dynamic evaluation of the fingers movements: flexion, extension, abduction and adduction. This framework estimates angle measurements from joints computed by a hand pose estimation algorithm using a depth sensor (Realsense SR300). Given depth maps as input, our framework uses Pose-REN, which is a state-of-art hand pose estimation method that estimates 3D hand joint positions using a deep convolutional neural network. The pose estimation algorithm runs in real-time, allowing users to visualise 3D skeleton tracking results at the same time as the depth images are acquired. Once 3D joint poses are obtained, our framework estimates a plane containing the wrist and MCP joints and measures flexion/extension and abduction/aduction angles by applying computational geometry operations with respect to this plane. We analysed flexion and abduction movement patterns using real data, extracting the movement trajectories. Our preliminary results show that this method allows an automatic discrimination of hands with Rheumatoid Arthritis (RA) and healthy patients. The angle between joints can be used as an indicative of current movement capabilities and function. Although the measurements can be noisy and less accurate than those obtained statically through goniometry, the acquisition is much easier, non-invasive and patient-friendly, which shows the potential of our approach. The system can be used with and without orthosis. Our framework allows the acquisition of measurements with minimal intervention and significantly reduces the evaluation time.



### Generative Adversarial Networks: recent developments
- **Arxiv ID**: http://arxiv.org/abs/1903.12266v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.12266v1)
- **Published**: 2019-03-16 18:10:35+00:00
- **Updated**: 2019-03-16 18:10:35+00:00
- **Authors**: Maciej Zamorski, Adrian Zdobylak, Maciej Zięba, Jerzy Świątek
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In traditional generative modeling, good data representation is very often a base for a good machine learning model. It can be linked to good representations encoding more explanatory factors that are hidden in the original data. With the invention of Generative Adversarial Networks (GANs), a subclass of generative models that are able to learn representations in an unsupervised and semi-supervised fashion, we are now able to adversarially learn good mappings from a simple prior distribution to a target data distribution. This paper presents an overview of recent developments in GANs with a focus on learning latent space representations.



### Domain adaptation for holistic skin detection
- **Arxiv ID**: http://arxiv.org/abs/1903.06969v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1903.06969v2)
- **Published**: 2019-03-16 18:26:42+00:00
- **Updated**: 2020-03-28 20:54:03+00:00
- **Authors**: Aloisio Dourado, Frederico Guth, Teofilo Emidio de Campos, Li Weigang
- **Comment**: 11 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Human skin detection in images is a widely studied topic of Computer Vision for which it is commonly accepted that analysis of pixel color or local patches may suffice. This is because skin regions appear to be relatively uniform and many argue that there is a small chromatic variation among different samples. However, we found that there are strong biases in the datasets commonly used to train or tune skin detection methods. Furthermore, the lack of contextual information may hinder the performance of local approaches. In this paper we present a comprehensive evaluation of holistic and local Convolutional Neural Network (CNN) approaches on in-domain and cross-domain experiments and compare with state-of-the-art pixel-based approaches. We also propose a combination of inductive transfer learning and unsupervised domain adaptation methods, which are evaluated on different domains under several amounts of labelled data availability. We show a clear superiority of CNN over pixel-based approaches even without labelled training samples on the target domain. Furthermore, we provide experimental support for the counter-intuitive superiority of holistic over local approaches for human skin detection.



### Visual Query Answering by Entity-Attribute Graph Matching and Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1903.06994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06994v1)
- **Published**: 2019-03-16 21:58:04+00:00
- **Updated**: 2019-03-16 21:58:04+00:00
- **Authors**: Peixi Xiong, Huayi Zhan, Xin Wang, Baivab Sinha, Ying Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Query Answering (VQA) is of great significance in offering people convenience: one can raise a question for details of objects, or high-level understanding about the scene, over an image. This paper proposes a novel method to address the VQA problem. In contrast to prior works, our method that targets single scene VQA, replies on graph-based techniques and involves reasoning. In a nutshell, our approach is centered on three graphs. The first graph, referred to as inference graph GI , is constructed via learning over labeled data. The other two graphs, referred to as query graph Q and entity-attribute graph GEA, are generated from natural language query Qnl and image Img, that are issued from users, respectively. As GEA often does not take sufficient information to answer Q, we develop techniques to infer missing information of GEA with GI . Based on GEA and Q, we provide techniques to find matches of Q in GEA, as the answer of Qnl in Img. Unlike commonly used VQA methods that are based on end-to-end neural networks, our graph-based method shows well-designed reasoning capability, and thus is highly interpretable. We also create a dataset on soccer match (Soccer-VQA) with rich annotations. The experimental results show that our approach outperforms the state-of-the-art method and has high potential for future investigation.



### GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.06999v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.06999v2)
- **Published**: 2019-03-16 22:55:47+00:00
- **Updated**: 2019-03-21 22:48:57+00:00
- **Authors**: Yang Zheng, Izzat H. Izzat, Shahrzad Ziaee
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Pedestrian detection is an essential task in autonomous driving research. In addition to typical color images, thermal images benefit the detection in dark environments. Hence, it is worthwhile to explore an integrated approach to take advantage of both color and thermal images simultaneously. In this paper, we propose a novel approach to fuse color and thermal sensors using deep neural networks (DNN). Current state-of-the-art DNN object detectors vary from two-stage to one-stage mechanisms. Two-stage detectors, like Faster-RCNN, achieve higher accuracy, while one-stage detectors such as Single Shot Detector (SSD) demonstrate faster performance. To balance the trade-off, especially in the consideration of autonomous driving applications, we investigate a fusion strategy to combine two SSDs on color and thermal inputs. Traditional fusion methods stack selected features from each channel and adjust their weights. In this paper, we propose two variations of novel Gated Fusion Units (GFU), that learn the combination of feature maps generated by the two SSD middle layers. Leveraging GFUs for the entire feature pyramid structure, we propose several mixed versions of both stack fusion and gated fusion. Experiments are conducted on the KAIST multispectral pedestrian detection dataset. Our Gated Fusion Double SSD (GFD-SSD) outperforms the stacked fusion and achieves the lowest miss rate in the benchmark, at an inference speed that is two times faster than Faster-RCNN based fusion networks.



