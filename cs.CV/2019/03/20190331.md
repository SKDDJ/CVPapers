# Arxiv Papers in cs.CV on 2019-03-31
### Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1904.00310v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.00310v3)
- **Published**: 2019-03-31 00:35:36+00:00
- **Updated**: 2019-05-21 16:36:25+00:00
- **Authors**: Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.



### Discrete Rotation Equivariance for Point Cloud Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.00319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00319v1)
- **Published**: 2019-03-31 01:50:36+00:00
- **Updated**: 2019-03-31 01:50:36+00:00
- **Authors**: Jiaxin Li, Yingcai Bi, Gim Hee Lee
- **Comment**: The 2019 International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Despite the recent active research on processing point clouds with deep networks, few attention has been on the sensitivity of the networks to rotations. In this paper, we propose a deep learning architecture that achieves discrete $\mathbf{SO}(2)$/$\mathbf{SO}(3)$ rotation equivariance for point cloud recognition. Specifically, the rotation of an input point cloud with elements of a rotation group is similar to shuffling the feature vectors generated by our approach. The equivariance is easily reduced to invariance by eliminating the permutation with operations such as maximum or average. Our method can be directly applied to any existing point cloud based networks, resulting in significant improvements in their performance for rotated inputs. We show state-of-the-art results in the classification tasks with various datasets under both $\mathbf{SO}(2)$ and $\mathbf{SO}(3)$ rotations. In addition, we further analyze the necessary conditions of applying our approach to PointNet based networks. Source codes at https://github.com/lijx10/rot-equ-net



### NM-Net: Mining Reliable Neighbors for Robust Feature Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1904.00320v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00320v1)
- **Published**: 2019-03-31 01:50:37+00:00
- **Updated**: 2019-03-31 01:50:37+00:00
- **Authors**: Chen Zhao, Zhiguo Cao, Chi Li, Xin Li, Jiaqi Yang
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR
  2019) (oral)
- **Journal**: None
- **Summary**: Feature correspondence selection is pivotal to many feature-matching based tasks in computer vision. Searching for spatially k-nearest neighbors is a common strategy for extracting local information in many previous works. However, there is no guarantee that the spatially k-nearest neighbors of correspondences are consistent because the spatial distribution of false correspondences is often irregular. To address this issue, we present a compatibility-specific mining method to search for consistent neighbors. Moreover, in order to extract and aggregate more reliable features from neighbors, we propose a hierarchical network named NM-Net with a series of convolution layers taking the generated graph as input, which is insensitive to the order of correspondences. Our experimental results have shown the proposed method achieves the state-of-the-art performance on four datasets with various inlier ratios and varying numbers of feature consistencies.



### ImageGCN: Multi-Relational Image Graph Convolutional Networks for Disease Identification with Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1904.00325v2
- **DOI**: 10.1109/TMI.2022.3153322.
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.00325v2)
- **Published**: 2019-03-31 02:42:19+00:00
- **Updated**: 2022-02-25 16:16:26+00:00
- **Authors**: Chengsheng Mao, Liang Yao, Yuan Luo
- **Comment**: in IEEE Transactions on Medical Imaging, 2022
- **Journal**: None
- **Summary**: Image representation is a fundamental task in computer vision. However, most of the existing approaches for image representation ignore the relations between images and consider each input image independently. Intuitively, relations between images can help to understand the images and maintain model consistency over related images, leading to better explainability. In this paper, we consider modeling the image-level relations to generate more informative image representations, and propose ImageGCN, an end-to-end graph convolutional network framework for inductive multi-relational image modeling. We apply ImageGCN to chest X-ray images where rich relational information is available for disease identification. Unlike previous image representation models, ImageGCN learns the representation of an image using both its original pixel features and its relationship with other images. Besides learning informative representations for images, ImageGCN can also be used for object detection in a weakly supervised manner. The experimental results on 3 open-source x-ray datasets, ChestX-ray14, CheXpert and MIMIC-CXR demonstrate that ImageGCN can outperform respective baselines in both disease identification and localization tasks and can achieve comparable and often better results than the state-of-the-art methods.



### An Efficient Approach for Cell Segmentation in Phase Contrast Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1904.00328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00328v1)
- **Published**: 2019-03-31 03:01:10+00:00
- **Updated**: 2019-03-31 03:01:10+00:00
- **Authors**: Lin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new model to segment cells in phase contrast microscopy images. Cell images collected from the similar scenario share a similar background. Inspired by this, we separate cells from the background in images by formulating the problem as a low-rank and structured sparse matrix decomposition problem. Then, we propose the inverse diffraction pattern filtering method to further segment individual cells in the images. This is a deconvolution process that has a much lower computational complexity when compared to the other restoration methods. Experiments demonstrate the effectiveness of the proposed model when it is compared with recent works.



### Fully Learnable Group Convolution for Acceleration of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.00346v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00346v1)
- **Published**: 2019-03-31 06:24:07+00:00
- **Updated**: 2019-03-31 06:24:07+00:00
- **Authors**: Xijun Wang, Meina Kan, Shiguang Shan, Xilin Chen
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Benefitted from its great success on many tasks, deep learning is increasingly used on low-computational-cost devices, e.g. smartphone, embedded devices, etc. To reduce the high computational and memory cost, in this work, we propose a fully learnable group convolution module (FLGC for short) which is quite efficient and can be embedded into any deep neural networks for acceleration. Specifically, our proposed method automatically learns the group structure in the training stage in a fully end-to-end manner, leading to a better structure than the existing pre-defined, two-steps, or iterative strategies. Moreover, our method can be further combined with depthwise separable convolution, resulting in 5 times acceleration than the vanilla Resnet50 on single CPU. An additional advantage is that in our FLGC the number of groups can be set as any value, but not necessarily 2^k as in most existing methods, meaning better tradeoff between accuracy and speed. As evaluated in our experiments, our method achieves better performance than existing learnable group convolution and standard group convolution when using the same number of groups.



### Fast and Full-Resolution Light Field Deblurring using a Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.00352v1
- **DOI**: 10.1109/LSP.2019.2947379
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00352v1)
- **Published**: 2019-03-31 07:36:27+00:00
- **Updated**: 2019-03-31 07:36:27+00:00
- **Authors**: Jonathan Samuel Lumentut, Tae Hyun Kim, Ravi Ramamoorthi, In Kyu Park
- **Comment**: 9 pages, 8 figures
- **Journal**: IEEE Signal Processing Letters, vol. 26, no. 12, pp. 1788-1792,
  December 2019
- **Summary**: Restoring a sharp light field image from its blurry input has become essential due to the increasing popularity of parallax-based image processing. State-of-the-art blind light field deblurring methods suffer from several issues such as slow processing, reduced spatial size, and a limited motion blur model. In this work, we address these challenging problems by generating a complex blurry light field dataset and proposing a learning-based deblurring approach. In particular, we model the full 6-degree of freedom (6-DOF) light field camera motion, which is used to create the blurry dataset using a combination of real light fields captured with a Lytro Illum camera, and synthetic light field renderings of 3D scenes. Furthermore, we propose a light field deblurring network that is built with the capability of large receptive fields. We also introduce a simple strategy of angular sampling to train on the large-scale blurry light field effectively. We evaluate our method through both quantitative and qualitative measurements and demonstrate superior performance compared to the state-of-the-art method with a massive speedup in execution time. Our method is about 16K times faster than Srinivasan et. al. [22] and can deblur a full-resolution light field in less than 2 seconds.



### Pedestrian re-identification based on Tree branch network with local and global learning
- **Arxiv ID**: http://arxiv.org/abs/1904.00355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00355v1)
- **Published**: 2019-03-31 07:51:08+00:00
- **Updated**: 2019-03-31 07:51:08+00:00
- **Authors**: Hui Li, Meng Yang, Zhihui Lai, Weishi Zheng, Zitong Yu
- **Comment**: accepted by ICME2019(Oral)
- **Journal**: None
- **Summary**: Deep part-based methods in recent literature have revealed the great potential of learning local part-level representation for pedestrian image in the task of person re-identification. However, global features that capture discriminative holistic information of human body are usually ignored or not well exploited. This motivates us to investigate joint learning global and local features from pedestrian images. Specifically, in this work, we propose a novel framework termed tree branch network (TBN) for person re-identification. Given a pedestrain image, the feature maps generated by the backbone CNN, are partitioned recursively into several pieces, each of which is followed by a bottleneck structure that learns finer-grained features for each level in the hierarchical tree-like framework. In this way, representations are learned in a coarse-to-fine manner and finally assembled to produce more discriminative image descriptions. Experimental results demonstrate the effectiveness of the global and local feature learning method in the proposed TBN framework. We also show significant improvement in performance over state-of-the-art methods on three public benchmarks: Market-1501, CUHK-03 and DukeMTMC.



### Variational Adversarial Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.00370v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00370v3)
- **Published**: 2019-03-31 09:54:17+00:00
- **Updated**: 2019-10-28 18:03:08+00:00
- **Authors**: Samarth Sinha, Sayna Ebrahimi, Trevor Darrell
- **Comment**: First two authors contributed equally, listed alphabetically.
  Accepted as Oral at ICCV 2019
- **Journal**: None
- **Summary**: Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on $\text{CIFAR10/100}$, $\text{Caltech-256}$, $\text{ImageNet}$, $\text{Cityscapes}$, and $\text{BDD100K}$. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.



### PyramidBox++: High Performance Detector for Finding Tiny Face
- **Arxiv ID**: http://arxiv.org/abs/1904.00386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00386v1)
- **Published**: 2019-03-31 11:44:31+00:00
- **Updated**: 2019-03-31 11:44:31+00:00
- **Authors**: Zhihang Li, Xu Tang, Junyu Han, Jingtuo Liu, Ran He
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid development of deep convolutional neural network, face detection has made great progress in recent years. WIDER FACE dataset, as a main benchmark, contributes greatly to this area. A large amount of methods have been put forward where PyramidBox designs an effective data augmentation strategy (Data-anchor-sampling) and context-based module for face detector. In this report, we improve each part to further boost the performance, including Balanced-data-anchor-sampling, Dual-PyramidAnchors and Dense Context Module. Specifically, Balanced-data-anchor-sampling obtains more uniform sampling of faces with different sizes. Dual-PyramidAnchors facilitate feature learning by introducing progressive anchor loss. Dense Context Module with dense connection not only enlarges receptive filed, but also passes information efficiently. Integrating these techniques, PyramidBox++ is constructed and achieves state-of-the-art performance in hard set.



### Multi-vision Attention Networks for On-line Red Jujube Grading
- **Arxiv ID**: http://arxiv.org/abs/1904.00388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00388v1)
- **Published**: 2019-03-31 11:57:41+00:00
- **Updated**: 2019-03-31 11:57:41+00:00
- **Authors**: Xiaoye Sun, Liyan Ma, Gongyan Li
- **Comment**: None
- **Journal**: None
- **Summary**: To solve the red jujube classification problem, this paper designs a convolutional neural network model with low computational cost and high classification accuracy. The architecture of the model is inspired by the multi-visual mechanism of the organism and DenseNet. To further improve our model, we add the attention mechanism of SE-Net. We also construct a dataset which contains 23,735 red jujube images captured by a jujube grading system. According to the appearance of the jujube and the characteristics of the grading system, the dataset is divided into four classes: invalid, rotten, wizened and normal. The numerical experiments show that the classification accuracy of our model reaches to 91.89%, which is comparable to DenseNet-121, InceptionV3, InceptionV4, and Inception-ResNet v2. However, our model has real-time performance.



### Road Scene Understanding by Occupancy Grid Learning from Sparse Radar Clusters using Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00415v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00415v2)
- **Published**: 2019-03-31 14:04:16+00:00
- **Updated**: 2019-09-02 18:35:09+00:00
- **Authors**: Liat Sless, Gilad Cohen, Bat El Shlomo, Shaul Oron
- **Comment**: None
- **Journal**: ICCV 2019 CVRSUAD
- **Summary**: Occupancy grid mapping is an important component in road scene understanding for autonomous driving. It encapsulates information of the drivable area, road obstacles and enables safe autonomous driving. Radars are an emerging sensor in autonomous vehicle vision, becoming more widely used due to their long range sensing, low cost, and robustness to severe weather conditions. Despite recent advances in deep learning technology, occupancy grid mapping from radar data is still mostly done using classical filtering approaches.In this work, we propose learning the inverse sensor model used for occupancy grid mapping from clustered radar data. This is done in a data driven approach that leverages computer vision techniques. This task is very challenging due to data sparsity and noise characteristics of the radar sensor. The problem is formulated as a semantic segmentation task and we show how it can be learned using lidar data for generating ground truth. We show both qualitatively and quantitatively that our learned occupancy net outperforms classic methods by a large margin using the recently released NuScenes real-world driving data.



### Single Path One-Shot Neural Architecture Search with Uniform Sampling
- **Arxiv ID**: http://arxiv.org/abs/1904.00420v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00420v4)
- **Published**: 2019-03-31 14:34:43+00:00
- **Updated**: 2020-07-08 10:55:28+00:00
- **Authors**: Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, Jian Sun
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally.   Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.



