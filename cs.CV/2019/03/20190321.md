# Arxiv Papers in cs.CV on 2019-03-21
### Networks for Joint Affine and Non-parametric Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1903.08811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08811v1)
- **Published**: 2019-03-21 02:35:42+00:00
- **Updated**: 2019-03-21 02:35:42+00:00
- **Authors**: Zhengyang Shen, Xu Han, Zhenlin Xu, Marc Niethammer
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: We introduce an end-to-end deep-learning framework for 3D medical image registration. In contrast to existing approaches, our framework combines two registration methods: an affine registration and a vector momentum-parameterized stationary velocity field (vSVF) model. Specifically, it consists of three stages. In the first stage, a multi-step affine network predicts affine transform parameters. In the second stage, we use a Unet-like network to generate a momentum, from which a velocity field can be computed via smoothing. Finally, in the third stage, we employ a self-iterable map-based vSVF component to provide a non-parametric refinement based on the current estimate of the transformation map. Once the model is trained, a registration is completed in one forward pass. To evaluate the performance, we conducted longitudinal and cross-subject experiments on 3D magnetic resonance images (MRI) of the knee of the Osteoarthritis Initiative (OAI) dataset. Results show that our framework achieves comparable performance to state-of-the-art medical image registration approaches, but it is much faster, with a better control of transformation regularity including the ability to produce approximately symmetric transformations, and combining affine and non-parametric registration.



### Prostate Segmentation from Ultrasound Images using Residual Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1903.08814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08814v1)
- **Published**: 2019-03-21 02:53:03+00:00
- **Updated**: 2019-03-21 02:53:03+00:00
- **Authors**: M. S. Hossain, A. P. Paplinski, J. M. Betts
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Medical imaging based prostate cancer diagnosis procedure uses intra-operative transrectal ultrasound (TRUS) imaging to visualize the prostate shape and location to collect tissue samples. Correct tissue sampling from prostate requires accurate prostate segmentation in TRUS images. To achieve this, this study uses a novel residual connection based fully convolutional network. The advantage of this segmentation technique is that it requires no pre-processing of TRUS images to perform the segmentation. Thus, it offers a faster and straightforward prostate segmentation from TRUS images. Results show that the proposed technique can achieve around 86% Dice Similarity accuracy using only few TRUS datasets.



### Evolving Deep Neural Networks by Multi-objective Particle Swarm Optimization for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.09035v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09035v2)
- **Published**: 2019-03-21 02:55:14+00:00
- **Updated**: 2019-04-22 04:07:17+00:00
- **Authors**: Bin Wang, Yanan Sun, Bing Xue, Mengjie Zhang
- **Comment**: conditionally accepted by gecco2019
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) have become deeper in order to achieve better classification accuracy in image classification. However, it is difficult to deploy the state-of-the-art deep CNNs for industrial use due to the difficulty of manually fine-tuning the hyperparameters and the trade-off between classification accuracy and computational cost. This paper proposes a novel multi-objective optimization method for evolving state-of-the-art deep CNNs in real-life applications, which automatically evolves the non-dominant solutions at the Pareto front. Three major contributions are made: Firstly, a new encoding strategy is designed to encode one of the best state-of-the-art CNNs; With the classification accuracy and the number of floating point operations as the two objectives, a multi-objective particle swarm optimization method is developed to evolve the non-dominant solutions; Last but not least, a new infrastructure is designed to boost the experiments by concurrently running the experiments on multiple GPUs across multiple machines, and a Python library is developed and released to manage the infrastructure. The experimental results demonstrate that the non-dominant solutions found by the proposed algorithm form a clear Pareto front, and the proposed infrastructure is able to almost linearly reduce the running time.



### Dual Residual Networks Leveraging the Potential of Paired Operations for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1903.08817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08817v2)
- **Published**: 2019-03-21 03:09:19+00:00
- **Updated**: 2019-04-07 12:54:07+00:00
- **Authors**: Xing Liu, Masanori Suganuma, Zhun Sun, Takayuki Okatani
- **Comment**: i) Accepted to CVPR 2019 ii) Code, trained models and additional
  results for visual comparison will be provided at
  https://github.com/liu-vis/DualResidualNetworks
- **Journal**: None
- **Summary**: In this paper, we study design of deep neural networks for tasks of image restoration. We propose a novel style of residual connections dubbed "dual residual connection", which exploits the potential of paired operations, e.g., up- and down-sampling or convolution with large- and small-size kernels. We design a modular block implementing this connection style; it is equipped with two containers to which arbitrary paired operations are inserted. Adopting the "unraveled" view of the residual networks proposed by Veit et al., we point out that a stack of the proposed modular blocks allows the first operation in a block interact with the second operation in any subsequent blocks. Specifying the two operations in each of the stacked blocks, we build a complete network for each individual task of image restoration. We experimentally evaluate the proposed approach on five image restoration tasks using nine datasets. The results show that the proposed networks with properly chosen paired operations outperform previous methods on almost all of the tasks and datasets.



### Value of Temporal Dynamics Information in Driving Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.00758v1)
- **Published**: 2019-03-21 03:56:30+00:00
- **Updated**: 2019-03-21 03:56:30+00:00
- **Authors**: Li Ding, Jack Terwilliger, Rini Sherony, Bryan Reimer, Lex Fridman
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic scene segmentation has primarily been addressed by forming representations of single images both with supervised and unsupervised methods. The problem of semantic segmentation in dynamic scenes has begun to recently receive attention with video object segmentation approaches. What is not known is how much extra information the temporal dynamics of the visual scene carries that is complimentary to the information available in the individual frames of the video. There is evidence that the human visual system can effectively perceive the scene from temporal dynamics information of the scene's changing visual characteristics without relying on the visual characteristics of individual snapshots themselves. Our work takes steps to explore whether machine perception can exhibit similar properties by combining appearance-based representations and temporal dynamics representations in a joint-learning problem that reveals the contribution of each toward successful dynamic scene segmentation. Additionally, we provide the MIT Driving Scene Segmentation dataset, which is a large-scale full driving scene segmentation dataset, densely annotated for every pixel and every one of 5,000 video frames. This dataset is intended to help further the exploration of the value of temporal dynamics information for semantic segmentation in video.



### Non-target Structural Displacement Measurement Using Reference Frame Based Deepflow
- **Arxiv ID**: http://arxiv.org/abs/1903.08831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08831v1)
- **Published**: 2019-03-21 05:03:19+00:00
- **Updated**: 2019-03-21 05:03:19+00:00
- **Authors**: Jongbin Won, Jong-Woong Park, Do-Soo Moon
- **Comment**: None
- **Journal**: None
- **Summary**: Structural displacement is crucial for structural health monitoring, although it is very challenging to measure in field conditions. Most existing displacement measurement methods are costly, labor intensive, and insufficiently accurate for measuring small dynamic displacements. Computer vision (CV) based methods incorporate optical devices with advanced image processing algorithms to accurately, cost-effectively, and remotely measure structural displacement with easy installation. However, non-target based CV methods are still limited by insufficient feature points, incorrect feature point detection, occlusion, and drift induced by tracking error accumulation. This paper presents a reference frame based Deepflow algorithm integrated with masking and signal filtering for non-target based displacement measurements. The proposed method allows the user to select points of interest for images with a low gradient for displacement tracking and directly calculate displacement without drift accumulated by measurement error. The proposed method is experimentally validated on a cantilevered beam under ambient and occluded test conditions. The accuracy of the proposed method is compared with that of a reference laser displacement sensor for validation. The significant advantage of the proposed method is its flexibility in extracting structural displacement in any region on structures that do not have distinct natural features.



### Towards Robust Curve Text Detection with Conditional Spatial Expansion
- **Arxiv ID**: http://arxiv.org/abs/1903.08836v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08836v1)
- **Published**: 2019-03-21 05:46:15+00:00
- **Updated**: 2019-03-21 05:46:15+00:00
- **Authors**: Zichuan Liu, Guosheng Lin, Sheng Yang, Fayao Liu, Weisi Lin, Wang Ling Goh
- **Comment**: This paper has been accepted by IEEE International Conference on
  Computer Vision and Pattern Recognition (CVPR 2019)
- **Journal**: None
- **Summary**: It is challenging to detect curve texts due to their irregular shapes and varying sizes. In this paper, we first investigate the deficiency of the existing curve detection methods and then propose a novel Conditional Spatial Expansion (CSE) mechanism to improve the performance of curve text detection. Instead of regarding the curve text detection as a polygon regression or a segmentation problem, we treat it as a region expansion process. Our CSE starts with a seed arbitrarily initialized within a text region and progressively merges neighborhood regions based on the extracted local features by a CNN and contextual information of merged regions. The CSE is highly parameterized and can be seamlessly integrated into existing object detection frameworks. Enhanced by the data-dependent CSE mechanism, our curve text detection system provides robust instance-level text region extraction with minimal post-processing. The analysis experiment shows that our CSE can handle texts with various shapes, sizes, and orientations, and can effectively suppress the false-positives coming from text-like textures or unexpected texts included in the same RoI. Compared with the existing curve text detection algorithms, our method is more robust and enjoys a simpler processing flow. It also creates a new state-of-art performance on curve text benchmarks with F-score of up to 78.4$\%$.



### Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.08839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08839v2)
- **Published**: 2019-03-21 06:00:15+00:00
- **Updated**: 2019-03-27 06:10:11+00:00
- **Authors**: Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Xiaogang Wang, Liang Lin
- **Comment**: Accepted as a CVPR 2019 oral paper. Project page:
  https://kwanyeelin.github.io/
- **Journal**: None
- **Summary**: Recent studies have shown remarkable advances in 3D human pose estimation from monocular images, with the help of large-scale in-door 3D datasets and sophisticated network architectures. However, the generalizability to different environments remains an elusive goal. In this work, we propose a geometry-aware 3D representation for the human pose to address this limitation by using multiple views in a simple auto-encoder model at the training stage and only 2D keypoint information as supervision. A view synthesis framework is proposed to learn the shared 3D representation between viewpoints with synthesizing the human pose from one viewpoint to the other one. Instead of performing a direct transfer in the raw image-level, we propose a skeleton-based encoder-decoder mechanism to distil only pose-related representation in the latent space. A learning-based representation consistency constraint is further introduced to facilitate the robustness of latent 3D representation. Since the learnt representation encodes 3D geometry information, mapping it to 3D pose will be much easier than conventional frameworks that use an image or 2D coordinates as the input of 3D pose estimator. We demonstrate our approach on the task of 3D human pose estimation. Comprehensive experiments on three popular benchmarks show that our model can significantly improve the performance of state-of-the-art methods with simply injecting the representation as a robust 3D prior.



### Parametic Classification of Handvein Patterns Based on Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1903.08847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08847v1)
- **Published**: 2019-03-21 06:58:12+00:00
- **Updated**: 2019-03-21 06:58:12+00:00
- **Authors**: Harbi AlMahafzah, Mohammad Imranand, Supreetha Gowda H. D.
- **Comment**: 8 pages, International Conference on Electrical, Electronics,
  Materials and Applied Science (ICEEMAS). AIP: Proceedings International
  Conference on Electrical, Electronics, Materials and Applied Science
  (ICEEMAS),22nd and 23rd December 2017
- **Journal**: None
- **Summary**: In this paper, we have developed Biometric recognition system adopting hand based modality Handvein, which has the unique pattern for each individual and it is impossible to counterfeit and fabricate as it is an internal feature. We have opted in choosing feature extraction algorithms such as LBP-visual descriptor ,LPQ-blur insensitive texture operator, Log-Gabor-Texture descriptor. We have chosen well known classifiers such as KNN and SVM for classification. We have experimented and tabulated results of single algorithm recognition rate for Handvein under different distance measures and kernel options. The feature level fusion is carried out which increased the performance level.



### Classification of EEG-Based Brain Connectivity Networks in Schizophrenia Using a Multi-Domain Connectome Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1903.08858v1
- **DOI**: 10.1109/JBHI.2019.2941222
- **Categories**: **cs.LG**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.08858v1)
- **Published**: 2019-03-21 07:35:54+00:00
- **Updated**: 2019-03-21 07:35:54+00:00
- **Authors**: Chun-Ren Phang, Chee-Ming Ting, Fuad Noman, Hernando Ombao
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: We exploit altered patterns in brain functional connectivity as features for automatic discriminative analysis of neuropsychiatric patients. Deep learning methods have been introduced to functional network classification only very recently for fMRI, and the proposed architectures essentially focused on a single type of connectivity measure. We propose a deep convolutional neural network (CNN) framework for classification of electroencephalogram (EEG)-derived brain connectome in schizophrenia (SZ). To capture complementary aspects of disrupted connectivity in SZ, we explore combination of various connectivity features consisting of time and frequency-domain metrics of effective connectivity based on vector autoregressive model and partial directed coherence, and complex network measures of network topology. We design a novel multi-domain connectome CNN (MDC-CNN) based on a parallel ensemble of 1D and 2D CNNs to integrate the features from various domains and dimensions using different fusion strategies. Hierarchical latent representations learned by the multiple convolutional layers from EEG connectivity reveal apparent group differences between SZ and healthy controls (HC). Results on a large resting-state EEG dataset show that the proposed CNNs significantly outperform traditional support vector machine classifiers. The MDC-CNN with combined connectivity features further improves performance over single-domain CNNs using individual features, achieving remarkable accuracy of $93.06\%$ with a decision-level fusion. The proposed MDC-CNN by integrating information from diverse brain connectivity descriptors is able to accurately discriminate SZ from HC. The new framework is potentially useful for developing diagnostic tools for SZ and other disorders.



### Learning Disentangled Representations of Satellite Image Time Series
- **Arxiv ID**: http://arxiv.org/abs/1903.08863v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08863v1)
- **Published**: 2019-03-21 07:55:11+00:00
- **Updated**: 2019-03-21 07:55:11+00:00
- **Authors**: Eduardo Sanchez, Mathieu Serrurier, Mathias Ortner
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate how to learn a suitable representation of satellite image time series in an unsupervised manner by leveraging large amounts of unlabeled data. Additionally , we aim to disentangle the representation of time series into two representations: a shared representation that captures the common information between the images of a time series and an exclusive representation that contains the specific information of each image of the time series. To address these issues, we propose a model that combines a novel component called cross-domain autoencoders with the variational autoencoder (VAE) and generative ad-versarial network (GAN) methods. In order to learn disentangled representations of time series, our model learns the multimodal image-to-image translation task. We train our model using satellite image time series from the Sentinel-2 mission. Several experiments are carried out to evaluate the obtained representations. We show that these disentangled representations can be very useful to perform multiple tasks such as image classification, image retrieval, image segmentation and change detection.



### Individualized Multilayer Tensor Learning with An Application in Imaging Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.08871v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.08871v1)
- **Published**: 2019-03-21 08:18:08+00:00
- **Updated**: 2019-03-21 08:18:08+00:00
- **Authors**: Xiwei Tang, Xuan Bi, Annie Qu
- **Comment**: None
- **Journal**: None
- **Summary**: This work is motivated by multimodality breast cancer imaging data, which is quite challenging in that the signals of discrete tumor-associated microvesicles (TMVs) are randomly distributed with heterogeneous patterns. This imposes a significant challenge for conventional imaging regression and dimension reduction models assuming a homogeneous feature structure. We develop an innovative multilayer tensor learning method to incorporate heterogeneity to a higher-order tensor decomposition and predict disease status effectively through utilizing subject-wise imaging features and multimodality information. Specifically, we construct a multilayer decomposition which leverages an individualized imaging layer in addition to a modality-specific tensor structure. One major advantage of our approach is that we are able to efficiently capture the heterogeneous spatial features of signals that are not characterized by a population structure as well as integrating multimodality information simultaneously. To achieve scalable computing, we develop a new bi-level block improvement algorithm. In theory, we investigate both the algorithm convergence property, tensor signal recovery error bound and asymptotic consistency for prediction model estimation. We also apply the proposed method for simulated and human breast cancer imaging data. Numerical results demonstrate that the proposed method outperforms other existing competing methods.



### Tensor-Ring Nuclear Norm Minimization and Application for Visual Data Completion
- **Arxiv ID**: http://arxiv.org/abs/1903.08888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08888v1)
- **Published**: 2019-03-21 09:15:08+00:00
- **Updated**: 2019-03-21 09:15:08+00:00
- **Authors**: Jinshi Yu, Chao Li, Qibin Zhao, Guoxu Zhou
- **Comment**: This paper has been accepted by ICASSP 2019
- **Journal**: None
- **Summary**: Tensor ring (TR) decomposition has been successfully used to obtain the state-of-the-art performance in the visual data completion problem. However, the existing TR-based completion methods are severely non-convex and computationally demanding. In addition, the determination of the optimal TR rank is a tough work in practice. To overcome these drawbacks, we first introduce a class of new tensor nuclear norms by using tensor circular unfolding. Then we theoretically establish connection between the rank of the circularly-unfolded matrices and the TR ranks. We also develop an efficient tensor completion algorithm by minimizing the proposed tensor nuclear norm. Extensive experimental results demonstrate that our proposed tensor completion method outperforms the conventional tensor completion methods in the image/video in-painting problem with striped missing values.



### Context-Constrained Accurate Contour Extraction for Occlusion Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.08890v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08890v1)
- **Published**: 2019-03-21 09:21:41+00:00
- **Updated**: 2019-03-21 09:21:41+00:00
- **Authors**: Rui Lu, Menghan Zhou, Anlong Ming, Yu Zhou
- **Comment**: To appear in ICME 2019
- **Journal**: None
- **Summary**: Occlusion edge detection requires both accurate locations and context constraints of the contour. Existing CNN-based pipeline does not utilize adaptive methods to filter the noise introduced by low-level features. To address this dilemma, we propose a novel Context-constrained accurate Contour Extraction Network (CCENet). Spatial details are retained and contour-sensitive context is augmented through two extraction blocks, respectively. Then, an elaborately designed fusion module is available to integrate features, which plays a complementary role to restore details and remove clutter. Weight response of attention mechanism is eventually utilized to enhance occluded contours and suppress noise. The proposed CCENet significantly surpasses state-of-the-art methods on PIOD and BSDS ownership dataset of object edge detection and occlusion orientation detection.



### Learning with Batch-wise Optimal Transport Loss for 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.08923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08923v1)
- **Published**: 2019-03-21 11:10:05+00:00
- **Updated**: 2019-03-21 11:10:05+00:00
- **Authors**: Lin Xu, Han Sun, Yuai Liu
- **Comment**: 10 pages, 4 figures Accepted by CVPR2019
- **Journal**: None
- **Summary**: Deep metric learning is essential for visual recognition. The widely used pair-wise (or triplet) based loss objectives cannot make full use of semantical information in training samples or give enough attention to those hard samples during optimization. Thus, they often suffer from a slow convergence rate and inferior performance. In this paper, we show how to learn an importance-driven distance metric via optimal transport programming from batches of samples. It can automatically emphasize hard examples and lead to significant improvements in convergence. We propose a new batch-wise optimal transport loss and combine it in an end-to-end deep metric learning manner. We use it to learn the distance metric and deep feature representation jointly for recognition. Empirical results on visual retrieval and classification tasks with six benchmark datasets, i.e., MNIST, CIFAR10, SHREC13, SHREC14, ModelNet10, and ModelNet40, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving a state-of-the-art recognition performance. For example, in 3D shape recognition experiments, we show that our method can achieve better recognition performance within only 5 epochs than what can be obtained by mainstream 3D shape recognition approaches after 200 epochs.



### The CASE Dataset of Candidate Spaces for Advert Implantation
- **Arxiv ID**: http://arxiv.org/abs/1903.08943v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08943v2)
- **Published**: 2019-03-21 12:13:46+00:00
- **Updated**: 2019-04-29 09:32:48+00:00
- **Authors**: Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, François Pitié
- **Comment**: Published in Proc. International Conference on Machine Vision
  Applications (MVA), 2019
- **Journal**: None
- **Summary**: With the advent of faster internet services and growth of multimedia content, we observe a massive growth in the number of online videos. The users generate these video contents at an unprecedented rate, owing to the use of smart-phones and other hand-held video capturing devices. This creates immense potential for the advertising and marketing agencies to create personalized content for the users. In this paper, we attempt to assist the video editors to generate augmented video content, by proposing candidate spaces in video frames. We propose and release a large-scale dataset of outdoor scenes, along with manually annotated maps for candidate spaces. We also benchmark several deep-learning based semantic segmentation algorithms on this proposed dataset.



### Short-Term Prediction and Multi-Camera Fusion on Semantic Grids
- **Arxiv ID**: http://arxiv.org/abs/1903.08960v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.08960v2)
- **Published**: 2019-03-21 12:49:31+00:00
- **Updated**: 2019-07-26 18:31:06+00:00
- **Authors**: Lukas Hoyer, Patrick Kesper, Anna Khoreva, Volker Fischer
- **Comment**: None
- **Journal**: None
- **Summary**: An environment representation (ER) is a substantial part of every autonomous system. It introduces a common interface between perception and other system components, such as decision making, and allows downstream algorithms to deal with abstracted data without knowledge of the used sensor. In this work, we propose and evaluate a novel architecture that generates an egocentric, grid-based, predictive, and semantically-interpretable ER. In particular, we provide a proof of concept for the spatio-temporal fusion of multiple camera sequences and short-term prediction in such an ER. Our design utilizes a strong semantic segmentation network together with depth and egomotion estimates to first extract semantic information from multiple camera streams and then transform these separately into egocentric temporally-aligned bird's-eye view grids. A deep encoder-decoder network is trained to fuse a stack of these grids into a unified semantic grid representation and to predict the dynamics of its surrounding. We evaluate this representation on real-world sequences of the Cityscapes dataset and show that our architecture can make accurate predictions in complex sensor fusion scenarios and significantly outperforms a model-driven baseline in a category-based evaluation.



### Localization of Unmanned Aerial Vehicles in Corridor Environments using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.09021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09021v1)
- **Published**: 2019-03-21 14:21:11+00:00
- **Updated**: 2019-03-21 14:21:11+00:00
- **Authors**: Ram Prasad Padhy, Shahzad Ahmad, Sachin Verma, Pankaj Kumar Sa, Sambit Bakshi
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Vision-based pose estimation of Unmanned Aerial Vehicles (UAV) in unknown environments is a rapidly growing research area in the field of robot vision. The task becomes more complex when the only available sensor is a static single camera (monocular vision). In this regard, we propose a monocular vision assisted localization algorithm, that will help a UAV to navigate safely in indoor corridor environments. Always, the aim is to navigate the UAV through a corridor in the forward direction by keeping it at the center with no orientation either to the left or right side. The algorithm makes use of the RGB image, captured from the UAV front camera, and passes it through a trained deep neural network (DNN) to predict the position of the UAV as either on the left or center or right side of the corridor. Depending upon the divergence of the UAV with respect to the central bisector line (CBL) of the corridor, a suitable command is generated to bring the UAV to the center. When the UAV is at the center of the corridor, a new image is passed through another trained DNN to predict the orientation of the UAV with respect to the CBL of the corridor. If the UAV is either left or right tilted, an appropriate command is generated to rectify the orientation. We also propose a new corridor dataset, named NITRCorrV1, which contains images as captured by the UAV front camera when the UAV is at all possible locations of a variety of corridors. An exhaustive set of experiments in different corridors reveal the efficacy of the proposed algorithm.



### Megapixel Photon-Counting Color Imaging using Quanta Image Sensor
- **Arxiv ID**: http://arxiv.org/abs/1903.09036v2
- **DOI**: 10.1364/OE.27.017298
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09036v2)
- **Published**: 2019-03-21 14:49:07+00:00
- **Updated**: 2019-05-20 20:38:55+00:00
- **Authors**: Abhiram Gnanasambandam, Omar Elgendy, Jiaju Ma, and Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Quanta Image Sensor (QIS) is a single-photon detector designed for extremely low light imaging conditions. Majority of the existing QIS prototypes are monochrome based on single-photon avalanche diodes (SPAD). Passive color imaging has not been demonstrated with single-photon detectors due to the intrinsic difficulty of shrinking the pixel size and increasing the spatial resolution while maintaining acceptable intra-pixel cross-talk. In this paper, we present image reconstruction of the first color QIS with a resolution of $1024 \times 1024$ pixels, supporting both single-bit and multi-bit photon counting capability. Our color image reconstruction is enabled by a customized joint demosaicing-denoising algorithm, leveraging truncated Poisson statistics and variance stabilizing transforms. Experimental results of the new sensor and algorithm demonstrate superior color imaging performance for very low-light conditions with a mean exposure of as low as a few photons per pixel in both real and simulated images.



### An Efficient Solution to Non-Minimal Case Essential Matrix Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.09067v3
- **DOI**: 10.1109/TPAMI.2020.3030161
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09067v3)
- **Published**: 2019-03-21 15:47:37+00:00
- **Updated**: 2020-10-07 09:52:04+00:00
- **Authors**: Ji Zhao
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
- **Journal**: None
- **Summary**: Finding relative pose between two calibrated images is a fundamental task in computer vision. Given five point correspondences, the classical five-point methods can be used to calculate the essential matrix efficiently. For the case of $N$ ($N > 5$) inlier point correspondences, which is called $N$-point problem, existing methods are either inefficient or prone to local minima. In this paper, we propose a certifiably globally optimal and efficient solver for the $N$-point problem. First we formulate the problem as a quadratically constrained quadratic program (QCQP). Then a certifiably globally optimal solution to this problem is obtained by semidefinite relaxation. This allows us to obtain certifiably globally optimal solutions to the original non-convex QCQPs in polynomial time. The theoretical guarantees of the semidefinite relaxation are also provided, including tightness and local stability. To deal with outliers, we propose a robust $N$-point method using M-estimators. Though global optimality cannot be guaranteed for the overall robust framework, the proposed robust $N$-point method can achieve good performance when the outlier ratio is not high. Extensive experiments on synthetic and real-world datasets demonstrated that our $N$-point method is $2\sim3$ orders of magnitude faster than state-of-the-art methods. Moreover, our robust $N$-point method outperforms state-of-the-art methods in terms of robustness and accuracy.



### Quotienting Impertinent Camera Kinematics for 3D Video Stabilization
- **Arxiv ID**: http://arxiv.org/abs/1903.09073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09073v2)
- **Published**: 2019-03-21 15:51:31+00:00
- **Updated**: 2019-08-05 16:25:01+00:00
- **Authors**: Thomas W. Mitchel, Christian Wuelker, Jin Seob Kim, Sipu Ruan, Gregory S. Chirikjian
- **Comment**: Added acknowledgements
- **Journal**: None
- **Summary**: With the recent advent of methods that allow for real-time computation, dense 3D flows have become a viable basis for fast camera motion estimation. Most importantly, dense flows are more robust than the sparse feature matching techniques used by existing 3D stabilization methods, able to better handle large camera displacements and occlusions similar to those often found in consumer videos. Here we introduce a framework for 3D video stabilization that relies on dense scene flow alone. The foundation of this approach is a novel camera motion model that allows for real-world camera poses to be recovered directly from 3D motion fields. Moreover, this model can be extended to describe certain types of non-rigid artifacts that are commonly found in videos, such as those resulting from zooms. This framework gives rise to several robust regimes that produce high-quality stabilization of the kind achieved by prior full 3D methods while avoiding the fragility typically present in feature-based approaches. As an added benefit, our framework is fast: the simplicity of our motion model and efficient flow calculations combine to enable stabilization at a high frame rate.



### Levelling the Playing Field: A Comprehensive Comparison of Visual Place Recognition Approaches under Changing Conditions
- **Arxiv ID**: http://arxiv.org/abs/1903.09107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09107v2)
- **Published**: 2019-03-21 16:46:25+00:00
- **Updated**: 2019-04-29 18:47:18+00:00
- **Authors**: Mubariz Zaffar, Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Klaus McDonald-Maier
- **Comment**: ICRA 2019 Workshop on Database Generation and Benchmarking of SLAM
  Algorithms for Robotics and VR/AR
- **Journal**: None
- **Summary**: In recent years there has been significant improvement in the capability of Visual Place Recognition (VPR) methods, building on the success of both hand-crafted and learnt visual features, temporal filtering and usage of semantic scene information. The wide range of approaches and the relatively recent growth in interest in the field has meant that a wide range of datasets and assessment methodologies have been proposed, often with a focus only on precision-recall type metrics, making comparison difficult. In this paper we present a comprehensive approach to evaluating the performance of 10 state-of-the-art recently-developed VPR techniques, which utilizes three standardized metrics: (a) Matching Performance b) Matching Time c) Memory Footprint. Together this analysis provides an up-to-date and widely encompassing snapshot of the various strengths and weaknesses of contemporary approaches to the VPR problem. The aim of this work is to help move this particular research field towards a more mature and unified approach to the problem, enabling better comparison and hence more progress to be made in future research.



### Closed-Form Optimal Two-View Triangulation Based on Angular Errors
- **Arxiv ID**: http://arxiv.org/abs/1903.09115v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09115v3)
- **Published**: 2019-03-21 17:11:56+00:00
- **Updated**: 2019-07-29 17:02:18+00:00
- **Authors**: Seong Hun Lee, Javier Civera
- **Comment**: Accepted to ICCV2019
- **Journal**: None
- **Summary**: In this paper, we study closed-form optimal solutions to two-view triangulation with known internal calibration and pose. By formulating the triangulation problem as $L_1$ and $L_\infty$ minimization of angular reprojection errors, we derive the exact closed-form solutions that guarantee global optimality under respective cost functions. To the best of our knowledge, we are the first to present such solutions. Since the angular error is rotationally invariant, our solutions can be applied for any type of central cameras, be it perspective, fisheye or omnidirectional. Our methods also require significantly less computation than the existing optimal methods. Experimental results on synthetic and real datasets validate our theoretical derivations.



### PProCRC: Probabilistic Collaboration of Image Patches
- **Arxiv ID**: http://arxiv.org/abs/1903.09123v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09123v3)
- **Published**: 2019-03-21 17:30:46+00:00
- **Updated**: 2020-11-09 20:32:42+00:00
- **Authors**: Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal
- **Comment**: None
- **Journal**: None
- **Summary**: We present a conditional probabilistic framework for collaborative representation of image patches. It incorporates background compensation and outlier patch suppression into the main formulation itself, thus doing away with the need for pre-processing steps to handle the same. A closed form non-iterative solution of the cost function is derived. The proposed method (PProCRC) outperforms earlier CRC formulations: patch based (PCRC, GP-CRC) as well as the state-of-the-art probabilistic (ProCRC and EProCRC) on three fine-grained species recognition datasets (Oxford Flowers, Oxford-IIIT Pets and CUB Birds) using two CNN backbones (Vgg-19 and ResNet-50).



### Progressive Sparse Local Attention for Video object detection
- **Arxiv ID**: http://arxiv.org/abs/1903.09126v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09126v3)
- **Published**: 2019-03-21 17:33:22+00:00
- **Updated**: 2019-08-16 13:08:37+00:00
- **Authors**: Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.



### Quantitative Depth Quality Assessment of RGBD Cameras At Close Range Using 3D Printed Fixtures
- **Arxiv ID**: http://arxiv.org/abs/1903.09169v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.09169v1)
- **Published**: 2019-03-21 18:03:29+00:00
- **Updated**: 2019-03-21 18:03:29+00:00
- **Authors**: Michele Pratusevich, Jason Chrisos, Shreyas Aditya
- **Comment**: 4 pages; submitted to ICRA 2019 HAMM Workshop
- **Journal**: None
- **Summary**: Mobile robots that manipulate their environments require high-accuracy scene understanding at close range. Typically this understanding is achieved with RGBD cameras, but the evaluation process for selecting an appropriate RGBD camera for the application is minimally quantitative. Limited manufacturer-published metrics do not translate to observed quality in real-world cluttered environments, since quality is application-specific. To bridge the gap, we present a method for quantitatively measuring depth quality using a set of extendable 3D printed fixtures that approximate real-world conditions. By framing depth quality as point cloud density and root mean square error (RMSE) from a known geometry, we present a method that is extendable by other system integrators for custom environments. We show a comparison of 3 cameras and present a case study for camera selection, provide reference meshes and analysis code, and discuss further extensions.



### Comparison of State-of-the-Art Deep Learning APIs for Image Multi-Label Classification using Semantic Metrics
- **Arxiv ID**: http://arxiv.org/abs/1903.09190v4
- **DOI**: 10.1016/j.eswa.2020.113656
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09190v4)
- **Published**: 2019-03-21 18:43:39+00:00
- **Updated**: 2020-06-11 08:26:39+00:00
- **Authors**: Adam Kubany, Shimon Ben Ishay, Ruben-sacha Ohayon, Armin Shmilovici, Lior Rokach, Tomer Doitshman
- **Comment**: None
- **Journal**: None
- **Summary**: Image understanding heavily relies on accurate multi-label classification. In recent years, deep learning algorithms have become very successful for such tasks, and various commercial and open-source APIs have been released for public use. However, these APIs are often trained on different datasets, which, besides affecting their performance, might pose a challenge to their performance evaluation. This challenge concerns the different object-class dictionaries of the APIs' training dataset and the benchmark dataset, in which the predicted labels are semantically similar to the benchmark labels but considered different simply because they have different wording in the dictionaries. To face this challenge, we propose semantic similarity metrics to obtain richer understating of the APIs predicted labels and thus their performance. In this study, we evaluate and compare the performance of 13 of the most prominent commercial and open-source APIs in a best-of-breed challenge on the Visual Genome and Open Images benchmark datasets. Our findings demonstrate that, while using traditional metrics, the Microsoft Computer Vision, Imagga, and IBM APIs performed better than others. However, applying semantic metrics also unveil the InceptionResNet-v2, Inception-v3, and ResNet50 APIs, which are trained only with the simple ImageNet dataset, as challengers for top semantic performers.



### Sparse2Dense: From direct sparse odometry to dense 3D reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1903.09199v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.09199v1)
- **Published**: 2019-03-21 19:01:37+00:00
- **Updated**: 2019-03-21 19:01:37+00:00
- **Authors**: Jiexiong Tang, John Folkesson, Patric Jensfelt
- **Comment**: Accepted to ICRA 2019 (RA-L option), video demo available at
  https://www.youtube.com/watch?v=3pbSHX72JC8&t=22s
- **Journal**: None
- **Summary**: In this paper, we proposed a new deep learning based dense monocular SLAM method. Compared to existing methods, the proposed framework constructs a dense 3D model via a sparse to dense mapping using learned surface normals. With single view learned depth estimation as prior for monocular visual odometry, we obtain both accurate positioning and high quality depth reconstruction. The depth and normal are predicted by a single network trained in a tightly coupled manner.Experimental results show that our method significantly improves the performance of visual tracking and depth prediction in comparison to the state-of-the-art in deep monocular dense SLAM.



### Multi-person Articulated Tracking with Spatial and Temporal Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1903.09214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09214v1)
- **Published**: 2019-03-21 19:42:27+00:00
- **Updated**: 2019-03-21 19:42:27+00:00
- **Authors**: Sheng Jin, Wentao Liu, Wanli Ouyang, Chen Qian
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: We propose a unified framework for multi-person pose estimation and tracking. Our framework consists of two main components,~\ie~SpatialNet and TemporalNet. The SpatialNet accomplishes body part detection and part-level data association in a single frame, while the TemporalNet groups human instances in consecutive frames into trajectories. Specifically, besides body part detection heatmaps, SpatialNet also predicts the Keypoint Embedding (KE) and Spatial Instance Embedding (SIE) for body part association. We model the grouping procedure into a differentiable Pose-Guided Grouping (PGG) module to make the whole part detection and grouping pipeline fully end-to-end trainable. TemporalNet extends spatial grouping of keypoints to temporal grouping of human instances. Given human proposals from two consecutive frames, TemporalNet exploits both appearance features encoded in Human Embedding (HE) and temporally consistent geometric features embodied in Temporal Instance Embedding (TIE) for robust tracking. Extensive experiments demonstrate the effectiveness of our proposed model. Remarkably, we demonstrate substantial improvements over the state-of-the-art pose tracking method from 65.4\% to 71.8\% Multi-Object Tracking Accuracy (MOTA) on the ICCV'17 PoseTrack Dataset.



### SkelNetOn 2019: Dataset and Challenge on Deep Learning for Geometric Shape Understanding
- **Arxiv ID**: http://arxiv.org/abs/1903.09233v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09233v3)
- **Published**: 2019-03-21 20:43:19+00:00
- **Updated**: 2019-06-22 21:59:49+00:00
- **Authors**: Ilke Demir, Camilla Hahn, Kathryn Leonard, Geraldine Morin, Dana Rahbani, Athina Panotopoulou, Amelie Fondevilla, Elena Balashova, Bastien Durix, Adam Kortylewski
- **Comment**: Dataset paper for SkelNetOn Challenge, in association with Deep
  Learning for Geometric Shape Understanding Workshop at CVPR 2019
- **Journal**: None
- **Summary**: We present SkelNetOn 2019 Challenge and Deep Learning for Geometric Shape Understanding workshop to utilize existing and develop novel deep learning architectures for shape understanding. We observed that unlike traditional segmentation and detection tasks, geometry understanding is still a new area for deep learning techniques. SkelNetOn aims to bring together researchers from different domains to foster learning methods on global shape understanding tasks. We aim to improve and evaluate the state-of-the-art shape understanding approaches, and to serve as reference benchmarks for future research. Similar to other challenges in computer vision, SkelNetOn proposes three datasets and corresponding evaluation methodologies; all coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2019 conference. In this paper, we describe and analyze characteristics of datasets, define the evaluation criteria of the public competitions, and provide baselines for each task.



### Deep Radiomics for Brain Tumor Detection and Classification from Multi-Sequence MRI
- **Arxiv ID**: http://arxiv.org/abs/1903.09240v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.09240v1)
- **Published**: 2019-03-21 21:20:37+00:00
- **Updated**: 2019-03-21 21:20:37+00:00
- **Authors**: Subhashis Banerjee, Sushmita Mitra, Francesco Masulli, Stefano Rovetta
- **Comment**: None
- **Journal**: None
- **Summary**: Glioma constitutes 80% of malignant primary brain tumors and is usually classified as HGG and LGG. The LGG tumors are less aggressive, with slower growth rate as compared to HGG, and are responsive to therapy. Tumor biopsy being challenging for brain tumor patients, noninvasive imaging techniques like Magnetic Resonance Imaging (MRI) have been extensively employed in diagnosing brain tumors. Therefore automated systems for the detection and prediction of the grade of tumors based on MRI data becomes necessary for assisting doctors in the framework of augmented intelligence. In this paper, we thoroughly investigate the power of Deep ConvNets for classification of brain tumors using multi-sequence MR images. We propose novel ConvNet models, which are trained from scratch, on MRI patches, slices, and multi-planar volumetric slices. The suitability of transfer learning for the task is next studied by applying two existing ConvNets models (VGGNet and ResNet) trained on ImageNet dataset, through fine-tuning of the last few layers. LOPO testing, and testing on the holdout dataset are used to evaluate the performance of the ConvNets. Results demonstrate that the proposed ConvNets achieve better accuracy in all cases where the model is trained on the multi-planar volumetric dataset. Unlike conventional models, it obtains a testing accuracy of 95% for the low/high grade glioma classification problem. A score of 97% is generated for classification of LGG with/without 1p/19q codeletion, without any additional effort towards extraction and selection of features. We study the properties of self-learned kernels/ filters in different layers, through visualization of the intermediate layer outputs. We also compare the results with that of state-of-the-art methods, demonstrating a maximum improvement of 7% on the grading performance of ConvNets and 9% on the prediction of 1p/19q codeletion status.



### CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1903.09254v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09254v4)
- **Published**: 2019-03-21 22:03:25+00:00
- **Updated**: 2019-04-05 22:52:17+00:00
- **Authors**: Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang
- **Comment**: Accepted for oral presentation at CVPR 2019 with review ratings of 2
  strong accepts and 1 accept (work done during an internship at NVIDIA)
- **Journal**: None
- **Summary**: Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.



### Deep Learning with Anatomical Priors: Imitating Enhanced Autoencoders in Latent Space for Improved Pelvic Bone Segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/1903.09263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09263v1)
- **Published**: 2019-03-21 22:28:14+00:00
- **Updated**: 2019-03-21 22:28:14+00:00
- **Authors**: Duc Duy Pham, Gurbandurdy Dovletov, Sebastian Warwas, Stefan Landgraeber, Marcus Jäger, Josef Pauli
- **Comment**: Accepted for IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019
- **Journal**: None
- **Summary**: We propose a 2D Encoder-Decoder based deep learning architecture for semantic segmentation, that incorporates anatomical priors by imitating the encoder component of an autoencoder in latent space. The autoencoder is additionally enhanced by means of hierarchical features, extracted by an U-Net module. Our suggested architecture is trained in an end-to-end manner and is evaluated on the example of pelvic bone segmentation in MRI. A comparison to the standard U-Net architecture shows promising improvements.



### Adversarial camera stickers: A physical camera-based attack on deep learning systems
- **Arxiv ID**: http://arxiv.org/abs/1904.00759v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00759v4)
- **Published**: 2019-03-21 23:33:12+00:00
- **Updated**: 2019-06-08 19:23:56+00:00
- **Authors**: Juncheng Li, Frank R. Schmidt, J. Zico Kolter
- **Comment**: None
- **Journal**: Proceedings of the 36th International Conference on Machine
  Learning, PMLR 97:3896-3904, 2019
- **Summary**: Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable). For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54



### Fast and accurate reconstruction of HARDI using a 1D encoder-decoder convolutional network
- **Arxiv ID**: http://arxiv.org/abs/1903.09272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09272v1)
- **Published**: 2019-03-21 23:56:16+00:00
- **Updated**: 2019-03-21 23:56:16+00:00
- **Authors**: Shi Yin, Zhengqiang Zhang, Qinmu Peng, Xinge You
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: High angular resolution diffusion imaging (HARDI) demands a lager amount of data measurements compared to diffusion tensor imaging, restricting its use in practice. In this work, we explore a learning-based approach to reconstruct HARDI from a smaller number of measurements in q-space. The approach aims to directly learn the mapping relationship between the measured and HARDI signals from the collecting HARDI acquisitions of other subjects. Specifically, the mapping is represented as a 1D encoder-decoder convolutional neural network under the guidance of the compressed sensing (CS) theory for HARDI reconstruction. The proposed network architecture mainly consists of two parts: an encoder network produces the sparse coefficients and a decoder network yields a reconstruction result. Experiment results demonstrate we can robustly reconstruct HARDI signals with the accurate results and fast speed.



