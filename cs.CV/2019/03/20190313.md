# Arxiv Papers in cs.CV on 2019-03-13
### All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.05285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05285v1)
- **Published**: 2019-03-13 01:44:39+00:00
- **Updated**: 2019-03-13 01:44:39+00:00
- **Authors**: Weijie Chen, Di Xie, Yuan Zhang, Shiliang Pu
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: Shift operation is an efficient alternative over depthwise separable convolution. However, it is still bottlenecked by its implementation manner, namely memory movement. To put this direction forward, a new and novel basic component named Sparse Shift Layer (SSL) is introduced in this paper to construct efficient convolutional neural networks. In this family of architectures, the basic block is only composed by 1x1 convolutional layers with only a few shift operations applied to the intermediate feature maps. To make this idea feasible, we introduce shift operation penalty during optimization and further propose a quantization-aware shift learning method to impose the learned displacement more friendly for inference. Extensive ablation studies indicate that only a few shift operations are sufficient to provide spatial information communication. Furthermore, to maximize the role of SSL, we redesign an improved network architecture to Fully Exploit the limited capacity of neural Network (FE-Net). Equipped with SSL, this network can achieve 75.0% top-1 accuracy on ImageNet with only 563M M-Adds. It surpasses other counterparts constructed by depthwise separable convolution and the networks searched by NAS in terms of accuracy and practical speed.



### Multimodal Emotion Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.12520v1
- **DOI**: 10.1145/3308560.3316549
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1903.12520v1)
- **Published**: 2019-03-13 07:54:29+00:00
- **Updated**: 2019-03-13 07:54:29+00:00
- **Authors**: Anurag Illendula, Amit Sheth
- **Comment**: Accepted at the 2nd Emoji Workshop co-located with The Web Conference
  2019
- **Journal**: Companion Proceedings of the 2019 World Wide Web Conference
- **Summary**: Most NLP and Computer Vision tasks are limited to scarcity of labelled data. In social media emotion classification and other related tasks, hashtags have been used as indicators to label data. With the rapid increase in emoji usage of social media, emojis are used as an additional feature for major social NLP tasks. However, this is less explored in case of multimedia posts on social media where posts are composed of both image and text. At the same time, w.e have seen a surge in the interest to incorporate domain knowledge to improve machine understanding of text. In this paper, we investigate whether domain knowledge for emoji can improve the accuracy of emotion classification task. We exploit the importance of different modalities from social media post for emotion classification task using state-of-the-art deep learning architectures. Our experiments demonstrate that the three modalities (text, emoji and images) encode different information to express emotion and therefore can complement each other. Our results also demonstrate that emoji sense depends on the textual context, and emoji combined with text encodes better information than considered separately. The highest accuracy of 71.98\% is achieved with a training data of 550k posts.



### CIA-Net: Robust Nuclei Instance Segmentation with Contour-aware Information Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1903.05358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05358v1)
- **Published**: 2019-03-13 08:43:01+00:00
- **Updated**: 2019-03-13 08:43:01+00:00
- **Authors**: Yanning Zhou, Omer Fahri Onder, Qi Dou, Efstratios Tsougenis, Hao Chen, Pheng-Ann Heng
- **Comment**: Accepted for the 26th Conference on Information Processing in Medical
  Imaging (IPMI 2019)
- **Journal**: None
- **Summary**: Accurate segmenting nuclei instances is a crucial step in computer-aided image analysis to extract rich features for cellular estimation and following diagnosis as well as treatment. While it still remains challenging because the wide existence of nuclei clusters, along with the large morphological variances among different organs make nuclei instance segmentation susceptible to over-/under-segmentation. Additionally, the inevitably subjective annotating and mislabeling prevent the network learning from reliable samples and eventually reduce the generalization capability for robustly segmenting unseen organ nuclei. To address these issues, we propose a novel deep neural network, namely Contour-aware Informative Aggregation Network (CIA-Net) with multi-level information aggregation module between two task-specific decoders. Rather than independent decoders, it leverages the merit of spatial and texture dependencies between nuclei and contour by bi-directionally aggregating task-specific features. Furthermore, we proposed a novel smooth truncated loss that modulates losses to reduce the perturbation from outliers. Consequently, the network can focus on learning from reliable and informative samples, which inherently improves the generalization capability. Experiments on the 2018 MICCAI challenge of Multi-Organ-Nuclei-Segmentation validated the effectiveness of our proposed method, surpassing all the other 35 competitive teams by a significant margin.



### Asymmetric Residual Neural Network for Accurate Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.05359v3
- **DOI**: 10.3390/info10060203
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05359v3)
- **Published**: 2019-03-13 08:44:01+00:00
- **Updated**: 2019-06-11 13:05:37+00:00
- **Authors**: Jun Long, WuQing Sun, Zhan Yang, Osolo Ian Raymond
- **Comment**: Accepted by Information
- **Journal**: None
- **Summary**: Human Activity Recognition (HAR) using deep neural network has become a hot topic in human-computer interaction. Machine can effectively identify human naturalistic activities by learning from a large collection of sensor data. Activity recognition is not only an interesting research problem, but also has many real-world practical applications. Based on the success of residual networks in achieving a high level of aesthetic representation of the automatic learning, we propose a novel \textbf{A}symmetric \textbf{R}esidual \textbf{N}etwork, named ARN. ARN is implemented using two identical path frameworks consisting of (1) a short time window, which is used to capture spatial features, and (2) a long time window, which is used to capture fine temporal features. The long time window path can be made very lightweight by reducing its channel capacity, yet still being able to learn useful temporal representations for activity recognition. In this paper, we mainly focus on proposing a new model to improve the accuracy of HAR. In order to demonstrate the effectiveness of ARN model, we carried out extensive experiments on benchmark datasets (i.e., OPPORTUNITY, UniMiB-SHAR) and compared with some conventional and state-of-the-art learning-based methods. Then, we discuss the influence of networks parameters on performance to provide insights about its optimization. Results from our experiments show that ARN is effective in recognizing human activities via wearable datasets.



### Face Liveness Detection Based on Client Identity Using Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1903.05369v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05369v1)
- **Published**: 2019-03-13 09:12:38+00:00
- **Updated**: 2019-03-13 09:12:38+00:00
- **Authors**: Huiling Hao, Mingtao Pei
- **Comment**: None
- **Journal**: None
- **Summary**: Face liveness detection is an essential prerequisite for face recognition applications. Previous face liveness detection methods usually train a binary classifier to differentiate between a fake face and a real face before face recognition. The client identity information is not utilized in previous face liveness detection methods. However, in practical face recognition applications, face spoofing attacks are always aimed at a specific client, and the client identity information can provide useful clues for face liveness detection. In this paper, we propose a face liveness detection method based on the client identity using Siamese network. We detect face liveness after face recognition instead of before face recognition, that is, we detect face liveness with the client identity information. We train a Siamese network with image pairs. Each image pair consists of two real face images or one real and one fake face images. The face images in each pair come from a same client. Given a test face image, the face image is firstly recognized by face recognition system, then the real face image of the identified client is retrieved to help the face liveness detection. Experiment results demonstrate the effectiveness of our method.



### Depth Coefficients for Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1903.05421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05421v1)
- **Published**: 2019-03-13 11:39:00+00:00
- **Updated**: 2019-03-13 11:39:00+00:00
- **Authors**: Saif Imran, Yunfei Long, Xiaoming Liu, Daniel Morris
- **Comment**: to appear in Proc. IEEE Conf. Computer Vision and Pattern Recognition
  (CVPR) 2019
- **Journal**: None
- **Summary**: Depth completion involves estimating a dense depth image from sparse depth measurements, often guided by a color image. While linear upsampling is straight forward, it results in artifacts including depth pixels being interpolated in empty space across discontinuities between objects. Current methods use deep networks to upsample and "complete" the missing depth pixels. Nevertheless, depth smearing between objects remains a challenge. We propose a new representation for depth called Depth Coefficients (DC) to address this problem. It enables convolutions to more easily avoid inter-object depth mixing. We also show that the standard Mean Squared Error (MSE) loss function can promote depth mixing, and thus propose instead to use cross-entropy loss for DC. With quantitative and qualitative evaluation on benchmarks, we show that switching out sparse depth input and MSE loss with our DC representation and cross-entropy loss is a simple way to improve depth completion performance, and reduce pixel depth mixing, which leads to improved depth-based object detection.



### Visual Semantic Information Pursuit: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1903.05434v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05434v1)
- **Published**: 2019-03-13 12:01:12+00:00
- **Updated**: 2019-03-13 12:01:12+00:00
- **Authors**: Daqi Liu, Miroslaw Bober, Josef Kittler
- **Comment**: Preliminary work. Under review by IEEE Transactions on Pattern
  Analysis and Machine Intelligence (PAMI). Do not distribute
- **Journal**: None
- **Summary**: Visual semantic information comprises two important parts: the meaning of each visual semantic unit and the coherent visual semantic relation conveyed by these visual semantic units. Essentially, the former one is a visual perception task while the latter one corresponds to visual context reasoning. Remarkable advances in visual perception have been achieved due to the success of deep learning. In contrast, visual semantic information pursuit, a visual scene semantic interpretation task combining visual perception and visual context reasoning, is still in its early stage. It is the core task of many different computer vision applications, such as object detection, visual semantic segmentation, visual relationship detection or scene graph generation. Since it helps to enhance the accuracy and the consistency of the resulting interpretation, visual context reasoning is often incorporated with visual perception in current deep end-to-end visual semantic information pursuit methods. However, a comprehensive review for this exciting area is still lacking. In this survey, we present a unified theoretical paradigm for all these methods, followed by an overview of the major developments and the future trends in each potential direction. The common benchmark datasets, the evaluation metrics and the comparisons of the corresponding methods are also introduced.



### Towards Accurate Camera Geopositioning by Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1903.05454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05454v1)
- **Published**: 2019-03-13 12:37:42+00:00
- **Updated**: 2019-03-13 12:37:42+00:00
- **Authors**: Raffaele Imbriaco, Clint Sebastian, Egor Bondarev, Peter de With
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a camera geopositioning system based on matching a query image against a database with panoramic images. For matching, our system uses memory vectors aggregated from global image descriptors based on convolutional features to facilitate fast searching in the database. To speed up searching, a clustering algorithm is used to balance geographical positioning and computation time. We refine the obtained position from the query image using a new outlier removal algorithm. The matching of the query image is obtained with a recall@5 larger than 90% for panorama-to-panorama matching. We cluster available panoramas from geographically adjacent locations into a single compact representation and observe computational gains of approximately 50% at the cost of only a small (approximately 3%) recall loss. Finally, we present a coordinate estimation algorithm that reduces the median geopositioning error by up to 20%.



### Hardness-Aware Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.05503v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05503v2)
- **Published**: 2019-03-13 14:14:54+00:00
- **Updated**: 2019-12-04 13:10:32+00:00
- **Authors**: Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, Jie Zhou
- **Comment**: Accepted as CVPR 2019 Oral. Source code available at
  https://github.com/wzzheng/HDML
- **Journal**: None
- **Summary**: This paper presents a hardness-aware deep metric learning (HDML) framework. Most previous deep metric learning methods employ the hard negative mining strategy to alleviate the lack of informative samples for training. However, this mining strategy only utilizes a subset of training data, which may not be enough to characterize the global geometry of the embedding space comprehensively. To address this problem, we perform linear interpolation on embeddings to adaptively manipulate their hard levels and generate corresponding label-preserving synthetics for recycled training, so that information buried in all samples can be fully exploited and the metric is always challenged with proper difficulty. Our method achieves very competitive performance on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets.



### Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.05558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05558v2)
- **Published**: 2019-03-13 15:51:50+00:00
- **Updated**: 2019-04-23 12:25:42+00:00
- **Authors**: Ruirui Li, Mingming Li, Jiacheng Li, Yating Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We develop a connection sensitive attention U-Net(CSAU) for accurate retinal vessel segmentation. This method improves the recent attention U-Net for semantic segmentation with four key improvements: (1) connection sensitive loss that models the structure properties to improve the accuracy of pixel-wise segmentation; (2) attention gate with novel neural network structure and concatenating DOWN-Link to effectively learn better attention weights on fine vessels; (3) integration of connection sensitive loss and attention gate to further improve the accuracy on detailed vessels by additionally concatenating attention weights to features before output; (4) metrics of connection sensitive accuracy to reflect the segmentation performance on boundaries and thin vessels.   Our method can effectively improve state-of-the-art vessel segmentation methods that suffer from difficulties in presence of abnormalities, bifurcation and microvascular. This connection sensitive loss tightly integrates with the proposed attention U-Net to accurately (i) segment retinal vessels, and (ii) reserve the connectivity of thin vessels by modeling the structural properties. Our method achieves the leading position on DRIVE, STARE and HRF datasets among the state-of-the-art methods.



### Privacy Preserving Image-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1903.05572v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05572v1)
- **Published**: 2019-03-13 16:12:04+00:00
- **Updated**: 2019-03-13 16:12:04+00:00
- **Authors**: Pablo Speciale, Johannes L. Schönberger, Sing Bing Kang, Sudipta N. Sinha, Marc Pollefeys
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based localization is a core component of many augmented/mixed reality (AR/MR) and autonomous robotic systems. Current localization systems rely on the persistent storage of 3D point clouds of the scene to enable camera pose estimation, but such data reveals potentially sensitive scene information. This gives rise to significant privacy risks, especially as for many applications 3D mapping is a background process that the user might not be fully aware of. We pose the following question: How can we avoid disclosing confidential information about the captured 3D scene, and yet allow reliable camera pose estimation? This paper proposes the first solution to what we call privacy preserving image-based localization. The key idea of our approach is to lift the map representation from a 3D point cloud to a 3D line cloud. This novel representation obfuscates the underlying scene geometry while providing sufficient geometric constraints to enable robust and accurate 6-DOF camera pose estimation. Extensive experiments on several datasets and localization scenarios underline the high practical relevance of our proposed approach.



### Two-Stream Action Recognition-Oriented Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.05577v2
- **DOI**: 10.1109/ICCV.2019.00889
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05577v2)
- **Published**: 2019-03-13 16:22:36+00:00
- **Updated**: 2019-08-16 01:40:03+00:00
- **Authors**: Haochen Zhang, Dong Liu, Zhiwei Xiong
- **Comment**: Accepted to ICCV 2019. Code:
  https://github.com/AlanZhang1995/TwoStreamSR
- **Journal**: None
- **Summary**: We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.



### Hyperspectral Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.05580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05580v1)
- **Published**: 2019-03-13 16:27:38+00:00
- **Updated**: 2019-03-13 16:27:38+00:00
- **Authors**: Jakub Nalepa, Michal Myller, Michal Kawulok
- **Comment**: Submitted to IEEE Geoscience and Remote Sensing Letters
- **Journal**: None
- **Summary**: Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks. It plays a pivotal role in remote-sensing scenarios in which the amount of high-quality ground truth data is limited, and acquiring new examples is costly or impossible. This is a common problem in hyperspectral imaging, where manual annotation of image data is difficult, expensive, and prone to human bias. In this letter, we propose online data augmentation of hyperspectral data which is executed during the inference rather than before the training of deep networks. This is in contrast to all other state-of-the-art hyperspectral augmentation algorithms which increase the size (and representativeness) of training sets. Additionally, we introduce a new principal component analysis based augmentation. The experiments revealed that our data augmentation algorithms improve generalization of deep networks, work in real-time, and the online approach can be effectively combined with offline techniques to enhance the classification accuracy.



### LiDAR-assisted Large-scale Privacy Protection in Street-view Cycloramas
- **Arxiv ID**: http://arxiv.org/abs/1903.05598v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05598v1)
- **Published**: 2019-03-13 16:57:03+00:00
- **Updated**: 2019-03-13 16:57:03+00:00
- **Authors**: Clint Sebastian, Bas Boom, Egor Bondarev, Peter H. N. de With
- **Comment**: Accepted at Electronic Imaging 2019
- **Journal**: None
- **Summary**: Recently, privacy has a growing importance in several domains, especially in street-view images. The conventional way to achieve this is to automatically detect and blur sensitive information from these images. However, the processing cost of blurring increases with the ever-growing resolution of images. We propose a system that is cost-effective even after increasing the resolution by a factor of 2.5. The new system utilizes depth data obtained from LiDAR to significantly reduce the search space for detection, thereby reducing the processing cost. Besides this, we test several detectors after reducing the detection space and provide an alternative solution based on state-of-the-art deep learning detectors to the existing HoG-SVM-Deep system that is faster and has a higher performance.



### RVOS: End-to-End Recurrent Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.05612v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05612v2)
- **Published**: 2019-03-13 17:26:15+00:00
- **Updated**: 2019-05-21 06:56:56+00:00
- **Authors**: Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, Xavier Giro-i-Nieto
- **Comment**: CVPR 2019 camera ready. Project website:
  https://imatge-upc.github.io/rvos/
- **Journal**: None
- **Summary**: Multiple object video object segmentation is a challenging task, specially for the zero-shot case, when no object mask is given at the initial frame and the model has to find the objects to be segmented along the sequence. In our work, we propose a Recurrent network for multiple object Video Object Segmentation (RVOS) that is fully end-to-end trainable. Our model incorporates recurrence on two different domains: (i) the spatial, which allows to discover the different object instances within a frame, and (ii) the temporal, which allows to keep the coherence of the segmented objects along time. We train RVOS for zero-shot video object segmentation and are the first ones to report quantitative results for DAVIS-2017 and YouTube-VOS benchmarks. Further, we adapt RVOS for one-shot video object segmentation by using the masks obtained in previous time steps as inputs to be processed by the recurrent module. Our model reaches comparable results to state-of-the-art techniques in YouTube-VOS benchmark and outperforms all previous video object segmentation methods not using online learning in the DAVIS-2017 benchmark. Moreover, our model achieves faster inference runtimes than previous methods, reaching 44ms/frame on a P100 GPU.



### Tracking without bells and whistles
- **Arxiv ID**: http://arxiv.org/abs/1903.05625v3
- **DOI**: 10.1109/ICCV.2019.00103
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05625v3)
- **Published**: 2019-03-13 17:45:49+00:00
- **Updated**: 2019-08-17 14:40:56+00:00
- **Authors**: Philipp Bergmann, Tim Meinhardt, Laura Leal-Taixe
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.



### Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1903.05628v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05628v6)
- **Published**: 2019-03-13 17:50:36+00:00
- **Updated**: 2019-05-04 02:12:05+00:00
- **Authors**: Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, Ming-Hsuan Yang
- **Comment**: CVPR 2019. Code: https://github.com/HelenMao/MSGAN
- **Journal**: None
- **Summary**: Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.



### Scalable Deep Convolutional Neural Networks for Sparse, Locally Dense Liquid Argon Time Projection Chamber Data
- **Arxiv ID**: http://arxiv.org/abs/1903.05663v3
- **DOI**: 10.1103/PhysRevD.102.012005
- **Categories**: **hep-ex**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05663v3)
- **Published**: 2019-03-13 18:25:20+00:00
- **Updated**: 2019-12-21 03:21:11+00:00
- **Authors**: Laura Dominé, Kazuhiro Terao
- **Comment**: Corrected URL to dataset Corrected figures and numbers
- **Journal**: Phys. Rev. D 102, 012005 (2020)
- **Summary**: Deep convolutional neural networks (CNNs) show strong promise for analyzing scientific data in many domains including particle imaging detectors such as a liquid argon time projection chamber (LArTPC). Yet the high sparsity of LArTPC data challenges traditional CNNs which were designed for dense data such as photographs. A naive application of CNNs on LArTPC data results in inefficient computations and a poor scalability to large LArTPC detectors such as the Short Baseline Neutrino Program and Deep Underground Neutrino Experiment. Recently Submanifold Sparse Convolutional Networks (SSCNs) have been proposed to address this challenge. We report their performance on a 3D semantic segmentation task on simulated LArTPC samples. In comparison with standard CNNs, we observe that the computation memory and wall-time cost for inference are reduced by factor of 364 and 33 respectively without loss of accuracy. The same factors for 2D samples are found to be 93 and 3.1 respectively. Using SSCN, we present the first machine learning-based approach to the reconstruction of Michel electrons using public 3D LArTPC samples. We find a Michel electron identification efficiency of 93.9% with 96.7% of true positive rate. Reconstructed Michel electron clusters yield 95.4% in average pixel clustering efficiency and 95.5% in purity. The results are compelling to show strong promise of scalable data reconstruction technique using deep neural networks for large scale LArTPC detectors.



### Neural Scene Decomposition for Multi-Person Motion Capture
- **Arxiv ID**: http://arxiv.org/abs/1903.05684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05684v1)
- **Published**: 2019-03-13 19:02:46+00:00
- **Updated**: 2019-03-13 19:02:46+00:00
- **Authors**: Helge Rhodin, Victor Constantin, Isinsu Katircioglu, Mathieu Salzmann, Pascal Fua
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Learning general image representations has proven key to the success of many computer vision tasks. For example, many approaches to image understanding problems rely on deep networks that were initially trained on ImageNet, mostly because the learned features are a valuable starting point to learn from limited labeled data. However, when it comes to 3D motion capture of multiple people, these features are only of limited use.   In this paper, we therefore propose an approach to learning features that are useful for this purpose. To this end, we introduce a self-supervised approach to learning what we call a neural scene decomposition (NSD) that can be exploited for 3D pose estimation. NSD comprises three layers of abstraction to represent human subjects: spatial layout in terms of bounding-boxes and relative depth; a 2D shape representation in terms of an instance segmentation mask; and subject-specific appearance and 3D pose information. By exploiting self-supervision coming from multiview data, our NSD model can be trained end-to-end without any 2D or 3D supervision. In contrast to previous approaches, it works for multiple persons and full-frame images. Because it encodes 3D geometry, NSD can then be effectively leveraged to train a 3D pose estimation network from small amounts of annotated data.



### Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1903.05690v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05690v2)
- **Published**: 2019-03-13 19:16:15+00:00
- **Updated**: 2019-03-15 19:28:24+00:00
- **Authors**: Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, Jan Kautz
- **Comment**: https://sites.google.com/view/3d-affordance-cvpr19
- **Journal**: CVPR 2019
- **Summary**: Affordance modeling plays an important role in visual understanding. In this paper, we aim to predict affordances of 3D indoor scenes, specifically what human poses are afforded by a given indoor environment, such as sitting on a chair or standing on the floor. In order to predict valid affordances and learn possible 3D human poses in indoor scenes, we need to understand the semantic and geometric structure of a scene as well as its potential interactions with a human. To learn such a model, a large-scale dataset of 3D indoor affordances is required. In this work, we build a fully automatic 3D pose synthesizer that fuses semantic knowledge from a large number of 2D poses extracted from TV shows as well as 3D geometric knowledge from voxel representations of indoor scenes. With the data created by the synthesizer, we introduce a 3D pose generative model to predict semantically plausible and physically feasible human poses within a given scene (provided as a single RGB, RGB-D, or depth image). We demonstrate that our human affordance prediction method consistently outperforms existing state-of-the-art methods.



### Aesthetics of Neural Network Art
- **Arxiv ID**: http://arxiv.org/abs/1903.05696v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05696v2)
- **Published**: 2019-03-13 19:45:54+00:00
- **Updated**: 2019-03-18 17:58:15+00:00
- **Authors**: Aaron Hertzmann
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a way to understand neural network artworks as juxtapositions of natural image cues. It is hypothesized that images with unusual combinations of realistic visual cues are interesting, and, neural models trained to model natural images are well-suited to creating interesting images. Art using neural models produces new images similar to those of natural images, but with weird and intriguing variations. This analysis is applied to neural art based on Generative Adversarial Networks, image stylization, Deep Dreams, and Perception Engines.



### PointNetLK: Robust & Efficient Point Cloud Registration using PointNet
- **Arxiv ID**: http://arxiv.org/abs/1903.05711v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05711v2)
- **Published**: 2019-03-13 20:56:03+00:00
- **Updated**: 2019-04-04 16:27:35+00:00
- **Authors**: Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivatsan, Simon Lucey
- **Comment**: Accepted in CVPR 2019. v2: updated affiliations, additional result in
  Fig 1
- **Journal**: None
- **Summary**: PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent extensions are state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable "imaging" function. As a consequence, classical vision algorithms for image alignment can be applied on the problem - namely the Lucas & Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency - opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK.



### Inferring 3D Shapes of Unknown Rigid Objects in Clutter through Inverse Physics Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1903.05749v1
- **DOI**: 10.1109/LRA.2018.2885579
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05749v1)
- **Published**: 2019-03-13 23:05:08+00:00
- **Updated**: 2019-03-13 23:05:08+00:00
- **Authors**: Changkyu Song, Abdeslam Boularias
- **Comment**: None
- **Journal**: The IEEE Robotics and Automation Letters (RA-L) with the IEEE
  International Conference on Robotics and Automation (ICRA 2019)
- **Summary**: We present a probabilistic approach for building, on the fly, 3-D models of unknown objects while being manipulated by a robot. We specifically consider manipulation tasks in piles of clutter that contain previously unseen objects. Most manipulation algorithms for performing such tasks require known geometric models of the objects in order to grasp or rearrange them robustly. One of the novel aspects of this work is the utilization of a physics engine for verifying hypothesized geometries in simulation. The evidence provided by physics simulations is used in a probabilistic framework that accounts for the fact that mechanical properties of the objects are uncertain. We present an efficient algorithm for inferring occluded parts of objects based on their observed motions and mutual interactions. Experiments using a robot show that this approach is efficient for constructing physically realistic 3-D models, which can be useful for manipulation planning. Experiments also show that the proposed approach significantly outperforms alternative approaches in terms of shape accuracy.



### VRKitchen: an Interactive 3D Virtual Environment for Task-oriented Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.05757v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05757v1)
- **Published**: 2019-03-13 23:31:21+00:00
- **Updated**: 2019-03-13 23:31:21+00:00
- **Authors**: Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu Wang, Song-Chun Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: One of the main challenges of advancing task-oriented learning such as visual task planning and reinforcement learning is the lack of realistic and standardized environments for training and testing AI agents. Previously, researchers often relied on ad-hoc lab environments. There have been recent advances in virtual systems built with 3D physics engines and photo-realistic rendering for indoor and outdoor environments, but the embodied agents in those systems can only conduct simple interactions with the world (e.g., walking around, moving objects, etc.). Most of the existing systems also do not allow human participation in their simulated environments. In this work, we design and implement a virtual reality (VR) system, VRKitchen, with integrated functions which i) enable embodied agents powered by modern AI methods (e.g., planning, reinforcement learning, etc.) to perform complex tasks involving a wide range of fine-grained object manipulations in a realistic environment, and ii) allow human teachers to perform demonstrations to train agents (i.e., learning from demonstration). We also provide standardized evaluation benchmarks and data collection tools to facilitate a broad use in research on task-oriented learning and beyond.



### LPM: Learnable Pooling Module for Efficient Full-Face Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.05761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05761v2)
- **Published**: 2019-03-13 23:47:23+00:00
- **Updated**: 2019-03-21 03:16:40+00:00
- **Authors**: Reo Ogusu, Takao Yamanaka
- **Comment**: FG2019
- **Journal**: None
- **Summary**: Gaze tracking is an important technology in many domains. Techniques such as Convolutional Neural Networks (CNN) has allowed the invention of gaze tracking method that relies only on commodity hardware such as the camera on a personal computer. It has been shown that the full-face region for gaze estimation can provide better performance than from an eye image alone. However, a problem with using the full-face image is the heavy computation due to the larger image size. This study tackles this problem through compression of the input full-face image by removing redundant information using a novel learnable pooling module. The module can be trained end-to-end by backpropagation to learn the size of the grid in the pooling filter. The learnable pooling module keeps the resolution of valuable regions high and vice versa. This proposed method preserved the gaze estimation accuracy at a certain level when the image was reduced to a smaller size.



