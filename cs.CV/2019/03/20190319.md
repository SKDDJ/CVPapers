# Arxiv Papers in cs.CV on 2019-03-19
### Probabilistic End-to-end Noise Correction for Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1903.07788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07788v1)
- **Published**: 2019-03-19 01:38:08+00:00
- **Updated**: 2019-03-19 01:38:08+00:00
- **Authors**: Kun Yi, Jianxin Wu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Deep learning has achieved excellent performance in various computer vision tasks, but requires a lot of training examples with clean labels. It is easy to collect a dataset with noisy labels, but such noise makes networks overfit seriously and accuracies drop dramatically. To address this problem, we propose an end-to-end framework called PENCIL, which can update both network parameters and label estimations as label distributions. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is more general and robust than existing methods and is easy to apply. PENCIL outperforms previous state-of-the-art methods by large margins on both synthetic and real-world datasets with different noise types and noise rates. Experiments show that PENCIL is robust on clean datasets, too.



### Predicting Citywide Crowd Flows in Irregular Regions Using Multi-View Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.07789v2
- **DOI**: 10.1109/TKDE.2020.3008774
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07789v2)
- **Published**: 2019-03-19 01:46:11+00:00
- **Updated**: 2020-07-17 08:34:11+00:00
- **Authors**: Junkai Sun, Junbo Zhang, Qiaofei Li, Xiuwen Yi, Yuxuan Liang, Yu Zheng
- **Comment**: 12 pages, 13 figures, 5 tables. Published in IEEE TKDE, Date of
  Publication: Jul. 13, 2020
- **Journal**: IEEE Transactions on Knowledge and Data Engineering (TKDE), 2020
- **Summary**: Being able to predict the crowd flows in each and every part of a city, especially in irregular regions, is strategically important for traffic control, risk assessment, and public safety. However, it is very challenging because of interactions and spatial correlations between different regions. In addition, it is affected by many factors: i) multiple temporal correlations among different time intervals: closeness, period, trend; ii) complex external influential factors: weather, events; iii) meta features: time of the day, day of the week, and so on. In this paper, we formulate crowd flow forecasting in irregular regions as a spatio-temporal graph (STG) prediction problem in which each node represents a region with time-varying flows. By extending graph convolution to handle the spatial information, we propose using spatial graph convolution to build a multi-view graph convolutional network (MVGCN) for the crowd flow forecasting problem, where different views can capture different factors as mentioned above. We evaluate MVGCN using four real-world datasets (taxicabs and bikes) and extensive experimental results show that our approach outperforms the adaptations of state-of-the-art methods. And we have developed a crowd flow forecasting system for irregular regions that can now be used internally.



### Robust Visual Tracking Using Dynamic Classifier Selection with Sparse Representation of Label Noise
- **Arxiv ID**: http://arxiv.org/abs/1903.07801v1
- **DOI**: 10.1007/978-3-642-37431-9_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07801v1)
- **Published**: 2019-03-19 02:48:44+00:00
- **Updated**: 2019-03-19 02:48:44+00:00
- **Authors**: Yuefeng Chen, Qing Wang
- **Comment**: accepted at ACCV2012, Oral
- **Journal**: None
- **Summary**: Recently a category of tracking methods based on "tracking-by-detection" is widely used in visual tracking problem. Most of these methods update the classifier online using the samples generated by the tracker to handle the appearance changes. However, the self-updating scheme makes these methods suffer from drifting problem because of the incorrect labels of weak classifiers in training samples. In this paper, we split the class labels into true labels and noise labels and model them by sparse representation. A novel dynamic classifier selection method, robust to noisy training data, is proposed. Moreover, we apply the proposed classifier selection algorithm to visual tracking by integrating a part based online boosting framework. We have evaluated our proposed method on 12 challenging sequences involving severe occlusions, significant illumination changes and large pose variations. Both the qualitative and quantitative evaluations demonstrate that our approach tracks objects accurately and robustly and outperforms state-of-the-art trackers.



### Dynamic Deep Networks for Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.07803v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07803v2)
- **Published**: 2019-03-19 03:02:24+00:00
- **Updated**: 2019-03-27 14:37:17+00:00
- **Authors**: Aashis Khanal, Rolando Estrada
- **Comment**: None
- **Journal**: None
- **Summary**: Segmenting the retinal vasculature entails a trade-off between how much of the overall vascular structure we identify vs. how precisely we segment individual vessels. In particular, state-of-the-art methods tend to under-segment faint vessels, as well as pixels that lie on the edges of thicker vessels. Thus, they underestimate the width of individual vessels, as well as the ratio of large to small vessels. More generally, many crucial bio-markers---including the artery-vein (AV) ratio, branching angles, number of bifurcation, fractal dimension, tortuosity, vascular length-to-diameter ratio and wall-to-lumen length---require precise measurements of individual vessels. To address this limitation, we propose a novel, stochastic training scheme for deep neural networks that better classifies the faint, ambiguous regions of the image. Our approach relies on two key innovations. First, we train our deep networks with dynamic weights that fluctuate during each training iteration. This stochastic approach forces the network to learn a mapping that robustly balances precision and recall. Second, we decouple the segmentation process into two steps. In the first half of our pipeline, we estimate the likelihood of every pixel and then use these likelihoods to segment pixels that are clearly vessel or background. In the latter part of our pipeline, we use a second network to classify the ambiguous regions in the image. Our proposed method obtained state-of-the-art results on five retinal datasets---DRIVE, STARE, CHASE-DB, AV-WIDE, and VEVIO---by learning a robust balance between false positive and false negative rates. In addition, we are the first to report segmentation results on the AV-WIDE dataset, and we have made the ground-truth annotations for this dataset publicly available.



### Self-Weighted Multiview Metric Learning by Maximizing the Cross Correlations
- **Arxiv ID**: http://arxiv.org/abs/1903.07812v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07812v1)
- **Published**: 2019-03-19 03:40:40+00:00
- **Updated**: 2019-03-19 03:40:40+00:00
- **Authors**: Huibing Wang, Jinjia Peng, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of multimedia time, one sample can always be described from multiple views which contain compatible and complementary information. Most algorithms cannot take information from multiple views into considerations and fail to achieve desirable performance in most situations. For many applications, such as image retrieval, face recognition, etc., an appropriate distance metric can better reflect the similarities between various samples. Therefore, how to construct a good distance metric learning methods which can deal with multiview data has been an important topic during the last decade. In this paper, we proposed a novel algorithm named Self-weighted Multiview Metric Learning (SM2L) which can finish this task by maximizing the cross correlations between different views. Furthermore, because multiple views have different contributions to the learning procedure of SM2L, we adopt a self-weighted learning framework to assign multiple views with different weights. Various experiments on benchmark datasets can verify the performance of our proposed method.



### Mask-guided Style Transfer Network for Purifying Real Images
- **Arxiv ID**: http://arxiv.org/abs/1903.08152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08152v1)
- **Published**: 2019-03-19 03:54:55+00:00
- **Updated**: 2019-03-19 03:54:55+00:00
- **Authors**: Tongtong Zhao, Yuxiao Yan, Jinjia Peng, Huibing Wang, Xianping Fu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.05820
- **Journal**: None
- **Summary**: Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared with real images, the desired performance cannot be achieved. To solve this problem, the previous method learned a model to improve the realism of the synthetic images. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a mask-guided style transfer network to learn style features separately from the attention and bkgd(background) regions and learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on various public datasets, including LPW, COCO and MPIIGaze. Experimental results show that the proposed method is effective and achieves the state-of-the-art results.



### Trick or TReAT: Thematic Reinforcement for Artistic Typography
- **Arxiv ID**: http://arxiv.org/abs/1903.07820v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07820v1)
- **Published**: 2019-03-19 04:08:51+00:00
- **Updated**: 2019-03-19 04:08:51+00:00
- **Authors**: Purva Tendulkar, Kalpesh Krishna, Ramprasaath R. Selvaraju, Devi Parikh
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: An approach to make text visually appealing and memorable is semantic reinforcement - the use of visual cues alluding to the context or theme in which the word is being used to reinforce the message (e.g., Google Doodles). We present a computational approach for semantic reinforcement called TReAT - Thematic Reinforcement for Artistic Typography. Given an input word (e.g. exam) and a theme (e.g. education), the individual letters of the input word are replaced by cliparts relevant to the theme which visually resemble the letters - adding creative context to the potentially boring input word. We use an unsupervised approach to learn a latent space to represent letters and cliparts and compute similarities between the two. Human studies show that participants can reliably recognize the word as well as the theme in our outputs (TReATs) and find them more creative compared to meaningful baselines.



### Compressed Sensing: From Research to Clinical Practice with Data-Driven Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.07824v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.07824v1)
- **Published**: 2019-03-19 04:28:07+00:00
- **Updated**: 2019-03-19 04:28:07+00:00
- **Authors**: Joseph Y. Cheng, Feiyu Chen, Christopher Sandino, Morteza Mardani, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: Submitted to the Special Issue on Computational MRI: Compressed
  Sensing and Beyond in the IEEE Signal Processing Magazine
- **Journal**: None
- **Summary**: Compressed sensing in MRI enables high subsampling factors while maintaining diagnostic image quality. This technique enables shortened scan durations and/or improved image resolution. Further, compressed sensing can increase the diagnostic information and value from each scan performed. Overall, compressed sensing has significant clinical impact in improving the diagnostic quality and patient experience for imaging exams. However, a number of challenges exist when moving compressed sensing from research to the clinic. These challenges include hand-crafted image priors, sensitive tuning parameters, and long reconstruction times. Data-driven learning provides a solution to address these challenges. As a result, compressed sensing can have greater clinical impact. In this tutorial, we will review the compressed sensing formulation and outline steps needed to transform this formulation to a deep learning framework. Supplementary open source code in python will be used to demonstrate this approach with open databases. Further, we will discuss considerations in applying data-driven compressed sensing in the clinical setting.



### Low-Rank Discriminative Least Squares Regression for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.07832v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07832v4)
- **Published**: 2019-03-19 04:48:39+00:00
- **Updated**: 2019-10-08 07:37:26+00:00
- **Authors**: Zhe Chen, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Latest least squares regression (LSR) methods mainly try to learn slack regression targets to replace strict zero-one labels. However, the difference of intra-class targets can also be highlighted when enlarging the distance between different classes, and roughly persuing relaxed targets may lead to the problem of overfitting. To solve above problems, we propose a low-rank discriminative least squares regression model (LRDLSR) for multi-class image classification. Specifically, LRDLSR class-wisely imposes low-rank constraint on the intra-class regression targets to encourage its compactness and similarity. Moreover, LRDLSR introduces an additional regularization term on the learned targets to avoid the problem of overfitting. These two improvements are helpful to learn a more discriminative projection for regression and thus achieving better classification performance. Experimental results over a range of image databases demonstrate the effectiveness of the proposed LRDLSR method.



### Fisher Discriminative Least Squares Regression for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.07833v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07833v3)
- **Published**: 2019-03-19 04:53:48+00:00
- **Updated**: 2020-08-04 07:13:30+00:00
- **Authors**: Zhe Chen, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: Discriminative least squares regression (DLSR) has been shown to achieve promising performance in multi-class image classification tasks. Its key idea is to force the regression labels of different classes to move in opposite directions by means of the proposed the joint use of the $\epsilon$-draggings technique, yielding discriminative regression model exhibiting wider margins, and the Fisher criterion. The $\epsilon$-draggings technique ignores an important problem: its non-negative relaxation matrix is dynamically updated in optimization, which means the dragging values can also cause the labels from the same class to be uncorrelated. In order to learn a more powerful discriminative projection, as well as regression labels, we propose a Fisher regularized DLSR (FDLSR) framework by constraining the relaxed labels using the Fisher criterion. On one hand, the Fisher criterion improves the intra-class compactness of the relaxed labels during relaxation learning. On the other hand, it is expected further to enhance the inter-class separability of $\epsilon$-draggings technique. FDLSR for the first time ever attempts to integrate the Fisher discriminant criterion and $\epsilon$-draggings technique into one unified model because they are absolutely complementary in learning discriminative projection. Extensive experiments on various datasets demonstrate that the proposed FDLSR method achieves performance that is superior to other state-of-the-art classification methods. The Matlab codes of this paper are available at https://github.com/chenzhe207/FDLSR.



### Non-negative representation based discriminative dictionary learning for face recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.07836v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07836v2)
- **Published**: 2019-03-19 05:08:21+00:00
- **Updated**: 2019-09-28 09:23:04+00:00
- **Authors**: Zhe Chen, Xiao-Jun Wu, Josef Kittler
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a non-negative representation based discriminative dictionary learning algorithm (NRDL) for multicategory face classification. In contrast to traditional dictionary learning methods, NRDL investigates the use of non-negative representation (NR), which contributes to learning discriminative dictionary atoms. In order to make the learned dictionary more suitable for classification, NRDL seamlessly incorporates nonnegative representation constraint, discriminative dictionary learning and linear classifier training into a unified model. Specifically, NRDL introduces a positive constraint on representation matrix to find distinct atoms from heterogeneous training samples, which results in sparse and discriminative representation. Moreover, a discriminative dictionary encouraging function is proposed to enhance the uniqueness of class-specific sub-dictionaries. Meanwhile, an inter-class incoherence constraint and a compact graph based regularization term are constructed to respectively improve the discriminability of learned classifier. Experimental results on several benchmark face data sets verify the advantages of our NRDL algorithm over the state-of-the-art dictionary learning methods.



### The Probabilistic Object Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/1903.07840v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07840v2)
- **Published**: 2019-03-19 05:18:52+00:00
- **Updated**: 2019-04-08 00:58:24+00:00
- **Authors**: John Skinner, David Hall, Haoyang Zhang, Feras Dayoub, Niko Sünderhauf
- **Comment**: 4 pages, workshop paper
- **Journal**: None
- **Summary**: We introduce a new challenge for computer and robotic vision, the first ACRV Robotic Vision Challenge, Probabilistic Object Detection. Probabilistic object detection is a new variation on traditional object detection tasks, requiring estimates of spatial and semantic uncertainty. We extend the traditional bounding box format of object detection to express spatial uncertainty using gaussian distributions for the box corners. The challenge introduces a new test dataset of video sequences, which are designed to more closely resemble the kind of data available to a robotic system. We evaluate probabilistic detections using a new probability-based detection quality (PDQ) measure. The goal in creating this challenge is to draw the computer and robotic vision communities together, toward applying object detection solutions for practical robotics applications.



### Class-incremental Learning via Deep Model Consolidation
- **Arxiv ID**: http://arxiv.org/abs/1903.07864v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07864v4)
- **Published**: 2019-03-19 07:20:38+00:00
- **Updated**: 2020-01-16 02:21:49+00:00
- **Authors**: Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry Heck, Heming Zhang, C. -C. Jay Kuo
- **Comment**: WACV 2020 camera-ready
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) often suffer from "catastrophic forgetting" during incremental learning (IL) --- an abrupt degradation of performance on the original set of classes when the training objective is adapted to a newly added set of classes. Existing IL approaches tend to produce a model that is biased towards either the old classes or new classes, unless with the help of exemplars of the old data. To address this issue, we propose a class-incremental learning paradigm called Deep Model Consolidation (DMC), which works well even when the original training data is not available. The idea is to first train a separate model only for the new classes, and then combine the two individual models trained on data of two distinct set of classes (old classes and new classes) via a novel double distillation training objective. The two existing models are consolidated by exploiting publicly available unlabeled auxiliary data. This overcomes the potential difficulties due to the unavailability of original training data. Compared to the state-of-the-art techniques, DMC demonstrates significantly better performance in image classification (CIFAR-100 and CUB-200) and object detection (PASCAL VOC 2007) in the single-headed IL setting.



### Cross Domain Knowledge Transfer for Unsupervised Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1903.07868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07868v1)
- **Published**: 2019-03-19 07:35:58+00:00
- **Updated**: 2019-03-19 07:35:58+00:00
- **Authors**: Jinjia Peng, Huibing Wang, Tongtong Zhao, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (reID) is to identify a target vehicle in different cameras with non-overlapping views. When deploy the well-trained model to a new dataset directly, there is a severe performance drop because of differences among datasets named domain bias. To address this problem, this paper proposes an domain adaptation framework which contains an image-to-image translation network named vehicle transfer generative adversarial network (VTGAN) and an attention-based feature learning network (ATTNet). VTGAN could make images from the source domain (well-labeled) have the style of target domain (unlabeled) and preserve identity information of source domain. To further improve the domain adaptation ability for various backgrounds, ATTNet is proposed to train generated images with the attention structure for vehicle reID. Comprehensive experimental results clearly demonstrate that our method achieves excellent performance on VehicleID dataset.



### Pose-Invariant Object Recognition for Event-Based Vision with Slow-ELM
- **Arxiv ID**: http://arxiv.org/abs/1903.07873v1
- **DOI**: 10.1007/978-3-319-44781-0_54
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07873v1)
- **Published**: 2019-03-19 08:02:42+00:00
- **Updated**: 2019-03-19 08:02:42+00:00
- **Authors**: Rohan Ghosh, Siyi Tang, Mahdi Rasouli, Nitish Thakor, Sunil Kukreja
- **Comment**: Appeared in 25th International Conference on Artificial Neural
  Networks (ICANN), Barcelona, Spain
- **Journal**: None
- **Summary**: Neuromorphic image sensors produce activity-driven spiking output at every pixel. These low-power consuming imagers which encode visual change information in the form of spikes help reduce computational overhead and realize complex real-time systems; object recognition and pose-estimation to name a few. However, there exists a lack of algorithms in event-based vision aimed towards capturing invariance to transformations. In this work, we propose a methodology for recognizing objects invariant to their pose with the Dynamic Vision Sensor (DVS). A novel slow-ELM architecture is proposed which combines the effectiveness of Extreme Learning Machines and Slow Feature Analysis. The system, tested on an Intel Core i5-4590 CPU, can perform 10,000 classifications per second and achieves 1% classification error for 8 objects with views accumulated over 90 degrees of 2D pose.



### Improving Video Compression With Deep Visual-Attention Models
- **Arxiv ID**: http://arxiv.org/abs/1903.07912v1
- **DOI**: 10.1145/3332340.3332358
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07912v1)
- **Published**: 2019-03-19 10:06:04+00:00
- **Updated**: 2019-03-19 10:06:04+00:00
- **Authors**: Vitaliy Lyudvichenko, Mikhail Erofeev, Alexander Ploshkin, Dmitriy Vatolin
- **Comment**: None
- **Journal**: Proceedings of the 2019 International Conference on Intelligent
  Medicine and Image Processing
- **Summary**: Recent advances in deep learning have markedly improved the quality of visual-attention modelling. In this work we apply these advances to video compression.   We propose a compression method that uses a saliency model to adaptively compress frame areas in accordance with their predicted saliency. We selected three state-of-the-art saliency models, adapted them for video compression and analyzed their results. The analysis includes objective evaluation of the models as well as objective and subjective evaluation of the compressed videos.   Our method, which is based on the x264 video codec, can produce videos with the same visual quality as regular x264, but it reduces the bitrate by 25% according to the objective evaluation and by 17% according to the subjective one. Also, both the subjective and objective evaluations demonstrate that saliency models can compete with gaze maps for a single observer.   Our method can extend to most video bitstream formats and can improve video compression quality without requiring a switch to a new video encoding standard.



### Geometry-constrained Car Recognition Using a 3D Perspective Network
- **Arxiv ID**: http://arxiv.org/abs/1903.07916v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07916v3)
- **Published**: 2019-03-19 10:17:47+00:00
- **Updated**: 2019-11-18 00:16:24+00:00
- **Authors**: Rui Zeng, Zongyuan Ge, Simon Denman, Sridha Sridharan, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel learning framework for vehicle recognition from a single RGB image. Unlike existing methods which only use attention mechanisms to locate 2D discriminative information, our work learns a novel 3D perspective feature representation of a vehicle, which is then fused with 2D appearance feature to predict the category. The framework is composed of a global network (GN), a 3D perspective network (3DPN), and a fusion network. The GN is used to locate the region of interest (RoI) and generate the 2D global feature. With the assistance of the RoI, the 3DPN estimates the 3D bounding box under the guidance of the proposed vanishing point loss, which provides a perspective geometry constraint. Then the proposed 3D representation is generated by eliminating the viewpoint variance of the 3D bounding box using perspective transformation. Finally, the 3D and 2D feature are fused to predict the category of the vehicle. We present qualitative and quantitative results on the vehicle classification and verification tasks in the BoxCars dataset. The results demonstrate that, by learning such a concise 3D representation, we can achieve superior performance to methods that only use 2D information while retain 3D meaningful information without the challenge of requiring a 3D CAD model.



### What the Constant Velocity Model Can Teach Us About Pedestrian Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.07933v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.07933v3)
- **Published**: 2019-03-19 10:56:44+00:00
- **Updated**: 2020-01-22 11:52:02+00:00
- **Authors**: Christoph Schöller, Vincent Aravantinos, Florian Lay, Alois Knoll
- **Comment**: Accepted for publication in the IEEE Robotics and Automation Letters
  (RA-L) and for presentation at the 2020 International Conference on Robotics
  and Automation (ICRA)
- **Journal**: None
- **Summary**: Pedestrian motion prediction is a fundamental task for autonomous robots and vehicles to operate safely. In recent years many complex approaches based on neural networks have been proposed to address this problem. In this work we show that - surprisingly - a simple Constant Velocity Model can outperform even state-of-the-art neural models. This indicates that either neural networks are not able to make use of the additional information they are provided with, or that this information is not as relevant as commonly believed. Therefore, we analyze how neural networks process their input and how it impacts their predictions. Our analysis reveals pitfalls in training neural networks for pedestrian motion prediction and clarifies false assumptions about the problem itself. In particular, neural networks implicitly learn environmental priors that negatively impact their generalization capability, the motion history of pedestrians is irrelevant and interactions are too complex to predict. Our work shows how neural networks for pedestrian motion prediction can be thoroughly evaluated and our results indicate which research directions for neural motion prediction are promising in future.



### A Matrix-in-matrix Neural Network for Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.07949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07949v1)
- **Published**: 2019-03-19 11:39:54+00:00
- **Updated**: 2019-03-19 11:39:54+00:00
- **Authors**: Hailong Ma, Xiangxiang Chu, Bo Zhang, Shaohua Wan, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning methods have achieved impressive results with higher peak signal-to-noise ratio in single image super-resolution (SISR) tasks by utilizing deeper layers. However, their application is quite limited since they require high computing power. In addition, most of the existing methods rarely take full advantage of the intermediate features which are helpful for restoration. To address these issues, we propose a moderate-size SISR net work named matrixed channel attention network (MCAN) by constructing a matrix ensemble of multi-connected channel attention blocks (MCAB). Several models of different sizes are released to meet various practical requirements. Conclusions can be drawn from our extensive benchmark experiments that the proposed models achieve better performance with much fewer multiply-adds and parameters. Our models will be made publicly available.



### Deep Eikonal Solvers
- **Arxiv ID**: http://arxiv.org/abs/1903.07973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07973v1)
- **Published**: 2019-03-19 13:03:51+00:00
- **Updated**: 2019-03-19 13:03:51+00:00
- **Authors**: Moshe Lichtenstein, Gautam Pai, Ron Kimmel
- **Comment**: Accepted for oral presentation at Seventh International Conference on
  Scale Space and Variational Methods in Computer Vision (SSVM) 2019
- **Journal**: None
- **Summary**: A deep learning approach to numerically approximate the solution to the Eikonal equation is introduced. The proposed method is built on the fast marching scheme which comprises of two components: a local numerical solver and an update scheme. We replace the formulaic local numerical solver with a trained neural network to provide highly accurate estimates of local distances for a variety of different geometries and sampling conditions. Our learning approach generalizes not only to flat Euclidean domains but also to curved surfaces enabled by the incorporation of certain invariant features in the neural network architecture. We show a considerable gain in performance, validated by smaller errors and higher orders of accuracy for the numerical solutions of the Eikonal equation computed on different surfaces The proposed approach leverages the approximation power of neural networks to enhance the performance of numerical algorithms, thereby, connecting the somewhat disparate themes of numerical geometry and learning.



### Efficient Smoothing of Dilated Convolutions for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.07992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07992v1)
- **Published**: 2019-03-19 13:32:27+00:00
- **Updated**: 2019-03-19 13:32:27+00:00
- **Authors**: Thomas Ziegler, Manuel Fritsche, Lorenz Kuhn, Konstantin Donhauser
- **Comment**: None
- **Journal**: None
- **Summary**: Dilated Convolutions have been shown to be highly useful for the task of image segmentation. By introducing gaps into convolutional filters, they enable the use of larger receptive fields without increasing the original kernel size. Even though this allows for the inexpensive capturing of features at different scales, the structure of the dilated convolutional filter leads to a loss of information. We hypothesise that inexpensive modifications to Dilated Convolutional Neural Networks, such as additional averaging layers, could overcome this limitation. In this project we test this hypothesis by evaluating the effect of these modifications for a state-of-the art image segmentation system and compare them to existing approaches with the same objective. Our experiments show that our proposed methods improve the performance of dilated convolutions for image segmentation. Crucially, our modifications achieve these results at a much lower computational cost than previous smoothing approaches.



### Identity-Free Facial Expression Recognition using conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1903.08051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08051v2)
- **Published**: 2019-03-19 15:31:53+00:00
- **Updated**: 2021-05-20 21:25:23+00:00
- **Authors**: Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, Shizhong Han, Yan Tong
- **Comment**: None
- **Journal**: None
- **Summary**: A novel Identity-Free conditional Generative Adversarial Network (IF-GAN) was proposed for Facial Expression Recognition (FER) to explicitly reduce high inter-subject variations caused by identity-related facial attributes, e.g., age, race, and gender. As part of an end-to-end system, a cGAN was designed to transform a given input facial expression image to an "average" identity face with the same expression as the input. Then, identity-free FER is possible since the generated images have the same synthetic "average" identity and differ only in their displayed expressions. Experiments on four facial expression datasets, one with spontaneous expressions, show that IF-GAN outperforms the baseline CNN and achieves state-of-the-art performance for FER.



### Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.08066v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08066v3)
- **Published**: 2019-03-19 15:50:24+00:00
- **Updated**: 2020-02-28 18:21:29+00:00
- **Authors**: Sambhav R. Jain, Albert Gural, Michael Wu, Chris H. Dick
- **Comment**: Link to Conference (Oral & Poster) Schedule -
  https://mlsys.org/Conferences/2020/ScheduleMultitrack?event=1431
- **Journal**: Proceedings of the 3rd Machine Learning and Systems (MLSys)
  Conference, Austin, TX, USA, 2020
- **Summary**: We propose a method of training quantization thresholds (TQT) for uniform symmetric quantizers using standard backpropagation and gradient descent. Contrary to prior work, we show that a careful analysis of the straight-through estimator for threshold gradients allows for a natural range-precision trade-off leading to better optima. Our quantizers are constrained to use power-of-2 scale-factors and per-tensor scaling of weights and activations to make it amenable for hardware implementations. We present analytical support for the general robustness of our methods and empirically validate them on various CNNs for ImageNet classification. We are able to achieve near-floating-point accuracy on traditionally difficult networks such as MobileNets with less than 5 epochs of quantized (8-bit) retraining. Finally, we present Graffitist, a framework that enables automatic quantization of TensorFlow graphs for TQT (available at https://github.com/Xilinx/graffitist ).



### Max-plus Operators Applied to Filter Selection and Model Pruning in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.08072v2
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, cs.LG, cs.NE, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1903.08072v2)
- **Published**: 2019-03-19 15:58:43+00:00
- **Updated**: 2019-04-08 12:51:57+00:00
- **Authors**: Yunxiang Zhang, Samy Blusseau, Santiago Velasco-Forero, Isabelle Bloch, Jesus Angulo
- **Comment**: None
- **Journal**: International Symposium on Mathematical Morphology, Jul 2019,
  Saarbr{\"u}cken, Germany
- **Summary**: Following recent advances in morphological neural networks, we propose to study in more depth how Max-plus operators can be exploited to define morphological units and how they behave when incorporated in layers of conventional neural networks. Besides showing that they can be easily implemented with modern machine learning frameworks , we confirm and extend the observation that a Max-plus layer can be used to select important filters and reduce redundancy in its previous layer, without incurring performance loss. Experimental results demonstrate that the filter selection strategy enabled by a Max-plus is highly efficient and robust, through which we successfully performed model pruning on different neural network architectures. We also point out that there is a close connection between Maxout networks and our pruned Max-plus networks by comparing their respective characteristics. The code for reproducing our experiments is available online.



### Corners for Layout: End-to-End Layout Recovery from 360 Images
- **Arxiv ID**: http://arxiv.org/abs/1903.08094v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08094v2)
- **Published**: 2019-03-19 16:32:06+00:00
- **Updated**: 2019-03-25 09:49:09+00:00
- **Authors**: Clara Fernandez-Labrador, Jose M. Facil, Alejandro Perez-Yus, Cédric Demonceaux, Javier Civera, Jose J. Guerrero
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of 3D layout recovery in indoor scenes has been a core research topic for over a decade. However, there are still several major challenges that remain unsolved. Among the most relevant ones, a major part of the state-of-the-art methods make implicit or explicit assumptions on the scenes -- e.g. box-shaped or Manhattan layouts. Also, current methods are computationally expensive and not suitable for real-time applications like robot navigation and AR/VR. In this work we present CFL (Corners for Layout), the first end-to-end model for 3D layout recovery on 360 images. Our experimental results show that we outperform the state of the art relaxing assumptions about the scene and at a lower cost. We also show that our model generalizes better to camera position variations than conventional approaches by using EquiConvs, a type of convolution applied directly on the sphere projection and hence invariant to the equirectangular distortions.   CFL Webpage: https://cfernandezlab.github.io/CFL/



### Preconditioned P-ULA for Joint Deconvolution-Segmentation of Ultrasound Images -- Extended Version
- **Arxiv ID**: http://arxiv.org/abs/1903.08111v4
- **DOI**: 10.1109/LSP.2019.2935610
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.08111v4)
- **Published**: 2019-03-19 16:59:32+00:00
- **Updated**: 2020-01-21 19:04:12+00:00
- **Authors**: Corbineau Marie-Caroline, Kouamé Denis, Chouzenoux Emilie, Tourneret Jean-Yves, Pesquet Jean-Christophe
- **Comment**: None
- **Journal**: In IEEE Signal Processing Letters, vol.26, no.10, pp.1456-1460
  (2019)
- **Summary**: Joint deconvolution and segmentation of ultrasound images is a challenging problem in medical imaging. By adopting a hierarchical Bayesian model, we propose an accelerated Markov chain Monte Carlo scheme where the tissue reflectivity function is sampled thanks to a recently introduced proximal unadjusted Langevin algorithm. This new approach is combined with a forward-backward step and a preconditioning strategy to accelerate the convergence, and with a method based on the majorization-minimization principle to solve the inner nonconvex minimization problems. As demonstrated in numerical experiments conducted on both simulated and in vivo ultrasound images, the proposed method provides high-quality restoration and segmentation results and is up to six times faster than an existing Hamiltonian Monte Carlo method.



### 3D human action analysis and recognition through GLAC descriptor on 2D motion and static posture images
- **Arxiv ID**: http://arxiv.org/abs/1904.00764v1
- **DOI**: 10.1007/s11042-019-7365-2
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00764v1)
- **Published**: 2019-03-19 17:52:16+00:00
- **Updated**: 2019-03-19 17:52:16+00:00
- **Authors**: Mohammad Farhad Bulbul, Saiful Islam, Hazrat Ali
- **Comment**: Multimed Tools Appl (2019)
- **Journal**: None
- **Summary**: In this paper, we present an approach for identification of actions within depth action videos. First, we process the video to get motion history images (MHIs) and static history images (SHIs) corresponding to an action video based on the use of 3D Motion Trail Model (3DMTM). We then characterize the action video by extracting the Gradient Local Auto-Correlations (GLAC) features from the SHIs and the MHIs. The two sets of features i.e., GLAC features from MHIs and GLAC features from SHIs are concatenated to obtain a representation vector for action. Finally, we perform the classification on all the action samples by using the l2-regularized Collaborative Representation Classifier (l2-CRC) to recognize different human actions in an effective way. We perform evaluation of the proposed method on three action datasets, MSR-Action3D, DHA and UTD-MHAD. Through experimental results, we observe that the proposed method performs superior to other approaches.



### Interactive segmentation of medical images through fully convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.08205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08205v1)
- **Published**: 2019-03-19 18:28:49+00:00
- **Updated**: 2019-03-19 18:28:49+00:00
- **Authors**: Tomas Sakinis, Fausto Milletari, Holger Roth, Panagiotis Korfiatis, Petro Kostandy, Kenneth Philbrick, Zeynettin Akkus, Ziyue Xu, Daguang Xu, Bradley J. Erickson
- **Comment**: None
- **Journal**: None
- **Summary**: Image segmentation plays an essential role in medicine for both diagnostic and interventional tasks. Segmentation approaches are either manual, semi-automated or fully-automated. Manual segmentation offers full control over the quality of the results, but is tedious, time consuming and prone to operator bias. Fully automated methods require no human effort, but often deliver sub-optimal results without providing users with the means to make corrections. Semi-automated approaches keep users in control of the results by providing means for interaction, but the main challenge is to offer a good trade-off between precision and required interaction. In this paper we present a deep learning (DL) based semi-automated segmentation approach that aims to be a "smart" interactive tool for region of interest delineation in medical images. We demonstrate its use for segmenting multiple organs on computed tomography (CT) of the abdomen. Our approach solves some of the most pressing clinical challenges: (i) it requires only one to a few user clicks to deliver excellent 2D segmentations in a fast and reliable fashion; (ii) it can generalize to previously unseen structures and "corner cases"; (iii) it delivers results that can be corrected quickly in a smart and intuitive way up to an arbitrary degree of precision chosen by the user and (iv) ensures high accuracy. We present our approach and compare it to other techniques and previous work to show the advantages brought by our method.



### Cross-task weakly supervised learning from instructional videos
- **Arxiv ID**: http://arxiv.org/abs/1903.08225v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08225v2)
- **Published**: 2019-03-19 19:30:29+00:00
- **Updated**: 2019-04-29 10:08:57+00:00
- **Authors**: Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, Josef Sivic
- **Comment**: 18 pages, 17 figures, to be published in proceedings of the CVPR,
  2019
- **Journal**: None
- **Summary**: In this paper we investigate learning visual models for the steps of ordinary tasks using weak supervision via instructional narrations and an ordered list of steps instead of strong supervision via temporal annotations. At the heart of our approach is the observation that weakly supervised learning may be easier if a model shares components while learning different steps: `pour egg' should be trained jointly with other tasks involving `pour' and `egg'. We formalize this in a component model for recognizing steps and a weakly supervised learning framework that can learn this model under temporal constraints from narration and the list of steps. Past data does not permit systematic studying of sharing and so we also gather a new dataset, CrossTask, aimed at assessing cross-task sharing. Our experiments demonstrate that sharing across tasks improves performance, especially when done at the component level and that our component model can parse previously unseen tasks by virtue of its compositionality.



### Characterization of the Handwriting Skills as a Biomarker for Parkinson Disease
- **Arxiv ID**: http://arxiv.org/abs/1903.08226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08226v1)
- **Published**: 2019-03-19 19:32:00+00:00
- **Updated**: 2019-03-19 19:32:00+00:00
- **Authors**: R. Castrillon, A. Acien, J. R. Orozco-Arroyave, A. Morales, J. F. Vargas, R. Vera-Rodrıguez, J. Fierrez, J. Ortega-Garcia, A. Villegas
- **Comment**: Accepted in 14th IEEE International Conference on Automatic Face &
  Gesture Recognition (FG 2019) - Human Health Monitoring Based on Computer
  Vision
- **Journal**: None
- **Summary**: In this paper we evaluate the suitability of handwriting patterns as potential biomarkers to model Parkinson disease (PD). Although the study of PD is attracting the interest of many researchers around the world, databases to evaluate handwriting patterns are scarce and knowledge about patterns associated to PD is limited and biased to the existing datasets. This paper introduces a database with a total of 935 handwriting tasks collected from 55 PD patients and 94 healthy controls (45 young and 49 old). Three feature sets are extracted from the signals: neuromotor, kinematic, and nonlinear dynamic. Different classifiers are used to discriminate between PD and healthy subjects: support vector machines, knearest neighbors, and a multilayer perceptron. The proposed features and classifiers enable to detect PD with accuracies between 81% and 97%. Additionally, new insights are presented on the utility of the studied features for monitoring and detecting PD.



### A smartphone application to detection and classification of coffee leaf miner and coffee leaf rust
- **Arxiv ID**: http://arxiv.org/abs/1904.00742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00742v1)
- **Published**: 2019-03-19 21:45:47+00:00
- **Updated**: 2019-03-19 21:45:47+00:00
- **Authors**: Giuliano L. Manso, Helder Knidel, Renato A. Krohling, Jose A. Ventura
- **Comment**: None
- **Journal**: None
- **Summary**: Generally, the identification and classification of plant diseases and/or pests are performed by an expert . One of the problems facing coffee farmers in Brazil is crop infestation, particularly by leaf rust Hemileia vastatrix and leaf miner Leucoptera coffeella. The progression of the diseases and or pests occurs spatially and temporarily. So, it is very important to automatically identify the degree of severity. The main goal of this article consists on the development of a method and its i implementation as an App that allow the detection of the foliar damages from images of coffee leaf that are captured using a smartphone, and identify whether it is rust or leaf miner, and in turn the calculation of its severity degree. The method consists of identifying a leaf from the image and separates it from the background with the use of a segmentation algorithm. In the segmentation process, various types of backgrounds for the image using the HSV and YCbCr color spaces are tested. In the segmentation of foliar damages, the Otsu algorithm and the iterative threshold algorithm, in the YCgCr color space, have been used and compared to k-means. Next, features of the segmented foliar damages are calculated. For the classification, artificial neural network trained with extreme learning machine have been used. The results obtained shows the feasibility and effectiveness of the approach to identify and classify foliar damages, and the automatic calculation of the severity. The results obtained are very promising according to experts.



