# Arxiv Papers in cs.CV on 2019-03-08
### Ranked List Loss for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.03238v8
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03238v8)
- **Published**: 2019-03-08 01:28:17+00:00
- **Updated**: 2021-03-19 10:10:48+00:00
- **Authors**: Xinshao Wang, Yang Hua, Elyor Kodirov, Neil M. Robertson
- **Comment**: Accepted to T-PAMI. Therefore, to read the offical version, please go
  to IEEE Xplore. Fine-grained image retrieval task. Our source code is
  available online: https://github.com/XinshaoAmosWang/Ranked-List-Loss-for-DML
- **Journal**: None
- **Summary**: The objective of deep metric learning (DML) is to learn embeddings that can capture semantic similarity and dissimilarity information among data points. Existing pairwise or tripletwise loss functions used in DML are known to suffer from slow convergence due to a large proportion of trivial pairs or triplets as the model improves. To improve this, ranking-motivated structured losses are proposed recently to incorporate multiple examples and exploit the structured information among them. They converge faster and achieve state-of-the-art performance. In this work, we unveil two limitations of existing ranking-motivated structured losses and propose a novel ranked list loss to solve both of them. First, given a query, only a fraction of data points is incorporated to build the similarity structure. Consequently, some useful examples are ignored and the structure is less informative. To address this, we propose to build a set-based similarity structure by exploiting all instances in the gallery. The learning setting can be interpreted as few-shot retrieval: given a mini-batch, every example is iteratively used as a query, and the rest ones compose the gallery to search, i.e., the support set in few-shot setting. The rest examples are split into a positive set and a negative set. For every mini-batch, the learning objective of ranked list loss is to make the query closer to the positive set than to the negative set by a margin. Second, previous methods aim to pull positive pairs as close as possible in the embedding space. As a result, the intraclass data distribution tends to be extremely compressed. In contrast, we propose to learn a hypersphere for each class in order to preserve useful similarity structure inside it, which functions as regularisation. Extensive experiments demonstrate the superiority of our proposal by comparing with the state-of-the-art methods.



### FastDepth: Fast Monocular Depth Estimation on Embedded Systems
- **Arxiv ID**: http://arxiv.org/abs/1903.03273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.03273v1)
- **Published**: 2019-03-08 04:07:01+00:00
- **Updated**: 2019-03-08 04:07:01+00:00
- **Authors**: Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, Vivienne Sze
- **Comment**: Accepted for presentation at ICRA 2019. 8 pages, 6 figures, 7 tables
- **Journal**: None
- **Summary**: Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.



### Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.03295v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03295v2)
- **Published**: 2019-03-08 05:47:11+00:00
- **Updated**: 2019-04-18 02:58:45+00:00
- **Authors**: Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, Svetha Venkatesh
- **Comment**: Accepted for publication in CVPR'19; Included link for source code
- **Journal**: None
- **Summary**: Appearance features have been widely used in video anomaly detection even though they contain complex entangled factors. We propose a new method to model the normal patterns of human movements in surveillance video for anomaly detection using dynamic skeleton features. We decompose the skeletal movements into two sub-components: global body movement and local body posture. We model the dynamics and interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network. We observed that the decoupled features collaboratively interact in our spatio-temporal model to accurately identify human-related irregular events from surveillance video sequences. Compared to traditional appearance-based models, our method achieves superior outlier detection performance. Our model also offers "open-box" examination and decision explanation made possible by the semantically understandable features and a network architecture supporting interpretability.



### You Only Recognize Once: Towards Fast Video Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1903.03299v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03299v3)
- **Published**: 2019-03-08 06:21:10+00:00
- **Updated**: 2021-10-25 09:35:38+00:00
- **Authors**: Zhanzhan Cheng, Jing Lu, Yi Niu, Shiliang Pu, Fei Wu, Shuigeng Zhou
- **Comment**: Accepted by ACM Multimedia 2019. Code is available at
  https://davar-lab.github.io/publication.html or
  https://github.com/hikopensource/DAVAR-Lab-OCR
- **Journal**: None
- **Summary**: Video text spotting is still an important research topic due to its various real-applications. Previous approaches usually fall into the four-staged pipeline: text detection in individual images, framewisely recognizing localized text regions, tracking text streams and generating final results with complicated post-processing skills, which might suffer from the huge computational cost as well as the interferences of low-quality text. In this paper, we propose a fast and robust video text spotting framework by only recognizing the localized text one-time instead of frame-wisely recognition. Specifically, we first obtain text regions in videos with a well-designed spatial-temporal detector. Then we concentrate on developing a novel text recommender for selecting the highest-quality text from text streams and only recognizing the selected ones. Here, the recommender assembles text tracking, quality scoring and recognition into an end-to-end trainable module, which not only avoids the interferences from low-quality text but also dramatically speeds up the video text spotting process. In addition, we collect a larger scale video text dataset (LSVTD) for promoting the video text spotting community, which contains 100 text videos from 22 different real-life scenarios. Extensive experiments on two public benchmarks show that our method greatly speeds up the recognition process averagely by 71 times compared with the frame-wise manner, and also achieves the remarkable state-of-the-art.



### Learning from Synthetic Data for Crowd Counting in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1903.03303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03303v1)
- **Published**: 2019-03-08 06:40:50+00:00
- **Updated**: 2019-03-08 06:40:50+00:00
- **Authors**: Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Recently, counting the number of people for crowd scenes is a hot topic because of its widespread applications (e.g. video surveillance, public security). It is a difficult task in the wild: changeable environment, large-range number of people cause the current methods can not work well. In addition, due to the scarce data, many methods suffer from over-fitting to a different extent. To remedy the above two problems, firstly, we develop a data collector and labeler, which can generate the synthetic crowd scenes and simultaneously annotate them without any manpower. Based on it, we build a large-scale, diverse synthetic dataset. Secondly, we propose two schemes that exploit the synthetic data to boost the performance of crowd counting in the wild: 1) pretrain a crowd counter on the synthetic data, then finetune it using the real data, which significantly prompts the model's performance on real data; 2) propose a crowd counting method via domain adaptation, which can free humans from heavy data annotations. Extensive experiments show that the first method achieves the state-of-the-art performance on four real datasets, and the second outperforms our baselines. The dataset and source code are available at https://gjy3035.github.io/GCC-CL/.



### A Mutual Bootstrapping Model for Automated Skin Lesion Segmentation and Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.03313v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03313v4)
- **Published**: 2019-03-08 07:57:40+00:00
- **Updated**: 2020-02-12 01:18:30+00:00
- **Authors**: Yutong Xie, Jianpeng Zhang, Yong Xia, Chunhua Shen
- **Comment**: Accepted at IEEE Transactions on Medical Imaging, Early Access
- **Journal**: None
- **Summary**: Automated skin lesion segmentation and classification are two most essential and related tasks in the computer-aided diagnosis of skin cancer. Despite their prevalence, deep learning models are usually designed for only one task, ignoring the potential benefits in jointly performing both tasks. In this paper, we propose the mutual bootstrapping deep convolutional neural networks (MB-DCNN) model for simultaneous skin lesion segmentation and classification. This model consists of a coarse segmentation network (coarse-SN), a mask-guided classification network (mask-CN), and an enhanced segmentation network (enhanced-SN). On one hand, the coarse-SN generates coarse lesion masks that provide a prior bootstrapping for mask-CN to help it locate and classify skin lesions accurately. On the other hand, the lesion localization maps produced by mask-CN are then fed into enhanced-SN, aiming to transfer the localization information learned by mask-CN to enhanced-SN for accurate lesion segmentation. In this way, both segmentation and classification networks mutually transfer knowledge between each other and facilitate each other in a bootstrapping way. Meanwhile, we also design a novel rank loss and jointly use it with the Dice loss in segmentation networks to address the issues caused by class imbalance and hard-easy pixel imbalance. We evaluate the proposed MB-DCNN model on the ISIC-2017 and PH2 datasets, and achieve a Jaccard index of 80.4% and 89.4% in skin lesion segmentation and an average AUC of 93.8% and 97.7% in skin lesion classification, which are superior to the performance of representative state-of-the-art skin lesion segmentation and classification methods. Our results suggest that it is possible to boost the performance of skin lesion segmentation and classification simultaneously via training a unified model to perform both tasks in a mutual bootstrapping way.



### 3DN: 3D Deformation Network
- **Arxiv ID**: http://arxiv.org/abs/1903.03322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03322v1)
- **Published**: 2019-03-08 08:35:48+00:00
- **Updated**: 2019-03-08 08:35:48+00:00
- **Authors**: Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann
- **Comment**: None
- **Journal**: None
- **Summary**: Applications in virtual and augmented reality create a demand for rapid creation and easy access to large sets of 3D models. An effective way to address this demand is to edit or deform existing 3D models based on a reference, e.g., a 2D image which is very easy to acquire. Given such a source 3D model and a target which can be a 2D image, 3D model, or a point cloud acquired as a depth scan, we introduce 3DN, an end-to-end network that deforms the source model to resemble the target. Our method infers per-vertex offset displacements while keeping the mesh connectivity of the source model fixed. We present a training strategy which uses a novel differentiable operation, mesh sampling operator, to generalize our method across source and target models with varying mesh densities. Mesh sampling operator can be seamlessly integrated into the network to handle meshes with different topologies. Qualitative and quantitative results show that our method generates higher quality results compared to the state-of-the art learning-based methods for 3D shape generation. Code is available at github.com/laughtervv/3DN.



### Knowledge-Embedded Routing Network for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.03326v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03326v1)
- **Published**: 2019-03-08 08:53:02+00:00
- **Updated**: 2019-03-08 08:53:02+00:00
- **Authors**: Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: To understand a scene in depth not only involves locating/recognizing individual objects, but also requires to infer the relationships and interactions among them. However, since the distribution of real-world relationships is seriously unbalanced, existing methods perform quite poorly for the less frequent relationships. In this work, we find that the statistical correlations between object pairs and their relationships can effectively regularize semantic space and make prediction less ambiguous, and thus well address the unbalanced distribution issue. To achieve this, we incorporate these statistical correlations into deep neural networks to facilitate scene graph generation by developing a Knowledge-Embedded Routing Network. More specifically, we show that the statistical correlations between objects appearing in images and their relationships, can be explicitly represented by a structured knowledge graph, and a routing mechanism is learned to propagate messages through the graph to explore their interactions. Extensive experiments on the large-scale Visual Genome dataset demonstrate the superiority of the proposed method over current state-of-the-art competitors.



### Complex Valued Gated Auto-encoder for Video Frame Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.03336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03336v1)
- **Published**: 2019-03-08 09:31:48+00:00
- **Updated**: 2019-03-08 09:31:48+00:00
- **Authors**: Niloofar Azizi, Nils Wandel, Sven Behnke
- **Comment**: To appear in: 27th European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning (ESANN), Bruges, Belgium,
  2019
- **Journal**: None
- **Summary**: In recent years, complex valued artificial neural networks have gained increasing interest as they allow neural networks to learn richer representations while potentially incorporating less parameters. Especially in the domain of computer graphics, many traditional operations rely heavily on computations in the complex domain, thus complex valued neural networks apply naturally. In this paper, we perform frame predictions in video sequences using a complex valued gated auto-encoder. First, our method is motivated showing how the Fourier transform can be seen as the basis for translational operations. Then, we present how a complex neural network can learn such transformations and compare its performance and parameter efficiency to a real-valued gated autoencoder. Furthermore, we show how extending both - the real and the complex valued - neural networks by using convolutional units can significantly improve prediction performance and parameter efficiency. The networks are assessed on a moving noise and a bouncing ball dataset.



### Learning to Estimate Pose and Shape of Hand-Held Objects from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1903.03340v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03340v3)
- **Published**: 2019-03-08 09:40:51+00:00
- **Updated**: 2019-11-11 13:37:32+00:00
- **Authors**: Mia Kokic, Danica Kragic, Jeannette Bohg
- **Comment**: None
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2019
- **Summary**: We develop a system for modeling hand-object interactions in 3D from RGB images that show a hand which is holding a novel object from a known category. We design a Convolutional Neural Network (CNN) for Hand-held Object Pose and Shape estimation called HOPS-Net and utilize prior work to estimate the hand pose and configuration. We leverage the insight that information about the hand facilitates object pose and shape estimation by incorporating the hand into both training and inference of the object pose and shape as well as the refinement of the estimated pose. The network is trained on a large synthetic dataset of objects in interaction with a human hand. To bridge the gap between real and synthetic images, we employ an image-to-image translation model (Augmented CycleGAN) that generates realistically textured objects given a synthetic rendering. This provides a scalable way of generating annotated data for training HOPS-Net. Our quantitative experiments show that even noisy hand parameters significantly help object pose and shape estimation. The qualitative experiments show results of pose and shape estimation of objects held by a hand "in the wild".



### ICDAR 2019 Historical Document Reading Challenge on Large Structured Chinese Family Records
- **Arxiv ID**: http://arxiv.org/abs/1903.03341v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03341v3)
- **Published**: 2019-03-08 09:48:32+00:00
- **Updated**: 2019-05-10 06:49:26+00:00
- **Authors**: Rajkumar Saini, Derek Dobson, Jon Morrey, Marcus Liwicki, Foteini Simistira Liwicki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Historical Document Reading Challenge on Large Chinese Structured Family Records, in short ICDAR2019 HDRC CHINESE. The objective of the proposed competition is to recognize and analyze the layout, and finally detect and recognize the textlines and characters of the large historical document collection containing more than 20 000 pages kindly provided by FamilySearch.



### Computer aided detection of tuberculosis on chest radiographs: An evaluation of the CAD4TB v6 system
- **Arxiv ID**: http://arxiv.org/abs/1903.03349v2
- **DOI**: 10.1038/s41598-020-62148-y
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.03349v2)
- **Published**: 2019-03-08 10:04:44+00:00
- **Updated**: 2020-04-02 12:55:37+00:00
- **Authors**: Keelin Murphy, Shifa Salman Habib, Syed Mohammad Asad Zaidi, Saira Khowaja, Aamir Khan, Jaime Melendez, Ernst T. Scholten, Farhan Amad, Steven Schalekamp, Maurits Verhagen, Rick H. H. M. Philipsen, Annet Meijers, Bram van Ginneken
- **Comment**: Published in Scientific Reports
- **Journal**: Scientific Reports 10, 5492 (2020)
- **Summary**: There is a growing interest in the automated analysis of chest X-Ray (CXR) as a sensitive and inexpensive means of screening susceptible populations for pulmonary tuberculosis. In this work we evaluate the latest version of CAD4TB, a commercial software platform designed for this purpose. Version 6 of CAD4TB was released in 2018 and is here tested on a fully independent dataset of 5565 CXR images with GeneXpert (Xpert) sputum test results available (854 Xpert positive subjects). A subset of 500 subjects (50% Xpert positive) was reviewed and annotated by 5 expert observers independently to obtain a radiological reference standard. The latest version of CAD4TB is found to outperform all previous versions in terms of area under receiver operating curve (ROC) with respect to both Xpert and radiological reference standards. Improvements with respect to Xpert are most apparent at high sensitivity levels with a specificity of 76% obtained at a fixed 90% sensitivity. When compared with the radiological reference standard, CAD4TB v6 also outperformed previous versions by a considerable margin and achieved 98% specificity at the 90% sensitivity setting. No substantial difference was found between the performance of CAD4TB v6 and any of the various expert observers against the Xpert reference standard. A cost and efficiency analysis on this dataset demonstrates that in a standard clinical situation, operating at 90% sensitivity, users of CAD4TB v6 can process 132 subjects per day at n average cost per screen of \$5.95 per subject, while users of version 3 process only 85 subjects per day at a cost of \$8.38 per subject. At all tested operating points version 6 is shown to be more efficient and cost effective than any other version.



### Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.03372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03372v1)
- **Published**: 2019-03-08 11:20:39+00:00
- **Updated**: 2019-03-08 11:20:39+00:00
- **Authors**: Anjan Dutta, Zeynep Akata
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2019
- **Journal**: None
- **Summary**: Zero-shot sketch-based image retrieval (SBIR) is an emerging task in computer vision, allowing to retrieve natural images relevant to sketch queries that might not been seen in the training phase. Existing works either require aligned sketch-image pairs or inefficient memory fusion layer for mapping the visual information to a semantic space. In this work, we propose a semantically aligned paired cycle-consistent generative (SEM-PCYC) model for zero-shot SBIR, where each branch maps the visual information to a common semantic space via an adversarial training. Each of these branches maintains a cycle consistency that only requires supervision at category levels, and avoids the need of highly-priced aligned sketch-image pairs. A classification criteria on the generators' outputs ensures the visual to semantic space mapping to be discriminating. Furthermore, we propose to combine textual and hierarchical side information via a feature selection auto-encoder that selects discriminating side information within a same end-to-end model. Our results demonstrate a significant boost in zero-shot SBIR performance over the state-of-the-art on the challenging Sketchy and TU-Berlin datasets.



### Unsupervised Medical Image Translation Using Cycle-MedGAN
- **Arxiv ID**: http://arxiv.org/abs/1903.03374v1
- **DOI**: 10.23919/EUSIPCO.2019.8902799
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03374v1)
- **Published**: 2019-03-08 11:27:34+00:00
- **Updated**: 2019-03-08 11:27:34+00:00
- **Authors**: Karim Armanious, Chenming Jiang, Sherif Abdulatif, Thomas Küstner, Sergios Gatidis, Bin Yang
- **Comment**: Submitted to EUSIPCO 2019, 5 pages
- **Journal**: None
- **Summary**: Image-to-image translation is a new field in computer vision with multiple potential applications in the medical domain. However, for supervised image translation frameworks, co-registered datasets, paired in a pixel-wise sense, are required. This is often difficult to acquire in realistic medical scenarios. On the other hand, unsupervised translation frameworks often result in blurred translated images with unrealistic details. In this work, we propose a new unsupervised translation framework which is titled Cycle-MedGAN. The proposed framework utilizes new non-adversarial cycle losses which direct the framework to minimize the textural and perceptual discrepancies in the translated images. Qualitative and quantitative comparisons against other unsupervised translation approaches demonstrate the performance of the proposed framework for PET-CT translation and MR motion correction.



### Joint Learning of Brain Lesion and Anatomy Segmentation from Heterogeneous Datasets
- **Arxiv ID**: http://arxiv.org/abs/1903.03445v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03445v2)
- **Published**: 2019-03-08 13:49:44+00:00
- **Updated**: 2019-04-15 15:23:14+00:00
- **Authors**: Nicolas Roulet, Diego Fernandez Slezak, Enzo Ferrante
- **Comment**: Accepted for publication at MIDL 2019. Open reviews available at:
  https://openreview.net/forum?id=Syest0rxlN
- **Journal**: None
- **Summary**: Brain lesion and anatomy segmentation in magnetic resonance images are fundamental tasks in neuroimaging research and clinical practice. Given enough training data, convolutional neuronal networks (CNN) proved to outperform all existent techniques in both tasks independently. However, to date, little work has been done regarding simultaneous learning of brain lesion and anatomy segmentation from disjoint datasets.   In this work we focus on training a single CNN model to predict brain tissue and lesion segmentations using heterogeneous datasets labeled independently, according to only one of these tasks (a common scenario when using publicly available datasets). We show that label contradiction issues can arise in this case, and propose a novel adaptive cross entropy (ACE) loss function that makes such training possible. We provide quantitative evaluation in two different scenarios, benchmarking the proposed method in comparison with a multi-network approach. Our experiments suggest that ACE loss enables training of single models when standard cross entropy and Dice loss functions tend to fail. Moreover, we show that it is possible to achieve competitive results when comparing with multiple networks trained for independent tasks.



### On Boosting Semantic Street Scene Segmentation with Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1903.03462v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03462v2)
- **Published**: 2019-03-08 14:16:11+00:00
- **Updated**: 2019-07-16 14:07:16+00:00
- **Authors**: Panagiotis Meletis, Gijs Dubbelman
- **Comment**: Oral presentation IEEE IV 2019
- **Journal**: None
- **Summary**: Training convolutional networks for semantic segmentation requires per-pixel ground truth labels, which are very time consuming and hence costly to obtain. Therefore, in this work, we research and develop a hierarchical deep network architecture and the corresponding loss for semantic segmentation that can be trained from weak supervision, such as bounding boxes or image level labels, as well as from strong per-pixel supervision. We demonstrate that the hierarchical structure and the simultaneous training on strong (per-pixel) and weak (bounding boxes) labels, even from separate datasets, constantly increases the performance against per-pixel only training. Moreover, we explore the more challenging case of adding weak image-level labels. We collect street scene images and weak labels from the immense Open Images dataset to generate the OpenScapes dataset, and we use this novel dataset to increase segmentation performance on two established per-pixel labeled datasets, Cityscapes and Vistas. We report performance gains up to +13.2% mIoU on crucial street scene classes, and inference speed of 20 fps on a Titan V GPU for Cityscapes at 512 x 1024 resolution. Our network and OpenScapes dataset are shared with the research community.



### Auto-Encoding Progressive Generative Adversarial Networks For 3D Multi Object Scenes
- **Arxiv ID**: http://arxiv.org/abs/1903.03477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03477v1)
- **Published**: 2019-03-08 14:53:16+00:00
- **Updated**: 2019-03-08 14:53:16+00:00
- **Authors**: Vedant Singh, Manan Oza, Himanshu Vaghela, Pratik Kanani
- **Comment**: None
- **Journal**: None
- **Summary**: 3D multi object generative models allow us to synthesize a large range of novel 3D multi object scenes and also identify objects, shapes, layouts and their positions. But multi object scenes are difficult to create because of the dataset being multimodal in nature. The conventional 3D generative adversarial models are not efficient in generating multi object scenes, they usually tend to generate either one object or generate fuzzy results of multiple objects. Auto-encoder models have much scope in feature extraction and representation learning using the unsupervised paradigm in probabilistic spaces. We try to make use of this property in our proposed model. In this paper we propose a novel architecture using 3DConvNets trained with the progressive training paradigm that has been able to generate realistic high resolution 3D scenes of rooms, bedrooms, offices etc. with various pieces of furniture and objects. We make use of the adversarial auto-encoder along with the WGAN-GP loss parameter in our discriminator loss function. Finally this new approach to multi object scene generation has also been able to generate more number of objects per scene.



### NEARBY Platform for Automatic Asteroids Detection and EURONEAR Surveys
- **Arxiv ID**: http://arxiv.org/abs/1903.03479v1
- **DOI**: None
- **Categories**: **astro-ph.IM**, cs.CV, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1903.03479v1)
- **Published**: 2019-03-08 14:56:01+00:00
- **Updated**: 2019-03-08 14:56:01+00:00
- **Authors**: Dorian Gorgan, Ovidiu Vaduvescu, Teodor Stefanut, Victor Bacu, Adrian Sabou, Denisa Copandean Balazs, Constantin Nandra, Costin Boldea, Afrodita Boldea, Marian Predatu, Viktoria Pinter, Adrian Stanica
- **Comment**: ESA NEO and Debris Detection Conference, ESA/ESOC, Darmstadt,
  Germany, 22-24 Jan 2019
- **Journal**: Published online by the ESA Space Safety Programme Office, 2019
- **Summary**: The survey of the nearby space and continuous monitoring of the Near Earth Objects (NEOs) and especially Near Earth Asteroids (NEAs) are essential for the future of our planet and should represent a priority for our solar system research and nearby space exploration. More computing power and sophisticated digital tracking algorithms are needed to cope with the larger astronomy imaging cameras dedicated for survey telescopes. The paper presents the NEARBY platform that aims to experiment new algorithms for automatic image reduction, detection and validation of moving objects in astronomical surveys, specifically NEAs. The NEARBY platform has been developed and experimented through a collaborative research work between the Technical University of Cluj-Napoca (UTCN) and the University of Craiova, Romania, using observing infrastructure of the Instituto de Astrofisica de Canarias (IAC) and Isaac Newton Group (ING), La Palma, Spain. The NEARBY platform has been developed and deployed on the UTCN's cloud infrastructure and the acquired images are processed remotely by the astronomers who transfer it from ING through the web interface of the NEARBY platform. The paper analyzes and highlights the main aspects of the NEARBY platform development, and the results and conclusions on the EURONEAR surveys.



### Stable Backward Diffusion Models that Minimise Convex Energies
- **Arxiv ID**: http://arxiv.org/abs/1903.03491v2
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.03491v2)
- **Published**: 2019-03-08 15:15:14+00:00
- **Updated**: 2020-06-17 16:13:58+00:00
- **Authors**: Leif Bergerhoff, Marcelo Cárdenas, Joachim Weickert, Martin Welk
- **Comment**: None
- **Journal**: None
- **Summary**: The inverse problem of backward diffusion is known to be ill-posed and highly unstable. Backward diffusion processes appear naturally in image enhancement and deblurring applications. It is therefore greatly desirable to establish a backward diffusion model which implements a smart stabilisation approach that can be used in combination with an easy to handle numerical scheme. So far, existing stabilisation strategies in literature require sophisticated numerics to solve the underlying initial value problem. We derive a class of space-discrete one-dimensional backward diffusion as gradient descent of energies where we gain stability by imposing range constraints. Interestingly, these energies are even convex. Furthermore, we establish a comprehensive theory for the time-continuous evolution and we show that stability carries over to a simple explicit time discretisation of our model. Finally, we confirm the stability and usefulness of our technique in experiments in which we enhance the contrast of digital greyscale and colour images.



### Research on the pixel-based and object-oriented methods of urban feature extraction with GF-2 remote-sensing images
- **Arxiv ID**: http://arxiv.org/abs/1903.03412v1
- **DOI**: None
- **Categories**: **cs.OH**, cs.CV, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1903.03412v1)
- **Published**: 2019-03-08 15:19:36+00:00
- **Updated**: 2019-03-08 15:19:36+00:00
- **Authors**: Dong-dong Zhang, Lei Zhang, Vladimir Zaborovsky, Feng Xie, Yan-wen Wu, Ting-ting Lu
- **Comment**: None
- **Journal**: None
- **Summary**: During the rapid urbanization construction of China, acquisition of urban geographic information and timely data updating are important and fundamental tasks for the refined management of cities. With the development of domestic remote sensing technology, the application of Gaofen-2 (GF-2) high-resolution remote sensing images can greatly improve the accuracy of information extraction. This paper introduces an approach using object-oriented classification methods for urban feature extraction based on GF-2 satellite data. A combination of spectral, spatial attributes and membership functions was employed for mapping the urban features of Qinhuai District, Nanjing. The data preprocessing is carried out by ENVI software, and the subsequent data is exported into the eCognition software for object-oriented classification and extraction of urban feature information. Finally, the obtained raster image classification results are vectorized using the ARCGIS software, and the vector graphics are stored in the library, which can be used for further analysis and modeling. Accuracy assessment was performed using ground truth data acquired by visual interpretation and from other reliable secondary data sources. Compared with the result of pixel-based supervised (neural net) classification, the developed object-oriented method can significantly improve extraction accuracy, and after manual interpretation, an overall accuracy of 95.44% can be achieved, with a Kappa coefficient of 0.9405, which objectively confirmed the superiority of the object-oriented method and the feasibility of the utilization of GF-2 satellite data.



### A Three-Player GAN: Generating Hard Samples To Improve Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.03496v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03496v1)
- **Published**: 2019-03-08 15:24:37+00:00
- **Updated**: 2019-03-08 15:24:37+00:00
- **Authors**: Simon Vandenhende, Bert De Brabandere, Davy Neven, Luc Van Gool
- **Comment**: Accepted for oral presentation at MVA2019
- **Journal**: None
- **Summary**: We propose a Three-Player Generative Adversarial Network to improve classification networks. In addition to the game played between the discriminator and generator, a competition is introduced between the generator and the classifier. The generator's objective is to synthesize samples that are both realistic and hard to label for the classifier. Even though we make no assumptions on the type of augmentations to learn, we find that the model is able to synthesize realistically looking examples that are hard for the classification model. Furthermore, the classifier becomes more robust when trained on these difficult samples. The method is evaluated on a public dataset for traffic sign recognition.



### Unsupervised Data Imputation via Variational Inference of Deep Subspaces
- **Arxiv ID**: http://arxiv.org/abs/1903.03503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03503v1)
- **Published**: 2019-03-08 15:28:48+00:00
- **Updated**: 2019-03-08 15:28:48+00:00
- **Authors**: Adrian V. Dalca, John Guttag, Mert R. Sabuncu
- **Comment**: None
- **Journal**: None
- **Summary**: A wide range of systems exhibit high dimensional incomplete data. Accurate estimation of the missing data is often desired, and is crucial for many downstream analyses. Many state-of-the-art recovery methods involve supervised learning using datasets containing full observations. In contrast, we focus on unsupervised estimation of missing image data, where no full observations are available - a common situation in practice. Unsupervised imputation methods for images often employ a simple linear subspace to capture correlations between data dimensions, omitting more complex relationships. In this work, we introduce a general probabilistic model that describes sparse high dimensional imaging data as being generated by a deep non-linear embedding. We derive a learning algorithm using a variational approximation based on convolutional neural networks and discuss its relationship to linear imputation models, the variational auto encoder, and deep image priors. We introduce sparsity-aware network building blocks that explicitly model observed and missing data. We analyze proposed sparsity-aware network building blocks, evaluate our method on public domain imaging datasets, and conclude by showing that our method enables imputation in an important real-world problem involving medical images. The code is freely available as part of the \verb|neuron| library at http://github.com/adalca/neuron.



### OpenCL-based FPGA accelerator for disparity map generation with stereoscopic event cameras
- **Arxiv ID**: http://arxiv.org/abs/1903.03509v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03509v1)
- **Published**: 2019-03-08 15:39:27+00:00
- **Updated**: 2019-03-08 15:39:27+00:00
- **Authors**: David Castells-Rufas, Jordi Carrabina
- **Comment**: Presented at HIP3ES, 2019
- **Journal**: None
- **Summary**: Although event-based cameras are already commercially available. Vision algorithms based on them are still not common. As a consequence, there are few Hardware Accelerators for them. In this work we present some experiments to create FPGA accelerators for a well-known vision algorithm using event-based cameras. We present a stereo matching algorithm to create a stream of disparity events disparity map and implement several accelerators using the Intel FPGA OpenCL tool-chain. The results show that multiple designs can be easily tested and that a performance speedup of more than 8x can be achieved with simple code transformations.



### DSM Building Shape Refinement from Combined Remote Sensing Images based on Wnet-cGANs
- **Arxiv ID**: http://arxiv.org/abs/1903.03519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03519v1)
- **Published**: 2019-03-08 15:47:03+00:00
- **Updated**: 2019-03-08 15:47:03+00:00
- **Authors**: Ksenia Bittner, Marco Körner, Peter Reinartz
- **Comment**: None
- **Journal**: None
- **Summary**: We describe the workflow of a digital surface models (DSMs) refinement algorithm using a hybrid conditional generative adversarial network (cGAN) where the generative part consists of two parallel networks merged at the last stage forming a WNet architecture. The inputs to the so-called WNet-cGAN are stereo DSMs and panchromatic (PAN) half-meter resolution satellite images. Fusing these helps to propagate fine detailed information from a spectral image and complete the missing 3D knowledge from a stereo DSM about building shapes. Besides, it refines the building outlines and edges making them more rectangular and sharp.



### Fast Deep Stereo with 2D Convolutional Processing of Cost Signatures
- **Arxiv ID**: http://arxiv.org/abs/1903.04939v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.04939v1)
- **Published**: 2019-03-08 16:39:28+00:00
- **Updated**: 2019-03-08 16:39:28+00:00
- **Authors**: Kyle Yee, Ayan Chakrabarti
- **Comment**: Project site at https://projects.ayanc.org/fdscs/
- **Journal**: None
- **Summary**: Modern neural network-based algorithms are able to produce highly accurate depth estimates from stereo image pairs, nearly matching the reliability of measurements from more expensive depth sensors. However, this accuracy comes with a higher computational cost since these methods use network architectures designed to compute and process matching scores across all candidate matches at all locations, with floating point computations repeated across a match volume with dimensions corresponding to both space and disparity. This leads to longer running times to process each image pair, making them impractical for real-time use in robots and autonomous vehicles. We propose a new stereo algorithm that employs a significantly more efficient network architecture. Our method builds an initial match cost volume using traditional matching costs that are fast to compute, and trains a network to estimate disparity from this volume. Crucially, our network only employs per-pixel and two-dimensional convolution operations: to summarize the match information at each location as a low-dimensional feature vector, and to spatially process these `cost-signature' features to produce a dense disparity map. Experimental results on the KITTI benchmark show that our method delivers competitive accuracy at significantly higher speeds---running at 48 frames per second on a modern GPU.



### Unsupervised Learning of Probabilistic Diffeomorphic Registration for Images and Surfaces
- **Arxiv ID**: http://arxiv.org/abs/1903.03545v2
- **DOI**: 10.1016/j.media.2019.07.006
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1903.03545v2)
- **Published**: 2019-03-08 16:48:41+00:00
- **Updated**: 2019-07-23 18:06:56+00:00
- **Authors**: Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu
- **Comment**: MedIA: Medical Image Analysis (MICCAI2018 Special Issue). Expands on
  MICCAI 2018 paper (arXiv:1805.04605) by introducing an extension to
  anatomical surface registration, new experiments, and analysis of
  diffeomorphic implementations. Keywords: medical image registration;
  diffeomorphic; invertible; probabilistic modeling; variational inference.
  Code available at http://voxelmorph.csail.mit.edu. arXiv admin note: text
  overlap with arXiv:1805.04605
- **Journal**: None
- **Summary**: Classical deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates.   In this paper, we build a connection between classical and learning-based methods. We present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that uses insights from classical registration methods and makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task for both images and anatomical surfaces, and provide extensive empirical analyses. Our principled approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees. Our implementation is available at http://voxelmorph.csail.mit.edu.



### Prediction and Sampling with Local Graph Transforms for Quasi-Lossless Light Field Compression
- **Arxiv ID**: http://arxiv.org/abs/1903.03546v1
- **DOI**: 10.1109/TIP.2019.2959215
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03546v1)
- **Published**: 2019-03-08 16:49:49+00:00
- **Updated**: 2019-03-08 16:49:49+00:00
- **Authors**: Mira Rizkallah, Thomas Maugey, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: Graph-based transforms have been shown to be powerful tools in terms of image energy compaction. However, when the support increases to best capture signal dependencies, the computation of the basis functions becomes rapidly untractable. This problem is in particular compelling for high dimensional imaging data such as light fields. The use of local transforms with limited supports is a way to cope with this computational difficulty. Unfortunately, the locality of the support may not allow us to fully exploit long term signal dependencies present in both the spatial and angular dimensions in the case of light fields. This paper describes sampling and prediction schemes with local graph-based transforms enabling to efficiently compact the signal energy and exploit dependencies beyond the local graph support. The proposed approach is investigated and is shown to be very efficient in the context of spatio-angular transforms for quasi-lossless compression of light fields.



### Geometry-Aware Graph Transforms for Light Field Compact Representation
- **Arxiv ID**: http://arxiv.org/abs/1903.03556v1
- **DOI**: 10.1109/TIP.2019.2928873
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03556v1)
- **Published**: 2019-03-08 16:56:11+00:00
- **Updated**: 2019-03-08 16:56:11+00:00
- **Authors**: Mira Rizkallah, Xin Su, Thomas Maugey, Christine Guillemot
- **Comment**: None
- **Journal**: None
- **Summary**: The paper addresses the problem of energy compaction of dense 4D light fields by designing geometry-aware local graph-based transforms. Local graphs are constructed on super-rays that can be seen as a grouping of spatially and geometry-dependent angularly correlated pixels. Both non separable and separable transforms are considered. Despite the local support of limited size defined by the super-rays, the Laplacian matrix of the non separable graph remains of high dimension and its diagonalization to compute the transform eigen vectors remains computationally expensive. To solve this problem, we then perform the local spatio-angular transform in a separable manner. We show that when the shape of corresponding super-pixels in the different views is not isometric, the basis functions of the spatial transforms are not coherent, resulting in decreased correlation between spatial transform coefficients. We hence propose a novel transform optimization method that aims at preserving angular correlation even when the shapes of the super-pixels are not isometric. Experimental results show the benefit of the approach in terms of energy compaction. A coding scheme is also described to assess the rate-distortion perfomances of the proposed transforms and is compared to state of the art encoders namely HEVC and JPEG Pleno VM 1.1.



### Mix and match networks: cross-modal alignment for zero-pair image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/1903.04294v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04294v2)
- **Published**: 2019-03-08 17:27:29+00:00
- **Updated**: 2020-05-15 13:20:23+00:00
- **Authors**: Yaxing Wang, Luis Herranz, Joost van de Weijer
- **Comment**: Accepted by IJCV
- **Journal**: None
- **Summary**: This paper addresses the problem of inferring unseen cross-modal image-to-image translations between multiple modalities. We assume that only some of the pairwise translations have been seen (i.e. trained) and infer the remaining unseen translations (where training pairs are not available). We propose mix and match networks, an approach where multiple encoders and decoders are aligned in such a way that the desired translation can be obtained by simply cascading the source encoder and the target decoder, even when they have not interacted during the training stage (i.e. unseen). The main challenge lies in the alignment of the latent representations at the bottlenecks of encoder-decoder pairs. We propose an architecture with several tools to encourage alignment, including autoencoders and robust side information and latent consistency losses. We show the benefits of our approach in terms of effectiveness and scalability compared with other pairwise image-to-image translation approaches. We also propose zero-pair cross-modal image translation, a challenging setting where the objective is inferring semantic segmentation from depth (and vice-versa) without explicit segmentation-depth pairs, and only from two (disjoint) segmentation-RGB and depth-RGB training sets. We observe that a certain part of the shared information between unseen modalities might not be reachable, so we further propose a variant that leverages pseudo-pairs which allows us to exploit this shared information between the unseen modalities.



### A Grid-based Method for Removing Overlaps of Dimensionality Reduction Scatterplot Layouts
- **Arxiv ID**: http://arxiv.org/abs/1903.06262v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06262v7)
- **Published**: 2019-03-08 18:20:36+00:00
- **Updated**: 2023-05-09 01:58:21+00:00
- **Authors**: Gladys M. Hilasaca, Wilson E. Marcílio-Jr, Danilo M. Eler, Rafael M. Martins, Fernando V. Paulovich
- **Comment**: 12 pages and 10 figures, Evaluation and Comparison Section update
- **Journal**: None
- **Summary**: Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous visualization tool for analyzing multidimensional datasets. Despite their popularity, such scatterplots suffer from occlusion, especially when informative glyphs are used to represent data instances, potentially obfuscating critical information for the analysis under execution. Different strategies have been devised to address this issue, either producing overlap-free layouts which lack the powerful capabilities of contemporary DR techniques in uncovering interesting data patterns or eliminating overlaps as a post-processing strategy. Despite the good results of post-processing techniques, most of the best methods typically expand or distort the scatterplot area, thus reducing glyphs' size (sometimes) to unreadable dimensions, defeating the purpose of removing overlaps. This paper presents Distance Grid (DGrid), a novel post-processing strategy to remove overlaps from DR layouts that faithfully preserves the original layout's characteristics and bounds the minimum glyph sizes. We show that DGrid surpasses the state-of-the-art in overlap removal (through an extensive comparative evaluation considering multiple different metrics) while also being one of the fastest techniques, especially for large datasets. A user study with 51 participants also shows that DGrid is consistently ranked among the top techniques for preserving the original scatterplots' visual characteristics and the aesthetics of the final results.



### RoPAD: Robust Presentation Attack Detection through Unsupervised Adversarial Invariance
- **Arxiv ID**: http://arxiv.org/abs/1903.03691v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03691v2)
- **Published**: 2019-03-08 22:43:01+00:00
- **Updated**: 2019-03-20 22:48:49+00:00
- **Authors**: Ayush Jaiswal, Shuai Xia, Iacopo Masi, Wael AbdAlmageed
- **Comment**: To appear in Proceedings of International Conference on Biometrics
  (ICB), 2019
- **Journal**: None
- **Summary**: For enterprise, personal and societal applications, there is now an increasing demand for automated authentication of identity from images using computer vision. However, current authentication technologies are still vulnerable to presentation attacks. We present RoPAD, an end-to-end deep learning model for presentation attack detection that employs unsupervised adversarial invariance to ignore visual distractors in images for increased robustness and reduced overfitting. Experiments show that the proposed framework exhibits state-of-the-art performance on presentation attack detection on several benchmark datasets.



### Image Privacy Prediction Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.03695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1903.03695v1)
- **Published**: 2019-03-08 23:12:12+00:00
- **Updated**: 2019-03-08 23:12:12+00:00
- **Authors**: Ashwini Tonge, Cornelia Caragea
- **Comment**: None
- **Journal**: None
- **Summary**: Images today are increasingly shared online on social networking sites such as Facebook, Flickr, Foursquare, and Instagram. Despite that current social networking sites allow users to change their privacy preferences, this is often a cumbersome task for the vast majority of users on the Web, who face difficulties in assigning and managing privacy settings. Thus, automatically predicting images' privacy to warn users about private or sensitive content before uploading these images on social networking sites has become a necessity in our current interconnected world.   In this paper, we explore learning models to automatically predict appropriate images' privacy as private or public using carefully identified image-specific features. We study deep visual semantic features that are derived from various layers of Convolutional Neural Networks (CNNs) as well as textual features such as user tags and deep tags generated from deep CNNs. Particularly, we extract deep (visual and tag) features from four pre-trained CNN architectures for object recognition, i.e., AlexNet, GoogLeNet, VGG-16, and ResNet, and compare their performance for image privacy prediction. Results of our experiments on a Flickr dataset of over thirty thousand images show that the learning models trained on features extracted from ResNet outperform the state-of-the-art models for image privacy prediction. We further investigate the combination of user tags and deep tags derived from CNN architectures using two settings: (1) SVM on the bag-of-tags features; and (2) text-based CNN. Our results show that even though the models trained on the visual features perform better than those trained on the tag features, the combination of deep visual features with image tags shows improvements in performance over the individual feature sets.



