# Arxiv Papers in cs.CV on 2019-03-25
### End-to-End Learning Using Cycle Consistency for Image-to-Caption Transformations
- **Arxiv ID**: http://arxiv.org/abs/1903.10118v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.10118v1)
- **Published**: 2019-03-25 03:40:15+00:00
- **Updated**: 2019-03-25 03:40:15+00:00
- **Authors**: Keisuke Hagiwara, Yusuke Mukuta, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: So far, research to generate captions from images has been carried out from the viewpoint that a caption holds sufficient information for an image. If it is possible to generate an image that is close to the input image from a generated caption, i.e., if it is possible to generate a natural language caption containing sufficient information to reproduce the image, then the caption is considered to be faithful to the image. To make such regeneration possible, learning using the cycle-consistency loss is effective. In this study, we propose a method of generating captions by learning end-to-end mutual transformations between images and texts. To evaluate our method, we perform comparative experiments with and without the cycle consistency. The results are evaluated by an automatic evaluation and crowdsourcing, demonstrating that our proposed method is effective.



### Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.10122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10122v1)
- **Published**: 2019-03-25 03:54:11+00:00
- **Updated**: 2019-03-25 03:54:11+00:00
- **Authors**: Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing
- **Comment**: None
- **Journal**: None
- **Summary**: Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewrites the templates according to specific cases. The core of KERP is a proposed generic implementation unit---Graph Transformer (GTR) that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images and sequences. Experiments show that the proposed approach generates structured and robust reports supported with accurate abnormality description and explainable attentive regions, achieving the state-of-the-art results on two medical report benchmarks, with the best medical abnormality and disease classification accuracy and improved human evaluation performance.



### Recurrent Back-Projection Network for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.10128v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10128v1)
- **Published**: 2019-03-25 04:13:36+00:00
- **Updated**: 2019-03-25 04:13:36+00:00
- **Authors**: Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita
- **Comment**: To appear in CVPR2019
- **Journal**: None
- **Summary**: We proposed a novel architecture for the problem of video super-resolution. We integrate spatial and temporal contexts from continuous video frames using a recurrent encoder-decoder module, that fuses multi-frame information with the more traditional, single frame super-resolution path for the target frame. In contrast to most prior work where frames are pooled together by stacking or warping, our model, the Recurrent Back-Projection Network (RBPN) treats each context frame as a separate source of information. These sources are combined in an iterative refinement framework inspired by the idea of back-projection in multiple-image super-resolution. This is aided by explicitly representing estimated inter-frame motion with respect to the target, rather than explicitly aligning frames. We propose a new video super-resolution benchmark, allowing evaluation at a larger scale and considering videos in different motion regimes. Experimental results demonstrate that our RBPN is superior to existing methods on several datasets.



### f-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.10132v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10132v1)
- **Published**: 2019-03-25 04:35:03+00:00
- **Updated**: 2019-03-25 04:35:03+00:00
- **Authors**: Yongqin Xian, Saurabh Sharma, Bernt Schiele, Zeynep Akata
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: When labeled training data is scarce, a promising data augmentation approach is to generate visual features of unknown classes using their attributes. To learn the class conditional distribution of CNN features, these models rely on pairs of image features and class attributes. Hence, they can not make use of the abundance of unlabeled data samples. In this paper, we tackle any-shot learning problems i.e. zero-shot and few-shot, in a unified feature generating framework that operates in both inductive and transductive learning settings. We develop a conditional generative model that combines the strength of VAE and GANs and in addition, via an unconditional discriminator, learns the marginal feature distribution of unlabeled images. We empirically show that our model learns highly discriminative CNN features for five datasets, i.e. CUB, SUN, AWA and ImageNet, and establish a new state-of-the-art in any-shot learning, i.e. inductive and transductive (generalized) zero- and few-shot learning settings. We also demonstrate that our learned features are interpretable: we visualize them by inverting them back to the pixel space and we explain them by generating textual arguments of why they are associated with a certain label.



### Training Data Independent Image Registration With GANs Using Transfer Learning And Segmentation Information
- **Arxiv ID**: http://arxiv.org/abs/1903.10139v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10139v2)
- **Published**: 2019-03-25 05:33:04+00:00
- **Updated**: 2019-04-11 07:27:28+00:00
- **Authors**: Dwarikanath Mahapatra, Zongyuan Ge
- **Comment**: None
- **Journal**: None
- **Summary**: Registration is an important task in automated medical image analysis. Although deep learning (DL) based image registration methods out perform time consuming conventional approaches, they are heavily dependent on training data and do not generalize well for new images types. We present a DL based approach that can register an image pair which is different from the training images. This is achieved by training generative adversarial networks (GANs) in combination with segmentation information and transfer learning. Experiments on chest Xray and brain MR images show that our method gives better registration performance over conventional methods.



### Iris R-CNN: Accurate Iris Segmentation in Non-cooperative Environment
- **Arxiv ID**: http://arxiv.org/abs/1903.10140v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10140v1)
- **Published**: 2019-03-25 05:33:06+00:00
- **Updated**: 2019-03-25 05:33:06+00:00
- **Authors**: Chunyang Feng, Yufeng Sun, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the significant advances in iris segmentation, accomplishing accurate iris segmentation in non-cooperative environment remains a grand challenge. In this paper, we present a deep learning framework, referred to as Iris R-CNN, to offer superior accuracy for iris segmentation. The proposed framework is derived from Mask R-CNN, and several novel techniques are proposed to carefully explore the unique characteristics of iris. First, we propose two novel networks: (i) Double-Circle Region Proposal Network (DC-RPN), and (ii) Double-Circle Classification and Regression Network (DC-CRN) to take into account the iris and pupil circles to maximize the accuracy for iris segmentation. Second, we propose a novel normalization scheme for Regions of Interest (RoIs) to facilitate a radically new pooling operation over a double-circle region. Experimental results on two challenging iris databases, UBIRIS.v2 and MICHE, demonstrate the superior accuracy of the proposed approach over other state-of-the-art methods.



### Unconstrained Facial Action Unit Detection via Latent Feature Domain
- **Arxiv ID**: http://arxiv.org/abs/1903.10143v4
- **DOI**: 10.1109/TAFFC.2021.3091331
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10143v4)
- **Published**: 2019-03-25 06:16:32+00:00
- **Updated**: 2021-06-20 13:46:30+00:00
- **Authors**: Zhiwen Shao, Jianfei Cai, Tat-Jen Cham, Xuequan Lu, Lizhuang Ma
- **Comment**: This paper has been accepted by IEEE Transactions on Affective
  Computing
- **Journal**: None
- **Summary**: Facial action unit (AU) detection in the wild is a challenging problem, due to the unconstrained variability in facial appearances and the lack of accurate annotations. Most existing methods depend on either impractical labor-intensive labeling or inaccurate pseudo labels. In this paper, we propose an end-to-end unconstrained facial AU detection framework based on domain adaptation, which transfers accurate AU labels from a constrained source domain to an unconstrained target domain by exploiting labels of AU-related facial landmarks. Specifically, we map a source image with label and a target image without label into a latent feature domain by combining source landmark-related feature with target landmark-free feature. Due to the combination of source AU-related information and target AU-free information, the latent feature domain with transferred source label can be learned by maximizing the target-domain AU detection performance. Moreover, we introduce a novel landmark adversarial loss to disentangle the landmark-free feature from the landmark-related feature by treating the adversarial learning as a multi-player minimax game. Our framework can also be naturally extended for use with target-domain pseudo AU labels. Extensive experiments show that our method soundly outperforms lower-bounds and upper-bounds of the basic model, as well as state-of-the-art approaches on the challenging in-the-wild benchmarks. The code is available at https://github.com/ZhiwenShao/ADLD.



### Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing
- **Arxiv ID**: http://arxiv.org/abs/1903.10145v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.10145v3)
- **Published**: 2019-03-25 06:28:24+00:00
- **Updated**: 2019-06-10 21:43:02+00:00
- **Authors**: Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, Lawrence Carin
- **Comment**: Published in NAACL 2019; The first two authors contribute equally;
  Code: https://github.com/haofuml/cyclical_annealing
- **Journal**: None
- **Summary**: Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter \beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for \beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing \beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.



### PI-REC: Progressive Image Reconstruction Network With Edge and Color Domain
- **Arxiv ID**: http://arxiv.org/abs/1903.10146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10146v1)
- **Published**: 2019-03-25 06:29:36+00:00
- **Updated**: 2019-03-25 06:29:36+00:00
- **Authors**: Sheng You, Ning You, Minxue Pan
- **Comment**: 15 pages, 13 figures
- **Journal**: None
- **Summary**: We propose a universal image reconstruction method to represent detailed images purely from binary sparse edge and flat color domain. Inspired by the procedures of painting, our framework, based on generative adversarial network, consists of three phases: Imitation Phase aims at initializing networks, followed by Generating Phase to reconstruct preliminary images. Moreover, Refinement Phase is utilized to fine-tune preliminary images into final outputs with details. This framework allows our model generating abundant high frequency details from sparse input information. We also explore the defects of disentangling style latent space implicitly from images, and demonstrate that explicit color domain in our model performs better on controllability and interpretability. In our experiments, we achieve outstanding results on reconstructing realistic images and translating hand drawn drafts into satisfactory paintings. Besides, within the domain of edge-to-image translation, our model PI-REC outperforms existing state-of-the-art methods on evaluations of realism and accuracy, both quantitatively and qualitatively.



### Enhanced Transfer Learning with ImageNet Trained Classification Layer
- **Arxiv ID**: http://arxiv.org/abs/1903.10150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10150v2)
- **Published**: 2019-03-25 06:43:26+00:00
- **Updated**: 2019-09-19 13:33:05+00:00
- **Authors**: Tasfia Shermin, Shyh Wei Teng, Manzur Murshed, Guojun Lu, Ferdous Sohel, Manoranjan Paul
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Parameter fine tuning is a transfer learning approach whereby learned parameters from pre-trained source network are transferred to the target network followed by fine-tuning. Prior research has shown that this approach is capable of improving task performance. However, the impact of the ImageNet pre-trained classification layer in parameter fine-tuning is mostly unexplored in the literature. In this paper, we propose a fine-tuning approach with the pre-trained classification layer. We employ layer-wise fine-tuning to determine which layers should be frozen for optimal performance. Our empirical analysis demonstrates that the proposed fine-tuning performs better than traditional fine-tuning. This finding indicates that the pre-trained classification layer holds less category-specific or more global information than believed earlier. Thus, we hypothesize that the presence of this layer is crucial for growing network depth to adapt better to a new task. Our study manifests that careful normalization and scaling are essential for creating harmony between the pre-trained and new layers for target domain adaptation. We evaluate the proposed depth augmented networks for fine-tuning on several challenging benchmark datasets and show that they can achieve higher classification accuracy than contemporary transfer learning approaches.



### SAC-Net: Spatial Attenuation Context for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.10152v3
- **DOI**: 10.1109/TCSVT.2020.2995220
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10152v3)
- **Published**: 2019-03-25 06:56:15+00:00
- **Updated**: 2020-05-12 12:45:17+00:00
- **Authors**: Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Tianyu Wang, Pheng-Ann Heng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new deep neural network design for salient object detection by maximizing the integration of local and global image context within, around, and beyond the salient objects. Our key idea is to adaptively propagate and aggregate the image context features with variable attenuation over the entire feature maps. To achieve this, we design the spatial attenuation context (SAC) module to recurrently translate and aggregate the context features independently with different attenuation factors and then to attentively learn the weights to adaptively integrate the aggregated context features. By further embedding the module to process individual layers in a deep network, namely SAC-Net, we can train the network end-to-end and optimize the context features for detecting salient objects. Compared with 29 state-of-the-art methods, experimental results show that our method performs favorably over all the others on six common benchmark data, both quantitatively and visually.



### DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a Single Color Image
- **Arxiv ID**: http://arxiv.org/abs/1903.10153v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10153v3)
- **Published**: 2019-03-25 06:56:24+00:00
- **Updated**: 2019-03-28 10:31:07+00:00
- **Authors**: Pengfei Yao, Zheng Fang, Fan Wu, Yao Feng, Jiwei Li
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Recovering 3D human body shape and pose from 2D images is a challenging task due to high complexity and flexibility of human body, and relatively less 3D labeled data. Previous methods addressing these issues typically rely on predicting intermediate results such as body part segmentation, 2D/3D joints, silhouette mask to decompose the problem into multiple sub-tasks in order to utilize more 2D labels. Most previous works incorporated parametric body shape model in their methods and predict parameters in low-dimensional space to represent human body. In this paper, we propose to directly regress the 3D human mesh from a single color image using Convolutional Neural Network(CNN). We use an efficient representation of 3D human shape and pose which can be predicted through an encoder-decoder neural network. The proposed method achieves state-of-the-art performance on several 3D human body datasets including Human3.6M, SURREAL and UP-3D with even faster running speed.



### Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1903.10157v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10157v1)
- **Published**: 2019-03-25 07:23:07+00:00
- **Updated**: 2019-03-25 07:23:07+00:00
- **Authors**: Dongwon Park, Jisoo Kim, Se Young Chun
- **Comment**: 10 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: Multi-scale approach has been used for blind image / video deblurring problems to yield excellent performance for both conventional and recent deep-learning-based state-of-the-art methods. Bicubic down-sampling is a typical choice for multi-scale approach to reduce spatial dimension after filtering with a fixed kernel. However, this fixed kernel may be sub-optimal since it may destroy important information for reliable deblurring such as strong edges. We propose convolutional neural network (CNN)-based down-scale methods for multi-scale deep-learning-based non-uniform single image deblurring. We argue that our CNN-based down-scaling effectively reduces the spatial dimension of the original image, while learned kernels with multiple channels may well-preserve necessary details for deblurring tasks. For each scale, we adopt to use RCAN (Residual Channel Attention Networks) as a backbone network to further improve performance. Our proposed method yielded state-of-the-art performance on GoPro dataset by large margin. Our proposed method was able to achieve 2.59dB higher PSNR than the current state-of-the-art method by Tao. Our proposed CNN-based down-scaling was the key factor for this excellent performance since the performance of our network without it was decreased by 1.98dB. The same networks trained with GoPro set were also evaluated on large-scale Su dataset and our proposed method yielded 1.15dB better PSNR than the Tao's method. Qualitative comparisons on Lai dataset also confirmed the superior performance of our proposed method over other state-of-the-art methods.



### Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/1904.02654v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.02654v1)
- **Published**: 2019-03-25 07:32:30+00:00
- **Updated**: 2019-03-25 07:32:30+00:00
- **Authors**: Chaohui Yu, Jindong Wang, Yiqiang Chen, Zijing Wu
- **Comment**: Accepted by International Joint Conference on Neural Networks (IJCNN)
  2019; 8 pages
- **Journal**: None
- **Summary**: Deep unsupervised domain adaptation (UDA) has recently received increasing attention from researchers. However, existing methods are computationally intensive due to the computation cost of Convolutional Neural Networks (CNN) adopted by most work. To date, there is no effective network compression method for accelerating these models. In this paper, we propose a unified Transfer Channel Pruning (TCP) approach for accelerating UDA models. TCP is capable of compressing the deep UDA model by pruning less important channels while simultaneously learning transferable features by reducing the cross-domain distribution divergence. Therefore, it reduces the impact of negative transfer and maintains competitive performance on the target task. To the best of our knowledge, TCP is the first approach that aims at accelerating deep UDA models. TCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two common backbone networks-VGG16 and ResNet50. Experimental results demonstrate that TCP achieves comparable or better classification accuracy than other comparison methods while significantly reducing the computational cost. To be more specific, in VGG16, we get even higher accuracy after pruning 26% floating point operations (FLOPs); in ResNet50, we also get higher accuracy on half of the tasks after pruning 12% FLOPs. We hope that TCP will open a new door for future research on accelerating transfer learning models.



### Efficient Bird Eye View Proposals for 3D Siamese Tracking
- **Arxiv ID**: http://arxiv.org/abs/1903.10168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10168v2)
- **Published**: 2019-03-25 08:09:13+00:00
- **Updated**: 2020-05-06 06:41:26+00:00
- **Authors**: Jesus Zarzar, Silvio Giancola, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Tracking vehicles in LIDAR point clouds is a challenging task due to the sparsity of the data and the dense search space. The lack of structure in point clouds impedes the use of convolution filters usually employed in 2D object tracking. In addition, structuring point clouds is cumbersome and implies losing fine-grained information. As a result, generating proposals in 3D space is expensive and inefficient. In this paper, we leverage the dense and structured Bird Eye View (BEV) representation of LIDAR point clouds to efficiently search for objects of interest. We use an efficient Region Proposal Network and generate a small number of object proposals in 3D. Successively, we refine our selection of 3D object candidates by exploiting the similarity capability of a 3D Siamese network. We regularize the latter 3D Siamese network for shape completion to enhance its discrimination capability. Our method attempts to solve both for an efficient search space in the BEV space and a meaningful selection using 3D LIDAR point cloud. We show that the Region Proposal in the BEV outperforms Bayesian methods such as Kalman and Particle Filters in providing proposal by a significant margin and that such candidates are suitable for the 3D Siamese network. By training our method end-to-end, we outperform the previous baseline in vehicle tracking by 12% / 18% in Success and Precision when using only 16 candidates.



### LOGAN: Unpaired Shape Transform in Latent Overcomplete Space
- **Arxiv ID**: http://arxiv.org/abs/1903.10170v3
- **DOI**: 10.1145/3355089.3356494
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10170v3)
- **Published**: 2019-03-25 08:20:48+00:00
- **Updated**: 2019-09-03 01:06:02+00:00
- **Authors**: Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang
- **Comment**: Download supplementary material here ->
  https://kangxue.org/papers/logan_supp.pdf
- **Journal**: ACM Transactions on Graphics(Proc. of SIGGRAPH Asia), 38(6),
  198:1-198:13, 2019
- **Summary**: We introduce LOGAN, a deep neural network aimed at learning general-purpose shape transforms from unpaired domains. The network is trained on two sets of shapes, e.g., tables and chairs, while there is neither a pairing between shapes from the domains as supervision nor any point-wise correspondence between any shapes. Once trained, LOGAN takes a shape from one domain and transforms it into the other. Our network consists of an autoencoder to encode shapes from the two input domains into a common latent space, where the latent codes concatenate multi-scale shape features, resulting in an overcomplete representation. The translator is based on a generative adversarial network (GAN), operating in the latent space, where an adversarial loss enforces cross-domain translation while a feature preservation loss ensures that the right shape features are preserved for a natural shape transform. We conduct ablation studies to validate each of our key network designs and demonstrate superior capabilities in unpaired shape transforms on a variety of examples over baselines and state-of-the-art approaches. We show that LOGAN is able to learn what shape features to preserve during shape translation, either local or non-local, whether content or style, depending solely on the input domains for training.



### Looking Fast and Slow: Memory-Guided Mobile Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.10172v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10172v1)
- **Published**: 2019-03-25 08:30:58+00:00
- **Updated**: 2019-03-25 08:30:58+00:00
- **Authors**: Mason Liu, Menglong Zhu, Marie White, Yinxiao Li, Dmitry Kalenichenko
- **Comment**: None
- **Journal**: None
- **Summary**: With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the "gist" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.



### A Novel Method for the Absolute Pose Problem with Pairwise Constraints
- **Arxiv ID**: http://arxiv.org/abs/1903.10175v2
- **DOI**: 10.3390/rs11243007
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10175v2)
- **Published**: 2019-03-25 08:37:49+00:00
- **Updated**: 2019-03-28 06:18:58+00:00
- **Authors**: Yinlong Liu, Xuechen Li, Manning Wang, Guang Chen, Zhijian Song, Alois Knoll
- **Comment**: 10 pages, 7figures
- **Journal**: None
- **Summary**: Absolute pose estimation is a fundamental problem in computer vision, and it is a typical parameter estimation problem, meaning that efforts to solve it will always suffer from outlier-contaminated data. Conventionally, for a fixed dimensionality d and the number of measurements N, a robust estimation problem cannot be solved faster than O(N^d). Furthermore, it is almost impossible to remove d from the exponent of the runtime of a globally optimal algorithm. However, absolute pose estimation is a geometric parameter estimation problem, and thus has special constraints. In this paper, we consider pairwise constraints and propose a globally optimal algorithm for solving the absolute pose estimation problem. The proposed algorithm has a linear complexity in the number of correspondences at a given outlier ratio. Concretely, we first decouple the rotation and the translation subproblems by utilizing the pairwise constraints, and then we solve the rotation subproblem using the branch-and-bound algorithm. Lastly, we estimate the translation based on the known rotation by using another branch-and-bound algorithm. The advantages of our method are demonstrated via thorough testing on both synthetic and real-world data



### DeepRED: Deep Image Prior Powered by RED
- **Arxiv ID**: http://arxiv.org/abs/1903.10176v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.10176v3)
- **Published**: 2019-03-25 08:39:53+00:00
- **Updated**: 2019-10-24 10:50:39+00:00
- **Authors**: Gary Mataev, Michael Elad, Peyman Milanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Inverse problems in imaging are extensively studied, with a variety of strategies, tools, and theory that have been accumulated over the years. Recently, this field has been immensely influenced by the emergence of deep-learning techniques. One such contribution, which is the focus of this paper, is the Deep Image Prior (DIP) work by Ulyanov, Vedaldi, and Lempitsky (2018). DIP offers a new approach towards the regularization of inverse problems, obtained by forcing the recovered image to be synthesized from a given deep architecture. While DIP has been shown to be quite an effective unsupervised approach, its results still fall short when compared to state-of-the-art alternatives.   In this work, we aim to boost DIP by adding an explicit prior, which enriches the overall regularization effect in order to lead to better-recovered images. More specifically, we propose to bring-in the concept of Regularization by Denoising (RED), which leverages existing denoisers for regularizing inverse problems. Our work shows how the two (DIP and RED) can be merged into a highly effective unsupervised recovery process while avoiding the need to differentiate the chosen denoiser, and leading to very effective results, demonstrated for several tested problems.



### Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.10195v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.10195v1)
- **Published**: 2019-03-25 09:27:44+00:00
- **Updated**: 2019-03-25 09:27:44+00:00
- **Authors**: Amanda Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres, Xavier Giro-i-Nieto
- **Comment**: ICASSP 2019. Projevct website at
  https://imatge-upc.github.io/wav2pix/
- **Journal**: None
- **Summary**: Speech is a rich biometric signal that contains information about the identity, gender and emotional state of the speaker. In this work, we explore its potential to generate face images of a speaker by conditioning a Generative Adversarial Network (GAN) with raw speech input. We propose a deep neural network that is trained from scratch in an end-to-end fashion, generating a face directly from the raw speech waveform without any additional identity information (e.g reference image or one-hot encoding). Our model is trained in a self-supervised approach by exploiting the audio and visual signals naturally aligned in videos. With the purpose of training from video data, we present a novel dataset collected for this work, with high-quality videos of youtubers with notable expressiveness in both the speech and visual signals.



### Dual Variational Generation for Low-Shot Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.10203v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10203v3)
- **Published**: 2019-03-25 09:39:33+00:00
- **Updated**: 2019-12-01 11:54:02+00:00
- **Authors**: Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, Ran He
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results. The related code is available at https://github.com/BradyFU/DVG.



### Accurate Global Trajectory Alignment using Poles and Road Markings
- **Arxiv ID**: http://arxiv.org/abs/1903.10205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10205v1)
- **Published**: 2019-03-25 09:40:11+00:00
- **Updated**: 2019-03-25 09:40:11+00:00
- **Authors**: Haohao Hu, Marc Sons, Christoph Stiller
- **Comment**: 6 packages, 6 figures, conference
- **Journal**: None
- **Summary**: Currently, digital maps are indispensable for automated driving. However, due to the low precision and reliability of GNSS particularly in urban areas, fusing trajectories of independent recording sessions and different regions is a challenging task. To bypass the flaws from direct incorporation of GNSS measurements for geo-referencing, the usage of aerial imagery seems promising. Furthermore, more accurate geo-referencing improves the global map accuracy and allows to estimate the sensor calibration error. In this paper, we present a novel geo-referencing approach to align trajectories to aerial imagery using poles and road markings. To match extracted features from sensor observations to aerial imagery landmarks robustly, a RANSAC-based matching approach is applied in a sliding window. For that, we assume that the trajectories are roughly referenced to the imagery which can be achieved by rough GNSS measurements from a low-cost GNSS receiver. Finally, we align the initial trajectories precisely to the aerial imagery by minimizing a geometric cost function comprising all determined matches. Evaluations performed on data recorded in Karlsruhe, Germany show that our algorithm yields trajectories which are accurately referenced to the used aerial imagery.



### Deep Shape from Polarization
- **Arxiv ID**: http://arxiv.org/abs/1903.10210v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10210v2)
- **Published**: 2019-03-25 09:55:08+00:00
- **Updated**: 2020-05-25 05:36:22+00:00
- **Authors**: Yunhao Ba, Alex Ross Gilbert, Franklin Wang, Jinfa Yang, Rui Chen, Yiqin Wang, Lei Yan, Boxin Shi, Achuta Kadambi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper makes a first attempt to bring the Shape from Polarization (SfP) problem to the realm of deep learning. The previous state-of-the-art methods for SfP have been purely physics-based. We see value in these principled models, and blend these physical models as priors into a neural network architecture. This proposed approach achieves results that exceed the previous state-of-the-art on a challenging dataset we introduce. This dataset consists of polarization images taken over a range of object textures, paints, and lighting conditions. We report that our proposed method achieves the lowest test error on each tested condition in our dataset, showing the value of blending data-driven and physics-driven approaches.



### Manifold Criterion Guided Transfer Learning via Intermediate Domain Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.10211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10211v1)
- **Published**: 2019-03-25 09:58:51+00:00
- **Updated**: 2019-03-25 09:58:51+00:00
- **Authors**: Lei Zhang, Shanshan Wang, Guang-Bin Huang, Wangmeng Zuo, Jian Yang, David Zhang
- **Comment**: This paper has been accepted by IEEE Transactions on Neural Networks
  and Learning Systems
- **Journal**: None
- **Summary**: In many practical transfer learning scenarios, the feature distribution is different across the source and target domains (i.e. non-i.i.d.). Maximum mean discrepancy (MMD), as a domain discrepancy metric, has achieved promising performance in unsupervised domain adaptation (DA). We argue that MMD-based DA methods ignore the data locality structure, which, to some extent, would cause the negative transfer effect. The locality plays an important role in minimizing the nonlinear local domain discrepancy underlying the marginal distributions. For better exploiting the domain locality, a novel local generative discrepancy metric (LGDM) based intermediate domain generation learning called Manifold Criterion guided Transfer Learning (MCTL) is proposed in this paper. The merits of the proposed MCTL are four-fold: 1) the concept of manifold criterion (MC) is first proposed as a measure validating the distribution matching across domains, and domain adaptation is achieved if the MC is satisfied; 2) the proposed MC can well guide the generation of the intermediate domain sharing similar distribution with the target domain, by minimizing the local domain discrepancy; 3) a global generative discrepancy metric (GGDM) is presented, such that both the global and local discrepancy can be effectively and positively reduced; 4) a simplified version of MCTL called MCTL-S is presented under a perfect domain generation assumption for more generic learning scenario. Experiments on a number of benchmark visual transfer tasks demonstrate the superiority of the proposed manifold criterion guided generative transfer method, by comparing with other state-of-the-art methods. The source code is available in https://github.com/wangshanshanCQU/MCTL.



### Learning from Adversarial Features for Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.10225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10225v1)
- **Published**: 2019-03-25 10:30:23+00:00
- **Updated**: 2019-03-25 10:30:23+00:00
- **Authors**: Wei Shen, Ziqiang Shi, Jun Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Many recent few-shot learning methods concentrate on designing novel model architectures. In this paper, we instead show that with a simple backbone convolutional network we can even surpass state-of-the-art classification accuracy. The essential part that contributes to this superior performance is an adversarial feature learning strategy that improves the generalization capability of our model. In this work, adversarial features are those features that can cause the classifier uncertain about its prediction. In order to generate adversarial features, we firstly locate adversarial regions based on the derivative of the entropy with respect to an averaging mask. Then we use the adversarial region attention to aggregate the feature maps to obtain the adversarial features. In this way, we can explore and exploit the entire spatial area of the feature maps to mine more diverse discriminative knowledge. We perform extensive model evaluations and analyses on miniImageNet and tieredImageNet datasets demonstrating the effectiveness of the proposed method.



### Convolutional neural network for breathing phase detection in lung sounds
- **Arxiv ID**: http://arxiv.org/abs/1903.10251v1
- **DOI**: 10.3390/s19081798
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10251v1)
- **Published**: 2019-03-25 11:47:05+00:00
- **Updated**: 2019-03-25 11:47:05+00:00
- **Authors**: Cristina Jácome, Johan Ravn, Einar Holsbø, Juan Carlos Aviles-Solis, Hasse Melbye, Lars Ailo Bongo
- **Comment**: None
- **Journal**: None
- **Summary**: We applied deep learning to create an algorithm for breathing phase detection in lung sound recordings, and we compared the breathing phases detected by the algorithm and manually annotated by two experienced lung sound researchers. Our algorithm uses a convolutional neural network with spectrograms as the features, removing the need to specify features explicitly. We trained and evaluated the algorithm using three subsets that are larger than previously seen in the literature. We evaluated the performance of the method using two methods. First, discrete count of agreed breathing phases (using 50% overlap between a pair of boxes), shows a mean agreement with lung sound experts of 97% for inspiration and 87% for expiration. Second, the fraction of time of agreement (in seconds) gives higher pseudo-kappa values for inspiration (0.73-0.88) than expiration (0.63-0.84), showing an average sensitivity of 97% and an average specificity of 84%. With both evaluation methods, the agreement between the annotators and the algorithm shows human level performance for the algorithm. The developed algorithm is valid for detecting breathing phases in lung sound recordings.



### MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/1903.10258v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10258v3)
- **Published**: 2019-03-25 12:05:27+00:00
- **Updated**: 2019-08-14 03:41:11+00:00
- **Authors**: Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting Cheng, Jian Sun
- **Comment**: ICCV 2019 Camera ready version. Codes are available on
  https://github.com/liuzechun/MetaPruning
- **Journal**: None
- **Summary**: In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning.



### AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss
- **Arxiv ID**: http://arxiv.org/abs/1903.10297v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1903.10297v5)
- **Published**: 2019-03-25 13:14:23+00:00
- **Updated**: 2020-11-25 12:55:11+00:00
- **Authors**: Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas Guibas, Hao Zhang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: We introduce AdaCoSeg, a deep neural network architecture for adaptive co-segmentation of a set of 3D shapes represented as point clouds. Differently from the familiar single-instance segmentation problem, co-segmentation is intrinsically contextual: how a shape is segmented can vary depending on the set it is in. Hence, our network features an adaptive learning module to produce a consistent shape segmentation which adapts to a set. Specifically, given an input set of unsegmented shapes, we first employ an offline pre-trained part prior network to propose per-shape parts. Then, the co-segmentation network iteratively and} jointly optimizes the part labelings across the set subjected to a novel group consistency loss defined by matrix ranks. While the part prior network can be trained with noisy and inconsistently segmented shapes, the final output of AdaCoSeg is a consistent part labeling for the input set, with each shape segmented into up to (a user-specified) K parts. Overall, our method is weakly supervised, producing segmentations tailored to the test set, without consistent ground-truth segmentations. We show qualitative and quantitative results from AdaCoSeg and evaluate it via ablation studies and comparisons to state-of-the-art co-segmentation methods.



### Apple Leaf Disease Identification through Region-of-Interest-Aware Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1903.10356v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10356v2)
- **Published**: 2019-03-25 14:15:40+00:00
- **Updated**: 2019-07-29 12:23:35+00:00
- **Authors**: Hee-Jin Yu, Chang-Hwan Son
- **Comment**: None
- **Journal**: Journal of Imaging Science and Technology, vol. 64, no. 2, pp.
  20507-1-20507-10, Jan. 2020
- **Summary**: A new method of recognizing apple leaf diseases through region-of-interest-aware deep convolutional neural network is proposed in this paper. The primary idea is that leaf disease symptoms appear in the leaf area whereas the background region contains no useful information regarding leaf diseases. To realize this idea, two subnetworks are first designed. One is for the division of the input image into three areas: background, leaf area, and spot area indicating the leaf diseases, which is the region of interest, and the other is for the classification of leaf diseases. The two subnetworks exhibit the architecture types of an encoder-decoder network and VGG network, respectively; subsequently, they are trained separately through transfer learning with a new training set containing class information, according to the types of leaf diseases and the ground truth images where the background, leaf area, and spot area are separated. Next, to connect these subnetworks and subsequently train the connected whole network in an end-to-end manner, the predicted ROI feature map is stacked on the top of the input image through a fusion layer, and subsequently fed into the subnetwork used for the leaf disease identification. The experimental results indicate that correct recognition accuracy can be increased using the predicted ROI feature map. It is also shown that the proposed method obtains better performance than the conventional state-of-the-art methods: transfer-learning-based methods, bilinear model, and multiscale-based deep feature extraction, and pooling approach.



### Noise-Tolerant Paradigm for Training Face Recognition CNNs
- **Arxiv ID**: http://arxiv.org/abs/1903.10357v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10357v2)
- **Published**: 2019-03-25 14:16:06+00:00
- **Updated**: 2019-03-26 05:57:35+00:00
- **Authors**: Wei Hu, Yangyu Huang, Fan Zhang, Ruirui Li
- **Comment**: None
- **Journal**: None
- **Summary**: Benefit from large-scale training datasets, deep Convolutional Neural Networks(CNNs) have achieved impressive results in face recognition(FR). However, tremendous scale of datasets inevitably lead to noisy data, which obviously reduce the performance of the trained CNN models. Kicking out wrong labels from large-scale FR datasets is still very expensive, although some cleaning approaches are proposed. According to the analysis of the whole process of training CNN models supervised by angular margin based loss(AM-Loss) functions, we find that the $\theta$ distribution of training samples implicitly reflects their probability of being clean. Thus, we propose a novel training paradigm that employs the idea of weighting samples based on the above probability. Without any prior knowledge of noise, we can train high performance CNN models with large-scale FR datasets. Experiments demonstrate the effectiveness of our training paradigm. The codes are available at https://github.com/huangyangyu/NoiseFace.



### Structured 2D Representation of 3D Data for Shape Processing
- **Arxiv ID**: http://arxiv.org/abs/1903.10360v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10360v2)
- **Published**: 2019-03-25 14:21:08+00:00
- **Updated**: 2023-08-11 12:08:58+00:00
- **Authors**: Kripasindhu Sarkar, Elizabeth Mathews, Didier Stricker
- **Comment**: Results of some of the experiments were incorrect
- **Journal**: None
- **Summary**: We represent 3D shape by structured 2D representations of fixed length making it feasible to apply well investigated 2D convolutional neural networks (CNN) for both discriminative and geometric tasks on 3D shapes. We first provide a general introduction to such structured descriptors, analyze their different forms and show how a simple 2D CNN can be used to achieve good classification result. With a specialized classification network for images and our structured representation, we achieve the classification accuracy of 99.7\% in the ModelNet40 test set - improving the previous state-of-the-art by a large margin. We finally provide a novel framework for performing the geometric task of 3D segmentation using 2D CNNs and the structured representation - concluding the utility of such descriptors for both discriminative and geometric tasks.



### MeshGAN: Non-linear 3D Morphable Models of Faces
- **Arxiv ID**: http://arxiv.org/abs/1903.10384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10384v1)
- **Published**: 2019-03-25 15:03:35+00:00
- **Updated**: 2019-03-25 15:03:35+00:00
- **Authors**: Shiyang Cheng, Michael Bronstein, Yuxiang Zhou, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are currently the method of choice for generating visual data. Certain GAN architectures and training methods have demonstrated exceptional performance in generating realistic synthetic images (in particular, of human faces). However, for 3D object, GANs still fall short of the success they have had with images. One of the reasons is due to the fact that so far GANs have been applied as 3D convolutional architectures to discrete volumetric representations of 3D objects. In this paper, we propose the first intrinsic GANs architecture operating directly on 3D meshes (named as MeshGAN). Both quantitative and qualitative results are provided to show that MeshGAN can be used to generate high-fidelity 3D face with rich identities and expressions.



### Generalized Feedback Loop for Joint Hand-Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.10883v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10883v1)
- **Published**: 2019-03-25 15:40:34+00:00
- **Updated**: 2019-03-25 15:40:34+00:00
- **Authors**: Markus Oberweger, Paul Wohlhart, Vincent Lepetit
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1609.09698
- **Journal**: Transactions on Pattern Analysis and Machine Intelligence 2019
- **Summary**: We propose an approach to estimating the 3D pose of a hand, possibly handling an object, given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. This approach can be generalized to a hand interacting with an object. Therefore, we jointly estimate the 3D pose of the hand and the 3D pose of the object. Our approach performs en-par with state-of-the-art methods for 3D hand pose estimation, and outperforms state-of-the-art methods for joint hand-object pose estimation when using depth images only. Also, our approach is efficient as our implementation runs in real-time on a single GPU.



### ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views
- **Arxiv ID**: http://arxiv.org/abs/1903.10412v1
- **DOI**: None
- **Categories**: **cs.CV**, I.7.5
- **Links**: [PDF](http://arxiv.org/pdf/1903.10412v1)
- **Published**: 2019-03-25 15:52:32+00:00
- **Updated**: 2019-03-25 15:52:32+00:00
- **Authors**: Chongsheng Zhang, Guowen Peng, Yuefeng Tao, Feifei Fu, Wei Jiang, George Almpanidis, Ke Chen
- **Comment**: 10 pages, 2 figures, 5 tables
- **Journal**: None
- **Summary**: In this paper, we introduce the ShopSign dataset, which is a newly developed natural scene text dataset of Chinese shop signs in street views. Although a few scene text datasets are already publicly available (e.g. ICDAR2015, COCO-Text), there are few images in these datasets that contain Chinese texts/characters. Hence, we collect and annotate the ShopSign dataset to advance research in Chinese scene text detection and recognition.   The new dataset has three distinctive characteristics: (1) large-scale: it contains 25,362 Chinese shop sign images, with a total number of 196,010 text-lines. (2) diversity: the images in ShopSign were captured in different scenes, from downtown to developing regions, using more than 50 different mobile phones. (3) difficulty: the dataset is very sparse and imbalanced. It also includes five categories of hard images (mirror, wooden, deformed, exposed and obscure). To illustrate the challenges in ShopSign, we run baseline experiments using state-of-the-art scene text detection methods (including CTPN, TextBoxes++ and EAST), and cross-dataset validation to compare their corresponding performance on the related datasets such as CTW, RCTW and ICPR 2018 MTWI challenge dataset.   The sample images and detailed descriptions of our ShopSign dataset are publicly available at: https://github.com/chongshengzhang/shopsign.



### Locomotion and gesture tracking in mice and small animals for neurosceince applications: A survey
- **Arxiv ID**: http://arxiv.org/abs/1903.10422v1
- **DOI**: None
- **Categories**: **cs.CV**, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1903.10422v1)
- **Published**: 2019-03-25 16:07:28+00:00
- **Updated**: 2019-03-25 16:07:28+00:00
- **Authors**: Waseem Abbas, David Masip Rodo
- **Comment**: 41 pages, 2 figures
- **Journal**: None
- **Summary**: Neuroscience has traditionally relied on manually observing lab animals in controlled environments. Researchers usually record animals behaving in free or restrained manner and then annotate the data manually. The manual annotation is not desirable for three reasons; one, it is time consuming, two, it is prone to human errors and three, no two human annotators will 100\% agree on annotation, so it is not reproducible. Consequently, automated annotation of such data has gained traction because it is efficient and replicable. Usually, the automatic annotation of neuroscience data relies on computer vision and machine leaning techniques. In this article, we have covered most of the approaches taken by researchers for locomotion and gesture tracking of lab animals. We have divided these papers in categories based upon the hardware they use and the software approach they take. We also have summarized their strengths and weaknesses.



### Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1903.10427v1
- **DOI**: 10.1109/CVPR.2019.00636
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10427v1)
- **Published**: 2019-03-25 16:12:20+00:00
- **Updated**: 2019-03-25 16:12:20+00:00
- **Authors**: Jaime Spencer, Richard Bowden, Simon Hadfield
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: How do computers and intelligent agents view the world around them? Feature extraction and representation constitutes one the basic building blocks towards answering this question. Traditionally, this has been done with carefully engineered hand-crafted techniques such as HOG, SIFT or ORB. However, there is no ``one size fits all'' approach that satisfies all requirements. In recent years, the rising popularity of deep learning has resulted in a myriad of end-to-end solutions to many computer vision problems. These approaches, while successful, tend to lack scalability and can't easily exploit information learned by other systems. Instead, we propose SAND features, a dedicated deep learning solution to feature extraction capable of providing hierarchical context information. This is achieved by employing sparse relative labels indicating relationships of similarity/dissimilarity between image locations. The nature of these labels results in an almost infinite set of dissimilar examples to choose from. We demonstrate how the selection of negative examples during training can be used to modify the feature space and vary it's properties. To demonstrate the generality of this approach, we apply the proposed features to a multitude of tasks, each requiring different properties. This includes disparity estimation, semantic segmentation, self-localisation and SLAM. In all cases, we show how incorporating SAND features results in better or comparable results to the baseline, whilst requiring little to no additional training. Code can be found at: https://github.com/jspenmar/SAND_features



### CODA: Counting Objects via Scale-aware Adversarial Density Adaption
- **Arxiv ID**: http://arxiv.org/abs/1903.10442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10442v1)
- **Published**: 2019-03-25 16:24:03+00:00
- **Updated**: 2019-03-25 16:24:03+00:00
- **Authors**: Li Wang, Yongbo Li, Xiangyang Xue
- **Comment**: Accepted to ICME2019
- **Journal**: None
- **Summary**: Recent advances in crowd counting have achieved promising results with increasingly complex convolutional neural network designs. However, due to the unpredictable domain shift, generalizing trained model to unseen scenarios is often suboptimal. Inspired by the observation that density maps of different scenarios share similar local structures, we propose a novel adversarial learning approach in this paper, i.e., CODA (\emph{Counting Objects via scale-aware adversarial Density Adaption}). To deal with different object scales and density distributions, we perform adversarial training with pyramid patches of multi-scales from both source- and target-domain. Along with a ranking constraint across levels of the pyramid input, consistent object counts can be produced for different scales. Extensive experiments demonstrate that our network produces much better results on unseen datasets compared with existing counting adaption models. Notably, the performance of our CODA is comparable with the state-of-the-art fully-supervised models that are trained on the target dataset. Further analysis indicates that our density adaption framework can effortlessly extend to scenarios with different objects. \emph{The code is available at https://github.com/Willy0919/CODA.}



### The functional role of cue-driven feature-based feedback in object recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.10446v1
- **DOI**: 10.32470/CCN.2018.1044-0
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10446v1)
- **Published**: 2019-03-25 16:27:07+00:00
- **Updated**: 2019-03-25 16:27:07+00:00
- **Authors**: Sushrut Thorat, Marcel van Gerven, Marius Peelen
- **Comment**: 4 pages, 4 figures, published at the Conference on Cognitive
  Computational Neuroscience (CCN) 2018
- **Journal**: None
- **Summary**: Visual object recognition is not a trivial task, especially when the objects are degraded or surrounded by clutter or presented briefly. External cues (such as verbal cues or visual context) can boost recognition performance in such conditions. In this work, we build an artificial neural network to model the interaction between the object processing stream (OPS) and the cue. We study the effects of varying neural and representational capacities of the OPS on the performance boost provided by cue-driven feature-based feedback in the OPS. We observe that the feedback provides performance boosts only if the category-specific features about the objects cannot be fully represented in the OPS. This representational limit is more dependent on task demands than neural capacity. We also observe that the feedback scheme trained to maximise recognition performance boost is not the same as tuning-based feedback, and actually performs better than tuning-based feedback.



### Learning Quadrangulated Patches For 3D Shape Processing
- **Arxiv ID**: http://arxiv.org/abs/1903.10885v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10885v1)
- **Published**: 2019-03-25 16:54:59+00:00
- **Updated**: 2019-03-25 16:54:59+00:00
- **Authors**: Kripasindhu Sarkar, Kiran Varanasi, Didier Stricker
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1709.06868
- **Journal**: None
- **Summary**: We propose a system for surface completion and inpainting of 3D shapes using generative models, learnt on local patches. Our method uses a novel encoding of height map based local patches parameterized using 3D mesh quadrangulation of the low resolution input shape. This provides us sufficient amount of local 3D patches to learn a generative model for the task of repairing moderate sized holes. Following the ideas from the recent progress in 2D inpainting, we investigated both linear dictionary based model and convolutional denoising autoencoders based model for the task for inpainting, and show our results to be better than the previous geometry based method of surface inpainting. We validate our method on both synthetic shapes and real world scans.



### DeepCenterline: a Multi-task Fully Convolutional Network for Centerline Extraction
- **Arxiv ID**: http://arxiv.org/abs/1903.10481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10481v1)
- **Published**: 2019-03-25 17:29:30+00:00
- **Updated**: 2019-03-25 17:29:30+00:00
- **Authors**: Zhihui Guo, Junjie Bai, Yi Lu, Xin Wang, Kunlin Cao, Qi Song, Milan Sonka, Youbing Yin
- **Comment**: Accepted by the international conference on Information Processing in
  Medical Imaging (IPMI) 2019
- **Journal**: None
- **Summary**: A novel centerline extraction framework is reported which combines an end-to-end trainable multi-task fully convolutional network (FCN) with a minimal path extractor. The FCN simultaneously computes centerline distance maps and detects branch endpoints. The method generates single-pixel-wide centerlines with no spurious branches. It handles arbitrary tree-structured object with no prior assumption regarding depth of the tree or its bifurcation pattern. It is also robust to substantial scale changes across different parts of the target object and minor imperfections of the object's segmentation mask. To the best of our knowledge, this is the first deep-learning based centerline extraction method that guarantees single-pixel-wide centerline for a complex tree-structured object. The proposed method is validated in coronary artery centerline extraction on a dataset of 620 patients (400 of which used as test set). This application is challenging due to the large number of coronary branches, branch tortuosity, and large variations in length, thickness, shape, etc. The proposed method generates well-positioned centerlines, exhibiting lower number of missing branches and is more robust in the presence of minor imperfections of the object segmentation mask. Compared to a state-of-the-art traditional minimal path approach, our method improves patient-level success rate of centerline extraction from 54.3% to 88.8% according to independent human expert review.



### Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness
- **Arxiv ID**: http://arxiv.org/abs/1903.10484v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.10484v1)
- **Published**: 2019-03-25 17:29:52+00:00
- **Updated**: 2019-03-25 17:29:52+00:00
- **Authors**: Jörn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramèr, Nicolas Papernot
- **Comment**: Accepted at the ICLR 2019 SafeML Workshop
- **Journal**: None
- **Summary**: Adversarial examples are malicious inputs crafted to cause a model to misclassify them. Their most common instantiation, "perturbation-based" adversarial examples introduce changes to the input that leave its true label unchanged, yet result in a different model prediction. Conversely, "invariance-based" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed.   In this paper, we demonstrate that robustness to perturbation-based adversarial examples is not only insufficient for general robustness, but worse, it can also increase vulnerability of the model to invariance-based adversarial examples. In addition to analytical constructions, we empirically study vision classifiers with state-of-the-art robustness to perturbation-based adversaries constrained by an $\ell_p$ norm. We mount attacks that exploit excessive model invariance in directions relevant to the task, which are able to find adversarial examples within the $\ell_p$ ball. In fact, we find that classifiers trained to be $\ell_p$-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts.   Excessive invariance is not limited to models trained to be robust to perturbation-based $\ell_p$-norm adversaries. In fact, we argue that the term adversarial example is used to capture a series of model limitations, some of which may not have been discovered yet. Accordingly, we call for a set of precise definitions that taxonomize and address each of these shortcomings in learning.



### Micro-Batch Training with Batch-Channel Normalization and Weight Standardization
- **Arxiv ID**: http://arxiv.org/abs/1903.10520v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10520v2)
- **Published**: 2019-03-25 18:00:05+00:00
- **Updated**: 2020-08-09 21:25:15+00:00
- **Authors**: Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, Alan Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.



### Learning Embodied Semantics via Music and Dance Semiotic Correlations
- **Arxiv ID**: http://arxiv.org/abs/1903.10534v1
- **DOI**: 10.1007/s00521-021-06090-8
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1903.10534v1)
- **Published**: 2019-03-25 18:09:03+00:00
- **Updated**: 2019-03-25 18:09:03+00:00
- **Authors**: Francisco Afonso Raposo, David Martins de Matos, Ricardo Ribeiro
- **Comment**: 24 pages, 1 figure, 5 tables
- **Journal**: Neural Computing and Applications, vol. 33, pp. 14481-14493, 2021
- **Summary**: Music semantics is embodied, in the sense that meaning is biologically mediated by and grounded in the human body and brain. This embodied cognition perspective also explains why music structures modulate kinetic and somatosensory perception. We leverage this aspect of cognition, by considering dance as a proxy for music perception, in a statistical computational model that learns semiotic correlations between music audio and dance video. We evaluate the ability of this model to effectively capture underlying semantics in a cross-modal retrieval task. Quantitative results, validated with statistical significance testing, strengthen the body of evidence for embodied cognition in music and show the model can recommend music audio for dance video queries and vice-versa.



### Learning Monocular Visual Odometry through Geometry-Aware Curriculum Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.10543v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.10543v2)
- **Published**: 2019-03-25 18:26:06+00:00
- **Updated**: 2019-11-05 21:22:03+00:00
- **Authors**: Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Sen Wang, Andrew Markham, Niki Trigoni
- **Comment**: accepted in IEEE ICRA 2019
- **Journal**: None
- **Summary**: Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.



### Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph
- **Arxiv ID**: http://arxiv.org/abs/1903.10547v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10547v2)
- **Published**: 2019-03-25 18:41:06+00:00
- **Updated**: 2019-03-27 15:01:34+00:00
- **Authors**: Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan Salakhutdinov, Ali Farhadi
- **Comment**: CVPR 2019. Supplementary included. Fixing a small typo
- **Journal**: None
- **Summary**: Visual relationship reasoning is a crucial yet challenging task for understanding rich interactions across visual concepts. For example, a relationship 'man, open, door' involves a complex relation 'open' between concrete entities 'man, door'. While much of the existing work has studied this problem in the context of still images, understanding visual relationships in videos has received limited attention. Due to their temporal nature, videos enable us to model and reason about a more comprehensive set of visual relationships, such as those requiring multiple (temporal) observations (e.g., 'man, lift up, box' vs. 'man, put down, box'), as well as relationships that are often correlated through time (e.g., 'woman, pay, money' followed by 'woman, buy, coffee'). In this paper, we construct a Conditional Random Field on a fully-connected spatio-temporal graph that exploits the statistical dependency between relational entities spatially and temporally. We introduce a novel gated energy function parametrization that learns adaptive relations conditioned on visual observations. Our model optimization is computationally efficient, and its space computation complexity is significantly amortized through our proposed parameterization. Experimental results on benchmark video datasets (ImageNet Video and Charades) demonstrate state-of-the-art performance across three standard relationship reasoning tasks: Detection, Tagging, and Recognition.



### A Novel Pixel-Averaging Technique for Extracting Training Data from a Single Image, Used in ML-Based Image Enlargement
- **Arxiv ID**: http://arxiv.org/abs/1904.00747v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00747v1)
- **Published**: 2019-03-25 18:48:36+00:00
- **Updated**: 2019-03-25 18:48:36+00:00
- **Authors**: Amir Rastar
- **Comment**: None
- **Journal**: None
- **Summary**: Size of the training dataset is an important factor in the performance of a machine learning algorithms and tools used in medical image processing are not exceptions. Machine learning tools normally require a decent amount of training data before they could efficiently predict a target. For image processing and computer vision, the number of images determines the validity and reliability of the training set. Medical images in some cases, suffer from poor quality and inadequate quantity required for a suitable training set. The proposed algorithm in this research obviates the need for large or even small image datasets used in machine learning based image enlargement techniques by extracting the required data from a single image. The extracted data was then introduced to a decision tree regressor for upscaling greyscale medical images at different zoom levels. Results from the algorithm are relatively acceptable compared to third-party applications and promising for future research. This technique could be tailored to the requirements of other machine learning tools and the results may be improved by further tweaking of the tools hyperparameters.



### Deep Learning for Localization in the Lung
- **Arxiv ID**: http://arxiv.org/abs/1903.10554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10554v1)
- **Published**: 2019-03-25 19:11:39+00:00
- **Updated**: 2019-03-25 19:11:39+00:00
- **Authors**: Jake Sganga, David Eng, Chauncey Graetzel, David B. Camarillo
- **Comment**: 35 pages double-spaced, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Lung cancer is the leading cause of cancer-related death worldwide, and early diagnosis is critical to improving patient outcomes. To diagnose cancer, a highly trained pulmonologist must navigate a flexible bronchoscope deep into the branched structure of the lung for biopsy. The biopsy fails to sample the target tissue in 26-33% of cases largely because of poor registration with the preoperative CT map. We developed two deep learning approaches to localize the bronchoscope in the preoperative CT map in real time and tested the algorithms across 13 trajectories in a lung phantom and 68 trajectories in 11 human cadaver lungs. In the lung phantom, we observe performance reaching 95% precision and recall of visible airways and 3 mm average position error. On a successful cadaver lung sequence, the algorithms trained on simulation alone achieved 77%-94% precision and recall of visible airways and 4-6 mm average position error. We also compare the effect of GAN-stylizing images and we look at aggregate statistics over the entire set of trajectories.



### Augmenting Gastrointestinal Health: A Deep Learning Approach to Human Stool Recognition and Characterization in Macroscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1903.10578v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.10578v1)
- **Published**: 2019-03-25 20:08:17+00:00
- **Updated**: 2019-03-25 20:08:17+00:00
- **Authors**: David Hachuel, Akshay Jha, Deborah Estrin, Alfonso Martinez, Kyle Staller, Christopher Velez
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose - Functional bowel diseases, including irritable bowel syndrome, chronic constipation, and chronic diarrhea, are some of the most common diseases seen in clinical practice. Many patients describe a range of triggers for altered bowel consistency and symptoms. However, characterization of the relationship between symptom triggers using bowel diaries is hampered by poor compliance and lack of objective stool consistency measurements. We sought to develop a stool detection and tracking system using computer vision and deep convolutional neural networks (CNN) that could be used by patients, providers, and researchers in the assessment of chronic gastrointestinal (GI) disease.



### Reducing the dilution: An analysis of the information sensitiveness of capsule network with a practical improvement method
- **Arxiv ID**: http://arxiv.org/abs/1903.10588v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.10588v3)
- **Published**: 2019-03-25 20:28:44+00:00
- **Updated**: 2019-05-02 21:42:46+00:00
- **Authors**: Zonglin Yang, Xinggang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule network has shown various advantages over convolutional neural network (CNN). It keeps more precise spatial information than CNN and uses equivariance instead of invariance during inference and highly potential to be a new effective tool for visual tasks. However, the current capsule networks have incompatible performance with CNN when facing datasets with background and complex target objects and are lacking in universal and efficient regularization method.   We analyze a main reason of the incompatible performance as the conflict between information sensitiveness of capsule network and unreasonably higher activation value distribution of capsules in primary capsule layer. Correspondingly, we propose a practical improvement method by restraining the activation value of capsules in primary capsule layer to suppress non-informative capsules and highlight discriminative capsules. In the experiments, the method has achieved better performances on various mainstream datasets. In addition, the proposed improvement methods can be seen as a suitable, simple and efficient regularization method that can be generally used in capsule network.



### Motion Deblurring with an Adaptive Network
- **Arxiv ID**: http://arxiv.org/abs/1903.11394v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.11394v4)
- **Published**: 2019-03-25 21:13:48+00:00
- **Updated**: 2022-02-07 00:13:21+00:00
- **Authors**: Kuldeep Purohit, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, increasing the network capacity in this manner comes at the expense of increase in model size and inference speed, and ignoring the non-uniform nature of blur. We present a new architecture composed of spatially adaptive residual learning modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local relationships among the intermediate features and enhances the receptive field. We then incorporate a spatiotemporal recurrent module in the design to also facilitate efficient video deblurring. Our networks can implicitly model the spatially-varying deblurring process, while dispensing with multi-scale processing and large filters entirely. Extensive qualitative and quantitative comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via reduction in model-size and significant improvements in accuracy and speed, enabling almost real-time deblurring.



### Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.10601v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10601v2)
- **Published**: 2019-03-25 21:34:39+00:00
- **Updated**: 2019-08-26 11:56:24+00:00
- **Authors**: Qian Wang, Penghui Bu, Toby P. Breckon
- **Comment**: International Joint Conference on Neural Networks 2019, Budapest
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to transfer knowledge from a source domain to a target domain so that the target domain data can be recognized without any explicit labelling information for this domain. One limitation of the problem setting is that testing data, despite having no labels, from the target domain is needed during training, which prevents the trained model being directly applied to classify unseen test instances. We formulate a new cross-domain classification problem arising from real-world scenarios where labelled data is available for a subset of classes (known classes) in the target domain, and we expect to recognize new samples belonging to any class (known and unseen classes) once the model is learned. This is a generalized zero-shot learning problem where the side information comes from the source domain in the form of labelled samples instead of class-level semantic representations commonly used in traditional zero-shot learning. We present a unified domain adaptation framework for both unsupervised and zero-shot learning conditions. Our approach learns a joint subspace from source and target domains so that the projections of both data in the subspace can be domain invariant and easily separable. We use the supervised locality preserving projection (SLPP) as the enabling technique and conduct experiments under both unsupervised and zero-shot learning conditions, achieving state-of-the-art results on three domain adaptation benchmark datasets: Office-Caltech, Office31 and Office-Home.



### An Approach for Adaptive Automatic Threat Recognition Within 3D Computed Tomography Images for Baggage Security Screening
- **Arxiv ID**: http://arxiv.org/abs/1903.10604v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1903.10604v2)
- **Published**: 2019-03-25 21:46:30+00:00
- **Updated**: 2019-11-18 22:13:49+00:00
- **Authors**: Qian Wang, Khalid N. Ismail, Toby P. Breckon
- **Comment**: Technical Report, Durham University
- **Journal**: None
- **Summary**: The screening of baggage using X-ray scanners is now routine in aviation security with automatic threat detection approaches, based on 3D X-ray computed tomography (CT) images, known as Automatic Threat Recognition (ATR) within the aviation security industry. These current strategies use pre-defined threat material signatures in contrast to adaptability towards new and emerging threat signatures. To address this issue, the concept of adaptive automatic threat recognition (AATR) was proposed in previous work. In this paper, we present a solution to AATR based on such X-ray CT baggage scan imagery. This aims to address the issues of rapidly evolving threat signatures within the screening requirements. Ideally, the detection algorithms deployed within the security scanners should be readily adaptable to different situations with varying requirements of threat characteristics (e.g., threat material, physical properties of objects). We tackle this issue using a novel adaptive machine learning methodology with our solution consisting of a multi-scale 3D CT image segmentation algorithm, a multi-class support vector machine (SVM) classifier for object material recognition and a strategy to enable the adaptability of our approach. Experiments are conducted on both open and sequestered 3D CT baggage image datasets specifically collected for the AATR study. Our proposed approach performs well on both recognition and adaptation. Overall our approach can achieve the probability of detection around 90% with a probability of false alarm below 20%. Our AATR shows the capabilities of adapting to varying types of materials, even the unknown materials which are not available in the training data, adapting to varying required probability of detection and adapting to varying scales of the threat object.



