# Arxiv Papers in cs.CV on 2019-03-15
### Mutual Linear Regression-based Discrete Hashing
- **Arxiv ID**: http://arxiv.org/abs/1904.00744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00744v1)
- **Published**: 2019-03-15 01:13:34+00:00
- **Updated**: 2019-03-15 01:13:34+00:00
- **Authors**: Xingbo Liu, Xiushan Nie, Yilong Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Label information is widely used in hashing methods because of its effectiveness of improving the precision. The existing hashing methods always use two different projections to represent the mutual regression between hash codes and class labels. In contrast to the existing methods, we propose a novel learning-based hashing method termed stable supervised discrete hashing with mutual linear regression (S2DHMLR) in this study, where only one stable projection is used to describe the linear correlation between hash codes and corresponding labels. To the best of our knowledge, this strategy has not been used for hashing previously. In addition, we further use a boosting strategy to improve the final performance of the proposed method without adding extra constraints and with little extra expenditure in terms of time and space. Extensive experiments conducted on three image benchmarks demonstrate the superior performance of the proposed method.



### Pose Graph Optimization for Unsupervised Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1903.06315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06315v1)
- **Published**: 2019-03-15 01:30:34+00:00
- **Updated**: 2019-03-15 01:30:34+00:00
- **Authors**: Yang Li, Yoshitaka Ushiku, Tatsuya Harada
- **Comment**: Accepted to ICRA'2019
- **Journal**: None
- **Summary**: Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.



### Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera for Telepresence Robots
- **Arxiv ID**: http://arxiv.org/abs/1903.06319v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1903.06319v1)
- **Published**: 2019-03-15 01:51:05+00:00
- **Updated**: 2019-03-15 01:51:05+00:00
- **Authors**: Yanmei Dong, Mingtao Pei, Lijia Zhang, Bin Xu, Yuwei Wu, Yunde Jia
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: Many telepresence robots are equipped with a forward-facing camera for video communication and a downward-facing camera for navigation. In this paper, we propose to stitch videos from the FF-camera with a wide-angle lens and the DF-camera with a fisheye lens for telepresence robots. We aim at providing more compact and efficient visual feedback for the user interface of telepresence robots with user-friendly interactive experiences. To this end, we present a multi-homography-based video stitching method which stitches videos from a wide-angle camera and a fisheye camera. The method consists of video image alignment, seam cutting, and image blending. We directly align the wide-angle video image and the fisheye video image based on the multi-homography alignment without calibration, distortion correction, and unwarping procedures. Thus, we can obtain a stitched video with shape preservation in the non-overlapping regions and alignment in the overlapping area for telepresence. To alleviate ghosting effects caused by moving objects and/or moving cameras during telepresence robot driving, an optimal seam is found for aligned video composition, and the optimal seam will be updated in subsequent frames, considering spatial and temporal coherence. The final stitched video is created by image blending based on the optimal seam. We conducted a user study to demonstrate the effectiveness of our method and the superiority of telepresence robots with a stitched video as visual feedback.



### SimulCap : Single-View Human Performance Capture with Cloth Simulation
- **Arxiv ID**: http://arxiv.org/abs/1903.06323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06323v2)
- **Published**: 2019-03-15 02:04:37+00:00
- **Updated**: 2019-03-18 14:09:34+00:00
- **Authors**: Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, Yebin Liu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: This paper proposes a new method for live free-viewpoint human performance capture with dynamic details (e.g., cloth wrinkles) using a single RGBD camera. Our main contributions are: (i) a multi-layer representation of garments and body, and (ii) a physics-based performance capture procedure. We first digitize the performer using multi-layer surface representation, which includes the undressed body surface and separate clothing meshes. For performance capture, we perform skeleton tracking, cloth simulation, and iterative depth fitting sequentially for the incoming frame. By incorporating cloth simulation into the performance capture pipeline, we can simulate plausible cloth dynamics and cloth-body interactions even in the occluded regions, which was not possible in previous capture methods. Moreover, by formulating depth fitting as a physical process, our system produces cloth tracking results consistent with the depth observation while still maintaining physical constraints. Results and evaluations show the effectiveness of our method. Our method also enables new types of applications such as cloth retargeting, free-viewpoint video rendering and animations.



### Unsupervised Person Re-identification by Soft Multilabel Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.06325v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06325v2)
- **Published**: 2019-03-15 02:10:57+00:00
- **Updated**: 2019-04-08 03:45:20+00:00
- **Authors**: Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, Jian-Huang Lai
- **Comment**: CVPR19, oral
- **Journal**: None
- **Summary**: Although unsupervised person re-identification (RE-ID) has drawn increasing research attentions due to its potential to address the scalability problem of supervised RE-ID models, it is very challenging to learn discriminative information in the absence of pairwise labels across disjoint camera views. To overcome this problem, we propose a deep model for the soft multilabel learning for unsupervised RE-ID. The idea is to learn a soft multilabel (real-valued label likelihood vector) for each unlabeled person by comparing (and representing) the unlabeled person with a set of known reference persons from an auxiliary domain. We propose the soft multilabel-guided hard negative mining to learn a discriminative embedding for the unlabeled target domain by exploring the similarity consistency of the visual features and the soft multilabels of unlabeled target pairs. Since most target pairs are cross-view pairs, we develop the cross-view consistent soft multilabel learning to achieve the learning goal that the soft multilabels are consistently good across different camera views. To enable effecient soft multilabel learning, we introduce the reference agent learning to represent each reference person by a reference agent in a joint embedding. We evaluate our unified deep model on Market-1501 and DukeMTMC-reID. Our model outperforms the state-of-the-art unsupervised RE-ID methods by clear margins. Code is available at https://github.com/KovenYu/MAR.



### Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.06332v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06332v2)
- **Published**: 2019-03-15 02:43:44+00:00
- **Updated**: 2019-08-22 14:54:34+00:00
- **Authors**: Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
- **Comment**: ICCV Workshop 2019
- **Journal**: None
- **Summary**: With the popularity of dual cameras in recently released smart phones, a growing number of super-resolution (SR) methods have been proposed to enhance the resolution of stereo image pairs. However, the lack of high-quality stereo datasets has limited the research in this area. To facilitate the training and evaluation of novel stereo SR algorithms, in this paper, we present a large-scale stereo dataset named Flickr1024, which contains 1024 pairs of high-quality images and covers diverse scenarios. We first introduce the data acquisition and processing pipeline, and then compare several popular stereo datasets. Finally, we conduct crossdataset experiments to investigate the potential benefits introduced by our dataset. Experimental results show that, as compared to the KITTI and Middlebury datasets, our Flickr1024 dataset can help to handle the over-fitting problem and significantly improves the performance of stereo SR methods. The Flickr1024 dataset is available online at: https://yingqianwang.github.io/Flickr1024.



### Unsupervised Deep Transfer Feature Learning for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.06342v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06342v2)
- **Published**: 2019-03-15 03:21:14+00:00
- **Updated**: 2019-03-26 03:57:38+00:00
- **Authors**: Euijoon Ahn, Ashnil Kumar, Dagan Feng, Michael Fulham, Jinman Kim
- **Comment**: 4 pages, 1 figure, 3 tables, Accepted (Oral) as IEEE International
  Symposium on Biomedical Imaging 2019
- **Journal**: None
- **Summary**: The accuracy and robustness of image classification with supervised deep learning are dependent on the availability of large-scale, annotated training data. However, there is a paucity of annotated data available due to the complexity of manual annotation. To overcome this problem, a popular approach is to use transferable knowledge across different domains by: 1) using a generic feature extractor that has been pre-trained on large-scale general images (i.e., transfer-learned) but which not suited to capture characteristics from medical images; or 2) fine-tuning generic knowledge with a relatively smaller number of annotated images. Our aim is to reduce the reliance on annotated training data by using a new hierarchical unsupervised feature extractor with a convolutional auto-encoder placed atop of a pre-trained convolutional neural network. Our approach constrains the rich and generic image features from the pre-trained domain to a sophisticated representation of the local image characteristics from the unannotated medical image domain. Our approach has a higher classification accuracy than transfer-learned approaches and is competitive with state-of-the-art supervised fine-tuned methods.



### A Lightweight Optical Flow CNN - Revisiting Data Fidelity and Regularization
- **Arxiv ID**: http://arxiv.org/abs/1903.07414v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07414v3)
- **Published**: 2019-03-15 04:20:58+00:00
- **Updated**: 2020-03-15 02:32:44+00:00
- **Authors**: Tak-Wai Hui, Xiaoou Tang, Chen Change Loy
- **Comment**: Accepted to TPAMI 2020. arXiv admin note: substantial text overlap
  with arXiv:1805.07036
- **Journal**: None
- **Summary**: Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2, the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet, LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.



### Turbo Learning Framework for Human-Object Interactions Recognition and Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.06355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06355v1)
- **Published**: 2019-03-15 04:32:35+00:00
- **Updated**: 2019-03-15 04:32:35+00:00
- **Authors**: Wei Feng, Wentao Liu, Tong Li, Jing Peng, Chen Qian, Xiaolin Hu
- **Comment**: AAAI2019
- **Journal**: None
- **Summary**: Human-object interactions (HOI) recognition and pose estimation are two closely related tasks. Human pose is an essential cue for recognizing actions and localizing the interacted objects. Meanwhile, human action and their interacted objects' localizations provide guidance for pose estimation. In this paper, we propose a turbo learning framework to perform HOI recognition and pose estimation simultaneously. First, two modules are designed to enforce message passing between the tasks, i.e. pose aware HOI recognition module and HOI guided pose estimation module. Then, these two modules form a closed loop to utilize the complementary information iteratively, which can be trained in an end-to-end manner. The proposed method achieves the state-of-the-art performance on two public benchmarks including Verbs in COCO (V-COCO) and HICO-DET datasets.



### Did You Miss the Sign? A False Negative Alarm System for Traffic Sign Detectors
- **Arxiv ID**: http://arxiv.org/abs/1903.06391v1
- **DOI**: 10.1109/IROS40897.2019.8968525
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06391v1)
- **Published**: 2019-03-15 07:34:32+00:00
- **Updated**: 2019-03-15 07:34:32+00:00
- **Authors**: Quazi Marufur Rahman, Niko Sünderhauf, Feras Dayoub
- **Comment**: Submitted to the 2019 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: Object detection is an integral part of an autonomous vehicle for its safety-critical and navigational purposes. Traffic signs as objects play a vital role in guiding such systems. However, if the vehicle fails to locate any critical sign, it might make a catastrophic failure. In this paper, we propose an approach to identify traffic signs that have been mistakenly discarded by the object detector. The proposed method raises an alarm when it discovers a failure by the object detector to detect a traffic sign. This approach can be useful to evaluate the performance of the detector during the deployment phase. We trained a single shot multi-box object detector to detect traffic signs and used its internal features to train a separate false negative detector (FND). During deployment, FND decides whether the traffic sign detector (TSD) has missed a sign or not. We are using precision and recall to measure the accuracy of FND in two different datasets. For 80% recall, FND has achieved 89.9% precision in Belgium Traffic Sign Detection dataset and 90.8% precision in German Traffic Sign Recognition Benchmark dataset respectively. To the best of our knowledge, our method is the first to tackle this critical aspect of false negative detection in robotic vision. Such a fail-safe mechanism for object detection can improve the engagement of robotic vision systems in our daily life.



### DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB Guidance
- **Arxiv ID**: http://arxiv.org/abs/1903.06397v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.06397v4)
- **Published**: 2019-03-15 07:50:18+00:00
- **Updated**: 2019-08-14 07:00:09+00:00
- **Authors**: Yilun Zhang, Ty Nguyen, Ian D. Miller, Shreyas S. Shivakumar, Steven Chen, Camillo J. Taylor, Vijay Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Depth estimation is an important capability for autonomous vehicles to understand and reconstruct 3D environments as well as avoid obstacles during the execution. Accurate depth sensors such as LiDARs are often heavy, expensive and can only provide sparse depth while lighter depth sensors such as stereo cameras are noiser in comparison. We propose an end-to-end learning algorithm that is capable of using sparse, noisy input depth for refinement and depth completion. Our model also produces the camera pose as a byproduct, making it a great solution for autonomous systems. We evaluate our approach on both indoor and outdoor datasets. Empirical results show that our method performs well on the KITTI~\cite{kitti_geiger2012we} dataset when compared to other competing methods, while having superior performance in dealing with sparse, noisy input depth on the TUM~\cite{sturm12iros} dataset.



### Quality-aware Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1903.06399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06399v1)
- **Published**: 2019-03-15 07:59:07+00:00
- **Updated**: 2019-03-15 07:59:07+00:00
- **Authors**: Lei Chen, Le Wu, Zhenzhen Hu, Meng Wang
- **Comment**: IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been widely used for the image-to-image translation task. While these models rely heavily on the labeled image pairs, recently some GAN variants have been proposed to tackle the unpaired image translation task. These models exploited supervision at the domain level with a reconstruction process for unpaired image translation. On the other hand, parallel works have shown that leveraging perceptual loss functions based on high level deep features could enhance the generated image quality. Nevertheless, as these GAN-based models either depended on the pretrained deep network structure or relied on the labeled image pairs, they could not be directly applied to the unpaired image translation task. Moreover, despite the improvement of the introduced perceptual losses from deep neural networks, few researchers have explored the possibility of improving the generated image quality from classical image quality measures. To tackle the above two challenges, in this paper, we propose a unified quality-aware GAN-based framework for unpaired image-to-image translation, where a quality-aware loss is explicitly incorporated by comparing each source image and the reconstructed image at the domain level. Specifically, we design two detailed implementations of the quality loss. The first method is based on a classical image quality assessment measure by defining a classical quality-aware loss. The second method proposes an adaptive deep network based loss. Finally, extensive experimental results on many real-world datasets clearly show the quality improvement of our proposed framework, and the superiority of leveraging classical image quality measures for unpaired image translation compared to the deep network based model.



### BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.06405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.06405v1)
- **Published**: 2019-03-15 08:39:11+00:00
- **Updated**: 2019-03-15 08:39:11+00:00
- **Authors**: Jianru Xue, Jianwu Fang, Tao Li, Bohua Zhang, Pu Zhang, Zhen Ye, Jian Dou
- **Comment**: To appear in ICRA2019
- **Journal**: None
- **Summary**: In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D/2D object detection, stereo vision, semantic/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249,129 3D annotations, 4,902 independent individuals for tracking with the length of overall 214,922 points, 6,004 valid fragments for 5D interactive event recognition, and 4,900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https://github.com/VCCIV/BLVD/.



### DeepHuman: 3D Human Reconstruction from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1903.06473v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06473v2)
- **Published**: 2019-03-15 11:38:15+00:00
- **Updated**: 2019-03-28 05:44:21+00:00
- **Authors**: Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D human reconstruction from a single RGB image. To reduce the ambiguities associated with the surface geometry reconstruction, even for the reconstruction of invisible areas, we propose and leverage a dense semantic representation generated from SMPL model as an additional input. One key feature of our network is that it fuses different scales of image features into the 3D space through volumetric feature transformation, which helps to recover accurate surface geometry. The visible surface details are further refined through a normal refinement network, which can be concatenated with the volume generation network using our proposed volumetric normal projection layer. We also contribute THuman, a 3D real-world human model dataset containing about 7000 models. The network is trained using training data generated from the dataset. Overall, due to the specific design of our network and the diversity in our dataset, our method enables 3D human model estimation given only a single image and outperforms state-of-the-art approaches.



### SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/1903.06482v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06482v2)
- **Published**: 2019-03-15 12:02:44+00:00
- **Updated**: 2019-03-18 16:55:47+00:00
- **Authors**: Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, Andrew J. Davison
- **Comment**: To be published in Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR 2019)
- **Journal**: None
- **Summary**: Systems which incrementally create 3D semantic maps from image sequences must store and update representations of both geometry and semantic entities. However, while there has been much work on the correct formulation for geometrical estimation, state-of-the-art systems usually rely on simple semantic representations which store and update independent label estimates for each surface element (depth pixels, surfels, or voxels). Spatial correlation is discarded, and fused label maps are incoherent and noisy.   We introduce a new compact and optimisable semantic representation by training a variational auto-encoder that is conditioned on a colour image. Using this learned latent space, we can tackle semantic label fusion by jointly optimising the low-dimenional codes associated with each of a set of overlapping images, producing consistent fused label maps which preserve spatial correlation. We also show how this approach can be used within a monocular keyframe based semantic mapping system where a similar code approach is used for geometry. The probabilistic formulation allows a flexible formulation where we can jointly estimate motion, geometry and semantics in a unified optimisation.



### Projectron -- A Shallow and Interpretable Network for Classifying Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1904.00740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00740v1)
- **Published**: 2019-03-15 12:13:23+00:00
- **Updated**: 2019-03-15 12:13:23+00:00
- **Authors**: Aditya Sriram, Shivam Kalra, H. R. Tizhoosh
- **Comment**: Accepted for publication in the 2019 International Joint Conference
  on Neural Networks (IJCNN), Budapest, Hungary
- **Journal**: None
- **Summary**: This paper introduces the `Projectron' as a new neural network architecture that uses Radon projections to both classify and represent medical images. The motivation is to build shallow networks which are more interpretable in the medical imaging domain. Radon transform is an established technique that can reconstruct images from parallel projections. The Projectron first applies global Radon transform to each image using equidistant angles and then feeds these transformations for encoding to a single layer of neurons followed by a layer of suitable kernels to facilitate a linear separation of projections. Finally, the Projectron provides the output of the encoding as an input to two more layers for final classification. We validate the Projectron on five publicly available datasets, a general dataset (namely MNIST) and four medical datasets (namely Emphysema, IDC, IRMA, and Pneumonia). The results are encouraging as we compared the Projectron's performance against MLPs with raw images and Radon projections as inputs, respectively. Experiments clearly demonstrate the potential of the proposed Projectron for representing/classifying medical images.



### MFAS: Multimodal Fusion Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1903.06496v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.06496v1)
- **Published**: 2019-03-15 12:45:13+00:00
- **Updated**: 2019-03-15 12:45:13+00:00
- **Authors**: Juan-Manuel Pérez-Rúa, Valentin Vielzeuf, Stéphane Pateux, Moez Baccouche, Frédéric Jurie
- **Comment**: CVPR 2019, Jun 2019, Long Beach, United States
  http://cvpr2019.thecvf.com/
- **Journal**: None
- **Summary**: We tackle the problem of finding good architectures for multimodal classification problems. We propose a novel and generic search space that spans a large number of possible fusion architectures. In order to find an optimal architecture for a given dataset in the proposed search space, we leverage an efficient sequential model-based exploration approach that is tailored for the problem. We demonstrate the value of posing multimodal fusion as a neural architecture search problem by extensive experimentation on a toy dataset and two other real multimodal datasets. We discover fusion architectures that exhibit state-of-the-art performance for problems with different domain and dataset size, including the NTU RGB+D dataset, the largest multi-modal action recognition dataset available.



### Phenotypic Profiling of High Throughput Imaging Screens with Generic Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1903.06516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06516v1)
- **Published**: 2019-03-15 13:11:03+00:00
- **Updated**: 2019-03-15 13:11:03+00:00
- **Authors**: Philip T. Jackson, Yinhai Wang, Sinead Knight, Hongming Chen, Thierry Dorval, Martin Brown, Claus Bendtsen, Boguslaw Obara
- **Comment**: None
- **Journal**: None
- **Summary**: While deep learning has seen many recent applications to drug discovery, most have focused on predicting activity or toxicity directly from chemical structure. Phenotypic changes exhibited in cellular images are also indications of the mechanism of action (MoA) of chemical compounds. In this paper, we show how pre-trained convolutional image features can be used to assist scientists in discovering interesting chemical clusters for further investigation. Our method reduces the dimensionality of raw fluorescent stained images from a high throughput imaging (HTI) screen, producing an embedding space that groups together images with similar cellular phenotypes. Running standard unsupervised clustering on this embedding space yields a set of distinct phenotypic clusters. This allows scientists to further select and focus on interesting clusters for downstream analyses. We validate the consistency of our embedding space qualitatively with t-sne visualizations, and quantitatively by measuring embedding variance among images that are known to be similar. Results suggested the usefulness of our proposed workflow using deep learning and clustering and it can lead to robust HTI screening and compound triage.



### GolfDB: A Video Database for Golf Swing Sequencing
- **Arxiv ID**: http://arxiv.org/abs/1903.06528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06528v1)
- **Published**: 2019-03-15 13:20:16+00:00
- **Updated**: 2019-03-15 13:20:16+00:00
- **Authors**: William McNally, Kanav Vats, Tyler Pinto, Chris Dulhanty, John McPhee, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: The golf swing is a complex movement requiring considerable full-body coordination to execute proficiently. As such, it is the subject of frequent scrutiny and extensive biomechanical analyses. In this paper, we introduce the notion of golf swing sequencing for detecting key events in the golf swing and facilitating golf swing analysis. To enable consistent evaluation of golf swing sequencing performance, we also introduce the benchmark database GolfDB, consisting of 1400 high-quality golf swing videos, each labeled with event frames, bounding box, player name and sex, club type, and view type. Furthermore, to act as a reference baseline for evaluating golf swing sequencing performance on GolfDB, we propose a lightweight deep neural network called SwingNet, which possesses a hybrid deep convolutional and recurrent neural network architecture. SwingNet correctly detects eight golf swing events at an average rate of 76.1%, and six out of eight events at a rate of 91.8%. In line with the proposed baseline SwingNet, we advocate the use of computationally efficient models in future research to promote in-the-field analysis via deployment on readily-available mobile devices.



### Multi-label Cloud Segmentation Using a Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1903.06562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06562v1)
- **Published**: 2019-03-15 14:09:49+00:00
- **Updated**: 2019-03-15 14:09:49+00:00
- **Authors**: Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler
- **Comment**: None
- **Journal**: Published in Proc. IEEE AP-S Symposium on Antennas and Propagation
  and USNC-URSI Radio Science Meeting, 2019
- **Summary**: Different empirical models have been developed for cloud detection. There is a growing interest in using the ground-based sky/cloud images for this purpose. Several methods exist that perform binary segmentation of clouds. In this paper, we propose to use a deep learning architecture (U-Net) to perform multi-label sky/cloud image segmentation. The proposed approach outperforms recent literature by a large margin.



### Inserting Videos into Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.06571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06571v1)
- **Published**: 2019-03-15 14:30:22+00:00
- **Updated**: 2019-03-15 14:30:22+00:00
- **Authors**: Donghoon Lee, Tomas Pfister, Ming-Hsuan Yang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we introduce a new problem of manipulating a given video by inserting other videos into it. Our main task is, given an object video and a scene video, to insert the object video at a user-specified location in the scene video so that the resulting video looks realistic. We aim to handle different object motions and complex backgrounds without expensive segmentation annotations. As it is difficult to collect training pairs for this problem, we synthesize fake training pairs that can provide helpful supervisory signals when training a neural network with unpaired real data. The proposed network architecture can take both real and fake pairs as input and perform both supervised and unsupervised training in an adversarial learning scheme. To synthesize a realistic video, the network renders each frame based on the current input and previous frames. Within this framework, we observe that injecting noise into previous frames while generating the current frame stabilizes training. We conduct experiments on real-world videos in object tracking and person re-identification benchmark datasets. Experimental results demonstrate that the proposed algorithm is able to synthesize long sequences of realistic videos with a given object video inserted.



### Selective Kernel Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.06586v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06586v2)
- **Published**: 2019-03-15 15:04:22+00:00
- **Updated**: 2019-03-18 03:08:24+00:00
- **Authors**: Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus/SKNet.



### PifPaf: Composite Fields for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.06593v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06593v2)
- **Published**: 2019-03-15 15:14:42+00:00
- **Updated**: 2019-04-05 11:46:33+00:00
- **Authors**: Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We propose a new bottom-up method for multi-person 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, single-shot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modified COCO keypoint task for the transportation domain.



### Live Reconstruction of Large-Scale Dynamic Outdoor Worlds
- **Arxiv ID**: http://arxiv.org/abs/1903.06708v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06708v2)
- **Published**: 2019-03-15 15:50:59+00:00
- **Updated**: 2019-04-15 18:40:16+00:00
- **Authors**: Ondrej Miksik, Vibhav Vineet
- **Comment**: CVPR 2019 workshop on dynamic scene reconstruction
- **Journal**: None
- **Summary**: Standard 3D reconstruction pipelines assume stationary world, therefore suffer from `ghost artifacts' whenever dynamic objects are present in the scene. Recent approaches has started tackling this issue, however, they typically either only discard dynamic information, represent it using bounding boxes or per-frame depth or rely on approaches that are inherently slow and not suitable to online settings.   We propose an end-to-end system for live reconstruction of large-scale outdoor dynamic environments. We leverage recent advances in computationally efficient data-driven approaches for 6-DoF object pose estimation to segment the scene into objects and stationary `background'. This allows us to represent the scene using a time-dependent (dynamic) map, in which each object is explicitly represented as a separate instance and reconstructed in its own volume. For each time step, our dynamic map maintains a relative pose of each volume with respect to the stationary background. Our system operates in incremental manner which is essential for on-line reconstruction, handles large-scale environments with objects at large distances and runs in (near) real-time. We demonstrate the efficacy of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense 3D reconstructions of a number of dynamic scenes.



### SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.07663v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.07663v1)
- **Published**: 2019-03-15 16:00:23+00:00
- **Updated**: 2019-03-15 16:00:23+00:00
- **Authors**: Tianchen Wang, Jinjun Xiong, Xiaowei Xu, Yiyu Shi
- **Comment**: AAAI19
- **Journal**: None
- **Summary**: Various convolutional neural networks (CNNs) were developed recently that achieved accuracy comparable with that of human beings in computer vision tasks such as image recognition, object detection and tracking, etc. Most of these networks, however, process one single frame of image at a time, and may not fully utilize the temporal and contextual correlation typically present in multiple channels of the same image or adjacent frames from a video, thus limiting the achievable throughput. This limitation stems from the fact that existing CNNs operate on deterministic numbers. In this paper, we propose a novel statistical convolutional neural network (SCNN), which extends existing CNN architectures but operates directly on correlated distributions rather than deterministic numbers. By introducing a parameterized canonical model to model correlated data and defining corresponding operations as required for CNN training and inference, we show that SCNN can process multiple frames of correlated images effectively, hence achieving significant speedup over existing CNN models. We use a CNN based video object detection as an example to illustrate the usefulness of the proposed SCNN as a general network model. Experimental results show that even a non-optimized implementation of SCNN can still achieve 178% speedup over existing CNNs with slight accuracy degradation.



### Adversarial Networks for Camera Pose Regression and Refinement
- **Arxiv ID**: http://arxiv.org/abs/1903.06646v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06646v3)
- **Published**: 2019-03-15 16:32:51+00:00
- **Updated**: 2019-10-27 21:17:06+00:00
- **Authors**: Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, Shadi Albarqouni
- **Comment**: None
- **Journal**: None
- **Summary**: Despite recent advances on the topic of direct camera pose regression using neural networks, accurately estimating the camera pose of a single RGB image still remains a challenging task. To address this problem, we introduce a novel framework based, in its core, on the idea of implicitly learning the joint distribution of RGB images and their corresponding camera poses using a discriminator network and adversarial learning. Our method allows not only to regress the camera pose from a single image, however, also offers a solely RGB-based solution for camera pose refinement using the discriminator network. Further, we show that our method can effectively be used to optimize the predicted camera poses and thus improve the localization accuracy. To this end, we validate our proposed method on the publicly available 7-Scenes dataset improving upon the results of direct camera pose regression methods.



### Crowd Counting with Decomposed Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1903.07427v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07427v3)
- **Published**: 2019-03-15 16:53:50+00:00
- **Updated**: 2020-04-22 05:02:35+00:00
- **Authors**: Min-hwan Oh, Peder A. Olsen, Karthikeyan Natesan Ramamurthy
- **Comment**: Accepted in AAAI 2020 (Main Technical Track)
- **Journal**: None
- **Summary**: Research in neural networks in the field of computer vision has achieved remarkable accuracy for point estimation. However, the uncertainty in the estimation is rarely addressed. Uncertainty quantification accompanied by point estimation can lead to a more informed decision, and even improve the prediction quality. In this work, we focus on uncertainty estimation in the domain of crowd counting. With increasing occurrences of heavily crowded events such as political rallies, protests, concerts, etc., automated crowd analysis is becoming an increasingly crucial task. The stakes can be very high in many of these real-world applications. We propose a scalable neural network framework with quantification of decomposed uncertainty using a bootstrap ensemble. We demonstrate that the proposed uncertainty quantification method provides additional insight to the crowd counting problem and is simple to implement. We also show that our proposed method exhibits the state of the art performances in many benchmark crowd counting datasets.



### Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset
- **Arxiv ID**: http://arxiv.org/abs/1903.06754v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06754v2)
- **Published**: 2019-03-15 18:55:07+00:00
- **Updated**: 2019-09-07 20:17:17+00:00
- **Authors**: Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl S. Muller, Jake A. Whritner, Luxin Zhang, Mary M. Hayhoe, Dana H. Ballard
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115\% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.



### Through-Wall Pose Imaging in Real-Time with a Many-to-Many Encoder/Decoder Paradigm
- **Arxiv ID**: http://arxiv.org/abs/1904.00739v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00739v2)
- **Published**: 2019-03-15 19:05:05+00:00
- **Updated**: 2019-10-20 05:52:38+00:00
- **Authors**: Kevin Meng, Yu Meng
- **Comment**: None
- **Journal**: None
- **Summary**: Overcoming the visual barrier and developing "see-through vision" has been one of mankind's long-standing dreams. Unlike visible light, Radio Frequency (RF) signals penetrate opaque obstructions and reflect highly off humans. This paper establishes a deep-learning model that can be trained to reconstruct continuous video of a 15-point human skeleton even through visual occlusion. The training process adopts a student/teacher learning procedure inspired by the Feynman learning technique, in which video frames and RF data are first collected simultaneously using a co-located setup containing an optical camera and an RF antenna array transceiver. Next, the video frames are processed with a computer-vision-based gait analysis "teacher" module to generate ground-truth human skeletons for each frame. Then, the same type of skeleton is predicted from corresponding RF data using a "student" deep-learning model consisting of a Residual Convolutional Neural Network (CNN), Region Proposal Network (RPN), and Recurrent Neural Network with Long-Short Term Memory (LSTM) that 1) extracts spatial features from RF images, 2) detects all people present in a scene, and 3) aggregates information over many time-steps, respectively. The model is shown to both accurately and completely predict the pose of humans behind visual obstruction solely using RF signals. Primary academic contributions include the novel many-to-many imaging methodology, unique integration of RPN and LSTM networks, and original training pipeline.



### Smart, Deep Copy-Paste
- **Arxiv ID**: http://arxiv.org/abs/1903.06763v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.06763v1)
- **Published**: 2019-03-15 19:07:34+00:00
- **Updated**: 2019-03-15 19:07:34+00:00
- **Authors**: Tiziano Portenier, Qiyang Hu, Paolo Favaro, Matthias Zwicker
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: In this work, we propose a novel system for smart copy-paste, enabling the synthesis of high-quality results given a masked source image content and a target image context as input. Our system naturally resolves both shading and geometric inconsistencies between source and target image, resulting in a merged result image that features the content from the pasted source image, seamlessly pasted into the target context. Our framework is based on a novel training image transformation procedure that allows to train a deep convolutional neural network end-to-end to automatically learn a representation that is suitable for copy-pasting. Our training procedure works with any image dataset without additional information such as labels, and we demonstrate the effectiveness of our system on two popular datasets, high-resolution face images and the more complex Cityscapes dataset. Our technique outperforms the current state of the art on face images, and we show promising results on the Cityscapes dataset, demonstrating that our system generalizes to much higher resolution than the training data.



### Zero Shot Learning with the Isoperimetric Loss
- **Arxiv ID**: http://arxiv.org/abs/1903.06781v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06781v2)
- **Published**: 2019-03-15 19:55:38+00:00
- **Updated**: 2019-12-03 19:17:12+00:00
- **Authors**: Shay Deutsch, Andrea Bertozzi, Stefano Soatto
- **Comment**: Accepted to AAAI-20
- **Journal**: None
- **Summary**: We introduce the isoperimetric loss as a regularization criterion for learning the map from a visual representation to a semantic embedding, to be used to transfer knowledge to unknown classes in a zero-shot learning setting. We use a pre-trained deep neural network model as a visual representation of image data, a Word2Vec embedding of class labels, and linear maps between the visual and semantic embedding spaces. However, the spaces themselves are not linear, and we postulate the sample embedding to be populated by noisy samples near otherwise smooth manifolds. We exploit the graph structure defined by the sample points to regularize the estimates of the manifolds by inferring the graph connectivity using a generalization of the isoperimetric inequalities from Riemannian geometry to graphs. Surprisingly, this regularization alone, paired with the simplest baseline model, outperforms the state-of-the-art among fully automated methods in zero-shot learning benchmarks such as AwA and CUB. This improvement is achieved solely by learning the structure of the underlying spaces by imposing regularity.



### Calibration of Asynchronous Camera Networks: CALICO
- **Arxiv ID**: http://arxiv.org/abs/1903.06811v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06811v2)
- **Published**: 2019-03-15 21:35:13+00:00
- **Updated**: 2019-11-14 19:13:24+00:00
- **Authors**: Amy Tabb, Henry Medeiros, Mitchell J. Feldmann, Thiago T. Santos
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Camera network and multi-camera calibration for external parameters is a necessary step for a variety of contexts in computer vision and robotics, ranging from three-dimensional reconstruction to human activity tracking. This paper describes CALICO, a method for camera network and/or multi-camera calibration suitable for challenging contexts: the cameras may not share a common field of view and the network may be asynchronous. The calibration object required is one or more rigidly attached planar calibration patterns, which are distinguishable from one another, such as aruco or charuco patterns.   We formulate the camera network and/or multi-camera calibration problem using rigidity constraints, represented as a system of equations, and an approximate solution is found through a two-step process. Simulated and real experiments, including an asynchronous camera network, multicamera system, and rotating imaging system, demonstrate the method in a variety of settings. Median reconstruction accuracy error was less than $0.41$ mm$^2$ for all datasets. This method is suitable for novice users to calibrate a camera network, and the modularity of the calibration object also allows for disassembly, shipping, and the use of this method in a variety of large and small spaces.



### Generate What You Can't See - a View-dependent Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.06814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.06814v1)
- **Published**: 2019-03-15 21:50:54+00:00
- **Updated**: 2019-03-15 21:50:54+00:00
- **Authors**: Karol Piaskowski, Rafal Staszak, Dominik Belter
- **Comment**: Submitted to IROS 2019. Copyright 2019 IEEE. Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses. Supplementary video: https://youtu.be/gCAoJ7BM5F0
- **Journal**: None
- **Summary**: In order to operate autonomously, a robot should explore the environment and build a model of each of the surrounding objects. A common approach is to carefully scan the whole workspace. This is time-consuming. It is also often impossible to reach all the viewpoints required to acquire full knowledge about the environment. Humans can perform shape completion of occluded objects by relying on past experience. Therefore, we propose a method that generates images of an object from various viewpoints using a single input RGB image. A deep neural network is trained to imagine the object appearance from many viewpoints. We present the whole pipeline, which takes a single RGB image as input and returns a sequence of RGB and depth images of the object. The method utilizes a CNN-based object detector to extract the object from the natural scene. Then, the proposed network generates a set of RGB and depth images. We show the results both on a synthetic dataset and on real images.



### Detecting GAN generated Fake Images using Co-occurrence Matrices
- **Arxiv ID**: http://arxiv.org/abs/1903.06836v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.06836v2)
- **Published**: 2019-03-15 23:24:08+00:00
- **Updated**: 2019-10-03 00:54:21+00:00
- **Authors**: Lakshmanan Nataraj, Tajuddin Manhar Mohammed, Shivkumar Chandrasekaran, Arjuna Flenner, Jawadul H. Bappy, Amit K. Roy-Chowdhury, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: The advent of Generative Adversarial Networks (GANs) has brought about completely novel ways of transforming and manipulating pixels in digital images. GAN based techniques such as Image-to-Image translations, DeepFakes, and other automated methods have become increasingly popular in creating fake images. In this paper, we propose a novel approach to detect GAN generated fake images using a combination of co-occurrence matrices and deep learning. We extract co-occurrence matrices on three color channels in the pixel domain and train a model using a deep convolutional neural network (CNN) framework. Experimental results on two diverse and challenging GAN datasets comprising more than 56,000 images based on unpaired image-to-image translations (cycleGAN [1]) and facial attributes/expressions (StarGAN [2]) show that our approach is promising and achieves more than 99% classification accuracy in both datasets. Further, our approach also generalizes well and achieves good results when trained on one dataset and tested on the other.



### Visual recognition in the wild by sampling deep similarity functions
- **Arxiv ID**: http://arxiv.org/abs/1903.06837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06837v1)
- **Published**: 2019-03-15 23:26:13+00:00
- **Updated**: 2019-03-15 23:26:13+00:00
- **Authors**: Mikhail Usvyatsov, Konrad Schindler
- **Comment**: None
- **Journal**: None
- **Summary**: Recognising relevant objects or object states in its environment is a basic capability for an autonomous robot. The dominant approach to object recognition in images and range images is classification by supervised machine learning, nowadays mostly with deep convolutional neural networks (CNNs). This works well for target classes whose variability can be completely covered with training examples. However, a robot moving in the wild, i.e., in an environment that is not known at the time the recognition system is trained, will often face \emph{domain shift}: the training data cannot be assumed to exhaustively cover all the within-class variability that will be encountered in the test data. In that situation, learning is in principle possible, since the training set does capture the defining properties, respectively dissimilarities, of the target classes. But directly training a CNN to predict class probabilities is prone to overfitting to irrelevant correlations between the class labels and the specific subset of the target class that is represented in the training set. We explore the idea to instead learn a Siamese CNN that acts as similarity function between pairs of training examples. Class predictions are then obtained by measuring the similarities between a new test instance and the training samples. We show that the CNN embedding correctly recovers the relative similarities to arbitrary class exemplars in the training set. And that therefore few, randomly picked training exemplars are sufficient to achieve good predictions, making the procedure efficient.



