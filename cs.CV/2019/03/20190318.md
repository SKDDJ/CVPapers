# Arxiv Papers in cs.CV on 2019-03-18
### AttoNets: Compact and Efficient Deep Neural Networks for the Edge via Human-Machine Collaborative Design
- **Arxiv ID**: http://arxiv.org/abs/1903.07209v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.07209v2)
- **Published**: 2019-03-18 00:16:04+00:00
- **Updated**: 2019-04-15 16:05:35+00:00
- **Authors**: Alexander Wong, Zhong Qiu Lin, Brendan Chwyl
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: While deep neural networks have achieved state-of-the-art performance across a large number of complex tasks, it remains a big challenge to deploy such networks for practical, on-device edge scenarios such as on mobile devices, consumer devices, drones, and vehicles. In this study, we take a deeper exploration into a human-machine collaborative design approach for creating highly efficient deep neural networks through a synergy between principled network design prototyping and machine-driven design exploration. The efficacy of human-machine collaborative design is demonstrated through the creation of AttoNets, a family of highly efficient deep neural networks for on-device edge deep learning. Each AttoNet possesses a human-specified network-level macro-architecture comprising of custom modules with unique machine-designed module-level macro-architecture and micro-architecture designs, all driven by human-specified design requirements. Experimental results for the task of object recognition showed that the AttoNets created via human-machine collaborative design has significantly fewer parameters and computational costs than state-of-the-art networks designed for efficiency while achieving noticeably higher accuracy (with the smallest AttoNet achieving ~1.8% higher accuracy while requiring ~10x fewer multiply-add operations and parameters than MobileNet-V1). Furthermore, the efficacy of the AttoNets is demonstrated for the task of instance-level object segmentation and object detection, where an AttoNet-based Mask R-CNN network was constructed with significantly fewer parameters and computational costs (~5x fewer multiply-add operations and ~2x fewer parameters) than a ResNet-50 based Mask R-CNN network.



### Multidimensional ground reaction forces and moments from wearable sensor accelerations via deep learning
- **Arxiv ID**: http://arxiv.org/abs/1903.07221v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07221v3)
- **Published**: 2019-03-18 01:24:08+00:00
- **Updated**: 2020-07-03 22:57:54+00:00
- **Authors**: William R. Johnson, Ajmal Mian, Mark A. Robinson, Jasper Verheul, David G. Lloyd, Jacqueline A. Alderson
- **Comment**: None
- **Journal**: None
- **Summary**: Monitoring athlete internal workload exposure, including prevention of catastrophic non-contact knee injuries, relies on the existence of a custom early-warning detection system. This system must be able to estimate accurate, reliable, and valid musculoskeletal joint loads, for sporting maneuvers in near real-time and during match play. However, current methods are constrained to laboratory instrumentation, are labor and cost intensive, and require highly trained specialist knowledge, thereby limiting their ecological validity and wider deployment. An informative next step towards this goal would be a new method to obtain ground kinetics in the field. Here we show that kinematic data obtained from wearable sensor accelerometers, in lieu of embedded force platforms, can leverage recent supervised learning techniques to predict near real-time multidimensional ground reaction forces and moments (GRF/M). Competing convolutional neural network (CNN) deep learning models were trained using laboratory-derived stance phase GRF/M data and simulated sensor accelerations for running and sidestepping maneuvers derived from nearly half a million legacy motion trials. Then, predictions were made from each model driven by five sensor accelerations recorded during independent inter-laboratory data capture sessions. The proposed deep learning workbench achieved correlations to ground truth, by maximum discrete GRF component, of vertical Fz 0.97, anterior Fy 0.96 (both running), and lateral Fx 0.87 (sidestepping), with the strongest mean recorded across GRF components 0.89, and for GRM 0.65 (both sidestepping). These best-case correlations indicate the plausibility of the approach although the range of results was disappointing. The goal to accurately estimate near real-time on-field GRF/M will be improved by the lessons learned in this study [truncated].



### An End-to-End Joint Unsupervised Learning of Deep Model and Pseudo-Classes for Remote Sensing Scene Representation
- **Arxiv ID**: http://arxiv.org/abs/1903.07224v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07224v1)
- **Published**: 2019-03-18 01:41:33+00:00
- **Updated**: 2019-03-18 01:41:33+00:00
- **Authors**: Zhiqiang Gong, Ping Zhong, Weidong Hu, Fang Liu, Bingwei Hui
- **Comment**: Submitted to IJCNN 2019
- **Journal**: None
- **Summary**: This work develops a novel end-to-end deep unsupervised learning method based on convolutional neural network (CNN) with pseudo-classes for remote sensing scene representation. First, we introduce center points as the centers of the pseudo classes and the training samples can be allocated with pseudo labels based on the center points. Therefore, the CNN model, which is used to extract features from the scenes, can be trained supervised with the pseudo labels. Moreover, a pseudo-center loss is developed to decrease the variance between the samples and the corresponding pseudo center point. The pseudo-center loss is important since it can update both the center points with the training samples and the CNN model with the center points in the training process simultaneously. Finally, joint learning of the pseudo-center loss and the pseudo softmax loss which is formulated with the samples and the pseudo labels is developed for unsupervised remote sensing scene representation to obtain discriminative representations from the scenes. Experiments are conducted over two commonly used remote sensing scene datasets to validate the effectiveness of the proposed method and the experimental results show the superiority of the proposed method when compared with other state-of-the-art methods.



### Complex Scene Classification of PolSAR Imagery based on a Self-paced Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1903.07243v1
- **DOI**: 10.1109/JSTARS.2018.2879440
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.07243v1)
- **Published**: 2019-03-18 03:31:02+00:00
- **Updated**: 2019-03-18 03:31:02+00:00
- **Authors**: Wenshuai Chen, Shuiping Gou, Xinlin Wang, Licheng Jiao, Changzhe Jiao, Alina Zare
- **Comment**: None
- **Journal**: None
- **Summary**: Existing polarimetric synthetic aperture radar (PolSAR) image classification methods cannot achieve satisfactory performance on complex scenes characterized by several types of land cover with significant levels of noise or similar scattering properties across land cover types. Hence, we propose a supervised classification method aimed at constructing a classifier based on self-paced learning (SPL). SPL has been demonstrated to be effective at dealing with complex data while providing classifier. In this paper, a novel Support Vector Machine (SVM) algorithm based on SPL with neighborhood constraints (SVM_SPLNC) is proposed. The proposed method leverages the easiest samples first to obtain an initial parameter vector. Then, more complex samples are gradually incorporated to update the parameter vector iteratively. Moreover, neighborhood constraints are introduced during the training process to further improve performance. Experimental results on three real PolSAR images show that the proposed method performs well on complex scenes.



### QATM: Quality-Aware Template Matching For Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.07254v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07254v2)
- **Published**: 2019-03-18 04:58:51+00:00
- **Updated**: 2019-04-09 23:11:15+00:00
- **Authors**: Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan
- **Comment**: Accepted as CVPR 2019 paper. Camera ready version
- **Journal**: None
- **Summary**: Finding a template in a search image is one of the core problems many computer vision, such as semantic image semantic, image-to-GPS verification \etc. We propose a novel quality-aware template matching method, QATM, which is not only used as a standalone template matching algorithm, but also a trainable layer that can be easily embedded into any deep neural network. Specifically, we assess the quality of a matching pair using soft-ranking among all matching pairs, and thus different matching scenarios such as 1-to-1, 1-to-many, and many-to-many will be all reflected to different values. Our extensive evaluation on classic template matching benchmarks and deep learning tasks demonstrate the effectiveness of QATM. It not only outperforms state-of-the-art template matching methods when used alone, but also largely improves existing deep network solutions.



### Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.07256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07256v1)
- **Published**: 2019-03-18 05:07:09+00:00
- **Updated**: 2019-03-18 05:07:09+00:00
- **Authors**: Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, Ge Li
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Video anomaly detection under weak labels is formulated as a typical multiple-instance learning problem in previous works. In this paper, we provide a new perspective, i.e., a supervised learning task under noisy labels. In such a viewpoint, as long as cleaning away label noise, we can directly apply fully supervised action classifiers to weakly supervised anomaly detection, and take maximum advantage of these well-developed classifiers. For this purpose, we devise a graph convolutional network to correct noisy labels. Based upon feature similarity and temporal consistency, our network propagates supervisory signals from high-confidence snippets to low-confidence ones. In this manner, the network is capable of providing cleaned supervision for action classifiers. During the test phase, we only need to obtain snippet-wise predictions from the action classifier without any extra post-processing. Extensive experiments on 3 datasets at different scales with 2 types of action classifiers demonstrate the efficacy of our method. Remarkably, we obtain the frame-level AUC score of 82.12% on UCF-Crime.



### Semantic Image Synthesis with Spatially-Adaptive Normalization
- **Arxiv ID**: http://arxiv.org/abs/1903.07291v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.LG, I.5; I.5.4; I.3.3
- **Links**: [PDF](http://arxiv.org/pdf/1903.07291v2)
- **Published**: 2019-03-18 08:12:23+00:00
- **Updated**: 2019-11-05 15:41:27+00:00
- **Authors**: Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu
- **Comment**: Accepted as a CVPR 2019 oral paper
- **Journal**: CVPR 2019
- **Summary**: We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .



### Appearance-Based Gaze Estimation Using Dilated-Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1903.07296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07296v1)
- **Published**: 2019-03-18 08:29:32+00:00
- **Updated**: 2019-03-18 08:29:32+00:00
- **Authors**: Zhaokang Chen, Bertram E. Shi
- **Comment**: 16 pages, 7 figures. To appear in ACCV2018
- **Journal**: None
- **Summary**: Appearance-based gaze estimation has attracted more and more attention because of its wide range of applications. The use of deep convolutional neural networks has improved the accuracy significantly. In order to improve the estimation accuracy further, we focus on extracting better features from eye images. Relatively large changes in gaze angles may result in relatively small changes in eye appearance. We argue that current architectures for gaze estimation may not be able to capture such small changes, as they apply multiple pooling layers or other downsampling layers so that the spatial resolution of the high-level layers is reduced significantly. To evaluate whether the use of features extracted at high resolution can benefit gaze estimation, we adopt dilated-convolutions to extract high-level features without reducing spatial resolution. In cross-subject experiments on the Columbia Gaze dataset for eye contact detection and the MPIIGaze dataset for 3D gaze vector regression, the resulting Dilated-Nets achieve significant (up to 20.8%) gains when compared to similar networks without dilated-convolutions. Our proposed Dilated-Net achieves state-of-the-art results on both the Columbia Gaze and the MPIIGaze datasets.



### Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.07309v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07309v3)
- **Published**: 2019-03-18 08:53:48+00:00
- **Updated**: 2019-06-19 15:58:41+00:00
- **Authors**: Alex Wong, Byung-Woo Hong, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning methods to infer (hypothesize) depth of a scene from a single image require costly per-pixel ground-truth. We follow a geometric approach that exploits abundant stereo imagery to learn a model to hypothesize scene structure without direct supervision. Although we train a network with stereo pairs, we only require a single image at test time to hypothesize disparity or depth. We propose a novel objective function that exploits the bilateral cyclic relationship between the left and right disparities and we introduce an adaptive regularization scheme that allows the network to handle both the co-visible and occluded regions in a stereo pair. This process ultimately produces a model to generate hypotheses for the 3-dimensional structure of the scene as viewed in a single image. When used to generate a single (most probable) estimate of depth, our method outperforms state-of-the-art unsupervised monocular depth prediction methods on the KITTI benchmarks. We show that our method generalizes well by applying our models trained on KITTI to the Make3d dataset.



### IvaNet: Learning to jointly detect and segment objets with the help of Local Top-Down Modules
- **Arxiv ID**: http://arxiv.org/abs/1903.07360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07360v1)
- **Published**: 2019-03-18 10:58:54+00:00
- **Updated**: 2019-03-18 10:58:54+00:00
- **Authors**: Shihua Huang, Lu Wang
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Driven by Convolutional Neural Networks, object detection and semantic segmentation have gained significant improvements. However, existing methods on the basis of a full top-down module have limited robustness in handling those two tasks simultaneously. To this end, we present a joint multi-task framework, termed IvaNet. Different from existing methods, our IvaNet backwards abstract semantic information from higher layers to augment lower layers using local top-down modules. The comparisons against some counterparts on the PASCAL VOC and MS COCO datasets demonstrate the functionality of IvaNet.



### Self-calibrating Deep Photometric Stereo Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.07366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07366v1)
- **Published**: 2019-03-18 11:16:29+00:00
- **Updated**: 2019-03-18 11:16:29+00:00
- **Authors**: Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K. Wong
- **Comment**: CVPR 2019 Oral, Project Page: http://guanyingc.github.io/SDPS-Net/
- **Journal**: None
- **Summary**: This paper proposes an uncalibrated photometric stereo method for non-Lambertian scenes based on deep learning. Unlike previous approaches that heavily rely on assumptions of specific reflectances and light source distributions, our method is able to determine both shape and light directions of a scene with unknown arbitrary reflectances observed under unknown varying light directions. To achieve this goal, we propose a two-stage deep learning architecture, called SDPS-Net, which can effectively take advantage of intermediate supervision, resulting in reduced learning difficulty compared to a single-stage model. Experiments on both synthetic and real datasets show that our proposed approach significantly outperforms previous uncalibrated photometric stereo methods.



### Fashion Outfit Generation for E-commerce
- **Arxiv ID**: http://arxiv.org/abs/1904.00741v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00741v1)
- **Published**: 2019-03-18 11:19:34+00:00
- **Updated**: 2019-03-18 11:19:34+00:00
- **Authors**: Elaine M. Bettaney, Stephen R. Hardwick, Odysseas Zisimopoulos, Benjamin Paul Chamberlain
- **Comment**: 9 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Combining items of clothing into an outfit is a major task in fashion retail. Recommending sets of items that are compatible with a particular seed item is useful for providing users with guidance and inspiration, but is currently a manual process that requires expert stylists and is therefore not scalable or easy to personalise. We use a multilayer neural network fed by visual and textual features to learn embeddings of items in a latent style space such that compatible items of different types are embedded close to one another. We train our model using the ASOS outfits dataset, which consists of a large number of outfits created by professional stylists and which we release to the research community. Our model shows strong performance in an offline outfit compatibility prediction task. We use our model to generate outfits and for the first time in this field perform an AB test, comparing our generated outfits to those produced by a baseline model which matches appropriate product types but uses no information on style. Users approved of outfits generated by our model 21% and 34% more frequently than those generated by the baseline model for womenswear and menswear respectively.



### Evaluating Sequence-to-Sequence Models for Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.07377v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07377v2)
- **Published**: 2019-03-18 11:51:33+00:00
- **Updated**: 2019-07-15 11:40:53+00:00
- **Authors**: Johannes Michael, Roger Labahn, Tobias Grüning, Jochen Zöllner
- **Comment**: 8 pages, 1 figure, 8 tables
- **Journal**: None
- **Summary**: Encoder-decoder models have become an effective approach for sequence learning tasks like machine translation, image captioning and speech recognition, but have yet to show competitive results for handwritten text recognition. To this end, we propose an attention-based sequence-to-sequence model. It combines a convolutional neural network as a generic feature extractor with a recurrent neural network to encode both the visual information, as well as the temporal context between characters in the input image, and uses a separate recurrent neural network to decode the actual character sequence. We make experimental comparisons between various attention mechanisms and positional encodings, in order to find an appropriate alignment between the input and output sequence. The model can be trained end-to-end and the optional integration of a hybrid loss allows the encoder to retain an interpretable and usable output, if desired. We achieve competitive results on the IAM and ICFHR2016 READ data sets compared to the state-of-the-art without the use of a language model, and we significantly improve over any recent sequence-to-sequence approaches.



### MUSEFood: Multi-sensor-based Food Volume Estimation on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1903.07437v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07437v3)
- **Published**: 2019-03-18 13:40:23+00:00
- **Updated**: 2019-06-08 17:55:16+00:00
- **Authors**: Junyi Gao, Weihao Tan, Liantao Ma, Yasha Wang, Wen Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Researches have shown that diet recording can help people increase awareness of food intake and improve nutrition management, and thereby maintain a healthier life. Recently, researchers have been working on smartphone-based diet recording methods and applications that help users accomplish two tasks: record what they eat and how much they eat. Although the former task has made great progress through adopting image recognition technology, it is still a challenge to estimate the volume of foods accurately and conveniently. In this paper, we propose a novel method, named MUSEFood, for food volume estimation. MUSEFood uses the camera to capture photos of the food, but unlike existing volume measurement methods, MUSEFood requires neither training images with volume information nor placing a reference object of known size while taking photos. In addition, considering the impact of different containers on the contour shape of foods, MUSEFood uses a multi-task learning framework to improve the accuracy of food segmentation, and uses a differential model applicable for various containers to further reduce the negative impact of container differences on volume estimation accuracy. Furthermore, MUSEFood uses the microphone and the speaker to accurately measure the vertical distance from the camera to the food in a noisy environment, thus scaling the size of food in the image to its actual size. The experiments on real foods indicate that MUSEFood outperforms state-of-the-art approaches, and highly improves the speed of food volume estimation.



### Offline and Online Deep Learning for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.07479v1
- **DOI**: 10.1109/EXPAT.2017.7984421
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.07479v1)
- **Published**: 2019-03-18 14:39:20+00:00
- **Updated**: 2019-03-18 14:39:20+00:00
- **Authors**: Nguyen Huu Phong, Bernardete Ribeiro
- **Comment**: 5 pages
- **Journal**: 2017 4th Experiment@International Conference (exp.at'17)
- **Summary**: Image recognition using Deep Learning has been evolved for decades though advances in the field through different settings is still a challenge. In this paper, we present our findings in searching for better image classifiers in offline and online environments. We resort to Convolutional Neural Network and its variations of fully connected Multi-layer Perceptron. Though still preliminary, these results are encouraging and may provide a better understanding about the field and directions toward future works.



### Bilinear Representation for Language-based Image Editing Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.07499v1
- **DOI**: 10.1109/ICASSP.2019.8683008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07499v1)
- **Published**: 2019-03-18 15:13:55+00:00
- **Updated**: 2019-03-18 15:13:55+00:00
- **Authors**: Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Tao Xiong, Yuan He, Hui Xue
- **Comment**: To appear at ICASSP 2019. Implementation:
  https://github.com/vtddggg/BilinearGAN_for_LBIE
- **Journal**: None
- **Summary**: The task of Language-Based Image Editing (LBIE) aims at generating a target image by editing the source image based on the given language description. The main challenge of LBIE is to disentangle the semantics in image and text and then combine them to generate realistic images. Therefore, the editing performance is heavily dependent on the learned representation. In this work, conditional generative adversarial network (cGAN) is utilized for LBIE. We find that existing conditioning methods in cGAN lack of representation power as they cannot learn the second-order correlation between two conditioning vectors. To solve this problem, we propose an improved conditional layer named Bilinear Residual Layer (BRL) to learning more powerful representations for LBIE task. Qualitative and quantitative comparisons demonstrate that our method can generate images with higher quality when compared to previous LBIE techniques.



### Understanding the Limitations of CNN-based Absolute Camera Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1903.07504v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07504v1)
- **Published**: 2019-03-18 15:24:11+00:00
- **Updated**: 2019-03-18 15:24:11+00:00
- **Authors**: Torsten Sattler, Qunjie Zhou, Marc Pollefeys, Laura Leal-Taixe
- **Comment**: Initial version of a paper accepted to CVPR 2019
- **Journal**: None
- **Summary**: Visual localization is the task of accurate camera pose estimation in a known scene. It is a key problem in computer vision and robotics, with applications including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality. Traditionally, the localization problem has been tackled using 3D geometry. Recently, end-to-end approaches based on convolutional neural networks have become popular. These methods learn to directly regress the camera pose from an input image. However, they do not achieve the same level of pose accuracy as 3D structure-based methods. To understand this behavior, we develop a theoretical model for camera pose regression. We use our model to predict failure cases for pose regression techniques and verify our predictions through experiments. We furthermore use our model to show that pose regression is more closely related to pose approximation via image retrieval than to accurate pose estimation via 3D structure. A key result is that current approaches do not consistently outperform a handcrafted image retrieval baseline. This clearly shows that additional research is needed before pose regression algorithms are ready to compete with structure-based methods.



### EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1903.07520v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07520v2)
- **Published**: 2019-03-18 15:51:25+00:00
- **Updated**: 2020-01-12 23:58:10+00:00
- **Authors**: Anton Mitrokhin, Chengxi Ye, Cornelia Fermuller, Yiannis Aloimonos, Tobi Delbruck
- **Comment**: 8 pages, 6 figures. Submitted to 2019 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: We present the first event-based learning approach for motion segmentation in indoor scenes and the first event-based dataset - EV-IMO - which includes accurate pixel-wise motion masks, egomotion and ground truth depth. Our approach is based on an efficient implementation of the SfM learning pipeline using a low parameter neural network architecture on event data. In addition to camera egomotion and a dense depth map, the network estimates pixel-wise independently moving object segmentation and computes per-object 3D translational velocities for moving objects. We also train a shallow network with just 40k parameters, which is able to compute depth and egomotion.   Our EV-IMO dataset features 32 minutes of indoor recording with up to 3 fast moving objects simultaneously in the camera field of view. The objects and the camera are tracked by the VICON motion capture system. By 3D scanning the room and the objects, accurate depth map ground truth and pixel-wise object masks are obtained, which are reliable even in poor lighting conditions and during fast motion. We then train and evaluate our learning pipeline on EV-IMO and demonstrate that our approach far surpasses its rivals and is well suited for scene constrained robotics applications.



### Boosted Attention: Leveraging Human Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.00767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00767v1)
- **Published**: 2019-03-18 15:59:44+00:00
- **Updated**: 2019-03-18 15:59:44+00:00
- **Authors**: Shi Chen, Qi Zhao
- **Comment**: Published in ECCV 2018
- **Journal**: None
- **Summary**: Visual attention has shown usefulness in image captioning, with the goal of enabling a caption model to selectively focus on regions of interest. Existing models typically rely on top-down language information and learn attention implicitly by optimizing the captioning objectives. While somewhat effective, the learned top-down attention can fail to focus on correct regions of interest without direct supervision of attention. Inspired by the human visual system which is driven by not only the task-specific top-down signals but also the visual stimuli, we in this work propose to use both types of attention for image captioning. In particular, we highlight the complementary nature of the two types of attention and develop a model (Boosted Attention) to integrate them for image captioning. We validate the proposed approach with state-of-the-art performance across various evaluation metrics.



### PZnet: Efficient 3D ConvNet Inference on Manycore CPUs
- **Arxiv ID**: http://arxiv.org/abs/1903.07525v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1903.07525v1)
- **Published**: 2019-03-18 16:07:12+00:00
- **Updated**: 2019-03-18 16:07:12+00:00
- **Authors**: Sergiy Popovych, Davit Buniatyan, Aleksandar Zlateski, Kai Li, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional nets have been shown to achieve state-of-the-art accuracy in many biomedical image analysis tasks. Many tasks within biomedical analysis domain involve analyzing volumetric (3D) data acquired by CT, MRI and Microscopy acquisition methods. To deploy convolutional nets in practical working systems, it is important to solve the efficient inference problem. Namely, one should be able to apply an already-trained convolutional network to many large images using limited computational resources. In this paper we present PZnet, a CPU-only engine that can be used to perform inference for a variety of 3D convolutional net architectures. PZNet outperforms MKL-based CPU implementations of PyTorch and Tensorflow by more than 3.5x for the popular U-net architecture. Moreover, for 3D convolutions with low featuremap numbers, cloud CPU inference with PZnet outperfroms cloud GPU inference in terms of cost efficiency.



### Visual Cue Integration for Small Target Motion Detection in Natural Cluttered Backgrounds
- **Arxiv ID**: http://arxiv.org/abs/1903.07546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07546v1)
- **Published**: 2019-03-18 16:33:47+00:00
- **Updated**: 2019-03-18 16:33:47+00:00
- **Authors**: Hongxin Wang, Jigen Peng, Qinbing Fu, Huatian Wang, Shigang Yue
- **Comment**: 7 pages, 7 figures
- **Journal**: None
- **Summary**: The robust detection of small targets against cluttered background is important for future artificial visual systems in searching and tracking applications. The insects' visual systems have demonstrated excellent ability to avoid predators, find prey or identify conspecifics - which always appear as small dim speckles in the visual field. Build a computational model of the insects' visual pathways could provide effective solutions to detect small moving targets. Although a few visual system models have been proposed, they only make use of small-field visual features for motion detection and their detection results often contain a number of false positives. To address this issue, we develop a new visual system model for small target motion detection against cluttered moving backgrounds. Compared to the existing models, the small-field and wide-field visual features are separately extracted by two motion-sensitive neurons to detect small target motion and background motion. These two types of motion information are further integrated to filter out false positives. Extensive experiments showed that the proposed model can outperform the existing models in terms of detection rates.



### Human Activity Recognition for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1903.07563v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07563v1)
- **Published**: 2019-03-18 16:56:57+00:00
- **Updated**: 2019-03-18 16:56:57+00:00
- **Authors**: Manjot Bilkhu, Hammababdullah Ayyubi
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Video activity Recognition has recently gained a lot of momentum with the release of massive Kinetics (400 and 600) data. Architectures such as I3D and C3D networks have shown state-of-the-art performances for activity recognition. The one major pitfall with these state-of-the-art networks is that they require a lot of compute. In this paper we explore how we can achieve comparable results to these state-of-the-art networks for devices-on-edge. We primarily explore two architectures - I3D and Temporal Segment Network. We show that comparable results can be achieved using one tenth the memory usage by changing the testing procedure. We also report our results on Resnet architecture as our backbone apart from the original Inception architecture. Specifically, we achieve 84.54\% top-1 accuracy on UCF-101 dataset using only RGB frames.



### Learning Correspondence from the Cycle-Consistency of Time
- **Arxiv ID**: http://arxiv.org/abs/1903.07593v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07593v2)
- **Published**: 2019-03-18 17:36:00+00:00
- **Updated**: 2019-04-02 05:56:01+00:00
- **Authors**: Xiaolong Wang, Allan Jabri, Alexei A. Efros
- **Comment**: CVPR 2019 Oral. Project page: http://ajabri.github.io/timecycle
- **Journal**: None
- **Summary**: We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods.



### Neural Sequential Phrase Grounding (SeqGROUND)
- **Arxiv ID**: http://arxiv.org/abs/1903.07669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07669v1)
- **Published**: 2019-03-18 18:52:09+00:00
- **Updated**: 2019-03-18 18:52:09+00:00
- **Authors**: Pelin Dogan, Leonid Sigal, Markus Gross
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: We propose an end-to-end approach for phrase grounding in images. Unlike prior methods that typically attempt to ground each phrase independently by building an image-text embedding, our architecture formulates grounding of multiple phrases as a sequential and contextual process. Specifically, we encode region proposals and all phrases into two stacks of LSTM cells, along with so-far grounded phrase-region pairs. These LSTM stacks collectively capture context for grounding of the next phrase. The resulting architecture, which we call SeqGROUND, supports many-to-many matching by allowing an image region to be matched to multiple phrases and vice versa. We show competitive performance on the Flickr30K benchmark dataset and, through ablation studies, validate the efficacy of sequential grounding as well as individual design choices in our model architecture.



### Direct Object Recognition Without Line-of-Sight Using Optical Coherence
- **Arxiv ID**: http://arxiv.org/abs/1903.07705v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1903.07705v1)
- **Published**: 2019-03-18 20:34:41+00:00
- **Updated**: 2019-03-18 20:34:41+00:00
- **Authors**: Xin Lei, Liangyu He, Yixuan Tan, Ken Xingze Wang, Xinggang Wang, Yihan Du, Shanhui Fan, Zongfu Yu
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Visual object recognition under situations in which the direct line-of-sight is blocked, such as when it is occluded around the corner, is of practical importance in a wide range of applications. With coherent illumination, the light scattered from diffusive walls forms speckle patterns that contain information of the hidden object. It is possible to realize non-line-of-sight (NLOS) recognition with these speckle patterns. We introduce a novel approach based on speckle pattern recognition with deep neural network, which is simpler and more robust than other NLOS recognition methods. Simulations and experiments are performed to verify the feasibility and performance of this approach.



### Learning to Augment Synthetic Images for Sim2Real Policy Transfer
- **Arxiv ID**: http://arxiv.org/abs/1903.07740v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.07740v2)
- **Published**: 2019-03-18 22:01:57+00:00
- **Updated**: 2019-07-30 14:47:57+00:00
- **Authors**: Alexander Pashevich, Robin Strudel, Igor Kalevatykh, Ivan Laptev, Cordelia Schmid
- **Comment**: 7 pages
- **Journal**: IROS 2019
- **Summary**: Vision and learning have made significant progress that could improve robotics policies for complex tasks and environments. Learning deep neural networks for image understanding, however, requires large amounts of domain-specific visual data. While collecting such data from real robots is possible, such an approach limits the scalability as learning policies typically requires thousands of trials. In this work we attempt to learn manipulation policies in simulated environments. Simulators enable scalability and provide access to the underlying world state during training. Policies learned in simulators, however, do not transfer well to real scenes given the domain gap between real and synthetic data. We follow recent work on domain randomization and augment synthetic images with sequences of random transformations. Our main contribution is to optimize the augmentation strategy for sim2real transfer and to enable domain-independent policy learning. We design an efficient search for depth image augmentations using object localization as a proxy task. Given the resulting sequence of random transformations, we use it to augment synthetic depth images during policy learning. Our augmentation strategy is policy-independent and enables policy learning with no real images. We demonstrate our approach to significantly improve accuracy on three manipulation tasks evaluated on a real robot.



