# Arxiv Papers in cs.CV on 2019-03-20
### Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening
- **Arxiv ID**: http://arxiv.org/abs/1903.08297v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.08297v1)
- **Published**: 2019-03-20 00:51:01+00:00
- **Updated**: 2019-03-20 00:51:01+00:00
- **Authors**: Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanisław Jastrzębski, Thibault Févry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: We present a deep convolutional neural network for breast cancer screening exam classification, trained and evaluated on over 200,000 exams (over 1,000,000 images). Our network achieves an AUC of 0.895 in predicting whether there is a cancer in the breast, when tested on the screening population. We attribute the high accuracy of our model to a two-stage training procedure, which allows us to use a very high-capacity patch-level network to learn from pixel-level labels alongside a network learning from macroscopic breast-level labels. To validate our model, we conducted a reader study with 14 readers, each reading 720 screening mammogram exams, and find our model to be as accurate as experienced radiologists when presented with the same data. Finally, we show that a hybrid model, averaging probability of malignancy predicted by a radiologist with a prediction of our neural network, is more accurate than either of the two separately. To better understand our results, we conduct a thorough analysis of our network's performance on different subpopulations of the screening population, model design, training procedure, errors, and properties of its internal representations.



### Non-rigid 3D shape retrieval based on multi-view metric learning
- **Arxiv ID**: http://arxiv.org/abs/1904.00765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00765v1)
- **Published**: 2019-03-20 02:03:09+00:00
- **Updated**: 2019-03-20 02:03:09+00:00
- **Authors**: Haohao Li, Shengfa Wang, Nannan Li, Zhixun Su, Ximin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents a novel multi-view metric learning algorithm, which aims to improve 3D non-rigid shape retrieval. With the development of non-rigid 3D shape analysis, there exist many shape descriptors. The intrinsic descriptors can be explored to construct various intrinsic representations for non-rigid 3D shape retrieval task. The different intrinsic representations (features) focus on different geometric properties to describe the same 3D shape, which makes the representations are related. Therefore, it is possible and necessary to learn multiple metrics for different representations jointly. We propose an effective multi-view metric learning algorithm by extending the Marginal Fisher Analysis (MFA) into the multi-view domain, and exploring Hilbert-Schmidt Independence Criteria (HSCI) as a diversity term to jointly learning the new metrics. The different classes can be separated by MFA in our method. Meanwhile, HSCI is exploited to make the multiple representations to be consensus. The learned metrics can reduce the redundancy between the multiple representations, and improve the accuracy of the retrieval results. Experiments are performed on SHREC'10 benchmarks, and the results show that the proposed method outperforms the state-of-the-art non-rigid 3D shape retrieval methods.



### Face Detection in Repeated Settings
- **Arxiv ID**: http://arxiv.org/abs/1903.08649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08649v1)
- **Published**: 2019-03-20 05:03:27+00:00
- **Updated**: 2019-03-20 05:03:27+00:00
- **Authors**: Mohammad Nayeem Teli, Bruce A. Draper, J. Ross Beveridge
- **Comment**: 14 pages, 21 figures
- **Journal**: None
- **Summary**: Face detection is an important first step before face verification and recognition. In unconstrained settings it is still an open challenge because of the variation in pose, lighting, scale, background and location. However, for the purposes of verification we can have a control on background and location. Images are primarily captured in places such as the entrance to a sensitive building, in front of a door or some location where the background does not change. We present a correlation based face detection algorithm to detect faces in such settings, where we control the location, and leave lighting, pose, and scale uncontrolled. In these scenarios the results indicate that our algorithm is easy and fast to train, outperforms Viola and Jones face detection accuracy and is faster to test.



### Photon-Flooded Single-Photon 3D Cameras
- **Arxiv ID**: http://arxiv.org/abs/1903.08347v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.08347v2)
- **Published**: 2019-03-20 05:29:18+00:00
- **Updated**: 2019-04-29 04:51:28+00:00
- **Authors**: Anant Gupta, Atul Ingle, Andreas Velten, Mohit Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Single photon avalanche diodes (SPADs) are starting to play a pivotal role in the development of photon-efficient, long-range LiDAR systems. However, due to non-linearities in their image formation model, a high photon flux (e.g., due to strong sunlight) leads to distortion of the incident temporal waveform, and potentially, large depth errors. Operating SPADs in low flux regimes can mitigate these distortions, but, often requires attenuating the signal and thus, results in low signal-to-noise ratio. In this paper, we address the following basic question: what is the optimal photon flux that a SPAD-based LiDAR should be operated in? We derive a closed form expression for the optimal flux, which is quasi-depth-invariant, and depends on the ambient light strength. The optimal flux is lower than what a SPAD typically measures in real world scenarios, but surprisingly, considerably higher than what is conventionally suggested for avoiding distortions. We propose a simple, adaptive approach for achieving the optimal flux by attenuating incident flux based on an estimate of ambient light strength. Using extensive simulations and a hardware prototype, we show that the optimal flux criterion holds for several depth estimators, under a wide range of illumination conditions.



### GRIP: Generative Robust Inference and Perception for Semantic Robot Manipulation in Adversarial Environments
- **Arxiv ID**: http://arxiv.org/abs/1903.08352v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.08352v3)
- **Published**: 2019-03-20 06:08:27+00:00
- **Updated**: 2019-12-19 06:00:54+00:00
- **Authors**: Xiaotong Chen, Rui Chen, Zhiqiang Sui, Zhefan Ye, Yanqi Liu, R. Iris Bahar, Odest Chadwicke Jenkins
- **Comment**: 9 pages, 7 figures, published on IROS 2019. contact: cxt@umich.edu
- **Journal**: None
- **Summary**: Recent advancements have led to a proliferation of machine learning systems used to assist humans in a wide range of tasks. However, we are still far from accurate, reliable, and resource-efficient operations of these systems. For robot perception, convolutional neural networks (CNNs) for object detection and pose estimation are recently coming into widespread use. However, neural networks are known to suffer overfitting during training process and are less robust within unseen conditions, which are especially vulnerable to adversarial scenarios. In this work, we propose Generative Robust Inference and Perception (GRIP) as a two-stage object detection and pose estimation system that aims to combine relative strengths of discriminative CNNs and generative inference methods to achieve robust estimation. Our results show that a second stage of sample-based generative inference is able to recover from false object detection by CNNs, and produce robust estimations in adversarial conditions. We demonstrate the efficacy of GRIP robustness through comparison with state-of-the-art learning-based pose estimators and pick-and-place manipulation in dark and cluttered environments.



### Regularize, Expand and Compress: Multi-task based Lifelong Learning via NonExpansive AutoML
- **Arxiv ID**: http://arxiv.org/abs/1903.08362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08362v1)
- **Published**: 2019-03-20 07:16:58+00:00
- **Updated**: 2019-03-20 07:16:58+00:00
- **Authors**: Jie Zhang, Junting Zhang, Shalini Ghosh, Dawei Li, Jingwen Zhu, Heming Zhang, Yalin Wang
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Lifelong learning, the problem of continual learning where tasks arrive in sequence, has been lately attracting more attention in the computer vision community. The aim of lifelong learning is to develop a system that can learn new tasks while maintaining the performance on the previously learned tasks. However, there are two obstacles for lifelong learning of deep neural networks: catastrophic forgetting and capacity limitation. To solve the above issues, inspired by the recent breakthroughs in automatically learning good neural network architectures, we develop a Multi-task based lifelong learning via nonexpansive AutoML framework termed Regularize, Expand and Compress (REC). REC is composed of three stages: 1) continually learns the sequential tasks without the learned tasks' data via a newly proposed multi-task weight consolidation (MWC) algorithm; 2) expands the network to help the lifelong learning with potentially improved model capability and performance by network-transformation based AutoML; 3) compresses the expanded model after learning every new task to maintain model efficiency and performance. The proposed MWC and REC algorithms achieve superior performance over other lifelong learning algorithms on four different datasets.



### Part-based approximations for morphological operators using asymmetric auto-encoders
- **Arxiv ID**: http://arxiv.org/abs/1904.00763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1904.00763v2)
- **Published**: 2019-03-20 08:16:48+00:00
- **Updated**: 2019-04-03 12:03:34+00:00
- **Authors**: Bastien Ponchon, Santiago Velasco-Forero, Samy Blusseau, Jesus Angulo, Isabelle Bloch
- **Comment**: None
- **Journal**: International Symposium on Mathematical Morphology, Jul 2019,
  Saarbr{\"u}cken, Germany
- **Summary**: This paper addresses the issue of building a part-based representation of a dataset of images. More precisely, we look for a non-negative, sparse decomposition of the images on a reduced set of atoms, in order to unveil a morphological and interpretable structure of the data. Additionally, we want this decomposition to be computed online for any new sample that is not part of the initial dataset. Therefore, our solution relies on a sparse, non-negative auto-encoder where the encoder is deep (for accuracy) and the decoder shallow (for interpretability). This method compares favorably to the state-of-the-art online methods on two datasets (MNIST and Fashion MNIST), according to classical metrics and to a new one we introduce, based on the invariance of the representation to morphological dilation.



### Convolution with even-sized kernels and symmetric padding
- **Arxiv ID**: http://arxiv.org/abs/1903.08385v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08385v2)
- **Published**: 2019-03-20 08:34:20+00:00
- **Updated**: 2019-05-22 03:01:43+00:00
- **Authors**: Shuang Wu, Guanrui Wang, Pei Tang, Feng Chen, Luping Shi
- **Comment**: 12 pages, 4 figures, 4 tables
- **Journal**: None
- **Summary**: Compact convolutional neural networks gain efficiency mainly through depthwise convolutions, expanded channels and complex topologies, which contrarily aggravate the training process. Besides, 3x3 kernels dominate the spatial representation in these models, whereas even-sized kernels (2x2, 4x4) are rarely adopted. In this work, we quantify the shift problem occurs in even-sized kernel convolutions by an information erosion hypothesis, and eliminate it by proposing symmetric padding on four sides of the feature maps (C2sp, C4sp). Symmetric padding releases the generalization capabilities of even-sized kernels at little computational cost, making them outperform 3x3 kernels in image classification and generation tasks. Moreover, C2sp obtains comparable accuracy to emerging compact models with much less memory and time consumption during training. Symmetric padding coupled with even-sized convolutions can be neatly implemented into existing frameworks, providing effective elements for architecture designs, especially on online and continual learning occasions where training efforts are emphasized.



### On Class Imbalance and Background Filtering in Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.08456v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08456v2)
- **Published**: 2019-03-20 11:46:24+00:00
- **Updated**: 2019-03-21 18:42:41+00:00
- **Authors**: Alessio Sarullo, Tingting Mu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we investigate the problems of class imbalance and irrelevant relationships in Visual Relationship Detection (VRD). State-of-the-art deep VRD models still struggle to predict uncommon classes, limiting their applicability. Moreover, many methods are incapable of properly filtering out background relationships while predicting relevant ones. Although these problems are very apparent, they have both been overlooked so far. We analyse why this is the case and propose modifications to both model and training to alleviate the aforementioned issues, as well as suggesting new measures to complement existing ones and give a more holistic picture of the efficacy of a model.



### In Defense of Pre-trained ImageNet Architectures for Real-time Semantic Segmentation of Road-driving Images
- **Arxiv ID**: http://arxiv.org/abs/1903.08469v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08469v2)
- **Published**: 2019-03-20 12:23:21+00:00
- **Updated**: 2019-04-12 09:46:06+00:00
- **Authors**: Marin Oršić, Ivan Krešo, Petra Bevandić, Siniša Šegvić
- **Comment**: Accepted to CVPR 2019. 8 pages, 8 figures, 5 tables. PyTorch source
  is available at https://github.com/orsic/swiftnet
- **Journal**: None
- **Summary**: Recent success of semantic segmentation approaches on demanding road driving datasets has spurred interest in many related application fields. Many of these applications involve real-time prediction on mobile platforms such as cars, drones and various kinds of robots. Real-time setup is challenging due to extraordinary computational complexity involved. Many previous works address the challenge with custom lightweight architectures which decrease computational complexity by reducing depth, width and layer capacity with respect to general purpose architectures. We propose an alternative approach which achieves a significantly better performance across a wide range of computing budgets. First, we rely on a light-weight general purpose architecture as the main recognition engine. Then, we leverage light-weight upsampling with lateral connections as the most cost-effective solution to restore the prediction resolution. Finally, we propose to enlarge the receptive field by fusing shared features at multiple resolutions in a novel fashion. Experiments on several road driving datasets show a substantial advantage of the proposed approach, either with ImageNet pre-trained parameters or when we learn from scratch. Our Cityscapes test submission entitled SwiftNetRN-18 delivers 75.5% MIoU and achieves 39.9 Hz on 1024x2048 images on GTX1080Ti.



### Deep Octonion Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.08478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08478v1)
- **Published**: 2019-03-20 12:30:56+00:00
- **Updated**: 2019-03-20 12:30:56+00:00
- **Authors**: Jiasong Wu, Ling Xu, Youyong Kong, Lotfi Senhadji, Huazhong Shu
- **Comment**: 27 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning is a research hot topic in the field of machine learning. Real-value neural networks (Real NNs), especially deep real networks (DRNs), have been widely used in many research fields. In recent years, the deep complex networks (DCNs) and the deep quaternion networks (DQNs) have attracted more and more attentions. The octonion algebra, which is an extension of complex algebra and quaternion algebra, can provide more efficient and compact expression. This paper constructs a general framework of deep octonion networks (DONs) and provides the main building blocks of DONs such as octonion convolution, octonion batch normalization and octonion weight initialization; DONs are then used in image classification tasks for CIFAR-10 and CIFAR-100 data sets. Compared with the DRNs, the DCNs, and the DQNs, the proposed DONs have better convergence and higher classification accuracy. The success of DONs is also explained by multi-task learning.



### Three-dimensional Segmentation of Trees Through a Flexible Multi-Class Graph Cut Algorithm (MCGC)
- **Arxiv ID**: http://arxiv.org/abs/1903.08481v1
- **DOI**: 10.1109/TGRS.2019.2940146
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08481v1)
- **Published**: 2019-03-20 12:35:17+00:00
- **Updated**: 2019-03-20 12:35:17+00:00
- **Authors**: Jonathan Williams, Carola-Bibiane Schönlieb, Tom Swinfield, Juheon Lee, Xiaohao Cai, Lan Qie, David A. Coomes
- **Comment**: None
- **Journal**: None
- **Summary**: Developing a robust algorithm for automatic individual tree crown (ITC) detection from laser scanning datasets is important for tracking the responses of trees to anthropogenic change. Such approaches allow the size, growth and mortality of individual trees to be measured, enabling forest carbon stocks and dynamics to be tracked and understood. Many algorithms exist for structurally simple forests including coniferous forests and plantations. Finding a robust solution for structurally complex, species-rich tropical forests remains a challenge; existing segmentation algorithms often perform less well than simple area-based approaches when estimating plot-level biomass. Here we describe a Multi-Class Graph Cut (MCGC) approach to tree crown delineation. This uses local three-dimensional geometry and density information, alongside knowledge of crown allometries, to segment individual tree crowns from LiDAR point clouds. Our approach robustly identifies trees in the top and intermediate layers of the canopy, but cannot recognise small trees. From these three-dimensional crowns, we are able to measure individual tree biomass. Comparing these estimates to those from permanent inventory plots, our algorithm is able to produce robust estimates of hectare-scale carbon density, demonstrating the power of ITC approaches in monitoring forests. The flexibility of our method to add additional dimensions of information, such as spectral reflectance, make this approach an obvious avenue for future development and extension to other sources of three-dimensional data, such as structure from motion datasets.



### Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set
- **Arxiv ID**: http://arxiv.org/abs/1903.08527v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08527v2)
- **Published**: 2019-03-20 14:44:45+00:00
- **Updated**: 2020-04-09 16:42:09+00:00
- **Authors**: Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, Xin Tong
- **Comment**: minor revision of the layout; update contact information
- **Journal**: None
- **Summary**: Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency.However, training deep neural networks typically requires a large volume of data, whereas face images with ground-truth 3D face shapes are scarce. In this paper, we propose a novel deep 3D face reconstruction approach that 1) leverages a robust, hybrid loss function for weakly-supervised learning which takes into account both low-level and perception-level information for supervision, and 2) performs multi-image face reconstruction by exploiting complementary information from different images for shape aggregation. Our method is fast, accurate, and robust to occlusion and large pose. We provide comprehensive experiments on three datasets, systematically comparing our method with fifteen recent methods and demonstrating its state-of-the-art performance.



### Segmentation-Based Deep-Learning Approach for Surface-Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.08536v3
- **DOI**: 10.1007/s10845-019-01476-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08536v3)
- **Published**: 2019-03-20 15:03:17+00:00
- **Updated**: 2019-06-11 10:07:09+00:00
- **Authors**: Domen Tabernik, Samo Šela, Jure Skvarč, Danijel Skočaj
- **Comment**: Journal of Intelligent Manufacturing 2019
- **Journal**: None
- **Summary**: Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25-30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.



### Learning Convolutional Transforms for Lossy Point Cloud Geometry Compression
- **Arxiv ID**: http://arxiv.org/abs/1903.08548v2
- **DOI**: 10.1109/ICIP.2019.8803413
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.08548v2)
- **Published**: 2019-03-20 15:14:15+00:00
- **Updated**: 2019-05-22 15:56:14+00:00
- **Authors**: Maurice Quach, Giuseppe Valenzise, Frederic Dufaux
- **Comment**: Published in ICIP 2019. The source code can be found at
  https://github.com/mauriceqch/pcc_geo_cnn and the supplementary material can
  be found at https://www.mauricequach.com/pcc_geo_cnn_samples
- **Journal**: None
- **Summary**: Efficient point cloud compression is fundamental to enable the deployment of virtual and mixed reality applications, since the number of points to code can range in the order of millions. In this paper, we present a novel data-driven geometry compression method for static point clouds based on learned convolutional transforms and uniform quantization. We perform joint optimization of both rate and distortion using a trade-off parameter. In addition, we cast the decoding process as a binary classification of the point cloud occupancy map. Our method outperforms the MPEG reference solution in terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with 51.5% BDBR savings on average. Moreover, while octree-based methods face exponential diminution of the number of points at low bitrates, our method still produces high resolution outputs even at low bitrates. Code and supplementary material are available at https://github.com/mauriceqch/pcc_geo_cnn .



### Convolutional Sparse Coding for Compressed Sensing CT Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1903.08549v1
- **DOI**: 10.1109/TMI.2019.2906853
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.08549v1)
- **Published**: 2019-03-20 15:14:58+00:00
- **Updated**: 2019-03-20 15:14:58+00:00
- **Authors**: Peng Bao, Wenjun Xia, Kang Yang, Weiyan Chen, Mianyi Chen, Yan Xi, Shanzhou Niu, Jiliu Zhou, He Zhang, Huaiqiang Sun, Zhangyang Wang, Yi Zhang
- **Comment**: Accepted by IEEE TMI
- **Journal**: None
- **Summary**: Over the past few years, dictionary learning (DL)-based methods have been successfully used in various image reconstruction problems. However, traditional DL-based computed tomography (CT) reconstruction methods are patch-based and ignore the consistency of pixels in overlapped patches. In addition, the features learned by these methods always contain shifted versions of the same features. In recent years, convolutional sparse coding (CSC) has been developed to address these problems. In this paper, inspired by several successful applications of CSC in the field of signal processing, we explore the potential of CSC in sparse-view CT reconstruction. By directly working on the whole image, without the necessity of dividing the image into overlapped patches in DL-based methods, the proposed methods can maintain more details and avoid artifacts caused by patch aggregation. With predetermined filters, an alternating scheme is developed to optimize the objective function. Extensive experiments with simulated and real CT data were performed to validate the effectiveness of the proposed methods. Qualitative and quantitative results demonstrate that the proposed methods achieve better performance than several existing state-of-the-art methods.



### OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations
- **Arxiv ID**: http://arxiv.org/abs/1903.08550v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08550v1)
- **Published**: 2019-03-20 15:15:05+00:00
- **Updated**: 2019-03-20 15:15:05+00:00
- **Authors**: Pramuditha Perera, Ramesh Nallapati, Bing Xiang
- **Comment**: CVPR 2019 Accepted Paper
- **Journal**: None
- **Summary**: We present a novel model called OCGAN for the classical problem of one-class novelty detection, where, given a set of examples from a particular class, the goal is to determine if a query example is from the same class. Our solution is based on learning latent representations of in-class examples using a denoising auto-encoder network. The key contribution of our work is our proposal to explicitly constrain the latent space to exclusively represent the given class. In order to accomplish this goal, firstly, we force the latent space to have bounded support by introducing a tanh activation in the encoder's output layer. Secondly, using a discriminator in the latent space that is trained adversarially, we ensure that encoded representations of in-class examples resemble uniform random samples drawn from the same bounded space. Thirdly, using a second adversarial discriminator in the input space, we ensure all randomly drawn latent samples generate examples that look real. Finally, we introduce a gradient-descent based sampling technique that explores points in the latent space that generate potential out-of-class examples, which are fed back to the network to further train it to generate in-class examples from those points. The effectiveness of the proposed method is measured across four publicly available datasets using two one-class novelty detection protocols where we achieve state-of-the-art results.



### Single Image Deraining: A Comprehensive Benchmark Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.08558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08558v1)
- **Published**: 2019-03-20 15:31:07+00:00
- **Updated**: 2019-03-20 15:31:07+00:00
- **Authors**: Siyuan Li, Iago Breno Araujo, Wenqi Ren, Zhangyang Wang, Eric K. Tokuda, Roberto Hirata Junior, Roberto Cesar-Junior, Jiawan Zhang, Xiaojie Guo, Xiaochun Cao
- **Comment**: None
- **Journal**: None
- **Summary**: We present a comprehensive study and evaluation of existing single image deraining algorithms, using a new large-scale benchmark consisting of both synthetic and real-world rainy images.This dataset highlights diverse data sources and image contents, and is divided into three subsets (rain streak, rain drop, rain and mist), each serving different training or evaluation purposes. We further provide a rich variety of criteria for dehazing algorithm evaluation, ranging from full-reference metrics, to no-reference metrics, to subjective evaluation and the novel task-driven evaluation. Experiments on the dataset shed light on the comparisons and limitations of state-of-the-art deraining algorithms, and suggest promising future directions.



### Data Augmentation for Leaf Segmentation and Counting Tasks in Rosette Plants
- **Arxiv ID**: http://arxiv.org/abs/1903.08583v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.08583v1)
- **Published**: 2019-03-20 16:13:10+00:00
- **Updated**: 2019-03-20 16:13:10+00:00
- **Authors**: Dmitry Kuznichov, Alon Zvirin, Yaron Honen, Ron Kimmel
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques involving image processing and data analysis are constantly evolving. Many domains adapt these techniques for object segmentation, instantiation and classification. Recently, agricultural industries adopted those techniques in order to bring automation to farmers around the globe. One analysis procedure required for automatic visual inspection in this domain is leaf count and segmentation. Collecting labeled data from field crops and greenhouses is a complicated task due to the large variety of crops, growth seasons, climate changes, phenotype diversity, and more, especially when specific learning tasks require a large amount of labeled data for training. Data augmentation for training deep neural networks is well established, examples include data synthesis, using generative semi-synthetic models, and applying various kinds of transformations. In this paper we propose a method that preserves the geometric structure of the data objects, thus keeping the physical appearance of the data-set as close as possible to imaged plants in real agricultural scenes. The proposed method provides state of the art results when applied to the standard benchmark in the field, namely, the ongoing Leaf Segmentation Challenge hosted by Computer Vision Problems in Plant Phenotyping.



### A Polynomial-time Solution for Robust Registration with Extreme Outlier Rates
- **Arxiv ID**: http://arxiv.org/abs/1903.08588v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, 68T40, 74Pxx, 46N10, 65D19, I.2.9; G.1.6; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1903.08588v2)
- **Published**: 2019-03-20 16:16:28+00:00
- **Updated**: 2019-06-30 15:37:56+00:00
- **Authors**: Heng Yang, Luca Carlone
- **Comment**: 18 pages, Accepted for publication in Robotics: Science and Systems,
  2019
- **Journal**: Robotics: Science and Systems, 2019
- **Summary**: We propose a robust approach for the registration of two sets of 3D points in the presence of a large amount of outliers. Our first contribution is to reformulate the registration problem using a Truncated Least Squares (TLS) cost that makes the estimation insensitive to a large fraction of spurious point-to-point correspondences. The second contribution is a general framework to decouple rotation, translation, and scale estimation, which allows solving in cascade for the three transformations. Since each subproblem (scale, rotation, and translation estimation) is still non-convex and combinatorial in nature, out third contribution is to show that (i) TLS scale and (component-wise) translation estimation can be solved exactly and in polynomial time via an adaptive voting scheme, (ii) TLS rotation estimation can be relaxed to a semidefinite program and the relaxation is tight in practice, even in the presence of an extreme amount of outliers. We validate the proposed algorithm, named TEASER (Truncated least squares Estimation And SEmidefinite Relaxation), in standard registration benchmarks showing that the algorithm outperforms RANSAC and robust local optimization techniques, and favorably compares with Branch-and-Bound methods, while being a polynomial-time algorithm. TEASER can tolerate up to 99% outliers and returns highly-accurate solutions.



### DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.08589v2
- **DOI**: 10.1016/j.ins.2020.02.067
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08589v2)
- **Published**: 2019-03-20 16:19:20+00:00
- **Updated**: 2022-09-04 07:11:47+00:00
- **Authors**: Zhanchao Huang, Jianlin Wang, Xuesong Fu, Tao Yu, Yongqi Guo, Rutong Wang
- **Comment**: 23 pages, 9 figures, 9 tables
- **Journal**: None
- **Summary**: Although the YOLOv2 method is extremely fast on object detection, its detection accuracy is restricted due to the low performance of its backbone network and the underutilization of multi-scale region features. Therefore, a dense connection (DC) and spatial pyramid pooling (SPP) based YOLO (DC-SPP-YOLO) method for ameliorating the object detection accuracy of YOLOv2 is proposed in this paper. Specifically, the dense connection of convolution layers is employed in the backbone network of YOLOv2 to strengthen the feature extraction and alleviate the vanishing-gradient problem. Moreover, an improved spatial pyramid pooling is introduced to pool and concatenate the multi-scale region features, so that the network can learn the object features more comprehensively. The DC-SPP-YOLO model is established and trained based on a new loss function composed of MSE (mean square error) loss and cross-entropy loss. The experimental results indicated that the mAP (mean Average Precision) of DC-SPP-YOLO is higher than that of YOLOv2 on the PASCAL VOC datasets and the UA-DETRAC datasets. The effectiveness of DC-SPP-YOLO method proposed is demonstrated.



### Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet
- **Arxiv ID**: http://arxiv.org/abs/1904.00760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00760v1)
- **Published**: 2019-03-20 16:37:17+00:00
- **Updated**: 2019-03-20 16:37:17+00:00
- **Authors**: Wieland Brendel, Matthias Bethge
- **Comment**: Published as a conference paper at the Seventh International
  Conference on Learning Representations (ICLR 2019)
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6% top-5 for 33 x 33 px features and Alexnet performance for 17 x 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.



### Plug and play methods for magnetic resonance imaging (long version)
- **Arxiv ID**: http://arxiv.org/abs/1903.08616v5
- **DOI**: 10.1109/MSP.2019.2949470
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08616v5)
- **Published**: 2019-03-20 16:59:04+00:00
- **Updated**: 2019-12-10 19:19:39+00:00
- **Authors**: Rizwan Ahmad, Charles A. Bouman, Gregery T. Buzzard, Stanley Chan, Sizhou Liu, Edward T. Reehorst, Philip Schniter
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic tool that provides excellent soft-tissue contrast without the use of ionizing radiation. Compared to other clinical imaging modalities (e.g., CT or ultrasound), however, the data acquisition process for MRI is inherently slow, which motivates undersampling and thus drives the need for accurate, efficient reconstruction methods from undersampled datasets. In this article, we describe the use of "plug-and-play" (PnP) algorithms for MRI image recovery. We first describe the linearly approximated inverse problem encountered in MRI. Then we review several PnP methods, where the unifying commonality is to iteratively call a denoising subroutine as one step of a larger optimization-inspired algorithm. Next, we describe how the result of the PnP method can be interpreted as a solution to an equilibrium equation, allowing convergence analysis from the equilibrium perspective. Finally, we present illustrative examples of PnP methods applied to MRI image recovery.



### An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM
- **Arxiv ID**: http://arxiv.org/abs/1903.08636v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.08636v1)
- **Published**: 2019-03-20 17:50:49+00:00
- **Updated**: 2019-03-20 17:50:49+00:00
- **Authors**: Patrick Geneva, James Maley, Guoquan Huang
- **Comment**: Accepted to the 2019 Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Journal**: 2019 Conference on Computer Vision and Pattern Recognition (CVPR)
- **Summary**: It holds great implications for practical applications to enable centimeter-accuracy positioning for mobile and wearable sensor systems. In this paper, we propose a novel, high-precision, efficient visual-inertial (VI)-SLAM algorithm, termed Schmidt-EKF VI-SLAM (SEVIS), which optimally fuses IMU measurements and monocular images in a tightly-coupled manner to provide 3D motion tracking with bounded error. In particular, we adapt the Schmidt Kalman filter formulation to selectively include informative features in the state vector while treating them as nuisance parameters (or Schmidt states) once they become matured. This change in modeling allows for significant computational savings by no longer needing to constantly update the Schmidt states (or their covariance), while still allowing the EKF to correctly account for their cross-correlations with the active states. As a result, we achieve linear computational complexity in terms of map size, instead of quadratic as in the standard SLAM systems. In order to fully exploit the map information to bound navigation drifts, we advocate efficient keyframe-aided 2D-to-2D feature matching to find reliable correspondences between current 2D visual measurements and 3D map features. The proposed SEVIS is extensively validated in both simulations and experiments.



### Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1903.08642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1903.08642v1)
- **Published**: 2019-03-20 17:58:38+00:00
- **Updated**: 2019-03-20 17:58:38+00:00
- **Authors**: Chen-Hsuan Lin, Oliver Wang, Bryan C. Russell, Eli Shechtman, Vladimir G. Kim, Matthew Fisher, Simon Lucey
- **Comment**: Accepted to CVPR 2019 (project page & code:
  https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/)
- **Journal**: None
- **Summary**: In this paper, we address the problem of 3D object mesh reconstruction from RGB videos. Our approach combines the best of multi-view geometric and data-driven methods for 3D reconstruction by optimizing object meshes for multi-view photometric consistency while constraining mesh deformations with a shape prior. We pose this as a piecewise image alignment problem for each mesh face projection. Our approach allows us to update shape parameters from the photometric error without any depth or mask information. Moreover, we show how to avoid a degeneracy of zero photometric gradients via rasterizing from a virtual viewpoint. We demonstrate 3D object mesh reconstruction results from both synthetic and real-world videos with our photometric mesh optimization, which is unachievable with either na\"ive mesh generation networks or traditional pipelines of surface reconstruction without heavy manual post-processing.



### Gradient based sample selection for online continual learning
- **Arxiv ID**: http://arxiv.org/abs/1903.08671v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.08671v5)
- **Published**: 2019-03-20 18:01:55+00:00
- **Updated**: 2019-10-31 14:45:47+00:00
- **Authors**: Rahaf Aljundi, Min Lin, Baptiste Goujaud, Yoshua Bengio
- **Comment**: Neurips 2019
- **Journal**: None
- **Summary**: A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.



### Im2Pencil: Controllable Pencil Illustration from Photographs
- **Arxiv ID**: http://arxiv.org/abs/1903.08682v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08682v1)
- **Published**: 2019-03-20 18:25:04+00:00
- **Updated**: 2019-03-20 18:25:04+00:00
- **Authors**: Yijun Li, Chen Fang, Aaron Hertzmann, Eli Shechtman, Ming-Hsuan Yang
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: We propose a high-quality photo-to-pencil translation method with fine-grained control over the drawing style. This is a challenging task due to multiple stroke types (e.g., outline and shading), structural complexity of pencil shading (e.g., hatching), and the lack of aligned training data pairs. To address these challenges, we develop a two-branch model that learns separate filters for generating sketchy outlines and tonal shading from a collection of pencil drawings. We create training data pairs by extracting clean outlines and tonal illustrations from original pencil drawings using image filtering techniques, and we manually label the drawing styles. In addition, our model creates different pencil styles (e.g., line sketchiness and shading style) in a user-controllable manner. Experimental results on different types of pencil drawings show that the proposed algorithm performs favorably against existing methods in terms of quality, diversity and user evaluations.



### Implicit Generation and Generalization in Energy-Based Models
- **Arxiv ID**: http://arxiv.org/abs/1903.08689v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.08689v6)
- **Published**: 2019-03-20 18:34:29+00:00
- **Updated**: 2020-06-30 03:25:59+00:00
- **Authors**: Yilun Du, Igor Mordatch
- **Comment**: None
- **Journal**: None
- **Summary**: Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.



### LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.08701v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.08701v1)
- **Published**: 2019-03-20 19:02:44+00:00
- **Updated**: 2019-03-20 19:02:44+00:00
- **Authors**: Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, Carl K. Wellington
- **Comment**: Accepted for publication at CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present LaserNet, a computationally efficient method for 3D object detection from LiDAR data for autonomous driving. The efficiency results from processing LiDAR data in the native range view of the sensor, where the input data is naturally compact. Operating in the range view involves well known challenges for learning, including occlusion and scale variation, but it also provides contextual information based on how the sensor data was captured. Our approach uses a fully convolutional network to predict a multimodal distribution over 3D boxes for each point and then it efficiently fuses these distributions to generate a prediction for each object. Experiments show that modeling each detection as a distribution rather than a single deterministic box leads to better overall detection performance. Benchmark results show that this approach has significantly lower runtime than other recent detectors and that it achieves state-of-the-art performance when compared on a large dataset that has enough data to overcome the challenges of training on the range view.



### Affordance Learning In Direct Perception for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.08746v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1903.08746v1)
- **Published**: 2019-03-20 21:15:08+00:00
- **Updated**: 2019-03-20 21:15:08+00:00
- **Authors**: Chen Sun, Jean M. Uwabeza Vianney, Dongpu Cao
- **Comment**: 9 pages, 13 figures
- **Journal**: None
- **Summary**: Recent development in autonomous driving involves high-level computer vision and detailed road scene understanding. Today, most autonomous vehicles are using mediated perception approach for path planning and control, which highly rely on high-definition 3D maps and real time sensors. Recent research efforts aim to substitute the massive HD maps with coarse road attributes. In this paper, we follow the direct perception based method to train a deep neural network for affordance learning in autonomous driving. Our goal in this work is to develop the affordance learning model based on freely available Google Street View panoramas and Open Street Map road vector attributes. Driving scene understanding can be achieved by learning affordances from the images captured by car-mounted cameras. Such scene understanding by learning affordances may be useful for corroborating base maps such as HD maps so that the required data storage space is minimized and available for processing in real time. We compare capability in road attribute identification between human volunteers and our model by experimental evaluation. Our results indicate that this method could act as a cheaper way for training data collection in autonomous driving. The cross validation results also indicate the effectiveness of our model.



### Robust Image Segmentation Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1903.08773v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.08773v3)
- **Published**: 2019-03-20 23:07:47+00:00
- **Updated**: 2020-05-27 17:24:49+00:00
- **Authors**: Leixin Zhou, Wenxiang Deng, Xiaodong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based image segmentation methods have achieved great success, even having human-level accuracy in some applications. However, due to the black box nature of deep learning, the best method may fail in some situations. Thus predicting segmentation quality without ground truth would be very crucial especially in clinical practice. Recently, people proposed to train neural networks to estimate the quality score by regression. Although it can achieve promising prediction accuracy, the network suffers robustness problem, e.g. it is vulnerable to adversarial attacks. In this paper, we propose to alleviate this problem by utilizing the difference between the input image and the reconstructed image, which is conditioned on the segmentation to be assessed, to lower the chance to overfit to the undesired image features from the original input image, and thus to increase the robustness. Results on ACDC17 dataset demonstrated our method is promising.



