# Arxiv Papers in cs.CV on 2019-03-11
### A Hybrid Framework for Action Recognition in Low-Quality Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1903.04090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04090v1)
- **Published**: 2019-03-11 00:57:20+00:00
- **Updated**: 2019-03-11 00:57:20+00:00
- **Authors**: Tej Singh, Dinesh Kumar Vishwakarma
- **Comment**: 13 pages, 9 Figures
- **Journal**: None
- **Summary**: Vision-based activity recognition is essential for security, monitoring and surveillance applications. Further, real-time analysis having low-quality video and contain less information about surrounding due to poor illumination, and occlusions. Therefore, it needs a more robust and integrated model for low quality and night security operations. In this context, we proposed a hybrid model for illumination invariant human activity recognition based on sub-image histogram equalization enhancement and k-key pose human silhouettes. This feature vector gives good average recognition accuracy on three low exposure video sequences subset of original actions video datasets. Finally, the performance of the proposed approach is tested over three manually downgraded low qualities Weizmann action, KTH, and Ballet Movement dataset. This model outperformed on low exposure videos over existing technique and achieved comparable classification accuracy to similar state-of-the-art methods.



### MTRNet: A Generic Scene Text Eraser
- **Arxiv ID**: http://arxiv.org/abs/1903.04092v3
- **DOI**: 10.1109/ICDAR.2019.00016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04092v3)
- **Published**: 2019-03-11 01:03:43+00:00
- **Updated**: 2019-10-22 06:24:29+00:00
- **Authors**: Osman Tursun, Rui Zeng, Simon Denman, Sabesan Sivapalan, Sridha Sridharan, Clinton Fookes
- **Comment**: Presented at ICDAR2019 Conference
- **Journal**: None
- **Summary**: Text removal algorithms have been proposed for uni-lingual scripts with regular shapes and layouts. However, to the best of our knowledge, a generic text removal method which is able to remove all or user-specified text regions regardless of font, script, language or shape is not available. Developing such a generic text eraser for real scenes is a challenging task, since it inherits all the challenges of multi-lingual and curved text detection and inpainting. To fill this gap, we propose a mask-based text removal network (MTRNet). MTRNet is a conditional adversarial generative network (cGAN) with an auxiliary mask. The introduced auxiliary mask not only makes the cGAN a generic text eraser, but also enables stable training and early convergence on a challenging large-scale synthetic dataset, initially proposed for text detection in real scenes. What's more, MTRNet achieves state-of-the-art results on several real-world datasets including ICDAR 2013, ICDAR 2017 MLT, and CTW1500, without being explicitly trained on this data, outperforming previous state-of-the-art methods trained directly on these datasets.



### Spatial-Aware Non-Local Attention for Fashion Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.04104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04104v1)
- **Published**: 2019-03-11 02:35:39+00:00
- **Updated**: 2019-03-11 02:35:39+00:00
- **Authors**: Yixin Li, Shengqin Tang, Yun Ye, Jinwen Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Fashion landmark detection is a challenging task even using the current deep learning techniques, due to the large variation and non-rigid deformation of clothes. In order to tackle these problems, we propose Spatial-Aware Non-Local (SANL) block, an attentive module in deep neural network which can utilize spatial information while capturing global dependency. Actually, the SANL block is constructed from the non-local block in the residual manner which can learn the spatial related representation by taking a spatial attention map from Grad-CAM. We then establish our fashion landmark detection framework on feature pyramid network, equipped with four SANL blocks in the backbone. It is demonstrated by the experimental results on two large-scale fashion datasets that our proposed fashion landmark detection approach with the SANL blocks outperforms the current state-of-the-art methods considerably. Some supplementary experiments on fine-grained image classification also show the effectiveness of the proposed SANL block.



### Alignment Based Matching Networks for One-Shot Classification and Open-Set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.06538v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06538v1)
- **Published**: 2019-03-11 02:50:27+00:00
- **Updated**: 2019-03-11 02:50:27+00:00
- **Authors**: Paresh Malalur, Tommi Jaakkola
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1% to 1.4% and in MiniImageNet from 53.5% to 46.5% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.



### HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1903.04120v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.04120v2)
- **Published**: 2019-03-11 04:20:38+00:00
- **Updated**: 2019-03-25 09:52:55+00:00
- **Authors**: Pravendra Singh, Vinay Kumar Verma, Piyush Rai, Vinay P. Namboodiri
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: We present a novel deep learning architecture in which the convolution operation leverages heterogeneous kernels. The proposed HetConv (Heterogeneous Kernel-Based Convolution) reduces the computation (FLOPs) and the number of parameters as compared to standard convolution operation while still maintaining representational efficiency. To show the effectiveness of our proposed convolution, we present extensive experimental results on the standard convolutional neural network (CNN) architectures such as VGG \cite{vgg2014very} and ResNet \cite{resnet}. We find that after replacing the standard convolutional filters in these architectures with our proposed HetConv filters, we achieve 3X to 8X FLOPs based improvement in speed while still maintaining (and sometimes improving) the accuracy. We also compare our proposed convolutions with group/depth wise convolutions and show that it achieves more FLOPs reduction with significantly higher accuracy.



### Manipulation by Feel: Touch-Based Control with Deep Predictive Models
- **Arxiv ID**: http://arxiv.org/abs/1903.04128v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.04128v1)
- **Published**: 2019-03-11 05:14:34+00:00
- **Updated**: 2019-03-11 05:14:34+00:00
- **Authors**: Stephen Tian, Frederik Ebert, Dinesh Jayaraman, Mayur Mudigonda, Chelsea Finn, Roberto Calandra, Sergey Levine
- **Comment**: Accepted to ICRA 2019
- **Journal**: None
- **Summary**: Touch sensing is widely acknowledged to be important for dexterous robotic manipulation, but exploiting tactile sensing for continuous, non-prehensile manipulation is challenging. General purpose control techniques that are able to effectively leverage tactile sensing as well as accurate physics models of contacts and forces remain largely elusive, and it is unclear how to even specify a desired behavior in terms of tactile percepts. In this paper, we take a step towards addressing these issues by combining high-resolution tactile sensing with data-driven modeling using deep neural network dynamics models. We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision. We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball, analog stick, and 20-sided die, learning from unsupervised autonomous interaction and then using the learned tactile predictive model to reposition each object to user-specified configurations, indicated by a goal tactile reading. Videos, visualizations and the code are available here: https://sites.google.com/view/deeptactilempc



### The Unconstrained Ear Recognition Challenge 2019 - ArXiv Version With Appendix
- **Arxiv ID**: http://arxiv.org/abs/1903.04143v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04143v3)
- **Published**: 2019-03-11 06:59:02+00:00
- **Updated**: 2019-03-14 04:47:49+00:00
- **Authors**: Žiga Emeršič, Aruna Kumar S. V., B. S. Harish, Weronika Gutfeter, Jalil Nourmohammadi Khiarak, Andrzej Pacut, Earnest Hansley, Mauricio Pamplona Segundo, Sudeep Sarkar, Hyeonjung Park, Gi Pyo Nam, Ig-Jae Kim, Sagar G. Sangodkar, Ümit Kaçar, Murvet Kirci, Li Yuan, Jishou Yuan, Haonan Zhao, Fei Lu, Junying Mao, Xiaoshuang Zhang, Dogucan Yaman, Fevziye Irem Eyiokur, Kadir Bulut Özler, Hazım Kemal Ekenel, Debbrota Paul Chowdhury, Sambit Bakshi, Pankaj K. Sa, Banshidhar Majhi, Peter Peer, Vitomir Štruc
- **Comment**: The content of this paper was published in ICB, 2019. This ArXiv
  version is from before the peer review
- **Journal**: None
- **Summary**: This paper presents a summary of the 2019 Unconstrained Ear Recognition Challenge (UERC), the second in a series of group benchmarking efforts centered around the problem of person recognition from ear images captured in uncontrolled settings. The goal of the challenge is to assess the performance of existing ear recognition techniques on a challenging large-scale ear dataset and to analyze performance of the technology from various viewpoints, such as generalization abilities to unseen data characteristics, sensitivity to rotations, occlusions and image resolution and performance bias on sub-groups of subjects, selected based on demographic criteria, i.e. gender and ethnicity. Research groups from 12 institutions entered the competition and submitted a total of 13 recognition approaches ranging from descriptor-based methods to deep-learning models. The majority of submissions focused on ensemble based methods combining either representations from multiple deep models or hand-crafted with learned image descriptors. Our analysis shows that methods incorporating deep learning models clearly outperform techniques relying solely on hand-crafted descriptors, even though both groups of techniques exhibit similar behaviour when it comes to robustness to various covariates, such presence of occlusions, changes in (head) pose, or variability in image resolution. The results of the challenge also show that there has been considerable progress since the first UERC in 2017, but that there is still ample room for further research in this area.



### Deep Generative Models: Deterministic Prediction with an Application in Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/1903.04144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04144v1)
- **Published**: 2019-03-11 07:01:51+00:00
- **Updated**: 2019-03-11 07:01:51+00:00
- **Authors**: Shima Kamyab, Rasool Sabzi, Zohreh Azimifar
- **Comment**: None
- **Journal**: None
- **Summary**: Deep generative models are stochastic neural networks capable of learning the distribution of data so as to generate new samples. Conditional Variational Autoencoder (CVAE) is a powerful deep generative model aiming at maximizing the lower bound of training data log-likelihood. In the CVAE structure, there is appropriate regularizer, which makes it applicable for suitably constraining the solution space in solving ill-posed problems and providing high generalization power. Considering the stochastic prediction characteristic in CVAE, depending on the problem at hand, it is desirable to be able to control the uncertainty in CVAE predictions. Therefore, in this paper we analyze the impact of CVAE's condition on the diversity of solutions given by our designed CVAE in 3D shape inverse rendering as a prediction problem. The experimental results using Modelnet10 and Shapenet datasets show the appropriate performance of our designed CVAE and verify the hypothesis: \emph{"The more informative the conditions in terms of object pose are, the less diverse the CVAE predictions are}".



### MSFD:Multi-Scale Receptive Field Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1903.04147v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04147v1)
- **Published**: 2019-03-11 07:13:31+00:00
- **Updated**: 2019-03-11 07:13:31+00:00
- **Authors**: Qiushan Guo, Yuan Dong, Yu Guo, Hongliang Bai
- **Comment**: Accepted by ICPR 2018
- **Journal**: None
- **Summary**: We aim to study the multi-scale receptive fields of a single convolutional neural network to detect faces of varied scales. This paper presents our Multi-Scale Receptive Field Face Detector (MSFD), which has superior performance on detecting faces at different scales and enjoys real-time inference speed. MSFD agglomerates context and texture by hierarchical structure. More additional information and rich receptive field bring significant improvement but generate marginal time consumption. We simultaneously propose an anchor assignment strategy which can cover faces with a wide range of scales to improve the recall rate of small faces and rotated faces. To reduce the false positive rate, we train our detector with focal loss which keeps the easy samples from overwhelming. As a result, MSFD reaches superior results on the FDDB, Pascal-Faces and WIDER FACE datasets, and can run at 31 FPS on GPU for VGA-resolution images.



### Investigation on Combining 3D Convolution of Image Data and Optical Flow to Generate Temporal Action Proposals
- **Arxiv ID**: http://arxiv.org/abs/1903.04176v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04176v2)
- **Published**: 2019-03-11 08:55:02+00:00
- **Updated**: 2019-03-14 10:42:51+00:00
- **Authors**: Patrick Schlosser, David Münch, Michael Arens
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action recognition utilizing 3D convolutions in combination with two-stream networks and based on the Single-Stream Temporal Action Proposals (SST) architecture, four different two-stream architectures utilizing sequences of images on one stream and sequences of images of optical flow on the other stream are subsequently investigated. The four architectures fuse the two separate streams at different depths in the model; for each of them, a broad range of parameters is investigated systematically as well as an optimal parametrization is empirically determined. The experiments on the THUMOS'14 dataset show that all four two-stream architectures are able to outperform the original single-stream SST and achieve state of the art results. Additional experiments revealed that the improvements are not restricted to a single method of calculating optical flow by exchanging the formerly used method of Brox with FlowNet2 and still achieving improvements.



### A cross-center smoothness prior for variational Bayesian brain tissue segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.04191v1
- **DOI**: 10.1007/978-3-030-20351-1_27
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.04191v1)
- **Published**: 2019-03-11 09:54:07+00:00
- **Updated**: 2019-03-11 09:54:07+00:00
- **Authors**: Wouter M. Kouw, Silas N. Ørting, Jens Petersen, Kim S. Pedersen, Marleen de Bruijne
- **Comment**: 12 pages, 2 figures, 1 table. Accepted to the International
  Conference on Information Processing in Medical Imaging (2019)
- **Journal**: International Conference on Information Processing in Medical
  Imaging (IPMI), Hong Kong, 2019, pp. 360-371
- **Summary**: Suppose one is faced with the challenge of tissue segmentation in MR images, without annotators at their center to provide labeled training data. One option is to go to another medical center for a trained classifier. Sadly, tissue classifiers do not generalize well across centers due to voxel intensity shifts caused by center-specific acquisition protocols. However, certain aspects of segmentations, such as spatial smoothness, remain relatively consistent and can be learned separately. Here we present a smoothness prior that is fit to segmentations produced at another medical center. This informative prior is presented to an unsupervised Bayesian model. The model clusters the voxel intensities, such that it produces segmentations that are similarly smooth to those of the other medical center. In addition, the unsupervised Bayesian model is extended to a semi-supervised variant, which needs no visual interpretation of clusters into tissues.



### Structured Knowledge Distillation for Dense Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.04197v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04197v7)
- **Published**: 2019-03-11 10:05:09+00:00
- **Updated**: 2020-06-14 13:37:24+00:00
- **Authors**: Yifan Liu, Changyong Shun, Jingdong Wang, Chunhua Shen
- **Comment**: v1:10 pages cvpr2019 accepted; v2:15 pages for a journal version;
  Code is available at:
  https://github.com/irfanICMLL/structure_knowledge_distillation; fix typos
- **Journal**: None
- **Summary**: In this work, we consider transferring the structure information from large networks to compact ones for dense prediction tasks in computer vision. Previous knowledge distillation strategies used for dense prediction tasks often directly borrow the distillation scheme for image classification and perform knowledge distillation for each pixel separately, leading to sub-optimal performance. Here we propose to distill structured knowledge from large networks to compact networks, taking into account the fact that dense prediction is a structured prediction problem. Specifically, we study two structured distillation schemes: i) pair-wise distillation that distills the pair-wise similarities by building a static graph; and ii) holistic distillation that uses adversarial training to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by experiments on three dense prediction tasks: semantic segmentation, depth estimation and object detection. Code is available at: https://git.io/StructKD



### Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.04202v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04202v2)
- **Published**: 2019-03-11 10:29:29+00:00
- **Updated**: 2019-04-20 13:52:36+00:00
- **Authors**: Andrea Pilzer, Stéphane Lathuilière, Nicu Sebe, Elisa Ricci
- **Comment**: Accepted at CVPR2019
- **Journal**: None
- **Summary**: Nowadays, the majority of state of the art monocular depth estimation techniques are based on supervised deep learning models. However, collecting RGB images with associated depth maps is a very time consuming procedure. Therefore, recent works have proposed deep architectures for addressing the monocular depth prediction task as a reconstruction problem, thus avoiding the need of collecting ground-truth depth. Following these works, we propose a novel self-supervised deep model for estimating depth maps. Our framework exploits two main strategies: refinement via cycle-inconsistency and distillation. Specifically, first a \emph{student} network is trained to predict a disparity map such as to recover from a frame in a camera view the associated image in the opposite view. Then, a backward cycle network is applied to the generated image to re-synthesize back the input image, estimating the opposite disparity. A third network exploits the inconsistency between the original and the reconstructed input frame in order to output a refined depth map. Finally, knowledge distillation is exploited, such as to transfer information from the refinement network to the student. Our extensive experimental evaluation demonstrate the effectiveness of the proposed framework which outperforms state of the art unsupervised methods on the KITTI benchmark.



### Distributed deep learning for robust multi-site segmentation of CT imaging after traumatic brain injury
- **Arxiv ID**: http://arxiv.org/abs/1903.04207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04207v1)
- **Published**: 2019-03-11 10:35:26+00:00
- **Updated**: 2019-03-11 10:35:26+00:00
- **Authors**: Samuel Remedios, Snehashis Roy, Justin Blaber, Camilo Bermudez, Vishwesh Nath, Mayur B. Patel, John A. Butman, Bennett A. Landman, Dzung L. Pham
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models are becoming commonplace in the domain of medical imaging, and with these methods comes an ever-increasing need for more data. However, to preserve patient anonymity it is frequently impractical or prohibited to transfer protected health information (PHI) between institutions. Additionally, due to the nature of some studies, there may not be a large public dataset available on which to train models. To address this conundrum, we analyze the efficacy of transferring the model itself in lieu of data between different sites. By doing so we accomplish two goals: 1) the model gains access to training on a larger dataset that it could not normally obtain and 2) the model better generalizes, having trained on data from separate locations. In this paper, we implement multi-site learning with disparate datasets from the National Institutes of Health (NIH) and Vanderbilt University Medical Center (VUMC) without compromising PHI. Three neural networks are trained to convergence on a computed tomography (CT) brain hematoma segmentation task: one only with NIH data,one only with VUMC data, and one multi-site model alternating between NIH and VUMC data. Resultant lesion masks with the multi-site model attain an average Dice similarity coefficient of 0.64 and the automatically segmented hematoma volumes correlate to those done manually with a Pearson correlation coefficient of 0.87,corresponding to an 8% and 5% improvement, respectively, over the single-site model counterparts.



### Pluralistic Image Completion
- **Arxiv ID**: http://arxiv.org/abs/1903.04227v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04227v2)
- **Published**: 2019-03-11 11:44:56+00:00
- **Updated**: 2019-04-05 15:24:19+00:00
- **Authors**: Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai
- **Comment**: 21 pages, 16 figures
- **Journal**: None
- **Summary**: Most image completion methods produce only one result for each masked input, although there may be many reasonable possibilities. In this paper, we present an approach for \textbf{pluralistic image completion} -- the task of generating multiple and diverse plausible solutions for image completion. A major challenge faced by learning-based approaches is that usually only one ground truth training instance per label. As such, sampling from conditional VAEs still leads to minimal diversity. To overcome this, we propose a novel and probabilistically principled framework with two parallel paths. One is a reconstructive path that utilizes the only one given ground truth to get prior distribution of missing parts and rebuild the original image from this distribution. The other is a generative path for which the conditional prior is coupled to the distribution obtained in the reconstructive path. Both are supported by GANs. We also introduce a new short+long term attention layer that exploits distant relations among decoder and encoder features, improving appearance consistency. When tested on datasets with buildings (Paris), faces (CelebA-HQ), and natural images (ImageNet), our method not only generated higher-quality completion results, but also with multiple and diverse plausible outputs.



### Instance- and Category-level 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.04229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04229v1)
- **Published**: 2019-03-11 11:45:46+00:00
- **Updated**: 2019-03-11 11:45:46+00:00
- **Authors**: Caner Sahin, Guillermo Garcia-Hernando, Juil Sock, Tae-Kyun Kim
- **Comment**: Book Chapter Submission. arXiv admin note: substantial text overlap
  with arXiv:1706.03285
- **Journal**: None
- **Summary**: 6D object pose estimation is an important task that determines the 3D position and 3D rotation of an object in camera-centred coordinates. By utilizing such a task, one can propose promising solutions for various problems related to scene understanding, augmented reality, control and navigation of robotics. Recent developments on visual depth sensors and low-cost availability of depth data significantly facilitate object pose estimation. Using depth information from RGB-D sensors, substantial progress has been made in the last decade by the methods addressing the challenges such as viewpoint variability, occlusion and clutter, and similar looking distractors. Particularly, with the recent advent of convolutional neural networks, RGB-only based solutions have been presented. However, improved results have only been reported for recovering the pose of known instances, i.e., for the instance-level object pose estimation tasks. More recently, state-of-the-art approaches target to solve object pose estimation problem at the level of categories, recovering the 6D pose of unknown instances. To this end, they address the challenges of the category-level tasks such as distribution shift among source and target domains, high intra-class variations, and shape discrepancies between objects.



### Similarity Learning via Kernel Preserving Embedding
- **Arxiv ID**: http://arxiv.org/abs/1903.04235v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.04235v1)
- **Published**: 2019-03-11 11:58:40+00:00
- **Updated**: 2019-03-11 11:58:40+00:00
- **Authors**: Zhao Kang, Yiwei Lu, Yuanzhang Su, Changsheng Li, Zenglin Xu
- **Comment**: Published in AAAI 2019
- **Journal**: None
- **Summary**: Data similarity is a key concept in many data-driven applications. Many algorithms are sensitive to similarity measures. To tackle this fundamental problem, automatically learning of similarity information from data via self-expression has been developed and successfully applied in various models, such as low-rank representation, sparse subspace learning, semi-supervised learning. However, it just tries to reconstruct the original data and some valuable information, e.g., the manifold structure, is largely ignored. In this paper, we argue that it is beneficial to preserve the overall relations when we extract similarity information. Specifically, we propose a novel similarity learning framework by minimizing the reconstruction error of kernel matrices, rather than the reconstruction error of original data adopted by existing work. Taking the clustering task as an example to evaluate our method, we observe considerable improvements compared to other state-of-the-art methods. More importantly, our proposed framework is very general and provides a novel and fundamental building block for many other similarity-based tasks. Besides, our proposed kernel preserving opens up a large number of possibilities to embed high-dimensional data into low-dimensional space.



### Manifold Mixup improves text recognition with CTC loss
- **Arxiv ID**: http://arxiv.org/abs/1903.04246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04246v1)
- **Published**: 2019-03-11 12:34:16+00:00
- **Updated**: 2019-03-11 12:34:16+00:00
- **Authors**: Bastien Moysset, Ronaldo Messina
- **Comment**: None
- **Journal**: None
- **Summary**: Modern handwritten text recognition techniques employ deep recurrent neural networks. The use of these techniques is especially efficient when a large amount of annotated data is available for parameter estimation. Data augmentation can be used to enhance the performance of the systems when data is scarce. Manifold Mixup is a modern method of data augmentation that meld two images or the feature maps corresponding to these images and the targets are fused accordingly. We propose to apply the Manifold Mixup to text recognition while adapting it to work with a Connectionist Temporal Classification cost. We show that Manifold Mixup improves text recognition results on various languages and datasets.



### A Unified Formulation for Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1903.04253v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04253v1)
- **Published**: 2019-03-11 12:44:14+00:00
- **Updated**: 2019-03-11 12:44:14+00:00
- **Authors**: Georges Younes, Daniel Asmar, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular Odometry systems can be broadly categorized as being either Direct, Indirect, or a hybrid of both. While Indirect systems process an alternative image representation to compute geometric residuals, Direct methods process the image pixels directly to generate photometric residuals. Both paradigms have distinct but often complementary properties. This paper presents a Unified Formulation for Visual Odometry, referred to as UFVO, with the following key contributions: (1) a tight coupling of photometric (Direct) and geometric (Indirect) measurements using a joint multi-objective optimization, (2) the use of a utility function as a decision maker that incorporates prior knowledge on both paradigms, (3) descriptor sharing, where a feature can have more than one type of descriptor and its different descriptors are used for tracking and mapping, (4) the depth estimation of both corner features and pixel features within the same map using an inverse depth parametrization, and (5) a corner and pixel selection strategy that extracts both types of information, while promoting a uniform distribution over the image domain. Experiments show that our proposed system can handle large inter-frame motions, inherits the sub-pixel accuracy of direct methods, can run efficiently in real-time, can generate an Indirect map representation at a marginal computational cost when compared to traditional Indirect systems, all while outperforming state of the art in Direct, Indirect and hybrid systems.



### Demonstration of Vector Flow Imaging using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.06254v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1903.06254v1)
- **Published**: 2019-03-11 13:01:38+00:00
- **Updated**: 2019-03-11 13:01:38+00:00
- **Authors**: Thomas Robins, Antonio Stanziola, Kai Reimer, Peter Weinberg, Meng-Xing Tang
- **Comment**: 2018 IEEE International Ultrasonics Symposium, Convolutional Neural
  Network, DeepLearning, Echo-PIV, UltrasoundToolbox, FieldII
- **Journal**: None
- **Summary**: Synthetic Aperture Vector Flow Imaging (SA-VFI) can visualize complex cardiac and vascular blood flow patterns at high temporal resolution with a large field of view. Convolutional neural networks (CNNs) are commonly used in image and video recognition and classification. However, most recently presented CNNs also allow for making per-pixel predictions as needed in optical flow velocimetry. To our knowledge we demonstrate here for the first time a CNN architecture to produce 2D full flow field predictions from high frame rate SA ultrasound images using supervised learning. The CNN was initially trained using CFD-generated and augmented noiseless SA ultrasound data of a realistic vessel geometry. Subsequently, a mix of noisy simulated and real \textit{in vivo} acquisitions were added to increase the network's robustness. The resulting flow field of the CNN resembled the ground truth accurately with an endpoint-error percentage between 6.5\% to 14.5\%. Furthermore, when confronted with an unknown geometry of an arterial bifurcation, the CNN was able to predict an accurate flow field indicating its ability for generalization. Remarkably, the CNN also performed well for rotational flows, which usually requires advanced, computationally intensive VFI methods. We have demonstrated that convolutional neural networks can be used to estimate complex multidirectional flow.



### Multi-Representational Learning for Offline Signature Verification using Multi-Loss Snapshot Ensemble of CNNs
- **Arxiv ID**: http://arxiv.org/abs/1903.06536v1
- **DOI**: 10.1016/j.eswa.2019.03.040
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06536v1)
- **Published**: 2019-03-11 14:11:21+00:00
- **Updated**: 2019-03-11 14:11:21+00:00
- **Authors**: Saeed Masoudnia, Omid Mersa, Babak N. Araabi, Abdol-Hossein Vahabie, Mohammad Amin Sadeghi, Majid Nili Ahmadabadi
- **Comment**: None
- **Journal**: Expert Systems with Applications, 2019, 133, 317-330
- **Summary**: Offline Signature Verification (OSV) is a challenging pattern recognition task, especially in presence of skilled forgeries that are not available during training. This study aims to tackle its challenges and meet the substantial need for generalization for OSV by examining different loss functions for Convolutional Neural Network (CNN). We adopt our new approach to OSV by asking two questions: 1. which classification loss provides more generalization for feature learning in OSV? , and 2. How integration of different losses into a unified multi-loss function lead to an improved learning framework? These questions are studied based on analysis of three loss functions, including cross entropy, Cauchy-Schwarz divergence, and hinge loss. According to complementary features of these losses, we combine them into a dynamic multi-loss function and propose a novel ensemble framework for simultaneous use of them in CNN. Our proposed Multi-Loss Snapshot Ensemble (MLSE) consists of several sequential trials. In each trial, a dominant loss function is selected from the multi-loss set, and the remaining losses act as a regularizer. Different trials learn diverse representations for each input based on signature identification task. This multi-representation set is then employed for the verification task. An ensemble of SVMs is trained on these representations, and their decisions are finally combined according to the selection of most generalizable SVM for each user. We conducted two sets of experiments based on two different protocols of OSV, i.e., writer-dependent and writer-independent on three signature datasets: GPDS-Synthetic, MCYT, and UT-SIG. Based on the writer-dependent OSV protocol, we achieved substantial improvements over the best EERs in the literature. The results of the second set of experiments also confirmed the robustness to the arrival of new users enrolled in the OSV system.



### Joint inference on structural and diffusion MRI for sequence-adaptive Bayesian segmentation of thalamic nuclei with probabilistic atlases
- **Arxiv ID**: http://arxiv.org/abs/1903.04352v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04352v1)
- **Published**: 2019-03-11 15:11:28+00:00
- **Updated**: 2019-03-11 15:11:28+00:00
- **Authors**: Juan Eugenio Iglesias, Koen Van Leemput, Polina Golland, Anastasia Yendiki
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Segmentation of structural and diffusion MRI (sMRI/dMRI) is usually performed independently in neuroimaging pipelines. However, some brain structures (e.g., globus pallidus, thalamus and its nuclei) can be extracted more accurately by fusing the two modalities. Following the framework of Bayesian segmentation with probabilistic atlases and unsupervised appearance modeling, we present here a novel algorithm to jointly segment multi-modal sMRI/dMRI data. We propose a hierarchical likelihood term for the dMRI defined on the unit ball, which combines the Beta and Dimroth-Scheidegger-Watson distributions to model the data at each voxel. This term is integrated with a mixture of Gaussians for the sMRI data, such that the resulting joint unsupervised likelihood enables the analysis of multi-modal scans acquired with any type of MRI contrast, b-values, or number of directions, which enables wide applicability. We also propose an inference algorithm to estimate the maximum-a-posteriori model parameters from input images, and to compute the most likely segmentation. Using a recently published atlas derived from histology, we apply our method to thalamic nuclei segmentation on two datasets: HCP (state of the art) and ADNI (legacy) - producing lower sample sizes than Bayesian segmentation with sMRI alone.



### ADS-ME: Anomaly Detection System for Micro-expression Spotting
- **Arxiv ID**: http://arxiv.org/abs/1903.04354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04354v1)
- **Published**: 2019-03-11 15:11:56+00:00
- **Updated**: 2019-03-11 15:11:56+00:00
- **Authors**: Dawood Al Chanti, Alice Caplier
- **Comment**: 35 pages, 9 figures, 3 tables
- **Journal**: None
- **Summary**: Micro-expressions (MEs) are infrequent and uncontrollable facial events that can highlight emotional deception and appear in a high-stakes environment. This paper propose an algorithm for spatiotemporal MEs spotting. Since MEs are unusual events, we treat them as abnormal patterns that diverge from expected Normal Facial Behaviour (NFBs) patterns. NFBs correspond to facial muscle activations, eye blink/gaze events and mouth opening/closing movements that are all facial deformation but not MEs. We propose a probabilistic model to estimate the probability density function that models the spatiotemporal distributions of NFBs patterns. To rank the outputs, we compute the negative log-likelihood and we developed an adaptive thresholding technique to identify MEs from NFBs. While working only with NFBs data, the main challenge is to capture intrinsic spatiotemoral features, hence we design a recurrent convolutional autoencoder for feature representation. Finally, we show that our system is superior to previous works for MEs spotting.



### Accuracy Booster: Performance Boosting using Feature Map Re-calibration
- **Arxiv ID**: http://arxiv.org/abs/1903.04407v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.04407v2)
- **Published**: 2019-03-11 16:16:03+00:00
- **Updated**: 2020-01-07 10:44:44+00:00
- **Authors**: Pravendra Singh, Pratik Mazumder, Vinay P. Namboodiri
- **Comment**: IEEE Winter Conference on Applications of Computer Vision (WACV),
  2020
- **Journal**: None
- **Summary**: Convolution Neural Networks (CNN) have been extremely successful in solving intensive computer vision tasks. The convolutional filters used in CNNs have played a major role in this success, by extracting useful features from the inputs. Recently researchers have tried to boost the performance of CNNs by re-calibrating the feature maps produced by these filters, e.g., Squeeze-and-Excitation Networks (SENets). These approaches have achieved better performance by Exciting up the important channels or feature maps while diminishing the rest. However, in the process, architectural complexity has increased. We propose an architectural block that introduces much lower complexity than the existing methods of CNN performance boosting while performing significantly better than them. We carry out experiments on the CIFAR, ImageNet and MS-COCO datasets, and show that the proposed block can challenge the state-of-the-art results. Our method boosts the ResNet-50 architecture to perform comparably to the ResNet-152 architecture, which is a three times deeper network, on classification. We also show experimentally that our method is not limited to classification but also generalizes well to other tasks such as object detection.



### Learning to Paint With Model-based Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.04411v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.04411v3)
- **Published**: 2019-03-11 16:21:46+00:00
- **Updated**: 2019-08-16 08:02:02+00:00
- **Authors**: Zhewei Huang, Wen Heng, Shuchang Zhou
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.



### The Past and the Present of the Color Checker Dataset Misuse
- **Arxiv ID**: http://arxiv.org/abs/1903.04473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04473v1)
- **Published**: 2019-03-11 17:50:17+00:00
- **Updated**: 2019-03-11 17:50:17+00:00
- **Authors**: Nikola Banić, Karlo Koš{č}ević, Marko Subašić, Sven Lon{č}arić
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: The pipelines of digital cameras contain a part for computational color constancy, which aims to remove the influence of the illumination on the scene colors. One of the best known and most widely used benchmark datasets for this problem is the Color Checker dataset. However, due to the improper handling of the black level in its images, this dataset has been widely misused and while some recent publications tried to alleviate the problem, they nevertheless erred and created additional wrong data. This paper gives a history of the Color Checker dataset usage, it describes the origins and reasons for its misuses, and it explains the old and new mistakes introduced in the most recent publications that tried to handle the issue. This should, hopefully, help to prevent similar future misuses.



### Video Generation from Single Semantic Label Map
- **Arxiv ID**: http://arxiv.org/abs/1903.04480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04480v1)
- **Published**: 2019-03-11 17:56:34+00:00
- **Updated**: 2019-03-11 17:56:34+00:00
- **Authors**: Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang
- **Comment**: Paper accepted at CVPR 2019. Source code and models available at
  https://github.com/junting/seg2vid/tree/master
- **Journal**: None
- **Summary**: This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain the temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods.



### GOGGLES: Automatic Image Labeling with Affinity Coding
- **Arxiv ID**: http://arxiv.org/abs/1903.04552v3
- **DOI**: 10.1145/3318464.3380592
- **Categories**: **cs.CV**, cs.DB
- **Links**: [PDF](http://arxiv.org/pdf/1903.04552v3)
- **Published**: 2019-03-11 19:19:30+00:00
- **Updated**: 2020-03-03 06:30:24+00:00
- **Authors**: Nilaksh Das, Sanya Chaba, Renzhi Wu, Sakshi Gandhi, Duen Horng Chau, Xu Chu
- **Comment**: Published at 2020 ACM SIGMOD International Conference on Management
  of Data
- **Journal**: None
- **Summary**: Generating large labeled training data is becoming the biggest bottleneck in building and deploying supervised machine learning models. Recently, the data programming paradigm has been proposed to reduce the human cost in labeling training data. However, data programming relies on designing labeling functions which still requires significant domain expertise. Also, it is prohibitively difficult to write labeling functions for image datasets as it is hard to express domain knowledge using raw features for images (pixels).   We propose affinity coding, a new domain-agnostic paradigm for automated training data labeling. The core premise of affinity coding is that the affinity scores of instance pairs belonging to the same class on average should be higher than those of pairs belonging to different classes, according to some affinity functions. We build the GOGGLES system that implements affinity coding for labeling image datasets by designing a novel set of reusable affinity functions for images, and propose a novel hierarchical generative model for class inference using a small development set.   We compare GOGGLES with existing data programming systems on 5 image labeling tasks from diverse domains. GOGGLES achieves labeling accuracies ranging from a minimum of 71% to a maximum of 98% without requiring any extensive human annotation. In terms of end-to-end performance, GOGGLES outperforms the state-of-the-art data programming system Snuba by 21% and a state-of-the-art few-shot learning technique by 5%, and is only 7% away from the fully supervised upper bound.



### Generating superpixels using deep image representations
- **Arxiv ID**: http://arxiv.org/abs/1903.04586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04586v1)
- **Published**: 2019-03-11 20:24:30+00:00
- **Updated**: 2019-03-11 20:24:30+00:00
- **Authors**: Thomas Verelst, Matthew Blaschko, Maxim Berman
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixel algorithms are a common pre-processing step for computer vision algorithms such as segmentation, object tracking and localization. Many superpixel methods only rely on colors features for segmentation, limiting performance in low-contrast regions and applicability to infrared or medical images where object boundaries have wide appearance variability. We study the inclusion of deep image features in the SLIC superpixel algorithm to exploit higher-level image representations. In addition, we devise a trainable superpixel algorithm, yielding an intermediate domain-specific image representation that can be applied to different tasks. A clustering-based superpixel algorithm is transformed into a pixel-wise classification task and superpixel training data is derived from semantic segmentation datasets. Our results demonstrate that this approach is able to improve superpixel quality consistently.



### Quality-Gated Convolutional LSTM for Enhancing Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/1903.04596v3
- **DOI**: 10.1109/ICME.2019.00098
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04596v3)
- **Published**: 2019-03-11 20:44:53+00:00
- **Updated**: 2019-04-14 08:50:34+00:00
- **Authors**: Ren Yang, Xiaoyan Sun, Mai Xu, Wenjun Zeng
- **Comment**: Accepted to IEEE International Conference on Multimedia and Expo
  (ICME) 2019
- **Journal**: None
- **Summary**: The past decade has witnessed great success in applying deep learning to enhance the quality of compressed video. However, the existing approaches aim at quality enhancement on a single frame, or only using fixed neighboring frames. Thus they fail to take full advantage of the inter-frame correlation in the video. This paper proposes the Quality-Gated Convolutional Long Short-Term Memory (QG-ConvLSTM) network with bi-directional recurrent structure to fully exploit the advantageous information in a large range of frames. More importantly, due to the obvious quality fluctuation among compressed frames, higher quality frames can provide more useful information for other frames to enhance quality. Therefore, we propose learning the "forget" and "input" gates in the ConvLSTM cell from quality-related features. As such, the frames with various quality contribute to the memory in ConvLSTM with different importance, making the information of each frame reasonably and adequately used. Finally, the experiments validate the effectiveness of our QG-ConvLSTM approach in advancing the state-of-the-art quality enhancement of compressed video, and the ablation study shows that our QG-ConvLSTM approach is learnt to make a trade-off between quality and correlation when leveraging multi-frame information. The project page: https://github.com/ryangchn/QG-ConvLSTM.git.



### Fast Registration for cross-source point clouds by using weak regional affinity and pixel-wise refinement
- **Arxiv ID**: http://arxiv.org/abs/1903.04630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1903.04630v1)
- **Published**: 2019-03-11 22:13:46+00:00
- **Updated**: 2019-03-11 22:13:46+00:00
- **Authors**: Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan
- **Comment**: ICME 2019
- **Journal**: None
- **Summary**: Many types of 3D acquisition sensors have emerged in recent years and point cloud has been widely used in many areas. Accurate and fast registration of cross-source 3D point clouds from different sensors is an emerged research problem in computer vision. This problem is extremely challenging because cross-source point clouds contain a mixture of various variances, such as density, partial overlap, large noise and outliers, viewpoint changing. In this paper, an algorithm is proposed to align cross-source point clouds with both high accuracy and high efficiency. There are two main contributions: firstly, two components, the weak region affinity and pixel-wise refinement, are proposed to maintain the global and local information of 3D point clouds. Then, these two components are integrated into an iterative tensor-based registration algorithm to solve the cross-source point cloud registration problem. We conduct experiments on synthetic cross-source benchmark dataset and real cross-source datasets. Comparison with six state-of-the-art methods, the proposed method obtains both higher efficiency and accuracy.



