# Arxiv Papers in cs.CV on 2019-03-03
### Keyframe-based Direct Thermal-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1903.00798v1
- **DOI**: 10.1109/ICRA.2019.8793927
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00798v1)
- **Published**: 2019-03-03 00:25:12+00:00
- **Updated**: 2019-03-03 00:25:12+00:00
- **Authors**: Shehryar Khattak, Christos Papachristos, Kostas Alexis
- **Comment**: 7 pages, 8 figures, Accepted at International Conference on Robotics
  and Automation (ICRA) 2019
- **Journal**: 2019 International Conference on Robotics and Automation (ICRA)
- **Summary**: This paper proposes an approach for fusing direct radiometric data from a thermal camera with inertial measurements to extend the robotic capabilities of aerial robots for navigation in GPS-denied and visually degraded environments in the conditions of darkness and in the presence of airborne obscurants such as dust, fog and smoke. An optimization based approach is developed that jointly minimizes the re-projection error of 3D landmarks and inertial measurement errors. The developed solution is extensively verified against both ground-truth in an indoor laboratory setting, as well as inside an underground mine under severely visually degraded conditions.



### 3D Hand Shape and Pose Estimation from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1903.00812v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00812v2)
- **Published**: 2019-03-03 02:27:13+00:00
- **Updated**: 2019-04-20 07:46:49+00:00
- **Authors**: Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, Junsong Yuan
- **Comment**: CVPR 2019 (Oral), project page:
  https://sites.google.com/site/geliuhaontu/home/cvpr2019
- **Journal**: None
- **Summary**: This work addresses a novel and challenging problem of estimating the full 3D hand shape and pose from a single RGB image. Most current methods in 3D hand analysis from monocular RGB images only focus on estimating the 3D locations of hand keypoints, which cannot fully express the 3D shape of hand. In contrast, we propose a Graph Convolutional Neural Network (Graph CNN) based method to reconstruct a full 3D mesh of hand surface that contains richer information of both 3D hand shape and pose. To train networks with full supervision, we create a large-scale synthetic dataset containing both ground truth 3D meshes and 3D poses. When fine-tuning the networks on real-world datasets without 3D ground truth, we propose a weakly-supervised approach by leveraging the depth map as a weak supervision in training. Through extensive evaluations on our proposed new datasets and two public datasets, we show that our proposed method can produce accurate and reasonable 3D hand mesh, and can achieve superior 3D hand pose estimation accuracy when compared with state-of-the-art methods.



### Visual-based Autonomous Driving Deployment from a Stochastic and Uncertainty-aware Perspective
- **Arxiv ID**: http://arxiv.org/abs/1903.00821v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00821v2)
- **Published**: 2019-03-03 03:59:41+00:00
- **Updated**: 2019-07-18 06:27:18+00:00
- **Authors**: Lei Tai, Peng Yun, Yuying Chen, Congcong Liu, Haoyang Ye, Ming Liu
- **Comment**: IROS 2019 camera-ready version, 7 pages, video:
  https://youtu.be/ZYtsb9-1zXk
- **Journal**: None
- **Summary**: End-to-end visual-based imitation learning has been widely applied in autonomous driving. When deploying the trained visual-based driving policy, a deterministic command is usually directly applied without considering the uncertainty of the input data. Such kind of policies may bring dramatical damage when applied in the real world. In this paper, we follow the recent real-to-sim pipeline by translating the testing world image back to the training domain when using the trained policy. In the translating process, a stochastic generator is used to generate various images stylized under the training domain randomly or directionally. Based on those translated images, the trained uncertainty-aware imitation learning policy would output both the predicted action and the data uncertainty motivated by the aleatoric loss function. Through the uncertainty-aware imitation learning policy, we can easily choose the safest one with the lowest uncertainty among the generated images. Experiments in the Carla navigation benchmark show that our strategy outperforms previous methods, especially in dynamic environments.



### A Model-Driven Stack-Based Fully Convolutional Network for Pancreas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.00832v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00832v3)
- **Published**: 2019-03-03 04:52:49+00:00
- **Updated**: 2020-10-17 01:24:22+00:00
- **Authors**: Hao Li, Jun Li, Xiaozhu Lin, Xiaohua Qian
- **Comment**: None
- **Journal**: None
- **Summary**: The irregular geometry and high inter-slice variability in computerized tomography (CT) scans of the human pancreas make an accurate segmentation of this crucial organ a challenging task for existing data-driven deep learning methods. To address this problem, we present a novel model-driven stack-based fully convolutional network with a sliding window fusion algorithm for pancreas segmentation, termed MDS-Net. The MDS-Net's cost function includes a data approximation term and a prior knowledge regularization term combined with a stack scheme for capturing and fusing the two-dimensional (2D) and local three-dimensional (3D) context information. Specifically, 3D CT scans are divided into multiple stacks to capture the local spatial context feature. To highlight the importance of single slices, the inter-slice relationships in the stack data are also incorporated in the MDS-Net framework. For implementing this proposed model-driven method, we create a stack-based U-Net architecture and successfully derive its back-propagation procedure for end-to-end training. Furthermore, a sliding window fusion algorithm is utilized to improve the consistency of adjacent CT slices and intra-stack. Finally, extensive quantitative assessments on the NIH Pancreas-CT dataset demonstrated higher pancreatic segmentation accuracy and reliability of MDS-Net compared to other state-of-the-art methods.



### Image Super-Resolution by Neural Texture Transfer
- **Arxiv ID**: http://arxiv.org/abs/1903.00834v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00834v2)
- **Published**: 2019-03-03 05:06:09+00:00
- **Updated**: 2019-03-06 20:55:14+00:00
- **Authors**: Zhifei Zhang, Zhaowen Wang, Zhe Lin, Hairong Qi
- **Comment**: Project Page:
  http://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html. arXiv admin
  note: text overlap with arXiv:1804.03360
- **Journal**: None
- **Summary**: Due to the significant information loss in low-resolution (LR) images, it has become extremely challenging to further advance the state-of-the-art of single image super-resolution (SISR). Reference-based super-resolution (RefSR), on the other hand, has proven to be promising in recovering high-resolution (HR) details when a reference (Ref) image with similar content as that of the LR input is given. However, the quality of RefSR can degrade severely when Ref is less similar. This paper aims to unleash the potential of RefSR by leveraging more texture details from Ref images with stronger robustness even when irrelevant Ref images are provided. Inspired by the recent work on image stylization, we formulate the RefSR problem as neural texture transfer. We design an end-to-end deep model which enriches HR details by adaptively transferring the texture from Ref images according to their textural similarity. Instead of matching content in the raw pixel space as done by previous methods, our key contribution is a multi-level matching conducted in the neural space. This matching scheme facilitates multi-scale neural transfer that allows the model to benefit more from those semantically related Ref patches, and gracefully degrade to SISR performance on the least relevant Ref inputs. We build a benchmark dataset for the general research of RefSR, which contains Ref images paired with LR inputs with varying levels of similarity. Both quantitative and qualitative evaluations demonstrate the superiority of our method over state-of-the-art.



### Improving Referring Expression Grounding with Cross-modal Attention-guided Erasing
- **Arxiv ID**: http://arxiv.org/abs/1903.00839v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1903.00839v2)
- **Published**: 2019-03-03 05:55:15+00:00
- **Updated**: 2019-04-02 07:33:37+00:00
- **Authors**: Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Referring expression grounding aims at locating certain objects or persons in an image with a referring expression, where the key challenge is to comprehend and align various types of information from visual and textual domain, such as visual attributes, location and interactions with surrounding regions. Although the attention mechanism has been successfully applied for cross-modal alignments, previous attention models focus on only the most dominant features of both modalities, and neglect the fact that there could be multiple comprehensive textual-visual correspondences between images and referring expressions. To tackle this issue, we design a novel cross-modal attention-guided erasing approach, where we discard the most dominant information from either textual or visual domains to generate difficult training samples online, and to drive the model to discover complementary textual-visual correspondences. Extensive experiments demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance on three referring expression grounding datasets.



### Crowd Counting and Density Estimation by Trellis Encoder-Decoder Network
- **Arxiv ID**: http://arxiv.org/abs/1903.00853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00853v2)
- **Published**: 2019-03-03 08:04:26+00:00
- **Updated**: 2019-04-19 06:27:59+00:00
- **Authors**: Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, Ling Shao
- **Comment**: CVPR 2019, Accepted
- **Journal**: None
- **Summary**: Crowd counting has recently attracted increasing interest in computer vision but remains a challenging problem. In this paper, we propose a trellis encoder-decoder network (TEDnet) for crowd counting, which focuses on generating high-quality density estimation maps. The major contributions are four-fold. First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which can handle large variations of objects. Second, we design dense skip connections interleaved across paths to facilitate sufficient multi-scale feature fusions and to absorb the supervision information. Third, we propose a new combinatorial loss to enforce local coherence and spatial correlation in density maps. By distributedly imposing this combinatorial loss on intermediate outputs, gradient vanishing can be largely alleviated for better back-propagation and faster convergence. Finally, our TEDnet achieves new state-of-the art performance on four benchmarks, with an improvement up to 14% in terms of MAE.



### CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing Imagery
- **Arxiv ID**: http://arxiv.org/abs/1903.00857v1
- **DOI**: 10.1109/TGRS.2019.2930982
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00857v1)
- **Published**: 2019-03-03 08:16:11+00:00
- **Updated**: 2019-03-03 08:16:11+00:00
- **Authors**: Gongjie Zhang, Shijian Lu, Wei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications such as urban planning, traffic control, searching and rescuing, etc. However, state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in term of sparse texture, low contrast, arbitrary orientations, large scale variations, etc. This paper presents a novel object detection network (CAD-Net) that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their correlations with the global scene (at scene-level) and the local neighboring objects or features (at object-level), respectively. In addition, it designs a spatial-and-scale-aware attention module that guides the network to focus on more informative regions and features as well as more appropriate feature scales. Experiments over two publicly available object detection datasets for remote sensing images demonstrate that the proposed CAD-Net achieves superior detection performance. The implementation codes will be made publicly available for facilitating future researches.



### Recognition of Multiple Food Items in a Single Photo for Use in a Buffet-Style Restaurant
- **Arxiv ID**: http://arxiv.org/abs/1903.00858v1
- **DOI**: 10.1587/transinf.2018EDL8183
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00858v1)
- **Published**: 2019-03-03 08:24:55+00:00
- **Updated**: 2019-03-03 08:24:55+00:00
- **Authors**: Masashi Anzawa, Sosuke Amano, Yoko Yamakata, Keiko Motonaga, Akiko Kamei, Kiyoharu Aizawa
- **Comment**: 5 pages, 7 figures
- **Journal**: IEICE TRANSACTIONS on Information and Systems, 2019
- **Summary**: We investigate image recognition of multiple food items in a single photo, focusing on a buffet restaurant application, where menu changes at every meal, and only a few images per class are available. After detecting food areas, we perform hierarchical recognition. We evaluate our results, comparing to two baseline methods.



### Less is More: Learning Highlight Detection from Video Duration
- **Arxiv ID**: http://arxiv.org/abs/1903.00859v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00859v1)
- **Published**: 2019-03-03 08:34:16+00:00
- **Updated**: 2019-03-03 08:34:16+00:00
- **Authors**: Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, Kristen Grauman
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Highlight detection has the potential to significantly ease video browsing, but existing methods often suffer from expensive supervision requirements, where human viewers must manually identify highlights in training videos. We propose a scalable unsupervised solution that exploits video duration as an implicit supervision signal. Our key insight is that video segments from shorter user-generated videos are more likely to be highlights than those from longer videos, since users tend to be more selective about the content when capturing shorter videos. Leveraging this insight, we introduce a novel ranking framework that prefers segments from shorter videos, while properly accounting for the inherent noise in the (unlabeled) training data. We use it to train a highlight detector with 10M hashtagged Instagram videos. In experiments on two challenging public video highlight detection benchmarks, our method substantially improves the state-of-the-art for unsupervised highlight detection.



### Face Image Reflection Removal
- **Arxiv ID**: http://arxiv.org/abs/1903.00865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00865v1)
- **Published**: 2019-03-03 09:11:17+00:00
- **Updated**: 2019-03-03 09:11:17+00:00
- **Authors**: Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, Alex C. Kot
- **Comment**: None
- **Journal**: None
- **Summary**: Face images captured through the glass are usually contaminated by reflections. The non-transmitted reflections make the reflection removal more challenging than for general scenes, because important facial features are completely occluded. In this paper, we propose and solve the face image reflection removal problem. We remove non-transmitted reflections by incorporating inpainting ideas into a guided reflection removal framework and recover facial features by considering various face-specific priors. We use a newly collected face reflection image dataset to train our model and compare with state-of-the-art methods. The proposed method shows advantages in estimating reflection-free face images for improving face recognition.



### Meta-SR: A Magnification-Arbitrary Network for Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.00875v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00875v4)
- **Published**: 2019-03-03 10:17:45+00:00
- **Updated**: 2019-04-03 15:10:13+00:00
- **Authors**: Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, Jian Sun
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Recent research on super-resolution has achieved great success due to the development of deep convolutional neural networks (DCNNs). However, super-resolution of arbitrary scale factor has been ignored for a long time. Most previous researchers regard super-resolution of different scale factors as independent tasks. They train a specific model for each scale factor which is inefficient in computing, and prior work only take the super-resolution of several integer scale factors into consideration. In this work, we propose a novel method called Meta-SR to firstly solve super-resolution of arbitrary scale factor (including non-integer scale factors) with a single model. In our Meta-SR, the Meta-Upscale Module is proposed to replace the traditional upscale module. For arbitrary scale factor, the Meta-Upscale Module dynamically predicts the weights of the upscale filters by taking the scale factor as input and use these weights to generate the HR image of arbitrary size. For any low-resolution image, our Meta-SR can continuously zoom in it with arbitrary scale factor by only using a single model. We evaluated the proposed method through extensive experiments on widely used benchmark datasets on single image super-resolution. The experimental results show the superiority of our Meta-Upscale.



### 3D convolutional neural network for abdominal aortic aneurysm segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.00879v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T, 68U
- **Links**: [PDF](http://arxiv.org/pdf/1903.00879v1)
- **Published**: 2019-03-03 10:55:48+00:00
- **Updated**: 2019-03-03 10:55:48+00:00
- **Authors**: Karen López-Linares, Inmaculada García, Ainhoa García-Familiar, Iván Macía, Miguel A. González Ballester
- **Comment**: Submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: An abdominal aortic aneurysm (AAA) is a focal dilation of the aorta that, if not treated, tends to grow and may rupture. A significant unmet need in the assessment of AAA disease, for the diagnosis, prognosis and follow-up, is the determination of rupture risk, which is currently based on the manual measurement of the aneurysm diameter in a selected Computed Tomography Angiography (CTA) scan. However, there is a lack of standardization determining the degree and rate of disease progression, due to the lack of robust, automated aneurysm segmentation tools that allow quantitatively analyzing the AAA. In this work, we aim at proposing the first 3D convolutional neural network for the segmentation of aneurysms both from preoperative and postoperative CTA scans. We extensively validate its performance in terms of diameter measurements, to test its applicability in the clinical practice, as well as regarding the relative volume difference, and Dice and Jaccard scores. The proposed method yields a mean diameter measurement error of 3.3 mm, a relative volume difference of 8.58 %, and Dice and Jaccard scores of 87 % and 77 %, respectively. At a clinical level, an aneurysm enlargement of 10 mm is considered relevant, thus, our method is suitable to automatically determine the AAA diameter and opens up the opportunity for more complex aneurysm analysis.



### MILDNet: A Lightweight Single Scaled Deep Ranking Architecture
- **Arxiv ID**: http://arxiv.org/abs/1903.00905v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.LG, I.2.10; I.4; I.5; I.2.8; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1903.00905v2)
- **Published**: 2019-03-03 13:26:37+00:00
- **Updated**: 2019-03-14 02:54:09+00:00
- **Authors**: Anirudha Vishvakarma
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Multi-scale deep CNN architecture [1, 2, 3] successfully captures both fine and coarse level image descriptors for visual similarity task, but they come up with expensive memory overhead and latency. In this paper, we propose a competing novel CNN architecture, called MILDNet, which merits by being vastly compact (about 3 times). Inspired by the fact that successive CNN layers represent the image with increasing levels of abstraction, we compressed our deep ranking model to a single CNN by coupling activations from multiple intermediate layers along with the last layer. Trained on the famous Street2shop dataset [4], we demonstrate that our approach performs as good as the current state-of-the-art models with only one third of the parameters, model size, training time and significant reduction in inference time. The significance of intermediate layers on image retrieval task has also been shown to be performing on popular datasets Holidays, Oxford, Paris [5]. So even though our experiments are done on ecommerce domain, it is applicable to other domains as well. We further did an ablation study to validate our hypothesis by checking the impact on adding each intermediate layer. With this we also present two more useful variants of MILDNet, a mobile model (12 times smaller) for on-edge devices and a compactly featured model (512-d feature embeddings) for systems with less RAMs and to reduce the ranking cost. Further we present an intuitive way to automatically create a tailored in-house triplet training dataset, which is very hard to create manually. This solution too can also be deployed as an all-inclusive visual similarity solution. Finally, we present our entire production level architecture which currently powers visual similarity at Fynd.



### Ground Plane based Absolute Scale Estimation for Monocular Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1903.00912v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00912v1)
- **Published**: 2019-03-03 13:45:50+00:00
- **Updated**: 2019-03-03 13:45:50+00:00
- **Authors**: Dingfu Zhou, Yuchao Dai, Hongdong Li
- **Comment**: Accepted by IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Recovering the absolute metric scale from a monocular camera is a challenging but highly desirable problem for monocular camera-based systems. By using different kinds of cues, various approaches have been proposed for scale estimation, such as camera height, object size etc. In this paper, firstly, we summarize different kinds of scale estimation approaches. Then, we propose a robust divide and conquer the absolute scale estimation method based on the ground plane and camera height by analyzing the advantages and disadvantages of different approaches. By using the estimated scale, an effective scale correction strategy has been proposed to reduce the scale drift during the Monocular Visual Odometry (VO) estimation process. Finally, the effectiveness and robustness of the proposed method have been verified on both public and self-collected image sequences.



### Unsupervised Bi-directional Flow-based Video Generation from one Snapshot
- **Arxiv ID**: http://arxiv.org/abs/1903.00913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00913v1)
- **Published**: 2019-03-03 13:48:32+00:00
- **Updated**: 2019-03-03 13:48:32+00:00
- **Authors**: Lu Sheng, Junting Pan, Jiaming Guo, Jing Shao, Xiaogang Wang, Chen Change Loy
- **Comment**: 11 pages, 12 figures. Technical report for a project in progress
- **Journal**: None
- **Summary**: Imagining multiple consecutive frames given one single snapshot is challenging, since it is difficult to simultaneously predict diverse motions from a single image and faithfully generate novel frames without visual distortions. In this work, we leverage an unsupervised variational model to learn rich motion patterns in the form of long-term bi-directional flow fields, and apply the predicted flows to generate high-quality video sequences. In contrast to the state-of-the-art approach, our method does not require external flow supervisions for learning. This is achieved through a novel module that performs bi-directional flows prediction from a single image. In addition, with the bi-directional flow consistency check, our method can handle occlusion and warping artifacts in a principled manner. Our method can be trained end-to-end based on arbitrarily sampled natural video clips, and it is able to capture multi-modal motion uncertainty and synthesizes photo-realistic novel sequences. Quantitative and qualitative evaluations over synthetic and real-world datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods.



### Pancreas segmentation with probabilistic map guided bi-directional recurrent UNet
- **Arxiv ID**: http://arxiv.org/abs/1903.00923v6
- **DOI**: 10.1088/1361-6560/abfce3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00923v6)
- **Published**: 2019-03-03 15:13:33+00:00
- **Updated**: 2022-08-11 15:33:22+00:00
- **Authors**: Jun Li, Xiaozhu Lin, Hui Che, Hao Li, Xiaohua Qian
- **Comment**: accepted by Physics in Medicine & Biology
- **Journal**: Physics in Medicine & Biology, 66(11), 115010 (2021)
- **Summary**: Pancreas segmentation in medical imaging data is of great significance for clinical pancreas diagnostics and treatment. However, the large population variations in the pancreas shape and volume cause enormous segmentation difficulties, even for state-of-the-art algorithms utilizing fully-convolutional neural networks (FCNs). Specifically, pancreas segmentation suffers from the loss of spatial information in 2D methods, and the high computational cost of 3D methods. To alleviate these problems, we propose a probabilistic-map-guided bi-directional recurrent UNet (PBR-UNet) architecture, which fuses intra-slice information and inter-slice probabilistic maps into a local 3D hybrid regularization scheme, which is followed by bi-directional recurrent network optimization. The PBR-UNet method consists of an initial estimation module for efficiently extracting pixel-level probabilistic maps and a primary segmentation module for propagating hybrid information through a 2.5D U-Net architecture. Specifically, local 3D information is inferred by combining an input image with the probabilistic maps of the adjacent slices into multichannel hybrid data, and then hierarchically aggregating the hybrid information of the entire segmentation network. Besides, a bi-directional recurrent optimization mechanism is developed to update the hybrid information in both the forward and the backward directions. This allows the proposed network to make full and optimal use of the local context information. Quantitative and qualitative evaluation was performed on the NIH Pancreas-CT dataset, and our proposed PBR-UNet method achieved better segmentation results with less computational cost compared to other state-of-the-art methods.



### Accelerating Training of Deep Neural Networks with a Standardization Loss
- **Arxiv ID**: http://arxiv.org/abs/1903.00925v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00925v1)
- **Published**: 2019-03-03 15:17:06+00:00
- **Updated**: 2019-03-03 15:17:06+00:00
- **Authors**: Jasmine Collins, Johannes Balle, Jonathon Shlens
- **Comment**: Technical report. Results presented at WiML 2018
- **Journal**: None
- **Summary**: A significant advance in accelerating neural network training has been the development of normalization methods, permitting the training of deep models both faster and with better accuracy. These advances come with practical challenges: for instance, batch normalization ties the prediction of individual examples with other examples within a batch, resulting in a network that is heavily dependent on batch size. Layer normalization and group normalization are data-dependent and thus must be continually used, even at test-time. To address the issues that arise from using explicit normalization techniques, we propose to replace existing normalization methods with a simple, secondary objective loss that we term a standardization loss. This formulation is flexible and robust across different batch sizes and surprisingly, this secondary objective accelerates learning on the primary training objective. Because it is a training loss, it is simply removed at test-time, and no further effort is needed to maintain normalized activations. We find that a standardization loss accelerates training on both small- and large-scale image classification experiments, works with a variety of architectures, and is largely robust to training across different batch sizes.



### Matching Thermal to Visible Face Images Using a Semantic-Guided Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1903.00963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00963v1)
- **Published**: 2019-03-03 19:00:05+00:00
- **Updated**: 2019-03-03 19:00:05+00:00
- **Authors**: Cunjian Chen, Arun Ross
- **Comment**: Accepted for publication in 2019 14th IEEE International Conference
  on Automatic Face & Gesture Recognition (FG 2019)
- **Journal**: None
- **Summary**: Designing face recognition systems that are capable of matching face images obtained in the thermal spectrum with those obtained in the visible spectrum is a challenging problem. In this work, we propose the use of semantic-guided generative adversarial network (SG-GAN) to automatically synthesize visible face images from their thermal counterparts. Specifically, semantic labels, extracted by a face parsing network, are used to compute a semantic loss function to regularize the adversarial network during training. These semantic cues denote high-level facial component information associated with each pixel. Further, an identity extraction network is leveraged to generate multi-scale features to compute an identity loss function. To achieve photo-realistic results, a perceptual loss function is introduced during network training to ensure that the synthesized visible face is perceptually similar to the target visible face image. We extensively evaluate the benefits of individual loss functions, and combine them effectively to learn the mapping from thermal to visible face images. Experiments involving two multispectral face datasets show that the proposed method achieves promising results in both face synthesis and cross-spectral face matching.



### X-Section: Cross-Section Prediction for Enhanced RGBD Fusion
- **Arxiv ID**: http://arxiv.org/abs/1903.00987v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00987v3)
- **Published**: 2019-03-03 20:58:04+00:00
- **Updated**: 2019-08-12 14:32:28+00:00
- **Authors**: Andrea Nicastro, Ronald Clark, Stefan Leutenegger
- **Comment**: None
- **Journal**: None
- **Summary**: Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g.\ in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g.\ for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects.



### Self-Supervised Learning of Face Representations for Video Face Clustering
- **Arxiv ID**: http://arxiv.org/abs/1903.01000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.01000v1)
- **Published**: 2019-03-03 21:53:15+00:00
- **Updated**: 2019-03-03 21:53:15+00:00
- **Authors**: Vivek Sharma, Makarand Tapaswi, M. Saquib Sarfraz, Rainer Stiefelhagen
- **Comment**: To appear at International Conference on Automatic Face and Gesture
  Recognition (2019) as an Oral. The datasets and code are available at
  https://github.com/vivoutlaw/SSIAM
- **Journal**: None
- **Summary**: Analyzing the story behind TV series and movies often requires understanding who the characters are and what they are doing. With improving deep face models, this may seem like a solved problem. However, as face detectors get better, clustering/identification needs to be revisited to address increasing diversity in facial appearance. In this paper, we address video face clustering using unsupervised methods. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, and thus can also be applied to image collections. We evaluate our proposed method on three video face clustering datasets. The experiments show that our methods outperform current state-of-the-art methods on all datasets. Video face clustering is lacking a common benchmark as current works are often evaluated with different metrics and/or different sets of face tracks.



### Hand Pose Estimation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1903.01013v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01013v2)
- **Published**: 2019-03-03 23:19:12+00:00
- **Updated**: 2019-06-02 20:58:48+00:00
- **Authors**: Bardia Doosti
- **Comment**: None
- **Journal**: None
- **Summary**: The success of Deep Convolutional Neural Networks (CNNs) in recent years in almost all the Computer Vision tasks on one hand, and the popularity of low-cost consumer depth cameras on the other, has made Hand Pose Estimation a hot topic in computer vision field. In this report, we will first explain the hand pose estimation problem and will review major approaches solving this problem, especially the two different problems of using depth maps or RGB images. We will survey the most important papers in each field and will discuss the strengths and weaknesses of each. Finally, we will explain the biggest datasets in this field in detail and list 22 datasets with all their properties. To the best of our knowledge this is the most complete list of all the datasets in the hand pose estimation field.



### A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1903.01015v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01015v2)
- **Published**: 2019-03-03 23:21:23+00:00
- **Updated**: 2019-05-09 00:37:01+00:00
- **Authors**: Saeid Asgari Taghanaki, Kumar Abhishek, Shekoofeh Azizi, Ghassan Hamarneh
- **Comment**: Accepted to CVPR 2019. 10 pages, 6 figures
- **Journal**: None
- **Summary**: The linear and non-flexible nature of deep convolutional models makes them vulnerable to carefully crafted adversarial perturbations. To tackle this problem, we propose a non-linear radial basis convolutional feature mapping by learning a Mahalanobis-like distance function. Our method then maps the convolutional features onto a linearly well-separated manifold, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary. We test the proposed method on three publicly available image classification and segmentation datasets namely, MNIST, ISBI ISIC 2017 skin lesion segmentation, and NIH Chest X-Ray-14. We evaluate the robustness of our method to different gradient (targeted and untargeted) and non-gradient based attacks and compare it to several non-gradient masking defense strategies. Our results demonstrate that the proposed method can increase the resilience of deep convolutional neural networks to adversarial perturbations without accuracy drop on clean data.



