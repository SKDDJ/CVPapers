# Arxiv Papers in cs.CV on 2019-03-29
### FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1903.12305v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12305v1)
- **Published**: 2019-03-29 00:42:52+00:00
- **Updated**: 2019-03-29 00:42:52+00:00
- **Authors**: Jingwei Huang, Yichao Zhou, Thomas Funkhouser, Leonidas Guibas
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image corresponds to a surface in the underlying 3D geometry, where a canonical frame can be identified as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB. Our first insight is that canonical frames computed automatically with recently introduced direction field synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3D canonical frames through projective geometry and orthogonality constraints. In our experiments, we find that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality.



### Relation-Aware Graph Attention Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1903.12314v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.12314v3)
- **Published**: 2019-03-29 01:24:19+00:00
- **Updated**: 2019-10-09 18:34:49+00:00
- **Authors**: Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.



### A Deep Dive into Understanding Tumor Foci Classification using Multiparametric MRI Based on Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1903.12331v3
- **DOI**: 10.1002/mp.14255
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1903.12331v3)
- **Published**: 2019-03-29 02:38:37+00:00
- **Updated**: 2020-05-14 15:28:55+00:00
- **Authors**: Weiwei Zong, Joon Lee, Chang Liu, Eric Carver, Aharon Feldman, Branislava Janic, Mohamed Elshaikh, Milan Pantelic, David Hearshen, Indrin Chetty, Benjamin Movsas, Ning Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have had a great success in disease classifications using large data pools of skin cancer images or lung X-rays. However, data scarcity has been the roadblock of applying deep learning models directly on prostate multiparametric MRI (mpMRI). Although model interpretation has been heavily studied for natural images for the past few years, there has been a lack of interpretation of deep learning models trained on medical images. This work designs a customized workflow for the small and imbalanced data set of prostate mpMRI where features were extracted from a deep learning model and then analyzed by a traditional machine learning classifier. In addition, this work contributes to revealing how deep learning models interpret mpMRI for prostate cancer patients stratification.



### ESFNet: Efficient Network for Building Extraction from High-Resolution Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1903.12337v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12337v2)
- **Published**: 2019-03-29 03:13:19+00:00
- **Updated**: 2019-04-19 13:03:33+00:00
- **Authors**: Jingbo Lin, Weipeng Jing, Houbing Song, Guangsheng Chen
- **Comment**: 10 pages, 3 figures, 4 tables. Accepted for IEEE Access
- **Journal**: None
- **Summary**: Building footprint extraction from high-resolution aerial images is always an essential part of urban dynamic monitoring, planning and management. It has also been a challenging task in remote sensing research. In recent years, deep neural networks have made great achievement in improving accuracy of building extraction from remote sensing imagery. However, most of existing approaches usually require large amount of parameters and floating point operations for high accuracy, it leads to high memory consumption and low inference speed which are harmful to research. In this paper, we proposed a novel efficient network named ESFNet which employs separable factorized residual block and utilizes the dilated convolutions, aiming to preserve slight accuracy loss with low computational cost and memory consumption. Our ESFNet obtains a better trade-off between accuracy and efficiency, it can run at over 100 FPS on single Tesla V100, requires 6x fewer FLOPs and has 18x fewer parameters than state-of-the-art real-time architecture ERFNet while preserving similar accuracy without any additional context module, post-processing and pre-trained scheme. We evaluated our networks on WHU Building Dataset and compared it with other state-of-the-art architectures. The result and comprehensive analysis show that our networks are benefit for efficient remote sensing researches, and the idea can be further extended to other areas. The code is public available at: https://github.com/mrluin/ESFNet-Pytorch



### Lending Orientation to Neural Networks for Cross-view Geo-localization
- **Arxiv ID**: http://arxiv.org/abs/1903.12351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12351v1)
- **Published**: 2019-03-29 04:40:01+00:00
- **Updated**: 2019-03-29 04:40:01+00:00
- **Authors**: Liu Liu, Hongdong Li
- **Comment**: CVPR2019. Codes and datasets are available at
  https://github.com/Liumouliu/OriCNN
- **Journal**: None
- **Summary**: This paper studies image-based geo-localization (IBL) problem using ground-to-aerial cross-view matching. The goal is to predict the spatial location of a ground-level query image by matching it to a large geotagged aerial image database (e.g., satellite imagery). This is a challenging task due to the drastic differences in their viewpoints and visual appearances. Existing deep learning methods for this problem have been focused on maximizing feature similarity between spatially close-by image pairs, while minimizing other images pairs which are far apart. They do so by deep feature embedding based on visual appearance in those ground-and-aerial images. However, in everyday life, humans commonly use {\em orientation} information as an important cue for the task of spatial localization. Inspired by this insight, this paper proposes a novel method which endows deep neural networks with the `commonsense' of orientation. Given a ground-level spherical panoramic image as query input (and a large georeferenced satellite image database), we design a Siamese network which explicitly encodes the orientation (i.e., spherical directions) of each pixel of the images. Our method significantly boosts the discriminative power of the learned deep features, leading to a much higher recall and precision outperforming all previous methods. Our network is also more compact using only 1/5th number of parameters than a previously best-performing network. To evaluate the generalization of our method, we also created a large-scale cross-view localization benchmark containing 100K geotagged ground-aerial pairs covering a city. Our codes and datasets are available at \url{https://github.com/Liumouliu/OriCNN}.



### Local Aggregation for Unsupervised Learning of Visual Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1903.12355v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.12355v2)
- **Published**: 2019-03-29 05:05:41+00:00
- **Updated**: 2019-04-10 06:37:42+00:00
- **Authors**: Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC.



### CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor
- **Arxiv ID**: http://arxiv.org/abs/1903.12363v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.12363v4)
- **Published**: 2019-03-29 06:23:06+00:00
- **Updated**: 2019-06-20 01:56:57+00:00
- **Authors**: Xiaohui Zhao, Endi Niu, Zhuo Wu, Xiaoguang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data.



### Synthesizing a 4D Spatio-Angular Consistent Light Field from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1903.12364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12364v1)
- **Published**: 2019-03-29 06:24:56+00:00
- **Updated**: 2019-03-29 06:24:56+00:00
- **Authors**: Andre Ivan, Williem, In Kyu Park
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing a densely sampled light field from a single image is highly beneficial for many applications. The conventional method reconstructs a depth map and relies on physical-based rendering and a secondary network to improve the synthesized novel views. Simple pixel-based loss also limits the network by making it rely on pixel intensity cue rather than geometric reasoning. In this study, we show that a different geometric representation, namely, appearance flow, can be used to synthesize a light field from a single image robustly and directly. A single end-to-end deep neural network that does not require a physical-based approach nor a post-processing subnetwork is proposed. Two novel loss functions based on known light field domain knowledge are presented to enable the network to preserve the spatio-angular consistency between sub-aperture images effectively. Experimental results show that the proposed model successfully synthesizes dense light fields and qualitatively and quantitatively outperforms the previous model . The method can be generalized to arbitrary scenes, rather than focusing on a particular class of object. The synthesized light field can be used for various applications, such as depth estimation and refocusing.



### DenseAttentionSeg: Segment Hands from Interacted Objects Using Depth Input
- **Arxiv ID**: http://arxiv.org/abs/1903.12368v2
- **DOI**: 10.1016/j.asoc.2020.106297
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12368v2)
- **Published**: 2019-03-29 06:38:22+00:00
- **Updated**: 2019-04-23 10:59:17+00:00
- **Authors**: Zihao Bo, Hao Zhang, Junhai Yong, Feng Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a real-time DNN-based technique to segment hand and object of interacting motions from depth inputs. Our model is called DenseAttentionSeg, which contains a dense attention mechanism to fuse information in different scales and improves the results quality with skip-connections. Besides, we introduce a contour loss in model training, which helps to generate accurate hand and object boundaries. Finally, we propose and release our InterSegHands dataset, a fine-scale hand segmentation dataset containing about 52k depth maps of hand-object interactions. Our experiments evaluate the effectiveness of our techniques and datasets, and indicate that our method outperforms the current state-of-the-art deep segmentation methods on interaction segmentation.



### On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models
- **Arxiv ID**: http://arxiv.org/abs/1903.12370v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.12370v4)
- **Published**: 2019-03-29 06:45:03+00:00
- **Updated**: 2019-11-27 20:16:29+00:00
- **Authors**: Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, Ying Nian Wu
- **Comment**: Code available at: https://github.com/point0bar1/ebm-anatomy
- **Journal**: AAAI 2020
- **Summary**: This study investigates the effects of Markov chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. We find that many of the techniques used to stabilize training in previous studies are not necessary. ML learning with a ConvNet potential requires only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend solely on the implementation of MCMC sampling.   On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our energy model can obtain realistic high-diversity samples from a noise signal after training.   On the other hand, ConvNet potentials learned with non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images.



### Few-Shot Deep Adversarial Learning for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1903.12395v3
- **DOI**: 10.1109/TIP.2019.2940684
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12395v3)
- **Published**: 2019-03-29 08:45:59+00:00
- **Updated**: 2019-09-12 05:23:08+00:00
- **Authors**: Lin Wu, Yang Wang, Hongzhi Yin, Meng Wang, Ling Shao
- **Comment**: Appearing at IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Video-based person re-identification (re-ID) refers to matching people across camera views from arbitrary unaligned video footages. Existing methods rely on supervision signals to optimise a projected space under which the distances between inter/intra-videos are maximised/minimised. However, this demands exhaustively labelling people across camera views, rendering them unable to be scaled in large networked cameras. Also, it is noticed that learning effective video representations with view invariance is not explicitly addressed for which features exhibit different distributions otherwise. Thus, matching videos for person re-ID demands flexible models to capture the dynamics in time-series observations and learn view-invariant representations with access to limited labeled training samples. In this paper, we propose a novel few-shot deep learning approach to video-based person re-ID, to learn comparable representations that are discriminative and view-invariant. The proposed method is developed on the variational recurrent neural networks (VRNNs) and trained adversarially to produce latent variables with temporal dependencies that are highly discriminative yet view-invariant in matching persons. Through extensive experiments conducted on three benchmark datasets, we empirically show the capability of our method in creating view-invariant temporal features and state-of-the-art performance achieved by our method.



### Deep, spatially coherent Inverse Sensor Models with Uncertainty Incorporation using the evidential Framework
- **Arxiv ID**: http://arxiv.org/abs/1904.00842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.00842v1)
- **Published**: 2019-03-29 11:50:13+00:00
- **Updated**: 2019-03-29 11:50:13+00:00
- **Authors**: Daniel Bauer, Lars Kuhnert, Lutz Eckstein
- **Comment**: Submitted for Intelligent Vehicle Symposium 2019
- **Journal**: None
- **Summary**: To perform high speed tasks, sensors of autonomous cars have to provide as much information in as few time steps as possible. However, radars, one of the sensor modalities autonomous cars heavily rely on, often only provide sparse, noisy detections. These have to be accumulated over time to reach a high enough confidence about the static parts of the environment. For radars, the state is typically estimated by accumulating inverse detection models (IDMs). We employ the recently proposed evidential convolutional neural networks which, in contrast to IDMs, compute dense, spatially coherent inference of the environment state. Moreover, these networks are able to incorporate sensor noise in a principled way which we further extend to also incorporate model uncertainty. We present experimental results that show This makes it possible to obtain a denser environment perception in fewer time steps.



### Asymmetric Deep Semantic Quantization for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.12493v2
- **DOI**: 10.1109/ACCESS.2019.2920712
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12493v2)
- **Published**: 2019-03-29 13:00:03+00:00
- **Updated**: 2019-06-01 02:05:24+00:00
- **Authors**: Zhan Yang, Osolo Ian Raymond, WuQing Sun, Jun Long
- **Comment**: Accepted to IEEE ACCESS. arXiv admin note: text overlap with
  arXiv:1812.01404
- **Journal**: None
- **Summary**: Due to its fast retrieval and storage efficiency capabilities, hashing has been widely used in nearest neighbor retrieval tasks. By using deep learning based techniques, hashing can outperform non-learning based hashing technique in many applications. However, we argue that the current deep learning based hashing methods ignore some critical problems (e.g., the learned hash codes are not discriminative due to the hashing methods being unable to discover rich semantic information and the training strategy having difficulty optimizing the discrete binary codes). In this paper, we propose a novel image hashing method, termed as \textbf{\underline{A}}symmetric \textbf{\underline{D}}eep \textbf{\underline{S}}emantic \textbf{\underline{Q}}uantization (\textbf{ADSQ}). \textbf{ADSQ} is implemented using three stream frameworks, which consist of one \emph{LabelNet} and two \emph{ImgNets}. The \emph{LabelNet} leverages the power of three fully-connected layers, which are used to capture rich semantic information between image pairs. For the two \emph{ImgNets}, they each adopt the same convolutional neural network structure, but with different weights (i.e., asymmetric convolutional neural networks). The two \emph{ImgNets} are used to generate discriminative compact hash codes. Specifically, the function of the \emph{LabelNet} is to capture rich semantic information that is used to guide the two \emph{ImgNets} in minimizing the gap between the real-continuous features and the discrete binary codes. Furthermore, \textbf{ADSQ} can utilize the most critical semantic information to guide the feature learning process and consider the consistency of the common semantic space and Hamming space. Experimental results on three benchmarks (i.e., CIFAR-10, NUS-WIDE, and ImageNet) demonstrate that the proposed \textbf{ADSQ} can outperforms current state-of-the-art methods.



### Training Object Detectors on Synthetic Images Containing Reflecting Materials
- **Arxiv ID**: http://arxiv.org/abs/1904.00824v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00824v1)
- **Published**: 2019-03-29 13:27:34+00:00
- **Updated**: 2019-03-29 13:27:34+00:00
- **Authors**: Sebastian Hartwig, Timo Ropinski
- **Comment**: None
- **Journal**: None
- **Summary**: One of the grand challenges of deep learning is the requirement to obtain large labeled training data sets. While synthesized data sets can be used to overcome this challenge, it is important that these data sets close the reality gap, i.e., a model trained on synthetic image data is able to generalize to real images. Whereas, the reality gap can be considered bridged in several application scenarios, training on synthesized images containing reflecting materials requires further research. Since the appearance of objects with reflecting materials is dominated by the surrounding environment, this interaction needs to be considered during training data generation. Therefore, within this paper we examine the effect of reflecting materials in the context of synthetic image generation for training object detectors. We investigate the influence of rendering approach used for image synthesis, the effect of domain randomization, as well as the amount of used training data. To be able to compare our results to the state-of-the-art, we focus on indoor scenes as they have been investigated extensively. Within this scenario, bathroom furniture is a natural choice for objects with reflecting materials, for which we report our findings on real and synthetic testing data.



### Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels
- **Arxiv ID**: http://arxiv.org/abs/1903.12529v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12529v1)
- **Published**: 2019-03-29 14:11:36+00:00
- **Updated**: 2019-03-29 14:11:36+00:00
- **Authors**: Kai Zhang, Wangmeng Zuo, Lei Zhang
- **Comment**: Accepted to CVPR2019; code is available at
  https://github.com/cszn/DPSR
- **Journal**: None
- **Summary**: While deep neural networks (DNN) based single image super-resolution (SISR) methods are rapidly gaining popularity, they are mainly designed for the widely-used bicubic degradation, and there still remains the fundamental challenge for them to super-resolve low-resolution (LR) image with arbitrary blur kernels. In the meanwhile, plug-and-play image restoration has been recognized with high flexibility due to its modular structure for easy plug-in of denoiser priors. In this paper, we propose a principled formulation and framework by extending bicubic degradation based deep SISR with the help of plug-and-play framework to handle LR images with arbitrary blur kernels. Specifically, we design a new SISR degradation model so as to take advantage of existing blind deblurring methods for blur kernel estimation. To optimize the new degradation induced energy function, we then derive a plug-and-play algorithm via variable splitting technique, which allows us to plug any super-resolver prior rather than the denoiser prior as a modular part. Quantitative and qualitative evaluations on synthetic and real LR images demonstrate that the proposed deep plug-and-play super-resolution framework is flexible and effective to deal with blurry LR images.



### Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.12530v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12530v4)
- **Published**: 2019-03-29 14:12:01+00:00
- **Updated**: 2019-11-20 10:23:17+00:00
- **Authors**: Zhe He, Adrian Spurr, Xucong Zhang, Otmar Hilliges
- **Comment**: Published on ICCV2019
- **Journal**: None
- **Summary**: Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.



### Learning More with Less: GAN-based Medical Image Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00838v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.00838v3)
- **Published**: 2019-03-29 14:41:28+00:00
- **Updated**: 2019-05-29 13:08:32+00:00
- **Authors**: Changhee Han, Kohei Murao, Shin'ichi Satoh, Hideki Nakayama
- **Comment**: 6 pages, 2 figures, to appear in MEDICAL IMAGING TECHNOLOGY Special
  Issue
- **Journal**: None
- **Summary**: Convolutional Neural Network (CNN)-based accurate prediction typically requires large-scale annotated training data. In Medical Imaging, however, both obtaining medical data and annotating them by expert physicians are challenging; to overcome this lack of data, Data Augmentation (DA) using Generative Adversarial Networks (GANs) is essential, since they can synthesize additional annotated training data to handle small and fragmented medical images from various scanners--those generated images, realistic but completely novel, can further fill the real image distribution uncovered by the original dataset. As a tutorial, this paper introduces GAN-based Medical Image Augmentation, along with tricks to boost classification/object detection/segmentation performance using them, based on our experience and related work. Moreover, we show our first GAN-based DA work using automatic bounding box annotation, for robust CNN-based brain metastases detection on 256 x 256 MR images; GAN-based DA can boost 10% sensitivity in diagnosis with a clinically acceptable number of additional False Positives, even with highly-rough and inconsistent bounding boxes.



### Adversarial Robustness vs Model Compression, or Both?
- **Arxiv ID**: http://arxiv.org/abs/1903.12561v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.12561v5)
- **Published**: 2019-03-29 15:06:41+00:00
- **Updated**: 2021-06-22 15:16:04+00:00
- **Authors**: Shaokai Ye, Kaidi Xu, Sijia Liu, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting, training a small model from scratch even with inherited initialization from the large model cannot achieve both adversarial robustness and high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.



### Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.12564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.12564v1)
- **Published**: 2019-03-29 15:16:15+00:00
- **Updated**: 2019-03-29 15:16:15+00:00
- **Authors**: Changhee Han, Leonardo Rundo, Ryosuke Araki, Yujiro Furukawa, Giancarlo Mauri, Hideki Nakayama, Hideaki Hayashi
- **Comment**: 13 pages, 6 figures, Accepted to Neural Approaches to Dynamics of
  Signal Exchanges as a Springer book chapter
- **Journal**: None
- **Summary**: Due to the lack of available annotated medical images, accurate computer-assisted diagnosis requires intensive Data Augmentation (DA) techniques, such as geometric/intensity transformations of original images; however, those transformed images intrinsically have a similar distribution to the original ones, leading to limited performance improvement. To fill the data lack in the real image distribution, we synthesize brain contrast-enhanced Magnetic Resonance (MR) images---realistic but completely different from the original ones---using Generative Adversarial Networks (GANs). This study exploits Progressive Growing of GANs (PGGANs), a multi-stage generative training method, to generate original-sized 256 X 256 MR images for Convolutional Neural Network-based brain tumor detection, which is challenging via conventional GANs; difficulties arise due to unstable GAN training with high resolution and a variety of tumors in size, location, shape, and contrast. Our preliminary results show that this novel PGGAN-based DA method can achieve promising performance improvement, when combined with classical DA, in tumor detection and also in other medical imaging tasks.



### CNN-based Prostate Zonal Segmentation on T2-weighted MR Images: A Cross-dataset Study
- **Arxiv ID**: http://arxiv.org/abs/1903.12571v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.12571v1)
- **Published**: 2019-03-29 15:30:38+00:00
- **Updated**: 2019-03-29 15:30:38+00:00
- **Authors**: Leonardo Rundo, Changhee Han, Jin Zhang, Ryuichiro Hataya, Yudai Nagano, Carmelo Militello, Claudio Ferretti, Marco S. Nobile, Andrea Tangherloni, Maria Carla Gilardi, Salvatore Vitabile, Hideki Nakayama, Giancarlo Mauri
- **Comment**: 12 pages, 3 figures, Accepted to Neural Approaches to Dynamics of
  Signal Exchanges as a Springer book chapter
- **Journal**: None
- **Summary**: Prostate cancer is the most common cancer among US men. However, prostate imaging is still challenging despite the advances in multi-parametric Magnetic Resonance Imaging (MRI), which provides both morphologic and functional information pertaining to the pathological regions. Along with whole prostate gland segmentation, distinguishing between the Central Gland (CG) and Peripheral Zone (PZ) can guide towards differential diagnosis, since the frequency and severity of tumors differ in these regions; however, their boundary is often weak and fuzzy. This work presents a preliminary study on Deep Learning to automatically delineate the CG and PZ, aiming at evaluating the generalization ability of Convolutional Neural Networks (CNNs) on two multi-centric MRI prostate datasets. Especially, we compared three CNN-based architectures: SegNet, U-Net, and pix2pix. In such a context, the segmentation performances achieved with/without pre-training were compared in 4-fold cross-validation. In general, U-Net outperforms the other methods, especially when training and testing are performed on multiple datasets.



### CroP: Color Constancy Benchmark Dataset Generator
- **Arxiv ID**: http://arxiv.org/abs/1903.12581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12581v1)
- **Published**: 2019-03-29 15:39:44+00:00
- **Updated**: 2019-03-29 15:39:44+00:00
- **Authors**: Nikola Banić, Karlo Koščević, Marko Subašić, Sven Lončarić
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Implementing color constancy as a pre-processing step in contemporary digital cameras is of significant importance as it removes the influence of scene illumination on object colors. Several benchmark color constancy datasets have been created for the purpose of developing and testing new color constancy methods. However, they all have numerous drawbacks including a small number of images, erroneously extracted ground-truth illuminations, long histories of misuses, violations of their stated assumptions, etc. To overcome such and similar problems, in this paper a color constancy benchmark dataset generator is proposed. For a given camera sensor it enables generation of any number of realistic raw images taken in a subset of the real world, namely images of printed photographs. Datasets with such images share many positive features with other existing real-world datasets, while some of the negative features are completely eliminated. The generated images can be successfully used to train methods that afterward achieve high accuracy on real-world datasets. This opens the way for creating large enough datasets for advanced deep learning techniques. Experimental results are presented and discussed. The source code is available at http://www.fer.unizg.hr/ipg/resources/color_constancy/.



### Thyroid Cancer Malignancy Prediction From Whole Slide Cytopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1904.00839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.00839v1)
- **Published**: 2019-03-29 17:47:28+00:00
- **Updated**: 2019-03-29 17:47:28+00:00
- **Authors**: David Dov, Shahar Kovalsky, Jonathan Cohen, Danielle Range, Ricardo Henao, Lawrence Carin
- **Comment**: None
- **Journal**: Proceedings of Machine Learning Research, 2019, Vol. 106
- **Summary**: We consider preoperative prediction of thyroid cancer based on ultra-high-resolution whole-slide cytopathology images. Inspired by how human experts perform diagnosis, our approach first identifies and classifies diagnostic image regions containing informative thyroid cells, which only comprise a tiny fraction of the entire image. These local estimates are then aggregated into a single prediction of thyroid malignancy. Several unique characteristics of thyroid cytopathology guide our deep-learning-based approach. While our method is closely related to multiple-instance learning, it deviates from these methods by using a supervised procedure to extract diagnostically relevant regions. Moreover, we propose to simultaneously predict thyroid malignancy, as well as a diagnostic score assigned by a human expert, which further allows us to devise an improved training strategy. Experimental results show that the proposed algorithm achieves performance comparable to human experts, and demonstrate the potential of using the algorithm for screening and as an assistive tool for the improved diagnosis of indeterminate cases.



### Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1903.12648v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.12648v3)
- **Published**: 2019-03-29 17:48:15+00:00
- **Updated**: 2019-10-26 17:17:50+00:00
- **Authors**: Kibok Lee, Kimin Lee, Jinwoo Shin, Honglak Lee
- **Comment**: ICCV 2019; v3 updated Figure 1
- **Journal**: None
- **Summary**: Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.



### SE2Net: Siamese Edge-Enhancement Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.00048v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00048v2)
- **Published**: 2019-03-29 18:50:10+00:00
- **Updated**: 2019-08-16 21:19:38+00:00
- **Authors**: Sanping Zhou, Jimuyang Zhang, Jinjun Wang, Fei Wang, Dong Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural network significantly boosted the capability of salient object detection in handling large variations of scenes and object appearances. However, convolution operations seek to generate strong responses on individual pixels, while lack the ability to maintain the spatial structure of objects. Moreover, the down-sampling operations, such as pooling and striding, lose spatial details of the salient objects. In this paper, we propose a simple yet effective Siamese Edge-Enhancement Network (SE2Net) to preserve the edge structure for salient object detection. Specifically, a novel multi-stage siamese network is built to aggregate the low-level and high-level features, and parallelly estimate the salient maps of edges and regions. As a result, the predicted regions become more accurate by enhancing the responses at edges, and the predicted edges become more semantic by suppressing the false positives in background. After the refined salient maps of edges and regions are produced by the SE2Net, an edge-guided inference algorithm is designed to further improve the resulting salient masks along the predicted edges. Extensive experiments on several benchmark datasets have been conducted, which show that our method is superior than the state-of-the-art approaches.



### Brain Tissue Segmentation Using NeuroNet With Different Pre-processing Techniques
- **Arxiv ID**: http://arxiv.org/abs/1904.00068v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00068v1)
- **Published**: 2019-03-29 19:42:34+00:00
- **Updated**: 2019-03-29 19:42:34+00:00
- **Authors**: Fakrul Islam Tushar, Basel Alyafi, Md. Kamrul Hasan, Lavsen Dahal
- **Comment**: 3rd International Conference on Imaging, Vision & Pattern Recognition
  (IVPR)2019
- **Journal**: None
- **Summary**: Automatic segmentation of brain Magnetic Resonance Imaging (MRI) images is one of the vital steps for quantitative analysis of brain for further inspection. In this paper, NeuroNet has been adopted to segment the brain tissues (white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF)) which uses Residual Network (ResNet) in encoder and Fully Convolution Network (FCN) in the decoder. To achieve the best performance, various hyper-parameters have been tuned, while, network parameters (kernel and bias) were initialized using the NeuroNet pre-trained model. Different pre-processing pipelines have also been introduced to get a robust trained model. The model has been trained and tested on IBSR18 data-set. To validate the research outcome, performance was measured quantitatively using Dice Similarity Coefficient (DSC) and is reported on average as 0.84 for CSF, 0.94 for GM, and 0.94 for WM. The outcome of the research indicates that for the IBSR18 data-set, pre-processing and proper tuning of hyper-parameters for NeuroNet model have improvement in DSC for the brain tissue segmentation.



### Unpaired Point Cloud Completion on Real Scans using Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1904.00069v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00069v3)
- **Published**: 2019-03-29 19:45:21+00:00
- **Updated**: 2020-02-23 11:38:31+00:00
- **Authors**: Xuelin Chen, Baoquan Chen, Niloy J. Mitra
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: As 3D scanning solutions become increasingly popular, several deep learning setups have been developed geared towards that task of scan completion, i.e., plausibly filling in regions there were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, i.e., partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training data. We develop a first approach that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. We evaluate the approach qualitatively on several real-world datasets (ScanNet, Matterport, KITTI), quantitatively on 3D-EPN shape completion benchmark dataset, and demonstrate realistic completions under varying levels of incompleteness.



### 3D Organ Shape Reconstruction from Topogram Images
- **Arxiv ID**: http://arxiv.org/abs/1904.00073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00073v1)
- **Published**: 2019-03-29 19:51:54+00:00
- **Updated**: 2019-03-29 19:51:54+00:00
- **Authors**: Elena Balashova, Jiangping Wang, Vivek Singh, Bogdan Georgescu, Brian Teixeira, Ankur Kapoor
- **Comment**: 12 pages, accepted to International Conference on Information
  Processing in Medical Imaging (IPMI)
- **Journal**: None
- **Summary**: Automatic delineation and measurement of main organs such as liver is one of the critical steps for assessment of hepatic diseases, planning and postoperative or treatment follow-up. However, addressing this problem typically requires performing computed tomography (CT) scanning and complicated postprocessing of the resulting scans using slice-by-slice techniques. In this paper, we show that 3D organ shape can be automatically predicted directly from topogram images, which are easier to acquire and have limited exposure to radiation during acquisition, compared to CT scans. We evaluate our approach on the challenging task of predicting liver shape using a generative model. We also demonstrate that our method can be combined with user annotations, such as a 2D mask, for improved prediction accuracy. We show compelling results on 3D liver shape reconstruction and volume estimation on 2129 CT scans.



