# Arxiv Papers in cs.CV on 2019-03-09
### Age prediction using a large chest X-ray dataset
- **Arxiv ID**: http://arxiv.org/abs/1903.06542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06542v1)
- **Published**: 2019-03-09 00:12:42+00:00
- **Updated**: 2019-03-09 00:12:42+00:00
- **Authors**: Alexandros Karargyris, Satyananda Kashyap, Joy T Wu, Arjun Sharma, Mehdi Moradi, Tanveer Syeda-Mahmood
- **Comment**: Presented at SPIE Medical Imaging Conference, San Diego, 2019
- **Journal**: None
- **Summary**: Age prediction based on appearances of different anatomies in medical images has been clinically explored for many decades. In this paper, we used deep learning to predict a persons age on Chest X-Rays. Specifically, we trained a CNN in regression fashion on a large publicly available dataset. Moreover, for interpretability, we explored activation maps to identify which areas of a CXR image are important for the machine (i.e. CNN) to predict a patients age, offering insight. Overall, amongst correctly predicted CXRs, we see areas near the clavicles, shoulders, spine, and mediastinum being most activated for age prediction, as one would expect biologically. Amongst incorrectly predicted CXRs, we have qualitatively identified disease patterns that could possibly make the anatomies appear older or younger than expected. A further technical and clinical evaluation would improve this work. As CXR is the most commonly requested imaging exam, a potential use case for estimating age may be found in the preventative counseling of patient health status compared to their age-expected average, particularly when there is a large discrepancy between predicted age and the real patient age.



### Sparse Representations for Object and Ego-motion Estimation in Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1903.03731v1
- **DOI**: 10.1109/TNNLS.2020.3006467
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.03731v1)
- **Published**: 2019-03-09 03:56:53+00:00
- **Updated**: 2019-03-09 03:56:53+00:00
- **Authors**: Hirak J Kashyap, Charless Fowlkes, Jeffrey L Krichmar
- **Comment**: With supplementary material
- **Journal**: None
- **Summary**: Dynamic scenes that contain both object motion and egomotion are a challenge for monocular visual odometry (VO). Another issue with monocular VO is the scale ambiguity, i.e. these methods cannot estimate scene depth and camera motion in real scale. Here, we propose a learning based approach to predict camera motion parameters directly from optic flow, by marginalizing depthmap variations and outliers. This is achieved by learning a sparse overcomplete basis set of egomotion in an autoencoder network, which is able to eliminate irrelevant components of optic flow for the task of camera parameter or motionfield estimation. The model is trained using a sparsity regularizer and a supervised egomotion loss, and achieves the state-of-the-art performances on trajectory prediction and camera rotation prediction tasks on KITTI and Virtual KITTI datasets, respectively. The sparse latent space egomotion representation learned by the model is robust and requires only 5% of the hidden layer neurons to maintain the best trajectory prediction accuracy on KITTI dataset. Additionally, in presence of depth information, the proposed method demonstrates faithful object velocity prediction for wide range of object sizes and speeds by global compensation of predicted egomotion and a divisive normalization procedure.



### How Effectively Can Indoor Wireless Positioning Relieve Visual Tracking Pains: A Camera-Rao Bound Viewpoint
- **Arxiv ID**: http://arxiv.org/abs/1903.03736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03736v1)
- **Published**: 2019-03-09 04:49:27+00:00
- **Updated**: 2019-03-09 04:49:27+00:00
- **Authors**: Panwen Hu, Zizheng Yan, Rui Huang, Feng Yin
- **Comment**: None
- **Journal**: None
- **Summary**: Visual tracking is fragile in some difficult scenarios, for instance, appearance ambiguity and variation, occlusion can easily degrade most of visual trackers to some extent. In this paper, visual tracking is empowered with wireless positioning to achieve high accuracy while maintaining robustness. Fundamentally different from the previous works, this study does not involve any specific wireless positioning algorithms. Instead, we use the confidence region derived from the wireless positioning Cramer-Rao bound (CRB) as the search region of visual trackers. The proposed framework is low-cost and very simple to implement, yet readily leads to enhanced and robustified visual tracking performance in difficult scenarios as corroborated by our experimental results. Most importantly, it is utmost valuable for the practioners to pre-evaluate how effectively can the wireless resources available at hand alleviate the visual tracking pains.



### Hierarchy Denoising Recursive Autoencoders for 3D Scene Layout Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.03757v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03757v2)
- **Published**: 2019-03-09 07:56:21+00:00
- **Updated**: 2019-04-10 15:38:09+00:00
- **Authors**: Yifei Shi, Angel Xuan Chang, Zhelun Wu, Manolis Savva, Kai Xu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Indoor scenes exhibit rich hierarchical structure in 3D object layouts. Many tasks in 3D scene understanding can benefit from reasoning jointly about the hierarchical context of a scene, and the identities of objects. We present a variational denoising recursive autoencoder (VDRAE) that generates and iteratively refines a hierarchical representation of 3D object layouts, interleaving bottom-up encoding for context aggregation and top-down decoding for propagation. We train our VDRAE on large-scale 3D scene datasets to predict both instance-level segmentations and a 3D object detections from an over-segmentation of an input point cloud. We show that our VDRAE improves object detection performance on real-world 3D point cloud datasets compared to baselines from prior work.



### Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1903.03777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03777v2)
- **Published**: 2019-03-09 10:41:41+00:00
- **Updated**: 2019-04-05 06:54:27+00:00
- **Authors**: Xin Li, Yiming Zhou, Zheng Pan, Jiashi Feng
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Achieving good speed and accuracy trade-off on a target platform is very important in deploying deep neural networks in real world scenarios. However, most existing automatic architecture search approaches only concentrate on high performance. In this work, we propose an algorithm that can offer better speed/accuracy trade-off of searched networks, which is termed "Partial Order Pruning". It prunes the architecture search space with a partial order assumption to automatically search for the architectures with the best speed and accuracy trade-off. Our algorithm explicitly takes profile information about the inference speed on the target platform into consideration. With the proposed algorithm, we present several Dongfeng (DF) networks that provide high accuracy and fast inference speed on various application GPU platforms. By further searching decoder architectures, our DF-Seg real-time segmentation networks yield state-of-the-art speed/accuracy trade-off on both the target embedded device and the high-end GPU.



### Combining 3D Morphable Models: A Large scale Face-and-Head Model
- **Arxiv ID**: http://arxiv.org/abs/1903.03785v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03785v1)
- **Published**: 2019-03-09 11:35:30+00:00
- **Updated**: 2019-03-09 11:35:30+00:00
- **Authors**: Stylianos Ploumpis, Haoyang Wang, Nick Pears, William A. P. Smith, Stefanos Zafeiriou
- **Comment**: 9 pages, 8 figures. To appear in the Proceedings of Computer Vision
  and Pattern Recognition (CVPR), June 2019, Los Angeles, USA
- **Journal**: None
- **Summary**: Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D surfaces of an object class. In this context, we identify an interesting question that has previously not received research attention: is it possible to combine two or more 3DMMs that (a) are built using different templates that perhaps only partly overlap, (b) have different representation capabilities and (c) are built from different datasets that may not be publicly-available? In answering this question, we make two contributions. First, we propose two methods for solving this problem: i. use a regressor to complete missing parts of one model using the other, ii. use the Gaussian Process framework to blend covariance matrices from multiple models. Second, as an example application of our approach, we build a new face-and-head shape model that combines the variability and facial detail of the LSFM with the full head modelling of the LYHM. The resulting combined shape model achieves state-of-the-art performance and outperforms existing head models by a large margin. Finally, as an application experiment, we reconstruct full head representations from single, unconstrained images by utilizing our proposed large-scale model in conjunction with the FaceWarehouse blendshapes for handling expressions.



### SSN: Learning Sparse Switchable Normalization via SparsestMax
- **Arxiv ID**: http://arxiv.org/abs/1903.03793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03793v1)
- **Published**: 2019-03-09 12:48:52+00:00
- **Updated**: 2019-03-09 12:48:52+00:00
- **Authors**: Wenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang, Yudian Li, Xiaogang Wang, Ping Luo
- **Comment**: 10 pages, 6 figures, accepted to CVPR 2019
- **Journal**: None
- **Summary**: Normalization methods improve both optimization and generalization of ConvNets. To further boost performance, the recently-proposed switchable normalization (SN) provides a new perspective for deep learning: it learns to select different normalizers for different convolution layers of a ConvNet. However, SN uses softmax function to learn importance ratios to combine normalizers, leading to redundant computations compared to a single normalizer.   This work addresses this issue by presenting Sparse Switchable Normalization (SSN) where the importance ratios are constrained to be sparse. Unlike $\ell_1$ and $\ell_0$ constraints that impose difficulties in optimization, we turn this constrained optimization problem into feed-forward computation by proposing SparsestMax, which is a sparse version of softmax. SSN has several appealing properties. (1) It inherits all benefits from SN such as applicability in various tasks and robustness to a wide range of batch sizes. (2) It is guaranteed to select only one normalizer for each normalization layer, avoiding redundant computations. (3) SSN can be transferred to various tasks in an end-to-end manner. Extensive experiments show that SSN outperforms its counterparts on various challenging benchmarks such as ImageNet, Cityscapes, ADE20K, and Kinetics.



### LumiPath -- Towards Real-time Physically-based Rendering on Embedded Devices
- **Arxiv ID**: http://arxiv.org/abs/1903.03837v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1903.03837v2)
- **Published**: 2019-03-09 17:49:44+00:00
- **Updated**: 2019-08-16 14:07:40+00:00
- **Authors**: Laura Fink, Sing Chun Lee, Jie Ying Wu, Xingtong Liu, Tianyu Song, Yordanka Stoyanova, Marc Stamminger, Nassir Navab, Mathias Unberath
- **Comment**: To be presented at MICCAI 2019
- **Journal**: None
- **Summary**: With the increasing computational power of today's workstations, real-time physically-based rendering is within reach, rapidly gaining attention across a variety of domains. These have expeditiously applied to medicine, where it is a powerful tool for intuitive 3D data visualization. Embedded devices such as optical see-through head-mounted displays (OST HMDs) have been a trend for medical augmented reality. However, leveraging the obvious benefits of physically-based rendering remains challenging on these devices because of limited computational power, memory usage, and power consumption. We navigate the compromise between device limitations and image quality to achieve reasonable rendering results by introducing a novel light field that can be sampled in real-time on embedded devices. We demonstrate its applications in medicine and discuss limitations of the proposed method. An open-source version of this project is available at https://github.com/lorafib/LumiPath which provides full insight on implementation and exemplary demonstrational material.



### BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1903.03838v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.03838v2)
- **Published**: 2019-03-09 17:56:31+00:00
- **Updated**: 2019-09-16 15:55:05+00:00
- **Authors**: Ali Harakeh, Michael Smart, Steven L. Waslander
- **Comment**: None
- **Journal**: None
- **Summary**: When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors non-maximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77\%-13.13\% on the minimum Gaussian uncertainty error metric and a reduction of 1.63\%-5.23\% on the minimum Categorical uncertainty error metric. Code will be released at {\url{https://github.com/asharakeh/bayes-od-rc}}.



### Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks
- **Arxiv ID**: http://arxiv.org/abs/1903.03878v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.03878v1)
- **Published**: 2019-03-09 22:03:02+00:00
- **Updated**: 2019-03-09 22:03:02+00:00
- **Authors**: Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese
- **Comment**: CVPR 2019 paper with supplementary material
- **Journal**: None
- **Summary**: Many robotic applications require the agent to perform long-horizon tasks in partially observable environments. In such applications, decision making at any step can depend on observations received far in the past. Hence, being able to properly memorize and utilize the long-term history is crucial. In this work, we propose a novel memory-based policy, named Scene Memory Transformer (SMT). The proposed policy embeds and adds each observation to a memory and uses the attention mechanism to exploit spatio-temporal dependencies. This model is generic and can be efficiently trained with reinforcement learning over long episodes. On a range of visual navigation tasks, SMT demonstrates superior performance to existing reactive and memory-based policies by a margin.



