# Arxiv Papers in cs.CV on 2019-03-14
### Improving Prostate Cancer Detection with Breast Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1903.05769v1
- **DOI**: 10.1007/978-3-030-23937-4_11
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.05769v1)
- **Published**: 2019-03-14 00:09:14+00:00
- **Updated**: 2019-03-14 00:09:14+00:00
- **Authors**: Umair Akhtar Hasan Khan, Carolin St√ºrenberg, Oguzhan Gencoglu, Kevin Sandeman, Timo Heikkinen, Antti Rannikko, Tuomas Mirtti
- **Comment**: 9 pages, 2 figures
- **Journal**: None
- **Summary**: Deep neural networks have introduced significant advancements in the field of machine learning-based analysis of digital pathology images including prostate tissue images. With the help of transfer learning, classification and segmentation performance of neural network models have been further increased. However, due to the absence of large, extensively annotated, publicly available prostate histopathology datasets, several previous studies employ datasets from well-studied computer vision tasks such as ImageNet dataset. In this work, we propose a transfer learning scheme from breast histopathology images to improve prostate cancer detection performance. We validate our approach on annotated prostate whole slide images by using a publicly available breast histopathology dataset as pre-training. We show that the proposed cross-cancer approach outperforms transfer learning from ImageNet dataset.



### Learning Parallax Attention for Stereo Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.05784v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05784v3)
- **Published**: 2019-03-14 01:17:27+00:00
- **Updated**: 2019-03-19 12:55:20+00:00
- **Authors**: Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Stereo image pairs can be used to improve the performance of super-resolution (SR) since additional information is provided from a second viewpoint. However, it is challenging to incorporate this information for SR since disparities between stereo images vary significantly. In this paper, we propose a parallax-attention stereo superresolution network (PASSRnet) to integrate the information from a stereo image pair for SR. Specifically, we introduce a parallax-attention mechanism with a global receptive field along the epipolar line to handle different stereo images with large disparity variations. We also propose a new and the largest dataset for stereo image SR (namely, Flickr1024). Extensive experiments demonstrate that the parallax-attention mechanism can capture correspondence between stereo images to improve SR performance with a small computational and memory cost. Comparative results show that our PASSRnet achieves the state-of-the-art performance on the Middlebury, KITTI 2012 and KITTI 2015 datasets.



### Diagnosing and Enhancing VAE Models
- **Arxiv ID**: http://arxiv.org/abs/1903.05789v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.05789v2)
- **Published**: 2019-03-14 02:11:17+00:00
- **Updated**: 2019-10-30 10:53:11+00:00
- **Authors**: Bin Dai, David Wipf
- **Comment**: None
- **Journal**: None
- **Summary**: Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/ TwoStageVAE.



### Neural Style Transfer for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1903.05807v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05807v1)
- **Published**: 2019-03-14 03:56:06+00:00
- **Updated**: 2019-03-14 03:56:06+00:00
- **Authors**: Xu Cao, Weimin Wang, Katashi Nagao
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: How can we edit or transform the geometric or color property of a point cloud? In this study, we propose a neural style transfer method for point clouds which allows us to transfer the style of geometry or color from one point cloud either independently or simultaneously to another. This transfer is achieved by manipulating the content representations and Gram-based style representations extracted from a pre-trained PointNet-based classification network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the same method can be extended to transfer the style extracted from an image to the color expression of a point cloud by merely treating the image as a set of pixels. Experimental results demonstrate the capability of the proposed method for transferring style from either an image or a point cloud to another point cloud of a single object or even an indoor scene.



### Purifying Naturalistic Images through a Real-time Style Transfer Semantics Network
- **Arxiv ID**: http://arxiv.org/abs/1903.05820v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05820v1)
- **Published**: 2019-03-14 05:33:08+00:00
- **Updated**: 2019-03-14 05:33:08+00:00
- **Authors**: Tongtong Zhao, Yuxiao Yan, Ibrahim Shehi Shehu, Xianping Fu, Huibing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared to real images, the desired performance cannot still be achieved. Real images consist of multiple forms of light orientation, while synthetic images consist of a uniform light orientation. These features are considered to be characteristic of outdoor and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve the realism of the synthetic image. Different from the previous methods, this paper takes the first step to purify real images. Through the style transfer task, the distribution of outdoor real images is converted into indoor synthetic images, thereby reducing the influence of light. Therefore, this paper proposes a real-time style transfer network that preserves image content information (eg, gaze direction, pupil center position) of an input image (real image) while inferring style information (eg, image color structure, semantic features) of style image (synthetic image). In addition, the network accelerates the convergence speed of the model and adapts to multi-scale images. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. Qualitatively, it compares the proposed method with the available methods in a series of indoor and outdoor scenarios of the LPW dataset. In quantitative terms, it evaluates the purified image by training a gaze estimation model on the cross data set. The results show a significant improvement over the baseline method compared to the raw real image.



### SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.05831v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05831v1)
- **Published**: 2019-03-14 06:40:29+00:00
- **Updated**: 2019-03-14 06:40:29+00:00
- **Authors**: Yuntao Chen, Chenxia Han, Yanghao Li, Zehao Huang, Yi Jiang, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: Tech Report
- **Journal**: None
- **Summary**: Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet supports up-to-date detection models with best practice. SimpleDet also supports distributed training with near linear scaling out of box. Codes, examples and documents of SimpleDet can be found at https://github.com/tusimple/simpledet .



### Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data
- **Arxiv ID**: http://arxiv.org/abs/1903.06519v5
- **DOI**: 10.3390/photonics8080333
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.06519v5)
- **Published**: 2019-03-14 06:40:47+00:00
- **Updated**: 2021-08-13 14:27:18+00:00
- **Authors**: Ganna Platonova, Dalibor Stys, Pavel Soucek, Kirill Lonhus, Jan Valenta, Renata Rychtarikova
- **Comment**: 18 pages, 11 figures
- **Journal**: Photonics 2021, 8(8), 333
- **Summary**: The most realistic information about the transparent sample such as a live cell can be obtained only using bright-field light microscopy. At high-intensity pulsing LED illumination, we captured a primary 12-bit-per-channel (bpc) response from an observed sample using a bright-field microscope equipped with a high-resolution (4872x3248) image sensor. In order to suppress data distortions originating from the light interactions with elements in the optical path, poor sensor reproduction (geometrical defects of the camera sensor and some peculiarities of sensor sensitivity), we propose a spectroscopic approach for the correction of this uncompressed 12-bpc data by simultaneous calibration of all parts of the experimental arrangement. Moreover, the final intensities of the corrected images are proportional to the photon fluxes detected by a camera sensor. It can be visualized in 8-bpc intensity depth after the Least Information Loss compression.



### Constrained Mutual Convex Cone Method for Image Set Based Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.06549v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06549v1)
- **Published**: 2019-03-14 07:14:34+00:00
- **Updated**: 2019-03-14 07:14:34+00:00
- **Authors**: Naoya Sogi, Rui Zhu, Jing-Hao Xue, Kazuhiro Fukui
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1805.12467
- **Journal**: None
- **Summary**: In this paper, we propose a method for image-set classification based on convex cone models. Image set classification aims to classify a set of images, which were usually obtained from video frames or multi-view cameras, into a target object. To accurately and stably classify a set, it is essential to represent structural information of the set accurately. There are various representative image features, such as histogram based features, HLAC, and Convolutional Neural Network (CNN) features. We should note that most of them have non-negativity and thus can be effectively represented by a convex cone. This leads us to introduce the convex cone representation to image-set classification. To establish a convex cone based framework, we mathematically define multiple angles between two convex cones, and then define the geometric similarity between the cones using the angles. Moreover, to enhance the framework, we introduce a discriminant space that maximizes the between-class variance (gaps) and minimizes the within-class variance of the projected convex cones onto the discriminant space, similar to the Fisher discriminant analysis. Finally, the classification is performed based on the similarity between projected convex cones. The effectiveness of the proposed method is demonstrated experimentally by using five databases: CMU PIE dataset, ETH-80, CMU Motion of Body dataset, Youtube Celebrity dataset, and a private database of multi-view hand shapes.



### MirrorGAN: Learning Text-to-image Generation by Redescription
- **Arxiv ID**: http://arxiv.org/abs/1903.05854v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05854v1)
- **Published**: 2019-03-14 08:31:05+00:00
- **Updated**: 2019-03-14 08:31:05+00:00
- **Authors**: Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning text-to-image generation by redescription and consists of three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word- and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods.



### Learning Orientation-Estimation Convolutional Neural Network for Building Detection in Optical Remote Sensing Image
- **Arxiv ID**: http://arxiv.org/abs/1903.05862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05862v1)
- **Published**: 2019-03-14 09:02:59+00:00
- **Updated**: 2019-03-14 09:02:59+00:00
- **Authors**: Yongliang Chen
- **Comment**: Presented in DICTA2018; Further Improved including extra experiments
- **Journal**: None
- **Summary**: Benefiting from the great success of deep learning in computer vision, CNN-based object detection methods have drawn significant attentions. Various frameworks have been proposed which show awesome and robust performance for a large range of datasets. However, for building detection in remote sensing images, buildings always pose a diversity of orientations which makes it a challenge for the application of off-the-shelf methods to building detection. In this work, we aim to integrate orientation regression into the popular axis-aligned bounding-box detection method to tackle this problem. To adapt the axis-aligned bounding boxes to arbitrarily orientated ones, we also develop an algorithm to estimate the Intersection over Union (IoU) overlap between any two arbitrarily oriented boxes which is convenient to implement in Graphics Processing Unit (GPU) for accelerating computation. The proposed method utilizes CNN for both robust feature extraction and rotated bounding box regression. We present our modelin an end-to-end fashion making it easy to train. The model is formulated and trained to predict orientation, location and extent simultaneously obtaining tighter bounding box and hence, higher mean average precision (mAP). Experiments on remote sensing images of different scales shows a promising performance over the conventional one.



### Learning to Reconstruct People in Clothing from a Single RGB Camera
- **Arxiv ID**: http://arxiv.org/abs/1903.05885v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05885v2)
- **Published**: 2019-03-14 09:55:44+00:00
- **Updated**: 2019-04-08 07:33:53+00:00
- **Authors**: Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based model to infer the personalized 3D shape of people from a few frames (1-8) of a monocular video in which the person is moving, in less than 10 seconds with a reconstruction accuracy of 5mm. Our model learns to predict the parameters of a statistical body model and instance displacements that add clothing and hair to the shape. The model achieves fast and accurate predictions based on two key design choices. First, by predicting shape in a canonical T-pose space, the network learns to encode the images of the person into pose-invariant latent codes, where the information is fused. Second, based on the observation that feed-forward predictions are fast but do not always align with the input images, we predict using both, bottom-up and top-down streams (one per view) allowing information to flow in both directions. Learning relies only on synthetic 3D data. Once learned, the model can take a variable number of frames as input, and is able to reconstruct shapes even from a single image with an accuracy of 6mm. Results on 3 different datasets demonstrate the efficacy and accuracy of our approach.



### Detection and Tracking of Small Objects in Sparse 3D Laser Range Data
- **Arxiv ID**: http://arxiv.org/abs/1903.05889v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05889v1)
- **Published**: 2019-03-14 10:03:44+00:00
- **Updated**: 2019-03-14 10:03:44+00:00
- **Authors**: Jan Razlaw, Jan Quenzel, Sven Behnke
- **Comment**: Accepted for IEEE International Conference on Robotics and Automation
  (ICRA), Montreal, Canada, to appear May 2019
- **Journal**: None
- **Summary**: Detection and tracking of dynamic objects is a key feature for autonomous behavior in a continuously changing environment. With the increasing popularity and capability of micro aerial vehicles (MAVs) efficient algorithms have to be utilized to enable multi object tracking on limited hardware and data provided by lightweight sensors. We present a novel segmentation approach based on a combination of median filters and an efficient pipeline for detection and tracking of small objects within sparse point clouds generated by a Velodyne VLP-16 sensor. We achieve real-time performance on a single core of our MAV hardware by exploiting the inherent structure of the data. Our approach is evaluated on simulated and real scans of in- and outdoor environments, obtaining results comparable to the state of the art. Additionally, we provide an application for filtering the dynamic and mapping the static part of the data, generating further insights into the performance of the pipeline on unlabeled data.



### Scalable Facial Image Compression with Deep Feature Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1903.05921v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05921v2)
- **Published**: 2019-03-14 11:28:49+00:00
- **Updated**: 2019-08-25 11:35:45+00:00
- **Authors**: Shurun Wang, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a scalable image compression scheme, including the base layer for feature representation and enhancement layer for texture representation. More specifically, the base layer is designed as the deep learning feature for analysis purpose, and it can also be converted to the fine structure with deep feature reconstruction. The enhancement layer, which serves to compress the residuals between the input image and the signals generated from the base layer, aims to faithfully reconstruct the input texture. The proposed scheme can feasibly inherit the advantages of both compress-then-analyze and analyze-then-compress schemes in surveillance applications. The performance of this framework is validated with facial images, and the conducted experiments provide useful evidences to show that the proposed framework can achieve better rate-accuracy and rate-distortion performance over conventional image compression schemes.



### Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning
- **Arxiv ID**: http://arxiv.org/abs/1903.05942v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1903.05942v4)
- **Published**: 2019-03-14 12:36:01+00:00
- **Updated**: 2019-09-22 07:24:51+00:00
- **Authors**: Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, In So Kweon
- **Comment**: CVPR 2019 (Under review for journal extension). Project page :
  https://sites.google.com/view/relcap
- **Journal**: None
- **Summary**: Our goal in this work is to train an image captioning model that generates more dense and informative captions. We introduce "relational captioning," a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in an image. Relational captioning is a framework that is advantageous in both diversity and amount of information, leading to image understanding based on relationships. Part-of speech (POS, i.e. subject-object-predicate categories) tags can be assigned to every English word. We leverage the POS as a prior to guide the correct sequence of words in a caption. To this end, we propose a multi-task triple-stream network (MTTSNet) which consists of three recurrent units for the respective POS and jointly performs POS prediction and captioning. We demonstrate more diverse and richer representations generated by the proposed model against several baselines and competing methods.



### Recurrence is required to capture the representational dynamics of the human visual system
- **Arxiv ID**: http://arxiv.org/abs/1903.05946v2
- **DOI**: 10.1073/pnas.1905544116
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05946v2)
- **Published**: 2019-03-14 12:43:38+00:00
- **Updated**: 2019-10-08 08:45:53+00:00
- **Authors**: Tim C Kietzmann, Courtney J Spoerer, Lynn S√∂rensen, Radoslaw M Cichy, Olaf Hauk, Nikolaus Kriegeskorte
- **Comment**: https://www.pnas.org/content/early/2019/10/04/1905544116.short?rss=1
- **Journal**: Proceedings of the National Academy of Sciences, p. 1-10 (2019)
- **Summary**: The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multi-region cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.



### Low-rank Kernel Learning for Graph-based Clustering
- **Arxiv ID**: http://arxiv.org/abs/1903.05962v1
- **DOI**: 10.1016/j.knosys.2018.09.009
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.05962v1)
- **Published**: 2019-03-14 12:59:52+00:00
- **Updated**: 2019-03-14 12:59:52+00:00
- **Authors**: Zhao Kang, Liangjian Wen, Wenyu Chen, Zenglin Xu
- **Comment**: None
- **Journal**: Knowledge-Based Systems, 2019
- **Summary**: Constructing the adjacency graph is fundamental to graph-based clustering. Graph learning in kernel space has shown impressive performance on a number of benchmark data sets. However, its performance is largely determined by the chosen kernel matrix. To address this issue, the previous multiple kernel learning algorithm has been applied to learn an optimal kernel from a group of predefined kernels. This approach might be sensitive to noise and limits the representation ability of the consensus kernel. In contrast to existing methods, we propose to learn a low-rank kernel matrix which exploits the similarity nature of the kernel matrix and seeks an optimal kernel from the neighborhood of candidate kernels. By formulating graph construction and kernel learning in a unified framework, the graph and consensus kernel can be iteratively enhanced by each other. Extensive experimental results validate the efficacy of the proposed method.



### Superpixel Contracted Graph-Based Learning for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.06548v3
- **DOI**: 10.1109/TGRS.2019.2961599
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06548v3)
- **Published**: 2019-03-14 13:23:43+00:00
- **Updated**: 2019-03-19 15:11:25+00:00
- **Authors**: Philip Sellars, Angelica Aviles-Rivero, Carola-Bibiane Sch√∂nlieb
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: A central problem in hyperspectral image classification is obtaining high classification accuracy when using a limited amount of labelled data. In this paper we present a novel graph-based framework, which aims to tackle this problem in the presence of large scale data input. Our approach utilises a novel superpixel method, specifically designed for hyperspectral data, to define meaningful local regions in an image, which with high probability share the same classification label. We then extract spectral and spatial features from these regions and use these to produce a contracted weighted graph-representation, where each node represents a region rather than a pixel. Our graph is then fed into a graph-based semi-supervised classifier which gives the final classification. We show that using superpixels in a graph representation is an effective tool for speeding up graphical classifiers applied to hyperspectral images. We demonstrate through exhaustive quantitative and qualitative results that our proposed method produces accurate classifications when an incredibly small amount of labelled data is used. We show that our approach mitigates the major drawbacks of existing approaches, resulting in our approach outperforming several comparative state-of-the-art techniques.



### Superpixel-based Color Transfer
- **Arxiv ID**: http://arxiv.org/abs/1903.06010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06010v1)
- **Published**: 2019-03-14 14:07:51+00:00
- **Updated**: 2019-03-14 14:07:51+00:00
- **Authors**: R√©mi Giraud, Vinh-Thong Ta, Nicolas Papadakis
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a fast superpixel-based color transfer method (SCT) between two images. Superpixels enable to decrease the image dimension and to extract a reduced set of color candidates. We propose to use a fast approximate nearest neighbor matching algorithm in which we enforce the match diversity by limiting the selection of the same superpixels. A fusion framework is designed to transfer the matched colors, and we demonstrate the improvement obtained over exact matching results. Finally, we show that SCT is visually competitive compared to state-of-the-art methods.



### Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with Dynamic Stream Weights
- **Arxiv ID**: http://arxiv.org/abs/1903.06031v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1903.06031v1)
- **Published**: 2019-03-14 14:23:32+00:00
- **Updated**: 2019-03-14 14:23:32+00:00
- **Authors**: Christopher Schymura, Dorothea Kolossa
- **Comment**: None
- **Journal**: None
- **Summary**: Data fusion plays an important role in many technical applications that require efficient processing of multimodal sensory observations. A prominent example is audiovisual signal processing, which has gained increasing attention in automatic speech recognition, speaker localization and related tasks. If appropriately combined with acoustic information, additional visual cues can help to improve the performance in these applications, especially under adverse acoustic conditions. A dynamic weighting of acoustic and visual streams based on instantaneous sensor reliability measures is an efficient approach to data fusion in this context. This paper presents a framework that extends the well-established theory of nonlinear dynamical systems with the notion of dynamic stream weights for an arbitrary number of sensory observations. It comprises a recursive state estimator based on the Gaussian filtering paradigm, which incorporates dynamic stream weights into a framework closely related to the extended Kalman filter. Additionally, a convex optimization approach to estimate oracle dynamic stream weights in fully observed dynamical systems utilizing a Dirichlet prior is presented. This serves as a basis for a generic parameter learning framework of dynamic stream weight estimators. The proposed system is application-independent and can be easily adapted to specific tasks and requirements. A study using audiovisual speaker tracking tasks is considered as an exemplary application in this work. An improved tracking performance of the dynamic stream weight-based estimation framework over state-of-the-art methods is demonstrated in the experiments.



### MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.06048v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06048v4)
- **Published**: 2019-03-14 14:33:26+00:00
- **Updated**: 2020-06-12 20:45:26+00:00
- **Authors**: Animesh Karnewar, Oliver Wang
- **Comment**: CVPR 2020 (Main Conference). Work sponsored by TomTom and Adobe. Code
  repository: https://github.com/akanimax/msg-stylegan-tf
- **Journal**: None
- **Summary**: While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets, in part due to instability during training and sensitivity to hyperparameters. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator become uninformative when there isn't enough overlap in the supports of the real and fake distributions. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters. When compared to state-of-the-art GANs, our approach matches or exceeds the performance in most of the cases we tried.



### Deep learning enabled multi-wavelength spatial coherence microscope for the classification of malaria-infected stages with limited labelled data size
- **Arxiv ID**: http://arxiv.org/abs/1903.06056v1
- **DOI**: 10.1016/j.optlastec.2020.106335
- **Categories**: **cs.CV**, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1903.06056v1)
- **Published**: 2019-03-14 14:51:42+00:00
- **Updated**: 2019-03-14 14:51:42+00:00
- **Authors**: Neeru Singla, Vishal Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Malaria is a life-threatening mosquito-borne blood disease, hence early detection is very crucial for health. The conventional method for the detection is a microscopic examination of Giemsa-stained blood smears, which needs a highly trained skilled technician. Automated classifications of different stages of malaria still a challenging task, especially having poor sensitivity in detecting the early trophozoite and late trophozoite or schizont stage with limited labelled datasize. The study aims to develop a fast, robust and fully automated system for the classification of different stages of malaria with limited data size by using the pre-trained convolutional neural networks (CNNs) as a classifier and multi-wavelength to increase the sample size. We also compare our customized CNN with other well-known CNNs and shows that our network have a comparable performance with less computational time. We believe that our proposed method can be applied to other limited labelled biological datasets.



### Unsupervised and interpretable scene discovery with Discrete-Attend-Infer-Repeat
- **Arxiv ID**: http://arxiv.org/abs/1903.06581v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06581v1)
- **Published**: 2019-03-14 16:30:27+00:00
- **Updated**: 2019-03-14 16:30:27+00:00
- **Authors**: Duo Wang, Mateja Jamnik, Pietro Lio
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present Discrete Attend Infer Repeat (Discrete-AIR), a Recurrent Auto-Encoder with structured latent distributions containing discrete categorical distributions, continuous attribute distributions, and factorised spatial attention. While inspired by the original AIR model andretaining AIR model's capability in identifying objects in an image, Discrete-AIR provides direct interpretability of the latent codes. We show that for Multi-MNIST and a multiple-objects version of dSprites dataset, the Discrete-AIR model needs just one categorical latent variable, one attribute variable (for Multi-MNIST only), together with spatial attention variables, for efficient inference. We perform analysis to show that the learnt categorical distributions effectively capture the categories of objects in the scene for Multi-MNIST and for Multi-Sprites.



### Deep Residual Autoencoder for quality independent JPEG restoration
- **Arxiv ID**: http://arxiv.org/abs/1903.06117v1
- **DOI**: 10.1109/ACCESS.2020.2984387
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06117v1)
- **Published**: 2019-03-14 16:51:18+00:00
- **Updated**: 2019-03-14 16:51:18+00:00
- **Authors**: Simone Zini, Simone Bianco, Raimondo Schettini
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose a deep residual autoencoder exploiting Residual-in-Residual Dense Blocks (RRDB) to remove artifacts in JPEG compressed images that is independent from the Quality Factor (QF) used. The proposed approach leverages both the learning capacity of deep residual networks and prior knowledge of the JPEG compression pipeline. The proposed model operates in the YCbCr color space and performs JPEG artifact restoration in two phases using two different autoencoders: the first one restores the luma channel exploiting 2D convolutions; the second one, using the restored luma channel as a guide, restores the chroma channels explotining 3D convolutions. Extensive experimental results on three widely used benchmark datasets (i.e. LIVE1, BDS500, and CLASSIC-5) show that our model is able to outperform the state of the art with respect to all the evaluation metrics considered (i.e. PSNR, PSNR-B, and SSIM). This results is remarkable since the approaches in the state of the art use a different set of weights for each compression quality, while the proposed model uses the same weights for all of them, making it applicable to images in the wild where the QF used for compression is unkwnown. Furthermore, the proposed model shows a greater robustness than state-of-the-art methods when applied to compression qualities not seen during training.



### Compression and Interpretability of Deep Neural Networks via Tucker Tensor Layer: From First Principles to Tensor Valued Back-Propagation
- **Arxiv ID**: http://arxiv.org/abs/1903.06133v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1903.06133v2)
- **Published**: 2019-03-14 17:19:38+00:00
- **Updated**: 2020-01-06 10:41:42+00:00
- **Authors**: Giuseppe G. Calvi, Ahmad Moniri, Mahmoud Mahfouz, Qibin Zhao, Danilo P. Mandic
- **Comment**: None
- **Journal**: None
- **Summary**: This work aims to help resolve the two main stumbling blocks in the application of Deep Neural Networks (DNNs), that is, the exceedingly large number of trainable parameters and their physical interpretability. This is achieved through a tensor valued approach, based on the proposed Tucker Tensor Layer (TTL), as an alternative to the dense weight-matrices of DNNs. This allows us to treat the weight-matrices of general DNNs as a matrix unfolding of a higher order weight-tensor. By virtue of the compression properties of tensor decompositions, this enables us to introduce a novel and efficient framework for exploiting the multi-way nature of the weight-tensor in order to dramatically reduce the number of DNN parameters. We also derive the tensor valued back-propagation algorithm within the TTL framework, by extending the notion of matrix derivatives to tensors. In this way, the physical interpretability of the Tucker decomposition is exploited to gain physical insights into the NN training, through the process of computing gradients with respect to each factor matrix. The proposed framework is validated on both synthetic data, and the benchmark datasets MNIST, Fashion-MNIST, and CIFAR-10. Overall, through the ability to provide the relative importance of each data feature in training, the TTL back-propagation is shown to help mitigate the "black-box" nature inherent to NNs. Experiments also illustrate that the TTL achieves a 66.63-fold compression on MNIST and Fashion-MNIST, while, by simplifying the VGG-16 network, it achieves a 10\% speed up in training time, at a comparable performance.



### Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-grained Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.06150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06150v2)
- **Published**: 2019-03-14 17:52:18+00:00
- **Updated**: 2019-06-11 08:25:32+00:00
- **Authors**: Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost. In this paper, we propose to learn such fine-grained features from hundreds of part proposals by Trilinear Attention Sampling Network (TASN) in an efficient teacher-student manner. Specifically, TASN consists of 1) a trilinear attention module, which generates attention maps by modeling the inter-channel relationships, 2) an attention-based sampler which highlights attended parts with high resolution, and 3) a feature distiller, which distills part features into a global one by weight sharing and feature preserving strategies. Extensive experiments verify that TASN yields the best performance under the same settings with the most competitive approaches, in iNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.



### Show, Translate and Tell
- **Arxiv ID**: http://arxiv.org/abs/1903.06275v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06275v1)
- **Published**: 2019-03-14 21:50:09+00:00
- **Updated**: 2019-03-14 21:50:09+00:00
- **Authors**: Dheeraj Peri, Shagan Sah, Raymond Ptucha
- **Comment**: None
- **Journal**: None
- **Summary**: Humans have an incredible ability to process and understand information from multiple sources such as images, video, text, and speech. Recent success of deep neural networks has enabled us to develop algorithms which give machines the ability to understand and interpret this information. There is a need to both broaden their applicability and develop methods which correlate visual information along with semantic content. We propose a unified model which jointly trains on images and captions, and learns to generate new captions given either an image or a caption query. We evaluate our model on three different tasks namely cross-modal retrieval, image captioning, and sentence paraphrasing. Our model gains insight into cross-modal vector embeddings, generalizes well on multiple tasks and is competitive to state of the art methods on retrieval.



