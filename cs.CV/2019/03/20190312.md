# Arxiv Papers in cs.CV on 2019-03-12
### Graph Hierarchical Convolutional Recurrent Neural Network (GHCRNN) for Vehicle Condition Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.06261v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.soc-ph
- **Links**: [PDF](http://arxiv.org/pdf/1903.06261v1)
- **Published**: 2019-03-12 01:24:47+00:00
- **Updated**: 2019-03-12 01:24:47+00:00
- **Authors**: Mingming Lu, Kunfang Zhang, Haiying Liu, Naixue Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: The prediction of urban vehicle flow and speed can greatly facilitate people's travel, and also can provide reasonable advice for the decision-making of relevant government departments. However, due to the spatial, temporal and hierarchy of vehicle flow and many influencing factors such as weather, it is difficult to prediction. Most of the existing research methods are to extract spatial structure information on the road network and extract time series information from the historical data. However, when extracting spatial features, these methods have higher time and space complexity, and incorporate a lot of noise. It is difficult to apply on large graphs, and only considers the influence of surrounding connected road nodes on the central node, ignoring a very important hierarchical relationship, namely, similar information of similar node features and road network structures. In response to these problems, this paper proposes the Graph Hierarchical Convolutional Recurrent Neural Network (GHCRNN) model. The model uses GCN (Graph Convolutional Networks) to extract spatial feature, GRU (Gated Recurrent Units) to extract temporal feature, and uses the learnable Pooling to extract hierarchical information, eliminate redundant information and reduce complexity. Applying this model to the vehicle flow and speed data of Shenzhen and Los Angeles has been well verified, and the time and memory consumption are effectively reduced under the compared precision.



### Transfer Adaptation Learning: A Decade Survey
- **Arxiv ID**: http://arxiv.org/abs/1903.04687v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04687v2)
- **Published**: 2019-03-12 01:32:59+00:00
- **Updated**: 2020-11-22 03:09:23+00:00
- **Authors**: Lei Zhang, Xinbo Gao
- **Comment**: 26 pages, 4 figures
- **Journal**: None
- **Summary**: The world we see is ever-changing and it always changes with people, things, and the environment. Domain is referred to as the state of the world at a certain moment. A research problem is characterized as transfer adaptation learning (TAL) when it needs knowledge correspondence between different moments/domains. Conventional machine learning aims to find a model with the minimum expected risk on test data by minimizing the regularized empirical risk on the training data, which, however, supposes that the training and test data share similar joint probability distribution. TAL aims to build models that can perform tasks of target domain by learning knowledge from a semantic related but distribution different source domain. It is an energetic research filed of increasing influence and importance, which is presenting a blowout publication trend. This paper surveys the advances of TAL methodologies in the past decade, and the technical challenges and essential problems of TAL have been observed and discussed with deep insights and new perspectives. Broader solutions of transfer adaptation learning being created by researchers are identified, i.e., instance re-weighting adaptation, feature adaptation, classifier adaptation, deep network adaptation and adversarial adaptation, which are beyond the early semi-supervised and unsupervised split. The survey helps researchers rapidly but comprehensively understand and identify the research foundation, research status, theoretical limitations, future challenges and under-studied issues (universality, interpretability, and credibility) to be broken in the field toward universal representation and safe applications in open-world scenarios.



### Knowledge Adaptation for Efficient Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.04688v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04688v1)
- **Published**: 2019-03-12 01:34:39+00:00
- **Updated**: 2019-03-12 01:34:39+00:00
- **Authors**: Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan
- **Comment**: Accepted to IEEE Conf. Computer Vision and Pattern Recognition, 2019
- **Journal**: None
- **Summary**: Both accuracy and efficiency are of significant importance to the task of semantic segmentation. Existing deep FCNs suffer from heavy computations due to a series of high-resolution feature maps for preserving the detailed knowledge in dense estimation. Although reducing the feature map resolution (i.e., applying a large overall stride) via subsampling operations (e.g., pooling and convolution striding) can instantly increase the efficiency, it dramatically decreases the estimation accuracy. To tackle this dilemma, we propose a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride. To handle the inconsistency between the features of the student and teacher network, we optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder. Moreover, an affinity distillation module is proposed to capture the long-range dependency by calculating the non-local interactions across the whole image. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three popular benchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly competitive baseline, our proposed method can improve the performance of a student network by 2.5\% (mIOU boosts from 70.2 to 72.7 on the cityscapes test set) and can train a better compact model with only 8\% float operations (FLOPS) of a model that achieves comparable performances.



### A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1903.04704v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04704v2)
- **Published**: 2019-03-12 02:33:00+00:00
- **Updated**: 2019-04-10 04:00:33+00:00
- **Authors**: Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong
- **Comment**: 8 pages paper, 3 pages supplementary material, CVPR Oral paper
- **Journal**: None
- **Summary**: This paper focuses on the challenging task of learning 3D object surface reconstructions from single RGB images. Existing methods achieve varying degrees of success by using different geometric representations. However, they all have their own drawbacks, and cannot well reconstruct those surfaces of complex topologies. To this end, we propose in this paper a skeleton-bridged, stage-wise learning approach to address the challenge. Our use of skeleton is due to its nice property of topology preservation, while being of lower complexity to learn. To learn skeleton from an input image, we design a deep architecture whose decoder is based on a novel design of parallel streams respectively for synthesis of curve- and surface-like skeleton points. We use different shape representations of point cloud, volume, and mesh in our stage-wise learning, in order to take their respective advantages. We also propose multi-stage use of the input image to correct prediction errors that are possibly accumulated in each stage. We conduct intensive experiments to investigate the efficacy of our proposed approach. Qualitative and quantitative results on representative object categories of both simple and complex topologies demonstrate the superiority of our approach over existing ones. We will make our ShapeNet-Skeleton dataset publicly available.



### High Frame Rate Video Reconstruction based on an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1903.06531v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06531v3)
- **Published**: 2019-03-12 02:34:23+00:00
- **Updated**: 2020-11-11 01:49:56+00:00
- **Authors**: Liyuan Pan, Richard Hartley, Cedric Scheerlinck, Miaomiao Liu, Xin Yu, Yuchao Dai
- **Comment**: TPAMI 2020. arXiv admin note: substantial text overlap with
  arXiv:1811.10180
- **Journal**: None
- **Summary**: Event-based cameras measure intensity changes (called `events') with microsecond accuracy under high-speed motion and challenging lighting conditions. With the `active pixel sensor' (APS), the `Dynamic and Active-pixel Vision Sensor' (DAVIS) allows the simultaneous output of intensity frames and events. However, the output images are captured at a relatively low frame rate and often suffer from motion blur. A blurred image can be regarded as the integral of a sequence of latent images, while events indicate changes between the latent images. Thus, we are able to model the blur-generation process by associating event data to a latent sharp image. Based on the abundant event data alongside a low frame rate, easily blurred images, we propose a simple yet effective approach to reconstruct high-quality and high frame rate sharp videos. Starting with a single blurred frame and its event data from DAVIS, we propose the Event-based Double Integral (EDI) model and solve it by adding regularization terms. Then, we extend it to multiple Event-based Double Integral (mEDI) model to get more smooth results based on multiple images and their events. Furthermore, we provide a new and more efficient solver to minimize the proposed energy model. By optimizing the energy function, we achieve significant improvements in removing blur and the reconstruction of a high temporal resolution video. The video generation is based on solving a simple non-convex optimization problem in a single scalar variable. Experimental results on both synthetic and real datasets demonstrate the superiority of our mEDI model and optimization method compared to the state-of-the-art.



### Deep Learning for Automated Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.04711v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.04711v1)
- **Published**: 2019-03-12 03:28:37+00:00
- **Updated**: 2019-03-12 03:28:37+00:00
- **Authors**: Wentao Zhu
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: Medical imaging is an essential tool in many areas of medical applications, used for both diagnosis and treatment. However, reading medical images and making diagnosis or treatment recommendations require specially trained medical specialists. The current practice of reading medical images is labor-intensive, time-consuming, costly, and error-prone. It would be more desirable to have a computer-aided system that can automatically make diagnosis and treatment recommendations. Recent advances in deep learning enable us to rethink the ways of clinician diagnosis based on medical images. In this thesis, we will introduce 1) mammograms for detecting breast cancers, the most frequently diagnosed solid cancer for U.S. women, 2) lung CT images for detecting lung cancers, the most frequently diagnosed malignant cancer, and 3) head and neck CT images for automated delineation of organs at risk in radiotherapy. First, we will show how to employ the adversarial concept to generate the hard examples improving mammogram mass segmentation. Second, we will demonstrate how to use the weakly labeled data for the mammogram breast cancer diagnosis by efficiently design deep learning for multi-instance learning. Third, the thesis will walk through DeepLung system which combines deep 3D ConvNets and GBM for automated lung nodule detection and classification. Fourth, we will show how to use weakly labeled data to improve existing lung nodule detection system by integrating deep learning with a probabilistic graphic model. Lastly, we will demonstrate the AnatomyNet which is thousands of times faster and more accurate than previous methods on automated anatomy segmentation.



### Progressive Generative Adversarial Binary Networks for Music Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.04722v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1903.04722v1)
- **Published**: 2019-03-12 04:16:20+00:00
- **Updated**: 2019-03-12 04:16:20+00:00
- **Authors**: Manan Oza, Himanshu Vaghela, Kriti Srivastava
- **Comment**: None
- **Journal**: None
- **Summary**: Recent improvements in generative adversarial network (GAN) training techniques prove that progressively training a GAN drastically stabilizes the training and improves the quality of outputs produced. Adding layers after the previous ones have converged has proven to help in better overall convergence and stability of the model as well as reducing the training time by a sufficient amount. Thus we use this training technique to train the model progressively in the time and pitch domain i.e. starting from a very small time value and pitch range we gradually expand the matrix sizes until the end result is a completely trained model giving outputs having tensor sizes [4 (bar) x 96 (time steps) x 84 (pitch values) x 8 (tracks)]. As proven in previously proposed models deterministic binary neurons also help in improving the results. Thus we make use of a layer of deterministic binary neurons at the end of the generator to get binary valued outputs instead of fractional values existing between 0 and 1.



### Occlusion-guided compact template learning for ensemble deep network-based pose-invariant face recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.04752v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04752v2)
- **Published**: 2019-03-12 07:24:33+00:00
- **Updated**: 2019-04-15 08:32:38+00:00
- **Authors**: Yuhang Wu, Ioannis A. Kakadiaris
- **Comment**: Accepted by International Conference on Biometrics (ICB 2019) as an
  Oral presentation
- **Journal**: None
- **Summary**: Concatenation of the deep network representations extracted from different facial patches helps to improve face recognition performance. However, the concatenated facial template increases in size and contains redundant information. Previous solutions aim to reduce the dimensionality of the facial template without considering the occlusion pattern of the facial patches. In this paper, we propose an occlusion-guided compact template learning (OGCTL) approach that only uses the information from visible patches to construct the compact template. The compact face representation is not sensitive to the number of patches that are used to construct the facial template and is more suitable for incorporating the information from different view angles for image-set based face recognition. Instead of using occlusion masks in face matching (e.g., DPRFS [38]), the proposed method uses occlusion masks in template construction and achieves significantly better image-set based face verification performance on a challenging database with a template size that is an order-of-magnitude smaller than DPRFS.



### Paradox in Deep Neural Networks: Similar yet Different while Different yet Similar
- **Arxiv ID**: http://arxiv.org/abs/1903.04772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04772v1)
- **Published**: 2019-03-12 08:04:44+00:00
- **Updated**: 2019-03-12 08:04:44+00:00
- **Authors**: Arash Akbarinia, Karl R. Gegenfurtner
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning is advancing towards a data-science approach, implying a necessity to a line of investigation to divulge the knowledge learnt by deep neuronal networks. Limiting the comparison among networks merely to a predefined intelligent ability, according to ground truth, does not suffice, it should be associated with innate similarity of these artificial entities. Here, we analysed multiple instances of an identical architecture trained to classify objects in static images (CIFAR and ImageNet data sets). We evaluated the performance of the networks under various distortions and compared it to the intrinsic similarity between their constituent kernels. While we expected a close correspondence between these two measures, we observed a puzzling phenomenon. Pairs of networks whose kernels' weights are over 99.9% correlated can exhibit significantly different performances, yet other pairs with no correlation can reach quite compatible levels of performance. We show implications of this for transfer learning, and argue its importance in our general understanding of what intelligence is, whether natural or artificial.



### Semi-Supervised Self-Taught Deep Learning for Finger Bones Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.04778v1
- **DOI**: 10.1109/BHI.2019.8834460
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04778v1)
- **Published**: 2019-03-12 08:32:33+00:00
- **Updated**: 2019-03-12 08:32:33+00:00
- **Authors**: Ziyuan Zhao, Xiaoman Zhang, Cen Chen, Wei Li, Songyou Peng, Jie Wang, Xulei Yang, Le Zhang, Zeng Zeng
- **Comment**: IEEE BHI 2019 accepted
- **Journal**: 2019 IEEE EMBS International Conference on Biomedical & Health
  Informatics (BHI)
- **Summary**: Segmentation stands at the forefront of many high-level vision tasks. In this study, we focus on segmenting finger bones within a newly introduced semi-supervised self-taught deep learning framework which consists of a student network and a stand-alone teacher module. The whole system is boosted in a life-long learning manner wherein each step the teacher module provides a refinement for the student network to learn with newly unlabeled data. Experimental results demonstrate the superiority of the proposed method over conventional supervised deep learning methods.



### Spiking-YOLO: Spiking Neural Network for Energy-Efficient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.06530v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06530v2)
- **Published**: 2019-03-12 08:34:47+00:00
- **Updated**: 2019-11-24 16:00:31+00:00
- **Authors**: Seijoon Kim, Seongsik Park, Byunggook Na, Sungroh Yoon
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: Over the past decade, deep neural networks (DNNs) have demonstrated remarkable performance in a variety of applications. As we try to solve more advanced problems, increasing demands for computing and power resources has become inevitable. Spiking neural networks (SNNs) have attracted widespread interest as the third-generation of neural networks due to their event-driven and low-powered nature. SNNs, however, are difficult to train, mainly owing to their complex dynamics of neurons and non-differentiable spike operations. Furthermore, their applications have been limited to relatively simple tasks such as image classification. In this study, we investigate the performance degradation of SNNs in a more challenging regression problem (i.e., object detection). Through our in-depth analysis, we introduce two novel methods: channel-wise normalization and signed neuron with imbalanced threshold, both of which provide fast and accurate information transmission for deep SNNs. Consequently, we present a first spiked-based object detection model, called Spiking-YOLO. Our experiments show that Spiking-YOLO achieves remarkable results that are comparable (up to 98%) to those of Tiny YOLO on non-trivial datasets, PASCAL VOC and MS COCO. Furthermore, Spiking-YOLO on a neuromorphic chip consumes approximately 280 times less energy than Tiny YOLO and converges 2.3 to 4 times faster than previous SNN conversion methods.



### Image Classification base on PCA of Multi-view Deep Representation
- **Arxiv ID**: http://arxiv.org/abs/1903.04814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04814v1)
- **Published**: 2019-03-12 10:14:57+00:00
- **Updated**: 2019-03-12 10:14:57+00:00
- **Authors**: Yaoqi Sun, Liang Li, Liang Zheng, Ji Hu, Yatong Jiang, Chenggang Yan
- **Comment**: None
- **Journal**: None
- **Summary**: In the age of information explosion, image classification is the key technology of dealing with and organizing a large number of image data. Currently, the classical image classification algorithms are mostly based on RGB images or grayscale images, and fail to make good use of the depth information about objects or scenes. The depth information in the images has a strong complementary effect, which can enhance the classification accuracy significantly. In this paper, we propose an image classification technology using principal component analysis based on multi-view depth characters. In detail, firstly, the depth image of the original image is estimated; secondly, depth characters are extracted from the RGB views and the depth view separately, and then the reducing dimension operation through the PCA is implemented. Eventually, the SVM is applied to image classification. The experimental results show that the method has good performance.



### Unsupervised motion saliency map estimation based on optical flow inpainting
- **Arxiv ID**: http://arxiv.org/abs/1903.04842v2
- **DOI**: 10.1109/ICIP.2019.8803542
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04842v2)
- **Published**: 2019-03-12 11:15:14+00:00
- **Updated**: 2019-11-04 10:06:40+00:00
- **Authors**: L. Maczyta, P. Bouthemy, O. Le Meur
- **Comment**: None
- **Journal**: International Conference on Image Processing (ICIP) 2019
- **Summary**: The paper addresses the problem of motion saliency in videos, that is, identifying regions that undergo motion departing from its context. We propose a new unsupervised paradigm to compute motion saliency maps. The key ingredient is the flow inpainting stage. Candidate regions are determined from the optical flow boundaries. The residual flow in these regions is given by the difference between the optical flow and the flow inpainted from the surrounding areas. It provides the cue for motion saliency. The method is flexible and general by relying on motion information only. Experimental results on the DAVIS 2016 benchmark demonstrate that the method compares favourably with state-of-the-art video saliency methods.



### Parallel Medical Imaging for Intelligent Medical Image Analysis: Concepts, Methods, and Applications
- **Arxiv ID**: http://arxiv.org/abs/1903.04855v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04855v3)
- **Published**: 2019-03-12 11:50:28+00:00
- **Updated**: 2021-06-29 09:05:14+00:00
- **Authors**: Chao Gou, Tianyu Shen, Wenbo Zheng, Huadan Xue, Hui Yu, Qiang Ji, Zhengyu Jin, Fei-Yue Wang
- **Comment**: None
- **Journal**: None
- **Summary**: There has been much progress in data-driven artificial intelligence technology for medical image analysis in the last decades. However, it still remains challenging due to its distinctive complexity of acquiring and annotating image data, extracting medical domain knowledge, and explaining the diagnostic decision for medical image analysis. In this paper, we propose a data-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for intelligent medical image analysis based on the methodology of interactive ACP-based parallel intelligence. In the PMI framework, computational experiments with predictive learning in a data-driven way are conducted to extract medical knowledge for diagnostic decision support. Artificial imaging systems are introduced to select and prescriptively generate medical image data in a knowledge-driven way to utilize medical domain knowledge. Through the closed-loop optimization based on parallel execution, our proposed PMI framework can boost the generalization ability and alleviate the limitation of medical interpretation for diagnostic decisions. Furthermore, we illustrate the preliminary implementation of PMI method through the case studies of mammogram analysis and skin lesion image analysis. Experimental results on several public medical image datasets demonstrate the effectiveness of proposed PMI.



### Noisy Supervision for Correcting Misaligned Cadaster Maps Without Perfect Ground Truth Data
- **Arxiv ID**: http://arxiv.org/abs/1903.06529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06529v1)
- **Published**: 2019-03-12 14:38:39+00:00
- **Updated**: 2019-03-12 14:38:39+00:00
- **Authors**: Nicolas Girard, Guillaume Charpiat, Yuliya Tarabalka
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning the best performance on a certain task is achieved by fully supervised methods when perfect ground truth labels are available. However, labels are often noisy, especially in remote sensing where manually curated public datasets are rare. We study the multi-modal cadaster map alignment problem for which available annotations are mis-aligned polygons, resulting in noisy supervision. We subsequently set up a multiple-rounds training scheme which corrects the ground truth annotations at each round to better train the model at the next round. We show that it is possible to reduce the noise of the dataset by iteratively training a better alignment model to correct the annotation alignment.



### Discriminative Principal Component Analysis: A REVERSE THINKING
- **Arxiv ID**: http://arxiv.org/abs/1903.04963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.04963v1)
- **Published**: 2019-03-12 14:43:12+00:00
- **Updated**: 2019-03-12 14:43:12+00:00
- **Authors**: Hanli Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach named by Discriminative Principal Component Analysis which is abbreviated as Discriminative PCA in order to enhance separability of PCA by Linear Discriminant Analysis (LDA). The proposed method performs feature extraction by determining a linear projection that captures the most scattered discriminative information. The most innovation of Discriminative PCA is performing PCA on discriminative matrix rather than original sample matrix. For calculating the required discriminative matrix under low complexity, we exploit LDA on a converted matrix to obtain within-class matrix and between-class matrix thereof. During the computation process, we utilise direct linear discriminant analysis (DLDA) to solve the encountered SSS problem. For evaluating the performances of Discriminative PCA in face recognition, we analytically compare it with DLAD and PCA on four well known facial databases, they are PIE, FERET, YALE and ORL respectively. Results in accuracy and running time obtained by nearest neighbour classifier are compared when different number of training images per person used. Not only the superiority and outstanding performance of Discriminative PCA showed in recognition rate, but also the comparable results of running time.



### Cascaded Projection: End-to-End Network Compression and Acceleration
- **Arxiv ID**: http://arxiv.org/abs/1903.04988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.04988v1)
- **Published**: 2019-03-12 15:20:10+00:00
- **Updated**: 2019-03-12 15:20:10+00:00
- **Authors**: Breton Minnehan, Andreas Savakis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a data-driven approach for deep convolutional neural network compression that achieves high accuracy with high throughput and low memory requirements. Current network compression methods either find a low-rank factorization of the features that requires more memory, or select only a subset of features by pruning entire filter channels. We propose the Cascaded Projection (CaP) compression method that projects the output and input filter channels of successive layers to a unified low dimensional space based on a low-rank projection. We optimize the projection to minimize classification loss and the difference between the next layer's features in the compressed and uncompressed networks. To solve this non-convex optimization problem we propose a new optimization method of a proxy matrix using backpropagation and Stochastic Gradient Descent (SGD) with geometric constraints. Our cascaded projection approach leads to improvements in all critical areas of network compression: high accuracy, low memory consumption, low parameter count and high processing speed. The proposed CaP method demonstrates state-of-the-art results compressing VGG16 and ResNet networks with over 4x reduction in the number of computations and excellent performance in top-5 accuracy on the ImageNet dataset before and after fine-tuning.



### Theory III: Dynamics and Generalization in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.04991v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.04991v5)
- **Published**: 2019-03-12 15:24:26+00:00
- **Updated**: 2020-04-11 00:21:50+00:00
- **Authors**: Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Fernanda De La Torre, Jack Hidary, Tomaso Poggio
- **Comment**: 47 pages, 11 figures. This replaces previous versions of Theory III,
  that appeared on Arxiv [arXiv:1806.11379, arXiv:1801.00173] or on the CBMM
  site. v5: Changes throughout the paper to the presentation and tightening
  some of the statements
- **Journal**: None
- **Summary**: The key to generalization is controlling the complexity of the network. However, there is no obvious control of complexity -- such as an explicit regularization term -- in the training of deep networks for classification. We will show that a classical form of norm control -- but kind of hidden -- is present in deep networks trained with gradient descent techniques on exponential-type losses. In particular, gradient descent induces a dynamics of the normalized weights which converge for $t \to \infty$ to an equilibrium which corresponds to a minimum norm (or maximum margin) solution. For sufficiently large but finite $\rho$ -- and thus finite $t$ -- the dynamics converges to one of several margin maximizers, with the margin monotonically increasing towards a limit stationary point of the flow. In the usual case of stochastic gradient descent, most of the stationary points are likely to be convex minima corresponding to a constrained minimizer -- the network with normalized weights-- which corresponds to vanishing regularization. The solution has zero generalization gap, for fixed architecture, asymptotically for $N \to \infty$, where $N$ is the number of training examples. Our approach extends some of the original results of Srebro from linear networks to deep networks and provides a new perspective on the implicit bias of gradient descent. We believe that the elusive complexity control we describe is responsible for the puzzling empirical finding of good predictive performance by deep networks, despite overparametrization.



### Generating Compact Geometric Track-Maps for Train Positioning Applications
- **Arxiv ID**: http://arxiv.org/abs/1903.05014v2
- **DOI**: 10.1109/IVS.2019.8813901
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05014v2)
- **Published**: 2019-03-12 16:00:01+00:00
- **Updated**: 2019-05-15 08:39:26+00:00
- **Authors**: Hanno Winter, Stefan Luthardt, Volker Willert, Jürgen Adamy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a method to generate compact geometric track-maps for train-borne localization applications. Therefore, we first give a brief overview on the purpose of track maps in train-positioning applications. It becomes apparent that there are hardly any adequate methods to generate suitable geometric track-maps. This is why we present a novel map generation procedure. It uses an optimization formulation to find the continuous sequence of track geometries that fits the available measurement data best. The optimization is initialized with the results from a localization filter developed in our previous work. The localization filter also provides the required information for shape identification and measurement association. The presented approach will be evaluated on simulated data as well as on real measurements.



### An End-to-End Network for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.05027v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05027v2)
- **Published**: 2019-03-12 16:30:11+00:00
- **Updated**: 2019-03-13 02:22:59+00:00
- **Authors**: Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, Wei Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Panoptic segmentation, which needs to assign a category label to each pixel and segment each object instance simultaneously, is a challenging topic. Traditionally, the existing approaches utilize two independent models without sharing features, which makes the pipeline inefficient to implement. In addition, a heuristic method is usually employed to merge the results. However, the overlapping relationship between object instances is difficult to determine without sufficient context information during the merging process. To address the problems, we propose a novel end-to-end network for panoptic segmentation, which can efficiently and effectively predict both the instance and stuff segmentation in a single network. Moreover, we introduce a novel spatial ranking module to deal with the occlusion problem between the predicted instances. Extensive experiments have been done to validate the performance of our proposed method and promising results have been achieved on the COCO Panoptic benchmark.



### Placental Flattening via Volumetric Parameterization
- **Arxiv ID**: http://arxiv.org/abs/1903.05044v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05044v3)
- **Published**: 2019-03-12 16:48:23+00:00
- **Updated**: 2019-09-30 18:30:15+00:00
- **Authors**: S. Mazdak Abulnaga, Esra Abaci Turk, Mikhail Bessmeltsev, P. Ellen Grant, Justin Solomon, Polina Golland
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: We present a volumetric mesh-based algorithm for flattening the placenta to a canonical template to enable effective visualization of local anatomy and function. Monitoring placental function in vivo promises to support pregnancy assessment and to improve care outcomes. We aim to alleviate visualization and interpretation challenges presented by the shape of the placenta when it is attached to the curved uterine wall. To do so, we flatten the volumetric mesh that captures placental shape to resemble the well-studied ex vivo shape. We formulate our method as a map from the in vivo shape to a flattened template that minimizes the symmetric Dirichlet energy to control distortion throughout the volume. Local injectivity is enforced via constrained line search during gradient descent. We evaluate the proposed method on 28 placenta shapes extracted from MRI images in a clinical study of placental function. We achieve sub-voxel accuracy in mapping the boundary of the placenta to the template while successfully controlling distortion throughout the volume. We illustrate how the resulting mapping of the placenta enhances visualization of placental anatomy and function. Our code is freely available at https://github.com/mabulnaga/placenta-flattening .



### Dense Classification and Implanting for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.05050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05050v1)
- **Published**: 2019-03-12 16:58:08+00:00
- **Updated**: 2019-03-12 16:58:08+00:00
- **Authors**: Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, Andrei Bursuc
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Training deep neural networks from few examples is a highly challenging and key problem for many computer vision tasks. In this context, we are targeting knowledge transfer from a set with abundant data to other sets with few available examples. We propose two simple and effective solutions: (i) dense classification over feature maps, which for the first time studies local activations in the domain of few-shot learning, and (ii) implanting, that is, attaching new neurons to a previously trained network to learn new, task-specific features. On miniImageNet, we improve the prior state-of-the-art on few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way 1-shot, 5-shot and 10-shot settings respectively.



### A total variation based regularizer promoting piecewise-Lipschitz reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1903.05079v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.05079v1)
- **Published**: 2019-03-12 17:57:13+00:00
- **Updated**: 2019-03-12 17:57:13+00:00
- **Authors**: Martin Burger, Yury Korolev, Carola-Bibiane Schönlieb, Christiane Stollenwerk
- **Comment**: 12 pages, 4 figures, accepted for publication in SSVM conference
  proceedings 2019
- **Journal**: None
- **Summary**: We introduce a new regularizer in the total variation family that promotes reconstructions with a given Lipschitz constant (which can also vary spatially). We prove regularizing properties of this functional and investigate its connections to total variation and infimal convolution type regularizers TVLp and, in particular, establish topological equivalence. Our numerical experiments show that the proposed regularizer can achieve similar performance as total generalized variation while having the advantage of a very intuitive interpretation of its free parameter, which is just a local estimate of the norm of the gradient. It also provides a natural approach to spatially adaptive regularization.



### Universally Slimmable Networks and Improved Training Techniques
- **Arxiv ID**: http://arxiv.org/abs/1903.05134v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1903.05134v2)
- **Published**: 2019-03-12 18:36:02+00:00
- **Updated**: 2019-10-20 19:52:57+00:00
- **Authors**: Jiahui Yu, Thomas Huang
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks



### Unsupervised Discovery of Parts, Structure, and Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1903.05136v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.05136v1)
- **Published**: 2019-03-12 18:39:10+00:00
- **Updated**: 2019-03-12 18:39:10+00:00
- **Authors**: Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: ICLR 2019. The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.



### Low Power Inference for On-Device Visual Recognition with a Quantization-Friendly Solution
- **Arxiv ID**: http://arxiv.org/abs/1903.06791v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06791v1)
- **Published**: 2019-03-12 18:50:34+00:00
- **Updated**: 2019-03-12 18:50:34+00:00
- **Authors**: Chen Feng, Tao Sheng, Zhiyu Liang, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Matthew Ardi, Alexander C. Berg, Yiran Chen, Bo Chen, Kent Gauen, Yung-Hsiang Lu
- **Comment**: Accepted At The 2nd Workshop on Machine Learning on the Phone and
  other Consumer Devices (MLPCD 2)
- **Journal**: None
- **Summary**: The IEEE Low-Power Image Recognition Challenge (LPIRC) is an annual competition started in 2015 that encourages joint hardware and software solutions for computer vision systems with low latency and power. Track 1 of the competition in 2018 focused on the innovation of software solutions with fixed inference engine and hardware. This decision allows participants to submit models online and not worry about building and bringing custom hardware on-site, which attracted a historically large number of submissions. Among the diverse solutions, the winning solution proposed a quantization-friendly framework for MobileNets that achieves an accuracy of 72.67% on the holdout dataset with an average latency of 27ms on a single CPU core of Google Pixel2 phone, which is superior to the best real-time MobileNet models at the time.



### A Visually Plausible Grasping System for Object Manipulation and Interaction in Virtual Reality Environments
- **Arxiv ID**: http://arxiv.org/abs/1903.05238v1
- **DOI**: 10.1016/j.cag.2019.07.003
- **Categories**: **cs.GR**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1903.05238v1)
- **Published**: 2019-03-12 22:15:51+00:00
- **Updated**: 2019-03-12 22:15:51+00:00
- **Authors**: Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez
- **Comment**: None
- **Journal**: None
- **Summary**: Interaction in virtual reality (VR) environments is essential to achieve a pleasant and immersive experience. Most of the currently existing VR applications, lack of robust object grasping and manipulation, which are the cornerstone of interactive systems. Therefore, we propose a realistic, flexible and robust grasping system that enables rich and real-time interactions in virtual environments. It is visually realistic because it is completely user-controlled, flexible because it can be used for different hand configurations, and robust because it allows the manipulation of objects regardless their geometry, i.e. hand is automatically fitted to the object shape. In order to validate our proposal, an exhaustive qualitative and quantitative performance analysis has been carried out. On the one hand, qualitative evaluation was used in the assessment of the abstract aspects such as: hand movement realism, interaction realism and motor control. On the other hand, for the quantitative evaluation a novel error metric has been proposed to visually analyze the performed grips. This metric is based on the computation of the distance from the finger phalanges to the nearest contact point on the object surface. These contact points can be used with different application purposes, mainly in the field of robotics. As a conclusion, system evaluation reports a similar performance between users with previous experience in virtual reality applications and inexperienced users, referring to a steep learning curve.



### Learning Feature Aggregation in Temporal Domain for Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1903.05244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.05244v1)
- **Published**: 2019-03-12 22:34:21+00:00
- **Updated**: 2019-03-12 22:34:21+00:00
- **Authors**: Jakub Špaňhel, Jakub Sochor, Roman Juránek, Petr Dobeš, Vojtěch Bartl, Adam Herout
- **Comment**: Under consideration at Computer Vision and Image Understanding
- **Journal**: None
- **Summary**: Person re-identification is a standard and established problem in the computer vision community. In recent years, vehicle re-identification is also getting more attention. In this paper, we focus on both these tasks and propose a method for aggregation of features in temporal domain as it is common to have multiple observations of the same object. The aggregation is based on weighting different elements of the feature vectors by different weights and it is trained in an end-to-end manner by a Siamese network. The experimental results show that our method outperforms other existing methods for feature aggregation in temporal domain on both vehicle and person re-identification tasks. Furthermore, to push research in vehicle re-identification further, we introduce a novel dataset CarsReId74k. The dataset is not limited to frontal/rear viewpoints. It contains 17,681 unique vehicles, 73,976 observed tracks, and 277,236 positive pairs. The dataset was captured by 66 cameras from various angles.



### Towards Unsupervised Cancer Subtyping: Predicting Prognosis Using A Histologic Visual Dictionary
- **Arxiv ID**: http://arxiv.org/abs/1903.05257v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.05257v1)
- **Published**: 2019-03-12 23:24:23+00:00
- **Updated**: 2019-03-12 23:24:23+00:00
- **Authors**: Hassan Muhammad, Carlie S. Sigel, Gabriele Campanella, Thomas Boerner, Linda M. Pak, Stefan Büttner, Jan N. M. IJzermans, Bas Groot Koerkamp, Michael Doukas, William R. Jarnagin, Amber Simpson, Thomas J. Fuchs
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Unlike common cancers, such as those of the prostate and breast, tumor grading in rare cancers is difficult and largely undefined because of small sample sizes, the sheer volume of time needed to undertake on such a task, and the inherent difficulty of extracting human-observed patterns. One of the most challenging examples is intrahepatic cholangiocarcinoma (ICC), a primary liver cancer arising from the biliary system, for which there is well-recognized tumor heterogeneity and no grading paradigm or prognostic biomarkers. In this paper, we propose a new unsupervised deep convolutional autoencoder-based clustering model that groups together cellular and structural morphologies of tumor in 246 ICC digitized whole slides, based on visual similarity. From this visual dictionary of histologic patterns, we use the clusters as covariates to train Cox-proportional hazard survival models. In univariate analysis, three clusters were significantly associated with recurrence-free survival. Combinations of these clusters were significant in multivariate analysis. In a multivariate analysis of all clusters, five showed significance to recurrence-free survival, however the overall model was not measured to be significant. Finally, a pathologist assigned clinical terminology to the significant clusters in the visual dictionary and found evidence supporting the hypothesis that collagen-enriched fibrosis plays a role in disease severity. These results offer insight into the future of cancer subtyping and show that computational pathology can contribute to disease prognostication, especially in rare cancers.



