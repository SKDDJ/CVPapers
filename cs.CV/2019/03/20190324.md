# Arxiv Papers in cs.CV on 2019-03-24
### Efficiently utilizing complex-valued PolSAR image data via a multi-task deep learning framework
- **Arxiv ID**: http://arxiv.org/abs/1903.09917v2
- **DOI**: 10.1016/j.isprsjprs.2019.09.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09917v2)
- **Published**: 2019-03-24 03:45:44+00:00
- **Updated**: 2019-09-12 08:56:09+00:00
- **Authors**: Lamei Zhang, Hongwei Dong, Bin Zou
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 157,
  November 2019, Pages 59-72
- **Summary**: Convolutional neural networks (CNNs) have been widely used to improve the accuracy of polarimetric synthetic aperture radar (PolSAR) image classification. However, in most studies, the difference between PolSAR images and optical images is rarely considered. Most of the existing CNNs are not tailored for the task of PolSAR image classification, in which complex-valued PolSAR data have been simply equated to real-valued data to fit the optical image processing architectures and avoid complex-valued operations. This is one of the reasons CNNs unable to perform their full capabilities in PolSAR classification. To solve the above problem, the objective of this paper is to develop a tailored CNN framework for PolSAR image classification, which can be implemented from two aspects: Seeking a better form of PolSAR data as the input of CNNs and building matched CNN architectures based on the proposed input form. In this paper, considering the properties of complex-valued numbers, amplitude and phase of complex-valued PolSAR data are extracted as the input for the first time to maintain the integrity of original information while avoiding immature complex-valued operations. Then, a multi-task CNN (MCNN) architecture is proposed to match the improved input form and achieve better classification results. Furthermore, depthwise separable convolution is introduced to the proposed architecture in order to better extract information from the phase information. Experiments on three PolSAR benchmark datasets not only prove that using amplitude and phase as the input do contribute to the improvement of PolSAR classification, but also verify the adaptability between the improved input form and the well-designed architectures.



### SRGAN: Training Dataset Matters
- **Arxiv ID**: http://arxiv.org/abs/1903.09922v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09922v1)
- **Published**: 2019-03-24 04:28:58+00:00
- **Updated**: 2019-03-24 04:28:58+00:00
- **Authors**: Nao Takano, Gita Alaghband
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) in supervised settings can generate photo-realistic corresponding output from low-definition input (SRGAN). Using the architecture presented in the SRGAN original paper [2], we explore how selecting a dataset affects the outcome by using three different datasets to see that SRGAN fundamentally learns objects, with their shape, color, and texture, and redraws them in the output rather than merely attempting to sharpen edges. This is further underscored with our demonstration that once the network learns the images of the dataset, it can generate a photo-like image with even a slight hint of what it might look like for the original from a very blurry edged sketch. Given a set of inference images, the network trained with the same dataset results in a better outcome over the one trained with arbitrary set of images, and we report its significance numerically with Frechet Inception Distance score [22].



### KPTransfer: improved performance and faster convergence from keypoint subset-wise domain transfer in human pose estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.09926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09926v1)
- **Published**: 2019-03-24 05:26:40+00:00
- **Updated**: 2019-03-24 05:26:40+00:00
- **Authors**: Kanav Vats, Helmut Neher, Alexander Wong, David A. Clausi, John Zelek
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel approach called KPTransfer for improving modeling performance for keypoint detection deep neural networks via domain transfer between different keypoint subsets. This approach is motivated by the notion that rich contextual knowledge can be transferred between different keypoint subsets representing separate domains. In particular, the proposed method takes into account various keypoint subsets/domains by sequentially adding and removing keypoints. Contextual knowledge is transferred between two separate domains via domain transfer. Experiments to demonstrate the efficacy of the proposed KPTransfer approach were performed for the task of human pose estimation on the MPII dataset, with comparisons against random initialization and frozen weight extraction configurations. Experimental results demonstrate the efficacy of performing domain transfer between two different joint subsets resulting in a PCKh improvement of up to 1.1 over random initialization on joints such as wrists and knee in certain joint splits with an overall PCKh improvement of 0.5. Domain transfer from a different set of joints not only results in improved accuracy but also results in faster convergence because of mutual co-adaptations of weights resulting from the contextual knowledge of the pose from a different set of joints.



### Variational Inference with Latent Space Quantization for Adversarial Resilience
- **Arxiv ID**: http://arxiv.org/abs/1903.09940v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09940v2)
- **Published**: 2019-03-24 07:47:01+00:00
- **Updated**: 2019-09-06 06:48:07+00:00
- **Authors**: Vinay Kyatham, Mayank Mishra, Tarun Kumar Yadav, Deepak Mishra, Prathosh AP
- **Comment**: None
- **Journal**: None
- **Summary**: Despite their tremendous success in modelling high-dimensional data manifolds, deep neural networks suffer from the threat of adversarial attacks - Existence of perceptually valid input-like samples obtained through careful perturbation that lead to degradation in the performance of the underlying model. Major concerns with existing defense mechanisms include non-generalizability across different attacks, models and large inference time. In this paper, we propose a generalized defense mechanism capitalizing on the expressive power of regularized latent space based generative models. We design an adversarial filter, devoid of access to classifier and adversaries, which makes it usable in tandem with any classifier. The basic idea is to learn a Lipschitz constrained mapping from the data manifold, incorporating adversarial perturbations, to a quantized latent space and re-map it to the true data manifold. Specifically, we simultaneously auto-encode the data manifold and its perturbations implicitly through the perturbations of the regularized and quantized generative latent space, realized using variational inference. We demonstrate the efficacy of the proposed formulation in providing resilience against multiple attack types (black and white box) and methods, while being almost real-time. Our experiments show that the proposed method surpasses the state-of-the-art techniques in several cases.



### Periphery-Fovea Multi-Resolution Driving Model guided by Human Attention
- **Arxiv ID**: http://arxiv.org/abs/1903.09950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09950v1)
- **Published**: 2019-03-24 09:13:52+00:00
- **Updated**: 2019-03-24 09:13:52+00:00
- **Authors**: Ye Xia, Jinkyu Kim, John Canny, Karl Zipser, David Whitney
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by human vision, we propose a new periphery-fovea multi-resolution driving model that predicts vehicle speed from dash camera videos. The peripheral vision module of the model processes the full video frames in low resolution. Its foveal vision module selects sub-regions and uses high-resolution input from those regions to improve its driving performance. We train the fovea selection module with supervision from driver gaze. We show that adding high-resolution input from predicted human driver gaze locations significantly improves the driving accuracy of the model. Our periphery-fovea multi-resolution model outperforms a uni-resolution periphery-only model that has the same amount of floating-point operations. More importantly, we demonstrate that our driving model achieves a significantly higher performance gain in pedestrian-involved critical situations than in other non-critical situations.



### MUSCO: Multi-Stage Compression of neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.09973v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.09973v4)
- **Published**: 2019-03-24 11:40:18+00:00
- **Updated**: 2019-11-15 12:27:25+00:00
- **Authors**: Julia Gusak, Maksym Kholiavchenko, Evgeny Ponomarev, Larisa Markeeva, Ivan Oseledets, Andrzej Cichocki
- **Comment**: None
- **Journal**: None
- **Summary**: The low-rank tensor approximation is very promising for the compression of deep neural networks. We propose a new simple and efficient iterative approach, which alternates low-rank factorization with a smart rank selection and fine-tuning. We demonstrate the efficiency of our method comparing to non-iterative ones. Our approach improves the compression rate while maintaining the accuracy for a variety of tasks.



### Joint Learning of Discriminative Low-dimensional Image Representations Based on Dictionary Learning and Two-layer Orthogonal Projections
- **Arxiv ID**: http://arxiv.org/abs/1903.09977v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09977v2)
- **Published**: 2019-03-24 12:06:49+00:00
- **Updated**: 2019-03-27 14:56:23+00:00
- **Authors**: Xian Wei, Hao Shen, Yuanxiang Li, Xuan Tang, Bo Jin, Lijun Zhao, Yi Lu Murphey
- **Comment**: Some inappropriate descriptions have been found in this paper
- **Journal**: None
- **Summary**: There are some inadequacies in the language description of this paper that require further improvement. This paper is based on a revision of a conference paper. It is now necessary to further explain the difference between the contributions of the two papers.



### Cluster Alignment with a Teacher for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1903.09980v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09980v2)
- **Published**: 2019-03-24 12:16:59+00:00
- **Updated**: 2019-10-09 05:12:15+00:00
- **Authors**: Zhijie Deng, Yucen Luo, Jun Zhu
- **Comment**: IEEE International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: Deep learning methods have shown promise in unsupervised domain adaptation, which aims to leverage a labeled source domain to learn a classifier for the unlabeled target domain with a different distribution. However, such methods typically learn a domain-invariant representation space to match the marginal distributions of the source and target domains, while ignoring their fine-level structures. In this paper, we propose Cluster Alignment with a Teacher (CAT) for unsupervised domain adaptation, which can effectively incorporate the discriminative clustering structures in both domains for better adaptation. Technically, CAT leverages an implicit ensembling teacher model to reliably discover the class-conditional structure in the feature space for the unlabeled target domain. Then CAT forces the features of both the source and the target domains to form discriminative class-conditional clusters and aligns the corresponding clusters across domains. Empirical results demonstrate that CAT achieves state-of-the-art results in several unsupervised domain adaptation scenarios.



### Pixel-aware Deep Function-mixture Network for Spectral Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.10501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10501v1)
- **Published**: 2019-03-24 13:42:05+00:00
- **Updated**: 2019-03-24 13:42:05+00:00
- **Authors**: Lei Zhang, Zhiqiang Lang, Peng Wang, Wei Wei, Shengcai Liao, Ling Shao, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Spectral super-resolution (SSR) aims at generating a hyperspectral image (HSI) from a given RGB image. Recently, a promising direction for SSR is to learn a complicated mapping function from the RGB image to the HSI counterpart using a deep convolutional neural network. This essentially involves mapping the RGB context within a size-specific receptive field centered at each pixel to its spectrum in the HSI. The focus thereon is to appropriately determine the receptive field size and establish the mapping function from RGB context to the corresponding spectrum. Due to their differences in category or spatial position, pixels in HSIs often require different-sized receptive fields and distinct mapping functions. However, few efforts have been invested to explicitly exploit this prior.   To address this problem, we propose a pixel-aware deep function-mixture network for SSR, which is composed of a new class of modules, termed function-mixture (FM) blocks. Each FM block is equipped with some basis functions, i.e., parallel subnets of different-sized receptive fields. Besides, it incorporates an extra subnet as a mixing function to generate pixel-wise weights, and then linearly mixes the outputs of all basis functions with those generated weights. This enables us to pixel-wisely determine the receptive field size and the mapping function. Moreover, we stack several such FM blocks to further increase the flexibility of the network in learning the pixel-wise mapping. To encourage feature reuse, intermediate features generated by the FM blocks are fused in late stage, which proves to be effective for boosting the SSR performance. Experimental results on three benchmark HSI datasets demonstrate the superiority of the proposed method.



### PLMP -- Point-Line Minimal Problems in Complete Multi-View Visibility
- **Arxiv ID**: http://arxiv.org/abs/1903.10008v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.AG, math.NA, 14M20, 14Q15, 14N99, 15A69, 65H20, 68T45, 13P10, 13P25
- **Links**: [PDF](http://arxiv.org/pdf/1903.10008v2)
- **Published**: 2019-03-24 16:07:49+00:00
- **Updated**: 2019-09-05 10:43:11+00:00
- **Authors**: Timothy Duff, Kathlén Kohn, Anton Leykin, Tomas Pajdla
- **Comment**: None
- **Journal**: None
- **Summary**: We present a complete classification of all minimal problems for generic arrangements of points and lines completely observed by calibrated perspective cameras. We show that there are only 30 minimal problems in total, no problems exist for more than 6 cameras, for more than 5 points, and for more than 6 lines. We present a sequence of tests for detecting minimality starting with counting degrees of freedom and ending with full symbolic and numeric verification of representative examples. For all minimal problems discovered, we present their algebraic degrees, i.e. the number of solutions, which measure their intrinsic difficulty. It shows how exactly the difficulty of problems grows with the number of views. Importantly, several new minimal problems have small degrees that might be practical in image matching and 3D reconstruction.



### Dynamic Spatial Verification for Large-Scale Object-Level Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.10019v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10019v4)
- **Published**: 2019-03-24 16:57:21+00:00
- **Updated**: 2019-12-02 13:34:46+00:00
- **Authors**: Joel Brogan, Aparna Bharati, Daniel Moreira, Kevin Bowyer, Patrick Flynn, Anderson Rocha, Walter Scheirer
- **Comment**: None
- **Journal**: None
- **Summary**: Images from social media can reflect diverse viewpoints, heated arguments, and expressions of creativity, adding new complexity to retrieval tasks. Researchers working onContent-Based Image Retrieval (CBIR) have traditionally tuned their algorithms to match filtered results with user search intent. However, we are now bombarded with composite images of unknown origin, authenticity, and even meaning. With such uncertainty, users may not have an initial idea of what the results of a search query should look like. For instance, hidden people, spliced objects, and subtly altered scenes can be difficult for a user to detect initially in a meme image, but may contribute significantly to its composition. We propose a new approach for spatial verification that aims at modeling object-level regions dynamically clustering keypoints in a 2D Hough space, which are then used to accurately weight small contributing objects within the results, without the need for costly object detection steps. We call this method Objects in Scene to Objects in Scene (OS2OS) score, and it is optimized for fast matrix operations on CPUs. OS2OS performs comparably to state-of-the-art methods in classic CBIR problems, on the Oxford5K, Paris 6K, and Google-Landmarks datasets, without the need for bounding boxes. It also succeeds in emerging retrieval tasks such as image composite matching in the NIST MFC2018 dataset and meme-style composite imagery fromReddit.



### Automated Classification of Histopathology Images Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.10035v2
- **DOI**: 10.1016/j.artmed.2019.101743
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10035v2)
- **Published**: 2019-03-24 18:32:35+00:00
- **Updated**: 2019-11-21 17:25:03+00:00
- **Authors**: Muhammed Talo
- **Comment**: None
- **Journal**: None
- **Summary**: There is a strong need for automated systems to improve diagnostic quality and reduce the analysis time in histopathology image processing. Automated detection and classification of pathological tissue characteristics with computer-aided diagnostic systems are a critical step in the early diagnosis and treatment of diseases. Once a pathology image is scanned by a microscope and loaded onto a computer, it can be used for automated detection and classification of diseases. In this study, the DenseNet-161 and ResNet-50 pre-trained CNN models have been used to classify digital histopathology patches into the corresponding whole slide images via transfer learning technique. The proposed pre-trained models were tested on grayscale and color histopathology images. The DenseNet-161 pre-trained model achieved a classification accuracy of 97.89% using grayscale images and the ResNet-50 model obtained the accuracy of 98.87% for color images. The proposed pre-trained models outperform state-of-the-art methods in all performance metrics to classify digital pathology patches into 24 categories.



### Residual Non-local Attention Networks for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1903.10082v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10082v1)
- **Published**: 2019-03-24 23:40:49+00:00
- **Updated**: 2019-03-24 23:40:49+00:00
- **Authors**: Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu
- **Comment**: To appear in ICLR 2019
- **Journal**: None
- **Summary**: In this paper, we propose a residual non-local attention network for high-quality image restoration. Without considering the uneven distribution of information in the corrupted images, previous methods are restricted by local convolutional operation and equal treatment of spatial- and channel-wise features. To address this issue, we design local and non-local attention blocks to extract features that capture the long-range dependencies between pixels and pay more attention to the challenging parts. Specifically, we design trunk branch and (non-)local mask branch in each (non-)local attention block. The trunk branch is used to extract hierarchical features. Local and non-local mask branches aim to adaptively rescale these hierarchical features with mixed attentions. The local mask branch concentrates on more local structures with convolutional operations, while non-local attention considers more about long-range dependencies in the whole feature map. Furthermore, we propose residual local and non-local attention learning to train the very deep network, which further enhance the representation ability of the network. Our proposed method can be generalized for various image restoration applications, such as image denoising, demosaicing, compression artifacts reduction, and super-resolution. Experiments demonstrate that our method obtains comparable or better results compared with recently leading methods quantitatively and visually.



