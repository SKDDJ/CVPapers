# Arxiv Papers in cs.CV on 2019-03-26
### INFER: INtermediate representations for FuturE pRediction
- **Arxiv ID**: http://arxiv.org/abs/1903.10641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.10641v1)
- **Published**: 2019-03-26 00:32:02+00:00
- **Updated**: 2019-03-26 00:32:02+00:00
- **Authors**: Shashank Srikanth, Junaid Ahmed Ansari, Karnik Ram R, Sarthak Sharma, Krishna Murthy J., Madhava Krishna K
- **Comment**: Manuscript under review. Submitted to IROS 2019
- **Journal**: None
- **Summary**: In urban driving scenarios, forecasting future trajectories of surrounding vehicles is of paramount importance. While several approaches for the problem have been proposed, the best-performing ones tend to require extremely detailed input representations (eg. image sequences). But, such methods do not generalize to datasets they have not been trained on. We propose intermediate representations that are particularly well-suited for future prediction. As opposed to using texture (color) information, we rely on semantics and train an autoregressive model to accurately predict future trajectories of traffic participants (vehicles) (see fig. above). We demonstrate that using semantics provides a significant boost over techniques that operate over raw pixel intensities/disparities. Uncharacteristic of state-of-the-art approaches, our representations and models generalize to completely different datasets, collected across several cities, and also across countries where people drive on opposite sides of the road (left-handed vs right-handed driving). Additionally, we demonstrate an application of our approach in multi-object tracking (data association). To foster further research in transferrable representations and ensure reproducibility, we release all our code and data.



### An Alarm System For Segmentation Algorithm Based On Shape Model
- **Arxiv ID**: http://arxiv.org/abs/1903.10645v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10645v3)
- **Published**: 2019-03-26 00:55:49+00:00
- **Updated**: 2019-08-21 00:26:53+00:00
- **Authors**: Fengze Liu, Yingda Xia, Dong Yang, Alan Yuille, Daguang Xu
- **Comment**: Accepted to ICCV 2019 (10 pages, 4 figures)
- **Journal**: None
- **Summary**: It is usually hard for a learning system to predict correctly on rare events that never occur in the training data, and there is no exception for segmentation algorithms. Meanwhile, manual inspection of each case to locate the failures becomes infeasible due to the trend of large data scale and limited human resource. Therefore, we build an alarm system that will set off alerts when the segmentation result is possibly unsatisfactory, assuming no corresponding ground truth mask is provided. One plausible solution is to project the segmentation results into a low dimensional feature space; then learn classifiers/regressors to predict their qualities. Motivated by this, in this paper, we learn a feature space using the shape information which is a strong prior shared among different datasets and robust to the appearance variation of input data.The shape feature is captured using a Variational Auto-Encoder (VAE) network that trained with only the ground truth masks. During testing, the segmentation results with bad shapes shall not fit the shape prior well, resulting in large loss values. Thus, the VAE is able to evaluate the quality of segmentation result on unseen data, without using ground truth. Finally, we learn a regressor in the one-dimensional feature space to predict the qualities of segmentation results. Our alarm system is evaluated on several recent state-of-art segmentation algorithms for 3D medical segmentation tasks. Compared with other standard quality assessment methods, our system consistently provides more reliable prediction on the qualities of segmentation results.



### A Probabilistic Bitwise Genetic Algorithm for B-Spline based Image Deformation Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.10657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10657v1)
- **Published**: 2019-03-26 02:24:07+00:00
- **Updated**: 2019-03-26 02:24:07+00:00
- **Authors**: Takumi Nakane, Takuya Akashi, Xuequan Lu, Chao Zhang
- **Comment**: GECCO2019
- **Journal**: None
- **Summary**: We propose a novel genetic algorithm to solve the image deformation estimation problem by preserving the genetic diversity. As a classical problem, there is always a trade-off between the complexity of deformation models and the difficulty of parameters search in image deformation. 2D cubic B-spline surface is a highly free-form deformation model and is able to handle complex deformations such as fluid image distortions. However, it is challenging to estimate an apposite global solution. To tackle this problem, we develop a genetic operation named probabilistic bitwise operation (PBO) to replace the crossover and mutation operations, which can preserve the diversity during generation iteration and achieve better coverage ratio of the solution space. Furthermore, a selection strategy named annealing selection is proposed to control the convergence. Qualitative and quantitative results on synthetic data show the effectiveness of our method.



### Unpaired Image Captioning via Scene Graph Alignments
- **Arxiv ID**: http://arxiv.org/abs/1903.10658v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10658v4)
- **Published**: 2019-03-26 02:52:22+00:00
- **Updated**: 2019-08-17 03:27:53+00:00
- **Authors**: Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, Gang Wang
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: Most of current image captioning models heavily rely on paired image-caption datasets. However, getting large scale image-caption paired data is labor-intensive and time-consuming. In this paper, we present a scene graph-based approach for unpaired image captioning. Our framework comprises an image scene graph generator, a sentence scene graph generator, a scene graph encoder, and a sentence decoder. Specifically, we first train the scene graph encoder and the sentence decoder on the text modality. To align the scene graphs between images and sentences, we propose an unsupervised feature alignment method that maps the scene graph features from the image to the sentence modality. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin.



### Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.10661v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10661v1)
- **Published**: 2019-03-26 03:19:42+00:00
- **Updated**: 2019-03-26 03:19:42+00:00
- **Authors**: Zhiwei Liu, Xiangyu Zhu, Guosheng Hu, Haiyun Guo, Ming Tang, Zhen Lei, Neil M. Robertson, Jinqiao Wang
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Recently, deep learning based facial landmark detection has achieved great success. Despite this, we notice that the semantic ambiguity greatly degrades the detection performance. Specifically, the semantic ambiguity means that some landmarks (e.g. those evenly distributed along the face contour) do not have clear and accurate definition, causing inconsistent annotations by annotators. Accordingly, these inconsistent annotations, which are usually provided by public databases, commonly work as the ground-truth to supervise network training, leading to the degraded accuracy. To our knowledge, little research has investigated this problem. In this paper, we propose a novel probabilistic model which introduces a latent variable, i.e. the 'real' ground-truth which is semantically consistent, to optimize. This framework couples two parts (1) training landmark detection CNN and (2) searching the 'real' ground-truth. These two parts are alternatively optimized: the searched 'real' ground-truth supervises the CNN training; and the trained CNN assists the searching of 'real' ground-truth. In addition, to recover the unconfidently predicted landmarks due to occlusion and low quality, we propose a global heatmap correction unit (GHCU) to correct outliers by considering the global face shape as a constraint. Extensive experiments on both image-based (300W and AFLW) and video-based (300-VW) databases demonstrate that our method effectively improves the landmark detection accuracy and achieves the state of the art performance.



### All about Structure: Adapting Structural Information across Domains for Boosting Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.12212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.12212v1)
- **Published**: 2019-03-26 03:23:30+00:00
- **Updated**: 2019-03-26 03:23:30+00:00
- **Authors**: Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, Wei-Chen Chiu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we tackle the problem of unsupervised domain adaptation for the task of semantic segmentation, where we attempt to transfer the knowledge learned upon synthetic datasets with ground-truth labels to real-world images without any annotation. With the hypothesis that the structural content of images is the most informative and decisive factor to semantic segmentation and can be readily shared across domains, we propose a Domain Invariant Structure Extraction (DISE) framework to disentangle images into domain-invariant structure and domain-specific texture representations, which can further realize image-translation across domains and enable label transfer to improve segmentation performance. Extensive experiments verify the effectiveness of our proposed DISE model and demonstrate its superiority over several state-of-the-art approaches.



### Combination of Multiple Global Descriptors for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.10663v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10663v4)
- **Published**: 2019-03-26 03:38:38+00:00
- **Updated**: 2020-04-23 06:20:02+00:00
- **Authors**: HeeJae Jun, Byungsoo Ko, Youngjoon Kim, Insik Kim, Jongtack Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies in image retrieval task have shown that ensembling different models and combining multiple global descriptors lead to performance improvement. However, training different models for the ensemble is not only difficult but also inefficient with respect to time and memory. In this paper, we propose a novel framework that exploits multiple global descriptors to get an ensemble effect while it can be trained in an end-to-end manner. The proposed framework is flexible and expandable by the global descriptor, CNN backbone, loss, and dataset. Moreover, we investigate the effectiveness of combining multiple global descriptors with quantitative and qualitative analysis. Our extensive experiments show that the combined descriptor outperforms a single global descriptor, as it can utilize different types of feature properties. In the benchmark evaluation, the proposed framework achieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop Clothes, and Stanford Online Products on image retrieval tasks. Our model implementations and pretrained models are publicly available.



### Blur Removal via Blurred-Noisy Image Pair
- **Arxiv ID**: http://arxiv.org/abs/1903.10667v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10667v4)
- **Published**: 2019-03-26 03:47:03+00:00
- **Updated**: 2020-11-16 06:41:59+00:00
- **Authors**: Chunzhi Gu, Xuequan Lu, Ying He, Chao Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Complex blur such as the mixup of space-variant and space-invariant blur, which is hard to model mathematically, widely exists in real images. In this paper, we propose a novel image deblurring method that does not need to estimate blur kernels. We utilize a pair of images that can be easily acquired in low-light situations: (1) a blurred image taken with low shutter speed and low ISO noise; and (2) a noisy image captured with high shutter speed and high ISO noise. Slicing the blurred image into patches, we extend the Gaussian mixture model (GMM) to model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We compute patch correspondences by analyzing the optical flow between the two images. The Expectation Maximization (EM) algorithm is utilized to estimate the parameters of GMM. To preserve sharp features, we add an additional bilateral term to the objective function in the M-step. We eventually add a detail layer to the deblurred image for refinement. Extensive experiments on both synthetic and real-world data demonstrate that our method outperforms state-of-the-art techniques, in terms of robustness, visual quality, and quantitative metrics.



### AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search
- **Arxiv ID**: http://arxiv.org/abs/1903.11059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11059v2)
- **Published**: 2019-03-26 04:54:53+00:00
- **Updated**: 2019-10-02 02:04:45+00:00
- **Authors**: Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca
- **Comment**: another search algorithm for NAS. arXiv admin note: substantial text
  overlap with arXiv:1805.07440
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\% top-1 accuracy on CIFAR-10, and 75.5\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.



### Mask-ShadowGAN: Learning to Remove Shadows from Unpaired Data
- **Arxiv ID**: http://arxiv.org/abs/1903.10683v3
- **DOI**: 10.1109/ICCV.2019.00256
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1903.10683v3)
- **Published**: 2019-03-26 05:30:55+00:00
- **Updated**: 2019-08-08 13:25:52+00:00
- **Authors**: Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, Pheng-Ann Heng
- **Comment**: Accepted to ICCV 2019
- **Journal**: IEEE International Conference on Computer Vision (ICCV), pp.
  2472-2481, 2019
- **Summary**: This paper presents a new method for shadow removal using unpaired data, enabling us to avoid tedious annotations and obtain more diverse training samples. However, directly employing adversarial learning and cycle-consistency constraints is insufficient to learn the underlying relationship between the shadow and shadow-free domains, since the mapping between shadow and shadow-free images is not simply one-to-one. To address the problem, we formulate Mask-ShadowGAN, a new deep framework that automatically learns to produce a shadow mask from the input shadow image and then takes the mask to guide the shadow generation via re-formulated cycle-consistency constraints. Particularly, the framework simultaneously learns to produce shadow masks and learns to remove shadows, to maximize the overall performance. Also, we prepared an unpaired dataset for shadow removal and demonstrated the effectiveness of Mask-ShadowGAN on various experiments, even it was trained on unpaired data.



### Unsupervised Multi-modal Hashing for Cross-modal retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.00726v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.00726v4)
- **Published**: 2019-03-26 07:47:13+00:00
- **Updated**: 2020-09-27 10:51:56+00:00
- **Authors**: Jun Yu, Xiao-Jun Wu
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: With the advantage of low storage cost and high efficiency, hashing learning has received much attention in the domain of Big Data. In this paper, we propose a novel unsupervised hashing learning method to cope with this open problem to directly preserve the manifold structure by hashing. To address this problem, both the semantic correlation in textual space and the locally geometric structure in the visual space are explored simultaneously in our framework. Besides, the `2;1-norm constraint is imposed on the projection matrices to learn the discriminative hash function for each modality. Extensive experiments are performed to evaluate the proposed method on the three publicly available datasets and the experimental results show that our method can achieve superior performance over the state-of-the-art methods.



### CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy
- **Arxiv ID**: http://arxiv.org/abs/1904.00787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00787v1)
- **Published**: 2019-03-26 08:56:07+00:00
- **Updated**: 2019-03-26 08:56:07+00:00
- **Authors**: Svitlana Alkhimova
- **Comment**: None
- **Journal**: Proceedings of the 2019 3rd International Conference on Frontiers
  of Image Processing (ICFIP 2019). - Florence, Italy, March 16-18, 2019. -
  P008
- **Summary**: This paper presents a new approach for relatively accurate brain region of interest (ROI) detection from dynamic susceptibility contrast (DSC) perfusion magnetic resonance (MR) images of a human head with abnormal brain anatomy. Such images produce problems for automatic brain segmentation algorithms, and as a result, poor perfusion ROI detection affects both quantitative measurements and visual assessment of perfusion data. In the proposed approach image segmentation is based on CUSUM filter usage that was adapted to be applicable to process DSC perfusion MR images. The result of segmentation is a binary mask of brain ROI that is generated via usage of brain boundary location. Each point of the boundary between the brain and surrounding tissues is detected as a change-point by CUSUM filter. Proposed adopted CUSUM filter operates by accumulating the deviations between the observed and expected intensities of image points at the time of moving on a trajectory. Motion trajectory is created by the iterative change of movement direction inside the background region in order to reach brain region, and vice versa after boundary crossing. Proposed segmentation approach was evaluated with Dice index comparing obtained results to the reference standard. Manually marked brain region pixels (reference standard), as well as visual inspection of detected with CUSUM filter usage brain ROI, were provided by experienced radiologists. The results showed that proposed approach is suitable to be used for brain ROI detection from DSC perfusion MR images of a human head with abnormal brain anatomy and can, therefore, be applied in the DSC perfusion data analysis.



### Trait of Gait: A Survey on Gait Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1903.10744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10744v1)
- **Published**: 2019-03-26 09:11:18+00:00
- **Updated**: 2019-03-26 09:11:18+00:00
- **Authors**: Ebenezer R. H. P. Isaac, Susan Elias, Srinivasan Rajagopalan, K. S. Easwarakumar
- **Comment**: None
- **Journal**: None
- **Summary**: Gait analysis is the study of the systematic methods that assess and quantify animal locomotion. The research on gait analysis has considerably evolved through time. It was an ancient art, and it still finds its application today in modern science and medicine. This paper describes how one's gait can be used as a biometric. It shall diversely cover salient research done within the field and explain the nuances and advances in each type of gait analysis. The prominent methods of gait recognition from the early era to the state of the art are covered. This survey also reviews the various gait datasets. The overall aim of this study is to provide a concise roadmap for anyone who wishes to do research in the field of gait biometrics.



### FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1903.10750v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10750v3)
- **Published**: 2019-03-26 09:26:46+00:00
- **Updated**: 2019-11-26 07:54:29+00:00
- **Authors**: Jie Zhou, Xin Tan, Zhiwei Shao, Lizhuang Ma
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: 3D object detection from raw and sparse point clouds has been far less treated to date, compared with its 2D counterpart. In this paper, we propose a novel framework called FVNet for 3D front-view proposal generation and object detection from point clouds. It consists of two stages: generation of front-view proposals and estimation of 3D bounding box parameters. Instead of generating proposals from camera images or bird's-eye-view maps, we first project point clouds onto a cylindrical surface to generate front-view feature maps which retains rich information. We then introduce a proposal generation network to predict 3D region proposals from the generated maps and further extrude objects of interest from the whole point cloud. Finally, we present another network to extract the point-wise features from the extruded object points and regress the final 3D bounding box parameters in the canonical coordinates. Our framework achieves real-time performance with 12ms per point cloud sample. Extensive experiments on the 3D detection benchmark KITTI show that the proposed architecture outperforms state-of-the-art techniques which take either camera images or point clouds as input, in terms of accuracy and inference time.



### Learning Where to See: A Novel Attention Model for Automated Immunohistochemical Scoring
- **Arxiv ID**: http://arxiv.org/abs/1903.10762v1
- **DOI**: 10.1109/TMI.2019.2907049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10762v1)
- **Published**: 2019-03-26 09:53:36+00:00
- **Updated**: 2019-03-26 09:53:36+00:00
- **Authors**: Talha Qaiser, Nasir M. Rajpoot
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating over-amplification of human epidermal growth factor receptor 2 (HER2) on invasive breast cancer (BC) is regarded as a significant predictive and prognostic marker. We propose a novel deep reinforcement learning (DRL) based model that treats immunohistochemical (IHC) scoring of HER2 as a sequential learning task. For a given image tile sampled from multi-resolution giga-pixel whole slide image (WSI), the model learns to sequentially identify some of the diagnostically relevant regions of interest (ROIs) by following a parameterized policy. The selected ROIs are processed by recurrent and residual convolution networks to learn the discriminative features for different HER2 scores and predict the next location, without requiring to process all the sub-image patches of a given tile for predicting the HER2 score, mimicking the histopathologist who would not usually analyze every part of the slide at the highest magnification. The proposed model incorporates a task-specific regularization term and inhibition of return mechanism to prevent the model from revisiting the previously attended locations. We evaluated our model on two IHC datasets: a publicly available dataset from the HER2 scoring challenge contest and another dataset consisting of WSIs of gastroenteropancreatic neuroendocrine tumor sections stained with Glo1 marker. We demonstrate that the proposed model outperforms other methods based on state-of-the-art deep convolutional networks. To the best of our knowledge, this is the first study using DRL for IHC scoring and could potentially lead to wider use of DRL in the domain of computational pathology reducing the computational burden of the analysis of large multigigapixel histology images.



### Veritatem Dies Aperit- Temporally Consistent Depth Prediction Enabled by a Multi-Task Geometric and Semantic Scene Understanding Approach
- **Arxiv ID**: http://arxiv.org/abs/1903.10764v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10764v2)
- **Published**: 2019-03-26 09:59:46+00:00
- **Updated**: 2019-07-19 09:28:14+00:00
- **Authors**: Amir Atapour-Abarghouei, Toby P. Breckon
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Robust geometric and semantic scene understanding is ever more important in many real-world applications such as autonomous driving and robotic navigation. In this paper, we propose a multi-task learning-based approach capable of jointly performing geometric and semantic scene understanding, namely depth prediction (monocular depth estimation and depth completion) and semantic scene segmentation. Within a single temporally constrained recurrent network, our approach uniquely takes advantage of a complex series of skip connections, adversarial training and the temporal constraint of sequential frame recurrence to produce consistent depth and semantic class labels simultaneously. Extensive experimental evaluation demonstrates the efficacy of our approach compared to other contemporary state-of-the-art techniques.



### Micro-expression detection in long videos using optical flow and recurrent neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.10765v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10765v1)
- **Published**: 2019-03-26 10:01:46+00:00
- **Updated**: 2019-03-26 10:01:46+00:00
- **Authors**: Michiel Verburg, Vlado Menkovski
- **Comment**: 6 pages, 6 figures and 1 table
- **Journal**: None
- **Summary**: Facial micro-expressions are subtle and involuntary expressions that can reveal concealed emotions. Micro-expressions are an invaluable source of information in application domains such as lie detection, mental health, sentiment analysis and more. One of the biggest challenges in this field of research is the small amount of available spontaneous micro-expression data. However, spontaneous data collection is burdened by time-consuming and expensive annotation. Hence, methods are needed which can reduce the amount of data that annotators have to review. This paper presents a novel micro-expression spotting method using a recurrent neural network (RNN) on optical flow features. We extract Histogram of Oriented Optical Flow (HOOF) features to encode the temporal changes in selected face regions. Finally, the RNN spots short intervals which are likely to contain occurrences of relevant facial micro-movements. The proposed method is evaluated on the SAMM database. Any chance of subject bias is eliminated by training the RNN using Leave-One-Subject-Out cross-validation. Comparing the spotted intervals with the labeled data shows that the method produced 1569 false positives while obtaining a recall of 0.4654. The initial results show that the proposed method would reduce the video length by a factor of 3.5, while still retaining almost half of the relevant micro-movements. Lastly, as the model gets more data, it becomes better at detecting intervals, which makes the proposed method suitable for supporting the annotation process.



### Improved Dynamic Time Warping (DTW) Approach for Online Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1904.00786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00786v1)
- **Published**: 2019-03-26 10:05:22+00:00
- **Updated**: 2019-03-26 10:05:22+00:00
- **Authors**: Azhar Ahmad Jaini, Ghazali Sulong, Amjad Rehman
- **Comment**: This paper is first author thesis paper
- **Journal**: None
- **Summary**: Online signature verification is the process of verifying time series signature data which is generally obtained from the tablet-based device. Unlike offline signature images, the online signature image data consists of points that are arranged in a sequence of time. The aim of this research is to develop an improved approach to map the strokes in both test and reference signatures. Current methods make use of the Dynamic Time Warping (DTW) algorithm and its variant to segment them before comparing each of its data dimension. This paper presents a modified DTW algorithm with the proposed Lost Box Recovery Algorithm aims to improve the mapping performance for online signature verification



### Cross-modal Subspace Learning via Kernel Correlation Maximization and Discriminative Structure Preserving
- **Arxiv ID**: http://arxiv.org/abs/1904.00776v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.00776v3)
- **Published**: 2019-03-26 11:29:47+00:00
- **Updated**: 2020-01-07 03:25:14+00:00
- **Authors**: Jun Yu, Xiao-Jun Wu
- **Comment**: The paper is under consideration at Multimedia Tools and Applications
- **Journal**: None
- **Summary**: The measure between heterogeneous data is still an open problem. Many research works have been developed to learn a common subspace where the similarity between different modalities can be calculated directly. However, most of existing works focus on learning a latent subspace but the semantically structural information is not well preserved. Thus, these approaches cannot get desired results. In this paper, we propose a novel framework, termed Cross-modal subspace learning via Kernel correlation maximization and Discriminative structure-preserving (CKD), to solve this problem in two aspects. Firstly, we construct a shared semantic graph to make each modality data preserve the neighbor relationship semantically. Secondly, we introduce the Hilbert-Schmidt Independence Criteria (HSIC) to ensure the consistency between feature-similarity and semantic-similarity of samples. Our model not only considers the inter-modality correlation by maximizing the kernel correlation but also preserves the semantically structural information within each modality. The extensive experiments are performed to evaluate the proposed framework on the three public datasets. The experimental results demonstrated that the proposed CKD is competitive compared with the classic subspace learning methods.



### A geometry-inspired decision-based attack
- **Arxiv ID**: http://arxiv.org/abs/1903.10826v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.10826v1)
- **Published**: 2019-03-26 12:18:31+00:00
- **Updated**: 2019-03-26 12:18:31+00:00
- **Authors**: Yujia Liu, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have recently achieved tremendous success in image classification. Recent studies have however shown that they are easily misled into incorrect classification decisions by adversarial examples. Adversaries can even craft attacks by querying the model in black-box settings, where no information about the model is released except its final decision. Such decision-based attacks usually require lots of queries, while real-world image recognition systems might actually restrict the number of queries. In this paper, we propose qFool, a novel decision-based attack algorithm that can generate adversarial examples using a small number of queries. The qFool method can drastically reduce the number of queries compared to previous decision-based attacks while reaching the same quality of adversarial examples. We also enhance our method by constraining adversarial perturbations in low-frequency subspace, which can make qFool even more computationally efficient. Altogether, we manage to fool commercial image recognition systems with a small number of queries, which demonstrates the actual effectiveness of our new algorithm in practice.



### SRM : A Style-based Recalibration Module for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.10829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10829v1)
- **Published**: 2019-03-26 12:23:52+00:00
- **Updated**: 2019-03-26 12:23:52+00:00
- **Authors**: HyunJae Lee, Hyo-Eun Kim, Hyeonseob Nam
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties.



### Large-scale interactive object segmentation with human annotators
- **Arxiv ID**: http://arxiv.org/abs/1903.10830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10830v2)
- **Published**: 2019-03-26 12:26:38+00:00
- **Updated**: 2019-04-17 15:11:28+00:00
- **Authors**: Rodrigo Benenson, Stefan Popov, Vittorio Ferrari
- **Comment**: Accepted at CVPR2019
- **Journal**: None
- **Summary**: Manually annotating object segmentation masks is very time consuming. Interactive object segmentation methods offer a more efficient alternative where a human annotator and a machine segmentation model collaborate. In this paper we make several contributions to interactive segmentation: (1) we systematically explore in simulation the design space of deep interactive segmentation models and report new insights and caveats; (2) we execute a large-scale annotation campaign with real human annotators, producing masks for 2.5M instances on the OpenImages dataset. We plan to release this data publicly, forming the largest existing dataset for instance segmentation. Moreover, by re-annotating part of the COCO dataset, we show that we can produce instance masks 3 times faster than traditional polygon drawing tools while also providing better quality. (3) We present a technique for automatically estimating the quality of the produced masks which exploits indirect signals from the annotation process.



### Attention Based Glaucoma Detection: A Large-scale Database and CNN Model
- **Arxiv ID**: http://arxiv.org/abs/1903.10831v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10831v3)
- **Published**: 2019-03-26 12:29:36+00:00
- **Updated**: 2019-04-21 15:07:48+00:00
- **Authors**: Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang, Hanruo Liu
- **Comment**: 10 pages, 10 figures
- **Journal**: None
- **Summary**: Recently, the attention mechanism has been successfully applied in convolutional neural networks (CNNs), significantly boosting the performance of many computer vision tasks. Unfortunately, few medical image recognition approaches incorporate the attention mechanism in the CNNs. In particular, there exists high redundancy in fundus images for glaucoma detection, such that the attention mechanism has potential in improving the performance of CNN-based glaucoma detection. This paper proposes an attention-based CNN for glaucoma detection (AG-CNN). Specifically, we first establish a large-scale attention based glaucoma (LAG) database, which includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432). The attention maps of the ophthalmologists are also collected in LAG database through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet and a glaucoma classification subnet. Different from other attention-based CNN methods, the features are also visualized as the localized pathological area, which can advance the performance of glaucoma detection. Finally, the experiment results show that the proposed AG-CNN approach significantly advances state-of-the-art glaucoma detection.



### Pixelation is NOT Done in Videos Yet
- **Arxiv ID**: http://arxiv.org/abs/1903.10836v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1903.10836v3)
- **Published**: 2019-03-26 12:38:45+00:00
- **Updated**: 2019-04-24 09:08:24+00:00
- **Authors**: Jizhe Zhou, Chi-Man Pun, YingYu Wang
- **Comment**: major modification on GP models
- **Journal**: None
- **Summary**: This paper introduces an algorithm to protect the privacy of individuals in streaming video data by blurring faces such that face cannot be reliably recognized. This thwarts any possible face recognition, but because all facial details are obscured, the result is of limited use. We propose a new clustering algorithm to create raw trajectories for detected faces. Associating faces across frames to form trajectories, it auto-generates cluster number and discovers new clusters through deep feature and position aggregated affinities. We introduce a Gaussian Process to refine the raw trajectories. We conducted an online experiment with 47 participants to evaluate the effectiveness of face blurring compared to the original photo (as-is), and users' experience (satisfaction, information sufficiency, enjoyment, social presence, and filter likeability)



### Netherlands Dataset: A New Public Dataset for Machine Learning in Seismic Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1904.00770v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.geo-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00770v1)
- **Published**: 2019-03-26 13:12:14+00:00
- **Updated**: 2019-03-26 13:12:14+00:00
- **Authors**: Reinaldo Mozart Silva, Lais Baroni, Rodrigo S. Ferreira, Daniel Civitarese, Daniela Szwarcman, Emilio Vital Brazil
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: Machine learning and, more specifically, deep learning algorithms have seen remarkable growth in their popularity and usefulness in the last years. This is arguably due to three main factors: powerful computers, new techniques to train deeper networks and larger datasets. Although the first two are readily available in modern computers and ML libraries, the last one remains a challenge for many domains. It is a fact that big data is a reality in almost all fields nowadays, and geosciences are not an exception. However, to achieve the success of general-purpose applications such as ImageNet - for which there are +14 million labeled images for 1000 target classes - we not only need more data, we need more high-quality labeled data. When it comes to the Oil&Gas industry, confidentiality issues hamper even more the sharing of datasets. In this work, we present the Netherlands interpretation dataset, a contribution to the development of machine learning in seismic interpretation. The Netherlands F3 dataset acquisition was carried out in the North Sea, Netherlands offshore. The data is publicly available and contains pos-stack data, 8 horizons and well logs of 4 wells. For the purposes of our machine learning tasks, the original dataset was reinterpreted, generating 9 horizons separating different seismic facies intervals. The interpreted horizons were used to generate approximatelly 190,000 labeled images for inlines and crosslines. Finally, we present two deep learning applications in which the proposed dataset was employed and produced compelling results.



### Photo-Realistic Facial Details Synthesis from Single Image
- **Arxiv ID**: http://arxiv.org/abs/1903.10873v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10873v5)
- **Published**: 2019-03-26 13:31:25+00:00
- **Updated**: 2019-12-03 08:38:31+00:00
- **Authors**: Anpei Chen, Zhang Chen, Guli Zhang, Ziheng Zhang, Kenny Mitchell, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 20K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions.



### High-Level Perceptual Similarity is Enabled by Learning Diverse Tasks
- **Arxiv ID**: http://arxiv.org/abs/1903.10920v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.10920v1)
- **Published**: 2019-03-26 14:32:02+00:00
- **Updated**: 2019-03-26 14:32:02+00:00
- **Authors**: Amir Rosenfeld, Richard Zemel, John K. Tsotsos
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting human perceptual similarity is a challenging subject of ongoing research. The visual process underlying this aspect of human vision is thought to employ multiple different levels of visual analysis (shapes, objects, texture, layout, color, etc). In this paper, we postulate that the perception of image similarity is not an explicitly learned capability, but rather one that is a byproduct of learning others. This claim is supported by leveraging representations learned from a diverse set of visual tasks and using them jointly to predict perceptual similarity. This is done via simple feature concatenation, without any further learning. Nevertheless, experiments performed on the challenging Totally-Looks-Like (TLL) benchmark significantly surpass recent baselines, closing much of the reported gap towards prediction of human perceptual similarity. We provide an analysis of these results and discuss them in a broader context of emergent visual capabilities and their implications on the course of machine-vision research.



### TAPA-MVS: Textureless-Aware PAtchMatch Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/1903.10929v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10929v1)
- **Published**: 2019-03-26 14:39:11+00:00
- **Updated**: 2019-03-26 14:39:11+00:00
- **Authors**: Andrea Romanoni, Matteo Matteucci
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most successful approaches in Multi-View Stereo estimates a depth map and a normal map for each view via PatchMatch-based optimization and fuses them into a consistent 3D points cloud. This approach relies on photo-consistency to evaluate the goodness of a depth estimate. It generally produces very accurate results; however, the reconstructed model often lacks completeness, especially in correspondence of broad untextured areas where the photo-consistency metrics are unreliable. Assuming the untextured areas piecewise planar, in this paper we generate novel PatchMatch hypotheses so to expand reliable depth estimates in neighboring untextured regions. At the same time, we modify the photo-consistency measure such to favor standard or novel PatchMatch depth hypotheses depending on the textureness of the considered area. We also propose a depth refinement step to filter wrong estimates and to fill the gaps on both the depth maps and normal maps while preserving the discontinuities. The effectiveness of our new methods has been tested against several state of the art algorithms in the publicly available ETH3D dataset containing a wide variety of high and low-resolution images.



### Exploring Confidence Measures for Word Spotting in Heterogeneous Datasets
- **Arxiv ID**: http://arxiv.org/abs/1903.10930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10930v1)
- **Published**: 2019-03-26 14:40:42+00:00
- **Updated**: 2019-03-26 14:40:42+00:00
- **Authors**: Fabian Wolf, Philipp Oberdiek, Gernot A. Fink
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, convolutional neural networks (CNNs) took over the field of document analysis and they became the predominant model for word spotting. Especially attribute CNNs, which learn the mapping between a word image and an attribute representation, showed exceptional performances. The drawback of this approach is the overconfidence of neural networks when used out of their training distribution. In this paper, we explore different metrics for quantifying the confidence of a CNN in its predictions, specifically on the retrieval problem of word spotting. With these confidence measures, we limit the inability of a retrieval list to reject certain candidates. We investigate four different approaches that are either based on the network's attribute estimations or make use of a surrogate model. Our approach also aims at answering the question for which part of a dataset the retrieval system gives reliable results. We further show that there exists a direct relation between the proposed confidence measures and the quality of an estimated attribute representation.



### Deep Demosaicing for Edge Implementation
- **Arxiv ID**: http://arxiv.org/abs/1904.00775v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00775v3)
- **Published**: 2019-03-26 15:04:17+00:00
- **Updated**: 2019-05-23 15:20:54+00:00
- **Authors**: Ramchalam Kinattinkara Ramakrishnan, Shangling Jui, Vahid Patrovi Nia
- **Comment**: Accepted in the 16th International Conference of Image Analysis and
  Recognition (ICIAR 2019)
- **Journal**: None
- **Summary**: Most digital cameras use sensors coated with a Color Filter Array (CFA) to capture channel components at every pixel location, resulting in a mosaic image that does not contain pixel values in all channels. Current research on reconstructing these missing channels, also known as demosaicing, introduces many artifacts, such as zipper effect and false color. Many deep learning demosaicing techniques outperform other classical techniques in reducing the impact of artifacts. However, most of these models tend to be over-parametrized. Consequently, edge implementation of the state-of-the-art deep learning-based demosaicing algorithms on low-end edge devices is a major challenge. We provide an exhaustive search of deep neural network architectures and obtain a pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the performance criterion versus the number of parameters as the model complexity that beats the state-of-the-art. Architectures on the pareto front can then be used to choose the best architecture for a variety of resource constraints. Simple architecture search methods such as exhaustive search and grid search require some conditions of the loss function to converge to the optimum. We clarify these conditions in a brief theoretical study.



### Reconstruction of r-Regular Objects from Trinary Images
- **Arxiv ID**: http://arxiv.org/abs/1903.10942v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.10942v1)
- **Published**: 2019-03-26 15:05:00+00:00
- **Updated**: 2019-03-26 15:05:00+00:00
- **Authors**: Helene Svane, Andrew du Plessis
- **Comment**: None
- **Journal**: None
- **Summary**: We study digital images of r-regular objects where a pixel is black if it is completely inside the object, white if it is completely inside the complement of the object, and grey otherwise. We call such images trinary. We discuss possible configurations of pixels in trinary images of r-regular objects at certain resolutions and propose a method for reconstructing objects from such images. We show that the reconstructed object is close to the original object in Hausdorff norm, and that there is a homeomorphism of the plane taking the reconstructed set to the original.



### GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.10955v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10955v2)
- **Published**: 2019-03-26 15:25:26+00:00
- **Updated**: 2019-03-27 02:34:23+00:00
- **Authors**: Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, Xiaogang Wang
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: We present an efficient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efficiently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by refinement. In contrast to previous state-of-the-art methods that only use the features extracted from the 2D bounding box for box refinement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box refinement and discover that a classification formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.



### Verification of Very Low-Resolution Faces Using An Identity-Preserving Deep Face Super-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1903.10974v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10974v1)
- **Published**: 2019-03-26 16:01:23+00:00
- **Updated**: 2019-03-26 16:01:23+00:00
- **Authors**: Esra Ataer-Cansizoglu, Michael Jones, Ziming Zhang, Alan Sullivan
- **Comment**: None
- **Journal**: None
- **Summary**: Face super-resolution methods usually aim at producing visually appealing results rather than preserving distinctive features for further face identification. In this work, we propose a deep learning method for face verification on very low-resolution face images that involves identity-preserving face super-resolution. Our framework includes a super-resolution network and a feature extraction network. We train a VGG-based deep face recognition network (Parkhi et al. 2015) to be used as feature extractor. Our super-resolution network is trained to minimize the feature distance between the high resolution ground truth image and the super-resolved image, where features are extracted using our pre-trained feature extraction network. We carry out experiments on FRGC, Multi-PIE, LFW-a, and MegaFace datasets to evaluate our method in controlled and uncontrolled settings. The results show that the presented method outperforms conventional super-resolution methods in low-resolution face verification.



### DetNAS: Backbone Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.10979v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10979v4)
- **Published**: 2019-03-26 16:08:30+00:00
- **Updated**: 2019-12-30 14:04:15+00:00
- **Authors**: Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Xinyu Xiao, Jian Sun
- **Comment**: In NeurIPS 2019. Code and models are available at
  https://github.com/megvii-model/DetNAS
- **Journal**: None
- **Summary**: Object detectors are usually equipped with backbone networks designed for image classification. It might be sub-optimal because of the gap between the tasks of image classification and object detection. In this work, we present DetNAS to use Neural Architecture Search (NAS) for the design of better backbones for object detection. It is non-trivial because detection training typically needs ImageNet pre-training while NAS systems require accuracies on the target detection task as supervisory signals. Based on the technique of one-shot supernet, which contains all possible networks in the search space, we propose a framework for backbone search on object detection. We train the supernet under the typical detector training schedule: ImageNet pre-training and detection fine-tuning. Then, the architecture search is performed on the trained supernet, using the detection task as the guidance. This framework makes NAS on backbones very efficient. In experiments, we show the effectiveness of DetNAS on various detectors, for instance, one-stage RetinaNet and the two-stage FPN. We empirically find that networks searched on object detection shows consistent superiority compared to those searched on ImageNet classification. The resulting architecture achieves superior performance than hand-crafted networks on COCO with much less FLOPs complexity.



### Learning Accurate, Comfortable and Human-like Driving
- **Arxiv ID**: http://arxiv.org/abs/1903.10995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10995v1)
- **Published**: 2019-03-26 16:36:15+00:00
- **Updated**: 2019-03-26 16:36:15+00:00
- **Authors**: Simon Hecker, Dengxin Dai, Luc Van Gool
- **Comment**: submitted to a conference, 10 pages, 3 figures
- **Journal**: None
- **Summary**: Autonomous vehicles are more likely to be accepted if they drive accurately, comfortably, but also similar to how human drivers would. This is especially true when autonomous and human-driven vehicles need to share the same road. The main research focus thus far, however, is still on improving driving accuracy only. This paper formalizes the three concerns with the aim of accurate, comfortable and human-like driving. Three contributions are made in this paper. First, numerical map data from HERE Technologies are employed for more accurate driving; a set of map features which are believed to be relevant to driving are engineered to navigate better. Second, the learning procedure is improved from a pointwise prediction to a sequence-based prediction and passengers' comfort measures are embedded into the learning algorithm. Finally, we take advantage of the advances in adversary learning to learn human-like driving; specifically, the standard L1 or L2 loss is augmented by an adversary loss which is based on a discriminator trained to distinguish between human driving and machine driving. Our model is trained and evaluated on the Drive360 dataset, which features 60 hours and 3000 km of real-world driving data. Extensive experiments show that our driving model is more accurate, more comfortable and behaves more like a human driver than previous methods. The resources of this work will be released on the project page.



### Domain Independent SVM for Transfer Learning in Brain Decoding
- **Arxiv ID**: http://arxiv.org/abs/1903.11020v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.11020v1)
- **Published**: 2019-03-26 17:04:44+00:00
- **Updated**: 2019-03-26 17:04:44+00:00
- **Authors**: Shuo Zhou, Wenwen Li, Christopher R. Cox, Haiping Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Brain imaging data are important in brain sciences yet expensive to obtain, with big volume (i.e., large p) but small sample size (i.e., small n). To tackle this problem, transfer learning is a promising direction that leverages source data to improve performance on related, target data. Most transfer learning methods focus on minimizing data distribution mismatch. However, a big challenge in brain imaging is the large domain discrepancies in cognitive experiment designs and subject-specific structures and functions. A recent transfer learning approach minimizes domain dependence to learn common features across domains, via the Hilbert-Schmidt Independence Criterion (HSIC). Inspired by this method, we propose a new Domain Independent Support Vector Machine (DI-SVM) for transfer learning in brain condition decoding. Specifically, DI-SVM simultaneously minimizes the SVM empirical risk and the dependence on domain information via a simplified HSIC. We use public data to construct 13 transfer learning tasks in brain decoding, including three interesting multi-source transfer tasks. Experiments show that DI-SVM's superior performance over eight competing methods on these tasks, particularly an improvement of more than 24% on multi-source transfer tasks.



### nuScenes: A multimodal dataset for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/1903.11027v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.11027v5)
- **Published**: 2019-03-26 17:19:56+00:00
- **Updated**: 2020-05-05 09:13:24+00:00
- **Authors**: Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom
- **Comment**: CVPR 2020 camera ready incl. supplementary material
- **Journal**: None
- **Summary**: Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.



### Optimising the Input Image to Improve Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.11029v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.11029v1)
- **Published**: 2019-03-26 17:21:06+00:00
- **Updated**: 2019-03-26 17:21:06+00:00
- **Authors**: Noel Mizzi, Adrian Muscat
- **Comment**: None
- **Journal**: None
- **Summary**: Visual Relationship Detection is defined as, given an image composed of a subject and an object, the correct relation is predicted. To improve the visual part of this difficult problem, ten preprocessing methods were tested to determine whether the widely used Union method yields the optimal results. Therefore, focusing solely on predicate prediction, no object detection and linguistic knowledge were used to prevent them from affecting the comparison results. Once fine-tuned, the Visual Geometry Group models were evaluated using Recall@1, per-predicate recall, activation maximisations, class activation maps, and error analysis. From this research it was found that using preprocessing methods such as the Union-Without-Background-and-with-Binary-mask (Union-WB-and-B) method yields significantly better results than the widely used Union method since, as designed, it enables the Convolutional Neural Network to also identify the subject and object in the convolutional layers instead of solely in the fully-connected layers.



### RILOD: Near Real-Time Incremental Learning for Object Detection at the Edge
- **Arxiv ID**: http://arxiv.org/abs/1904.00781v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00781v2)
- **Published**: 2019-03-26 17:22:01+00:00
- **Updated**: 2019-09-23 17:37:55+00:00
- **Authors**: Dawei Li, Serafettin Tasci, Shalini Ghosh, Jingwen Zhu, Junting Zhang, Larry Heck
- **Comment**: Camera-ready for ACM/IEEE SEC 2019
- **Journal**: None
- **Summary**: Object detection models shipped with camera-equipped edge devices cannot cover the objects of interest for every user. Therefore, the incremental learning capability is a critical feature for a robust and personalized object detection system that many applications would rely on. In this paper, we present an efficient yet practical system, RILOD, to incrementally train an existing object detection model such that it can detect new object classes without losing its capability to detect old classes. The key component of RILOD is a novel incremental learning algorithm that trains end-to-end for one-stage deep object detection models only using training data of new object classes. Specifically to avoid catastrophic forgetting, the algorithm distills three types of knowledge from the old model to mimic the old model's behavior on object classification, bounding box regression and feature extraction. In addition, since the training data for the new classes may not be available, a real-time dataset construction pipeline is designed to collect training images on-the-fly and automatically label the images with both category and bounding box annotations. We have implemented RILOD under both edge-cloud and edge-only setups. Experiment results show that the proposed system can learn to detect a new object class in just a few minutes, including both dataset construction and model training. In comparison, traditional fine-tuning based method may take a few hours for training, and in most cases would also need a tedious and costly manual dataset labeling step.



### SuSi: Supervised Self-Organizing Maps for Regression and Classification in Python
- **Arxiv ID**: http://arxiv.org/abs/1903.11114v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.11114v3)
- **Published**: 2019-03-26 18:52:45+00:00
- **Updated**: 2020-01-08 13:06:43+00:00
- **Authors**: Felix M. Riese, Sina Keller
- **Comment**: An extended and peer-reviewed version exists at
  doi:10.3390/rs12010007
- **Journal**: None
- **Summary**: In many research fields, the sizes of the existing datasets vary widely. Hence, there is a need for machine learning techniques which are well-suited for these different datasets. One possible technique is the self-organizing map (SOM), a type of artificial neural network which is, so far, weakly represented in the field of machine learning. The SOM's unique characteristic is the neighborhood relationship of the output neurons. This relationship improves the ability of generalization on small datasets. SOMs are mostly applied in unsupervised learning and few studies focus on using SOMs as supervised learning approach. Furthermore, no appropriate SOM package is available with respect to machine learning standards and in the widely used programming language Python. In this paper, we introduce the freely available Supervised Self-organizing maps (SuSi) Python package which performs supervised regression and classification. The implementation of SuSi is described with respect to the underlying mathematics. Then, we present first evaluations of the SOM for regression and classification datasets from two different domains of geospatial image analysis. Despite the early stage of its development, the SuSi framework performs well and is characterized by only small performance differences between the training and the test datasets. A comparison of the SuSi framework with existing Python and R packages demonstrates the importance of the SuSi framework. In future work, the SuSi framework will be extended, optimized and upgraded e.g. with tools to better understand and visualize the input data as well as the handling of missing and incomplete data.



### Deep segmentation networks predict survival of non-small cell lung cancer
- **Arxiv ID**: http://arxiv.org/abs/1903.11593v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11593v2)
- **Published**: 2019-03-26 19:55:44+00:00
- **Updated**: 2019-11-08 22:42:56+00:00
- **Authors**: Stephen Baek, Yusen He, Bryan G. Allen, John M. Buatti, Brian J. Smith, Ling Tong, Zhiyu Sun, Jia Wu, Maximilian Diehn, Billy W. Loo, Kristin A. Plichta, Steven N. Seyedin, Maggie Gannon, Katherine R. Cabel, Yusung Kim, Xiaodong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung cancer diagnoses and is the leading cause of cancer-related death worldwide. Recent studies indicate that image-based radiomics features from positron emission tomography-computed tomography (PET/CT) images have predictive power on NSCLC outcomes. To this end, easily calculated functional features such as the maximum and the mean of standard uptake value (SUV) and total lesion glycolysis (TLG) are most commonly used for NSCLC prognostication, but their prognostic value remains controversial. Meanwhile, convolutional neural networks (CNN) are rapidly emerging as a new premise for cancer image analysis, with significantly enhanced predictive power compared to other hand-crafted radiomics features. Here we show that CNN trained to perform the tumor segmentation task, with no other information than physician contours, identify a rich set of survival-related image features with remarkable prognostic value. In a retrospective study on 96 NSCLC patients before stereotactic-body radiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net) trained for tumor segmentation in PET/CT images, contained features having strong correlation with 2- and 5-year overall and disease-specific survivals. The U-net algorithm has not seen any other clinical information (e.g. survival, age, smoking history) than the images and the corresponding tumor contours provided by physicians. Furthermore, through visualization of the U-Net, we also found convincing evidence that the regions of progression appear to match with the regions where the U-Net features identified patterns that predicted higher likelihood of death. We anticipate our findings will be a starting point for more sophisticated non-intrusive patient specific cancer prognosis determination.



### Pix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable Renderer
- **Arxiv ID**: http://arxiv.org/abs/1903.11149v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.11149v2)
- **Published**: 2019-03-26 20:34:32+00:00
- **Updated**: 2019-05-26 17:05:53+00:00
- **Authors**: Felix Petersen, Amit H. Bermano, Oliver Deussen, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: The long-coveted task of reconstructing 3D geometry from images is still a standing problem. In this paper, we build on the power of neural networks and introduce Pix2Vex, a network trained to convert camera-captured images into 3D geometry. We present a novel differentiable renderer ($DR$) as a forward validation means during training. Our key insight is that $DR$s produce images of a particular appearance, different from typical input images. Hence, we propose adding an image-to-image translation component, converting between these rendering styles. This translation closes the training loop, while allowing to use minimal supervision only, without needing any 3D model as ground truth. Unlike state-of-the-art methods, our $DR$ is $C^\infty$ smooth and thus does not display any discontinuities at occlusions or dis-occlusions. Through our novel training scheme, our network can train on different types of images, where previous work can typically only train on images of a similar appearance to those rendered by a $DR$.



### Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-supervised Regression
- **Arxiv ID**: http://arxiv.org/abs/1903.11174v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.11174v1)
- **Published**: 2019-03-26 22:05:05+00:00
- **Updated**: 2019-03-26 22:05:05+00:00
- **Authors**: Wenshan Wang, Aayush Ahuja, Yanfu Zhang, Rogerio Bonatti, Sebastian Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: In the task of Autonomous aerial filming of a moving actor (e.g. a person or a vehicle), it is crucial to have a good heading direction estimation for the actor from the visual input. However, the models obtained in other similar tasks, such as pedestrian collision risk analysis and human-robot interaction, are very difficult to generalize to the aerial filming task, because of the difference in data distributions. Towards improving generalization with less amount of labeled data, this paper presents a semi-supervised algorithm for heading direction estimation problem. We utilize temporal continuity as the unsupervised signal to regularize the model and achieve better generalization ability. This semi-supervised algorithm is applied to both training and testing phases, which increases the testing performance by a large margin. We show that by leveraging unlabeled sequences, the amount of labeled data required can be significantly reduced. We also discuss several important details on improving the performance by balancing labeled and unlabeled loss, and making good combinations. Experimental results show that our approach robustly outputs the heading direction for different types of actor. The aesthetic value of the video is also improved in the aerial filming task.



