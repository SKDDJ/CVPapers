# Arxiv Papers in cs.CV on 2019-03-06
### Age Progression and Regression with Spatial Attention Modules
- **Arxiv ID**: http://arxiv.org/abs/1903.02133v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02133v2)
- **Published**: 2019-03-06 01:31:30+00:00
- **Updated**: 2019-10-07 01:51:48+00:00
- **Authors**: Qi Li, Yunfan Liu, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Age progression and regression refers to aesthetically render-ing a given face image to present effects of face aging and rejuvenation, respectively. Although numerous studies have been conducted in this topic, there are two major problems: 1) multiple models are usually trained to simulate different age mappings, and 2) the photo-realism of generated face images is heavily influenced by the variation of training images in terms of pose, illumination, and background. To address these issues, in this paper, we propose a framework based on conditional Generative Adversarial Networks (cGANs) to achieve age progression and regression simultaneously. Particularly, since face aging and rejuvenation are largely different in terms of image translation patterns, we model these two processes using two separate generators, each dedicated to one age changing process. In addition, we exploit spatial attention mechanisms to limit image modifications to regions closely related to age changes, so that images with high visual fidelity could be synthesized for in-the-wild cases. Experiments on multiple datasets demonstrate the ability of our model in synthesizing lifelike face images at desired ages with personalized features well preserved, and keeping age-irrelevant regions unchanged.



### Large-Scale Pedestrian Retrieval Competition
- **Arxiv ID**: http://arxiv.org/abs/1903.02137v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02137v1)
- **Published**: 2019-03-06 01:55:35+00:00
- **Updated**: 2019-03-06 01:55:35+00:00
- **Authors**: Da Li, Zhang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The Large-Scale Pedestrian Retrieval Competition (LSPRC) mainly focuses on person retrieval which is an important end application in intelligent vision system of surveillance. Person retrieval aims at searching the interested target with specific visual attributes or images. The low image quality, various camera viewpoints, large pose variations and occlusions in real scenes make it a challenge problem. By providing large-scale surveillance data in real scene and standard evaluation methods that are closer to real application, the competition aims to improve the robust of related algorithms and further meet the complicated situations in real application. LSPRC includes two kinds of tasks, i.e., Attribute based Pedestrian Retrieval (PR-A) and Re-IDentification (ReID) based Pedestrian Retrieval (PR-ID). The normal evaluation index, i.e., mean Average Precision (mAP), is used to measure the performances of the two tasks under various scale, pose and occlusion. While the method of system evaluation is introduced to evaluate the person retrieval system in which the related algorithms of the two tasks are integrated into a large-scale video parsing platform (named ISEE) combing with algorithm of pedestrian detection.



### Safeguarded Dynamic Label Regression for Generalized Noisy Supervision
- **Arxiv ID**: http://arxiv.org/abs/1903.02152v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.02152v2)
- **Published**: 2019-03-06 03:20:09+00:00
- **Updated**: 2021-08-21 12:31:49+00:00
- **Authors**: Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Jun Sun
- **Comment**: New complete and extended version will be updated in the future
- **Journal**: None
- **Summary**: Learning with noisy labels, which aims to reduce expensive labors on accurate annotations, has become imperative in the Big Data era. Previous noise transition based method has achieved promising results and presented a theoretical guarantee on performance in the case of class-conditional noise. However, this type of approaches critically depend on an accurate pre-estimation of the noise transition, which is usually impractical. Subsequent improvement adapts the pre-estimation along with the training progress via a Softmax layer. However, the parameters in the Softmax layer are highly tweaked for the fragile performance due to the ill-posed stochastic approximation. To address these issues, we propose a Latent Class-Conditional Noise model (LCCN) that naturally embeds the noise transition under a Bayesian framework. By projecting the noise transition into a Dirichlet-distributed space, the learning is constrained on a simplex based on the whole dataset, instead of some ad-hoc parametric space. We then deduce a dynamic label regression method for LCCN to iteratively infer the latent labels, to stochastically train the classifier and to model the noise. Our approach safeguards the bounded update of the noise transition, which avoids previous arbitrarily tuning via a batch of samples. We further generalize LCCN for open-set noisy labels and the semi-supervised setting. We perform extensive experiments with the controllable noise data sets, CIFAR-10 and CIFAR-100, and the agnostic noise data sets, Clothing1M and WebVision17. The experimental results have demonstrated that the proposed model outperforms several state-of-the-art methods.



### Semantic Adversarial Network with Multi-scale Pyramid Attention for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.02155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02155v1)
- **Published**: 2019-03-06 03:36:11+00:00
- **Updated**: 2019-03-06 03:36:11+00:00
- **Authors**: De Xie, Cheng Deng, Hao Wang, Chao Li, Dapeng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Two-stream architecture have shown strong performance in video classification task. The key idea is to learn spatio-temporal features by fusing convolutional networks spatially and temporally. However, there are some problems within such architecture. First, it relies on optical flow to model temporal information, which are often expensive to compute and store. Second, it has limited ability to capture details and local context information for video data. Third, it lacks explicit semantic guidance that greatly decrease the classification performance. In this paper, we proposed a new two-stream based deep framework for video classification to discover spatial and temporal information only from RGB frames, moreover, the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module is introduced and integrated in our framework. The MPA enables the network capturing global and local feature to generate a comprehensive representation for video, and the SAL can make this representation gradually approximate to the real video semantics in an adversarial manner. Experimental results on two public benchmarks demonstrate our proposed methods achieves state-of-the-art results on standard video datasets.



### Camera Obscurer: Generative Art for Design Inspiration
- **Arxiv ID**: http://arxiv.org/abs/1903.02165v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.02165v1)
- **Published**: 2019-03-06 04:05:47+00:00
- **Updated**: 2019-03-06 04:05:47+00:00
- **Authors**: Dilpreet Singh, Nina Rajcic, Simon Colton, Jon McCormack
- **Comment**: Accepted for EvoMUSART 2019: 8th International Conference on
  Computational Intelligence in Music, Sound, Art and Design. April 2019,
  Leipzig, Germany
- **Journal**: None
- **Summary**: We investigate using generated decorative art as a source of inspiration for design tasks. Using a visual similarity search for image retrieval, the \emph{Camera Obscurer} app enables rapid searching of tens of thousands of generated abstract images of various types. The seed for a visual similarity search is a given image, and the retrieved generated images share some visual similarity with the seed. Implemented in a hand-held device, the app empowers users to use photos of their surroundings to search through the archive of generated images and other image archives. Being abstract in nature, the retrieved images supplement the seed image rather than replace it, providing different visual stimuli including shapes, colours, textures and juxtapositions, in addition to affording their own interpretations. This approach can therefore be used to provide inspiration for a design task, with the abstract images suggesting new ideas that might give direction to a graphic design project. We describe a crowdsourcing experiment with the app to estimate user confidence in retrieved images, and we describe a pilot study where Camera Obscurer provided inspiration for a design task. These experiments have enabled us to describe future improvements, and to begin to understand sources of visual inspiration for design tasks.



### AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1903.02172v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.02172v1)
- **Published**: 2019-03-06 04:49:07+00:00
- **Updated**: 2019-03-06 04:49:07+00:00
- **Authors**: Marwan Mattar, Roozbeh Mottaghi, Julian Togelius, Danny Lange
- **Comment**: AAAI-2019 Workshop on Games and Simulations for Artificial
  Intelligence
- **Journal**: None
- **Summary**: This volume represents the accepted submissions from the AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence held on January 29, 2019 in Honolulu, Hawaii, USA. https://www.gamesim.ai



### Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.02193v2
- **DOI**: 10.1109/TVT.2019.2949603
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02193v2)
- **Published**: 2019-03-06 06:29:18+00:00
- **Updated**: 2020-04-29 16:00:39+00:00
- **Authors**: Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, Long Chen, Qian Wang
- **Comment**: IEEE Transactions on Vehicular Technology, 69(1), 2020
- **Journal**: None
- **Summary**: Lane detection in driving scenes is an important module for autonomous vehicles and advanced driver assistance systems. In recent years, many sophisticated lane detection methods have been proposed. However, most methods focus on detecting the lane from one single image, and often lead to unsatisfactory performance in handling some extremely-bad situations such as heavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In fact, lanes are continuous line structures on the road. Consequently, the lane that cannot be accurately detected in one current frame may potentially be inferred out by incorporating information of previous frames. To this end, we investigate lane detection by using multiple frames of a continuous driving scene, and propose a hybrid deep architecture by combining the convolutional neural network (CNN) and the recurrent neural network (RNN). Specifically, information of each frame is abstracted by a CNN block, and the CNN features of multiple continuous frames, holding the property of time-series, are then fed into the RNN block for feature learning and lane prediction. Extensive experiments on two large-scale datasets demonstrate that, the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations.



### Deep Transfer Learning for Multiple Class Novelty Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.02196v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02196v1)
- **Published**: 2019-03-06 06:36:57+00:00
- **Updated**: 2019-03-06 06:36:57+00:00
- **Authors**: Pramuditha Perera, Vishal M. Patel
- **Comment**: CVPR 2019 Accepted Paper
- **Journal**: None
- **Summary**: We propose a transfer learning-based solution for the problem of multiple class novelty detection. In particular, we propose an end-to-end deep-learning based approach in which we investigate how the knowledge contained in an external, out-of-distributional dataset can be used to improve the performance of a deep network for visual novelty detection. Our solution differs from the standard deep classification networks on two accounts. First, we use a novel loss function, membership loss, in addition to the classical cross-entropy loss for training networks. Secondly, we use the knowledge from the external dataset more effectively to learn globally negative filters, filters that respond to generic objects outside the known class set. We show that thresholding the maximal activation of the proposed network can be used to identify novel objects effectively. Extensive experiments on four publicly available novelty detection datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.



### Transfer feature generating networks with semantic classes structure for zero-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1903.02204v2
- **DOI**: 10.1109/ACCESS.2019.2958052
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02204v2)
- **Published**: 2019-03-06 06:56:48+00:00
- **Updated**: 2019-07-20 08:36:15+00:00
- **Authors**: Guangfeng Lin, Wanjun Chen, Kaiyang Liao, Xiaobing Kang, Caixia Fan
- **Comment**: None
- **Journal**: IEEE Access,2019
- **Summary**: Feature generating networks face to the most important question, which is the fitting difference (inconsistence) of the distribution between the generated feature and the real data. This inconsistence further influence the performance of the networks model, because training samples from seen classes is disjointed with testing samples from unseen classes in zero-shot learning (ZSL). In generalization zero-shot learning (GZSL), testing samples come from not only seen classes but also unseen classes for closer to the practical situation. Therefore, most of feature generating networks difficultly obtain satisfactory performance for the challenging GZSL by adversarial learning the distribution of semantic classes. To alleviate the negative influence of this inconsistence for ZSL and GZSL, transfer feature generating networks with semantic classes structure (TFGNSCS) is proposed to construct networks model for improving the performance of ZSL and GZSL. TFGNSCS can not only consider the semantic structure relationship between seen and unseen classes, but also learn the difference of generating features by transferring classification model information from seen to unseen classes in networks. The proposed method can integrate the transfer loss, the classification loss and the Wasserstein distance loss to generate enough CNN features, on which softmax classifiers are trained for ZSL and GZSL. Experiments demonstrate that the performance of TFGNSCS outperforms that of the state of the arts on four challenging datasets, which are CUB,FLO,SUN, AWA in GZSL.



### DepthwiseGANs: Fast Training Generative Adversarial Networks for Realistic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1903.02225v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.02225v1)
- **Published**: 2019-03-06 07:56:49+00:00
- **Updated**: 2019-03-06 07:56:49+00:00
- **Authors**: Mkhuseli Ngxande, Jules-Raymond Tapamo, Michael Burke
- **Comment**: 6 pages, 8 figures, To appear in the Proceedings of Southern African
  Universities Power EngineeringConference/Robotics and Mechatronics/Pattern
  Recognition Association of South Africa(SAUPEC/RobMech/PRASA), January 20-30
  2019, Bloemfotein, South Africa
- **Journal**: None
- **Summary**: Recent work has shown significant progress in the direction of synthetic data generation using Generative Adversarial Networks (GANs). GANs have been applied in many fields of computer vision including text-to-image conversion, domain transfer, super-resolution, and image-to-video applications. In computer vision, traditional GANs are based on deep convolutional neural networks. However, deep convolutional neural networks can require extensive computational resources because they are based on multiple operations performed by convolutional layers, which can consist of millions of trainable parameters. Training a GAN model can be difficult and it takes a significant amount of time to reach an equilibrium point. In this paper, we investigate the use of depthwise separable convolutions to reduce training time while maintaining data generation performance. Our results show that a DepthwiseGAN architecture can generate realistic images in shorter training periods when compared to a StarGan architecture, but that model capacity still plays a significant role in generative modelling. In addition, we show that depthwise separable convolutions perform best when only applied to the generator. For quality evaluation of generated images, we use the Fr\'echet Inception Distance (FID), which compares the similarity between the generated image distribution and that of the training dataset.



### Robust Video Background Identification by Dominant Rigid Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.02232v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02232v1)
- **Published**: 2019-03-06 08:11:12+00:00
- **Updated**: 2019-03-06 08:11:12+00:00
- **Authors**: Kaimo Lin, Nianjuan Jiang, Loong Fah Cheong, Jiangbo Lu, Xun Xu
- **Comment**: Asian Conference on Computer Vision 2018 (ACCV2018)
- **Journal**: None
- **Summary**: The ability to identify the static background in videos captured by a moving camera is an important pre-requisite for many video applications (e.g. video stabilization, stitching, and segmentation). Existing methods usually face difficulties when the foreground objects occupy a larger area than the background in the image. Many methods also cannot scale up to handle densely sampled feature trajectories. In this paper, we propose an efficient local-to-global method to identify background, based on the assumption that as long as there is sufficient camera motion, the cumulative background features will have the largest amount of trajectories. Our motion model at the two-frame level is based on the epipolar geometry so that there will be no over-segmentation problem, another issue that plagues the 2D motion segmentation approach. Foreground objects erroneously labelled due to intermittent motions are also taken care of by checking their global consistency with the final estimated background motion. Lastly, by virtue of its efficiency, our method can deal with densely sampled trajectories. It outperforms several state-of-the-art motion segmentation methods on public datasets, both quantitatively and qualitatively.



### Characterizing Human Behaviours Using Statistical Motion Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1903.02236v1
- **DOI**: 10.5121/sipij.2019.10102
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02236v1)
- **Published**: 2019-03-06 08:19:49+00:00
- **Updated**: 2019-03-06 08:19:49+00:00
- **Authors**: Eissa Jaber Alreshidi, Mohammad Bilal
- **Comment**: None
- **Journal**: Signal & Image Processing: An International Journal (SIPIJ) 2019
- **Summary**: Identifying human behaviors is a challenging research problem due to the complexity and variation of appearances and postures, the variation of camera settings, and view angles. In this paper, we try to address the problem of human behavior identification by introducing a novel motion descriptor based on statistical features. The method first divide the video into N number of temporal segments. Then for each segment, we compute dense optical flow, which provides instantaneous velocity information for all the pixels. We then compute Histogram of Optical Flow (HOOF) weighted by the norm and quantized into 32 bins. We then compute statistical features from the obtained HOOF forming a descriptor vector of 192- dimensions. We then train a non-linear multi-class SVM that classify different human behaviors with the accuracy of 72.1%. We evaluate our method by using publicly available human action data set. Experimental results shows that our proposed method out performs state of the art methods.



### Efficient Deep Neural Network for Photo-realistic Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.02240v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02240v5)
- **Published**: 2019-03-06 08:33:04+00:00
- **Updated**: 2022-03-15 03:36:46+00:00
- **Authors**: Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn
- **Comment**: Pattern Recognition
- **Journal**: None
- **Summary**: Recent progress in deep learning-based models has improved photo-realistic (or perceptual) single-image super-resolution significantly. However, despite their powerful performance, many methods are difficult to apply to real-world applications because of the heavy computational requirements. To facilitate the use of a deep model under such demands, we focus on keeping the network efficient while maintaining its performance. In detail, we design an architecture that implements a cascading mechanism on a residual network to boost the performance with limited resources via multi-level feature fusion. In addition, our proposed model adopts group convolution and recursive schemes in order to achieve extreme efficiency. We further improve the perceptual quality of the output by employing the adversarial learning paradigm and a multi-scale discriminator approach. The performance of our method is investigated through extensive internal experiments and benchmarks using various datasets. Our results show that our models outperform the recent methods with similar complexity, for both traditional pixel-based and perception-based tasks.



### Discourse Parsing in Videos: A Multi-modal Appraoch
- **Arxiv ID**: http://arxiv.org/abs/1903.02252v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02252v4)
- **Published**: 2019-03-06 09:09:47+00:00
- **Updated**: 2022-01-22 18:46:14+00:00
- **Authors**: Arjun R. Akula, Song-Chun Zhu
- **Comment**: Accepted in CVPR 2019 Workshop on Language and Vision (Oral
  Presentation)
- **Journal**: CVPR 2019 Workshop on Language and Vision (Oral Presentation)
- **Summary**: Text-level discourse parsing aims to unmask how two sentences in the text are related to each other. We propose the task of Visual Discourse Parsing, which requires understanding discourse relations among scenes in a video. Here we use the term scene to refer to a subset of video frames that can better summarize the video. In order to collect a dataset for learning discourse cues from videos, one needs to manually identify the scenes from a large pool of video frames and then annotate the discourse relations between them. This is clearly a time consuming, expensive and tedious task. In this work, we propose an approach to identify discourse cues from the videos without the need to explicitly identify and annotate the scenes. We also present a novel dataset containing 310 videos and the corresponding discourse cues to evaluate our approach. We believe that many of the multi-discipline AI problems such as Visual Dialog and Visual Storytelling would greatly benefit from the use of visual discourse cues.



### High-Fidelity Image Generation With Fewer Labels
- **Arxiv ID**: http://arxiv.org/abs/1903.02271v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.02271v2)
- **Published**: 2019-03-06 09:52:49+00:00
- **Updated**: 2019-05-14 15:27:42+00:00
- **Authors**: Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly
- **Comment**: Mario Lucic, Michael Tschannen, and Marvin Ritter contributed equally
  to this work. ICML 2019 camera-ready version. Code available at
  https://github.com/google/compare_gan
- **Journal**: None
- **Summary**: Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.



### Video-based surgical skill assessment using 3D convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.02306v3
- **DOI**: 10.1007/s11548-019-01995-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02306v3)
- **Published**: 2019-03-06 10:54:09+00:00
- **Updated**: 2019-09-04 12:49:50+00:00
- **Authors**: Isabel Funke, Sören Torge Mees, Jürgen Weitz, Stefanie Speidel
- **Comment**: IPCAI 2019/ IJCARS
- **Journal**: IJCARS 14.7 (2019) pp. 1217-1225
- **Summary**: Purpose: A profound education of novice surgeons is crucial to ensure that surgical interventions are effective and safe. One important aspect is the teaching of technical skills for minimally invasive or robot-assisted procedures. This includes the objective and preferably automatic assessment of surgical skill. Recent studies presented good results for automatic, objective skill evaluation by collecting and analyzing motion data such as trajectories of surgical instruments. However, obtaining the motion data generally requires additional equipment for instrument tracking or the availability of a robotic surgery system to capture kinematic data. In contrast, we investigate a method for automatic, objective skill assessment that requires video data only. This has the advantage that video can be collected effortlessly during minimally invasive and robot-assisted training scenarios.   Methods: Our method builds on recent advances in deep learning-based video classification. Specifically, we propose to use an inflated 3D ConvNet to classify snippets, i.e., stacks of a few consecutive frames, extracted from surgical video. The network is extended into a Temporal Segment Network during training.   Results: We evaluate the method on the publicly available JIGSAWS dataset, which consists of recordings of basic robot-assisted surgery tasks performed on a dry lab bench-top model. Our approach achieves high skill classification accuracies ranging from 95.1% to 100.0%.   Conclusions: Our results demonstrate the feasibility of deep learning-based assessment of technical skill from surgical video. Notably, the 3D ConvNet is able to learn meaningful patterns directly from the data, alleviating the need for manual feature engineering. Further evaluation will require more annotated data for training and testing.



### Self-Supervised Learning of 3D Human Pose using Multi-view Geometry
- **Arxiv ID**: http://arxiv.org/abs/1903.02330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02330v2)
- **Published**: 2019-03-06 11:41:32+00:00
- **Updated**: 2019-04-09 06:40:57+00:00
- **Authors**: Muhammed Kocabas, Salih Karagoz, Emre Akbas
- **Comment**: CVPR 2019 camera ready. Code is available at
  https://github.com/mkocabas/EpipolarPose
- **Journal**: None
- **Summary**: Training accurate 3D human pose estimators requires large amount of 3D ground-truth data which is costly to collect. Various weakly or self supervised pose estimation methods have been proposed due to lack of 3D data. Nevertheless, these methods, in addition to 2D ground-truth poses, require either additional supervision in various forms (e.g. unpaired 3D ground truth data, a small subset of labels) or the camera parameters in multiview settings. To address these problems, we present EpipolarPose, a self-supervised learning method for 3D human pose estimation, which does not need any 3D ground-truth data or camera extrinsics. During training, EpipolarPose estimates 2D poses from multi-view images, and then, utilizes epipolar geometry to obtain a 3D pose and camera geometry which are subsequently used to train a 3D pose estimator. We demonstrate the effectiveness of our approach on standard benchmark datasets i.e. Human3.6M and MPI-INF-3DHP where we set the new state-of-the-art among weakly/self-supervised methods. Furthermore, we propose a new performance measure Pose Structure Score (PSS) which is a scale invariant, structure aware measure to evaluate the structural plausibility of a pose with respect to its ground truth. Code and pretrained models are available at https://github.com/mkocabas/EpipolarPose



### CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.02351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02351v1)
- **Published**: 2019-03-06 13:10:28+00:00
- **Updated**: 2019-03-06 13:10:28+00:00
- **Authors**: Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, Chunhua Shen
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Recent progress in semantic segmentation is driven by deep Convolutional Neural Networks and large-scale labeled image datasets. However, data labeling for pixel-wise segmentation is tedious and costly. Moreover, a trained model can only make predictions within a set of pre-defined classes. In this paper, we present CANet, a class-agnostic segmentation network that performs few-shot segmentation on new classes with only a few annotated images available. Our network consists of a two-branch dense comparison module which performs multi-level feature comparison between the support image and the query image, and an iterative optimization module which iteratively refines the predicted results. Furthermore, we introduce an attention mechanism to effectively fuse information from multiple support examples under the setting of k-shot learning. Experiments on PASCAL VOC 2012 show that our method achieves a mean Intersection-over-Union score of 55.4% for 1-shot segmentation and 57.1% for 5-shot segmentation, outperforming state-of-the-art methods by a large margin of 14.6% and 13.2%, respectively.



### Compressing complex convolutional neural network based on an improved deep compression algorithm
- **Arxiv ID**: http://arxiv.org/abs/1903.02358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02358v1)
- **Published**: 2019-03-06 13:20:36+00:00
- **Updated**: 2019-03-06 13:20:36+00:00
- **Authors**: Jiasong Wu, Hongshan Ren, Youyong Kong, Chunfeng Yang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 5 pages, 2 figures, 4 tables
- **Journal**: None
- **Summary**: Although convolutional neural network (CNN) has made great progress, large redundant parameters restrict its deployment on embedded devices, especially mobile devices. The recent compression works are focused on real-value convolutional neural network (Real CNN), however, to our knowledge, there is no attempt for the compression of complex-value convolutional neural network (Complex CNN). Compared with the real-valued network, the complex-value neural network is easier to optimize, generalize, and has better learning potential. This paper extends the commonly used deep compression algorithm from real domain to complex domain and proposes an improved deep compression algorithm for the compression of Complex CNN. The proposed algorithm compresses the network about 8 times on CIFAR-10 dataset with less than 3% accuracy loss. On the ImageNet dataset, our method compresses the model about 16 times and the accuracy loss is about 2% without retraining.



### GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness Classifier
- **Arxiv ID**: http://arxiv.org/abs/1903.02489v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.02489v2)
- **Published**: 2019-03-06 16:53:46+00:00
- **Updated**: 2019-08-01 00:56:36+00:00
- **Authors**: Alexandre Gariépy, Jean-Christophe Ruel, Brahim Chaib-draa, Philippe Giguère
- **Comment**: None
- **Journal**: None
- **Summary**: Grasping is a fundamental robotic task needed for the deployment of household robots or furthering warehouse automation. However, few approaches are able to perform grasp detection in real time (frame rate). To this effect, we present Grasp Quality Spatial Transformer Network (GQ-STN), a one-shot grasp detection network. Being based on the Spatial Transformer Network (STN), it produces not only a grasp configuration, but also directly outputs a depth image centered at this configuration. By connecting our architecture to an externally-trained grasp robustness evaluation network, we can train efficiently to satisfy a robustness metric via the backpropagation of the gradient emanating from the evaluation network. This removes the difficulty of training detection networks on sparsely annotated databases, a common issue in grasping. We further propose to use this robustness classifier to compare approaches, being more reliable than the traditional rectangle metric. Our GQ-STN is able to detect robust grasps on the depth images of the Dex-Net 2.0 dataset with 92.4 % accuracy in a single pass of the network. We finally demonstrate in a physical benchmark that our method can propose robust grasps more often than previous sampling-based methods, while being more than 60 times faster.



### Object Counting and Instance Segmentation with Image-level Supervision
- **Arxiv ID**: http://arxiv.org/abs/1903.02494v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02494v2)
- **Published**: 2019-03-06 17:06:51+00:00
- **Updated**: 2019-05-13 05:41:30+00:00
- **Authors**: Hisham Cholakkal, Guolei Sun, Fahad Shahbaz Khan, Ling Shao
- **Comment**: The first two authors have equal contribution. To appear in CVPR 2019
- **Journal**: None
- **Summary**: Common object counting in a natural scene is a challenging problem in computer vision with numerous real-world applications. Existing image-level supervised common object counting approaches only predict the global object count and rely on additional instance-level supervision to also determine object locations. We propose an image-level supervised approach that provides both the global object count and the spatial distribution of object instances by constructing an object category density map. Motivated by psychological studies, we further reduce image-level supervision using a limited object count information (up to four). To the best of our knowledge, we are the first to propose image-level supervised density map estimation for common object counting and demonstrate its effectiveness in image-level supervised instance segmentation. Comprehensive experiments are performed on the PASCAL VOC and COCO datasets. Our approach outperforms existing methods, including those using instance-level supervision, on both datasets for common object counting. Moreover, our approach improves state-of-the-art image-level supervised instance segmentation with a relative gain of 17.8% in terms of average best overlap, on the PASCAL VOC 2012 dataset. Code link: https://github.com/GuoleiSun/CountSeg



### Hybrid LSTM and Encoder-Decoder Architecture for Detection of Image Forgeries
- **Arxiv ID**: http://arxiv.org/abs/1903.02495v1
- **DOI**: 10.1109/TIP.2019.2895466
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02495v1)
- **Published**: 2019-03-06 17:09:46+00:00
- **Updated**: 2019-03-06 17:09:46+00:00
- **Authors**: Jawadul H. Bappy, Cody Simons, Lakshmanan Nataraj, B. S. Manjunath, Amit K. Roy-Chowdhury
- **Comment**: None
- **Journal**: None
- **Summary**: With advanced image journaling tools, one can easily alter the semantic meaning of an image by exploiting certain manipulation techniques such as copy-clone, object splicing, and removal, which mislead the viewers. In contrast, the identification of these manipulations becomes a very challenging task as manipulated regions are not visually apparent. This paper proposes a high-confidence manipulation localization architecture which utilizes resampling features, Long-Short Term Memory (LSTM) cells, and encoder-decoder network to segment out manipulated regions from non-manipulated ones. Resampling features are used to capture artifacts like JPEG quality loss, upsampling, downsampling, rotation, and shearing. The proposed network exploits larger receptive fields (spatial maps) and frequency domain correlation to analyze the discriminative characteristics between manipulated and non-manipulated regions by incorporating encoder and LSTM network. Finally, decoder network learns the mapping from low-resolution feature maps to pixel-wise predictions for image tamper localization. With predicted mask provided by final layer (softmax) of the proposed architecture, end-to-end training is performed to learn the network parameters through back-propagation using ground-truth masks. Furthermore, a large image splicing dataset is introduced to guide the training process. The proposed method is capable of localizing image manipulations at pixel level with high precision, which is demonstrated through rigorous experimentation on three diverse datasets.



### Human Attention in Image Captioning: Dataset and Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.02499v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02499v3)
- **Published**: 2019-03-06 17:15:49+00:00
- **Updated**: 2019-08-07 08:44:21+00:00
- **Authors**: Sen He, Hamed R. Tavakoli, Ali Borji, Nicolas Pugeault
- **Comment**: To appear at ICCV 2019
- **Journal**: IEEE International Conference on Computer Vision (ICCV 2019)
- **Summary**: In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human attention and language constructs during perception and sentence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image captioning. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Humans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between described objects and attended objects ($97\%$ of the described objects are being attended), (3) a convolutional neural network as feature encoder accounts for human-attended regions during image captioning to a great extent (around $78\%$), (4) soft-attention mechanism differs from human attention, both spatially and temporally, and there is low correlation between caption scores and attention consistency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model's performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/Human-Attention-in-Image-Captioning.



### Prostate Segmentation from 3D MRI Using a Two-Stage Model and Variable-Input Based Uncertainty Measure
- **Arxiv ID**: http://arxiv.org/abs/1903.02500v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02500v1)
- **Published**: 2019-03-06 17:18:33+00:00
- **Updated**: 2019-03-06 17:18:33+00:00
- **Authors**: Huitong Pan, Yushan Feng, Quan Chen, Craig Meyer, Xue Feng
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI) 2019
- **Journal**: None
- **Summary**: This paper proposes a two-stage segmentation model, variable-input based uncertainty measures and an uncertainty-guided post-processing method for prostate segmentation on 3D magnetic resonance images (MRI). The two-stage model was based on 3D dilated U-Nets with the first stage to localize the prostate and the second stage to obtain an accurate segmentation from cropped images. For data augmentation, we proposed the variable-input method which crops the region of interest with additional random variations. Similar to other deep learning models, the proposed model also faced the challenge of suboptimal performance in certain testing cases due to varied training and testing image characteristics. Therefore, it is valuable to evaluate the confidence and performance of the network using uncertainty measures, which are often calculated from the probability maps or their standard deviations with multiple model outputs for the same testing case. However, few studies have quantitatively compared different methods of uncertainty calculation. Furthermore, unlike the commonly used Bayesian dropout during testing, we developed uncertainty measures based on the variable input images at the second stage and evaluated its performance by calculating the correlation with ground-truth-based performance metrics, such as Dice score. For performance estimation, we predicted Dice scores and Hausdorff distance with the most correlated uncertainty measure. For post-processing, we performed Gaussian filter on the underperformed slices to improve segmentation quality. Using PROMISE-12 data, we demonstrated the robustness of the two-stage model and showed high correlation of the proposed variable-input based uncertainty measures with GT-based performance. The uncertainty-guided post-processing method significantly improved label smoothness.



### Understanding and Visualizing Deep Visual Saliency Models
- **Arxiv ID**: http://arxiv.org/abs/1903.02501v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02501v3)
- **Published**: 2019-03-06 17:21:04+00:00
- **Updated**: 2019-04-03 10:28:07+00:00
- **Authors**: Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, Nicolas Pugeault
- **Comment**: To appear in CVPR2019, camera ready version
- **Journal**: None
- **Summary**: Recently, data-driven deep saliency models have achieved high performance and have outperformed classical saliency models, as demonstrated by results on datasets such as the MIT300 and SALICON. Yet, there remains a large gap between the performance of these models and the inter-human baseline. Some outstanding questions include what have these models learned, how and where they fail, and how they can be improved. This article attempts to answer these questions by analyzing the representations learned by individual neurons located at the intermediate layers of deep saliency models. To this end, we follow the steps of existing deep saliency models, that is borrowing a pre-trained model of object recognition to encode the visual features and learning a decoder to infer the saliency. We consider two cases when the encoder is used as a fixed feature extractor and when it is fine-tuned, and compare the inner representations of the network. To study how the learned representations depend on the task, we fine-tune the same network using the same image set but for two different tasks: saliency prediction versus scene classification. Our analyses reveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are already encoded within various layers of the network pre-trained for object recognition, 2) using modern datasets, we find that fine-tuning pre-trained models for saliency prediction makes them favor some categories (e.g. head) over some others (e.g. text), 3) although deep models of saliency outperform classical models on natural images, the converse is true for synthetic stimuli (e.g. pop-out search arrays), an evidence of significant difference between human and data-driven saliency models, and 4) we confirm that, after-fine tuning, the change in inner-representations is mostly due to the task and not the domain shift in the data.



### Image captioning with weakly-supervised attention penalty
- **Arxiv ID**: http://arxiv.org/abs/1903.02507v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02507v1)
- **Published**: 2019-03-06 17:32:44+00:00
- **Updated**: 2019-03-06 17:32:44+00:00
- **Authors**: Jiayun Li, Mohammad K. Ebrahimpour, Azadeh Moghtaderi, Yen-Yun Yu
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Stories are essential for genealogy research since they can help build emotional connections with people. A lot of family stories are reserved in historical photos and albums. Recent development on image captioning models makes it feasible to "tell stories" for photos automatically. The attention mechanism has been widely adopted in many state-of-the-art encoder-decoder based image captioning models, since it can bridge the gap between the visual part and the language part. Most existing captioning models implicitly trained attention modules with word-likelihood loss. Meanwhile, lots of studies have investigated intrinsic attentions for visual models using gradient-based approaches. Ideally, attention maps predicted by captioning models should be consistent with intrinsic attentions from visual models for any given visual concept. However, no work has been done to align implicitly learned attention maps with intrinsic visual attentions. In this paper, we proposed a novel model that measured consistency between captioning predicted attentions and intrinsic visual attentions. This alignment loss allows explicit attention correction without using any expensive bounding box annotations. We developed and evaluated our model on COCO dataset as well as a genealogical dataset from Ancestry.com Operations Inc., which contains billions of historical photos. The proposed model achieved better performances on all commonly used language evaluation metrics for both datasets.



### Learning multimodal representations for sample-efficient recognition of human actions
- **Arxiv ID**: http://arxiv.org/abs/1903.02511v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.02511v1)
- **Published**: 2019-03-06 17:37:21+00:00
- **Updated**: 2019-03-06 17:37:21+00:00
- **Authors**: Miguel Vasco, Francisco S. Melo, David Martins de Matos, Ana Paiva, Tetsunari Inamura
- **Comment**: 7 pages, 6 figures, submitted to 2019 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: Humans interact in rich and diverse ways with the environment. However, the representation of such behavior by artificial agents is often limited. In this work we present \textit{motion concepts}, a novel multimodal representation of human actions in a household environment. A motion concept encompasses a probabilistic description of the kinematics of the action along with its contextual background, namely the location and the objects held during the performance. Furthermore, we present Online Motion Concept Learning (OMCL), a new algorithm which learns novel motion concepts from action demonstrations and recognizes previously learned motion concepts. The algorithm is evaluated on a virtual-reality household environment with the presence of a human avatar. OMCL outperforms standard motion recognition algorithms on an one-shot recognition task, attesting to its potential for sample-efficient recognition of human actions.



### Temporal Registration in Application to In-utero MRI Time Series
- **Arxiv ID**: http://arxiv.org/abs/1903.02959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02959v1)
- **Published**: 2019-03-06 18:07:32+00:00
- **Updated**: 2019-03-06 18:07:32+00:00
- **Authors**: Ruizhi Liao, Esra A. Turk, Miaomiao Zhang, Jie Luo, Elfar Adalsteinsson, P. Ellen Grant, Polina Golland
- **Comment**: arXiv admin note: text overlap with arXiv:1608.03907
- **Journal**: None
- **Summary**: We present a robust method to correct for motion in volumetric in-utero MRI time series. Time-course analysis for in-utero volumetric MRI time series often suffers from substantial and unpredictable fetal motion. Registration provides voxel correspondences between images and is commonly employed for motion correction. Current registration methods often fail when aligning images that are substantially different from a template (reference image). To achieve accurate and robust alignment, we make a Markov assumption on the nature of motion and take advantage of the temporal smoothness in the image data. Forward message passing in the corresponding hidden Markov model (HMM) yields an estimation algorithm that only has to account for relatively small motion between consecutive frames. We evaluate the utility of the temporal model in the context of in-utero MRI time series alignment by examining the accuracy of propagated segmentation label maps. Our results suggest that the proposed model captures accurately the temporal dynamics of transformations in in-utero MRI time series.



### Combining Optimal Control and Learning for Visual Navigation in Novel Environments
- **Arxiv ID**: http://arxiv.org/abs/1903.02531v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1903.02531v2)
- **Published**: 2019-03-06 18:11:32+00:00
- **Updated**: 2019-07-17 22:32:51+00:00
- **Authors**: Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik, Claire Tomlin
- **Comment**: Project website: https://vtolani95.github.io/WayPtNav/
- **Journal**: None
- **Summary**: Model-based control is a popular paradigm for robot navigation because it can leverage a known dynamics model to efficiently plan robust robot trajectories. However, it is challenging to use model-based methods in settings where the environment is a priori unknown and can only be observed partially through on-board sensors on the robot. In this work, we address this short-coming by coupling model-based control with learning-based perception. The learning-based perception module produces a series of waypoints that guide the robot to the goal via a collision-free path. These waypoints are used by a model-based planner to generate a smooth and dynamically feasible trajectory that is executed on the physical system using feedback control. Our experiments in simulated real-world cluttered environments and on an actual ground vehicle demonstrate that the proposed approach can reach goal locations more reliably and efficiently in novel environments as compared to purely geometric mapping-based or end-to-end learning-based alternatives. Our approach does not rely on detailed explicit 3D maps of the environment, works well with low frame rates, and generalizes well from simulation to the real world. Videos describing our approach and experiments are available on the project website.



### Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1903.02547v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.02547v2)
- **Published**: 2019-03-06 18:54:55+00:00
- **Updated**: 2019-04-02 17:48:26+00:00
- **Authors**: Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa
- **Comment**: CVPR 2019 Oral, video demo: https://youtu.be/AD9TNohXoPA
- **Journal**: None
- **Summary**: We present the Frontier Aware Search with backTracking (FAST) Navigator, a general framework for action decoding, that achieves state-of-the-art results on the Room-to-Room (R2R) Vision-and-Language navigation challenge of Anderson et. al. (2018). Given a natural language instruction and photo-realistic image views of a previously unseen environment, the agent was tasked with navigating from source to target location as quickly as possible. While all current approaches make local action decisions or score entire trajectories using beam search, ours balances local and global signals when exploring an unobserved environment. Importantly, this lets us act greedily but use global signals to backtrack when necessary. Applying FAST framework to existing state-of-the-art models achieved a 17% relative gain, an absolute 6% gain on Success rate weighted by Path Length (SPL).



### Clear Skies Ahead: Towards Real-Time Automatic Sky Replacement in Video
- **Arxiv ID**: http://arxiv.org/abs/1903.02582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02582v1)
- **Published**: 2019-03-06 19:03:47+00:00
- **Updated**: 2019-03-06 19:03:47+00:00
- **Authors**: Tavi Halperin, Harel Cain, Ofir Bibi, Michael Werman
- **Comment**: Eurographics 2019. Supplementary video: https://youtu.be/1uZ46YzX-pI
- **Journal**: None
- **Summary**: Digital videos such as those captured by a smartphone often exhibit exposure inconsistencies, a poorly exposed sky, or simply suffer from an uninteresting or plain looking sky. Professionals may edit these videos using advanced and time-consuming tools unavailable to most users, to replace the sky with a more expressive or imaginative sky. In this work, we propose an algorithm for automatic replacement of the sky region in a video with a different sky, providing nonprofessional users with a simple yet efficient tool to seamlessly replace the sky. The method is fast, achieving close to real-time performance on mobile devices and the user's involvement can remain as limited as simply selecting the replacement sky.



### GanDef: A GAN based Adversarial Training Defense for Neural Network Classifier
- **Arxiv ID**: http://arxiv.org/abs/1903.02585v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.02585v1)
- **Published**: 2019-03-06 19:09:47+00:00
- **Updated**: 2019-03-06 19:09:47+00:00
- **Authors**: Guanxiong Liu, Issa Khalil, Abdallah Khreishah
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning models, especially neural network (NN) classifiers, are widely used in many applications including natural language processing, computer vision and cybersecurity. They provide high accuracy under the assumption of attack-free scenarios. However, this assumption has been defied by the introduction of adversarial examples -- carefully perturbed samples of input that are usually misclassified. Many researchers have tried to develop a defense against adversarial examples; however, we are still far from achieving that goal. In this paper, we design a Generative Adversarial Net (GAN) based adversarial training defense, dubbed GanDef, which utilizes a competition game to regulate the feature selection during the training. We analytically show that GanDef can train a classifier so it can defend against adversarial examples. Through extensive evaluation on different white-box adversarial examples, the classifier trained by GanDef shows the same level of test accuracy as those trained by state-of-the-art adversarial training defenses. More importantly, GanDef-Comb, a variant of GanDef, could utilize the discriminator to achieve a dynamic trade-off between correctly classifying original and adversarial examples. As a result, it achieves the highest overall test accuracy when the ratio of adversarial examples exceeds 41.7%.



### Conditional GANs For Painting Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.06259v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06259v1)
- **Published**: 2019-03-06 19:47:56+00:00
- **Updated**: 2019-03-06 19:47:56+00:00
- **Authors**: Adeel Mufti, Biagio Antonelli, Julius Monello
- **Comment**: None
- **Journal**: None
- **Summary**: We examined the use of modern Generative Adversarial Nets to generate novel images of oil paintings using the Painter By Numbers dataset. We implemented Spectral Normalization GAN (SN-GAN) and Spectral Normalization GAN with Gradient Penalty, and compared their outputs to a Deep Convolutional GAN. Visually, and quantitatively according to the Sliced Wasserstein Distance metric, we determined that the SN-GAN produced paintings that were most comparable to our training dataset. We then performed a series of experiments to add supervised conditioning to SN-GAN, the culmination of which is what we believe to be a novel architecture that can generate face paintings with user-specified characteristics.



### Hierarchical Autoregressive Image Models with Auxiliary Decoders
- **Arxiv ID**: http://arxiv.org/abs/1903.04933v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.04933v2)
- **Published**: 2019-03-06 22:13:52+00:00
- **Updated**: 2019-10-08 17:55:59+00:00
- **Authors**: Jeffrey De Fauw, Sander Dieleman, Karen Simonyan
- **Comment**: Updated: added human evaluation results, incorporated review feedback
- **Journal**: None
- **Summary**: Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\times$128 and 256$\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models.



### IMEXnet: A Forward Stable Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1903.02639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.02639v2)
- **Published**: 2019-03-06 22:33:06+00:00
- **Updated**: 2019-05-17 21:45:28+00:00
- **Authors**: Eldad Haber, Keegan Lensink, Eran Treister, Lars Ruthotto
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have revolutionized many machine learning and computer vision tasks, however, some remaining key challenges limit their wider use. These challenges include improving the network's robustness to perturbations of the input image and the limited ``field of view'' of convolution operators. We introduce the IMEXnet that addresses these challenges by adapting semi-implicit methods for partial differential equations. Compared to similar explicit networks, such as residual networks, our network is more stable, which has recently shown to reduce the sensitivity to small changes in the input features and improve generalization. The addition of an implicit step connects all pixels in each channel of the image and therefore addresses the field of view problem while still being comparable to standard convolutions in terms of the number of parameters and computational complexity. We also present a new dataset for semantic segmentation and demonstrate the effectiveness of our architecture using the NYU Depth dataset.



### Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.06260v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06260v1)
- **Published**: 2019-03-06 23:24:08+00:00
- **Updated**: 2019-03-06 23:24:08+00:00
- **Authors**: Tim Sodergren, Riddhish Bhalodia, Ross Whitaker, Joshua Cates, Nassir Marrouche, Shireen Elhabian
- **Comment**: Statistical Atlases and Computational Models of the Heart. Atrial
  Segmentation and LV Quantification Challenges 2019
- **Journal**: Statistical Atlases and Computational Models of the Heart. Atrial
  Segmentation and LV Quantification Challenges, 2019, Springer International
  Publishing, Cham 357--367,
- **Summary**: Difficult image segmentation problems, for instance left atrium MRI, can be addressed by incorporating shape priors to find solutions that are consistent with known objects. Nonetheless, a single multivariate Gaussian is not an adequate model in cases with significant nonlinear shape variation or where the prior distribution is multimodal. Nonparametric density estimation is more general, but has a ravenous appetite for training samples and poses serious challenges in optimization, especially in high dimensional spaces. Here, we propose a maximum-a-posteriori formulation that relies on a generative image model by incorporating both local intensity and global shape priors. We use deep autoencoders to capture the complex intensity distribution while avoiding the careful selection of hand-crafted features. We formulate the shape prior as a mixture of Gaussians and learn the corresponding parameters in a high-dimensional shape space rather than pre-projecting onto a low-dimensional subspace. In segmentation, we treat the identity of the mixture component as a latent variable and marginalize it within a generalized expectation-maximization framework. We present a conditional maximization-based scheme that alternates between a closed-form solution for component-specific shape parameters that provides a global update-based optimization strategy, and an intensity-based energy minimization that translates the global notion of a nonlinear shape prior into a set of local penalties. We demonstrate our approach on the left atrial segmentation from gadolinium-enhanced MRI, which is useful in quantifying the atrial geometry in patients with atrial fibrillation.



