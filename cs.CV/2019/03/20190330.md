# Arxiv Papers in cs.CV on 2019-03-30
### Dance Dance Generation: Motion Transfer for Internet Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.00129v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00129v1)
- **Published**: 2019-03-30 01:01:12+00:00
- **Updated**: 2019-03-30 01:01:12+00:00
- **Authors**: Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara L. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents computational methods for transferring body movements from one person to another with videos collected in the wild. Specifically, we train a personalized model on a single video from the Internet which can generate videos of this target person driven by the motions of other people. Our model is built on two generative networks: a human (foreground) synthesis net which generates photo-realistic imagery of the target person in a novel pose, and a fusion net which combines the generated foreground with the scene (background), adding shadows or reflections as needed to enhance realism. We validate the the efficacy of our proposed models over baselines with qualitative and quantitative evaluations as well as a subjective test.



### Robust Subspace Recovery Layer for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.00152v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00152v2)
- **Published**: 2019-03-30 05:30:54+00:00
- **Updated**: 2019-12-24 22:44:25+00:00
- **Authors**: Chieh-Hsin Lai, Dongmian Zou, Gilad Lerman
- **Comment**: This work is on the ICLR 2020 conference
- **Journal**: Eighth International Conference on Learning Representations
  (ICLR), 2020, https://openreview.net/pdf?id=rylb3eBtwr
- **Summary**: We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a "manifold" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall.



### UVA: A Universal Variational Framework for Continuous Age Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.00158v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00158v1)
- **Published**: 2019-03-30 07:07:06+00:00
- **Updated**: 2019-03-30 07:07:06+00:00
- **Authors**: Peipei Li, Huaibo Huang, Yibo Hu, Xiang Wu, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Conventional methods for facial age analysis tend to utilize accurate age labels in a supervised way. However, existing age datasets lies in a limited range of ages, leading to a long-tailed distribution. To alleviate the problem, this paper proposes a Universal Variational Aging (UVA) framework to formulate facial age priors in a disentangling manner. Benefiting from the variational evidence lower bound, the facial images are encoded and disentangled into an age-irrelevant distribution and an age-related distribution in the latent space. A conditional introspective adversarial learning mechanism is introduced to boost the image quality. In this way, when manipulating the age-related distribution, UVA can achieve age translation with arbitrary ages. Further, by sampling noise from the age-irrelevant distribution, we can generate photorealistic facial images with a specific age. Moreover, given an input face image, the mean value of age-related distribution can be treated as an age estimator. These indicate that UVA can efficiently and accurately estimate the age-related distribution by a disentangling manner, even if the training dataset performs a long-tailed age distribution. UVA is the first attempt to achieve facial age analysis tasks, including age translation, age generation and age estimation, in a universal framework. The qualitative and quantitative experiments demonstrate the superiority of UVA on five popular datasets, including CACD2000, Morph, UTKFace, MegaAge-Asian and FG-NET.



### Exposing GAN-synthesized Faces Using Landmark Locations
- **Arxiv ID**: http://arxiv.org/abs/1904.00167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00167v1)
- **Published**: 2019-03-30 08:27:46+00:00
- **Updated**: 2019-03-30 08:27:46+00:00
- **Authors**: Xin Yang, Yuezun Li, Honggang Qi, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversary networks (GANs) have recently led to highly realistic image synthesis results. In this work, we describe a new method to expose GAN-synthesized images using the locations of the facial landmark points. Our method is based on the observations that the facial parts configuration generated by GAN models are different from those of the real faces, due to the lack of global constraints. We perform experiments demonstrating this phenomenon, and show that an SVM classifier trained using the locations of facial landmark points is sufficient to achieve good classification performance for GAN-synthesized faces.



### M2FPA: A Multi-Yaw Multi-Pitch High-Quality Database and Benchmark for Facial Pose Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.00168v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00168v2)
- **Published**: 2019-03-30 08:35:36+00:00
- **Updated**: 2019-10-20 07:34:22+00:00
- **Authors**: Peipei Li, Xiang Wu, Yibo Hu, Ran He, Zhenan Sun
- **Comment**: Accepted for publication at ICCV2019; The M2FPA dataset is available
  at https://pp2li.github.io/M2FPA-dataset/
- **Journal**: None
- **Summary**: Facial images in surveillance or mobile scenarios often have large view-point variations in terms of pitch and yaw angles. These jointly occurred angle variations make face recognition challenging. Current public face databases mainly consider the case of yaw variations. In this paper, a new large-scale Multi-yaw Multi-pitch high-quality database is proposed for Facial Pose Analysis (M2FPA), including face frontalization, face rotation, facial pose estimation and pose-invariant face recognition. It contains 397,544 images of 229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is the most comprehensive multi-view face database for facial pose analysis. Further, we provide an effective benchmark for face frontalization and pose-invariant face recognition on M2FPA with several state-of-the-art methods, including DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and benchmark can significantly push forward the advance of facial pose analysis in real-world applications. Moreover, a simple yet effective parsing guided discriminator is introduced to capture the local consistency during GAN optimization. Extensive quantitative and qualitative results on M2FPA and Multi-PIE demonstrate the superiority of our face frontalization method. Baseline results for both face synthesis and face recognition from state-of-theart methods demonstrate the challenge offered by this new database.



### Adaptive Adjustment with Semantic Feature Space for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.00170v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00170v1)
- **Published**: 2019-03-30 08:39:03+00:00
- **Updated**: 2019-03-30 08:39:03+00:00
- **Authors**: Jingcai Guo, Song Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In most recent years, zero-shot recognition (ZSR) has gained increasing attention in machine learning and image processing fields. It aims at recognizing unseen class instances with knowledge transferred from seen classes. This is typically achieved by exploiting a pre-defined semantic feature space (FS), i.e., semantic attributes or word vectors, as a bridge to transfer knowledge between seen and unseen classes. However, due to the absence of unseen classes during training, the conventional ZSR easily suffers from domain shift and hubness problems. In this paper, we propose a novel ZSR learning framework that can handle these two issues well by adaptively adjusting semantic FS. To the best of our knowledge, our work is the first to consider the adaptive adjustment of semantic FS in ZSR. Moreover, our solution can be formulated to a more efficient framework that significantly boosts the training. Extensive experiments show the remarkable performance improvement of our model compared with other existing methods.



### An LBP-HOG Descriptor Based on Matrix Projection For Mammogram Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.00187v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00187v4)
- **Published**: 2019-03-30 09:51:40+00:00
- **Updated**: 2021-02-07 11:49:46+00:00
- **Authors**: Zainab Alhakeem, Se-In Jang
- **Comment**: 5 pages, submitted to ICIP 2021 conference
- **Journal**: None
- **Summary**: In image based feature descriptor design, local information from image patches are extracted using iterative scanning operations which cause high computational costs. In order to avoid such scanning operations, we present matrix multiplication based local feature descriptors, namely a Matrix projection based Local Binary Pattern (M-LBP) descriptor and a Matrix projection based Histogram of Oriented Gradients (M-HOG) descriptor. Additionally, an integrated formulation of M-LBP and M-HOG (M-LBP-HOG) is also proposed to perform the two descriptors together in a single step. The proposed descriptors are evaluated using a publicly available mammogram database. The results show promising performances in terms of classification accuracy and computational efficiency.



### Exploiting SIFT Descriptor for Rotation Invariant Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.00197v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00197v1)
- **Published**: 2019-03-30 11:00:21+00:00
- **Updated**: 2019-03-30 11:00:21+00:00
- **Authors**: Abhay Kumar, Nishant Jain, Chirag Singh, Suraj Tripathi
- **Comment**: Accepted in IEEE INDICON 2018
- **Journal**: None
- **Summary**: This paper presents a novel approach to exploit the distinctive invariant features in convolutional neural network. The proposed CNN model uses Scale Invariant Feature Transform (SIFT) descriptor instead of the max-pooling layer. Max-pooling layer discards the pose, i.e., translational and rotational relationship between the low-level features, and hence unable to capture the spatial hierarchies between low and high level features. The SIFT descriptor layer captures the orientation and the spatial relationship of the features extracted by convolutional layer. The proposed SIFT Descriptor CNN therefore combines the feature extraction capabilities of CNN model and rotation invariance of SIFT descriptor. Experimental results on the MNIST and fashionMNIST datasets indicates reasonable improvements over conventional methods available in literature.



### Boundary Aware Multi-Focus Image Fusion Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.00198v1
- **DOI**: 10.1109/ICME.2019.00201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00198v1)
- **Published**: 2019-03-30 11:01:16+00:00
- **Updated**: 2019-03-30 11:01:16+00:00
- **Authors**: Haoyu Ma, Juncheng Zhang, Shaojun Liu, Qingmin Liao
- **Comment**: None
- **Journal**: None
- **Summary**: Since it is usually difficult to capture an all-in-focus image of a 3D scene directly, various multi-focus image fusion methods are employed to generate it from several images focusing at different depths. However, the performance of existing methods is barely satisfactory and often degrades for areas near the focused/defocused boundary (FDB). In this paper, a boundary aware method using deep neural network is proposed to overcome this problem. (1) Aiming to acquire improved fusion images, a 2-channel deep network is proposed to better extract the relative defocus information of the two source images. (2) After analyzing the different situations for patches far away from and near the FDB, we use two networks to handle them respectively. (3) To simulate the reality more precisely, a new approach of dataset generation is designed. Experiments demonstrate that the proposed method outperforms the state-of-the-art methods, both qualitatively and quantitatively.



### A HVS-inspired Attention to Improve Loss Metrics for CNN-based Perception-Oriented Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.00205v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00205v2)
- **Published**: 2019-03-30 12:14:50+00:00
- **Updated**: 2019-07-27 15:19:46+00:00
- **Authors**: Taimoor Tariq, Juan Luis Gonzalez, Munchurl Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Network (CNN) features have been demonstrated to be effective perceptual quality features. The perceptual loss, based on feature maps of pre-trained CNN's has proven to be remarkably effective for CNN based perceptual image restoration problems. In this work, taking inspiration from the the Human Visual System (HVS) and visual perception, we propose a spatial attention mechanism based on the dependency human contrast sensitivity on spatial frequency. We identify regions in input images, based on the underlying spatial frequency, which are not generally well reconstructed during Super-Resolution but are most important in terms of visual sensitivity. Based on this prior, we design a spatial attention map that is applied to feature maps in the perceptual loss and its variants, helping them to identify regions that are of more perceptual importance. The results demonstrate the our technique improves the ability of the perceptual loss and contextual loss to deliver more natural images in CNN based super-resolution.



### RefineLoc: Iterative Refinement for Weakly-Supervised Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1904.00227v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00227v3)
- **Published**: 2019-03-30 14:51:44+00:00
- **Updated**: 2020-11-08 22:22:59+00:00
- **Authors**: Alejandro Pardo, Humam Alwassel, Fabian Caba Heilbron, Ali Thabet, Bernard Ghanem
- **Comment**: Accepted to WACV 2021. Project website:
  http://humamalwassel.com/publication/refineloc
- **Journal**: None
- **Summary**: Video action detectors are usually trained using datasets with fully-supervised temporal annotations. Building such datasets is an expensive task. To alleviate this problem, recent methods have tried to leverage weak labeling, where videos are untrimmed and only a video-level label is available. In this paper, we propose RefineLoc, a novel weakly-supervised temporal action localization method. RefineLoc uses an iterative refinement approach by estimating and training on snippet-level pseudo ground truth at every iteration. We show the benefit of this iterative approach and present an extensive analysis of five different pseudo ground truth generators. We show the effectiveness of our model on two standard action datasets, ActivityNet v1.2 and THUMOS14. RefineLoc shows competitive results with the state-of-the-art in weakly-supervised temporal localization. Additionally, our iterative refinement process is able to significantly improve the performance of two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.



### USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.00229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00229v1)
- **Published**: 2019-03-30 15:11:20+00:00
- **Updated**: 2019-03-30 15:11:20+00:00
- **Authors**: Jiaxin Li, Gim Hee Lee
- **Comment**: 19 pages
- **Journal**: None
- **Summary**: In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis of our USIP detector and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP



### MortonNet: Self-Supervised Learning of Local Features in 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.00230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00230v1)
- **Published**: 2019-03-30 15:12:49+00:00
- **Updated**: 2019-03-30 15:12:49+00:00
- **Authors**: Ali Thabet, Humam Alwassel, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: We present a self-supervised task on point clouds, in order to learn meaningful point-wise features that encode local structure around each point. Our self-supervised network, named MortonNet, operates directly on unstructured/unordered point clouds. Using a multi-layer RNN, MortonNet predicts the next point in a point sequence created by a popular and fast Space Filling Curve, the Morton-order curve. The final RNN state (coined Morton feature) is versatile and can be used in generic 3D tasks on point clouds. In fact, we show how Morton features can be used to significantly improve performance (+3% for 2 popular semantic segmentation algorithms) in the task of semantic segmentation of point clouds on the challenging and large-scale S3DIS dataset. We also show how MortonNet trained on S3DIS transfers well to another large-scale dataset, vKITTI, leading to an improvement over state-of-the-art of 3.8%. Finally, we use Morton features to train a much simpler and more stable model for part segmentation in ShapeNet. Our results show how our self-supervised task results in features that are useful for 3D segmentation tasks, and generalize well to other datasets.



### OSVNet: Convolutional Siamese Network for Writer Independent Online Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1904.00240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00240v2)
- **Published**: 2019-03-30 16:07:59+00:00
- **Updated**: 2019-05-21 11:42:14+00:00
- **Authors**: Chandra Sekhar, Prerana Mukherjee, Devanur S Guru, Viswanath Pulabaigari
- **Comment**: accepted in International Conference on Document Analysis and
  Recognition (ICDAR 2019), University of Technology Sydney (UTS), Australia
- **Journal**: None
- **Summary**: Online signature verification (OSV) is one of the most challenging tasks in writer identification and digital forensics. Owing to the large intra-individual variability, there is a critical requirement to accurately learn the intra-personal variations of the signature to achieve higher classification accuracy. To achieve this, in this paper, we propose an OSV framework based on deep convolutional Siamese network (DCSN). DCSN automatically extracts robust feature descriptions based on metric-based loss function which decreases intra-writer variability (Genuine-Genuine) and increases inter-individual variability (Genuine-Forgery) and directs the DCSN for effective discriminative representation learning for online signatures and extend it for one shot learning framework. Comprehensive experimentation conducted on three widely accepted benchmark datasets MCYT-100 (DB1), MCYT-330 (DB2) and SVC-2004-Task2 demonstrate the capability of our framework to distinguish the genuine and forgery samples. Experimental results confirm the efficiency of deep convolutional Siamese network based OSV by achieving a lower error rate as compared to many recent and state-of-the art OSV techniques.



### Person Re-identification with Bias-controlled Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1904.00244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00244v1)
- **Published**: 2019-03-30 16:25:23+00:00
- **Updated**: 2019-03-30 16:25:23+00:00
- **Authors**: Sara Iodice, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by the effectiveness of adversarial training in the area of Generative Adversarial Networks we present a new approach for learning feature representations in person re-identification. We investigate different types of bias that typically occur in re-ID scenarios, i.e., pose, body part and camera view, and propose a general approach to address them. We introduce an adversarial strategy for controlling bias, named Bias-controlled Adversarial framework (BCA), with two complementary branches to reduce or to enhance bias-related features. The results and comparison to the state of the art on different benchmarks show that our framework is an effective strategy for person re-identification. The performance improvements are in both full and partial views of persons.



### Classification of Motorcycles using Extracted Images of Traffic Monitoring Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.00247v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00247v2)
- **Published**: 2019-03-30 16:40:15+00:00
- **Updated**: 2019-06-20 15:32:26+00:00
- **Authors**: Adriano Belletti Felicio, André Luiz Cunha
- **Comment**: 12 pages, 6 figures, 4 table
- **Journal**: None
- **Summary**: Due to the great growth of motorcycles in the urban fleet and the growth of the study on its behavior and of how this vehicle affects the flow of traffic becomes necessary the development of tools and techniques different from the conventional ones to identify its presence in the traffic flow and be able to extract your information. The article in question attempts to contribute to the study on this type of vehicle by generating a motorcycle image bank and developing and calibrating a motorcycle classifier by combining the LBP techniques to create the characteristic vectors and the classification technique LinearSVC to perform the predictions. In this way the classifier of vehicles of the type motorcycle developed in this research can classify the images of vehicles extracted of videos of monitoring between two classes motorcycles and non-motorcycles with a precision and an accuracy superior to 0,9.



### Person-in-WiFi: Fine-grained Person Perception using WiFi
- **Arxiv ID**: http://arxiv.org/abs/1904.00276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00276v1)
- **Published**: 2019-03-30 19:48:59+00:00
- **Updated**: 2019-03-30 19:48:59+00:00
- **Authors**: Fei Wang, Sanping Zhou, Stanislav Panev, Jinsong Han, Dong Huang
- **Comment**: 10 pages, 13 figures, 3 tables
- **Journal**: None
- **Summary**: Fine-grained person perception such as body segmentation and pose estimation has been achieved with many 2D and 3D sensors such as RGB/depth cameras, radars (e.g., RF-Pose) and LiDARs. These sensors capture 2D pixels or 3D point clouds of person bodies with high spatial resolution, such that the existing Convolutional Neural Networks can be directly applied for perception. In this paper, we take one step forward to show that fine-grained person perception is possible even with 1D sensors: WiFi antennas. To our knowledge, this is the first work to perceive persons with pervasive WiFi devices, which is cheaper and power efficient than radars and LiDARs, invariant to illumination, and has little privacy concern comparing to cameras. We used two sets of off-the-shelf WiFi antennas to acquire signals, i.e., one transmitter set and one receiver set. Each set contains three antennas lined-up as a regular household WiFi router. The WiFi signal generated by a transmitter antenna, penetrates through and reflects on human bodies, furniture and walls, and then superposes at a receiver antenna as a 1D signal sample (instead of 2D pixels or 3D point clouds). We developed a deep learning approach that uses annotations on 2D images, takes the received 1D WiFi signals as inputs, and performs body segmentation and pose estimation in an end-to-end manner. Experimental results on over 100000 frames under 16 indoor scenes demonstrate that Person-in-WiFi achieved person perception comparable to approaches using 2D images.



### Can WiFi Estimate Person Pose?
- **Arxiv ID**: http://arxiv.org/abs/1904.00277v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00277v2)
- **Published**: 2019-03-30 19:50:52+00:00
- **Updated**: 2019-04-02 04:46:21+00:00
- **Authors**: Fei Wang, Stanislav Panev, Ziyi Dai, Jinsong Han, Dong Huang
- **Comment**: 11 pages, 9 figures, 2 tables
- **Journal**: None
- **Summary**: WiFi human sensing has achieved great progress in indoor localization, activity classification, etc. Retracing the development of these work, we have a natural question: can WiFi devices work like cameras for vision applications? In this paper We try to answer this question by exploring the ability of WiFi on estimating single person pose. We use a 3-antenna WiFi sender and a 3-antenna receiver to generate WiFi data. Meanwhile, we use a synchronized camera to capture person videos for corresponding keypoint annotations. We further propose a fully convolutional network (FCN), termed WiSPPN, to estimate single person pose from the collected data and annotations. Evaluation on over 80k images (16 sites and 8 persons) replies aforesaid question with a positive answer. Codes have been made publicly available at https://github.com/geekfeiw/WiSPPN.



### COCO-GAN: Generation by Parts via Conditional Coordinating
- **Arxiv ID**: http://arxiv.org/abs/1904.00284v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.00284v4)
- **Published**: 2019-03-30 20:37:24+00:00
- **Updated**: 2020-01-05 06:28:59+00:00
- **Authors**: Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen
- **Comment**: Accepted to ICCV'19 (oral). All images are compressed due to size
  limit, please access the full-resolution version via Google Drive:
  http://bit.ly/COCO-GAN-full
- **Journal**: None
- **Summary**: Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose \underline{CO}nditional \underline{CO}ordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never generated during training, we show that COCO-GAN can produce \textbf{state-of-the-art-quality} full images during inference. We further demonstrate a variety of novel applications enabled by teaching the network to be aware of coordinates. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called "beyond-boundary generation". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.



### Evaluating CNNs on the Gestalt Principle of Closure
- **Arxiv ID**: http://arxiv.org/abs/1904.00285v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00285v1)
- **Published**: 2019-03-30 20:54:50+00:00
- **Updated**: 2019-03-30 20:54:50+00:00
- **Authors**: Gregor Ehrensperger, Sebastian Stabinger, Antonio Rodríguez Sánchez
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are widely known for their outstanding performance in classification and regression tasks over high-dimensional data. This made them a popular and powerful tool for a large variety of applications in industry and academia. Recent publications show that seemingly easy classifaction tasks (for humans) can be very challenging for state of the art CNNs. An attempt to describe how humans perceive visual elements is given by the Gestalt principles. In this paper we evaluate AlexNet and GoogLeNet regarding their performance on classifying the correctness of the well known Kanizsa triangles, which heavily rely on the Gestalt principle of closure. Therefore we created various datasets containing valid as well as invalid variants of the Kanizsa triangle. Our findings suggest that perceiving objects by utilizing the principle of closure is very challenging for the applied network architectures but they appear to adapt to the effect of closure.



### Two-phase flow regime prediction using LSTM based deep recurrent neural network
- **Arxiv ID**: http://arxiv.org/abs/1904.00291v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.app-ph, physics.data-an, physics.flu-dyn
- **Links**: [PDF](http://arxiv.org/pdf/1904.00291v1)
- **Published**: 2019-03-30 21:23:26+00:00
- **Updated**: 2019-03-30 21:23:26+00:00
- **Authors**: Zhuoran Dang, Mamoru Ishii
- **Comment**: None
- **Journal**: None
- **Summary**: Long short-term memory (LSTM) and recurrent neural network (RNN) has achieved great successes on time-series prediction. In this paper, a methodology of using LSTM-based deep-RNN for two-phase flow regime prediction is proposed, motivated by previous research on constructing deep RNN. The method is featured with fast response and accuracy. The built RNN networks are trained and tested with time-series void fraction data collected using impedance void meter. The result shows that the prediction accuracy depends on the depth of network and the number of layer cells. However, deeper and larger network consumes more time in predicting.



