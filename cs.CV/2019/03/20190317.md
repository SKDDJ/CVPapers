# Arxiv Papers in cs.CV on 2019-03-17
### Deep Features for Tissue-Fold Detection in Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1903.07011v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.07011v1)
- **Published**: 2019-03-17 01:25:10+00:00
- **Updated**: 2019-03-17 01:25:10+00:00
- **Authors**: Morteza Babaie, H. R. Tizhoosh
- **Comment**: Accepted for publication in the 15th European Congress on Digital
  Pathology (ECDP 2019), University of Warwick, UK
- **Journal**: None
- **Summary**: Whole slide imaging (WSI) refers to the digitization of a tissue specimen which enables pathologists to explore high-resolution images on a monitor rather than through a microscope. The formation of tissue folds occur during tissue processing. Their presence may not only cause out-of-focus digitization but can also negatively affect the diagnosis in some cases. In this paper, we have compared five pre-trained convolutional neural networks (CNNs) of different depths as feature extractors to characterize tissue folds. We have also explored common classifiers to discriminate folded tissue against the normal tissue in hematoxylin and eosin (H\&E) stained biopsy samples. In our experiments, we manually select the folded area in roughly 2.5mm $\times$ 2.5mm patches at $20$x magnification level as the training data. The ``DenseNet'' with 201 layers alongside an SVM classifier outperformed all other configurations. Based on the leave-one-out validation strategy, we achieved $96.3\%$ accuracy, whereas with augmentation the accuracy increased to $97.2\%$. We have tested the generalization of our method with five unseen WSIs from the NIH (National Cancer Institute) dataset. The accuracy for patch-wise detection was $81\%$. One folded patch within an image suffices to flag the entire specimen for visual inspection.



### Patch Clustering for Representation of Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1903.07013v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.07013v1)
- **Published**: 2019-03-17 01:40:45+00:00
- **Updated**: 2019-03-17 01:40:45+00:00
- **Authors**: Wafa Chenni, Habib Herbi, Morteza Babaie, H. R. Tizhoosh
- **Comment**: Accepted for publication in the 15th European Congress on Digital
  Pathology (ECDP 2019), University of Warwick, UK
- **Journal**: None
- **Summary**: Whole Slide Imaging (WSI) has become an important topic during the last decade. Even though significant progress in both medical image processing and computational resources has been achieved, there are still problems in WSI that need to be solved. A major challenge is the scan size. The dimensions of digitized tissue samples may exceed 100,000 by 100,000 pixels causing memory and efficiency obstacles for real-time processing. The main contribution of this work is representing a WSI by selecting a small number of patches for algorithmic processing (e.g., indexing and search). As a result, we reduced the search time and storage by various factors between ($50\% - 90\%$), while losing only a few percentages in the patch retrieval accuracy. A self-organizing map (SOM) has been applied on local binary patterns (LBP) and deep features of the KimiaPath24 dataset in order to cluster patches that share the same characteristics. We used a Gaussian mixture model (GMM) to represent each class with a rather small ($10\%-50\%$) portion of patches. The results showed that LBP features can outperform deep features. By selecting only $50\%$ of all patches after SOM clustering and GMM patch selection, we received $65\%$ accuracy for retrieval of the best match, while the maximum accuracy (using all patches) was $69\%$.



### Reconstructing neuronal anatomy from whole-brain images
- **Arxiv ID**: http://arxiv.org/abs/1903.07027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1903.07027v1)
- **Published**: 2019-03-17 05:06:29+00:00
- **Updated**: 2019-03-17 05:06:29+00:00
- **Authors**: James Gornet, Kannan Umadevi Venkataraju, Arun Narasimhan, Nicholas Turner, Kisuk Lee, H. Sebastian Seung, Pavel Osten, Uygar Sümbül
- **Comment**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Journal**: None
- **Summary**: Reconstructing multiple molecularly defined neurons from individual brains and across multiple brain regions can reveal organizational principles of the nervous system. However, high resolution imaging of the whole brain is a technically challenging and slow process. Recently, oblique light sheet microscopy has emerged as a rapid imaging method that can provide whole brain fluorescence microscopy at a voxel size of 0.4 by 0.4 by 2.5 cubic microns. On the other hand, complex image artifacts due to whole-brain coverage produce apparent discontinuities in neuronal arbors. Here, we present connectivity-preserving methods and data augmentation strategies for supervised learning of neuroanatomy from light microscopy using neural networks. We quantify the merit of our approach by implementing an end-to-end automated tracing pipeline. Lastly, we demonstrate a scalable, distributed implementation that can reconstruct the large datasets that sub-micron whole-brain images produce.



### Discriminating Original Region from Duplicated One in Copy-Move Forgery
- **Arxiv ID**: http://arxiv.org/abs/1903.07044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07044v1)
- **Published**: 2019-03-17 08:52:55+00:00
- **Updated**: 2019-03-17 08:52:55+00:00
- **Authors**: Saba Salehi, Ahmad Mahmoodi-Aznaveh
- **Comment**: None
- **Journal**: None
- **Summary**: Since images are used as evidence in many cases, validation of digital images is essential. Copy-move forgery is a special kind of manipulation in which some parts of an image is copied and pasted into another part of the same image. Various methods have been proposed to detect copy-move forgery, which have achieved promising results. In previous methods, a binary mask determining the original and forged region is presented as the final result. However, it is not specified which part of the mask is the forged region. It should be noted that discriminating the original region from the duplicated one is not usually feasible by human visual system(HVS). On the other hand, exact localizing the forged region can be helpful for automatic forgery detection especially in combined forgeries. In real-world forgeries, some manipulations are performed in order to provide a visibly realistic scene. These modifications are usually applied on the boundary of the duplicated snippets. In this research, the texture information of the border regions of both the original and copied patches have been statistically investigated. Based on this analysis, we propose a method to discriminated copied snippets from original ones. In order to validate our method, GRIP dataset is utilized since it contains more realistic forged images which are not easily recognizable by HVS.



### AdaGraph: Unifying Predictive and Continuous Domain Adaptation through Graphs
- **Arxiv ID**: http://arxiv.org/abs/1903.07062v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07062v3)
- **Published**: 2019-03-17 11:56:45+00:00
- **Updated**: 2019-06-12 23:01:27+00:00
- **Authors**: Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci
- **Comment**: CVPR 2019 (oral)
- **Journal**: None
- **Summary**: The ability to categorize is a cornerstone of visual intelligence, and a key functionality for artificial, autonomous visual machines. This problem will never be solved without algorithms able to adapt and generalize across visual domains. Within the context of domain adaptation and generalization, this paper focuses on the predictive domain adaptation scenario, namely the case where no target data are available and the system has to learn to generalize from annotated source images plus unlabeled samples with associated metadata from auxiliary domains. Our contributionis the first deep architecture that tackles predictive domainadaptation, able to leverage over the information broughtby the auxiliary domains through a graph. Moreover, we present a simple yet effective strategy that allows us to take advantage of the incoming target data at test time, in a continuous domain adaptation scenario. Experiments on three benchmark databases support the value of our approach.



### Spatiotemporal Filtering for Event-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.07067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07067v1)
- **Published**: 2019-03-17 12:13:14+00:00
- **Updated**: 2019-03-17 12:13:14+00:00
- **Authors**: Rohan Ghosh, Anupam Gupta, Andrei Nakagawa, Alcimar Soares, Nitish Thakor
- **Comment**: Submitted to IEEE Transactions in Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: In this paper, we address the challenging problem of action recognition, using event-based cameras. To recognise most gestural actions, often higher temporal precision is required for sampling visual information. Actions are defined by motion, and therefore, when using event-based cameras it is often unnecessary to re-sample the entire scene. Neuromorphic, event-based cameras have presented an alternative to visual information acquisition by asynchronously time-encoding pixel intensity changes, through temporally precise spikes (10 micro-second resolution), making them well equipped for action recognition. However, other challenges exist, which are intrinsic to event-based imagers, such as higher signal-to-noise ratio, and a spatiotemporally sparse information. One option is to convert event-data into frames, but this could result in significant temporal precision loss. In this work we introduce spatiotemporal filtering in the spike-event domain, as an alternative way of channeling spatiotemporal information through to a convolutional neural network. The filters are local spatiotemporal weight matrices, learned from the spike-event data, in an unsupervised manner. We find that appropriate spatiotemporal filtering significantly improves CNN performance beyond state-of-the-art on the event-based DVS Gesture dataset. On our newly recorded action recognition dataset, our method shows significant improvement when compared with other, standard ways of generating the spatiotemporal filters.



### Bag of Tricks and A Strong Baseline for Deep Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1903.07071v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07071v3)
- **Published**: 2019-03-17 12:35:55+00:00
- **Updated**: 2019-04-19 07:24:53+00:00
- **Authors**: Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, Wei Jiang
- **Comment**: CVPR2019 Workshop. Code is available at
  https://github.com/michuanhaohao/reid-strong-baseline
- **Journal**: http://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf
- **Summary**: This paper explores a simple and efficient baseline for person re-identification (ReID). Person re-identification (ReID) with deep neural networks has made progress and achieved high performance in recent years. However, many state-of-the-arts methods design complex network structure and concatenate multi-branch features. In the literature, some effective training tricks are briefly appeared in several papers or source codes. This paper will collect and evaluate these effective training tricks in person ReID. By combining these tricks together, the model achieves 94.5% rank-1 and 85.9% mAP on Market1501 with only using global features. Our codes and models are available at https://github.com/michuanhaohao/reid-strong-baseline.



### STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer Networks for Partial Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1903.07072v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07072v2)
- **Published**: 2019-03-17 12:36:08+00:00
- **Updated**: 2020-01-07 10:31:00+00:00
- **Authors**: Hao Luo, Xing Fan, Chi Zhang, Wei Jiang
- **Comment**: Accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Partial person re-identification (ReID) is a challenging task because only partial information of person images is available for matching target persons. Few studies, especially on deep learning, have focused on matching partial person images with holistic person images. This study presents a novel deep partial ReID framework based on pairwise spatial transformer networks (STNReID), which can be trained on existing holistic person datasets. STNReID includes a spatial transformer network (STN) module and a ReID module. The STN module samples an affined image (a semantically corresponding patch) from the holistic image to match the partial image. The ReID module extracts the features of the holistic, partial, and affined images. Competition (or confrontation) is observed between the STN module and the ReID module, and two-stage training is applied to acquire a strong STNReID for partial ReID. Experimental results show that our STNReID obtains 66.7% and 54.6% rank-1 accuracies on partial ReID and partial iLIDS datasets, respectively. These values are at par with those obtained with state-of-the-art methods.



### A Weighted Multi-Criteria Decision Making Approach for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.00766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00766v1)
- **Published**: 2019-03-17 13:20:01+00:00
- **Updated**: 2019-03-17 13:20:01+00:00
- **Authors**: Hassan Maleki Galandouz, Mohsen Ebrahimi Moghaddam, Mehrnoush Shamsfard
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Image captioning aims at automatically generating descriptions of an image in natural language. This is a challenging problem in the field of artificial intelligence that has recently received significant attention in the computer vision and natural language processing. Among the existing approaches, visual retrieval based methods have been proven to be highly effective. These approaches search for similar images, then build a caption for the query image based on the captions of the retrieved images. In this study, we present a method for visual retrieval based image captioning, in which we use a multi criteria decision making algorithm to effectively combine several criteria with proportional impact weights to retrieve the most relevant caption for the query image. The main idea of the proposed approach is to design a mechanism to retrieve more semantically relevant captions with the query image and then selecting the most appropriate caption by imitation of the human act based on a weighted multi-criteria decision making algorithm. Experiments conducted on MS COCO benchmark dataset have shown that proposed method provides much more effective results in compare to the state-of-the-art models by using criteria with proportional impact weights .



### Inverse Path Tracing for Joint Material and Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/1903.07145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07145v1)
- **Published**: 2019-03-17 19:06:31+00:00
- **Updated**: 2019-03-17 19:06:31+00:00
- **Authors**: Dejan Azinović, Tzu-Mao Li, Anton Kaplanyan, Matthias Nießner
- **Comment**: CVPR'19 (Oral); Video: https://youtu.be/nC_t0t9u6ws
- **Journal**: None
- **Summary**: Modern computer vision algorithms have brought significant advancement to 3D geometry reconstruction. However, illumination and material reconstruction remain less studied, with current approaches assuming very simplified models for materials and illumination. We introduce Inverse Path Tracing, a novel approach to jointly estimate the material properties of objects and light sources in indoor scenes by using an invertible light transport simulation. We assume a coarse geometry scan, along with corresponding images and camera poses. The key contribution of this work is an accurate and simultaneous retrieval of light sources and physically based material properties (e.g., diffuse reflectance, specular reflectance, roughness, etc.) for the purpose of editing and re-rendering the scene under new conditions. To this end, we introduce a novel optimization method using a differentiable Monte Carlo renderer that computes derivatives with respect to the estimated unknown illumination and material properties. This enables joint optimization for physically correct light transport and material models using a tailored stochastic gradient descent.



### Robust Shape Regularity Criteria for Superpixel Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1903.07146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07146v1)
- **Published**: 2019-03-17 19:08:39+00:00
- **Updated**: 2019-03-17 19:08:39+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis
- **Comment**: International Conference on Image Processing 2017
- **Journal**: None
- **Summary**: Regular decompositions are necessary for most superpixel-based object recognition or tracking applications. So far in the literature, the regularity or compactness of a superpixel shape is mainly measured by its circularity. In this work, we first demonstrate that such measure is not adapted for superpixel evaluation, since it does not directly express regularity but circular appearance. Then, we propose a new metric that considers several shape regularity aspects: convexity, balanced repartition, and contour smoothness. Finally, we demonstrate that our measure is robust to scale and noise and enables to more relevantly compare superpixel methods.



### SCALP: Superpixels with Contour Adherence using Linear Path
- **Arxiv ID**: http://arxiv.org/abs/1903.07149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07149v1)
- **Published**: 2019-03-17 19:23:00+00:00
- **Updated**: 2019-03-17 19:23:00+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis
- **Comment**: International Conference on Pattern Recognition (ICPR) 2016
- **Journal**: None
- **Summary**: Superpixel decomposition methods are generally used as a pre-processing step to speed up image processing tasks. They group the pixels of an image into homogeneous regions while trying to respect existing contours. For all state-of-the-art superpixel decomposition methods, a trade-off is made between 1) computational time, 2) adherence to image contours and 3) regularity and compactness of the decomposition. In this paper, we propose a fast method to compute Superpixels with Contour Adherence using Linear Path (SCALP) in an iterative clustering framework. The distance computed when trying to associate a pixel to a superpixel during the clustering is enhanced by considering the linear path to the superpixel barycenter. The proposed framework produces regular and compact superpixels that adhere to the image contours. We provide a detailed evaluation of SCALP on the standard Berkeley Segmentation Dataset. The obtained results outperform state-of-the-art methods in terms of standard superpixel and contour detection metrics.



### Proximal Splitting Networks for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1903.07154v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.07154v1)
- **Published**: 2019-03-17 19:36:52+00:00
- **Updated**: 2019-03-17 19:36:52+00:00
- **Authors**: Raied Aljadaany, Dipan K. Pal, Marios Savvides
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration problems are typically ill-posed requiring the design of suitable priors. These priors are typically hand-designed and are fully instantiated throughout the process. In this paper, we introduce a novel framework for handling inverse problems related to image restoration based on elements from the half quadratic splitting method and proximal operators. Modeling the proximal operator as a convolutional network, we defined an implicit prior on the image space as a function class during training. This is in contrast to the common practice in literature of having the prior to be fixed and fully instantiated even during training stages. Further, we allow this proximal operator to be tuned differently for each iteration which greatly increases modeling capacity and allows us to reduce the number of iterations by an order of magnitude as compared to other approaches. Our final network is an end-to-end one whose run time matches the previous fastest algorithms while outperforming them in recovery fidelity on two image restoration tasks. Indeed, we find our approach achieves state-of-the-art results on benchmarks in image denoising and image super resolution while recovering more complex and finer details.



### Evaluation Framework of Superpixel Methods with a Global Regularity Measure
- **Arxiv ID**: http://arxiv.org/abs/1903.07162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07162v1)
- **Published**: 2019-03-17 20:29:55+00:00
- **Updated**: 2019-03-17 20:29:55+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis
- **Comment**: Journal of Electronic Imaging (JEI), 2017 Special issue on
  Superpixels for Image Processing and Computer Vision
- **Journal**: None
- **Summary**: In the superpixel literature, the comparison of state-of-the-art methods can be biased by the non-robustness of some metrics to decomposition aspects, such as the superpixel scale. Moreover, most recent decomposition methods allow to set a shape regularity parameter, which can have a substantial impact on the measured performances. In this paper, we introduce an evaluation framework, that aims to unify the comparison process of superpixel methods. We investigate the limitations of existing metrics, and propose to evaluate each of the three core decomposition aspects: color homogeneity, respect of image objects and shape regularity. To measure the regularity aspect, we propose a new global regularity measure (GR), which addresses the non-robustness of state-of-the-art metrics. We evaluate recent superpixel methods with these criteria, at several superpixel scales and regularity levels. The proposed framework reduces the bias in the comparison process of state-of-the-art superpixel methods. Finally, we demonstrate that the proposed GR measure is correlated with the performances of various applications.



### An Optimized PatchMatch for Multi-scale and Multi-feature Label Fusion
- **Arxiv ID**: http://arxiv.org/abs/1903.07165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07165v1)
- **Published**: 2019-03-17 20:40:09+00:00
- **Updated**: 2019-03-17 20:40:09+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis, José V. Manjón, D. Louis Collins, Pierrick Coupé, Alzheimer's Disease Neuroimaging Initiative
- **Comment**: Neuroimage 2016
- **Journal**: None
- **Summary**: Automatic segmentation methods are important tools for quantitative analysis of Magnetic Resonance Images (MRI). Recently, patch-based label fusion approaches have demonstrated state-of-the-art segmentation accuracy. In this paper, we introduce a new patch-based label fusion framework to perform segmentation of anatomical structures. The proposed approach uses an Optimized PAtchMatch Label fusion (OPAL) strategy that drastically reduces the computation time required for the search of similar patches. The reduced computation time of OPAL opens the way for new strategies and facilitates processing on large databases. In this paper, we investigate new perspectives offered by OPAL, by introducing a new multi-scale and multi-feature framework. During our validation on hippocampus segmentation we use two datasets: young adults in the ICBM cohort and elderly adults in the EADC-ADNI dataset. For both, OPAL is compared to state-of-the-art methods. Results show that OPAL obtained the highest median Dice coefficient (89.9% for ICBM and 90.1% for EADC-ADNI). Moreover, in both cases, OPAL produced a segmentation accuracy similar to inter-expert variability. On the EADC-ADNI dataset, we compare the hippocampal volumes obtained by manual and automatic segmentation. The volumes appear to be highly correlated that enables to perform more accurate separation of pathological populations.



### SuperPatchMatch: an Algorithm for Robust Correspondences using Superpixel Patches
- **Arxiv ID**: http://arxiv.org/abs/1903.07169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07169v1)
- **Published**: 2019-03-17 21:08:38+00:00
- **Updated**: 2019-03-17 21:08:38+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Aurélie Bugeau, Pierrick Coupé, Nicolas Papadakis
- **Comment**: IEEE Transactions on Image Processing (TIP), 2017 Selected for
  presentation at IEEE International Conference on Image Processing (ICIP) 2017
- **Journal**: None
- **Summary**: Superpixels have become very popular in many computer vision applications. Nevertheless, they remain underexploited since the superpixel decomposition may produce irregular and non stable segmentation results due to the dependency to the image content. In this paper, we first introduce a novel structure, a superpixel-based patch, called SuperPatch. The proposed structure, based on superpixel neighborhood, leads to a robust descriptor since spatial information is naturally included. The generalization of the PatchMatch method to SuperPatches, named SuperPatchMatch, is introduced. Finally, we propose a framework to perform fast segmentation and labeling from an image database, and demonstrate the potential of our approach since we outperform, in terms of computational cost and accuracy, the results of state-of-the-art methods on both face labeling and medical image segmentation.



### Training recurrent neural networks robust to incomplete data: application to Alzheimer's disease progression modeling
- **Arxiv ID**: http://arxiv.org/abs/1903.07173v1
- **DOI**: 10.1016/j.media.2019.01.004
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.07173v1)
- **Published**: 2019-03-17 21:14:33+00:00
- **Updated**: 2019-03-17 21:14:33+00:00
- **Authors**: Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso, Marc Modat, Sebastien Ourselin, Lauge Sørensen
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1808.05500
- **Journal**: Medical Image Analysis, Volume 53, Pages 39-46, 2019
- **Summary**: Disease progression modeling (DPM) using longitudinal data is a challenging machine learning task. Existing DPM algorithms neglect temporal dependencies among measurements, make parametric assumptions about biomarker trajectories, do not model multiple biomarkers jointly, and need an alignment of subjects' trajectories. In this paper, recurrent neural networks (RNNs) are utilized to address these issues. However, in many cases, longitudinal cohorts contain incomplete data, which hinders the application of standard RNNs and requires a pre-processing step such as imputation of the missing values. Instead, we propose a generalized training rule for the most widely used RNN architecture, long short-term memory (LSTM) networks, that can handle both missing predictor and target values. The proposed LSTM algorithm is applied to model the progression of Alzheimer's disease (AD) using six volumetric magnetic resonance imaging (MRI) biomarkers, i.e., volumes of ventricles, hippocampus, whole brain, fusiform, middle temporal gyrus, and entorhinal cortex, and it is compared to standard LSTM networks with data imputation and a parametric, regression-based DPM method. The results show that the proposed algorithm achieves a significantly lower mean absolute error (MAE) than the alternatives with p < 0.05 using Wilcoxon signed rank test in predicting values of almost all of the MRI biomarkers. Moreover, a linear discriminant analysis (LDA) classifier applied to the predicted biomarker values produces a significantly larger AUC of 0.90 vs. at most 0.84 with p < 0.001 using McNemar's test for clinical diagnosis of AD. Inspection of MAE curves as a function of the amount of missing data reveals that the proposed LSTM algorithm achieves the best performance up until more than 74% missing values. Finally, it is illustrated how the method can successfully be applied to data with varying time intervals.



### Weighted Mean Curvature
- **Arxiv ID**: http://arxiv.org/abs/1903.07189v2
- **DOI**: 10.1016/j.sigpro.2019.06.020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07189v2)
- **Published**: 2019-03-17 22:28:44+00:00
- **Updated**: 2019-05-07 09:26:33+00:00
- **Authors**: Yuanhao Gong, Orcun Goksel
- **Comment**: 12 pages
- **Journal**: Signal Processing 164 (2019) 329-339
- **Summary**: In image processing tasks, spatial priors are essential for robust computations, regularization, algorithmic design and Bayesian inference. In this paper, we introduce weighted mean curvature (WMC) as a novel image prior and present an efficient computation scheme for its discretization in practical image processing applications. We first demonstrate the favorable properties of WMC, such as sampling invariance, scale invariance, and contrast invariance with Gaussian noise model; and we show the relation of WMC to area regularization. We further propose an efficient computation scheme for discretized WMC, which is demonstrated herein to process over 33.2 giga-pixels/second on GPU. This scheme yields itself to a convolutional neural network representation. Finally, WMC is evaluated on synthetic and real images, showing its superiority quantitatively to total-variation and mean curvature.



### Learning-Based Animation of Clothing for Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/1903.07190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07190v1)
- **Published**: 2019-03-17 22:43:28+00:00
- **Updated**: 2019-03-17 22:43:28+00:00
- **Authors**: Igor Santesteban, Miguel A. Otaduy, Dan Casas
- **Comment**: Eurographics 2019
- **Journal**: None
- **Summary**: This paper presents a learning-based clothing animation method for highly efficient virtual try-on simulation. Given a garment, we preprocess a rich database of physically-based dressed character simulations, for multiple body shapes and animations. Then, using this database, we train a learning-based model of cloth drape and wrinkles, as a function of body shape and dynamics. We propose a model that separates global garment fit, due to body shape, from local garment wrinkles, due to both pose dynamics and body shape. We use a recurrent neural network to regress garment wrinkles, and we achieve highly plausible nonlinear effects, in contrast to the blending artifacts suffered by previous methods. At runtime, dynamic virtual try-on animations are produced in just a few milliseconds for garments with thousands of triangles. We show qualitative and quantitative analysis of results



### Robust superpixels using color and contour features along linear path
- **Arxiv ID**: http://arxiv.org/abs/1903.07193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.07193v1)
- **Published**: 2019-03-17 23:00:13+00:00
- **Updated**: 2019-03-17 23:00:13+00:00
- **Authors**: Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis
- **Comment**: Computer Vision and Image Understanding (CVIU), 2018
- **Journal**: None
- **Summary**: Superpixel decomposition methods are widely used in computer vision and image processing applications. By grouping homogeneous pixels, the accuracy can be increased and the decrease of the number of elements to process can drastically reduce the computational burden. For most superpixel methods, a trade-off is computed between 1) color homogeneity, 2) adherence to the image contours and 3) shape regularity of the decomposition. In this paper, we propose a framework that jointly enforces all these aspects and provides accurate and regular Superpixels with Contour Adherence using Linear Path (SCALP). During the decomposition, we propose to consider color features along the linear path between the pixel and the corresponding superpixel barycenter. A contour prior is also used to prevent the crossing of image boundaries when associating a pixel to a superpixel. Finally, in order to improve the decomposition accuracy and the robustness to noise, we propose to integrate the pixel neighborhood information, while preserving the same computational complexity. SCALP is extensively evaluated on standard segmentation dataset, and the obtained results outperform the ones of the state-of-the-art methods. SCALP is also extended for supervoxel decomposition on MRI images.



