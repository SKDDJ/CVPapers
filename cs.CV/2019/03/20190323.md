# Arxiv Papers in cs.CV on 2019-03-23
### Fast LLMMSE filter for low-dose CT imaging
- **Arxiv ID**: http://arxiv.org/abs/1903.09745v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09745v1)
- **Published**: 2019-03-23 02:17:10+00:00
- **Updated**: 2019-03-23 02:17:10+00:00
- **Authors**: Fengling Wang, Bowen Lin, Shujun Fu, Shiling Xie, Zhigang Zhao, Yuliang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Low-dose X-ray CT technology is one of important directions of current research and development of medical imaging equipment. A fast algorithm of blockwise sinogram filtering is presented for realtime low-dose CT imaging. A nonstationary Gaussian noise model of low-dose sinogram data is proposed in the low-mA (tube current) CT protocol. Then, according to the linear minimum mean square error principle, an adaptive blockwise algorithm is built to filter contaminated sinogram data caused by photon starvation. A moving sum technique is used to speed the algorithm into a linear time one, regardless of the block size and thedata range. The proposedfast filtering givesa better performance in noise reduction and detail preservation in the reconstructed images,which is verified in experiments on simulated and real data compared with some related filtering methods.



### Residual Pyramid Learning for Single-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.09746v1
- **DOI**: 10.1109/TITS.2019.2922252
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09746v1)
- **Published**: 2019-03-23 02:48:07+00:00
- **Updated**: 2019-03-23 02:48:07+00:00
- **Authors**: Xiaoyu Chen, Xiaotian Lou, Lianfa Bai, Jing Han
- **Comment**: 10 pages, 6 figures, 6 tables
- **Journal**: None
- **Summary**: Pixel-level semantic segmentation is a challenging task with a huge amount of computation, especially if the size of input is large. In the segmentation model, apart from the feature extraction, the extra decoder structure is often employed to recover spatial information. In this paper, we put forward a method for single-shot segmentation in a feature residual pyramid network (RPNet), which learns the main and residuals of segmentation by decomposing the label at different levels of residual blocks. Specifically speaking, we use the residual features to learn the edges and details, and the identity features to learn the main part of targets. At testing time, the predicted residuals are used to enhance the details of the top-level prediction. Residual learning blocks split the network into several shallow sub-networks which facilitates the training of the RPNet. We then evaluate the proposed method and compare it with recent state-of-the-art methods on CamVid and Cityscapes. The proposed single-shot segmentation based on RPNet achieves impressive results with high efficiency on pixel-level segmentation.



### V2CNet: A Deep Learning Framework to Translate Videos to Commands for Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1903.10869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.10869v1)
- **Published**: 2019-03-23 04:02:51+00:00
- **Updated**: 2019-03-23 04:02:51+00:00
- **Authors**: Anh Nguyen, Thanh-Toan Do, Ian Reid, Darwin G. Caldwell, Nikos G. Tsagarakis
- **Comment**: 15 pages. arXiv admin note: substantial text overlap with
  arXiv:1710.00290
- **Journal**: None
- **Summary**: We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.



### Trifocal Relative Pose from Lines at Points and its Efficient Solution
- **Arxiv ID**: http://arxiv.org/abs/1903.09755v4
- **DOI**: None
- **Categories**: **cs.CV**, 14Qxx, 12Yxx, 51N15, 14N05, 53A20, 17B81, 22E70, 53A04, 53A55,
  53Bxx, 53B5, 57R25, 58C25, 68T40, 68U05, 70B1, 70G55, 70G65, 90C30, I.4.5; I.4.8; I.2.9; I.2.10; I.1.2; G.1.3; G.1.5
- **Links**: [PDF](http://arxiv.org/pdf/1903.09755v4)
- **Published**: 2019-03-23 04:26:57+00:00
- **Updated**: 2022-11-30 01:55:22+00:00
- **Authors**: Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret Regan, David da Costa de Pinho, Elias Tsigaridas, Charles Wampler, Jonathan Hauenstein, Benjamin Kimia, Anton Leykin, Tomas Pajdla
- **Comment**: First appeared at CVPR - Computer Vision and Pattern Recognition
  Conference 2020. This material is based upon work supported by the National
  Science Foundation under Grant No. DMS-1439786 while most authors were in
  residence at Brown University's Institute for Computational and Experimental
  Research in Mathematics -- ICERM, in Providence, RI
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  preprint available December 2022
- **Summary**: We present a method for solving two minimal problems for relative camera pose estimation from three views, which are based on three view correspondences of i) three points and one line and the novel case of ii) three points and two lines through two of the points. These problems are too difficult to be efficiently solved by the state of the art Groebner basis methods. Our method is based on a new efficient homotopy continuation (HC) solver framework MINUS, which dramatically speeds up previous HC solving by specializing HC methods to generic cases of our problems. We characterize their number of solutions and show with simulated experiments that our solvers are numerically robust and stable under image noise, a key contribution given the borderline intractable degree of nonlinearity of trinocular constraints. We show in real experiments that i) SIFT feature location and orientation provide good enough point-and-line correspondences for three-view reconstruction and ii) that we can solve difficult cases with too few or too noisy tentative matches, where the state of the art structure from motion initialization fails.



### Photorealistic Style Transfer via Wavelet Transforms
- **Arxiv ID**: http://arxiv.org/abs/1903.09760v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09760v2)
- **Published**: 2019-03-23 05:02:00+00:00
- **Updated**: 2019-09-29 08:49:37+00:00
- **Authors**: Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, Jung-Woo Ha
- **Comment**: Accepted to ICCV 2019, Code and data: https://github.com/ClovaAI/WCT2
- **Journal**: None
- **Summary**: Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT$^2$) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a $1024\times1024$ resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, and pre-trained models are all available at https://github.com/ClovaAI/WCT2.



### Scene Understanding for Autonomous Manipulation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.09761v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.09761v1)
- **Published**: 2019-03-23 05:05:22+00:00
- **Updated**: 2019-03-23 05:05:22+00:00
- **Authors**: Anh Nguyen
- **Comment**: PhD Thesis, 88 pages
- **Journal**: None
- **Summary**: Over the past few years, deep learning techniques have achieved tremendous success in many visual understanding tasks such as object detection, image segmentation, and caption generation. Despite this thriving in computer vision and natural language processing, deep learning has not yet shown significant impact in robotics. Due to the gap between theory and application, there are many challenges when applying the results of deep learning to the real robotic systems. In this study, our long-term goal is to bridge the gap between computer vision and robotics by developing visual methods that can be used in real robots. In particular, this work tackles two fundamental visual problems for autonomous robotic manipulation: affordance detection and fine-grained action understanding. Theoretically, we propose different deep architectures to further improves the state of the art in each problem. Empirically, we show that the outcomes of our proposed methods can be applied in real robots and allow them to perform useful manipulation tasks.



### Fast Underwater Image Enhancement for Improved Visual Perception
- **Arxiv ID**: http://arxiv.org/abs/1903.09766v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09766v3)
- **Published**: 2019-03-23 05:21:05+00:00
- **Updated**: 2020-02-09 02:06:40+00:00
- **Authors**: Md Jahidul Islam, Youya Xia, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and unpaired collection of underwater images (of `poor' and `good' quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.



### Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning and Quantization Rates using ADMM
- **Arxiv ID**: http://arxiv.org/abs/1903.09769v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09769v2)
- **Published**: 2019-03-23 05:54:26+00:00
- **Updated**: 2019-03-30 03:27:38+00:00
- **Authors**: Shaokai Ye, Xiaoyu Feng, Tianyun Zhang, Xiaolong Ma, Sheng Lin, Zhengang Li, Kaidi Xu, Wujie Wen, Sijia Liu, Jian Tang, Makan Fardad, Xue Lin, Yongpan Liu, Yanzhi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Weight pruning and weight quantization are two important categories of DNN model compression. Prior work on these techniques are mainly based on heuristics. A recent work developed a systematic frame-work of DNN weight pruning using the advanced optimization technique ADMM (Alternating Direction Methods of Multipliers), achieving one of state-of-art in weight pruning results. In this work, we first extend such one-shot ADMM-based framework to guarantee solution feasibility and provide fast convergence rate, and generalize to weight quantization as well. We have further developed a multi-step, progressive DNN weight pruning and quantization framework, with dual benefits of (i) achieving further weight pruning/quantization thanks to the special property of ADMM regularization, and (ii) reducing the search space within each step. Extensive experimental results demonstrate the superior performance compared with prior work. Some highlights: (i) we achieve 246x,36x, and 8x weight pruning on LeNet-5, AlexNet, and ResNet-50 models, respectively, with (almost) zero accuracy loss; (ii) even a significant 61x weight pruning in AlexNet (ImageNet) results in only minor degradation in actual accuracy compared with prior work; (iii) we are among the first to derive notable weight pruning results for ResNet and MobileNet models; (iv) we derive the first lossless, fully binarized (for all layers) LeNet-5 for MNIST and VGG-16 for CIFAR-10; and (v) we derive the first fully binarized (for all layers) ResNet for ImageNet with reasonable accuracy loss.



### Auto-ReID: Searching for a Part-aware ConvNet for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1903.09776v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09776v4)
- **Published**: 2019-03-23 07:26:50+00:00
- **Updated**: 2019-08-20 08:20:43+00:00
- **Authors**: Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, Yi Yang
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50% parameters and 53% FLOPs compared to others.



### What Synthesis is Missing: Depth Adaptation Integrated with Weak Supervision for Indoor Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1903.09781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09781v1)
- **Published**: 2019-03-23 08:26:34+00:00
- **Updated**: 2019-03-23 08:26:34+00:00
- **Authors**: Keng-Chi Liu, Yi-Ting Shen, Jan P. Klopp, Liang-Gee Chen
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Scene Parsing is a crucial step to enable autonomous systems to understand and interact with their surroundings. Supervised deep learning methods have made great progress in solving scene parsing problems, however, come at the cost of laborious manual pixel-level annotation. To alleviate this effort synthetic data as well as weak supervision have both been investigated. Nonetheless, synthetically generated data still suffers from severe domain shift while weak labels are often imprecise. Moreover, most existing works for weakly supervised scene parsing are limited to salient foreground objects. The aim of this work is hence twofold: Exploit synthetic data where feasible and integrate weak supervision where necessary. More concretely, we address this goal by utilizing depth as transfer domain because its synthetic-to-real discrepancy is much lower than for color. At the same time, we perform weak localization from easily obtainable image level labels and integrate both using a novel contour-based scheme. Our approach is implemented as a teacher-student learning framework to solve the transfer learning problem by generating a pseudo ground truth. Using only depth-based adaptation, this approach already outperforms previous transfer learning approaches on the popular indoor scene parsing SUN RGB-D dataset. Our proposed two-stage integration more than halves the gap towards fully supervised methods when compared to previous state-of-the-art in transfer learning.



### An End-to-End Network for Generating Social Relationship Graphs
- **Arxiv ID**: http://arxiv.org/abs/1903.09784v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09784v1)
- **Published**: 2019-03-23 08:36:46+00:00
- **Updated**: 2019-03-23 08:36:46+00:00
- **Authors**: Arushi Goel, Keng Teck Ma, Cheston Tan
- **Comment**: None
- **Journal**: CVPR 2019
- **Summary**: Socially-intelligent agents are of growing interest in artificial intelligence. To this end, we need systems that can understand social relationships in diverse social contexts. Inferring the social context in a given visual scene not only involves recognizing objects, but also demands a more in-depth understanding of the relationships and attributes of the people involved. To achieve this, one computational approach for representing human relationships and attributes is to use an explicit knowledge graph, which allows for high-level reasoning. We introduce a novel end-to-end-trainable neural network that is capable of generating a Social Relationship Graph - a structured, unified representation of social relationships and attributes - from a given input image. Our Social Relationship Graph Generation Network (SRG-GN) is the first to use memory cells like Gated Recurrent Units (GRUs) to iteratively update the social relationship states in a graph using scene and attribute context. The neural network exploits the recurrent connections among the GRUs to implement message passing between nodes and edges in the graph, and results in significant improvement over previous methods for social relationship recognition.



### Cursive Overlapped Character Segmentation: An Enhanced Approach
- **Arxiv ID**: http://arxiv.org/abs/1904.00792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00792v1)
- **Published**: 2019-03-23 09:59:03+00:00
- **Updated**: 2019-03-23 09:59:03+00:00
- **Authors**: Amjad Rehman
- **Comment**: 10 Pages
- **Journal**: None
- **Summary**: Segmentation of highly slanted and horizontally overlapped characters is a challenging research area that is still fresh. Several techniques are reported in the state of art, but produce low accuracy for the highly slanted characters segmentation and cause overall low handwriting recognition precision. Accordingly, this paper presents a simple yet effective approach for character segmentation of such difficult slanted cursive words without using any slant correction technique. Rather a new concept of core-zone is introduced for segmenting such difficult slanted handwritten words. However, due to the inherent nature of cursive words, few characters are over-segmented and therefore, a threshold is selected heuristically to overcome this problem. For fair comparison, difficult words are extracted from the IAM benchmark database. Experiments thus performed exhibit promising result and high speed.



### Spatially-weighted Anomaly Detection with Regression Model
- **Arxiv ID**: http://arxiv.org/abs/1903.09798v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09798v2)
- **Published**: 2019-03-23 11:07:37+00:00
- **Updated**: 2019-03-28 08:59:17+00:00
- **Authors**: Daiki Kimura, Minori Narita, Asim Munawar, Ryuki Tachibana
- **Comment**: 4 pages, published as an oral presentation paper at Meeting on Image
  Recognition and Understanding (MIRU) 2018
- **Journal**: None
- **Summary**: Visual anomaly detection is common in several applications including medical screening and production quality check. Although a definition of the anomaly is an unknown trend in data, in many cases some hints or samples of the anomaly class can be given in advance. Conventional methods cannot use the available anomaly data, and also do not have a robustness of noise. In this paper, we propose a novel spatially-weighted reconstruction-loss-based anomaly detection with a likelihood value from a regression model trained by all known data. The spatial weights are calculated by a region of interest generated from employing visualization of the regression model. We introduce some ways to combine with various strategies to propose a state-of-the-art method. Comparing with other methods on three different datasets, we empirically verify the proposed method performs better than the others.



### Improving Adversarial Robustness via Guided Complement Entropy
- **Arxiv ID**: http://arxiv.org/abs/1903.09799v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.PF, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.09799v3)
- **Published**: 2019-03-23 11:14:59+00:00
- **Updated**: 2019-08-07 06:11:33+00:00
- **Authors**: Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan
- **Comment**: ICCV'19 Camera Ready
- **Journal**: None
- **Summary**: Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving "adversarial defense for free," which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a "guided" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.



### Retinal OCT disease classification with variational autoencoder regularization
- **Arxiv ID**: http://arxiv.org/abs/1904.00790v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00790v1)
- **Published**: 2019-03-23 11:20:51+00:00
- **Updated**: 2019-03-23 11:20:51+00:00
- **Authors**: Max-Heinrich Laves, Sontje Ihler, Lüder A. Kahrs, Tobias Ortmaier
- **Comment**: Accepted for publication at 33rd international conference on Computer
  Assisted Radiology and Surgery (CARS) 2019
- **Journal**: None
- **Summary**: According to the World Health Organization, 285 million people worldwide live with visual impairment. The most commonly used imaging technique for diagnosis in ophthalmology is optical coherence tomography (OCT). However, analysis of retinal OCT requires trained ophthalmologists and time, making a comprehensive early diagnosis unlikely. A recent study established a diagnostic tool based on convolutional neural networks (CNN), which was trained on a large database of retinal OCT images. The performance of the tool in classifying retinal conditions was on par to that of trained medical experts. However, the training of these networks is based on an enormous amount of labeled data, which is expensive and difficult to obtain. Therefore, this paper describes a method based on variational autoencoder regularization that improves classification performance when using a limited amount of labeled data. This work uses a two-path CNN model combining a classification network with an autoencoder (AE) for regularization. The key idea behind this is to prevent overfitting when using a limited training dataset size with small number of patients. Results show superior classification performance compared to a pre-trained and fully fine-tuned baseline ResNet-34. Clustering of the latent space in relation to the disease class is distinct. Neural networks for disease classification on OCTs can benefit from regularization using variational autoencoders when trained with limited amount of patient data. Especially in the medical imaging domain, data annotated by experts is expensive to obtain.



### BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation Function
- **Arxiv ID**: http://arxiv.org/abs/1903.09807v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09807v1)
- **Published**: 2019-03-23 11:52:23+00:00
- **Updated**: 2019-03-23 11:52:23+00:00
- **Authors**: Hyungjun Kim, Yulhwa Kim, Sungju Ryu, Jae-Joon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Significant computational cost and memory requirements for deep neural networks (DNNs) make it difficult to utilize DNNs in resource-constrained environments. Binary neural network (BNN), which uses binary weights and binary activations, has been gaining interests for its hardware-friendly characteristics and minimal resource requirement. However, BNN usually suffers from accuracy degradation. In this paper, we introduce "BitSplit-Net", a neural network which maintains the hardware-friendly characteristics of BNN while improving accuracy by using multi-bit precision. In BitSplit-Net, each bit of multi-bit activations propagates independently throughout the network before being merged at the end of the network. Thus, each bit path of the BitSplit-Net resembles BNN and hardware friendly features of BNN, such as bitwise binary activation function, are preserved in our scheme. We demonstrate that the BitSplit version of LeNet-5, VGG-9, AlexNet, and ResNet-18 can be trained to have similar classification accuracy at a lower computational cost compared to conventional multi-bit networks with low bit precision (<= 4-bit). We further evaluate BitSplit-Net on GPU with custom CUDA kernel, showing that BitSplit-Net can achieve better hardware performance in comparison to conventional multi-bit networks.



### Semantic denoising autoencoders for retinal optical coherence tomography
- **Arxiv ID**: http://arxiv.org/abs/1903.09809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09809v1)
- **Published**: 2019-03-23 12:01:03+00:00
- **Updated**: 2019-03-23 12:01:03+00:00
- **Authors**: Max-Heinrich Laves, Sontje Ihler, Lüder Alexander Kahrs, Tobias Ortmaier
- **Comment**: Accepted for publication at the SPIE/OSA European Conferences on
  Biomedical Optics (ECBO) 2019
- **Journal**: None
- **Summary**: Noise in speckle-prone optical coherence tomography tends to obfuscate important details necessary for medical diagnosis. In this paper, a denoising approach that preserves disease characteristics on retinal optical coherence tomography images in ophthalmology is presented. By combining a deep convolutional autoencoder with a priorly trained ResNet image classifier as regularizer, the perceptibility of delicate details is encouraged and only information-less background noise is filtered out. With our approach, higher peak signal-to-noise ratios with $ \mathrm{PSNR} = 31.2\,\mathrm{dB} $ and higher classification accuracy of $\mathrm{ACC} = 85.0\,\%$ can be achieved for denoised images compared to state-of-the-art denoising with $ \mathrm{PSNR} = 29.4\,\mathrm{dB} $ or $\mathrm{ACC} = 70.3\,\%$, depending on the method. It is shown that regularized autoencoders are capable of denoising retinal OCT images without blurring details of diseases.



### Feedback Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1903.09814v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09814v2)
- **Published**: 2019-03-23 12:29:18+00:00
- **Updated**: 2019-06-28 06:16:57+00:00
- **Authors**: Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Recent advances in image super-resolution (SR) explored the power of deep learning to achieve a better reconstruction performance. However, the feedback mechanism, which commonly exists in human visual system, has not been fully exploited in existing deep learning based image SR methods. In this paper, we propose an image super-resolution feedback network (SRFBN) to refine low-level representations with high-level information. Specifically, we use hidden states in an RNN with constraints to achieve such feedback manner. A feedback block is designed to handle the feedback connections and to generate powerful high-level representations. The proposed SRFBN comes with a strong early reconstruction ability and can create the final high-resolution image step by step. In addition, we introduce a curriculum learning strategy to make the network well suitable for more complicated tasks, where the low-resolution images are corrupted by multiple types of degradation. Extensive experimental results demonstrate the superiority of the proposed SRFBN in comparison with the state-of-the-art methods. Code is avaliable at https://github.com/Paper99/SRFBN_CVPR19.



### Color Filter Arrays for Quanta Image Sensors
- **Arxiv ID**: http://arxiv.org/abs/1903.09823v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09823v4)
- **Published**: 2019-03-23 13:35:10+00:00
- **Updated**: 2019-12-20 06:14:06+00:00
- **Authors**: Omar A. Elgendy, Stanley H. Chan
- **Comment**: None
- **Journal**: None
- **Summary**: Quanta image sensor (QIS) is envisioned to be the next generation image sensor after CCD and CMOS. In this paper, we discuss how to design color filter arrays for QIS and other small pixels. Designing color filter arrays for small pixels is challenging because maximizing the light efficiency while suppressing aliasing and crosstalk are conflicting tasks. We present an optimization-based framework which unifies several mainstream color filter array design methodologies. Our method offers greater generality and flexibility. Compared to existing methods, the new framework can simultaneously handle luminance sensitivity, chrominance sensitivity, cross-talk, anti-aliasing, manufacturability and orthogonality. Extensive experimental comparisons demonstrate the effectiveness of the framework.



### 1D-Convolutional Capsule Network for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1903.09834v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09834v1)
- **Published**: 2019-03-23 15:22:49+00:00
- **Updated**: 2019-03-23 15:22:49+00:00
- **Authors**: Haitao Zhang, Lingguo Meng, Xian Wei, Xiaoliang Tang, Xuan Tang, Xingping Wang, Bo Jin, Wei Yao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) have achieved excellent performances in many computer vision tasks. Specifically, for hyperspectral images (HSIs) classification, CNNs often require very complex structure due to the high dimension of HSIs. The complex structure of CNNs results in prohibitive training efforts. Moreover, the common situation in HSIs classification task is the lack of labeled samples, which results in accuracy deterioration of CNNs. In this work, we develop an easy-to-implement capsule network to alleviate the aforementioned problems, i.e., 1D-convolution capsule network (1D-ConvCapsNet). Firstly, 1D-ConvCapsNet separately extracts spatial and spectral information on spatial and spectral domains, which is more lightweight than 3D-convolution due to fewer parameters. Secondly, 1D-ConvCapsNet utilizes the capsule-wise constraint window method to reduce parameter amount and computational complexity of conventional capsule network. Finally, 1D-ConvCapsNet obtains accurate predictions with respect to input samples via dynamic routing. The effectiveness of the 1D-ConvCapsNet is verified by three representative HSI datasets. Experimental results demonstrate that 1D-ConvCapsNet is superior to state-of-the-art methods in both the accuracy and training effort.



### Detecting Curve Text with Local Segmentation Network and Curve Connection
- **Arxiv ID**: http://arxiv.org/abs/1903.09837v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09837v2)
- **Published**: 2019-03-23 15:46:34+00:00
- **Updated**: 2021-03-22 06:42:11+00:00
- **Authors**: Zhao Zhou, Hao Ye, Luhui Chen, Yingbin Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Curve text or arbitrary shape text is very common in real-world scenarios. In this paper, we propose a novel framework with the local segmentation network (LSN) followed by the curve connection to detect text in horizontal, oriented and curved forms. The LSN is composed of two elements, i.e., proposal generation to get the horizontal rectangle proposals with high overlap with text and text segmentation to find the arbitrary shape text region within proposals. The curve connection is then designed to connect the local mask to the detection results. We conduct experiments using the proposed framework on two real-world curve text detection datasets and demonstrate the effectiveness over previous approaches.



### Rotated Feature Network for multi-orientation object detection
- **Arxiv ID**: http://arxiv.org/abs/1903.09839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09839v2)
- **Published**: 2019-03-23 16:10:11+00:00
- **Updated**: 2019-03-27 03:06:33+00:00
- **Authors**: Zhixin Zhang, Xudong Chen, Jie Liu, Kaibo Zhou
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: General detectors follow the pipeline that feature maps extracted from ConvNets are shared between classification and regression tasks. However, there exists obvious conflicting requirements in multi-orientation object detection that classification is insensitive to orientations, while regression is quite sensitive. To address this issue, we provide an Encoder-Decoder architecture, called Rotated Feature Network (RFN), which produces rotation-sensitive feature maps (RS) for regression and rotation-invariant feature maps (RI) for classification. Specifically, the Encoder unit assigns weights for rotated feature maps. The Decoder unit extracts RS and RI by performing resuming operator on rotated and reweighed feature maps, respectively. To make the rotation-invariant characteristics more reliable, we adopt a metric to quantitatively evaluate the rotation-invariance by adding a constrain item in the loss, yielding a promising detection performance. Compared with the state-of-the-art methods, our method can achieve significant improvement on NWPU VHR-10 and RSOD datasets. We further evaluate the RFN on the scene classification in remote sensing images and object detection in natural images, demonstrating its good generalization ability. The proposed RFN can be integrated into an existing framework, leading to great performance with only a slight increase in model complexity.



### Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1903.09847v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09847v4)
- **Published**: 2019-03-23 17:23:44+00:00
- **Updated**: 2019-08-31 00:47:13+00:00
- **Authors**: Xinshuo Weng, Kris Kitani
- **Comment**: Camera Ready for ICCV Workshop on "Road Scene Understanding and
  Autonomous Driving"
- **Journal**: None
- **Summary**: Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D_PLiDAR.



### StartNet: Online Detection of Action Start in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.09868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09868v1)
- **Published**: 2019-03-23 19:14:53+00:00
- **Updated**: 2019-03-23 19:14:53+00:00
- **Authors**: Mingfei Gao, Mingze Xu, Larry S. Davis, Richard Socher, Caiming Xiong
- **Comment**: None
- **Journal**: None
- **Summary**: We propose StartNet to address Online Detection of Action Start (ODAS) where action starts and their associated categories are detected in untrimmed, streaming videos. Previous methods aim to localize action starts by learning feature representations that can directly separate the start point from its preceding background. It is challenging due to the subtle appearance difference near the action starts and the lack of training data. Instead, StartNet decomposes ODAS into two stages: action classification (using ClsNet) and start point localization (using LocNet). ClsNet focuses on per-frame labeling and predicts action score distributions online. Based on the predicted action scores of the past and current frames, LocNet conducts class-agnostic start detection by optimizing long-term localization rewards using policy gradient methods. The proposed framework is validated on two large-scale datasets, THUMOS'14 and ActivityNet. The experimental results show that StartNet significantly outperforms the state-of-the-art by 15%-30% p-mAP under the offset tolerance of 1-10 seconds on THUMOS'14, and achieves comparable performance on ActivityNet with 10 times smaller time offset.



### Long Range Neural Navigation Policies for the Real World
- **Arxiv ID**: http://arxiv.org/abs/1903.09870v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.09870v2)
- **Published**: 2019-03-23 19:36:11+00:00
- **Updated**: 2019-08-28 19:32:23+00:00
- **Authors**: Ayzaan Wahid, Alexander Toshev, Marek Fiser, Tsang-Wei Edward Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies -- a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving $0.80$ success rate over long navigation runs and outperforming SLAM-based models in the same settings.



### Automated pulmonary nodule detection using 3D deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1903.09876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1903.09876v1)
- **Published**: 2019-03-23 20:20:15+00:00
- **Updated**: 2019-03-23 20:20:15+00:00
- **Authors**: Hao Tang, Daniel R. Kim, Xiaohui Xie
- **Comment**: 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
  2018)
- **Journal**: None
- **Summary**: Early detection of pulmonary nodules in computed tomography (CT) images is essential for successful outcomes among lung cancer patients. Much attention has been given to deep convolutional neural network (DCNN)-based approaches to this task, but models have relied at least partly on 2D or 2.5D components for inherently 3D data. In this paper, we introduce a novel DCNN approach, consisting of two stages, that is fully three-dimensional end-to-end and utilizes the state-of-the-art in object detection. First, nodule candidates are identified with a U-Net-inspired 3D Faster R-CNN trained using online hard negative mining. Second, false positive reduction is performed by 3D DCNN classifiers trained on difficult examples produced during candidate screening. Finally, we introduce a method to ensemble models from both stages via consensus to give the final predictions. By using this framework, we ranked first of 2887 teams in Season One of Alibaba's 2017 TianChi AI Competition for Healthcare.



### Automatic Pulmonary Lobe Segmentation Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.09879v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09879v3)
- **Published**: 2019-03-23 20:31:45+00:00
- **Updated**: 2019-04-10 06:41:19+00:00
- **Authors**: Hao Tang, Chupeng Zhang, Xiaohui Xie
- **Comment**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Journal**: None
- **Summary**: Pulmonary lobe segmentation is an important task for pulmonary disease related Computer Aided Diagnosis systems (CADs). Classical methods for lobe segmentation rely on successful detection of fissures and other anatomical information such as the location of blood vessels and airways. With the success of deep learning in recent years, Deep Convolutional Neural Network (DCNN) has been widely applied to analyze medical images like Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), which, however, requires a large number of ground truth annotations. In this work, we release our manually labeled 50 CT scans which are randomly chosen from the LUNA16 dataset and explore the use of deep learning on this task. We propose pre-processing CT image by cropping region that is covered by the convex hull of the lungs in order to mitigate the influence of noise from outside the lungs. Moreover, we design a hybrid loss function with dice loss to tackle extreme class imbalance issue and focal loss to force model to focus on voxels that are hard to be discriminated. To validate the robustness and performance of our proposed framework trained with a small number of training examples, we further tested our model on CT scans from an independent dataset. Experimental results show the robustness of the proposed approach, which consistently improves performance across different datasets by a maximum of $5.87\%$ as compared to a baseline model.



### An End-to-end Framework For Integrated Pulmonary Nodule Detection and False Positive Reduction
- **Arxiv ID**: http://arxiv.org/abs/1903.09880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09880v1)
- **Published**: 2019-03-23 20:38:51+00:00
- **Updated**: 2019-03-23 20:38:51+00:00
- **Authors**: Hao Tang, Xingwei Liu, Xiaohui Xie
- **Comment**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Journal**: None
- **Summary**: Pulmonary nodule detection using low-dose Computed Tomography (CT) is often the first step in lung disease screening and diagnosis. Recently, algorithms based on deep convolutional neural nets have shown great promise for automated nodule detection. Most of the existing deep learning nodule detection systems are constructed in two steps: a) nodule candidates screening and b) false positive reduction, using two different models trained separately. Although it is commonly adopted, the two-step approach not only imposes significant resource overhead on training two independent deep learning models, but also is sub-optimal because it prevents cross-talk between the two. In this work, we present an end-to-end framework for nodule detection, integrating nodule candidate screening and false positive reduction into one model, trained jointly. We demonstrate that the end-to-end system improves the performance by 3.88\% over the two-step approach, while at the same time reducing model complexity by one third and cutting inference time by 3.6 fold. Code will be made publicly available.



### AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations
- **Arxiv ID**: http://arxiv.org/abs/1903.10863v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.10863v3)
- **Published**: 2019-03-23 21:31:10+00:00
- **Updated**: 2019-06-25 04:17:15+00:00
- **Authors**: Guo-Jun Qi, Liheng Zhang, Chang Wen Chen, Qi Tian
- **Comment**: http://maple-lab.net/projects/AVT.htm. arXiv admin note: text overlap
  with arXiv:1901.04596
- **Journal**: in Proceedings of International Conference in Computer Vision
  (ICCV 2019), Seoul, Kore, Oct. 27 -- Nov. 2, 2019
- **Summary**: The learning of Transformation-Equivariant Representations (TERs), which is introduced by Hinton et al. \cite{hinton2011transforming}, has been considered as a principle to reveal visual structures under various transformations. It contains the celebrated Convolutional Neural Networks (CNNs) as a special case that only equivary to the translations. In contrast, we seek to train TERs for a generic class of transformations and train them in an {\em unsupervised} fashion. To this end, we present a novel principled method by Autoencoding Variational Transformations (AVT), compared with the conventional approach to autoencoding data. Formally, given transformed images, the AVT seeks to train the networks by maximizing the mutual information between the transformations and representations. This ensures the resultant TERs of individual images contain the {\em intrinsic} information about their visual structures that would equivary {\em extricably} under various transformations in a generalized {\em nonlinear} case. Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models.



### DRASIC: Distributed Recurrent Autoencoder for Scalable Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1903.09887v3
- **DOI**: 10.1109/DCC47342.2020.00008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09887v3)
- **Published**: 2019-03-23 21:42:17+00:00
- **Updated**: 2019-12-28 01:51:24+00:00
- **Authors**: Enmao Diao, Jie Ding, Vahid Tarokh
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new architecture for distributed image compression from a group of distributed data sources. The work is motivated by practical needs of data-driven codec design, low power consumption, robustness, and data privacy. The proposed architecture, which we refer to as Distributed Recurrent Autoencoder for Scalable Image Compression (DRASIC), is able to train distributed encoders and one joint decoder on correlated data sources. Its compression capability is much better than the method of training codecs separately. Meanwhile, the performance of our distributed system with 10 distributed sources is only within 2 dB peak signal-to-noise ratio (PSNR) of the performance of a single codec trained with all data sources. We experiment distributed sources with different correlations and show how our data-driven methodology well matches the Slepian-Wolf Theorem in Distributed Source Coding (DSC). To the best of our knowledge, this is the first data-driven DSC framework for general distributed code design with deep learning.



### sharpDARTS: Faster and More Accurate Differentiable Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1903.09900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09900v1)
- **Published**: 2019-03-23 23:20:44+00:00
- **Updated**: 2019-03-23 23:20:44+00:00
- **Authors**: Andrew Hundt, Varun Jain, Gregory D. Hager
- **Comment**: 9 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has been a source of dramatic improvements in neural network design, with recent results meeting or exceeding the performance of hand-tuned architectures. However, our understanding of how to represent the search space for neural net architectures and how to search that space efficiently are both still in their infancy.   We have performed an in-depth analysis to identify limitations in a widely used search space and a recent architecture search method, Differentiable Architecture Search (DARTS). These findings led us to introduce novel network blocks with a more general, balanced, and consistent design; a better-optimized Cosine Power Annealing learning rate schedule; and other improvements. Our resulting sharpDARTS search is 50% faster with a 20-30% relative improvement in final model error on CIFAR-10 when compared to DARTS. Our best single model run has 1.93% (1.98+/-0.07) validation error on CIFAR-10 and 5.5% error (5.8+/-0.3) on the recently released CIFAR-10.1 test set. To our knowledge, both are state of the art for models of similar size. This model also generalizes competitively to ImageNet at 25.1% top-1 (7.8% top-5) error.   We found improvements for existing search spaces but does DARTS generalize to new domains? We propose Differentiable Hyperparameter Grid Search and the HyperCuboid search space, which are representations designed to leverage DARTS for more general parameter optimization. Here we find that DARTS fails to generalize when compared against a human's one shot choice of models. We look back to the DARTS and sharpDARTS search spaces to understand why, and an ablation study reveals an unusual generalization gap. We finally propose Max-W regularization to solve this problem, which proves significantly better than the handmade design. Code will be made available.



