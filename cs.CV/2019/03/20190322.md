# Arxiv Papers in cs.CV on 2019-03-22
### Towards Optimal Structured CNN Pruning via Generative Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.09291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09291v1)
- **Published**: 2019-03-22 01:26:33+00:00
- **Updated**: 2019-03-22 01:26:33+00:00
- **Authors**: Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, David Doermann
- **Comment**: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Journal**: None
- **Summary**: Structured pruning of filters or neurons has received increased focus for compressing convolutional neural networks. Most existing methods rely on multi-stage optimizations in a layer-wise manner for iteratively pruning and retraining which may not be optimal and may be computation intensive. Besides, these methods are designed for pruning a specific structure, such as filter or block structures without jointly pruning heterogeneous structures. In this paper, we propose an effective structured pruning approach that jointly prunes filters as well as other structures in an end-to-end manner. To accomplish this, we first introduce a soft mask to scale the output of these structures by defining a new objective function with sparsity regularization to align the output of baseline and network with this mask. We then effectively solve the optimization problem by generative adversarial learning (GAL), which learns a sparse soft mask in a label-free and an end-to-end manner. By forcing more scaling factors in the soft mask to zero, the fast iterative shrinkage-thresholding algorithm (FISTA) can be leveraged to fast and reliably remove the corresponding structures. Extensive experiments demonstrate the effectiveness of GAL on different datasets, including MNIST, CIFAR-10 and ImageNet ILSVRC 2012. For example, on ImageNet ILSVRC 2012, the pruned ResNet-50 achieves 10.88\% Top-5 error and results in a factor of 3.7x speedup. This significantly outperforms state-of-the-art methods.



### A resnet-based universal method for speckle reduction in optical coherence tomography images
- **Arxiv ID**: http://arxiv.org/abs/1903.09330v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.09330v1)
- **Published**: 2019-03-22 02:55:31+00:00
- **Updated**: 2019-03-22 02:55:31+00:00
- **Authors**: Cai Ning, Shi Fei, Hu Dianlin, Chen Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a ResNet-based universal method for speckle reduction in optical coherence tomography (OCT) images. The proposed model contains 3 main modules: Convolution-BN-ReLU, Branch and Residual module. Unlike traditional algorithms, the model can learn from training data instead of selecting parameters manually such as noise level. Application of this proposed method to the OCT images shows a more than 22 dB signal-to-noise ratio improvement in speckle noise reduction with minimal structure blurring. The proposed method provides strong generalization ability and can process noisy other types of OCT images without retraining. It outperforms other filtering methods in suppressing speckle noises and revealing subtle features.



### Unsupervised Deformable Registration for Multi-Modal Images via Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1903.09331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09331v1)
- **Published**: 2019-03-22 03:01:54+00:00
- **Updated**: 2019-03-22 03:01:54+00:00
- **Authors**: Chen Qin, Bibo Shi, Rui Liao, Tommaso Mansi, Daniel Rueckert, Ali Kamen
- **Comment**: Accepted as an oral presentation in IPMI 2019
- **Journal**: None
- **Summary**: We propose a fully unsupervised multi-modal deformable image registration method (UMDIR), which does not require any ground truth deformation fields or any aligned multi-modal image pairs during training. Multi-modal registration is a key problem in many medical image analysis applications. It is very challenging due to complicated and unknown relationships between different modalities. In this paper, we propose an unsupervised learning approach to reduce the multi-modal registration problem to a mono-modal one through image disentangling. In particular, we decompose images of both modalities into a common latent shape space and separate latent appearance spaces via an unsupervised multi-modal image-to-image translation approach. The proposed registration approach is then built on the factorized latent shape code, with the assumption that the intrinsic shape deformation existing in original image domain is preserved in this latent space. Specifically, two metrics have been proposed for training the proposed network: a latent similarity metric defined in the common shape space and a learningbased image similarity metric based on an adversarial loss. We examined different variations of our proposed approach and compared them with conventional state-of-the-art multi-modal registration methods. Results show that our proposed methods achieve competitive performance against other methods at substantially reduced computation time.



### Pose Estimation of Periacetabular Osteotomy Fragments with Intraoperative X-Ray Navigation
- **Arxiv ID**: http://arxiv.org/abs/1903.09339v2
- **DOI**: 10.1109/TBME.2019.2915165
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.09339v2)
- **Published**: 2019-03-22 03:21:15+00:00
- **Updated**: 2019-05-09 22:23:34+00:00
- **Authors**: Robert B. Grupp, Rachel A. Hegeman, Ryan J. Murphy, Clayton P. Alexander, Yoshito Otake, Benjamin A. McArthur, Mehran Armand, Russell H. Taylor
- **Comment**: Accepted for publication in IEEE Transactions on Biomedical
  Engineering
- **Journal**: IEEE Transactions on Biomedical Engineering, vol. 67, no. 2, pp.
  441-452, Feb. 2020
- **Summary**: Objective: State of the art navigation systems for pelvic osteotomies use optical systems with external fiducials. We propose the use of X-Ray navigation for pose estimation of periacetabular fragments without fiducials. Methods: A 2D/3D registration pipeline was developed to recover fragment pose. This pipeline was tested through an extensive simulation study and 6 cadaveric surgeries. Using osteotomy boundaries in the fluoroscopic images, the preoperative plan is refined to more accurately match the intraoperative shape. Results: In simulation, average fragment pose errors were 1.3{\deg}/1.7 mm when the planned fragment matched the intraoperative fragment, 2.2{\deg}/2.1 mm when the plan was not updated to match the true shape, and 1.9{\deg}/2.0 mm when the fragment shape was intraoperatively estimated. In cadaver experiments, the average pose errors were 2.2{\deg}/2.2 mm, 3.8{\deg}/2.5 mm, and 3.5{\deg}/2.2 mm when registering with the actual fragment shape, a preoperative plan, and an intraoperatively refined plan, respectively. Average errors of the lateral center edge angle were less than 2{\deg} for all fragment shapes in simulation and cadaver experiments. Conclusion: The proposed pipeline is capable of accurately reporting femoral head coverage within a range clinically identified for long-term joint survivability. Significance: Human interpretation of fragment pose is challenging and usually restricted to rotation about a single anatomical axis. The proposed pipeline provides an intraoperative estimate of rigid pose with respect to all anatomical axes, is compatible with minimally invasive incisions, and has no dependence on external fiducials.



### Overcoming Small Minirhizotron Datasets Using Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.09344v3
- **DOI**: 10.1016/j.compag.2020.105466
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09344v3)
- **Published**: 2019-03-22 03:40:25+00:00
- **Updated**: 2020-04-24 15:13:48+00:00
- **Authors**: Weihuang Xu, Guohao Yu, Alina Zare, Brendan Zurweller, Diane Rowland, Joel Reyes-Cabrera, Felix B Fritschi, Roser Matamala, Thomas E. Juenger
- **Comment**: None
- **Journal**: Computers and Electronics in Agriculture, 175 (2020)
- **Summary**: Minirhizotron technology is widely used for studying the development of roots. Such systems collect visible-wavelength color imagery of plant roots in-situ by scanning an imaging system within a clear tube driven into the soil. Automated analysis of root systems could facilitate new scientific discoveries that would be critical to address the world's pressing food, resource, and climate issues. A key component of automated analysis of plant roots from imagery is the automated pixel-level segmentation of roots from their surrounding soil. Supervised learning techniques appear to be an appropriate tool for the challenge due to varying local soil and root conditions, however, lack of enough annotated training data is a major limitation due to the error-prone and time-consuming manually labeling process. In this paper, we investigate the use of deep neural networks based on the U-net architecture for automated, precise pixel-wise root segmentation in minirhizotron imagery. We compiled two minirhizotron image datasets to accomplish this study: one with 17,550 peanut root images and another with 28 switchgrass root images. Both datasets were paired with manually labeled ground truth masks. We trained three neural networks with different architectures on the larger peanut root dataset to explore the effect of the neural network depth on segmentation performance. To tackle the more limited switchgrass root dataset, we showed that models initialized with features pre-trained on the peanut dataset and then fine-tuned on the switchgrass dataset can improve segmentation performance significantly. We obtained 99\% segmentation accuracy in switchgrass imagery using only 21 training images. We also observed that features pre-trained on a closely related but relatively moderate size dataset like our peanut dataset are more effective than features pre-trained on the large but unrelated ImageNet dataset.



### 3D Face Reconstruction from A Single Image Assisted by 2D Face Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1903.09359v2
- **DOI**: 10.1109/TMM.2020.2993962
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09359v2)
- **Published**: 2019-03-22 05:06:13+00:00
- **Updated**: 2020-05-03 01:41:03+00:00
- **Authors**: Xiaoguang Tu, Jian Zhao, Zihang Jiang, Yao Luo, Mei Xie, Yang Zhao, Linxiao He, Zheng Ma, Jiashi Feng
- **Comment**: 10 pages, 7figures, 3 tables
- **Journal**: None
- **Summary**: 3D face reconstruction from a single 2D image is a challenging problem with broad applications. Recent methods typically aim to learn a CNN-based 3D face model that regresses coefficients of 3D Morphable Model (3DMM) from 2D images to render 3D face reconstruction or dense face alignment. However, the shortage of training data with 3D annotations considerably limits performance of those methods. To alleviate this issue, we propose a novel 2D-assisted self-supervised learning (2DASL) method that can effectively use "in-the-wild" 2D face images with noisy landmark information to substantially improve 3D face model learning. Specifically, taking the sparse 2D facial landmarks as additional information, 2DSAL introduces four novel self-supervision schemes that view the 2D landmark and 3D landmark prediction as a self-mapping process, including the 2D and 3D landmark self-prediction consistency, cycle-consistency over the 2D landmark prediction and self-critic over the predicted 3DMM coefficients based on landmark predictions. Using these four self-supervision schemes, the 2DASL method significantly relieves demands on the the conventional paired 2D-to-3D annotations and gives much higher-quality 3D face models without requiring any additional 3D annotations. Experiments on multiple challenging datasets show that our method outperforms state-of-the-arts for both 3D face reconstruction and dense face alignment by a large margin.



### Few-shot Adaptive Faster R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1903.09372v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09372v1)
- **Published**: 2019-03-22 06:32:12+00:00
- **Updated**: 2019-03-22 06:32:12+00:00
- **Authors**: Tao Wang, Xiaopeng Zhang, Li Yuan, Jiashi Feng
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: To mitigate the detection performance drop caused by domain shift, we aim to develop a novel few-shot adaptation approach that requires only a few target domain images with limited bounding box annotations. To this end, we first observe several significant challenges. First, the target domain data is highly insufficient, making most existing domain adaptation methods ineffective. Second, object detection involves simultaneous localization and classification, further complicating the model adaptation process. Third, the model suffers from over-adaptation (similar to overfitting when training with a few data example) and instability risk that may lead to degraded detection performance in the target domain. To address these challenges, we first introduce a pairing mechanism over source and target features to alleviate the issue of insufficient target domain samples. We then propose a bi-level module to adapt the source trained detector to the target domain: 1) the split pooling based image level adaptation module uniformly extracts and aligns paired local patch features over locations, with different scale and aspect ratio; 2) the instance level adaptation module semantically aligns paired object features while avoids inter-class confusion. Meanwhile, a source model feature regularization (SMFR) is applied to stabilize the adaptation process of the two modules. Combining these contributions gives a novel few-shot adaptive Faster-RCNN framework, termed FAFRCNN, which effectively adapts to target domain with a few labeled samples. Experiments with multiple datasets show that our model achieves new state-of-the-art performance under both the interested few-shot domain adaptation(FDA) and unsupervised domain adaptation(UDA) setting.



### Fast Bayesian Uncertainty Estimation and Reduction of Batch Normalized Single Image Super-Resolution Network
- **Arxiv ID**: http://arxiv.org/abs/1903.09410v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09410v3)
- **Published**: 2019-03-22 09:05:32+00:00
- **Updated**: 2021-05-19 04:53:43+00:00
- **Authors**: Aupendu Kar, Prabir Kumar Biswas
- **Comment**: To appear in the Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR 2021)
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) has achieved unprecedented success in image super-resolution tasks in recent years. However, the network's performance depends on the distribution of the training sets and degrades on out-of-distribution samples. This paper adopts a Bayesian approach for estimating uncertainty associated with output and applies it in a deep image super-resolution model to address the concern mentioned above. We use the uncertainty estimation technique using the batch-normalization layer, where stochasticity of the batch mean and variance generate Monte-Carlo (MC) samples. The MC samples, which are nothing but different super-resolved images using different stochastic parameters, reconstruct the image, and provide a confidence or uncertainty map of the reconstruction. We propose a faster approach for MC sample generation, and it allows the variable image size during testing. Therefore, it will be useful for image reconstruction domain. Our experimental findings show that this uncertainty map strongly relates to the quality of reconstruction generated by the deep CNN model and explains its limitation. Furthermore, this paper proposes an approach to reduce the model's uncertainty for an input image, and it helps to defend the adversarial attacks on the image super-resolution model. The proposed uncertainty reduction technique also improves the performance of the model for out-of-distribution test images. To the best of our knowledge, we are the first to propose an adversarial defense mechanism in any image reconstruction domain.



### Disentangled Representation Learning in Cardiac Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1903.09467v4
- **DOI**: 10.1016/j.media.2019.101535
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09467v4)
- **Published**: 2019-03-22 12:08:08+00:00
- **Updated**: 2019-09-16 10:59:29+00:00
- **Authors**: Agisilaos Chartsias, Thomas Joyce, Giorgos Papanastasiou, Michelle Williams, David Newby, Rohan Dharmakumar, Sotirios A. Tsaftaris
- **Comment**: None
- **Journal**: Medical Image Analysis 58 (2019) 101535
- **Summary**: Typically, a medical image offers spatial information on the anatomy (and pathology) modulated by imaging specific characteristics. Many imaging modalities including Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) can be interpreted in this way. We can venture further and consider that a medical image naturally factors into some spatial factors depicting anatomy and factors that denote the imaging characteristics. Here, we explicitly learn this decomposed (disentangled) representation of imaging data, focusing in particular on cardiac images. We propose Spatial Decomposition Network (SDNet), which factorises 2D medical images into spatial anatomical factors and non-spatial modality factors. We demonstrate that this high-level representation is ideally suited for several medical image analysis tasks, such as semi-supervised segmentation, multi-task segmentation and regression, and image-to-image synthesis. Specifically, we show that our model can match the performance of fully supervised segmentation models, using only a fraction of the labelled images. Critically, we show that our factorised representation also benefits from supervision obtained either when we use auxiliary tasks to train the model in a multi-task setting (e.g. regressing to known cardiac indices), or when aggregating multimodal data from different sources (e.g. pooling together MRI and CT data). To explore the properties of the learned factorisation, we perform latent-space arithmetic and show that we can synthesise CT from MR and vice versa, by swapping the modality factors. We also demonstrate that the factor holding image specific information can be used to predict the input modality with high accuracy. Code will be made available at https://github.com/agis85/anatomy_modality_decomposition.



### Aggregated Deep Local Features for Remote Sensing Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.09469v1
- **DOI**: 10.3390/rs11050493
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09469v1)
- **Published**: 2019-03-22 12:17:04+00:00
- **Updated**: 2019-03-22 12:17:04+00:00
- **Authors**: Raffaele Imbriaco, Clint Sebastian, Egor Bondarev, Peter H. N. de With
- **Comment**: Published in Remote Sensing. The first two authors have equal
  contribution
- **Journal**: Remote Sensing, 2019
- **Summary**: Remote Sensing Image Retrieval remains a challenging topic due to the special nature of Remote Sensing Imagery. Such images contain various different semantic objects, which clearly complicates the retrieval task. In this paper, we present an image retrieval pipeline that uses attentive, local convolutional features and aggregates them using the Vector of Locally Aggregated Descriptors (VLAD) to produce a global descriptor. We study various system parameters such as the multiplicative and additive attention mechanisms and descriptor dimensionality. We propose a query expansion method that requires no external inputs. Experiments demonstrate that even without training, the local convolutional features and global representation outperform other systems. After system tuning, we can achieve state-of-the-art or competitive results. Furthermore, we observe that our query expansion method increases overall system performance by about 3%, using only the top-three retrieved images. Finally, we show how dimensionality reduction produces compact descriptors with increased retrieval performance and fast retrieval computation times, e.g. 50% faster than the current systems.



### Evaluation of a deep learning system for the joint automated detection of diabetic retinopathy and age-related macular degeneration
- **Arxiv ID**: http://arxiv.org/abs/1903.09555v1
- **DOI**: 10.1111/aos.14306
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09555v1)
- **Published**: 2019-03-22 15:27:18+00:00
- **Updated**: 2019-03-22 15:27:18+00:00
- **Authors**: Cristina González-Gonzalo, Verónica Sánchez-Gutiérrez, Paula Hernández-Martínez, Inés Contreras, Yara T. Lechanteur, Artin Domanian, Bram van Ginneken, Clara I. Sánchez
- **Comment**: None
- **Journal**: Acta Ophthalmologica. First published 26 November 2019
- **Summary**: Purpose: To validate the performance of a commercially-available, CE-certified deep learning (DL) system, RetCAD v.1.3.0 (Thirona, Nijmegen, The Netherlands), for the joint automatic detection of diabetic retinopathy (DR) and age-related macular degeneration (AMD) in color fundus (CF) images on a dataset with mixed presence of eye diseases.   Methods: Evaluation of joint detection of referable DR and AMD was performed on a DR-AMD dataset with 600 images acquired during routine clinical practice, containing referable and non-referable cases of both diseases. Each image was graded for DR and AMD by an experienced ophthalmologist to establish the reference standard (RS), and by four independent observers for comparison with human performance. Validation was furtherly assessed on Messidor (1200 images) for individual identification of referable DR, and the Age-Related Eye Disease Study (AREDS) dataset (133821 images) for referable AMD, against the corresponding RS.   Results: Regarding joint validation on the DR-AMD dataset, the system achieved an area under the ROC curve (AUC) of 95.1% for detection of referable DR (SE=90.1%, SP=90.6%). For referable AMD, the AUC was 94.9% (SE=91.8%, SP=87.5%). Average human performance for DR was SE=61.5% and SP=97.8%; for AMD, SE=76.5% and SP=96.1%. Regarding detection of referable DR in Messidor, AUC was 97.5% (SE=92.0%, SP=92.1%); for referable AMD in AREDS, AUC was 92.7% (SE=85.8%, SP=86.0%).   Conclusions: The validated system performs comparably to human experts at simultaneous detection of DR and AMD. This shows that DL systems can facilitate access to joint screening of eye diseases and become a quick and reliable support for ophthalmological experts.



### On the Importance of Video Action Recognition for Visual Lipreading
- **Arxiv ID**: http://arxiv.org/abs/1903.09616v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09616v2)
- **Published**: 2019-03-22 17:24:37+00:00
- **Updated**: 2019-09-16 15:32:15+00:00
- **Authors**: Xinshuo Weng
- **Comment**: This paper is withdrawn by the author due to errors and there will be
  no replacement in this thread
- **Journal**: None
- **Summary**: We focus on the word-level visual lipreading, which requires to decode the word from the speaker's video. Recently, many state-of-the-art visual lipreading methods explore the end-to-end trainable deep models, involving the use of 2D convolutional networks (e.g., ResNet) as the front-end visual feature extractor and the sequential model (e.g., Bi-LSTM or Bi-GRU) as the back-end. Although a deep 2D convolution neural network can provide informative image-based features, it ignores the temporal motion existing between the adjacent frames. In this work, we investigate the spatial-temporal capacity power of I3D (Inflated 3D ConvNet) for visual lipreading. We demonstrate that, after pre-trained on the large-scale video action recognition dataset (e.g., Kinetics), our models show a considerable improvement of performance on the task of lipreading. A comparison between a set of video model architectures and input data representation is also reported. Our extensive experiments on LRW shows that a two-stream I3D model with RGB video and optical flow as the inputs achieves the state-of-the-art performance.



### Capsule Networks with Max-Min Normalization
- **Arxiv ID**: http://arxiv.org/abs/1903.09662v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.09662v1)
- **Published**: 2019-03-22 18:09:37+00:00
- **Updated**: 2019-03-22 18:09:37+00:00
- **Authors**: Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P. Unnikrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule Networks (CapsNet) use the Softmax function to convert the logits of the routing coefficients into a set of normalized values that signify the assignment probabilities between capsules in adjacent layers. We show that the use of Softmax prevents capsule layers from forming optimal couplings between lower and higher-level capsules. Softmax constrains the dynamic range of the routing coefficients and leads to probabilities that remain mostly uniform after several routing iterations. Instead, we propose the use of Max-Min normalization. Max-Min performs a scale-invariant normalization of the logits that allows each lower-level capsule to take on an independent value, constrained only by the bounds of normalization. Max-Min provides consistent improvement in test accuracy across five datasets and allows more routing iterations without a decrease in network performance. A single CapsNet trained using Max-Min achieves an improved test error of 0.20% on the MNIST dataset. With a simple 3-model majority vote, we achieve a test error of 0.17% on MNIST.



### Generative Adversarial Minority Oversampling
- **Arxiv ID**: http://arxiv.org/abs/1903.09730v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.09730v3)
- **Published**: 2019-03-22 23:13:14+00:00
- **Updated**: 2020-08-26 18:05:57+00:00
- **Authors**: Sankha Subhra Mullick, Shounak Datta, Swagatam Das
- **Comment**: Codes are available at https://github.com/SankhaSubhra/GAMO
- **Journal**: None
- **Summary**: Class imbalance is a long-standing problem relevant to a number of real-world applications of deep learning. Oversampling techniques, which are effective for handling class imbalance in classical learning systems, can not be directly applied to end-to-end deep learning systems. We propose a three-player adversarial game between a convex generator, a multi-class classifier network, and a real/fake discriminator to perform oversampling in deep learning systems. The convex generator generates new samples from the minority classes as convex combinations of existing instances, aiming to fool both the discriminator as well as the classifier into misclassifying the generated samples. Consequently, the artificial samples are generated at critical locations near the peripheries of the classes. This, in turn, adjusts the classifier induced boundaries in a way which is more likely to reduce misclassification from the minority classes. Extensive experiments on multiple class imbalanced image datasets establish the efficacy of our proposal.



