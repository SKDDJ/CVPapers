# Arxiv Papers in cs.CV on 2019-03-04
### Active Authentication using an Autoencoder regularized CNN-based One-Class Classifier
- **Arxiv ID**: http://arxiv.org/abs/1903.01031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01031v1)
- **Published**: 2019-03-04 01:20:15+00:00
- **Updated**: 2019-03-04 01:20:15+00:00
- **Authors**: Poojan Oza, Vishal M. Patel
- **Comment**: Accepted and to appear at AFGR 2019
- **Journal**: None
- **Summary**: Active authentication refers to the process in which users are unobtrusively monitored and authenticated continuously throughout their interactions with mobile devices. Generally, an active authentication problem is modelled as a one class classification problem due to the unavailability of data from the impostor users. Normally, the enrolled user is considered as the target class (genuine) and the unauthorized users are considered as unknown classes (impostor). We propose a convolutional neural network (CNN) based approach for one class classification in which a zero centered Gaussian noise and an autoencoder are used to model the pseudo-negative class and to regularize the network to learn meaningful feature representations for one class data, respectively. The overall network is trained using a combination of the cross-entropy and the reconstruction error losses. A key feature of the proposed approach is that any pre-trained CNN can be used as the base network for one class classification. Effectiveness of the proposed framework is demonstrated using three publically available face-based active authentication datasets and it is shown that the proposed method achieves superior performance compared to the traditional one class classification methods. The source code is available at: github.com/otkupjnoz/oc-acnn.



### Spatiotemporal Pyramid Network for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.01038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01038v1)
- **Published**: 2019-03-04 01:36:41+00:00
- **Updated**: 2019-03-04 01:36:41+00:00
- **Authors**: Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Two-stream convolutional networks have shown strong performance in video action recognition tasks. The key idea is to learn spatiotemporal features by fusing convolutional networks spatially and temporally. However, it remains unclear how to model the correlations between the spatial and temporal structures at multiple abstraction levels. First, the spatial stream tends to fail if two videos share similar backgrounds. Second, the temporal stream may be fooled if two actions resemble in short snippets, though appear to be distinct in the long term. We propose a novel spatiotemporal pyramid network to fuse the spatial and temporal features in a pyramid structure such that they can reinforce each other. From the architecture perspective, our network constitutes hierarchical fusion strategies which can be trained as a whole using a unified spatiotemporal loss. A series of ablation experiments support the importance of each fusion strategy. From the technical perspective, we introduce the spatiotemporal compact bilinear operator into video analysis tasks. This operator enables efficient training of bilinear fusion operations which can capture full interactions between the spatial and temporal features. Our final network achieves state-of-the-art results on standard video datasets.



### Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities
- **Arxiv ID**: http://arxiv.org/abs/1903.01067v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.01067v2)
- **Published**: 2019-03-04 04:24:50+00:00
- **Updated**: 2019-07-29 16:36:41+00:00
- **Authors**: Antoni Rosinol, Torsten Sattler, Marc Pollefeys, Luca Carlone
- **Comment**: 7 pages, 5 figures, ICRA accepted
- **Journal**: IEEE Int. Conf. Robot. Autom. (ICRA), 2019
- **Summary**: Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud representation of the scene that does not model the topology of the environment. A 3D mesh instead offers a richer, yet lightweight, model. Nevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks triangulated by a VIO algorithm often results in a mesh that does not fit the real scene. In order to regularize the mesh, previous approaches decouple state estimation from the 3D mesh regularization step, and either limit the 3D mesh to the current frame or let the mesh grow indefinitely. We propose instead to tightly couple mesh regularization and state estimation by detecting and enforcing structural regularities in a novel factor-graph formulation. We also propose to incrementally build the mesh by restricting its extent to the time-horizon of the VIO optimization; the resulting 3D mesh covers a larger portion of the scene than a per-frame approach while its memory usage and computational complexity remain bounded. We show that our approach successfully regularizes the mesh, while improving localization accuracy, when structural regularities are present, and remains operational in scenes without regularities.



### COMIC: Towards A Compact Image Captioning Model with Attention
- **Arxiv ID**: http://arxiv.org/abs/1903.01072v3
- **DOI**: 10.1109/TMM.2019.2904878
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01072v3)
- **Published**: 2019-03-04 05:09:16+00:00
- **Updated**: 2019-06-11 18:43:19+00:00
- **Authors**: Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah
- **Comment**: Added source code link and new results in Table 3
- **Journal**: None
- **Summary**: Recent works in image captioning have shown very promising raw performance. However, we realize that most of these encoder-decoder style networks with attention do not scale naturally to large vocabulary size, making them difficult to be deployed on embedded system with limited hardware resources. This is because the size of word and output embedding matrices grow proportionally with the size of vocabulary, adversely affecting the compactness of these networks. To address this limitation, this paper introduces a brand new idea in the domain of image captioning. That is, we tackle the problem of compactness of image captioning models which is hitherto unexplored. We showed that, our proposed model, named COMIC for COMpact Image Captioning, achieves comparable results in five common evaluation metrics with state-of-the-art approaches on both MS-COCO and InstaPIC-1.1M datasets despite having an embedding vocabulary size that is 39x - 99x smaller. The source code and models are available at: https://github.com/jiahuei/COMIC-Compact-Image-Captioning-with-Attention



### Unsupervised Cross-spectral Stereo Matching by Learning to Synthesize
- **Arxiv ID**: http://arxiv.org/abs/1903.01078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01078v1)
- **Published**: 2019-03-04 05:29:00+00:00
- **Updated**: 2019-03-04 05:29:00+00:00
- **Authors**: Mingyang Liang, Xiaoyang Guo, Hongsheng Li, Xiaogang Wang, You Song
- **Comment**: accepted by AAAI-19
- **Journal**: None
- **Summary**: Unsupervised cross-spectral stereo matching aims at recovering disparity given cross-spectral image pairs without any supervision in the form of ground truth disparity or depth. The estimated depth provides additional information complementary to individual semantic features, which can be helpful for other vision tasks such as tracking, recognition and detection. However, there are large appearance variations between images from different spectral bands, which is a challenge for cross-spectral stereo matching. Existing deep unsupervised stereo matching methods are sensitive to the appearance variations and do not perform well on cross-spectral data. We propose a novel unsupervised cross-spectral stereo matching framework based on image-to-image translation. First, a style adaptation network transforms images across different spectral bands by cycle consistency and adversarial learning, during which appearance variations are minimized. Then, a stereo matching network is trained with image pairs from the same spectra using view reconstruction loss. At last, the estimated disparity is utilized to supervise the spectral-translation network in an end-to-end way. Moreover, a novel style adaptation network F-cycleGAN is proposed to improve the robustness of spectral translation. Our method can tackle appearance variations and enhance the robustness of unsupervised cross-spectral stereo matching. Experimental results show that our method achieves good performance without using depth supervision or explicit semantic information.



### Automatic microscopic cell counting by use of deeply-supervised density regression model
- **Arxiv ID**: http://arxiv.org/abs/1903.01084v3
- **DOI**: 10.1117/12.2513045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01084v3)
- **Published**: 2019-03-04 05:57:43+00:00
- **Updated**: 2019-03-22 15:20:56+00:00
- **Authors**: Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Mark Anastasio, Hua Li
- **Comment**: SPIE medical imaging 2019 oral presentation
- **Journal**: None
- **Summary**: Accurately counting cells in microscopic images is important for medical diagnoses and biological studies, but manual cell counting is very tedious, time-consuming, and prone to subjective errors, and automatic counting can be less accurate than desired. To improve the accuracy of automatic cell counting, we propose here a novel method that employs deeply-supervised density regression. A fully convolutional neural network (FCNN) serves as the primary FCNN for density map regression. Innovatively, a set of auxiliary FCNNs are employed to provide additional supervision for learning the intermediate layers of the primary CNN to improve network performance. In addition, the primary CNN is designed as a concatenating framework to integrate multi-scale features through shortcut connections in the network, which improves the granularity of the features extracted from the intermediate CNN layers and further supports the final density map estimation.



### Unpaired image denoising using a generative adversarial network in X-ray CT
- **Arxiv ID**: http://arxiv.org/abs/1903.06257v2
- **DOI**: 10.1109/ACCESS.2019.2934178
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06257v2)
- **Published**: 2019-03-04 06:01:42+00:00
- **Updated**: 2019-08-08 08:33:53+00:00
- **Authors**: Hyoung Suk Park, Jineon Baek, Sun Kyoung You, Jae Kyu Choi, Jin Keun Seo
- **Comment**: None
- **Journal**: IEEE Access, 2019
- **Summary**: This paper proposes a deep learning-based denoising method for noisy low-dose computerized tomography (CT) images in the absence of paired training data. The proposed method uses a fidelity-embedded generative adversarial network (GAN) to learn a denoising function from unpaired training data of low-dose CT (LDCT) and standard-dose CT (SDCT) images, where the denoising function is the optimal generator in the GAN framework. This paper analyzes the f-GAN objective to derive a suitable generator that is optimized by minimizing a weighted sum of two losses: the Kullback-Leibler divergence between an SDCT data distribution and a generated distribution, and the $\ell_2$ loss between the LDCT image and the corresponding generated images (or denoised image). The computed generator reflects the prior belief about SDCT data distribution through training. We observed that the proposed method allows the preservation of fine anomalous features while eliminating noise. The experimental results show that the proposed deep-learning method with unpaired datasets performs comparably to a method using paired datasets. A clinical experiment was also performed to show the validity of the proposed method for noise arising in the low-dose X-ray CT.



### Zero-Shot Task Transfer
- **Arxiv ID**: http://arxiv.org/abs/1903.01092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01092v1)
- **Published**: 2019-03-04 07:02:42+00:00
- **Updated**: 2019-03-04 07:02:42+00:00
- **Authors**: Arghya Pal, Vineeth N Balasubramanian
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a novel meta-learning algorithm, i.e. TTNet, that regresses model parameters for novel tasks for which no ground truth is available (zero-shot tasks). In order to adapt to novel zero-shot tasks, our meta-learner learns from the model parameters of known tasks (with ground truth) and the correlation of known tasks to zero-shot tasks. Such intuition finds its foothold in cognitive science, where a subject (human baby) can adapt to a novel-concept (depth understanding) by correlating it with old concepts (hand movement or self-motion), without receiving explicit supervision. We evaluated our model on the Taskonomy dataset, with four tasks as zero-shot: surface-normal, room layout, depth, and camera pose estimation. These tasks were chosen based on the data acquisition complexity and the complexity associated with the learning process using a deep network. Our proposed methodology out-performs state-of-the-art models (which use ground truth)on each of our zero-shot tasks, showing promise on zero-shot task transfer. We also conducted extensive experiments to study the various choices of our methodology, as well as showed how the proposed method can also be used in transfer learning. To the best of our knowledge, this is the firstsuch effort on zero-shot learning in the task space.



### Hyperspectral Image Classification with Deep Metric Learning and Conditional Random Field
- **Arxiv ID**: http://arxiv.org/abs/1903.06258v2
- **DOI**: 10.1109/LGRS.2019.2939356
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06258v2)
- **Published**: 2019-03-04 09:26:03+00:00
- **Updated**: 2019-07-16 02:30:41+00:00
- **Authors**: Yi Liang, Xin Zhao, Alan J. X. Guo, Fei Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the classification performance in the context of hyperspectral image processing, many works have been developed based on two common strategies, namely the spatial-spectral information integration and the utilization of neural networks. However, both strategies typically require more training data than the classical algorithms, aggregating the shortage of labeled samples. In this letter, we propose a novel framework that organically combines the spectrum-based deep metric learning model and the conditional random field algorithm. The deep metric learning model is supervised by the center loss to produce spectrum-based features that gather more tightly in Euclidean space within classes. The conditional random field with Gaussian edge potentials, which is firstly proposed for image segmentation tasks, is introduced to give the pixel-wise classification over the hyperspectral image by utilizing both the geographical distances between pixels and the Euclidean distances between the features produced by the deep metric learning model. The proposed framework is trained by spectral pixels at the deep metric learning stage and utilizes the half handcrafted spatial features at the conditional random field stage. This settlement alleviates the shortage of training data to some extent. Experiments on two real hyperspectral images demonstrate the advantages of the proposed method in terms of both classification accuracy and computation cost.



### PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things
- **Arxiv ID**: http://arxiv.org/abs/1903.01177v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.01177v2)
- **Published**: 2019-03-04 11:27:48+00:00
- **Updated**: 2019-09-09 07:59:08+00:00
- **Authors**: Gaku Narita, Takashi Seno, Tomoya Ishikawa, Yohsuke Kaji
- **Comment**: 8 pages, 6 figures, Accepted to 2019 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
- **Journal**: None
- **Summary**: We propose PanopticFusion, a novel online volumetric semantic mapping system at the level of stuff and things. In contrast to previous semantic mapping systems, PanopticFusion is able to densely predict class labels of a background region (stuff) and individually segment arbitrary foreground objects (things). In addition, our system has the capability to reconstruct a large-scale scene and extract a labeled mesh thanks to its use of a spatially hashed volumetric map representation. Our system first predicts pixel-wise panoptic labels (class labels for stuff regions and instance IDs for thing regions) for incoming RGB frames by fusing 2D semantic and instance segmentation outputs. The predicted panoptic labels are integrated into the volumetric map together with depth measurements while keeping the consistency of the instance IDs, which could vary frame to frame, by referring to the 3D map at that moment. In addition, we construct a fully connected conditional random field (CRF) model with respect to panoptic labels for map regularization. For online CRF inference, we propose a novel unary potential approximation and a map division strategy.   We evaluated the performance of our system on the ScanNet (v2) dataset. PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN methods in both semantic and instance segmentation benchmarks. Also, we demonstrate a promising augmented reality application using a 3D panoptic map generated by the proposed system.



### Complement Objective Training
- **Arxiv ID**: http://arxiv.org/abs/1903.01182v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.01182v2)
- **Published**: 2019-03-04 11:35:27+00:00
- **Updated**: 2019-03-21 18:33:12+00:00
- **Authors**: Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan
- **Comment**: ICLR'19 Camera Ready
- **Journal**: None
- **Summary**: Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.



### STEFANN: Scene Text Editor using Font Adaptive Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1903.01192v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1903.01192v3)
- **Published**: 2019-03-04 11:56:53+00:00
- **Updated**: 2023-03-29 17:30:25+00:00
- **Authors**: Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal
- **Comment**: Accepted in The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2020
- **Journal**: None
- **Summary**: Textual information in a captured scene plays an important role in scene interpretation and decision making. Though there exist methods that can successfully detect and interpret complex text regions present in a scene, to the best of our knowledge, there is no significant prior work that aims to modify the textual information in an image. The ability to edit text directly on images has several advantages including error correction, text restoration and image reusability. In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. We propose two different neural network architectures - (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consistency with neighboring characters. Our method works as a unified platform for modifying text in images. We present the effectiveness of our method on COCO-Text and ICDAR datasets both qualitatively and quantitatively.



### Collaborative Spatio-temporal Feature Learning for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.01197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01197v1)
- **Published**: 2019-03-04 12:03:03+00:00
- **Updated**: 2019-03-04 12:03:03+00:00
- **Authors**: Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Spatio-temporal feature learning is of central importance for action recognition in videos. Existing deep neural network models either learn spatial and temporal features independently (C2D) or jointly with unconstrained parameters (C3D). In this paper, we propose a novel neural operation which encodes spatio-temporal features collaboratively by imposing a weight-sharing constraint on the learnable parameters. In particular, we perform 2D convolution along three orthogonal views of volumetric video data,which learns spatial appearance and temporal motion cues respectively. By sharing the convolution kernels of different views, spatial and temporal features are collaboratively learned and thus benefit from each other. The complementary features are subsequently fused by a weighted summation whose coefficients are learned end-to-end. Our approach achieves state-of-the-art performance on large-scale benchmarks and won the 1st place in the Moments in Time Challenge 2018. Moreover, based on the learned coefficients of different views, we are able to quantify the contributions of spatial and temporal features. This analysis sheds light on interpretability of the model and may also guide the future design of algorithm for video recognition.



### Unsupervised Domain Adaptation Learning Algorithm for RGB-D Staircase Recognition
- **Arxiv ID**: http://arxiv.org/abs/1903.01212v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01212v4)
- **Published**: 2019-03-04 12:50:34+00:00
- **Updated**: 2019-03-20 22:09:41+00:00
- **Authors**: Jing Wang, Kuangen Zhang
- **Comment**: 7 pages, 5 figures, 17 reference
- **Journal**: None
- **Summary**: Detection and recognition of staircase as upstairs, downstairs and negative (e.g., ladder) are the fundamental of assisting the visually impaired to travel independently in unfamiliar environments. Previous researches have focused on using massive amounts of RGB-D scene data to train traditional machine learning (ML) based models to detect and recognize the staircase. However, the performance of traditional ML techniques is limited by the amount of labeled RGB-D staircase data. In this paper, we apply an unsupervised domain adaptation approach in deep architectures to transfer knowledge learned from the labeled RGB-D stationary staircase dataset to the unlabeled RGB-D escalator dataset. By utilizing the domain adaptation method, our feedforward convolutional neural networks (CNN) based feature extractor with 5 convolution layers can achieve 100% classification accuracy on testing the labeled stationary staircase data and 80.6% classification accuracy on testing the unlabeled escalator data. We demonstrate the success of the approach for classifying staircase on two domains with a limited amount of data. To further demonstrate the effectiveness of the approach, we also validate the same CNN model without domain adaptation and compare its results with those of our proposed architecture.



### Understanding the Mechanism of Deep Learning Framework for Lesion Detection in Pathological Images with Breast Cancer
- **Arxiv ID**: http://arxiv.org/abs/1903.01214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01214v1)
- **Published**: 2019-03-04 13:01:38+00:00
- **Updated**: 2019-03-04 13:01:38+00:00
- **Authors**: Wei-Wen Hsu, Chung-Hao Chen, Chang Hoa, Yu-Ling Hou, Xiang Gao, Yun Shao, Xueli Zhang, Jingjing Wang, Tao He, Yanghong Tai
- **Comment**: v1
- **Journal**: None
- **Summary**: The computer-aided detection (CADe) systems are developed to assist pathologists in slide assessment, increasing diagnosis efficiency and reducing missing inspections. Many studies have shown such a CADe system with deep learning approaches outperforms the one using conventional methods that rely on hand-crafted features based on field-knowledge. However, most developers who adopted deep learning models directly focused on the efficacy of outcomes, without providing comprehensive explanations on why their proposed frameworks can work effectively. In this study, we designed four experiments to verify the consecutive concepts, showing that the deep features learned from pathological patches are interpretable by domain knowledge of pathology and enlightening for clinical diagnosis in the task of lesion detection. The experimental results show the activation features work as morphological descriptors for specific cells or tissues, which agree with the clinical rules in classification. That is, the deep learning framework not only detects the distribution of tumor cells but also recognizes lymphocytes, collagen fibers, and some other non-cell structural tissues. Most of the characteristics learned by the deep learning models have summarized the detection rules that can be recognized by the experienced pathologists, whereas there are still some features may not be intuitive to domain experts but discriminative in classification for machines. Those features are worthy to be further studied in order to find out the reasonable correlations to pathological knowledge, from which pathological experts may draw inspirations for exploring new characteristics in diagnosis.



### Attention-based Lane Change Prediction
- **Arxiv ID**: http://arxiv.org/abs/1903.01246v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.01246v2)
- **Published**: 2019-03-04 13:56:25+00:00
- **Updated**: 2019-03-07 13:51:17+00:00
- **Authors**: Oliver Scheel, Naveen Shankar Nagaraja, Loren Schwarz, Nassir Navab, Federico Tombari
- **Comment**: To Appear in IEEE International Conference on Robotics and Automation
  (ICRA) 2019
- **Journal**: None
- **Summary**: Lane change prediction of surrounding vehicles is a key building block of path planning. The focus has been on increasing the accuracy of prediction by posing it purely as a function estimation problem at the cost of model understandability. However, the efficacy of any lane change prediction model can be improved when both corner and failure cases are humanly understandable. We propose an attention-based recurrent model to tackle both understandability and prediction quality. We also propose metrics which reflect the discomfort felt by the driver. We show encouraging results on a publicly available dataset and proprietary fleet data.



### Semi-Supervised Brain Lesion Segmentation with an Adapted Mean Teacher Model
- **Arxiv ID**: http://arxiv.org/abs/1903.01248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01248v1)
- **Published**: 2019-03-04 14:05:02+00:00
- **Updated**: 2019-03-04 14:05:02+00:00
- **Authors**: Wenhui Cui, Yanlin Liu, Yuxing Li, Menghao Guo, Yiming Li, Xiuli Li, Tianle Wang, Xiangzhu Zeng, Chuyang Ye
- **Comment**: Accepted by Information Processing in Medical Imaging 2019
- **Journal**: None
- **Summary**: Automated brain lesion segmentation provides valuable information for the analysis and intervention of patients. In particular, methods based on convolutional neural networks (CNNs) have achieved state-of-the-art segmentation performance. However, CNNs usually require a decent amount of annotated data, which may be costly and time-consuming to obtain. Since unannotated data is generally abundant, it is desirable to use unannotated data to improve the segmentation performance for CNNs when limited annotated data is available. In this work, we propose a semi-supervised learning (SSL) approach to brain lesion segmentation, where unannotated data is incorporated into the training of CNNs. We adapt the mean teacher model, which is originally developed for SSL-based image classification, for brain lesion segmentation. Assuming that the network should produce consistent outputs for similar inputs, a loss of segmentation consistency is designed and integrated into a self-ensembling framework. Specifically, we build a student model and a teacher model, which share the same CNN architecture for segmentation. The student and teacher models are updated alternately. At each step, the student model learns from the teacher model by minimizing the weighted sum of the segmentation loss computed from annotated data and the segmentation consistency loss between the teacher and student models computed from unannotated data. Then, the teacher model is updated by combining the updated student model with the historical information of teacher models using an exponential moving average strategy. For demonstration, the proposed approach was evaluated on ischemic stroke lesion segmentation, where it improves stroke lesion segmentation with the incorporation of unannotated data.



### Automatic Handgun Detection in X-ray Images using Bag of Words Model with Selective Search
- **Arxiv ID**: http://arxiv.org/abs/1903.01322v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.01322v1)
- **Published**: 2019-03-04 16:09:17+00:00
- **Updated**: 2019-03-04 16:09:17+00:00
- **Authors**: David Castro Piñol, Enrique Juan Marañón Reyes
- **Comment**: in Spanish
- **Journal**: None
- **Summary**: Baggage inspection systems using X-ray screening are crucial for security. Only 90% of threat objects are recognized from the X-ray system based in human inspection. Manual detection requires high concentration due to the images complexity and the challenges objects points of view. An algorithm based on Bag of Visual Word (BoVW) with Selective Search is proposed in this paper for handguns detection in single energy X-ray images from the public GDXray database. This approach is an adaptation of BoVW for X-ray baggage images context. In order to evaluate the proposed method the algorithm effectiveness recognition was tested on all bounding boxes returned by selective search algorithm in 200 images. The most relevant result is the precision and true positive rate (PPV = 80%, TPR= 92%). This approach achieves good performance for handgun recognition. In addition, it is the first time the Selective Search localization algorithm was tested in baggage X-ray images and showed possibilities with Bag of Visual Words.



### Joint segmentation and classification of retinal arteries/veins from fundus images
- **Arxiv ID**: http://arxiv.org/abs/1903.01330v1
- **DOI**: 10.1016/j.artmed.2019.02.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01330v1)
- **Published**: 2019-03-04 16:17:33+00:00
- **Updated**: 2019-03-04 16:17:33+00:00
- **Authors**: Fantin Girard, Conrad Kavalec, Farida Cheriet
- **Comment**: Preprint accepted in Artificial Intelligence in Medicine
- **Journal**: Artificial Intelligence in Medicine, Volume 94, 2019, Pages
  96-109, ISSN 0933-3657
- **Summary**: Objective Automatic artery/vein (A/V) segmentation from fundus images is required to track blood vessel changes occurring with many pathologies including retinopathy and cardiovascular pathologies. One of the clinical measures that quantifies vessel changes is the arterio-venous ratio (AVR) which represents the ratio between artery and vein diameters. This measure significantly depends on the accuracy of vessel segmentation and classification into arteries and veins. This paper proposes a fast, novel method for semantic A/V segmentation combining deep learning and graph propagation.   Methods A convolutional neural network (CNN) is proposed to jointly segment and classify vessels into arteries and veins. The initial CNN labeling is propagated through a graph representation of the retinal vasculature, whose nodes are defined as the vessel branches and edges are weighted by the cost of linking pairs of branches. To efficiently propagate the labels, the graph is simplified into its minimum spanning tree.   Results The method achieves an accuracy of 94.8% for vessels segmentation. The A/V classification achieves a specificity of 92.9% with a sensitivity of 93.7% on the CT-DRIVE database compared to the state-of-the-art-specificity and sensitivity, both of 91.7%.   Conclusion The results show that our method outperforms the leading previous works on a public dataset for A/V classification and is by far the fastest.   Significance The proposed global AVR calculated on the whole fundus image using our automatic A/V segmentation method can better track vessel changes associated to diabetic retinopathy than the standard local AVR calculated only around the optic disc.



### The StreetLearn Environment and Dataset
- **Arxiv ID**: http://arxiv.org/abs/1903.01292v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.01292v1)
- **Published**: 2019-03-04 16:21:22+00:00
- **Updated**: 2019-03-04 16:21:22+00:00
- **Authors**: Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell
- **Comment**: 13 pages, 6 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:1804.00168
- **Journal**: None
- **Summary**: Navigation is a rich and well-grounded problem domain that drives progress in many different areas of research: perception, planning, memory, exploration, and optimisation in particular. Historically these challenges have been separately considered and solutions built that rely on stationary datasets - for example, recorded trajectories through an environment. These datasets cannot be used for decision-making and reinforcement learning, however, and in general the perspective of navigation as an interactive learning task, where the actions and behaviours of a learning agent are learned simultaneously with the perception and planning, is relatively unsupported. Thus, existing navigation benchmarks generally rely on static datasets (Geiger et al., 2013; Kendall et al., 2015) or simulators (Beattie et al., 2016; Shah et al., 2018). To support and validate research in end-to-end navigation, we present StreetLearn: an interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task. The environment code, baseline agent code, and the dataset are available at http://streetlearn.cc



### Reduced Focal Loss: 1st Place Solution to xView object detection in Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1903.01347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01347v2)
- **Published**: 2019-03-04 16:36:13+00:00
- **Updated**: 2019-04-25 20:44:17+00:00
- **Authors**: Nikolay Sergievskiy, Alexander Ponamarev
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes our approach to the DIUx xView 2018 Detection Challenge [1]. This challenge focuses on a new satellite imagery dataset. The dataset contains 60 object classes that are highly imbalanced. Due to the imbalanced nature of the dataset, the training process becomes significantly more challenging. To address this problem, we introduce a novel Reduced Focal Loss function, which brought us 1st place in the DIUx xView 2018 Detection Challenge.



### An Adversarial Super-Resolution Remedy for Radar Design Trade-offs
- **Arxiv ID**: http://arxiv.org/abs/1903.01392v2
- **DOI**: 10.23919/EUSIPCO.2019.8902510
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01392v2)
- **Published**: 2019-03-04 17:41:26+00:00
- **Updated**: 2019-06-20 16:23:55+00:00
- **Authors**: Karim Armanious, Sherif Abdulatif, Fady Aziz, Urs Schneider, Bin Yang
- **Comment**: Accepted in EUSIPCO 2019, 5 pages
- **Journal**: None
- **Summary**: Radar is of vital importance in many fields, such as autonomous driving, safety and surveillance applications. However, it suffers from stringent constraints on its design parametrization leading to multiple trade-offs. For example, the bandwidth in FMCW radars is inversely proportional with both the maximum unambiguous range and range resolution. In this work, we introduce a new method for circumventing radar design trade-offs. We propose the use of recent advances in computer vision, more specifically generative adversarial networks (GANs), to enhance low-resolution radar acquisitions into higher resolution counterparts while maintaining the advantages of the low-resolution parametrization. The capability of the proposed method was evaluated on the velocity resolution and range-azimuth trade-offs in micro-Doppler signatures and FMCW uniform linear array (ULA) radars, respectively.



### VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1903.01434v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.01434v3)
- **Published**: 2019-03-04 18:55:45+00:00
- **Updated**: 2020-02-12 16:55:25+00:00
- **Authors**: Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, Durk Kingma
- **Comment**: ICLR 2020 Camera-Ready. Previous title: VideoFlow: A Flow-Based
  Generative Model for Video
- **Journal**: None
- **Summary**: Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modelling of video.



### M-VAD Names: a Dataset for Video Captioning with Naming
- **Arxiv ID**: http://arxiv.org/abs/1903.01489v1
- **DOI**: 10.1007/s11042-018-7040-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01489v1)
- **Published**: 2019-03-04 19:05:27+00:00
- **Updated**: 2019-03-04 19:05:27+00:00
- **Authors**: Stefano Pini, Marcella Cornia, Federico Bolelli, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: Source Code: https://github.com/aimagelab/mvad-names-dataset - Video
  Demo: https://youtu.be/dOvtAXbOOH4
- **Journal**: Multimedia Tools and Applications (2018)
- **Summary**: Current movie captioning architectures are not capable of mentioning characters with their proper name, replacing them with a generic "someone" tag. The lack of movie description datasets with characters' visual annotations surely plays a relevant role in this shortage. Recently, we proposed to extend the M-VAD dataset by introducing such information. In this paper, we present an improved version of the dataset, namely M-VAD Names, and its semi-automatic annotation procedure. The resulting dataset contains 63k visual tracks and 34k textual mentions, all associated with character identities. To showcase the features of the dataset and quantify the complexity of the naming task, we investigate multimodal architectures to replace the "someone" tags with proper character names in existing video captions. The evaluation is further extended by testing this application on videos outside of the M-VAD Names dataset.



### Fine-grained lesion annotation in CT images with knowledge mined from radiology reports
- **Arxiv ID**: http://arxiv.org/abs/1903.01505v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01505v2)
- **Published**: 2019-03-04 19:42:50+00:00
- **Updated**: 2019-03-26 19:33:49+00:00
- **Authors**: Ke Yan, Yifan Peng, Zhiyong Lu, Ronald M. Summers
- **Comment**: 4 pages, IEEE International Symposium on Biomedical Imaging (ISBI)
  2019, oral presentation
- **Journal**: None
- **Summary**: In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and write sentences in the radiology report to describe them. In this paper, we study the lesion description or annotation problem as an important step of computer-aided diagnosis (CAD). Given a lesion image, our aim is to predict multiple relevant labels, such as the lesion's body part, type, and attributes. To address this problem, we define a set of 145 labels based on RadLex to describe a large variety of lesions in the DeepLesion dataset. We directly mine training labels from the lesion's corresponding sentence in the radiology report, which requires minimal manual effort and is easily generalizable to large data and label sets. A multi-label convolutional neural network is then proposed for images with multi-scale structure and a noise-robust loss. Quantitative and qualitative experiments demonstrate the effectiveness of the framework. The average area under ROC curve on 1,872 test lesions is 0.9083.



### TKD: Temporal Knowledge Distillation for Active Perception
- **Arxiv ID**: http://arxiv.org/abs/1903.01522v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01522v2)
- **Published**: 2019-03-04 20:15:56+00:00
- **Updated**: 2020-01-06 22:34:42+00:00
- **Authors**: Mohammad Farhadi, Yezhou Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks based methods have been proved to achieve outstanding performance on object detection and classification tasks. Despite significant performance improvement, due to the deep structures, they still require prohibitive runtime to process images and maintain the highest possible performance for real-time applications. Observing the phenomenon that human vision system (HVS) relies heavily on the temporal dependencies among frames from the visual input to conduct recognition efficiently, we propose a novel framework dubbed as TKD: temporal knowledge distillation. This framework distills the temporal knowledge from a heavy neural networks based model over selected video frames (the perception of the moments) to a light-weight model. To enable the distillation, we put forward two novel procedures: 1) an Long-short Term Memory (LSTM) based key frame selection method; and 2) a novel teacher-bounded loss design. To validate, we conduct comprehensive empirical evaluations using different object detection methods over multiple datasets including Youtube-Objects and Hollywood scene dataset. Our results show consistent improvement in accuracy-speed trad-offs for object detection over the frames of the dynamic scene, compare to other modern object recognition methods.



### Learning of Image Dehazing Models for Segmentation Tasks
- **Arxiv ID**: http://arxiv.org/abs/1903.01530v2
- **DOI**: 10.23919/EUSIPCO.2019.8903046
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.01530v2)
- **Published**: 2019-03-04 20:41:05+00:00
- **Updated**: 2019-06-22 15:03:46+00:00
- **Authors**: Sébastien de Blois, Ihsen Hedhli, Christian Gagné
- **Comment**: Accepted in EUSIPCO 2019
- **Journal**: None
- **Summary**: To evaluate their performance, existing dehazing approaches generally rely on distance measures between the generated image and its corresponding ground truth. Despite its ability to produce visually good images, using pixel-based or even perceptual metrics do not guarantee, in general, that the produced image is fit for being used as input for low-level computer vision tasks such as segmentation. To overcome this weakness, we are proposing a novel end-to-end approach for image dehazing, fit for being used as input to an image segmentation procedure, while maintaining the visual quality of the generated images. Inspired by the success of Generative Adversarial Networks (GAN), we propose to optimize the generator by introducing a discriminator network and a loss function that evaluates segmentation quality of dehazed images. In addition, we make use of a supplementary loss function that verifies that the visual and the perceptual quality of the generated image are preserved in hazy conditions. Results obtained using the proposed technique are appealing, with a favorable comparison to state-of-the-art approaches when considering the performance of segmentation algorithms on the hazy images.



### Selective Sensor Fusion for Neural Visual-Inertial Odometry
- **Arxiv ID**: http://arxiv.org/abs/1903.01534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.01534v1)
- **Published**: 2019-03-04 20:51:37+00:00
- **Updated**: 2019-03-04 20:51:37+00:00
- **Authors**: Changhao Chen, Stefano Rosa, Yishu Miao, Chris Xiaoxuan Lu, Wei Wu, Andrew Markham, Niki Trigoni
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Deep learning approaches for Visual-Inertial Odometry (VIO) have proven successful, but they rarely focus on incorporating robust fusion strategies for dealing with imperfect input sensory data. We propose a novel end-to-end selective sensor fusion framework for monocular VIO, which fuses monocular images and inertial measurements in order to estimate the trajectory whilst improving robustness to real-life issues, such as missing and corrupted data or bad sensor synchronization. In particular, we propose two fusion modalities based on different masking strategies: deterministic soft fusion and stochastic hard fusion, and we compare with previously proposed direct fusion baselines. During testing, the network is able to selectively process the features of the available sensor modalities and produce a trajectory at scale. We present a thorough investigation on the performances on three public autonomous driving, Micro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate the effectiveness of the fusion strategies, which offer better performances compared to direct fusion, particularly in presence of corrupted data. In addition, we study the interpretability of the fusion networks by visualising the masking layers in different scenarios and with varying data corruption, revealing interesting correlations between the fusion networks and imperfect sensory input data.



### Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1903.01545v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1903.01545v1)
- **Published**: 2019-03-04 21:24:26+00:00
- **Updated**: 2019-03-04 21:24:26+00:00
- **Authors**: Svebor Karaman, Xudong Lin, Xuefeng Hu, Shih-Fu Chang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an unsupervised hashing method which aims to produce binary codes that preserve the ranking induced by a real-valued representation. Such compact hash codes enable the complete elimination of real-valued feature storage and allow for significant reduction of the computation complexity and storage cost of large-scale image retrieval applications. Specifically, we learn a neural network-based model, which transforms the input representation into a binary representation. We formalize the training objective of the network in an intuitive and effective way, considering each training sample as a query and aiming to obtain the same retrieval results using the produced hash codes as those obtained with the original features. This training formulation directly optimizes the hashing model for the target usage of the hash codes it produces. We further explore the addition of a decoder trained to obtain an approximated reconstruction of the original features. At test time, we retrieved the most promising database samples with an efficient graph-based search procedure using only our hash codes and perform re-ranking using the reconstructed features, thus without needing to access the original features at all. Experiments conducted on multiple publicly available large-scale datasets show that our method consistently outperforms all compared state-of-the-art unsupervised hashing methods and that the reconstruction procedure can effectively boost the search accuracy with a minimal constant additional cost.



### The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1903.01568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.01568v1)
- **Published**: 2019-03-04 22:17:46+00:00
- **Updated**: 2019-03-04 22:17:46+00:00
- **Authors**: Abhishek Patil, Srikanth Malla, Haiming Gang, Yi-Ting Chen
- **Comment**: The dataset is available at https://usa.honda-ri.com/H3D
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  2019
- **Summary**: 3D multi-object detection and tracking are crucial for traffic scene understanding. However, the community pays less attention to these areas due to the lack of a standardized benchmark dataset to advance the field. Moreover, existing datasets (e.g., KITTI) do not provide sufficient data and labels to tackle challenging scenes where highly interactive and occluded traffic participants are present. To address the issues, we present the Honda Research Institute 3D Dataset (H3D), a large-scale full-surround 3D multi-object detection and tracking dataset collected using a 3D LiDAR scanner. H3D comprises of 160 crowded and highly interactive traffic scenes with a total of 1 million labeled instances in 27,721 frames. With unique dataset size, rich annotations, and complex scenes, H3D is gathered to stimulate research on full-surround 3D multi-object detection and tracking. To effectively and efficiently annotate a large-scale 3D point cloud dataset, we propose a labeling methodology to speed up the overall annotation cycle. A standardized benchmark is created to evaluate full-surround 3D multi-object detection and tracking algorithms. 3D object detection and tracking algorithms are trained and tested on H3D. Finally, sources of errors are discussed for the development of future algorithms.



### On measuring the iconicity of a face
- **Arxiv ID**: http://arxiv.org/abs/1903.01581v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.01581v1)
- **Published**: 2019-03-04 23:16:36+00:00
- **Updated**: 2019-03-04 23:16:36+00:00
- **Authors**: Prithviraj Dhar, Carlos D. Castillo, Rama Chellappa
- **Comment**: Accepted to WACV 2019
- **Journal**: None
- **Summary**: For a given identity in a face dataset, there are certain iconic images which are more representative of the subject than others. In this paper, we explore the problem of computing the iconicity of a face. The premise of the proposed approach is as follows: For an identity containing a mixture of iconic and non iconic images, if a given face cannot be successfully matched with any other face of the same identity, then the iconicity of the face image is low. Using this information, we train a Siamese Multi-Layer Perceptron network, such that each of its twins predict iconicity scores of the image feature pair, fed in as input. We observe the variation of the obtained scores with respect to covariates such as blur, yaw, pitch, roll and occlusion to demonstrate that they effectively predict the quality of the image and compare it with other existing metrics. Furthermore, we use these scores to weight features for template-based face verification and compare it with media averaging of features.



