# Arxiv Papers in cs.CV on 2019-03-02
### PuVAE: A Variational Autoencoder to Purify Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1903.00585v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00585v1)
- **Published**: 2019-03-02 00:38:38+00:00
- **Updated**: 2019-03-02 00:38:38+00:00
- **Authors**: Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, Nam Ik Cho
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are widely used and exhibit excellent performance in many areas. However, they are vulnerable to adversarial attacks that compromise the network at the inference time by applying elaborately designed perturbation to input data. Although several defense methods have been proposed to address specific attacks, other attack methods can circumvent these defense mechanisms. Therefore, we propose Purifying Variational Autoencoder (PuVAE), a method to purify adversarial examples. The proposed method eliminates an adversarial perturbation by projecting an adversarial example on the manifold of each class, and determines the closest projection as a purified sample. We experimentally illustrate the robustness of PuVAE against various attack methods without any prior knowledge. In our experiments, the proposed method exhibits performances competitive with state-of-the-art defense methods, and the inference time is approximately 130 times faster than that of Defense-GAN that is the state-of-the art purifier model.



### Straight to the point: reinforcement learning for user guidance in ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1903.00586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00586v1)
- **Published**: 2019-03-02 00:38:54+00:00
- **Updated**: 2019-03-02 00:38:54+00:00
- **Authors**: Fausto Milletari, Vighnesh Birodkar, Michal Sofka
- **Comment**: None
- **Journal**: None
- **Summary**: Point of care ultrasound (POCUS) consists in the use of ultrasound imaging in critical or emergency situations to support clinical decisions by healthcare professionals and first responders. In this setting it is essential to be able to provide means to obtain diagnostic data to potentially inexperienced users who did not receive an extensive medical training. Interpretation and acquisition of ultrasound images is not trivial. First, the user needs to find a suitable sound window which can be used to get a clear image, and then he needs to correctly interpret it to perform a diagnosis. Although many recent approaches focus on developing smart ultrasound devices that add interpretation capabilities to existing systems, our goal in this paper is to present a reinforcement learning (RL) strategy which is capable to guide novice users to the correct sonic window and enable them to obtain clinically relevant pictures of the anatomy of interest. We apply our approach to cardiac images acquired from the parasternal long axis (PLAx) view of the left ventricle of the heart.



### Learning Robust Representations by Projecting Superficial Statistics Out
- **Arxiv ID**: http://arxiv.org/abs/1903.06256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.06256v1)
- **Published**: 2019-03-02 00:42:03+00:00
- **Updated**: 2019-03-02 00:42:03+00:00
- **Authors**: Haohan Wang, Zexue He, Zachary C. Lipton, Eric P. Xing
- **Comment**: To appear at ICLR 2019. Implementation:
  https://github.com/HaohanWang/HEX
- **Journal**: None
- **Summary**: Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.



### Unsupervised Traffic Accident Detection in First-Person Videos
- **Arxiv ID**: http://arxiv.org/abs/1903.00618v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00618v4)
- **Published**: 2019-03-02 03:24:21+00:00
- **Updated**: 2019-07-26 03:05:24+00:00
- **Authors**: Yu Yao, Mingze Xu, Yuchen Wang, David J. Crandall, Ella M. Atkins
- **Comment**: Accepted to IROS 2019
- **Journal**: None
- **Summary**: Recognizing abnormal events such as traffic violations and accidents in natural driving scenes is essential for successful autonomous driving and advanced driver assistance systems. However, most work on video anomaly detection suffers from two crucial drawbacks. First, they assume cameras are fixed and videos have static backgrounds, which is reasonable for surveillance applications but not for vehicle-mounted cameras. Second, they pose the problem as one-class classification, relying on arduously hand-labeled training datasets that limit recognition to anomaly categories that have been explicitly trained. This paper proposes an unsupervised approach for traffic accident detection in first-person (dashboard-mounted camera) videos. Our major novelty is to detect anomalies by predicting the future locations of traffic participants and then monitoring the prediction accuracy and consistency metrics with three different strategies. We evaluate our approach using a new dataset of diverse traffic accidents, AnAn Accident Detection (A3D), as well as another publicly-available dataset. Experimental results show that our approach outperforms the state-of-the-art.



### RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion
- **Arxiv ID**: http://arxiv.org/abs/1903.00620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00620v2)
- **Published**: 2019-03-02 04:14:31+00:00
- **Updated**: 2019-05-01 03:54:34+00:00
- **Authors**: Jie Li, Yu Liu, Dong Gong, Qinfeng Shi, Xia Yuan, Chunxia Zhao, Ian Reid
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: RGB images differentiate from depth images as they carry more details about the color and texture information, which can be utilized as a vital complementary to depth for boosting the performance of 3D semantic scene completion (SSC). SSC is composed of 3D shape completion (SC) and semantic scene labeling while most of the existing methods use depth as the sole input which causes the performance bottleneck. Moreover, the state-of-the-art methods employ 3D CNNs which have cumbersome networks and tremendous parameters. We introduce a light-weight Dimensional Decomposition Residual network (DDR) for 3D dense prediction tasks. The novel factorized convolution layer is effective for reducing the network parameters, and the proposed multi-scale fusion mechanism for depth and color image can improve the completion and segmentation accuracy simultaneously. Our method demonstrates excellent performance on two public datasets. Compared with the latest method SSCNet, we achieve 5.9% gains in SC-IoU and 5.7% gains in SSC-IOU, albeit with only 21% network parameters and 16.6% FLOPs employed compared with that of SSCNet.



### Feature Selective Anchor-Free Module for Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.00621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00621v1)
- **Published**: 2019-03-02 04:15:34+00:00
- **Updated**: 2019-03-02 04:15:34+00:00
- **Authors**: Chenchen Zhu, Yihui He, Marios Savvides
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We motivate and present feature selective anchor-free (FSAF) module, a simple and effective building block for single-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. The general concept of the FSAF module is online feature selection applied to the training of multi-level anchor-free branches. Specifically, an anchor-free branch is attached to each level of the feature pyramid, allowing box encoding and decoding in the anchor-free manner at an arbitrary level. During training, we dynamically assign each instance to the most suitable feature level. At the time of inference, the FSAF module can work jointly with anchor-based branches by outputting predictions in parallel. We instantiate this concept with simple implementations of anchor-free branches and online feature selection strategy. Experimental results on the COCO detection track show that our FSAF module performs better than anchor-based counterparts while being faster. When working jointly with anchor-based branches, the FSAF module robustly improves the baseline RetinaNet by a large margin under various settings, while introducing nearly free inference overhead. And the resulting best model can achieve a state-of-the-art 44.6% mAP, outperforming all existing single-shot detectors on COCO.



### Quaternion Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00658v1)
- **Published**: 2019-03-02 08:57:35+00:00
- **Updated**: 2019-03-02 08:57:35+00:00
- **Authors**: Xuanyu Zhu, Yi Xu, Hongteng Xu, Changjian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks in the real domain have been studied for a long time and achieved promising results in many vision tasks for recent years. However, the extensions of the neural network models in other number fields and their potential applications are not fully-investigated yet. Focusing on color images, which can be naturally represented as quaternion matrices, we propose a quaternion convolutional neural network (QCNN) model to obtain more representative features. In particular, we redesign the basic modules like convolution layer and fully-connected layer in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks. Moreover, these modules are compatible with almost all deep learning techniques and can be plugged into traditional CNNs easily. We test our QCNN models in both color image classification and denoising tasks. Experimental results show that they outperform the real-valued CNNs with same structures.



### OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional Cameras
- **Arxiv ID**: http://arxiv.org/abs/1903.00676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1903.00676v1)
- **Published**: 2019-03-02 10:16:38+00:00
- **Updated**: 2019-03-02 10:16:38+00:00
- **Authors**: G. Dias Pais, Tiago J. Dias, Jacinto C. Nascimento, Pedro Miraldo
- **Comment**: Accepted in 2019 IEEE Int'l Conf. Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Pedestrian detection is one of the most explored topics in computer vision and robotics. The use of deep learning methods allowed the development of new and highly competitive algorithms. Deep Reinforcement Learning has proved to be within the state-of-the-art in terms of both detection in perspective cameras and robotics applications. However, for detection in omnidirectional cameras, the literature is still scarce, mostly because of their high levels of distortion. This paper presents a novel and efficient technique for robust pedestrian detection in omnidirectional images. The proposed method uses deep Reinforcement Learning that takes advantage of the distortion in the image. By considering the 3D bounding boxes and their distorted projections into the image, our method is able to provide the pedestrian's position in the world, in contrast to the image positions provided by most state-of-the-art methods for perspective cameras. Our method avoids the need of pre-processing steps to remove the distortion, which is computationally expensive. Beyond the novel solution, our method compares favorably with the state-of-the-art methodologies that do not consider the underlying distortion for the detection task.



### Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated Temporal Fully-Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00695v1)
- **Published**: 2019-03-02 12:53:25+00:00
- **Updated**: 2019-03-02 12:53:25+00:00
- **Authors**: Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han Du, Klaus Fischer, Philipp Slusallek
- **Comment**: Eurographics 2019 - Short Papers
- **Journal**: None
- **Summary**: Human motion capture data has been widely used in data-driven character animation. In order to generate realistic, natural-looking motions, most data-driven approaches require considerable efforts of pre-processing, including motion segmentation and annotation. Existing (semi-) automatic solutions either require hand-crafted features for motion segmentation or do not produce the semantic annotations required for motion synthesis and building large-scale motion databases. In addition, human labeled annotation data suffers from inter- and intra-labeler inconsistencies by design. We propose a semi-automatic framework for semantic segmentation of motion capture data based on supervised machine learning techniques. It first transforms a motion capture sequence into a ``motion image'' and applies a convolutional neural network for image segmentation. Dilated temporal convolutions enable the extraction of temporal information from a large receptive field. Our model outperforms two state-of-the-art models for action segmentation, as well as a popular network for sequence modeling. Most of all, our method is very robust under noisy and inaccurate training labels and thus can handle human errors during the labeling process.



### Deep Optimization model for Screen Content Image Quality Assessment using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00705v1)
- **Published**: 2019-03-02 14:10:46+00:00
- **Updated**: 2019-03-02 14:10:46+00:00
- **Authors**: Xuhao Jiang, Liquan Shen, Guorui Feng, Liangwei Yu, Ping An
- **Comment**: 12pages, 9 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel quadratic optimized model based on the deep convolutional neural network (QODCNN) for full-reference and no-reference screen content image (SCI) quality assessment. Unlike traditional CNN methods taking all image patches as training data and using average quality pooling, our model is optimized to obtain a more effective model including three steps. In the first step, an end-to-end deep CNN is trained to preliminarily predict the image visual quality, and batch normalized (BN) layers and l2 regularization are employed to improve the speed and performance of network fitting. For second step, the pretrained model is fine-tuned to achieve better performance under analysis of the raw training data. An adaptive weighting method is proposed in the third step to fuse local quality inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to image patches containing texture and edge information. The novelty of our algorithm can be concluded as follows: 1) with the consideration of correlation between local quality and subjective differential mean opinion score (DMOS), the Euclidean distance is utilized to measure effectiveness of image patches, and the pretrained model is fine-tuned with more effective training data; 2) an adaptive pooling approach is employed to fuse patch quality of textual and pictorial regions, whose feature only extracted from distorted images owns strong noise robust and effects on both FR and NR IQA; 3) Considering the characteristics of SCIs, a deep and valid network architecture is designed for both NR and FR visual quality evaluation of SCIs. Experimental results verify that our model outperforms both current no-reference and full-reference image quality assessment methods on the benchmark screen content image quality assessment database (SIQAD).



### Strong homotopy of digitally continuous functions
- **Arxiv ID**: http://arxiv.org/abs/1903.00706v1
- **DOI**: None
- **Categories**: **math.GN**, cs.CV, math.CO, 55P10, 68R10
- **Links**: [PDF](http://arxiv.org/pdf/1903.00706v1)
- **Published**: 2019-03-02 14:11:43+00:00
- **Updated**: 2019-03-02 14:11:43+00:00
- **Authors**: P. Christopher Staecker
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new type of homotopy relation for digitally continuous functions which we call ``strong homotopy.'' Both digital homotopy and strong homotopy are natural digitizations of classical topological homotopy: the difference between them is analogous to the difference between digital 4-adjacency and 8-adjacency in the plane.   We explore basic properties of strong homotopy, and give some equivalent characterizations. In particular we show that strong homotopy is related to ``punctuated homotopy,'' in which the function changes by only one point in each homotopy time step.   We also show that strongly homotopic maps always have the same induced homomorphisms in the digital homology theory. This is not generally true for digitally homotopic maps, though we do show that it is true for any homotopic selfmaps on the digital cycle $C_n$ with $n\ge 4$.   We also define and consider strong homotopy equivalence of digital images. Using some computer assistance, we produce a catalog of all small digital images up to strong homotopy equivalence. We also briefly consider pointed strong homotopy equivalence, and give an example of a pointed contractible image which is not pointed strongly contractible.



### PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.00709v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00709v5)
- **Published**: 2019-03-02 14:36:49+00:00
- **Updated**: 2022-01-15 06:18:32+00:00
- **Authors**: Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, Kai Xu
- **Comment**: CVPR 2019; Corresponding author: Kai Xu (kevin.kai.xu@gmail.com);
  Project page: www.kevinkaixu.net/projects/partnet.html
- **Journal**: CVPR 2019
- **Summary**: Deep learning approaches to 3D shape segmentation are typically formulated as a multi-class labeling problem. Existing models are trained for a fixed set of labels, which greatly limits their flexibility and adaptivity. We opt for top-down recursive decomposition and develop the first deep learning model for hierarchical segmentation of 3D shapes, based on recursive neural networks. Starting from a full shape represented as a point cloud, our model performs recursive binary decomposition, where the decomposition network at all nodes in the hierarchy share weights. At each node, a node classifier is trained to determine the type (adjacency or symmetry) and stopping criteria of its decomposition. The features extracted in higher level nodes are recursively propagated to lower level ones. Thus, the meaningful decompositions in higher levels provide strong contextual cues constraining the segmentations in lower levels. Meanwhile, to increase the segmentation accuracy at each node, we enhance the recursive contextual feature with the shape feature extracted for the corresponding part. Our method segments a 3D shape in point cloud into an unfixed number of parts, depending on the shape complexity, showing strong generality and flexibility. It achieves the state-of-the-art performance, both for fine-grained and semantic segmentation, on the public benchmark and a new benchmark of fine-grained segmentation proposed in this work. We also demonstrate its application for fine-grained part refinements in image-to-shape reconstruction.



### Equilibrated Recurrent Neural Network: Neuronal Time-Delayed Self-Feedback Improves Accuracy and Stability
- **Arxiv ID**: http://arxiv.org/abs/1903.00755v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00755v1)
- **Published**: 2019-03-02 20:01:44+00:00
- **Updated**: 2019-03-02 20:01:44+00:00
- **Authors**: Ziming Zhang, Anil Kag, Alan Sullivan, Venkatesh Saligrama
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel {\it Equilibrated Recurrent Neural Network} (ERNN) to combat the issues of inaccuracy and instability in conventional RNNs. Drawing upon the concept of autapse in neuroscience, we propose augmenting an RNN with a time-delayed self-feedback loop. Our sole purpose is to modify the dynamics of each internal RNN state and, at any time, enforce it to evolve close to the equilibrium point associated with the input signal at that time. We show that such self-feedback helps stabilize the hidden state transitions leading to fast convergence during training while efficiently learning discriminative latent features that result in state-of-the-art results on several benchmark datasets at test-time. We propose a novel inexact Newton method to solve fixed-point conditions given model parameters for generating the latent features at each hidden state. We prove that our inexact Newton method converges locally with linear rate (under mild conditions). We leverage this result for efficient training of ERNNs based on backpropagation.



### Time-Delay Momentum: A Regularization Perspective on the Convergence and Generalization of Stochastic Momentum for Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1903.00760v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00760v2)
- **Published**: 2019-03-02 20:21:38+00:00
- **Updated**: 2019-06-01 23:05:37+00:00
- **Authors**: Ziming Zhang, Wenju Xu, Alan Sullivan
- **Comment**: has errors
- **Journal**: None
- **Summary**: In this paper we study the problem of convergence and generalization error bound of stochastic momentum for deep learning from the perspective of regularization. To do so, we first interpret momentum as solving an $\ell_2$-regularized minimization problem to learn the offsets between arbitrary two successive model parameters. We call this {\em time-delay momentum} because the model parameter is updated after a few iterations towards finding the minimizer. We then propose our learning algorithm, \ie stochastic gradient descent (SGD) with time-delay momentum. We show that our algorithm can be interpreted as solving a sequence of strongly convex optimization problems using SGD. We prove that under mild conditions our algorithm can converge to a stationary point with rate of $O(\frac{1}{\sqrt{K}})$ and generalization error bound of $O(\frac{1}{\sqrt{n\delta}})$ with probability at least $1-\delta$, where $K,n$ are the numbers of model updates and training samples, respectively. We demonstrate the empirical superiority of our algorithm in deep learning in comparison with the state-of-the-art deep learning solvers.



### Extreme Channel Prior Embedded Network for Dynamic Scene Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1903.00763v1
- **DOI**: 10.1109/TIP.2020.2995048
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00763v1)
- **Published**: 2019-03-02 20:49:00+00:00
- **Updated**: 2019-03-02 20:49:00+00:00
- **Authors**: Jianrui Cai, Wangmeng Zuo, Lei Zhang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Recent years have witnessed the significant progress on convolutional neural networks (CNNs) in dynamic scene deblurring. While CNN models are generally learned by the reconstruction loss defined on training data, incorporating suitable image priors as well as regularization terms into the network architecture could boost the deblurring performance. In this work, we propose an Extreme Channel Prior embedded Network (ECPeNet) to plug the extreme channel priors (i.e., priors on dark and bright channels) into a network architecture for effective dynamic scene deblurring. A novel trainable extreme channel prior embedded layer (ECPeL) is developed to aggregate both extreme channel and blurry image representations, and sparse regularization is introduced to regularize the ECPeNet model learning. Furthermore, we present an effective multi-scale network architecture that works in both coarse-to-fine and fine-to-coarse manners for better exploiting information flow across scales. Experimental results on GoPro and Kohler datasets show that our proposed ECPeNet performs favorably against state-of-the-art deep image deblurring methods in terms of both quantitative metrics and visual quality.



### Spatio-Temporal Vegetation Pixel Classification By Using Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1903.00774v1
- **DOI**: 10.1109/LGRS.2019.2903194
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00774v1)
- **Published**: 2019-03-02 22:07:25+00:00
- **Updated**: 2019-03-02 22:07:25+00:00
- **Authors**: Keiller Nogueira, Jefersson A. dos Santos, Nathalia Menini, Thiago S. F. Silva, Leonor Patricia C. Morellato, Ricardo da S. Torres
- **Comment**: None
- **Journal**: None
- **Summary**: Plant phenology studies rely on long-term monitoring of life cycles of plants. High-resolution unmanned aerial vehicles (UAVs) and near-surface technologies have been used for plant monitoring, demanding the creation of methods capable of locating and identifying plant species through time and space. However, this is a challenging task given the high volume of data, the constant data missing from temporal dataset, the heterogeneity of temporal profiles, the variety of plant visual patterns, and the unclear definition of individuals' boundaries in plant communities. In this letter, we propose a novel method, suitable for phenological monitoring, based on Convolutional Networks (ConvNets) to perform spatio-temporal vegetation pixel-classification on high resolution images. We conducted a systematic evaluation using high-resolution vegetation image datasets associated with the Brazilian Cerrado biome. Experimental results show that the proposed approach is effective, overcoming other spatio-temporal pixel-classification strategies.



### Marker based Thermal-Inertial Localization for Aerial Robots in Obscurant Filled Environments
- **Arxiv ID**: http://arxiv.org/abs/1903.00782v1
- **DOI**: 10.1007/978-3-030-03801-4_49
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00782v1)
- **Published**: 2019-03-02 22:33:02+00:00
- **Updated**: 2019-03-02 22:33:02+00:00
- **Authors**: Shehryar Khattak, Christos Papachristos, Kostas Alexis
- **Comment**: 10 pages, 5 figures, Published in International Symposium on Visual
  Computing 2018
- **Journal**: None
- **Summary**: For robotic inspection tasks in known environments fiducial markers provide a reliable and low-cost solution for robot localization. However, detection of such markers relies on the quality of RGB camera data, which degrades significantly in the presence of visual obscurants such as fog and smoke. The ability to navigate known environments in the presence of obscurants can be critical for inspection tasks especially, in the aftermath of a disaster. Addressing such a scenario, this work proposes a method for the design of fiducial markers to be used with thermal cameras for the pose estimation of aerial robots. Our low cost markers are designed to work in the long wave infrared spectrum, which is not affected by the presence of obscurants, and can be affixed to any object that has measurable temperature difference with respect to its surroundings. Furthermore, the estimated pose from the fiducial markers is fused with inertial measurements in an extended Kalman filter to remove high frequency noise and error present in the fiducial pose estimates. The proposed markers and the pose estimation method are experimentally evaluated in an obscurant filled environment using an aerial robot carrying a thermal camera.



### AIRD: Adversarial Learning Framework for Image Repurposing Detection
- **Arxiv ID**: http://arxiv.org/abs/1903.00788v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00788v3)
- **Published**: 2019-03-02 23:14:58+00:00
- **Updated**: 2019-04-09 21:17:49+00:00
- **Authors**: Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, Premkumar Natarajan
- **Comment**: Camera-ready version for the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: Image repurposing is a commonly used method for spreading misinformation on social media and online forums, which involves publishing untampered images with modified metadata to create rumors and further propaganda. While manual verification is possible, given vast amounts of verified knowledge available on the internet, the increasing prevalence and ease of this form of semantic manipulation call for the development of robust automatic ways of assessing the semantic integrity of multimedia data. In this paper, we present a novel method for image repurposing detection that is based on the real-world adversarial interplay between a bad actor who repurposes images with counterfeit metadata and a watchdog who verifies the semantic consistency between images and their accompanying metadata, where both players have access to a reference dataset of verified content, which they can use to achieve their goals. The proposed method exhibits state-of-the-art performance on location-identity, subject-identity and painting-artist verification, showing its efficacy across a diverse set of scenarios.



### Let's Transfer Transformations of Shared Semantic Representations
- **Arxiv ID**: http://arxiv.org/abs/1903.00793v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00793v1)
- **Published**: 2019-03-02 23:53:45+00:00
- **Updated**: 2019-03-02 23:53:45+00:00
- **Authors**: Nam Vo, Lu Jiang, James Hays
- **Comment**: None
- **Journal**: None
- **Summary**: With a good image understanding capability, can we manipulate the images high level semantic representation? Such transformation operation can be used to generate or retrieve similar images but with a desired modification (for example changing beach background to street background); similar ability has been demonstrated in zero shot learning, attribute composition and attribute manipulation image search. In this work we show how one can learn transformations with no training examples by learning them on another domain and then transfer to the target domain. This is feasible if: first, transformation training data is more accessible in the other domain and second, both domains share similar semantics such that one can learn transformations in a shared embedding space. We demonstrate this on an image retrieval task where search query is an image, plus an additional transformation specification (for example: search for images similar to this one but background is a street instead of a beach). In one experiment, we transfer transformation from synthesized 2D blobs image to 3D rendered image, and in the other, we transfer from text domain to natural image domain.



