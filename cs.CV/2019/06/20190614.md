# Arxiv Papers in cs.CV on 2019-06-14
### Image Captioning: Transforming Objects into Words
- **Arxiv ID**: http://arxiv.org/abs/1906.05963v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.05963v2)
- **Published**: 2019-06-14 00:00:29+00:00
- **Updated**: 2020-01-11 11:03:05+00:00
- **Authors**: Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Image captioning models typically follow an encoder-decoder architecture which uses abstract image feature vectors as input to the encoder. One of the most successful algorithms uses feature vectors extracted from the region proposals obtained from an object detector. In this work we introduce the Object Relation Transformer, that builds upon this approach by explicitly incorporating information about the spatial relationship between input detected objects through geometric attention. Quantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.



### Divide and Conquer the Embedding Space for Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.05990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.05990v1)
- **Published**: 2019-06-14 02:43:01+00:00
- **Updated**: 2019-06-14 02:43:01+00:00
- **Authors**: Artsiom Sanakoyeu, Vadim Tschernezki, Uta Büchler, Björn Ommer
- **Comment**: Source code:
  https://github.com/CompVis/metric-learning-divide-and-conquer
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 471-480
- **Summary**: Learning the embedding space, where semantically similar objects are located close together and dissimilar objects far apart, is a cornerstone of many computer vision applications. Existing approaches usually learn a single metric in the embedding space for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g. appearance, shape, color or semantic meaning. Approaches for learning a single distance metric often struggle to encode all different types of relationships and do not generalize well. In this work, we propose a novel easy-to-implement divide and conquer approach for deep metric learning, which significantly improves the state-of-the-art performance of metric learning. Our approach utilizes the embedding space more efficiently by jointly splitting the embedding space and data into $K$ smaller sub-problems. It divides both, the data and the embedding space into $K$ subsets and learns $K$ separate distance metrics in the non-overlapping subspaces of the embedding space, defined by groups of neurons in the embedding layer of the neural network. The proposed approach increases the convergence speed and improves generalization since the complexity of each sub-problem is reduced compared to the original one. We show that our approach outperforms the state-of-the-art by a large margin in retrieval, clustering and re-identification tasks on CUB200-2011, CARS196, Stanford Online Products, In-shop Clothes and PKU VehicleID datasets.



### Fusion vectors: Embedding Graph Fusions for Efficient Unsupervised Rank Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1906.06011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06011v2)
- **Published**: 2019-06-14 04:04:07+00:00
- **Updated**: 2019-07-02 01:48:06+00:00
- **Authors**: Icaro Cavalcante Dourado, Ricardo da Silva Torres
- **Comment**: None
- **Journal**: None
- **Summary**: The vast increase in amount and complexity of digital content led to a wide interest in ad-hoc retrieval systems in recent years. Complementary, the existence of heterogeneous data sources and retrieval models stimulated the proliferation of increasingly ingenious and effective rank aggregation functions. Although recently proposed rank aggregation functions are promising with respect to effectiveness, existing proposals in the area usually overlook efficiency aspects. We propose an innovative rank aggregation function that is unsupervised, intrinsically multimodal, and targeted for fast retrieval and top effectiveness performance. We introduce the concepts of embedding and indexing of graph-based rank-aggregation representation models, and their application for search tasks. Embedding formulations are also proposed for graph-based rank representations. We introduce the concept of fusion vectors, a late-fusion representation of objects based on ranks, from which an intrinsically rank-aggregation retrieval model is defined. Next, we present an approach for fast retrieval based on fusion vectors, thus promoting an efficient rank aggregation system. Our method presents top effectiveness performance among state-of-the-art related work, while bringing novel aspects of multimodality and effectiveness. Consistent speedups are achieved against the recent baselines in all datasets considered.



### Towards End-to-End Text Spotting in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1906.06013v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06013v6)
- **Published**: 2019-06-14 04:20:55+00:00
- **Updated**: 2021-06-26 03:36:25+00:00
- **Authors**: Peng Wang, Hui Li, Chunhua Shen
- **Comment**: Accepted to IEEE Trans. Pattern Analysis and Machine Intelligence
  (TPAMI). 16 pages
- **Journal**: None
- **Summary**: Text spotting in natural scene images is of great importance for many image understanding tasks. It includes two sub-tasks: text detection and recognition. In this work, we propose a unified network that simultaneously localizes and recognizes text with a single forward pass, avoiding intermediate processes such as image cropping and feature re-calculation, word separation, and character grouping.   In contrast to existing approaches that consider text detection and recognition as two distinct tasks and tackle them one by one, the proposed framework settles these two tasks concurrently. The whole framework can be trained end-to-end and is able to handle text of arbitrary shapes. The convolutional features are calculated only once and shared by both detection and recognition modules. Through multi-task training, the learned features become more discriminate and improve the overall performance. By employing the $2$D attention model in word recognition, the irregularity of text can be robustly addressed. It provides the spatial location for each character, which not only helps local feature extraction in word recognition, but also indicates an orientation angle to refine text localization. Our proposed method has achieved state-of-the-art performance on several standard text spotting benchmarks, including both regular and irregular ones.



### Utilizing the Instability in Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.06023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06023v1)
- **Published**: 2019-06-14 05:05:27+00:00
- **Updated**: 2019-06-14 05:05:27+00:00
- **Authors**: Yan Gao, Boxiao Liu, Nan Guo, Xiaochun Ye, Fang Wan, Haihang You, Dongrui Fan
- **Comment**: Accepted by CVPR 2019 Workshop
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) focuses on training object detector with only image-level annotations, and is challenging due to the gap between the supervision and the objective. Most of existing approaches model WSOD as a multiple instance learning (MIL) problem. However, we observe that the result of MIL based detector is unstable, i.e., the most confident bounding boxes change significantly when using different initializations. We quantitatively demonstrate the instability by introducing a metric to measure it, and empirically analyze the reason of instability. Although the instability seems harmful for detection task, we argue that it can be utilized to improve the performance by fusing the results of differently initialized detectors. To implement this idea, we propose an end-to-end framework with multiple detection branches, and introduce a simple fusion strategy. We further propose an orthogonal initialization method to increase the difference between detection branches. By utilizing the instability, we achieve 52.6% and 48.0% mAP on the challenging PASCAL VOC 2007 and 2012 datasets, which are both the new state-of-the-arts.



### Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary
- **Arxiv ID**: http://arxiv.org/abs/1906.06026v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06026v3)
- **Published**: 2019-06-14 05:11:12+00:00
- **Updated**: 2020-07-16 13:44:38+00:00
- **Authors**: Shashank Kotyan, Danilo Vasconcellos Vargas
- **Comment**: None
- **Journal**: None
- **Summary**: There exists a vast number of adversarial attacks and defences for machine learning algorithms of various types which makes assessing the robustness of algorithms a daunting task. To make matters worse, there is an intrinsic bias in these adversarial algorithms. Here, we organise the problems faced: a) Model Dependence, b) Insufficient Evaluation, c) False Adversarial Samples, and d) Perturbation Dependent Results). Based on this, we propose a model agnostic dual quality assessment method, together with the concept of robustness levels to tackle them. We validate the dual quality assessment on state-of-the-art neural networks (WideResNet, ResNet, AllConv, DenseNet, NIN, LeNet and CapsNet) as well as adversarial defences for image classification problem. We further show that current networks and defences are vulnerable at all levels of robustness. The proposed robustness assessment reveals that depending on the metric used (i.e., $L_0$ or $L_\infty$), the robustness may vary significantly. Hence, the duality should be taken into account for a correct evaluation. Moreover, a mathematical derivation, as well as a counter-example, suggest that $L_1$ and $L_2$ metrics alone are not sufficient to avoid spurious adversarial samples. Interestingly, the threshold attack of the proposed assessment is a novel $L_\infty$ black-box adversarial method which requires even less perturbation than the One-Pixel Attack (only $12\%$ of One-Pixel Attack's amount of perturbation) to achieve similar results.   Code is available at http://bit.ly/DualQualityAssessment.



### Low-light Image Enhancement Algorithm Based on Retinex and Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1906.06027v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06027v1)
- **Published**: 2019-06-14 05:16:54+00:00
- **Updated**: 2019-06-14 05:16:54+00:00
- **Authors**: Yangming Shi, Xiaopo Wu, Ming Zhu
- **Comment**: 9 pages,10 figures
- **Journal**: None
- **Summary**: Low-light image enhancement is generally regarded as a challenging task in image processing, especially for the complex visual tasks at night or weakly illuminated. In order to reduce the blurs or noises on the low-light images, a large number of papers have contributed to applying different technologies. Regretfully, most of them had served little purposes in coping with the extremely poor illumination parts of images or test in practice. In this work, the authors propose a novel approach for processing low-light images based on the Retinex theory and generative adversarial network (GAN), which is composed of the decomposition part for splitting the image into illumination image and reflected image, and the enhancement part for generating high-quality image. Such a discriminative network is expected to make the generated image clearer. Couples of experiments have been implemented under the circumstance of different lighting strength on the basis of Converted See-In-the-Dark (CSID) datasets, and the satisfactory results have been achieved with exceeding expectation that much encourages the authors. In a word, the proposed GAN-based network and employed Retinex theory in this work have proven to be effective in dealing with the low-light image enhancement problems, which will benefit the image processing with no doubt.



### Comparing Machine Learning Approaches for Table Recognition in Historical Register Books
- **Arxiv ID**: http://arxiv.org/abs/1906.11901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11901v1)
- **Published**: 2019-06-14 06:42:47+00:00
- **Updated**: 2019-06-14 06:42:47+00:00
- **Authors**: Stéphane Clinchant, Hervé Déjean, Jean-Luc Meunier, Eva Lang, Florian Kleber
- **Comment**: DAS 2018
- **Journal**: None
- **Summary**: We present in this paper experiments on Table Recognition in hand-written registry books. We first explain how the problem of row and column detection is modeled, and then compare two Machine Learning approaches (Conditional Random Field and Graph Convolutional Network) for detecting these table elements. Evaluation was conducted on death records provided by the Archive of the Diocese of Passau. Both methods show similar results, a 89 F1 score, a quality which allows for Information Extraction. Software and dataset are open source/data.



### Multi Scale Curriculum CNN for Context-Aware Breast MRI Malignancy Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.06058v2
- **DOI**: 10.1007/978-3-030-32251-9_54
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06058v2)
- **Published**: 2019-06-14 07:33:18+00:00
- **Updated**: 2019-06-17 06:16:35+00:00
- **Authors**: Christoph Haarburger, Michael Baumgartner, Daniel Truhn, Mirjam Broeckmann, Hannah Schneider, Simone Schrading, Christiane Kuhl, Dorit Merhof
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Classification of malignancy for breast cancer and other cancer types is usually tackled as an object detection problem: Individual lesions are first localized and then classified with respect to malignancy. However, the drawback of this approach is that abstract features incorporating several lesions and areas that are not labelled as a lesion but contain global medically relevant information are thus disregarded: especially for dynamic contrast-enhanced breast MRI, criteria such as background parenchymal enhancement and location within the breast are important for diagnosis and cannot be captured by object detection approaches properly.   In this work, we propose a 3D CNN and a multi scale curriculum learning strategy to classify malignancy globally based on an MRI of the whole breast. Thus, the global context of the whole breast rather than individual lesions is taken into account. Our proposed approach does not rely on lesion segmentations, which renders the annotation of training data much more effective than in current object detection approaches.   Achieving an AUROC of 0.89, we compare the performance of our approach to Mask R-CNN and Retina U-Net as well as a radiologist. Our performance is on par with approaches that, in contrast to our method, rely on pixelwise segmentations of lesions.



### MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.06059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06059v2)
- **Published**: 2019-06-14 07:39:03+00:00
- **Updated**: 2019-08-20 15:43:44+00:00
- **Authors**: Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi
- **Comment**: International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: We tackle the fundamentally ill-posed problem of 3D human localization from monocular RGB images. Driven by the limitation of neural networks outputting point estimates, we address the ambiguity in the task by predicting confidence intervals through a loss function based on the Laplace distribution. Our architecture is a light-weight feed-forward neural network that predicts 3D locations and corresponding confidence intervals given 2D human poses. The design is particularly well suited for small training data, cross-dataset generalization, and real-time applications. Our experiments show that we (i) outperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even outperform a stereo-based method for far-away pedestrians, and (iii) estimate meaningful confidence intervals. We further share insights on our model of uncertainty in cases of limited observations and out-of-distribution samples.



### Direct Image to Point Cloud Descriptors Matching for 6-DOF Camera Localization in Dense 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1906.06064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06064v1)
- **Published**: 2019-06-14 08:01:19+00:00
- **Updated**: 2019-06-14 08:01:19+00:00
- **Authors**: Uzair Nadeem, Mohammad A. A. K. Jalwana, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel concept to directly match feature descriptors extracted from RGB images, with feature descriptors extracted from 3D point clouds. We use this concept to localize the position and orientation (pose) of the camera of a query image in dense point clouds. We generate a dataset of matching 2D and 3D descriptors, and use it to train a proposed Descriptor-Matcher algorithm. To localize a query image in a point cloud, we extract 2D keypoints and descriptors from the query image. Then the Descriptor-Matcher is used to find the corresponding pairs 2D and 3D keypoints by matching the 2D descriptors with the pre-extracted 3D descriptors of the point cloud. This information is used in a robust pose estimation algorithm to localize the query image in the 3D point cloud. Experiments demonstrate that directly matching 2D and 3D descriptors is not only a viable idea but also achieves competitive accuracy compared to other state-of-the-art approaches for camera pose localization.



### Trimmed Action Recognition, Dense-Captioning Events in Videos, and Spatio-temporal Action Localization with Focus on ActivityNet Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1906.07016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07016v1)
- **Published**: 2019-06-14 08:39:52+00:00
- **Updated**: 2019-06-14 08:39:52+00:00
- **Authors**: Zhaofan Qiu, Dong Li, Yehao Li, Qi Cai, Yingwei Pan, Ting Yao
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1807.00686,
  arXiv:1710.08011
- **Journal**: None
- **Summary**: This notebook paper presents an overview and comparative analysis of our systems designed for the following three tasks in ActivityNet Challenge 2019: trimmed action recognition, dense-captioning events in videos, and spatio-temporal action localization.



### Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.06086v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06086v2)
- **Published**: 2019-06-14 09:17:19+00:00
- **Updated**: 2019-12-29 16:58:39+00:00
- **Authors**: Thomas Brunner, Frederik Diehl, Alois Knoll
- **Comment**: Presented at CVPR 2019 Workshop on Adversarial Machine Learning in
  Real-World Computer Vision Systems
- **Journal**: None
- **Summary**: Many optimization methods for generating black-box adversarial examples have been proposed, but the aspect of initializing said optimizers has not been considered in much detail. We show that the choice of starting points is indeed crucial, and that the performance of state-of-the-art attacks depends on it. First, we discuss desirable properties of starting points for attacking image classifiers, and how they can be chosen to increase query efficiency. Notably, we find that simply copying small patches from other images is a valid strategy. We then present an evaluation on ImageNet that clearly demonstrates the effectiveness of this method: Our initialization scheme reduces the number of queries required for a state-of-the-art Boundary Attack by 81%, significantly outperforming previous results reported for targeted black-box adversarial examples.



### Towards Compact and Robust Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.06110v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06110v1)
- **Published**: 2019-06-14 10:12:02+00:00
- **Updated**: 2019-06-14 10:12:02+00:00
- **Authors**: Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana
- **Comment**: 14 pages, 9 figures, 7 tables
- **Journal**: None
- **Summary**: Deep neural networks have achieved impressive performance in many applications but their large number of parameters lead to significant computational and storage overheads. Several recent works attempt to mitigate these overheads by designing compact networks using pruning of connections. However, we observe that most of the existing strategies to design compact networks fail to preserve network robustness against adversarial examples. In this work, we rigorously study the extension of network pruning strategies to preserve both benign accuracy and robustness of a network. Starting with a formal definition of the pruning procedure, including pre-training, weights pruning, and fine-tuning, we propose a new pruning method that can create compact networks while preserving both benign accuracy and robustness. Our method is based on two main insights: (1) we ensure that the training objectives of the pre-training and fine-tuning steps match the training objective of the desired robust model (e.g., adversarial robustness/verifiable robustness), and (2) we keep the pruning strategy agnostic to pre-training and fine-tuning objectives. We evaluate our method on four different networks on the CIFAR-10 dataset and measure benign accuracy, empirical robust accuracy, and verifiable robust accuracy. We demonstrate that our pruning method can preserve on average 93\% benign accuracy, 92.5\% empirical robust accuracy, and 85.0\% verifiable robust accuracy while compressing the tested network by 10$\times$.



### A Survey on Deep Learning Architectures for Image-based Depth Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1906.06113v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06113v1)
- **Published**: 2019-06-14 10:22:14+00:00
- **Updated**: 2019-06-14 10:22:14+00:00
- **Authors**: Hamid Laga
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating depth from RGB images is a long-standing ill-posed problem, which has been explored for decades by the computer vision, graphics, and machine learning communities. In this article, we provide a comprehensive survey of the recent developments in this field. We will focus on the works which use deep learning techniques to estimate depth from one or multiple images. Deep learning, coupled with the availability of large training datasets, have revolutionized the way the depth reconstruction problem is being approached by the research community. In this article, we survey more than 100 key contributions that appeared in the past five years, summarize the most commonly used pipelines, and discuss their benefits and limitations. In retrospect of what has been achieved so far, we also conjecture what the future may hold for learning-based depth reconstruction research.



### GAN-based Multiple Adjacent Brain MRI Slice Reconstruction for Unsupervised Alzheimer's Disease Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1906.06114v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06114v5)
- **Published**: 2019-06-14 10:26:18+00:00
- **Updated**: 2020-03-16 21:19:13+00:00
- **Authors**: Changhee Han, Leonardo Rundo, Kohei Murao, Zoltán Ádám Milacski, Kazuki Umemoto, Evis Sala, Hideki Nakayama, Shin'ichi Satoh
- **Comment**: 10 pages, 4 figures, Accepted to Lecture Notes in Bioinformatics
  (LNBI) as a volume in the Springer series
- **Journal**: None
- **Summary**: Unsupervised learning can discover various unseen diseases, relying on large-scale unannotated medical images of healthy subjects. Towards this, unsupervised methods reconstruct a single medical image to detect outliers either in the learned feature space or from high reconstruction loss. However, without considering continuity between multiple adjacent slices, they cannot directly discriminate diseases composed of the accumulation of subtle anatomical anomalies, such as Alzheimer's Disease (AD). Moreover, no study has shown how unsupervised anomaly detection is associated with disease stages. Therefore, we propose a two-step method using Generative Adversarial Network-based multiple adjacent brain MRI slice reconstruction to detect AD at various stages: (Reconstruction) Wasserstein loss with Gradient Penalty + L1 loss---trained on 3 healthy slices to reconstruct the next 3 ones---reconstructs unseen healthy/AD cases; (Diagnosis) Average/Maximum loss (e.g., L2 loss) per scan discriminates them, comparing the reconstructed/ground truth images. The results show that we can reliably detect AD at a very early stage with Area Under the Curve (AUC) 0.780 while also detecting AD at a late stage much more accurately with AUC 0.917; since our method is fully unsupervised, it should also discover and alert any anomalies including rare disease.



### Identifying Emotions from Walking using Affective and Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1906.11884v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.11884v4)
- **Published**: 2019-06-14 11:41:37+00:00
- **Updated**: 2020-01-09 21:06:50+00:00
- **Authors**: Tanmay Randhavane, Uttaran Bhattacharya, Kyra Kapsaskis, Kurt Gray, Aniket Bera, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new data-driven model and algorithm to identify the perceived emotions of individuals based on their walking styles. Given an RGB video of an individual walking, we extract his/her walking gait in the form of a series of 3D poses. Our goal is to exploit the gait features to classify the emotional state of the human into one of four emotions: happy, sad, angry, or neutral. Our perceived emotion recognition approach uses deep features learned via LSTM on labeled emotion datasets. Furthermore, we combine these features with affective features computed from gaits using posture and movement cues. These features are classified using a Random Forest Classifier. We show that our mapping between the combined feature space and the perceived emotional state provides 80.07% accuracy in identifying the perceived emotions. In addition to classifying discrete categories of emotions, our algorithm also predicts the values of perceived valence and arousal from gaits. We also present an EWalk (Emotion Walk) dataset that consists of videos of walking individuals with gaits and labeled emotions. To the best of our knowledge, this is the first gait-based model to identify perceived emotions from videos of walking individuals.



### Modality Conversion of Handwritten Patterns by Cross Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1906.06142v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06142v1)
- **Published**: 2019-06-14 11:53:39+00:00
- **Updated**: 2019-06-14 11:53:39+00:00
- **Authors**: Taichi Sumi, Brian Kenji Iwana, Hideaki Hayashi, Seiichi Uchida
- **Comment**: to appear at the International Conference on Document Analysis and
  Recognition (ICDAR) 2019
- **Journal**: None
- **Summary**: This research attempts to construct a network that can convert online and offline handwritten characters to each other. The proposed network consists of two Variational Auto-Encoders (VAEs) with a shared latent space. The VAEs are trained to generate online and offline handwritten Latin characters simultaneously. In this way, we create a cross-modal VAE (Cross-VAE). During training, the proposed Cross-VAE is trained to minimize the reconstruction loss of the two modalities, the distribution loss of the two VAEs, and a novel third loss called the space sharing loss. This third, space sharing loss is used to encourage the modalities to share the same latent space by calculating the distance between the latent variables. Through the proposed method mutual conversion of online and offline handwritten characters is possible. In this paper, we demonstrate the performance of the Cross-VAE through qualitative and quantitative analysis.



### A Partially Reversible U-Net for Memory-Efficient Volumetric Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.06148v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06148v2)
- **Published**: 2019-06-14 12:04:46+00:00
- **Updated**: 2019-06-20 15:02:02+00:00
- **Authors**: Robin Brügger, Christian F. Baumgartner, Ender Konukoglu
- **Comment**: Accepted to MICCAI 2019; Edit v2: Added reference to related work of
  Blumberg et al
- **Journal**: None
- **Summary**: One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to fit into a given memory budget. Motivated by the RevNet for image classification, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer's outputs from the subsequent layer's ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole field-of-view (FOV) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.



### Confluent-Drawing Parallel Coordinates: Web-Based Interactive Visual Analytics of Large Multi-Dimensional Data
- **Arxiv ID**: http://arxiv.org/abs/1906.10017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1906.10017v1)
- **Published**: 2019-06-14 12:14:50+00:00
- **Updated**: 2019-06-14 12:14:50+00:00
- **Authors**: Wenqiang Cui, Girts Strazdins, Hao Wang
- **Comment**: 7 pages, 13 figures
- **Journal**: None
- **Summary**: Parallel coordinates plot is one of the most popular and widely used visualization techniques for multi-dimensional data sets. Its main challenges for large-scale data sets are visual clutter and overplotting which hamper the recognition of patterns and trends in the data. In this paper, we propose a confluent drawing approach of parallel coordinates to support the web-based interactive visual analytics of large multi-dimensional data. The proposed method maps multi-dimensional data to node-link diagrams through the data binning-based clustering for each dimension. It uses density-based confluent drawing to visualize clusters and edges to reduce visual clutter and overplotting. Its rendering time is independent of the number of data items. It supports interactive visualization of large data sets without hardware acceleration in a normal web browser. Moreover, we design interactions to control the data binning process with this approach to support interactive visual analytics of large multi-dimensional data sets. Based on the proposed approach, we implement a web-based visual analytics application. The efficiency of the proposed method is examined through experiments on several data sets. The effectiveness of the proposed method is evaluated through a user study, in which two typical tasks of parallel coordinates plot are performed by participants to compare the proposed method with another parallel coordinates bundling technique. Results show that the proposed method significantly enhances the web-based interactive visual analytics of large multi-dimensional data.



### Enhancing temporal segmentation by nonlocal self-similarity
- **Arxiv ID**: http://arxiv.org/abs/1906.11335v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11335v1)
- **Published**: 2019-06-14 12:24:53+00:00
- **Updated**: 2019-06-14 12:24:53+00:00
- **Authors**: Mariella Dimiccoli, Herwig Wendt
- **Comment**: Accepted to ICIP 2019
- **Journal**: None
- **Summary**: Temporal segmentation of untrimmed videos and photo-streams is currently an active area of research in computer vision and image processing. This paper proposes a new approach to improve the temporal segmentation of photo-streams. The method consists in enhancing image representations by encoding long-range temporal dependencies. Our key contribution is to take advantage of the temporal stationarity assumption of photostreams for modeling each frame by its nonlocal self-similarity function. The proposed approach is put to test on the EDUB-Seg dataset, a standard benchmark for egocentric photostream temporal segmentation. Starting from seven different (CNN based) image features, the method yields consistent improvements in event segmentation quality, leading to an average increase of F-measure of 3.71% with respect to the state of the art.



### Global and Local Interpretability for Cardiac MRI Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.06188v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06188v2)
- **Published**: 2019-06-14 13:06:24+00:00
- **Updated**: 2019-08-12 11:19:43+00:00
- **Authors**: James R. Clough, Ilkay Oksuz, Esther Puyol-Anton, Bram Ruijsink, Andrew P. King, Julia A. Schnabel
- **Comment**: Accepted at MICCAI 2019, 9 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning methods for classifying medical images have demonstrated impressive accuracy in a wide range of tasks but often these models are hard to interpret, limiting their applicability in clinical practice. In this work we introduce a convolutional neural network model for identifying disease in temporal sequences of cardiac MR segmentations which is interpretable in terms of clinically familiar measurements. The model is based around a variational autoencoder, reducing the input into a low-dimensional latent space in which classification occurs. We then use the recently developed `concept activation vector' technique to associate concepts which are diagnostically meaningful (eg. clinical biomarkers such as `low left-ventricular ejection fraction') to certain vectors in the latent space. These concepts are then qualitatively inspected by observing the change in the image domain resulting from interpolations in the latent space in the direction of these vectors. As a result, when the model classifies images it is also capable of providing naturally interpretable concepts relevant to that classification and demonstrating the meaning of those concepts in the image domain. Our approach is demonstrated on the UK Biobank cardiac MRI dataset where we detect the presence of coronary artery disease.



### R2D2: Repeatable and Reliable Detector and Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1906.06195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06195v2)
- **Published**: 2019-06-14 13:30:40+00:00
- **Updated**: 2019-06-17 16:07:03+00:00
- **Authors**: Jerome Revaud, Philippe Weinzaepfel, César De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, Martin Humenberger
- **Comment**: None
- **Journal**: None
- **Summary**: Interest point detection and local feature description are fundamental steps in many computer vision applications. Classical methods for these tasks are based on a detect-then-describe paradigm where separate handcrafted methods are used to first identify repeatable keypoints and then represent them with a local descriptor. Neural networks trained with metric learning losses have recently caught up with these techniques, focusing on learning repeatable saliency maps for keypoint detection and learning descriptors at the detected keypoint locations. In this work, we argue that salient regions are not necessarily discriminative, and therefore can harm the performance of the description. Furthermore, we claim that descriptors should be learned only in regions for which matching can be performed with high confidence. We thus propose to jointly learn keypoint detection and description together with a predictor of the local descriptor discriminativeness. This allows us to avoid ambiguous areas and leads to reliable keypoint detections and descriptions. Our detection-and-description approach, trained with self-supervision, can simultaneously output sparse, repeatable and reliable keypoints that outperforms state-of-the-art detectors and descriptors on the HPatches dataset. It also establishes a record on the recently released Aachen Day-Night localization dataset.



### Factorized Higher-Order CNNs with an Application to Spatio-Temporal Emotion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.06196v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06196v2)
- **Published**: 2019-06-14 13:30:57+00:00
- **Updated**: 2020-03-31 23:57:47+00:00
- **Authors**: Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy Hospedales, Maja Pantic
- **Comment**: IEEE CVPR 2020
- **Journal**: None
- **Summary**: Training deep neural networks with spatio-temporal (i.e., 3D) or multidimensional convolutions of higher-order is computationally challenging due to millions of unknown parameters across dozens of layers. To alleviate this, one approach is to apply low-rank tensor decompositions to convolution kernels in order to compress the network and reduce its number of parameters. Alternatively, new convolutional blocks, such as MobileNet, can be directly designed for efficiency. In this paper, we unify these two approaches by proposing a tensor factorization framework for efficient multidimensional (separable) convolutions of higher-order. Interestingly, the proposed framework enables a novel higher-order transduction, allowing to train a network on a given domain (e.g., 2D images or N-dimensional data in general) and using transduction to generalize to higher-order data such as videos (or (N+K)-dimensional data in general), capturing for instance temporal dynamics while preserving the learnt spatial information.   We apply the proposed methodology, coined CP-Higher-Order Convolution (HO-CPConv), to spatio-temporal facial emotion analysis. Most existing facial affect models focus on static imagery and discard all temporal information. This is due to the above-mentioned burden of training 3D convolutional nets and the lack of large bodies of video data annotated by experts. We address both issues with our proposed framework. Initial training is first done on static imagery before using transduction to generalize to the temporal domain. We demonstrate superior performance on three challenging large scale affect estimation datasets, AffectNet, SEWA, and AFEW-VA.



### Deep neural network for fringe pattern filtering and normalisation
- **Arxiv ID**: http://arxiv.org/abs/1906.06224v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP, physics.optics, 68U10, 68T10, I.2.6; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1906.06224v2)
- **Published**: 2019-06-14 14:36:12+00:00
- **Updated**: 2020-10-28 01:53:59+00:00
- **Authors**: Alan Reyes-Figueroa, Mariano Rivera
- **Comment**: 19 pages (including references), 21 figures Comments: Full
  mathematical details of the arquitecture were replaced by tables in a more
  readable form (Tables 1-3). Added new series of experiments with high-levels
  of noise in Subsection 4.4. References added
- **Journal**: None
- **Summary**: We propose a new framework for processing Fringe Patterns (FP). Our novel approach builds upon the hypothesis that the denoising and normalisation of FPs can be learned by a deep neural network if enough pairs of corrupted and ideal FPs are provided. The main contributions of this paper are the following: (1) We propose the use of the U-net neural network architecture for FP normalisation tasks; (2) we propose a modification for the distribution of weights in the U-net, called here the V-net model, which is more convenient for reconstruction tasks, and we conduct extensive experimental evidence in which the V-net produces high-quality results for FP filtering and normalisation. (3) We also propose two modifications of the V-net scheme, namely, a residual version called ResV-net and a fast operating version of the V-net, to evaluate the potential improvements when modify our proposal. We evaluate the performance of our methods in various scenarios: FPs corrupted with different degrees of noise, and corrupted with different noise distributions. We compare our methodology versus other state-of-the-art methods. The experimental results (on both synthetic and real data) demonstrate the capabilities and potential of this new paradigm for processing interferograms.



### Universal Barcode Detector via Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.06281v2
- **DOI**: 10.1109/ICDAR.2019.00139
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06281v2)
- **Published**: 2019-06-14 16:44:10+00:00
- **Updated**: 2019-06-17 07:07:41+00:00
- **Authors**: Andrey Zharkov, Ivan Zagaynov
- **Comment**: None
- **Journal**: 2019 International Conference on Document Analysis and Recognition
  (ICDAR)
- **Summary**: Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.



### Realistic Speech-Driven Facial Animation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1906.06337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1906.06337v1)
- **Published**: 2019-06-14 16:52:27+00:00
- **Updated**: 2019-06-14 16:52:27+00:00
- **Authors**: Konstantinos Vougioukas, Stavros Petridis, Maja Pantic
- **Comment**: arXiv admin note: text overlap with arXiv:1805.09313
- **Journal**: None
- **Summary**: Speech-driven facial animation is the process that automatically synthesizes talking characters based on speech signals. The majority of work in this domain creates a mapping from audio features to visual features. This approach often requires post-processing using computer graphics techniques to produce realistic albeit subject dependent results. We present an end-to-end system that generates videos of a talking head, using only a still image of a person and an audio clip containing speech, without relying on handcrafted intermediate features. Our method generates videos which have (a) lip movements that are in sync with the audio and (b) natural facial expressions such as blinks and eyebrow movements. Our temporal GAN uses 3 discriminators focused on achieving detailed frames, audio-visual synchronization, and realistic expressions. We quantify the contribution of each component in our model using an ablation study and we provide insights into the latent representation of the model. The generated videos are evaluated based on sharpness, reconstruction quality, lip-reading accuracy, synchronization as well as their ability to generate natural blinks.



### Video-Driven Speech Reconstruction using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.06301v1
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1906.06301v1)
- **Published**: 2019-06-14 17:15:27+00:00
- **Updated**: 2019-06-14 17:15:27+00:00
- **Authors**: Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Speech is a means of communication which relies on both audio and visual information. The absence of one modality can often lead to confusion or misinterpretation of information. In this paper we present an end-to-end temporal model capable of directly synthesising audio from silent video, without needing to transform to-and-from intermediate features. Our proposed approach, based on GANs is capable of producing natural sounding, intelligible speech which is synchronised with the video. The performance of our model is evaluated on the GRID dataset for both speaker dependent and speaker independent scenarios. To the best of our knowledge this is the first method that maps video directly to raw audio and the first to produce intelligible speech when tested on previously unseen speakers. We evaluate the synthesised audio not only based on the sound quality but also on the accuracy of the spoken words.



### A Signal Propagation Perspective for Pruning Neural Networks at Initialization
- **Arxiv ID**: http://arxiv.org/abs/1906.06307v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06307v2)
- **Published**: 2019-06-14 17:26:29+00:00
- **Updated**: 2020-02-16 18:23:41+00:00
- **Authors**: Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, Philip H. S. Torr
- **Comment**: ICLR 2020
- **Journal**: None
- **Summary**: Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, by noting connection sensitivity as a form of gradient, we formally characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. Moreover, we analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Our modifications to the existing pruning at initialization method lead to improved results on all tested network models for image classification tasks. Furthermore, we empirically study the effect of supervision for pruning and demonstrate that our signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures.



### Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1906.06310v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06310v3)
- **Published**: 2019-06-14 17:36:11+00:00
- **Updated**: 2020-02-15 07:29:02+00:00
- **Authors**: Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger
- **Comment**: Accepted to International Conference on Learning Representations
  (ICLR) 2020
- **Journal**: None
- **Summary**: Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but extremely sparse LiDAR sensors, which alone provide insufficient information for 3D detection, to de-bias our depth estimation. We propose a depth-propagation algorithm, guided by the initial depth estimates, to diffuse these few exact measurements across the entire depth map. We show on the KITTI object detection benchmark that our combined approach yields substantial improvements in depth estimation and stereo-based 3D object detection --- outperforming the previous state-of-the-art detection accuracy for faraway objects by 40%. Our code is available at https://github.com/mileyan/Pseudo_Lidar_V2.



### Connecting Touch and Vision via Cross-Modal Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.06322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.06322v1)
- **Published**: 2019-06-14 17:55:54+00:00
- **Updated**: 2019-06-14 17:55:54+00:00
- **Authors**: Yunzhu Li, Jun-Yan Zhu, Russ Tedrake, Antonio Torralba
- **Comment**: Accepted to CVPR 2019. Project Page: http://visgel.csail.mit.edu/
- **Journal**: None
- **Summary**: Humans perceive the world using multi-modal sensory inputs such as vision, audition, and touch. In this work, we investigate the cross-modal connection between vision and touch. The main challenge in this cross-domain modeling task lies in the significant scale discrepancy between the two: while our eyes perceive an entire visual scene at once, humans can only feel a small region of an object at any given moment. To connect vision and touch, we introduce new tasks of synthesizing plausible tactile signals from visual inputs as well as imagining how we interact with objects given tactile data as input. To accomplish our goals, we first equip robots with both visual and tactile sensors and collect a large-scale dataset of corresponding vision and tactile image sequences. To close the scale gap, we present a new conditional adversarial model that incorporates the scale and location information of the touch. Human perceptual studies demonstrate that our model can produce realistic visual images from tactile data and vice versa. Finally, we present both qualitative and quantitative experimental results regarding different system designs, as well as visualizing the learned representations of our model.



### Instance Segmentation with Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.06392v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06392v1)
- **Published**: 2019-06-14 20:29:38+00:00
- **Updated**: 2019-06-14 20:29:38+00:00
- **Authors**: Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro, David Vazquez, Mark Schmidt
- **Comment**: None
- **Journal**: None
- **Summary**: Instance segmentation methods often require costly per-pixel labels. We propose a method that only requires point-level annotations. During training, the model only has access to a single pixel label per object, yet the task is to output full segmentation masks. To address this challenge, we construct a network with two branches: (1) a localization network (L-Net) that predicts the location of each object; and (2) an embedding network (E-Net) that learns an embedding space where pixels of the same object are close. The segmentation masks for the located objects are obtained by grouping pixels with similar embeddings. At training time, while L-Net only requires point-level annotations, E-Net uses pseudo-labels generated by a class-agnostic object proposal method. We evaluate our approach on PASCAL VOC, COCO, KITTI and CityScapes datasets. The experiments show that our method (1) obtains competitive results compared to fully-supervised methods in certain scenarios; (2) outperforms fully- and weakly- supervised methods with a fixed annotation budget; and (3) is a first strong baseline for instance segmentation with point-level supervision.



### Signatures in Shape Analysis: an Efficient Approach to Motion Identification
- **Arxiv ID**: http://arxiv.org/abs/1906.06406v1
- **DOI**: 10.1007/978-3-030-26980-7_3
- **Categories**: **math.DG**, cs.CV, stat.ML, 68T10, 62H30, 22F30
- **Links**: [PDF](http://arxiv.org/pdf/1906.06406v1)
- **Published**: 2019-06-14 21:14:34+00:00
- **Updated**: 2019-06-14 21:14:34+00:00
- **Authors**: Elena Celledoni, Pål Erik Lystad, Nikolas Tapia
- **Comment**: 7 pages, 3 figures. Conference paper for Geometric Science of
  Information 2019
- **Journal**: None
- **Summary**: Signatures provide a succinct description of certain features of paths in a reparametrization invariant way. We propose a method for classifying shapes based on signatures, and compare it to current approaches based on the SRV transform and dynamic programming.



### PredNet and Predictive Coding: A Critical Review
- **Arxiv ID**: http://arxiv.org/abs/1906.11902v3
- **DOI**: 10.1145/3372278.3390694
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11902v3)
- **Published**: 2019-06-14 21:58:00+00:00
- **Updated**: 2020-05-18 09:52:22+00:00
- **Authors**: Roshan Rane, Edit Szügyi, Vageesh Saxena, André Ofner, Sebastian Stober
- **Comment**: None
- **Journal**: None
- **Summary**: PredNet, a deep predictive coding network developed by Lotter et al., combines a biologically inspired architecture based on the propagation of prediction error with self-supervised representation learning in video. While the architecture has drawn a lot of attention and various extensions of the model exist, there is a lack of a critical analysis. We fill in the gap by evaluating PredNet both as an implementation of the predictive coding theory and as a self-supervised video prediction model using a challenging video action classification dataset. We design an extended model to test if conditioning future frame predictions on the action class of the video improves the model performance. We show that PredNet does not yet completely follow the principles of predictive coding. The proposed top-down conditioning leads to a performance gain on synthetic data, but does not scale up to the more complex real-world action classification dataset. Our analysis is aimed at guiding future research on similar architectures based on the predictive coding theory.



### Fixing the train-test resolution discrepancy
- **Arxiv ID**: http://arxiv.org/abs/1906.06423v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06423v4)
- **Published**: 2019-06-14 22:27:30+00:00
- **Updated**: 2022-01-20 11:02:01+00:00
- **Authors**: Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Hervé Jégou
- **Comment**: None
- **Journal**: None
- **Summary**: Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time.   We then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images.   Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.



### Multi-Adversarial Variational Autoencoder Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.06430v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06430v1)
- **Published**: 2019-06-14 23:04:44+00:00
- **Updated**: 2019-06-14 23:04:44+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Demetri Terzopoulos
- **Comment**: None
- **Journal**: None
- **Summary**: The unsupervised training of GANs and VAEs has enabled them to generate realistic images mimicking real-world distributions and perform image-based unsupervised clustering or semi-supervised classification. Combining the power of these two generative models, we introduce Multi-Adversarial Variational autoEncoder Networks (MAVENs), a novel network architecture that incorporates an ensemble of discriminators in a VAE-GAN network, with simultaneous adversarial learning and variational inference. We apply MAVENs to the generation of synthetic images and propose a new distribution measure to quantify the quality of the generated images. Our experimental results using datasets from the computer vision and medical imaging domains---Street View House Numbers, CIFAR-10, and Chest X-Ray datasets---demonstrate competitive performance against state-of-the-art semi-supervised models both in image generation and classification tasks.



### Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias
- **Arxiv ID**: http://arxiv.org/abs/1906.06439v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06439v3)
- **Published**: 2019-06-14 23:50:04+00:00
- **Updated**: 2020-10-03 21:33:55+00:00
- **Authors**: Emily Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, Andrew Zaldivar
- **Comment**: Presented at CVPR 2019 Workshop on Fairness Accountability
  Transparency and Ethics in Computer Vision
- **Journal**: None
- **Summary**: Facial analysis models are increasingly used in applications that have serious impacts on people's lives, ranging from authentication to surveillance tracking. It is therefore critical to develop techniques that can reveal unintended biases in facial classifiers to help guide the ethical use of facial analysis technology. This work proposes a framework called \textit{image counterfactual sensitivity analysis}, which we explore as a proof-of-concept in analyzing a smiling attribute classifier trained on faces of celebrities. The framework utilizes counterfactuals to examine how a classifier's prediction changes if a face characteristic slightly changes. We leverage recent advances in generative adversarial networks to build a realistic generative model of face images that affords controlled manipulation of specific image characteristics. We then introduce a set of metrics that measure the effect of manipulating a specific property on the output of the trained classifier. Empirically, we find several different factors of variation that affect the predictions of the smiling classifier. This proof-of-concept demonstrates potential ways generative models can be leveraged for fine-grained analysis of bias and fairness.



