# Arxiv Papers in cs.CV on 2019-06-09
### In Situ Cane Toad Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.03547v2
- **DOI**: 10.1109/DICTA.2018.8615780
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03547v2)
- **Published**: 2019-06-09 01:10:19+00:00
- **Updated**: 2019-09-06 05:11:15+00:00
- **Authors**: Dmitry A. Konovalov, Simindokht Jahangard, Lin Schwarzkopf
- **Comment**: Accepted for DICTA2018 https://doi.org/10.1109/DICTA.2018.8615780
- **Journal**: 2018 Digital Image Computing: Techniques and Applications (DICTA),
  Canberra, Australia, 2018, pp. 1-7
- **Summary**: Cane toads are invasive, toxic to native predators, compete with native insectivores, and have a devastating impact on Australian ecosystems, prompting the Australian government to list toads as a key threatening process under the Environment Protection and Biodiversity Conservation Act 1999. Mechanical cane toad traps could be made more native-fauna friendly if they could distinguish invasive cane toads from native species. Here we designed and trained a Convolution Neural Network (CNN) starting from the Xception CNN. The XToadGmp toad-recognition CNN we developed was trained end-to-end using heat-map Gaussian targets. After training, XToadGmp required minimum image pre/post-processing and when tested on 720x1280 shaped images, it achieved 97.1% classification accuracy on 1863 toad and 2892 not-toad test images, which were not used in training.



### Four Things Everyone Should Know to Improve Batch Normalization
- **Arxiv ID**: http://arxiv.org/abs/1906.03548v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03548v2)
- **Published**: 2019-06-09 01:14:48+00:00
- **Updated**: 2020-02-14 05:20:53+00:00
- **Authors**: Cecilia Summers, Michael J. Dinneen
- **Comment**: ICLR 2020, 18 pages
- **Journal**: None
- **Summary**: A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters gamma and beta; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet.



### Cross-view Semantic Segmentation for Sensing Surroundings
- **Arxiv ID**: http://arxiv.org/abs/1906.03560v3
- **DOI**: 10.1109/LRA.2020.3004325
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03560v3)
- **Published**: 2019-06-09 04:18:03+00:00
- **Updated**: 2020-06-18 06:56:18+00:00
- **Authors**: Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, Bolei Zhou
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 5 , Issue: 3 , July
  2020 )
- **Summary**: Sensing surroundings plays a crucial role in human spatial perception, as it extracts the spatial configuration of objects as well as the free space from the observations. To facilitate the robot perception with such a surrounding sensing capability, we introduce a novel visual task called Cross-view Semantic Segmentation as well as a framework named View Parsing Network (VPN) to address it. In the cross-view semantic segmentation task, the agent is trained to parse the first-view observations into a top-down-view semantic map indicating the spatial location of all the objects at pixel-level. The main issue of this task is that we lack the real-world annotations of top-down-view data. To mitigate this, we train the VPN in 3D graphics environment and utilize the domain adaptation technique to transfer it to handle real-world data. We evaluate our VPN on both synthetic and real-world agents. The experimental results show that our model can effectively make use of the information from different views and multi-modalities to understanding spatial information. Our further experiment on a LoCoBot robot shows that our model enables the surrounding sensing capability from 2D image input. Code and demo videos can be found at \url{https://view-parsing-network.github.io}.



### Joint Visual Grounding with Language Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1906.03561v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03561v2)
- **Published**: 2019-06-09 04:29:06+00:00
- **Updated**: 2020-04-10 16:05:29+00:00
- **Authors**: Daqing Liu, Hanwang Zhang, Zheng-Jun Zha, Meng Wang, Qianru Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Visual grounding is a task to ground referring expressions in images, e.g., localize "the white truck in front of the yellow one". To resolve this task fundamentally, the model should first find out the contextual objects (e.g., the "yellow" truck) and then exploit them to disambiguate the referent from other similar objects by using the attributes and relationships (e.g., "white", "yellow", "in front of"). However, due to the lack of annotations on contextual objects and their relationships, existing methods degenerate the above joint grounding process into a holistic association between the expression and regions, thus suffering from unsatisfactory performance and limited interpretability. In this paper, we alleviate the missing-annotation problem and enable the joint reasoning by leveraging the language scene graph which covers both labeled referent and unlabeled contexts (other objects, attributes, and relationships). Specifically, the language scene graph is a graphical representation where the nodes are objects with attributes and the edges are relationships. We construct a factor graph based on it and then perform marginalization over the graph, such that we can ground both referent and contexts on corresponding image regions to achieve the joint visual grounding (JVG). Experimental results demonstrate that the proposed approach is effective and interpretable, e.g., on three benchmarks, it outperforms the state-of-the-art methods while offers a complete grounding of all the objects mentioned in the referring expression.



### Adversarial Attack Generation Empowered by Min-Max Optimization
- **Arxiv ID**: http://arxiv.org/abs/1906.03563v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03563v3)
- **Published**: 2019-06-09 04:32:13+00:00
- **Updated**: 2021-11-01 17:21:53+00:00
- **Authors**: Jingkang Wang, Tianyun Zhang, Sijia Liu, Pin-Yu Chen, Jiacen Xu, Makan Fardad, Bo Li
- **Comment**: NeurIPS 2021
- **Journal**: None
- **Summary**: The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness. Nevertheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the adversarial context. In this paper, we show how a general framework of min-max optimization over multiple domains can be leveraged to advance the design of different types of adversarial attacks. In particular, given a set of risk sources, minimizing the worst-case attack loss can be reformulated as a min-max problem by introducing domain weights that are maximized over the probability simplex of the domain set. We showcase this unified framework in three attack generation problems -- attacking model ensembles, devising universal perturbation under multiple inputs, and crafting attacks resilient to data transformations. Extensive experiments demonstrate that our approach leads to substantial attack improvement over the existing heuristic strategies as well as robustness improvement over state-of-the-art defense methods trained to be robust against multiple perturbation types. Furthermore, we find that the self-adjusted domain weights learned from our min-max framework can provide a holistic tool to explain the difficulty level of attack across domains. Code is available at https://github.com/wangjksjtu/minmax-adv.



### Learning Deep Multi-Level Similarity for Thermal Infrared Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.03568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03568v1)
- **Published**: 2019-06-09 05:09:05+00:00
- **Updated**: 2019-06-09 05:09:05+00:00
- **Authors**: Qiao Liu, Xin Li, Zhenyu He, Nana Fan, Di Yuan, Hongpeng Wang
- **Comment**: 18 pages
- **Journal**: None
- **Summary**: Existing deep Thermal InfraRed (TIR) trackers only use semantic features to describe the TIR object, which lack the sufficient discriminative capacity for handling distractors. This becomes worse when the feature extraction network is only trained on RGB images.To address this issue, we propose a multi-level similarity model under a Siamese framework for robust TIR object tracking. Specifically, we compute different pattern similarities on two convolutional layers using the proposed multi-level similarity network. One of them focuses on the global semantic similarity and the other computes the local structural similarity of the TIR object. These two similarities complement each other and hence enhance the discriminative capacity of the network for handling distractors. In addition, we design a simple while effective relative entropy based ensemble subnetwork to integrate the semantic and structural similarities. This subnetwork can adaptive learn the weights of the semantic and structural similarities at the training stage. To further enhance the discriminative capacity of the tracker, we construct the first large scale TIR video sequence dataset for training the proposed model. The proposed TIR dataset not only benefits the training for TIR tracking but also can be applied to numerous TIR vision tasks. Extensive experimental results on the VOT-TIR2015 and VOT-TIR2017 benchmarks demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.



### A Preliminary Study on Data Augmentation of Deep Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.11887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11887v1)
- **Published**: 2019-06-09 07:21:09+00:00
- **Updated**: 2019-06-09 07:21:09+00:00
- **Authors**: Benlin Hu, Cheng Lei, Dong Wang, Shu Zhang, Zhenyu Chen
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: Deep learning models have a large number of freeparameters that need to be calculated by effective trainingof the models on a great deal of training data to improvetheir generalization performance. However, data obtaining andlabeling is expensive in practice. Data augmentation is one of themethods to alleviate this problem. In this paper, we conduct apreliminary study on how three variables (augmentation method,augmentation rate and size of basic dataset per label) can affectthe accuracy of deep learning for image classification. The studyprovides some guidelines: (1) it is better to use transformationsthat alter the geometry of the images rather than those justlighting and color. (2) 2-3 times augmentation rate is good enoughfor training. (3) the smaller amount of data, the more obviouscontributions could have.



### A Hierarchical Network for Diverse Trajectory Proposals
- **Arxiv ID**: http://arxiv.org/abs/1906.03584v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03584v1)
- **Published**: 2019-06-09 07:26:50+00:00
- **Updated**: 2019-06-09 07:26:50+00:00
- **Authors**: Sriram N. N., Gourav Kumar, Abhay Singh, M. Siva Karthik, Saket Saurav Brojeshwar Bhowmick, K. Madhava Krishna
- **Comment**: Accepted in IV 2019
- **Journal**: None
- **Summary**: Autonomous explorative robots frequently encounter scenarios where multiple future trajectories can be pursued. Often these are cases with multiple paths around an obstacle or trajectory options towards various frontiers. Humans in such situations can inherently perceive and reason about the surrounding environment to identify several possibilities of either manoeuvring around the obstacles or moving towards various frontiers. In this work, we propose a 2 stage Convolutional Neural Network architecture which mimics such an ability to map the perceived surroundings to multiple trajectories that a robot can choose to traverse. The first stage is a Trajectory Proposal Network which suggests diverse regions in the environment which can be occupied in the future. The second stage is a Trajectory Sampling network which provides a finegrained trajectory over the regions proposed by Trajectory Proposal Network. We evaluate our framework in diverse and complicated real life settings. For the outdoor case, we use the KITTI dataset and our own outdoor driving dataset. In the indoor setting, we use an autonomous drone to navigate various scenarios and also a ground robot which can explore the environment using the trajectories proposed by our framework. Our experiments suggest that the framework is able to develop a semantic understanding of the obstacles, open regions and identify diverse trajectories that a robot can traverse. Our comparisons portray the performance gain of the proposed architecture over a diverse set of methods against which it is compared.



### Coloring With Limited Data: Few-Shot Colorization via Memory-Augmented Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.11888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11888v1)
- **Published**: 2019-06-09 07:26:54+00:00
- **Updated**: 2019-06-09 07:26:54+00:00
- **Authors**: Seungjoo Yoo, Hyojin Bahng, Sunghyo Chung, Junsoo Lee, Jaehyuk Chang, Jaegul Choo
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Despite recent advancements in deep learning-based automatic colorization, they are still limited when it comes to few-shot learning. Existing models require a significant amount of training data. To tackle this issue, we present a novel memory-augmented colorization model MemoPainter that can produce high-quality colorization with limited data. In particular, our model is able to capture rare instances and successfully colorize them. We also propose a novel threshold triplet loss that enables unsupervised training of memory networks without the need of class labels. Experiments show that our model has superior quality in both few-shot and one-shot colorization tasks.



### What and Where to Translate: Local Mask-based Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.03598v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03598v2)
- **Published**: 2019-06-09 09:07:01+00:00
- **Updated**: 2020-01-21 14:04:11+00:00
- **Authors**: Wonwoong Cho, Seunghwan Choi, Junwoo Park, David Keetae Park, Tao Qin, Jaegul Choo
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, image-to-image translation has obtained significant attention. Among many, those approaches based on an exemplar image that contains the target style information has been actively studied, due to its capability to handle multimodality as well as its applicability in practical use. However, two intrinsic problems exist in the existing methods: what and where to transfer. First, those methods extract style from an entire exemplar which includes noisy information, which impedes a translation model from properly extracting the intended style of the exemplar. That is, we need to carefully determine what to transfer from the exemplar. Second, the extracted style is applied to the entire input image, which causes unnecessary distortion in irrelevant image regions. In response, we need to decide where to transfer the extracted style. In this paper, we propose a novel approach that extracts out a local mask from the exemplar that determines what style to transfer, and another local mask from the input image that determines where to transfer the extracted style. The main novelty of this paper lies in (1) the highway adaptive instance normalization technique and (2) an end-to-end translation framework which achieves an outstanding performance in reflecting a style of an exemplar. We demonstrate the quantitative and qualitative evaluation results to confirm the advantages of our proposed approach.



### Semi-supervised Complex-valued GAN for Polarimetric SAR Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.03605v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03605v1)
- **Published**: 2019-06-09 09:30:36+00:00
- **Updated**: 2019-06-09 09:30:36+00:00
- **Authors**: Qigong Sun, Xiufang Li, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: Polarimetric synthetic aperture radar (PolSAR) images are widely used in disaster detection and military reconnaissance and so on. However, their interpretation faces some challenges, e.g., deficiency of labeled data, inadequate utilization of data information and so on. In this paper, a complex-valued generative adversarial network (GAN) is proposed for the first time to address these issues. The complex number form of model complies with the physical mechanism of PolSAR data and in favor of utilizing and retaining amplitude and phase information of PolSAR data. GAN architecture and semi-supervised learning are combined to handle deficiency of labeled data. GAN expands training data and semi-supervised learning is used to train network with generated, labeled and unlabeled data. Experimental results on two benchmark data sets show that our model outperforms existing state-of-the-art models, especially for conditions with fewer labeled data.



### Pixel DAG-Recurrent Neural Network for Spectral-Spatial Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.03607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03607v1)
- **Published**: 2019-06-09 09:40:55+00:00
- **Updated**: 2019-06-09 09:40:55+00:00
- **Authors**: Xiufang Li, Qigong Sun, Lingling Li, Zhongle Ren, Fang Liu, Licheng Jiao
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting rich spatial and spectral features contributes to improve the classification accuracy of hyperspectral images (HSIs). In this paper, based on the mechanism of the population receptive field (pRF) in human visual cortex, we further utilize the spatial correlation of pixels in images and propose pixel directed acyclic graph recurrent neural network (Pixel DAG-RNN) to extract and apply spectral-spatial features for HSIs classification. In our model, an undirected cyclic graph (UCG) is used to represent the relevance connectivity of pixels in an image patch, and four DAGs are used to approximate the spatial relationship of UCGs. In order to avoid overfitting, weight sharing and dropout are adopted. The higher classification performance of our model on HSIs classification has been verified by experiments on three benchmark data sets.



### Distilling Object Detectors with Fine-grained Feature Imitation
- **Arxiv ID**: http://arxiv.org/abs/1906.03609v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03609v1)
- **Published**: 2019-06-09 09:57:26+00:00
- **Updated**: 2019-06-09 09:57:26+00:00
- **Authors**: Tao Wang, Li Yuan, Xiaopeng Zhang, Jiashi Feng
- **Comment**: accepted at CVPR2019
  code:https://github.com/twangnh/Distilling-Object-Detectors
- **Journal**: None
- **Summary**: State-of-the-art CNN based recognition models are often computationally prohibitive to deploy on low-end devices. A promising high level approach tackling this limitation is knowledge distillation, which let small student model mimic cumbersome teacher model's output to get improved generalization. However, related methods mainly focus on simple task of classification while do not consider complex tasks like object detection. We show applying the vanilla knowledge distillation to detection model gets minor gain. To address the challenge of distilling knowledge in detection model, we propose a fine-grained feature imitation method exploiting the cross-location discrepancy of feature response. Our intuition is that detectors care more about local near object regions. Thus the discrepancy of feature response on the near object anchor locations reveals important information of how teacher model tends to generalize. We design a novel mechanism to estimate those locations and let student model imitate the teacher on them to get enhanced performance. We first validate the idea on a developed lightweight toy detector which carries simplest notion of current state-of-the-art anchor based detection models on challenging KITTI dataset, our method generates up to 15% boost of mAP for the student model compared to the non-imitated counterpart. We then extensively evaluate the method with Faster R-CNN model under various scenarios with common object detection benchmark of Pascal VOC and COCO, imitation alleviates up to 74% performance drop of student model compared to teacher. Codes released at https://github.com/twangnh/Distilling-Object-Detectors



### Soft-ranking Label Encoding for Robust Facial Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.03625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03625v1)
- **Published**: 2019-06-09 12:16:57+00:00
- **Updated**: 2019-06-09 12:16:57+00:00
- **Authors**: Xusheng Zeng, Changxing Ding, Yonggang Wen, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic facial age estimation can be used in a wide range of real-world applications. However, this process is challenging due to the randomness and slowness of the aging process. Accordingly, in this paper, we propose a comprehensive framework aimed at overcoming the challenges associated with facial age estimation. First, we propose a novel age encoding method, referred to as 'Soft-ranking', which encodes two important properties of facial age, i.e., the ordinal property and the correlation between adjacent ages. Therefore, Soft-ranking provides a richer supervision signal for training deep models. Moreover, we also carefully analyze existing evaluation protocols for age estimation, finding that the overlap in identity between the training and testing sets affects the relative performance of different age encoding methods. Finally, since existing face databases for age estimation are generally small, deep models tend to suffer from an overfitting problem. To address this issue, we propose a novel regularization strategy to encourage deep models to learn more robust features from facial parts for age estimation purposes. Extensive experiments indicate that the proposed techniques improve the age estimation performance; moreover, we achieve state-of-the-art performance on the three most popular age databases, $i.e.$, Morph II, CLAP2015, and CLAP2016.



### Movable-Object-Aware Visual SLAM via Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.03629v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03629v2)
- **Published**: 2019-06-09 12:50:10+00:00
- **Updated**: 2019-07-31 09:00:52+00:00
- **Authors**: Ting Sun, Yuxiang Sun, Ming Liu, Dit-Yan Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: Moving objects can greatly jeopardize the performance of a visual simultaneous localization and mapping (vSLAM) system which relies on the static-world assumption. Motion removal have seen successful on solving this problem. Two main streams of solutions are based on either geometry constraints or deep semantic segmentation neural network. The former rely on static majority assumption, and the latter require labor-intensive pixel-wise annotations. In this paper we propose to adopt a novel weakly-supervised semantic segmentation method. The segmentation mask is obtained from a CNN pre-trained with image-level class labels only. Thus, we leverage the power of deep semantic segmentation CNNs, while avoid requiring expensive annotations for training. We integrate our motion removal approach with the ORB-SLAM2 system. Experimental results on the TUM RGB-D and the KITTI stereo datasets demonstrate our superiority over the state-of-the-art.



### Overcoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.03631v2
- **DOI**: 10.1109/CVPR.2019.00731
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03631v2)
- **Published**: 2019-06-09 13:02:26+00:00
- **Updated**: 2020-06-08 16:19:30+00:00
- **Authors**: Osama Makansi, Eddy Ilg, Özgün Cicek, Thomas Brox
- **Comment**: In CVPR 2019
- **Journal**: None
- **Summary**: Future prediction is a fundamental principle of intelligence that helps plan actions and avoid possible dangers. As the future is uncertain to a large extent, modeling the uncertainty and multimodality of the future states is of great relevance. Existing approaches are rather limited in this regard and mostly yield a single hypothesis of the future or, at the best, strongly constrained mixture components that suffer from instabilities in training and mode collapse. In this work, we present an approach that involves the prediction of several samples of the future with a winner-takes-all loss and iterative grouping of samples to multiple modes. Moreover, we discuss how to evaluate predicted multimodal distributions, including the common real scenario, where only a single sample from the ground-truth distribution is available for evaluation. We show on synthetic and real data that the proposed approach triggers good estimates of multimodal distributions and avoids mode collapse. Source code is available at $\href{https://github.com/lmb-freiburg/Multimodal-Future-Prediction}{\text{this https URL.}}$



### Consensus Neural Network for Medical Imaging Denoising with Only Noisy Training Samples
- **Arxiv ID**: http://arxiv.org/abs/1906.03639v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03639v1)
- **Published**: 2019-06-09 13:37:34+00:00
- **Updated**: 2019-06-09 13:37:34+00:00
- **Authors**: Dufan Wu, Kuang Gong, Kyungsang Kim, Quanzheng Li
- **Comment**: 9 pages, 2 figures, accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Deep neural networks have been proved efficient for medical image denoising. Current training methods require both noisy and clean images. However, clean images cannot be acquired for many practical medical applications due to naturally noisy signal, such as dynamic imaging, spectral computed tomography, arterial spin labeling magnetic resonance imaging, etc. In this paper we proposed a training method which learned denoising neural networks from noisy training samples only. Training data in the acquisition domain was split to two subsets and the network was trained to map one noisy set to the other. A consensus loss function was further proposed to efficiently combine the outputs from both subsets. A mathematical proof was provided that the proposed training scheme was equivalent to training with noisy and clean samples when the noise in the two subsets was uncorrelated and zero-mean. The method was validated on Low-dose CT Challenge dataset and NYU MRI dataset and achieved improved performance compared to existing unsupervised methods.



### Unsupervised Primitive Discovery for Improved 3D Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/1906.03650v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03650v1)
- **Published**: 2019-06-09 14:32:09+00:00
- **Updated**: 2019-06-09 14:32:09+00:00
- **Authors**: Salman H. Khan, Yulan Guo, Munawar Hayat, Nick Barnes
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: 3D shape generation is a challenging problem due to the high-dimensional output space and complex part configurations of real-world objects. As a result, existing algorithms experience difficulties in accurate generative modeling of 3D shapes. Here, we propose a novel factorized generative model for 3D shape generation that sequentially transitions from coarse to fine scale shape generation. To this end, we introduce an unsupervised primitive discovery algorithm based on a higher-order conditional random field model. Using the primitive parts for shapes as attributes, a parameterized 3D representation is modeled in the first stage. This representation is further refined in the next stage by adding fine scale details to shape. Our results demonstrate improved representation ability of the generative model and better quality samples of newly generated 3D shapes. Further, our primitive generation approach can accurately parse common objects into a simplified representation.



### HGC: Hierarchical Group Convolution for Highly Efficient Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.03657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03657v1)
- **Published**: 2019-06-09 15:24:42+00:00
- **Updated**: 2019-06-09 15:24:42+00:00
- **Authors**: Xukai Xie, Yuan Zhou, Sun-Yuan Kung
- **Comment**: arXiv admin note: text overlap with arXiv:1711.09224,
  arXiv:1904.00346, arXiv:1811.07083 by other authors
- **Journal**: None
- **Summary**: Group convolution works well with many deep convolutional neural networks (CNNs) that can effectively compress the model by reducing the number of parameters and computational cost. Using this operation, feature maps of different group cannot communicate, which restricts their representation capability. To address this issue, in this work, we propose a novel operation named Hierarchical Group Convolution (HGC) for creating computationally efficient neural networks. Different from standard group convolution which blocks the inter-group information exchange and induces the severe performance degradation, HGC can hierarchically fuse the feature maps from each group and leverage the inter-group information effectively. Taking advantage of the proposed method, we introduce a family of compact networks called HGCNets. Compared to networks using standard group convolution, HGCNets have a huge improvement in accuracy at the same model size and complexity level. Extensive experimental results on the CIFAR dataset demonstrate that HGCNets obtain significant reduction of parameters and computational cost to achieve comparable performance over the prior CNN architectures designed for mobile devices such as MobileNet and ShuffleNet.



### An Attention-based Recurrent Convolutional Network for Vehicle Taillight Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.03683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03683v1)
- **Published**: 2019-06-09 18:08:49+00:00
- **Updated**: 2019-06-09 18:08:49+00:00
- **Authors**: Kuan-Hui Lee, Takaaki Tagawa, Jia-En M. Pan, Adrien Gaidon, Bertrand Douillard
- **Comment**: Accepted by IV 2019
- **Journal**: None
- **Summary**: Vehicle taillight recognition is an important application for automated driving, especially for intent prediction of ado vehicles and trajectory planning of the ego vehicle. In this work, we propose an end-to-end deep learning framework to recognize taillights, i.e. rear turn and brake signals, from a sequence of images. The proposed method starts with a Convolutional Neural Network (CNN) to extract spatial features, and then applies a Long Short-Term Memory network (LSTM) to learn temporal dependencies. Furthermore, we integrate attention models in both spatial and temporal domains, where the attention models learn to selectively focus on both spatial and temporal features. Our method is able to outperform the state of the art in terms of accuracy on the UC Merced Vehicle Rear Signal Dataset, demonstrating the effectiveness of attention models for vehicle taillight recognition.



### Novelty Detection via Network Saliency in Visual-based Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.03685v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03685v1)
- **Published**: 2019-06-09 18:23:57+00:00
- **Updated**: 2019-06-09 18:23:57+00:00
- **Authors**: Valerie Chen, Man-Ki Yoon, Zhong Shao
- **Comment**: To be published in Dependable and Secure Machine Learning (DSML)
  workshop co-located with the IEEE Conference on Dependable Systems and
  Networks 2019
- **Journal**: None
- **Summary**: Machine-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. Often viewed as a black-box, it is non-obvious to determine when a model will make a safe decision and when it will make an erroneous, perhaps life-threatening one. Prior work on novelty detection deal with highly structured data and do not translate well to dynamic, real-world situations. This paper proposes a multi-step framework for the detection of novel scenarios in vision-based autonomous systems by leveraging information learned by the trained prediction model and a new image similarity metric. We demonstrate the efficacy of this method through experiments on a real-world driving dataset as well as on our in-house indoor racing environment.



### Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm
- **Arxiv ID**: http://arxiv.org/abs/1906.03720v1
- **DOI**: 10.1016/j.compbiomed.2019.05.002
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03720v1)
- **Published**: 2019-06-09 21:45:57+00:00
- **Updated**: 2019-06-09 21:45:57+00:00
- **Authors**: Mateusz Buda, Ashirbani Saha, Maciej A Mazurowski
- **Comment**: None
- **Journal**: Computers in Biology and Medicine, 109, 2019, 218-225
- **Summary**: Recent analysis identified distinct genomic subtypes of lower-grade glioma tumors which are associated with shape features. In this study, we propose a fully automatic way to quantify tumor imaging characteristics using deep learning-based segmentation and test whether these characteristics are predictive of tumor genomic subtypes. We used preoperative imaging and genomic data of 110 patients from 5 institutions with lower-grade gliomas from The Cancer Genome Atlas. Based on automatic deep learning segmentations, we extracted three features which quantify two-dimensional and three-dimensional characteristics of the tumors. Genomic data for the analyzed cohort of patients consisted of previously identified genomic clusters based on IDH mutation and 1p/19q co-deletion, DNA methylation, gene expression, DNA copy number, and microRNA expression. To analyze the relationship between the imaging features and genomic clusters, we conducted the Fisher exact test for 10 hypotheses for each pair of imaging feature and genomic subtype. To account for multiple hypothesis testing, we applied a Bonferroni correction. P-values lower than 0.005 were considered statistically significant. We found the strongest association between RNASeq clusters and the bounding ellipsoid volume ratio ($p<0.0002$) and between RNASeq clusters and margin fluctuation ($p<0.005$). In addition, we identified associations between bounding ellipsoid volume ratio and all tested molecular subtypes ($p<0.02$) as well as between angular standard deviation and RNASeq cluster ($p<0.02$). In terms of automatic tumor segmentation that was used to generate the quantitative image characteristics, our deep learning algorithm achieved a mean Dice coefficient of 82% which is comparable to human performance.



