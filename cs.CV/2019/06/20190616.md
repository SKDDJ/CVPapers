# Arxiv Papers in cs.CV on 2019-06-16
### Image Captioning with Integrated Bottom-Up and Multi-level Residual Top-Down Attention for Game Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1906.06632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06632v1)
- **Published**: 2019-06-16 01:45:15+00:00
- **Updated**: 2019-06-16 01:45:15+00:00
- **Authors**: Jian Zheng, Sudha Krishnamurthy, Ruxin Chen, Min-Hung Chen, Zhenhao Ge, Xiaohua Li
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning has attracted considerable attention in recent years. However, little work has been done for game image captioning which has some unique characteristics and requirements. In this work we propose a novel game image captioning model which integrates bottom-up attention with a new multi-level residual top-down attention mechanism. Firstly, a lower-level residual top-down attention network is added to the Faster R-CNN based bottom-up attention network to address the problem that the latter may lose important spatial information when extracting regional features. Secondly, an upper-level residual top-down attention network is implemented in the caption generation network to better fuse the extracted regional features for subsequent caption prediction. We create two game datasets to evaluate the proposed model. Extensive experiments show that our proposed model outperforms existing baseline models.



### Mixture separability loss in a deep convolutional network for image classification
- **Arxiv ID**: http://arxiv.org/abs/1906.06633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06633v1)
- **Published**: 2019-06-16 01:45:57+00:00
- **Updated**: 2019-06-16 01:45:57+00:00
- **Authors**: Trung Dung Do, Cheng-Bin Jin, Hakil Kim, Van Huan Nguyen
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: In machine learning, the cost function is crucial because it measures how good or bad a system is. In image classification, well-known networks only consider modifying the network structures and applying cross-entropy loss at the end of the network. However, using only cross-entropy loss causes a network to stop updating weights when all training images are correctly classified. This is the problem of the early saturation. This paper proposes a novel cost function, called mixture separability loss (MSL), which updates the weights of the network even when most of the training images are accurately predicted. MSL consists of between-class and within-class loss. Between-class loss maximizes the differences between inter-class images, whereas within-class loss minimizes the similarities between intra-class images. We designed the proposed loss function to attach to different convolutional layers in the network in order to utilize intermediate feature maps. Experiments show that a network with MSL deepens the learning process and obtains promising results with some public datasets, such as Street View House Number (SVHN), Canadian Institute for Advanced Research (CIFAR), and our self-collected Inha Computer Vision Lab (ICVL) gender dataset.



### STAR: A Structure and Texture Aware Retinex Model
- **Arxiv ID**: http://arxiv.org/abs/1906.06690v5
- **DOI**: 10.1109/TIP.2020.2974060
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06690v5)
- **Published**: 2019-06-16 13:58:52+00:00
- **Updated**: 2020-03-11 06:41:12+00:00
- **Authors**: Jun Xu, Yingkun Hou, Dongwei Ren, Li Liu, Fan Zhu, Mengyang Yu, Haoqian Wang, Ling Shao
- **Comment**: 16 pages, 13 figures, 3 tables, accepted by TIP
- **Journal**: None
- **Summary**: Retinex theory is developed mainly to decompose an image into the illumination and reflectance components by analyzing local image derivatives. In this theory, larger derivatives are attributed to the changes in reflectance, while smaller derivatives are emerged in the smooth illumination. In this paper, we utilize exponentiated local derivatives (with an exponent {\gamma}) of an observed image to generate its structure map and texture map. The structure map is produced by been amplified with {\gamma} > 1, while the texture map is generated by been shrank with {\gamma} < 1. To this end, we design exponential filters for the local derivatives, and present their capability on extracting accurate structure and texture maps, influenced by the choices of exponents {\gamma}. The extracted structure and texture maps are employed to regularize the illumination and reflectance components in Retinex decomposition. A novel Structure and Texture Aware Retinex (STAR) model is further proposed for illumination and reflectance decomposition of a single image. We solve the STAR model by an alternating optimization algorithm. Each sub-problem is transformed into a vectorized least squares regression, with closed-form solutions. Comprehensive experiments on commonly tested datasets demonstrate that, the proposed STAR model produce better quantitative and qualitative performance than previous competing methods, on illumination and reflectance decomposition, low-light image enhancement, and color correction. The code is publicly available at https://github.com/csjunxu/STAR.



### Learning Part Generation and Assembly for Structure-aware Shape Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1906.06693v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06693v4)
- **Published**: 2019-06-16 14:14:33+00:00
- **Updated**: 2020-01-25 14:23:11+00:00
- **Authors**: Jun Li, Chengjie Niu, Kai Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning powerful deep generative models for 3D shape synthesis is largely hindered by the difficulty in ensuring plausibility encompassing correct topology and reasonable geometry. Indeed, learning the distribution of plausible 3D shapes seems a daunting task for the holistic approaches, given the significant topological variations of 3D objects even within the same category. Enlightened by the fact that 3D shape structure is characterized as part composition and placement, we propose to model 3D shape variations with a part-aware deep generative network, coined as PAGENet. The network is composed of an array of per-part VAE-GANs, generating semantic parts composing a complete shape, followed by a part assembly module that estimates a transformation for each part to correlate and assemble them into a plausible structure. Through delegating the learning of part composition and part placement into separate networks, the difficulty of modeling structural variations of 3D shapes is greatly reduced. We demonstrate through both qualitative and quantitative evaluations that PAGENet generates 3D shapes with plausible, diverse and detailed structure, and show two applications, i.e., semantic shape segmentation and part-based shape editing.



### On training deep networks for satellite image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1906.06697v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06697v1)
- **Published**: 2019-06-16 14:21:23+00:00
- **Updated**: 2019-06-16 14:21:23+00:00
- **Authors**: Michal Kawulok, Szymon Piechaczek, Krzysztof Hrynczenko, Pawel Benecki, Daniel Kostrzewa, Jakub Nalepa
- **Comment**: IGARSS 2019 conference paper
- **Journal**: None
- **Summary**: The capabilities of super-resolution reconstruction (SRR)---techniques for enhancing image spatial resolution---have been recently improved significantly by the use of deep convolutional neural networks. Commonly, such networks are learned using huge training sets composed of original images alongside their low-resolution counterparts, obtained with bicubic downsampling. In this paper, we investigate how the SRR performance is influenced by the way such low-resolution training data are obtained, which has not been explored up to date. Our extensive experimental study indicates that the training data characteristics have a large impact on the reconstruction accuracy, and the widely-adopted approach is not the most effective for dealing with satellite images. Overall, we argue that developing better training data preparation routines may be pivotal in making SRR suitable for real-world applications.



### Beyond Product Quantization: Deep Progressive Quantization for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1906.06698v3
- **DOI**: 10.24963/ijcai.2019
- **Categories**: **cs.CV**, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1906.06698v3)
- **Published**: 2019-06-16 14:23:01+00:00
- **Updated**: 2020-12-05 03:22:03+00:00
- **Authors**: Lianli Gao, Xiaosu Zhu, Jingkuan Song, Zhou Zhao, Heng Tao Shen
- **Comment**: None
- **Journal**: Proceedings of the Twenty-Eighth International Joint Conference on
  Artificial Intelligence 1 (2019) 723-729
- **Summary**: Product Quantization (PQ) has long been a mainstream for generating an exponentially large codebook at very low memory/time cost. Despite its success, PQ is still tricky for the decomposition of high-dimensional vector space, and the retraining of model is usually unavoidable when the code length changes. In this work, we propose a deep progressive quantization (DPQ) model, as an alternative to PQ, for large scale image retrieval. DPQ learns the quantization codes sequentially and approximates the original feature space progressively. Therefore, we can train the quantization codes with different code lengths simultaneously. Specifically, we first utilize the label information for guiding the learning of visual features, and then apply several quantization blocks to progressively approach the visual features. Each quantization block is designed to be a layer of a convolutional neural network, and the whole framework can be trained in an end-to-end manner. Experimental results on the benchmark datasets show that our model significantly outperforms the state-of-the-art for image retrieval. Our model is trained once for different code lengths and therefore requires less computation time. Additional ablation study demonstrates the effect of each component of our proposed model. Our code is released at https://github.com/cfm-uestc/DPQ.



### Deep Recurrent Quantization for Generating Sequential Binary Codes
- **Arxiv ID**: http://arxiv.org/abs/1906.06699v3
- **DOI**: 10.24963/ijcai.2019
- **Categories**: **cs.CV**, H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1906.06699v3)
- **Published**: 2019-06-16 14:28:25+00:00
- **Updated**: 2020-12-05 03:18:56+00:00
- **Authors**: Jingkuan Song, Xiaosu Zhu, Lianli Gao, Xin-Shun Xu, Wu Liu, Heng Tao Shen
- **Comment**: None
- **Journal**: Proceedings of the Twenty-Eighth International Joint Conference on
  Artificial Intelligence 1 (2019) 912-918
- **Summary**: Quantization has been an effective technology in ANN (approximate nearest neighbour) search due to its high accuracy and fast search speed. To meet the requirement of different applications, there is always a trade-off between retrieval accuracy and speed, reflected by variable code lengths. However, to encode the dataset into different code lengths, existing methods need to train several models, where each model can only produce a specific code length. This incurs a considerable training time cost, and largely reduces the flexibility of quantization methods to be deployed in real applications. To address this issue, we propose a Deep Recurrent Quantization (DRQ) architecture which can generate sequential binary codes. To the end, when the model is trained, a sequence of binary codes can be generated and the code length can be easily controlled by adjusting the number of recurrent iterations. A shared codebook and a scalar factor is designed to be the learnable weights in the deep recurrent quantization block, and the whole framework can be trained in an end-to-end manner. As far as we know, this is the first quantization method that can be trained once and generate sequential binary codes. Experimental results on the benchmark datasets show that our model achieves comparable or even better performance compared with the state-of-the-art for image retrieval. But it requires significantly less number of parameters and training times. Our code is published online: https://github.com/cfm-uestc/DRQ.



### A fast tunable blurring algorithm for scattered data
- **Arxiv ID**: http://arxiv.org/abs/1906.06722v2
- **DOI**: 10.1137/19M1268781
- **Categories**: **stat.CO**, cs.CV, cs.LG, 65D15, 68U10, 65D10, 62M20, 93E11, 60G35, I.4.3; I.4.4; G.3; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1906.06722v2)
- **Published**: 2019-06-16 16:27:52+00:00
- **Updated**: 2020-04-22 17:11:21+00:00
- **Authors**: Gregor Robinson, Ian Grooms
- **Comment**: 19 pages, 5 figures
- **Journal**: None
- **Summary**: A blurring algorithm with linear time complexity can reduce the small-scale content of data observed at scattered locations in a spatially extended domain of arbitrary dimension. The method works by forming a Gaussian interpolant of the input data, and then convolving the interpolant with a multiresolution Gaussian approximation of the Green's function to a differential operator whose spectrum can be tuned for problem-specific considerations. Like conventional blurring algorithms, which the new algorithm generalizes to data measured at locations other than a uniform grid, applications include deblurring and separation of spatial scales. An example illustrates a possible application toward enabling importance sampling approaches to data assimilation of geophysical observations, which are often scattered over a spatial domain, since blurring observations can make particle filters more effective at state estimation of large scales. Another example, motivated by data analysis of dynamics like ocean eddies that have strong separation of spatial scales, uses the algorithm to decompose scattered oceanographic float measurements into large-scale and small-scale components.



### Defending Against Adversarial Attacks Using Random Forests
- **Arxiv ID**: http://arxiv.org/abs/1906.06765v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1906.06765v1)
- **Published**: 2019-06-16 20:46:44+00:00
- **Updated**: 2019-06-16 20:46:44+00:00
- **Authors**: Yifan Ding, Liqiang Wang, Huan Zhang, Jinfeng Yi, Deliang Fan, Boqing Gong
- **Comment**: None
- **Journal**: None
- **Summary**: As deep neural networks (DNNs) have become increasingly important and popular, the robustness of DNNs is the key to the safety of both the Internet and the physical world. Unfortunately, some recent studies show that adversarial examples, which are hard to be distinguished from real examples, can easily fool DNNs and manipulate their predictions. Upon observing that adversarial examples are mostly generated by gradient-based methods, in this paper, we first propose to use a simple yet very effective non-differentiable hybrid model that combines DNNs and random forests, rather than hide gradients from attackers, to defend against the attacks. Our experiments show that our model can successfully and completely defend the white-box attacks, has a lower transferability, and is quite resistant to three representative types of black-box attacks; while at the same time, our model achieves similar classification accuracy as the original DNNs. Finally, we investigate and suggest a criterion to define where to grow random forests in DNNs.



### On the Self-Similarity of Natural Stochastic Textures
- **Arxiv ID**: http://arxiv.org/abs/1906.06768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06768v1)
- **Published**: 2019-06-16 21:01:20+00:00
- **Updated**: 2019-06-16 21:01:20+00:00
- **Authors**: Samah Khawaled, Yehoshua Y. Zeevi
- **Comment**: 5 pages , 5 figures
- **Journal**: None
- **Summary**: Self-similarity is the essence of fractal images and, as such, characterizes natural stochastic textures. This paper is concerned with the property of self-similarity in the statistical sense in the case of fully-textured images that contain both stochastic texture and structural (mostly deterministic) information. We firstly decompose a textured image into two layers corresponding to its texture and structure, and show that the layer representing the stochastic texture is characterized by random phase of uniform distribution, unlike the phase of the structured information which is coherent. The uniform distribution of the the random phase is verified by using a suitable hypothesis testing framework. We proceed by proposing two approaches to assessment of self-similarity. The first is based on patch-wise calculation of the mutual information, while the second measures the mutual information that exists across scales. Quantifying the extent of self-similarity by means of mutual information is of paramount importance in the analysis of natural stochastic textures that are encountered in medical imaging, geology, agriculture and in computer vision algorithms that are designed for application on fully-textures images.



### Providentia -- A Large-Scale Sensor System for the Assistance of Autonomous Vehicles and Its Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1906.06789v5
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06789v5)
- **Published**: 2019-06-16 22:57:54+00:00
- **Updated**: 2021-12-08 12:27:47+00:00
- **Authors**: Annkathrin Krämmer, Christoph Schöller, Dhiraj Gulati, Venkatnarayanan Lakshminarasimhan, Franz Kurz, Dominik Rosenbaum, Claus Lenz, Alois Knoll
- **Comment**: Accepted for publication in the Journal of Field Robotics
- **Journal**: None
- **Summary**: The environmental perception of an autonomous vehicle is limited by its physical sensor ranges and algorithmic performance, as well as by occlusions that degrade its understanding of an ongoing traffic situation. This not only poses a significant threat to safety and limits driving speeds, but it can also lead to inconvenient maneuvers. Intelligent Infrastructure Systems can help to alleviate these problems. An Intelligent Infrastructure System can fill in the gaps in a vehicle's perception and extend its field of view by providing additional detailed information about its surroundings, in the form of a digital model of the current traffic situation, i.e. a digital twin. However, detailed descriptions of such systems and working prototypes demonstrating their feasibility are scarce. In this paper, we propose a hardware and software architecture that enables such a reliable Intelligent Infrastructure System to be built. We have implemented this system in the real world and demonstrate its ability to create an accurate digital twin of an extended highway stretch, thus enhancing an autonomous vehicle's perception beyond the limits of its on-board sensors. Furthermore, we evaluate the accuracy and reliability of the digital twin by using aerial images and earth observation methods for generating ground truth data.



### Floors are Flat: Leveraging Semantics for Real-Time Surface Normal Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.06792v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06792v1)
- **Published**: 2019-06-16 23:01:32+00:00
- **Updated**: 2019-06-16 23:01:32+00:00
- **Authors**: Steven Hickson, Karthik Raveendran, Alireza Fathi, Kevin Murphy, Irfan Essa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose 4 insights that help to significantly improve the performance of deep learning models that predict surface normals and semantic labels from a single RGB image. These insights are: (1) denoise the "ground truth" surface normals in the training set to ensure consistency with the semantic labels; (2) concurrently train on a mix of real and synthetic data, instead of pretraining on synthetic and finetuning on real; (3) jointly predict normals and semantics using a shared model, but only backpropagate errors on pixels that have valid training labels; (4) slim down the model and use grayscale instead of color inputs. Despite the simplicity of these steps, we demonstrate consistently improved results on several datasets, using a model that runs at 12 fps on a standard mobile phone.



### Equivariant neural networks and equivarification
- **Arxiv ID**: http://arxiv.org/abs/1906.07172v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07172v4)
- **Published**: 2019-06-16 23:26:03+00:00
- **Updated**: 2020-03-22 04:28:55+00:00
- **Authors**: Erkao Bao, Linqi Song
- **Comment**: More explanations added
- **Journal**: None
- **Summary**: We provide a process to modify a neural network to an equivariant one, which we call equivarification. As an illustration, we build an equivariant neural network for image classification by equivarifying a convolutional neural network.



### Back-Projection based Fidelity Term for Ill-Posed Linear Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1906.06794v2
- **DOI**: 10.1109/TIP.2020.2988779
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1906.06794v2)
- **Published**: 2019-06-16 23:27:37+00:00
- **Updated**: 2020-02-24 12:11:19+00:00
- **Authors**: Tom Tirer, Raja Giryes
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2020
- **Summary**: Ill-posed linear inverse problems appear in many image processing applications, such as deblurring, super-resolution and compressed sensing. Many restoration strategies involve minimizing a cost function, which is composed of fidelity and prior terms, balanced by a regularization parameter. While a vast amount of research has been focused on different prior models, the fidelity term is almost always chosen to be the least squares (LS) objective, that encourages fitting the linearly transformed optimization variable to the observations. In this paper, we examine a different fidelity term, which has been implicitly used by the recently proposed iterative denoising and backward projections (IDBP) framework. This term encourages agreement between the projection of the optimization variable onto the row space of the linear operator and the pseudo-inverse of the linear operator ("back-projection") applied on the observations. We analytically examine the difference between the two fidelity terms for Tikhonov regularization and identify cases (such as a badly conditioned linear operator) where the new term has an advantage over the standard LS one. Moreover, we demonstrate empirically that the behavior of the two induced cost functions for sophisticated convex and non-convex priors, such as total-variation, BM3D, and deep generative models, correlates with the obtained theoretical analysis.



