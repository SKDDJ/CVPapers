# Arxiv Papers in cs.CV on 2019-06-17
### Panoptic Image Annotation with a Collaborative Assistant
- **Arxiv ID**: http://arxiv.org/abs/1906.06798v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06798v4)
- **Published**: 2019-06-17 00:03:05+00:00
- **Updated**: 2020-12-15 17:57:37+00:00
- **Authors**: Jasper R. R. Uijlings, Mykhaylo Andriluka, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to reduce the time to annotate images for panoptic segmentation, which requires annotating segmentation masks and class labels for all object instances and stuff regions. We formulate our approach as a collaborative process between an annotator and an automated assistant who take turns to jointly annotate an image using a predefined pool of segments. Actions performed by the annotator serve as a strong contextual signal. The assistant intelligently reacts to this signal by annotating other parts of the image on its own, which reduces the amount of work required by the annotator. We perform thorough experiments on the COCO panoptic dataset, both in simulation and with human annotators. These demonstrate that our approach is significantly faster than the recent machine-assisted interface of [4], and 2.4x to 5x faster than manual polygon drawing. Finally, we show on ADE20k that our method can be used to efficiently annotate new datasets, bootstrapping from a very small amount of annotated data.



### Three-Dimensional Fourier Scattering Transform and Classification of Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1906.06804v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06804v2)
- **Published**: 2019-06-17 00:47:31+00:00
- **Updated**: 2020-11-21 20:12:20+00:00
- **Authors**: Ilya Kavalerov, Weilin Li, Wojciech Czaja, Rama Chellappa
- **Comment**: Accepted to IEEE Transactions On Geoscience And Remote Sensing
- **Journal**: None
- **Summary**: Recent developments in machine learning and signal processing have resulted in many new techniques that are able to effectively capture the intrinsic yet complex properties of hyperspectral imagery. Tasks ranging from anomaly detection to classification can now be solved by taking advantage of very efficient algorithms which have their roots in representation theory and in computational approximation. Time-frequency methods are one example of such techniques. They provide means to analyze and extract the spectral content from data. On the other hand, hierarchical methods such as neural networks incorporate spatial information across scales and model multiple levels of dependencies between spectral features. Both of these approaches have recently been proven to provide significant advances in the spectral-spatial classification of hyperspectral imagery. The 3D Fourier scattering transform, which is introduced in this paper, is an amalgamation of time-frequency representations with neural network architectures. It leverages the benefits provided by the Short-Time Fourier Transform with the numerical efficiency of deep learning network structures. We test the proposed method on several standard hyperspectral datasets, and we present results that indicate that the 3D Fourier scattering transform is highly effective at representing spectral content when compared with other state-of-the-art spectral-spatial classification methods.



### A Temporal Sequence Learning for Action Recognition and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.06813v1
- **DOI**: 10.1109/WACV.2018.00045
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06813v1)
- **Published**: 2019-06-17 01:33:21+00:00
- **Updated**: 2019-06-17 01:33:21+00:00
- **Authors**: Sangwoo Cho, Hassan Foroosh
- **Comment**: 10 pages, 8 figures, 2018 IEEE Winter Conference on Applications of
  Computer Vision (WACV)
- **Journal**: {IEEE} Winter Conference on Applications of Computer Vision, 2018,
  352-361
- **Summary**: In this work\footnote {This work was supported in part by the National Science Foundation under grant IIS-1212948.}, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, \textit{UCF101} and \textit{HMDB51} show that the method on average reaches 95\% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.



### Stacked Capsule Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1906.06818v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.06818v2)
- **Published**: 2019-06-17 02:31:37+00:00
- **Updated**: 2019-12-02 16:29:43+00:00
- **Authors**: Adam R. Kosiorek, Sara Sabour, Yee Whye Teh, Geoffrey E. Hinton
- **Comment**: NeurIPS 2019; 14 pages, 7 figures, 4 tables, code is available at
  https://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders
- **Journal**: None
- **Summary**: Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%). The code is available at https://github.com/google-research/google-research/tree/master/stacked_capsule_autoencoders



### A Fusion Adversarial Underwater Image Enhancement Network with a Public Test Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.06819v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06819v2)
- **Published**: 2019-06-17 02:41:42+00:00
- **Updated**: 2019-06-30 02:25:13+00:00
- **Authors**: Hanyu Li, Jingjing Li, Wei Wang
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Underwater image enhancement algorithms have attracted much attention in underwater vision task. However, these algorithms are mainly evaluated on different data sets and different metrics. In this paper, we set up an effective and pubic underwater test dataset named U45 including the color casts, low contrast and haze-like effects of underwater degradation and propose a fusion adversarial network for enhancing underwater images. Meanwhile, the well-designed the adversarial loss including Lgt loss and Lfe loss is presented to focus on image features of ground truth, and image features of the image enhanced by fusion enhance method, respectively. The proposed network corrects color casts effectively and owns faster testing time with fewer parameters. Experiment results on U45 dataset demonstrate that the proposed method achieves better or comparable performance than the other state-of-the-art methods in terms of qualitative and quantitative evaluations. Moreover, an ablation study demonstrates the contributions of each component, and the application test further shows the effectiveness of the enhanced images.



### Spatio-Temporal Fusion Networks for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.06822v1
- **DOI**: 10.1007/978-3-030-20887-5_22
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06822v1)
- **Published**: 2019-06-17 02:59:11+00:00
- **Updated**: 2019-06-17 02:59:11+00:00
- **Authors**: Sangwoo Cho, Hassan Foroosh
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision (2018) 347-364
- **Summary**: The video based CNN works have focused on effective ways to fuse appearance and motion networks, but they typically lack utilizing temporal information over video frames. In this work, we present a novel spatio-temporal fusion network (STFN) that integrates temporal dynamics of appearance and motion information from entire videos. The captured temporal dynamic information is then aggregated for a better video level representation and learned via end-to-end training. The spatio-temporal fusion network consists of two set of Residual Inception blocks that extract temporal dynamics and a fusion connection for appearance and motion features. The benefits of STFN are: (a) it captures local and global temporal dynamics of complementary data to learn video-wide information; and (b) it is applicable to any network for video classification to boost performance. We explore a variety of design choices for STFN and verify how the network performance is varied with the ablation studies. We perform experiments on two challenging human activity datasets, UCF101 and HMDB51, and achieve the state-of-the-art results with the best network.



### Scene Text Magnifier
- **Arxiv ID**: http://arxiv.org/abs/1907.00693v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00693v2)
- **Published**: 2019-06-17 03:14:08+00:00
- **Updated**: 2019-07-05 09:16:02+00:00
- **Authors**: Toshiki Nakamura, Anna Zhu, Seiichi Uchida
- **Comment**: to appear at the International Conference on Document Analysis and
  Recognition (ICDAR) 2019
- **Journal**: None
- **Summary**: Scene text magnifier aims to magnify text in natural scene images without recognition. It could help the special groups, who have myopia or dyslexia to better understand the scene. In this paper, we design the scene text magnifier through interacted four CNN-based networks: character erasing, character extraction, character magnify, and image synthesis. The architecture of the networks are extended based on the hourglass encoder-decoders. It inputs the original scene text image and outputs the text magnified image while keeps the background unchange. Intermediately, we can get the side-output results of text erasing and text extraction. The four sub-networks are first trained independently and fine-tuned in end-to-end mode. The training samples for each stage are processed through a flow with original image and text annotation in ICDAR2013 and Flickr dataset as input, and corresponding text erased image, magnified text annotation, and text magnified scene image as output. To evaluate the performance of text magnifier, the Structural Similarity is used to measure the regional changes in each character region. The experimental results demonstrate our method can magnify scene text effectively without effecting the background.



### PolSAR Image Classification based on Polarimetric Scattering Coding and Sparse Support Matrix Machine
- **Arxiv ID**: http://arxiv.org/abs/1906.07176v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07176v1)
- **Published**: 2019-06-17 03:17:13+00:00
- **Updated**: 2019-06-17 03:17:13+00:00
- **Authors**: Xu Liu, Licheng Jiao, Dan Zhang, Fang Liu
- **Comment**: arXiv admin note: text overlap with arXiv:1807.02975
- **Journal**: IGARSS2019
- **Summary**: POLSAR image has an advantage over optical image because it can be acquired independently of cloud cover and solar illumination. PolSAR image classification is a hot and valuable topic for the interpretation of POLSAR image. In this paper, a novel POLSAR image classification method is proposed based on polarimetric scattering coding and sparse support matrix machine. First, we transform the original POLSAR data to get a real value matrix by the polarimetric scattering coding, which is called polarimetric scattering matrix and is a sparse matrix. Second, the sparse support matrix machine is used to classify the sparse polarimetric scattering matrix and get the classification map. The combination of these two steps takes full account of the characteristics of POLSAR. The experimental results show that the proposed method can get better results and is an effective classification method.



### Sample-Efficient Neural Architecture Search by Learning Action Space
- **Arxiv ID**: http://arxiv.org/abs/1906.06832v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06832v2)
- **Published**: 2019-06-17 03:50:25+00:00
- **Updated**: 2021-03-31 19:13:16+00:00
- **Authors**: Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian
- **Comment**: Accepted at TPAMI-2021
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at https://github.com/facebookresearch/LaMCTS.



### NLH: A Blind Pixel-level Non-local Method for Real-world Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1906.06834v6
- **DOI**: 10.1109/TIP.2020.2980116
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06834v6)
- **Published**: 2019-06-17 04:08:42+00:00
- **Updated**: 2020-03-11 06:37:58+00:00
- **Authors**: Yingkun Hou, Jun Xu, Mingxia Liu, Guanghai Liu, Li Liu, Fan Zhu, Ling Shao
- **Comment**: 14 pages, 9 figures, 10 tables, accept by IEEE TIP
- **Journal**: None
- **Summary**: Non-local self similarity (NSS) is a powerful prior of natural images for image denoising. Most of existing denoising methods employ similar patches, which is a patch-level NSS prior. In this paper, we take one step forward by introducing a pixel-level NSS prior, i.e., searching similar pixels across a non-local region. This is motivated by the fact that finding closely similar pixels is more feasible than similar patches in natural images, which can be used to enhance image denoising performance. With the introduced pixel-level NSS prior, we propose an accurate noise level estimation method, and then develop a blind image denoising method based on the lifting Haar transform and Wiener filtering techniques. Experiments on benchmark datasets demonstrate that, the proposed method achieves much better performance than previous non-deep methods, and is still competitive with existing state-of-the-art deep learning based methods on real-world image denoising. The code is publicly available at https://github.com/njusthyk1972/NLH.



### LPaintB: Learning to Paint from Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.06841v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06841v2)
- **Published**: 2019-06-17 04:52:15+00:00
- **Updated**: 2019-09-21 15:14:21+00:00
- **Authors**: Biao Jia, Jonathan Brandt, Radomir Mech, Byungmoon Kim, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel reinforcement learning-based natural media painting algorithm. Our goal is to reproduce a reference image using brush strokes and we encode the objective through observations. Our formulation takes into account that the distribution of the reward in the action space is sparse and training a reinforcement learning algorithm from scratch can be difficult. We present an approach that combines self-supervised learning and reinforcement learning to effectively transfer negative samples into positive ones and change the reward distribution. We demonstrate the benefits of our painting agent to reproduce reference images with brush strokes. The training phase takes about one hour and the runtime algorithm takes about 30 seconds on a GTX1080 GPU reproducing a 1000x800 image with 20,000 strokes.



### Differentiated Backprojection Domain Deep Learning for Conebeam Artifact Removal
- **Arxiv ID**: http://arxiv.org/abs/1906.06854v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06854v2)
- **Published**: 2019-06-17 05:59:33+00:00
- **Updated**: 2020-06-03 13:05:48+00:00
- **Authors**: Yoseob Han, Junyoung Kim, Jong Chul Ye
- **Comment**: This paper is accepted for IEEE Trans. Medical Imaging
- **Journal**: None
- **Summary**: Conebeam CT using a circular trajectory is quite often used for various applications due to its relative simple geometry. For conebeam geometry, Feldkamp, Davis and Kress algorithm is regarded as the standard reconstruction method, but this algorithm suffers from so-called conebeam artifacts as the cone angle increases. Various model-based iterative reconstruction methods have been developed to reduce the cone-beam artifacts, but these algorithms usually require multiple applications of computational expensive forward and backprojections. In this paper, we develop a novel deep learning approach for accurate conebeam artifact removal. In particular, our deep network, designed on the differentiated backprojection domain, performs a data-driven inversion of an ill-posed deconvolution problem associated with the Hilbert transform. The reconstruction results along the coronal and sagittal directions are then combined using a spectral blending technique to minimize the spectral leakage. Experimental results show that our method outperforms the existing iterative methods despite significantly reduced runtime complexity.



### Inspirational Adversarial Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1906.11661v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11661v2)
- **Published**: 2019-06-17 06:52:40+00:00
- **Updated**: 2021-04-02 06:55:30+00:00
- **Authors**: Baptiste Rozière, Morgane Riviere, Olivier Teytaud, Jérémy Rapin, Yann LeCun, Camille Couprie
- **Comment**: None
- **Journal**: TIP 2021
- **Summary**: The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.



### Hierarchical Back Projection Network for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1906.06874v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06874v2)
- **Published**: 2019-06-17 07:23:01+00:00
- **Updated**: 2019-06-20 11:08:50+00:00
- **Authors**: Zhi-Song Liu, Li-Wen Wang, Chu-Tak Li, Wan-Chi Siu
- **Comment**: None
- **Journal**: 2019 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition
- **Summary**: Deep learning based single image super-resolution methods use a large number of training datasets and have recently achieved great quality progress both quantitatively and qualitatively. Most deep networks focus on nonlinear mapping from low-resolution inputs to high-resolution outputs via residual learning without exploring the feature abstraction and analysis. We propose a Hierarchical Back Projection Network (HBPN), that cascades multiple HourGlass (HG) modules to bottom-up and top-down process features across all scales to capture various spatial correlations and then consolidates the best representation for reconstruction. We adopt the back projection blocks in our proposed network to provide the error correlated up and down-sampling process to replace simple deconvolution and pooling process for better estimation. A new Softmax based Weighted Reconstruction (WR) process is used to combine the outputs of HG modules to further improve super-resolution. Experimental results on various datasets (including the validation dataset, NTIRE2019, of the Real Image Super-resolution Challenge) show that our proposed approach can achieve and improve the performance of the state-of-the-art methods for different scaling factors.



### Multi-task Learning For Detecting and Segmenting Manipulated Facial Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.06876v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06876v1)
- **Published**: 2019-06-17 07:27:54+00:00
- **Updated**: 2019-06-17 07:27:54+00:00
- **Authors**: Huy H. Nguyen, Fuming Fang, Junichi Yamagishi, Isao Echizen
- **Comment**: Accepted to be Published in Proceedings of the IEEE International
  Conference on Biometrics: Theory, Applications and Systems (BTAS) 2019,
  Florida, USA
- **Journal**: None
- **Summary**: Detecting manipulated images and videos is an important topic in digital media forensics. Most detection methods use binary classification to determine the probability of a query being manipulated. Another important topic is locating manipulated regions (i.e., performing segmentation), which are mostly created by three commonly used attacks: removal, copy-move, and splicing. We have designed a convolutional neural network that uses the multi-task learning approach to simultaneously detect manipulated images and videos and locate the manipulated regions for each query. Information gained by performing one task is shared with the other task and thereby enhance the performance of both tasks. A semi-supervised learning approach is used to improve the network's generability. The network includes an encoder and a Y-shaped decoder. Activation of the encoded features is used for the binary classification. The output of one branch of the decoder is used for segmenting the manipulated regions while that of the other branch is used for reconstructing the input, which helps improve overall performance. Experiments using the FaceForensics and FaceForensics++ databases demonstrated the network's effectiveness against facial reenactment attacks and face swapping attacks as well as its ability to deal with the mismatch condition for previously seen attacks. Moreover, fine-tuning using just a small amount of data enables the network to deal with unseen attacks.



### Noisy-As-Clean: Learning Self-supervised Denoising from the Corrupted Image
- **Arxiv ID**: http://arxiv.org/abs/1906.06878v4
- **DOI**: 10.1109/TIP.2020.3026622
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06878v4)
- **Published**: 2019-06-17 07:39:00+00:00
- **Updated**: 2020-05-09 09:04:17+00:00
- **Authors**: Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao
- **Comment**: 12 pages, 9 figures, 6 tables, the first two authors contribute
  equally
- **Journal**: None
- **Summary**: Supervised deep networks have achieved promisingperformance on image denoising, by learning image priors andnoise statistics on plenty pairs of noisy and clean images. Unsupervised denoising networks are trained with only noisy images. However, for an unseen corrupted image, both supervised andunsupervised networks ignore either its particular image prior, the noise statistics, or both. That is, the networks learned from external images inherently suffer from a domain gap problem: the image priors and noise statistics are very different between the training and test images. This problem becomes more clear when dealing with the signal dependent realistic noise. To circumvent this problem, in this work, we propose a novel "Noisy-As-Clean" (NAC) strategy of training self-supervised denoising networks. Specifically, the corrupted test image is directly taken as the "clean" target, while the inputs are synthetic images consisted of this corrupted image and a second and similar corruption. A simple but useful observation on our NAC is: as long as the noise is weak, it is feasible to learn a self-supervised network only with the corrupted image, approximating the optimal parameters of a supervised network learned with pairs of noisy and clean images. Experiments on synthetic and realistic noise removal demonstrate that, the DnCNN and ResNet networks trained with our self-supervised NAC strategy achieve comparable or better performance than the original ones and previous supervised/unsupervised/self-supervised networks. The code is publicly available at https://github.com/csjunxu/Noisy-As-Clean.



### ParNet: Position-aware Aggregated Relation Network for Image-Text matching
- **Arxiv ID**: http://arxiv.org/abs/1906.06892v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1906.06892v1)
- **Published**: 2019-06-17 08:26:43+00:00
- **Updated**: 2019-06-17 08:26:43+00:00
- **Authors**: Yaxian Xia, Lun Huang, Wenmin Wang, Xiaoyong Wei, Wenmin Wang
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Exploring fine-grained relationship between entities(e.g. objects in image or words in sentence) has great contribution to understand multimedia content precisely. Previous attention mechanism employed in image-text matching either takes multiple self attention steps to gather correspondences or uses image objects (or words) as context to infer image-text similarity. However, they only take advantage of semantic information without considering that objects' relative position also contributes to image understanding. To this end, we introduce a novel position-aware relation module to model both the semantic and spatial relationship simultaneously for image-text matching in this paper. Given an image, our method utilizes the location of different objects to capture spatial relationship innovatively. With the combination of semantic and spatial relationship, it's easier to understand the content of different modalities (images and sentences) and capture fine-grained latent correspondences of image-text pairs. Besides, we employ a two-step aggregated relation module to capture interpretable alignment of image-text pairs. The first step, we call it intra-modal relation mechanism, in which we computes responses between different objects in an image or different words in a sentence separately; The second step, we call it inter-modal relation mechanism, in which the query plays a role of textual context to refine the relationship among object proposals in an image. In this way, our position-aware aggregated relation network (ParNet) not only knows which entities are relevant by attending on different objects (words) adaptively, but also adjust the inter-modal correspondence according to the latent alignments according to query's content. Our approach achieves the state-of-the-art results on MS-COCO dataset.



### Improving Black-box Adversarial Attacks with a Transfer-based Prior
- **Arxiv ID**: http://arxiv.org/abs/1906.06919v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06919v3)
- **Published**: 2019-06-17 09:40:32+00:00
- **Updated**: 2020-07-26 14:00:51+00:00
- **Authors**: Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu
- **Comment**: NeurIPS 2019; Code available at
  https://github.com/thu-ml/Prior-Guided-RGF
- **Journal**: None
- **Summary**: We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.



### DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans
- **Arxiv ID**: http://arxiv.org/abs/1906.06962v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06962v3)
- **Published**: 2019-06-17 11:35:17+00:00
- **Updated**: 2020-03-23 09:02:23+00:00
- **Authors**: Ayush Dewan, Wolfram Burgard
- **Comment**: Accepted for ICRA-2020. Code and dataset available at
  https://github.com/ayushais/DBLiDARNet. Added results for Semantic Kitti
  Dataset
- **Journal**: None
- **Summary**: Understanding the semantic characteristics of the environment is a key enabler for autonomous robot operation. In this paper, we propose a deep convolutional neural network (DCNN) for the semantic segmentation of a LiDAR scan into the classes car, pedestrian or bicyclist. This architecture is based on dense blocks and efficiently utilizes depth separable convolutions to limit the number of parameters while still maintaining state-of-the-art performance. To make the predictions from the DCNN temporally consistent, we propose a Bayes filter based method. This method uses the predictions from the neural network to recursively estimate the current semantic state of a point in a scan. This recursive estimation uses the knowledge gained from previous scans, thereby making the predictions temporally consistent and robust towards isolated erroneous predictions. We compare the performance of our proposed architecture with other state-of-the-art neural network architectures and report substantial improvement. For the proposed Bayes filter approach, we show results on various sequences in the KITTI tracking benchmark.



### EnlightenGAN: Deep Light Enhancement without Paired Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.06972v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06972v2)
- **Published**: 2019-06-17 11:54:20+00:00
- **Updated**: 2021-01-24 18:22:22+00:00
- **Authors**: Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, Zhangyang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. The code is available at \url{https://github.com/yueruchen/EnlightenGAN}



### Multi-Scale Convolutions for Learning Context Aware Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/1906.06978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06978v1)
- **Published**: 2019-06-17 12:03:59+00:00
- **Updated**: 2019-06-17 12:03:59+00:00
- **Authors**: Nikolai Ufer, Kam To Lui, Katja Schwarz, Paul Warkentin, Björn Ommer
- **Comment**: None
- **Journal**: None
- **Summary**: Finding semantic correspondences is a challenging problem. With the breakthrough of CNNs stronger features are available for tasks like classification but not specifically for the requirements of semantic matching. In the following we present a weakly supervised metric learning approach which generates stronger features by encoding far more context than previous methods. First, we generate more suitable training data using a geometrically informed correspondence mining method which is less prone to spurious matches and requires only image category labels as supervision. Second, we introduce a new convolutional layer which is a learned mixture of differently strided convolutions and allows the network to encode implicitly more context while preserving matching accuracy at the same time. The strong geometric encoding on the feature side enables us to learn a semantic flow network, which generates more natural deformations than parametric transformation based models and is able to jointly predict foreground regions at the same time. Our semantic flow network outperforms current state-of-the-art on several semantic matching benchmarks and the learned features show astonishing performance regarding simple nearest neighbor matching.



### Hallucinated Adversarial Learning for Robust Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.07008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07008v1)
- **Published**: 2019-06-17 13:02:23+00:00
- **Updated**: 2019-06-17 13:02:23+00:00
- **Authors**: Qiangqiang Wu, Zhihui Chen, Lin Cheng, Yan Yan, Bo Li, Hanzi Wang
- **Comment**: Visual object tracking, data hallucination
- **Journal**: None
- **Summary**: Humans can easily learn new concepts from just a single exemplar, mainly due to their remarkable ability to imagine or hallucinate what the unseen exemplar may look like in different settings. Incorporating such an ability to hallucinate diverse new samples of the tracked instance can help the trackers alleviate the over-fitting problem in the low-data tracking regime. To achieve this, we propose an effective adversarial approach, denoted as adversarial "hallucinator" (AH), for robust visual tracking. The proposed AH is designed to firstly learn transferable non-linear deformations between a pair of same-identity instances, and then apply these deformations to an unseen tracked instance in order to generate diverse positive training samples. By incorporating AH into an online tracking-by-detection framework, we propose the hallucinated adversarial tracker (HAT), which jointly optimizes AH with an online classifier (e.g., MDNet) in an end-to-end manner. In addition, a novel selective deformation transfer (SDT) method is presented to better select the deformations which are more suitable for transfer. Extensive experiments on 3 popular benchmarks demonstrate that our HAT achieves the state-of-the-art performance.



### Semi-Supervised Semantic Mapping through Label Propagation with Semantic Texture Meshes
- **Arxiv ID**: http://arxiv.org/abs/1906.07029v1
- **DOI**: 10.1007/s11263-019-01187-z
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.07029v1)
- **Published**: 2019-06-17 13:36:21+00:00
- **Updated**: 2019-06-17 13:36:21+00:00
- **Authors**: Radu Alexandru Rosu, Jan Quenzel, Sven Behnke
- **Comment**: This is a pre-print of an article published in International Journal
  of Computer Vision (IJCV, 2019)
- **Journal**: None
- **Summary**: Scene understanding is an important capability for robots acting in unstructured environments. While most SLAM approaches provide a geometrical representation of the scene, a semantic map is necessary for more complex interactions with the surroundings. Current methods treat the semantic map as part of the geometry which limits scalability and accuracy. We propose to represent the semantic map as a geometrical mesh and a semantic texture coupled at independent resolution. The key idea is that in many environments the geometry can be greatly simplified without loosing fidelity, while semantic information can be stored at a higher resolution, independent of the mesh. We construct a mesh from depth sensors to represent the scene geometry and fuse information into the semantic texture from segmentations of individual RGB views of the scene. Making the semantics persistent in a global mesh enables us to enforce temporal and spatial consistency of the individual view predictions. For this, we propose an efficient method of establishing consensus between individual segmentations by iteratively retraining semantic segmentation with the information stored within the map and using the retrained segmentation to re-fuse the semantics. We demonstrate the accuracy and scalability of our approach by reconstructing semantic maps of scenes from NYUv2 and a scene spanning large buildings.



### Towards Real-Time Action Recognition on Mobile Devices Using Deep Models
- **Arxiv ID**: http://arxiv.org/abs/1906.07052v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07052v1)
- **Published**: 2019-06-17 14:27:24+00:00
- **Updated**: 2019-06-17 14:27:24+00:00
- **Authors**: Chen-Lin Zhang, Xin-Xin Liu, Jianxin Wu
- **Comment**: work in progress
- **Journal**: None
- **Summary**: Action recognition is a vital task in computer vision, and many methods are developed to push it to the limit. However, current action recognition models have huge computational costs, which cannot be deployed to real-world tasks on mobile devices. In this paper, we first illustrate the setting of real-time action recognition, which is different from current action recognition inference settings. Under the new inference setting, we investigate state-of-the-art action recognition models on the Kinetics dataset empirically. Our results show that designing efficient real-time action recognition models is different from designing efficient ImageNet models, especially in weight initialization. We show that pre-trained weights on ImageNet improve the accuracy under the real-time action recognition setting. Finally, we use the hand gesture recognition task as a case study to evaluate our compact real-time action recognition models in real-world applications on mobile phones. Results show that our action recognition models, being 6x faster and with similar accuracy as state-of-the-art, can roughly meet the real-time requirements on mobile devices. To our best knowledge, this is the first paper that deploys current deep learning action recognition models on mobile devices.



### Exemplar Guided Face Image Super-Resolution without Facial Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1906.07078v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07078v1)
- **Published**: 2019-06-17 15:08:09+00:00
- **Updated**: 2019-06-17 15:08:09+00:00
- **Authors**: Berk Dogan, Shuhang Gu, Radu Timofte
- **Comment**: Published in 2019 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshops
- **Journal**: None
- **Summary**: Nowadays, due to the ubiquitous visual media there are vast amounts of already available high-resolution (HR) face images. Therefore, for super-resolving a given very low-resolution (LR) face image of a person it is very likely to find another HR face image of the same person which can be used to guide the process. In this paper, we propose a convolutional neural network (CNN)-based solution, namely GWAInet, which applies super-resolution (SR) by a factor 8x on face images guided by another unconstrained HR face image of the same person with possible differences in age, expression, pose or size. GWAInet is trained in an adversarial generative manner to produce the desired high quality perceptual image results. The utilization of the HR guiding image is realized via the use of a warper subnetwork that aligns its contents to the input image and the use of a feature fusion chain for the extracted features from the warped guiding image and the input image. In training, the identity loss further helps in preserving the identity related features by minimizing the distance between the embedding vectors of SR and HR ground truth images. Contrary to the current state-of-the-art in face super-resolution, our method does not require facial landmark points for its training, which helps its robustness and allows it to produce fine details also for the surrounding face region in a uniform manner. Our method GWAInet produces photo-realistic images in upscaling factor 8x and outperforms state-of-the-art in quantitative terms and perceptual quality.



### An Attention-Guided Deep Regression Model for Landmark Detection in Cephalograms
- **Arxiv ID**: http://arxiv.org/abs/1906.07549v3
- **DOI**: 10.1007/978-3-030-32226-7_60
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07549v3)
- **Published**: 2019-06-17 15:11:31+00:00
- **Updated**: 2020-09-27 07:44:02+00:00
- **Authors**: Zhusi Zhong, Jie Li, Zhenxi Zhang, Zhicheng Jiao, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Cephalometric tracing method is usually used in orthodontic diagnosis and treatment planning. In this paper, we propose a deep learning based framework to automatically detect anatomical landmarks in cephalometric X-ray images. We train the deep encoder-decoder for landmark detection, and combine global landmark configuration with local high-resolution feature responses. The proposed frame-work is based on 2-stage u-net, regressing the multi-channel heatmaps for land-mark detection. In this framework, we embed attention mechanism with global stage heatmaps, guiding the local stage inferring, to regress the local heatmap patches in a high resolution. Besides, the Expansive Exploration strategy improves robustness while inferring, expanding the searching scope without increasing model complexity. We have evaluated our framework in the most widely-used public dataset of landmark detection in cephalometric X-ray images. With less computation and manually tuning, our framework achieves state-of-the-art results.



### Boosting Supervision with Self-Supervision for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.07079v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07079v1)
- **Published**: 2019-06-17 15:17:40+00:00
- **Updated**: 2019-06-17 15:17:40+00:00
- **Authors**: Jong-Chyi Su, Subhransu Maji, Bharath Hariharan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a technique to improve the transferability of deep representations learned on small labeled datasets by introducing self-supervised tasks as auxiliary loss functions. While recent approaches for self-supervised learning have shown the benefits of training on large unlabeled datasets, we find improvements in generalization even on small datasets and when combined with strong supervision. Learning representations with self-supervised losses reduces the relative error rate of a state-of-the-art meta-learner by 5-25% on several few-shot learning benchmarks, as well as off-the-shelf deep networks on standard classification tasks when training from scratch. We find the benefits of self-supervision increase with the difficulty of the task. Our approach utilizes the images within the dataset to construct self-supervised losses and hence is an effective way of learning transferable representations without relying on any external training data.



### Particle Swarm Optimization for Great Enhancement in Semi-Supervised Retinal Vessel Segmentation with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.07084v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07084v2)
- **Published**: 2019-06-17 15:28:29+00:00
- **Updated**: 2019-08-12 14:38:01+00:00
- **Authors**: Qiang Huo
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal vessel segmentation based on deep learning requires a lot of manual labeled data. That is time-consuming, laborious and professional. What is worse, the acquisition of abundant fundus images is difficult. These problems are more serious due to the presence of abnormalities, varying size and shape of the vessels, non-uniform illumination and anatomical changes. In this paper, we propose a data-efficient semi-supervised learning framework, which effectively combines the existing deep learning network with GAN and self-training ideas. In view of the difficulty of tuning hyper-parameters of semi-supervised learning, we propose a method for hyper-parameters selection based on particle swarm optimization algorithm. To the best of our knowledge, this work is the first demonstration that combines intelligent optimization with semi-supervised learning for achieving the best performance. Under the collaboration of adversarial learning, self-training and PSO to select optimal hyper-parameters, we obtain the performance of retinal vessel segmentation approximate to or even better than representative supervised learning using only one tenth of the labeled data from DRIVE.



### Active Generative Adversarial Network for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.07133v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07133v1)
- **Published**: 2019-06-17 17:11:07+00:00
- **Updated**: 2019-06-17 17:11:07+00:00
- **Authors**: Quan Kong, Bin Tong, Martin Klinkigt, Yuki Watanabe, Naoto Akira, Tomokazu Murakami
- **Comment**: AAAI2019
- **Journal**: None
- **Summary**: Sufficient supervised information is crucial for any machine learning models to boost performance. However, labeling data is expensive and sometimes difficult to obtain. Active learning is an approach to acquire annotations for data from a human oracle by selecting informative samples with a high probability to enhance performance. In recent emerging studies, a generative adversarial network (GAN) has been integrated with active learning to generate good candidates to be presented to the oracle. In this paper, we propose a novel model that is able to obtain labels for data in a cheaper manner without the need to query an oracle. In the model, a novel reward for each sample is devised to measure the degree of uncertainty, which is obtained from a classifier trained with existing labeled data. This reward is used to guide a conditional GAN to generate informative samples with a higher probability for a certain label. With extensive evaluations, we have confirmed the effectiveness of the model, showing that the generated samples are capable of improving the classification performance in popular image classification tasks.



### Machine-Assisted Map Editing
- **Arxiv ID**: http://arxiv.org/abs/1906.07138v1
- **DOI**: 10.1145/3274895.3274927
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07138v1)
- **Published**: 2019-06-17 17:21:10+00:00
- **Updated**: 2019-06-17 17:21:10+00:00
- **Authors**: Favyen Bastani, Songtao He, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden
- **Comment**: None
- **Journal**: Proceedings of the 26th ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems, pg 23-32, 2018
- **Summary**: Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.



### MMDetection: Open MMLab Detection Toolbox and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1906.07155v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07155v1)
- **Published**: 2019-06-17 17:58:12+00:00
- **Updated**: 2019-06-17 17:58:12+00:00
- **Authors**: Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, Dahua Lin
- **Comment**: Technical report of MMDetection. 11 pages
- **Journal**: None
- **Summary**: We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.



### NeoNav: Improving the Generalization of Visual Navigation via Generating Next Expected Observations
- **Arxiv ID**: http://arxiv.org/abs/1906.07207v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07207v4)
- **Published**: 2019-06-17 18:14:03+00:00
- **Updated**: 2022-01-10 04:10:11+00:00
- **Authors**: Qiaoyun Wu, Dinesh Manocha, Jun Wang, Kai Xu
- **Comment**: added experiments, accepted by AAAI, Corresponding author: Kai Xu
  (kevin.kai.xu@gmail.com)
- **Journal**: None
- **Summary**: We propose improving the cross-target and cross-scene generalization of visual navigation through learning an agent that is guided by conceiving the next observations it expects to see. This is achieved by learning a variational Bayesian model, called NeoNav, which generates the next expected observations (NEO) conditioned on the current observations of the agent and the target view. Our generative model is learned through optimizing a variational objective encompassing two key designs. First, the latent distribution is conditioned on current observations and the target view, leading to a model-based, target-driven navigation. Second, the latent space is modeled with a Mixture of Gaussians conditioned on the current observation and the next best action. Our use of mixture-of-posteriors prior effectively alleviates the issue of over-regularized latent space, thus significantly boosting the model generalization for new targets and in novel scenes. Moreover, the NEO generation models the forward dynamics of agent-environment interaction, which improves the quality of approximate inference and hence benefits data efficiency. We have conducted extensive evaluations on both real-world and synthetic benchmarks, and show that our model consistently outperforms the state-of-the-art models in terms of success rate, data efficiency, and generalization.



### Brain Network Construction and Classification Toolbox (BrainNetClass)
- **Arxiv ID**: http://arxiv.org/abs/1906.09908v1
- **DOI**: 10.1002/hbm.24979
- **Categories**: **q-bio.NC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.09908v1)
- **Published**: 2019-06-17 18:19:18+00:00
- **Updated**: 2019-06-17 18:19:18+00:00
- **Authors**: Zhen Zhou, Xiaobo Chen, Yu Zhang, Lishan Qiao, Renping Yu, Gang Pan, Han Zhang, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Brain functional network has become an increasingly used approach in understanding brain functions and diseases. Many network construction methods have been developed, whereas the majority of the studies still used static pairwise Pearson's correlation-based functional connectivity. The goal of this work is to introduce a toolbox namely "Brain Network Construction and Classification" (BrainNetClass) to the field to promote more advanced brain network construction methods. It comprises various brain network construction methods, including some state-of-the-art methods that were recently developed to capture more complex interactions among brain regions along with connectome feature extraction, reduction, parameter optimization towards network-based individualized classification. BrainNetClass is a MATLAB-based, open-source, cross-platform toolbox with graphical user-friendly interfaces for cognitive and clinical neuroscientists to perform rigorous computer-aided diagnosis with interpretable result presentations even though they do not possess neuroimage computing and machine learning knowledge. We demonstrate the implementations of this toolbox on real resting-state functional MRI datasets. BrainNetClass (v1.0) can be downloaded from https://github.com/zzstefan/BrainNetClass.



### Hardware Aware Neural Network Architectures using FbNet
- **Arxiv ID**: http://arxiv.org/abs/1906.07214v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07214v1)
- **Published**: 2019-06-17 18:34:01+00:00
- **Updated**: 2019-06-17 18:34:01+00:00
- **Authors**: Sai Vineeth Kalluru Srinivas, Harideep Nair, Vinay Vidyasagar
- **Comment**: 8 pages, 11 figures
- **Journal**: None
- **Summary**: We implement a differentiable Neural Architecture Search (NAS) method inspired by FBNet for discovering neural networks that are heavily optimized for a particular target device. The FBNet NAS method discovers a neural network from a given search space by optimizing over a loss function which accounts for accuracy and target device latency. We extend this loss function by adding an energy term. This will potentially enhance the ``hardware awareness" and help us find a neural network architecture that is optimal in terms of accuracy, latency and energy consumption, given a target device (Raspberry Pi in our case). We name our trained child architecture obtained at the end of search process as Hardware Aware Neural Network Architecture (HANNA). We prove the efficacy of our approach by benchmarking HANNA against two other state-of-the-art neural networks designed for mobile/embedded applications, namely MobileNetv2 and CondenseNet for CIFAR-10 dataset. Our results show that HANNA provides a speedup of about 2.5x and 1.7x, and reduces energy consumption by 3.8x and 2x compared to MobileNetv2 and CondenseNet respectively. HANNA is able to provide such significant speedup and energy efficiency benefits over the state-of-the-art baselines at the cost of a tolerable 4-5% drop in accuracy.



### An IoT Based Framework For Activity Recognition Using Deep Learning Technique
- **Arxiv ID**: http://arxiv.org/abs/1906.07247v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07247v1)
- **Published**: 2019-06-17 20:09:41+00:00
- **Updated**: 2019-06-17 20:09:41+00:00
- **Authors**: Ashwin Geet D'Sa, B. G. Prasad
- **Comment**: Intended to submit to a journal
- **Journal**: None
- **Summary**: Activity recognition is the ability to identify and recognize the action or goals of the agent. The agent can be any object or entity that performs action that has end goals. The agents can be a single agent performing the action or group of agents performing the actions or having some interaction. Human activity recognition has gained popularity due to its demands in many practical applications such as entertainment, healthcare, simulations and surveillance systems. Vision based activity recognition is gaining advantage as it does not require any human intervention or physical contact with humans. Moreover, there are set of cameras that are networked with the intention to track and recognize the activities of the agent. Traditional applications that were required to track or recognize human activities made use of wearable devices. However, such applications require physical contact of the person. To overcome such challenges, vision based activity recognition system can be used, which uses a camera to record the video and a processor that performs the task of recognition. The work is implemented in two stages. In the first stage, an approach for the Implementation of Activity recognition is proposed using background subtraction of images, followed by 3D- Convolutional Neural Networks. The impact of using Background subtraction prior to 3D-Convolutional Neural Networks has been reported. In the second stage, the work is further extended and implemented on Raspberry Pi, that can be used to record a stream of video, followed by recognizing the activity that was involved in the video. Thus, a proof-of-concept for activity recognition using small, IoT based device, is provided, which can enhance the system and extend its applications in various forms like, increase in portability, networking, and other capabilities of the device.



### Pose Guided Fashion Image Synthesis Using Deep Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1906.07251v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07251v2)
- **Published**: 2019-06-17 20:26:59+00:00
- **Updated**: 2019-09-17 18:37:42+00:00
- **Authors**: Wei Sun, Jawadul H. Bappy, Shanglin Yang, Yi Xu, Tianfu Wu, Hui Zhou
- **Comment**: KDD 2019 Workshop AI for Fashion
- **Journal**: None
- **Summary**: Generating a photorealistic image with intended human pose is a promising yet challenging research topic for many applications such as smart photo editing, movie making, virtual try-on, and fashion display. In this paper, we present a novel deep generative model to transfer an image of a person from a given pose to a new pose while keeping fashion item consistent. In order to formulate the framework, we employ one generator and two discriminators for image synthesis. The generator includes an image encoder, a pose encoder and a decoder. The two encoders provide good representation of visual and geometrical context which will be utilized by the decoder in order to generate a photorealistic image. Unlike existing pose-guided image generation models, we exploit two discriminators to guide the synthesis process where one discriminator differentiates between generated image and real images (training samples), and another discriminator verifies the consistency of appearance between a target pose and a generated image. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth images. The proposed generative model is capable of synthesizing a photorealistic image of a person given a target pose. We have demonstrated our results by conducting rigorous experiments on two data sets, both quantitatively and qualitatively.



### Content-aware Density Map for Crowd Counting and Density Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.07258v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07258v1)
- **Published**: 2019-06-17 20:45:29+00:00
- **Updated**: 2019-06-17 20:45:29+00:00
- **Authors**: Mahdi Maktabdar Oghaz, Anish R Khadka, Vasileios Argyriou, Paolo Remagnino
- **Comment**: None
- **Journal**: 32nd International Conference on Computer Animation and Social
  Agents (CASA 2019)
- **Summary**: Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.



### Coherent and Controllable Outfit Generation
- **Arxiv ID**: http://arxiv.org/abs/1906.07273v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07273v2)
- **Published**: 2019-06-17 21:12:36+00:00
- **Updated**: 2019-11-16 21:26:04+00:00
- **Authors**: Kedan Li, Chen Liu, David Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: When thinking about dressing oneself, people often have a theme in mind whether they're going to a tropical getaway or wish to appear attractive at a cocktail party. A useful outfit generation system should come up with clothing items that are compatible while matching a theme specified by the user. Existing methods use item-wise compatibility between products but lack an effective way to enforce a global constraint (e.g., style, occasion).   We introduce a method that generates outfits whose items match a theme described by a text query. Our method uses text and image embeddings to represent fashion items. We learn a multimodal embedding where the image representation for an item is close to its text representation, and use this embedding to measure item-query coherence. We then use a discriminator to compute compatibility between fashion items. This strategy yields a compatibility prediction method that meets or exceeds the state of the art.   Our method combines item-item compatibility and item-query coherence to construct an outfit whose items are (a) close to the query and (b) compatible with one another. Quantitative evaluation shows that the items in our outfits are tightly clustered compared to standard outfits. Furthermore, outfits produced by similar queries are close to one another, and outfits produced by very different queries are far apart. Qualitative evaluation shows that our method responds well to queries. A user study suggests that people understand the match between the queries and the outfits produced by our method.



### The Cells Out of Sample (COOS) dataset and benchmarks for measuring out-of-sample generalization of image classifiers
- **Arxiv ID**: http://arxiv.org/abs/1906.07282v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07282v3)
- **Published**: 2019-06-17 21:35:00+00:00
- **Updated**: 2020-01-06 15:46:56+00:00
- **Authors**: Alex X. Lu, Amy X. Lu, Wiebke Schormann, Marzyeh Ghassemi, David W. Andrews, Alan M. Moses
- **Comment**: None
- **Journal**: In Advances in Neural Information Processing Systems 32, pages
  1852-1860. NeurIPS 2019
- **Summary**: Understanding if classifiers generalize to out-of-sample datasets is a central problem in machine learning. Microscopy images provide a standardized way to measure the generalization capacity of image classifiers, as we can image the same classes of objects under increasingly divergent, but controlled factors of variation. We created a public dataset of 132,209 images of mouse cells, COOS-7 (Cells Out Of Sample 7-Class). COOS-7 provides a classification setting where four test datasets have increasing degrees of covariate shift: some images are random subsets of the training data, while others are from experiments reproduced months later and imaged by different instruments. We benchmarked a range of classification models using different representations, including transferred neural network features, end-to-end classification with a supervised deep CNN, and features from a self-supervised CNN. While most classifiers perform well on test datasets similar to the training dataset, all classifiers failed to generalize their performance to datasets with greater covariate shifts. These baselines highlight the challenges of covariate shifts in image data, and establish metrics for improving the generalization capacity of image classifiers.



### 4D CNN for semantic segmentation of cardiac volumetric sequences
- **Arxiv ID**: http://arxiv.org/abs/1906.07295v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07295v2)
- **Published**: 2019-06-17 22:43:06+00:00
- **Updated**: 2019-10-10 00:32:59+00:00
- **Authors**: Andriy Myronenko, Dong Yang, Varun Buch, Daguang Xu, Alvin Ihsani, Sean Doyle, Mark Michalski, Neil Tenenholtz, Holger Roth
- **Comment**: MICCAI, STACOM, 2019
- **Journal**: None
- **Summary**: We propose a 4D convolutional neural network (CNN) for the segmentation of retrospective ECG-gated cardiac CT, a series of single-channel volumetric data over time. While only a small subset of volumes in the temporal sequence is annotated, we define a sparse loss function on available labels to allow the network to leverage unlabeled images during training and generate a fully segmented sequence. We investigate the accuracy of the proposed 4D network to predict temporally consistent segmentations and compare with traditional 3D segmentation approaches. We demonstrate the feasibility of the 4D CNN and establish its performance on cardiac 4D CCTA.



