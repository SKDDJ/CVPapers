# Arxiv Papers in cs.CV on 2019-06-24
### CORAL8: Concurrent Object Regression for Area Localization in Medical Image Panels
- **Arxiv ID**: http://arxiv.org/abs/1906.09676v1
- **DOI**: 10.1007/978-3-030-32239-7_48
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.09676v1)
- **Published**: 2019-06-24 00:30:32+00:00
- **Updated**: 2019-06-24 00:30:32+00:00
- **Authors**: Sam Maksoud, Arnold Wiliem, Kun Zhao, Teng Zhang, Lin Wu, Brian C. Lovell
- **Comment**: Accepted for MICCAI 2019
- **Journal**: None
- **Summary**: This work tackles the problem of generating a medical report for multi-image panels. We apply our solution to the Renal Direct Immunofluorescence (RDIF) assay which requires a pathologist to generate a report based on observations across the eight different WSI in concert with existing clinical features. To this end, we propose a novel attention-based multi-modal generative recurrent neural network (RNN) architecture capable of dynamically sampling image data concurrently across the RDIF panel. The proposed methodology incorporates text from the clinical notes of the requesting physician to regulate the output of the network to align with the overall clinical context. In addition, we found the importance of regularizing the attention weights for word generation processes. This is because the system can ignore the attention mechanism by assigning equal weights for all members. Thus, we propose two regularizations which force the system to utilize the attention mechanism. Experiments on our novel collection of RDIF WSIs provided by a large clinical laboratory demonstrate that our framework offers significant improvements over existing methods.



### Remote Sensor Design for Visual Recognition with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.09677v1
- **DOI**: 10.1109/TGRS.2019.2925813
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09677v1)
- **Published**: 2019-06-24 00:43:32+00:00
- **Updated**: 2019-06-24 00:43:32+00:00
- **Authors**: Lucas Jaffe, Michael Zelinski, Wesam Sakla
- **Comment**: Accepted for publication in IEEE Transactions on Geoscience and
  Remote Sensing
- **Journal**: None
- **Summary**: While deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. In particular, remote sensing systems are usually constructed to optimize sensing cost-quality trade-offs with respect to human image interpretability. While some recent studies have explored remote sensing system design as a function of simple computer vision algorithm performance, there has been little work relating this design to the state-of-the-art in computer vision: deep learning with convolutional neural networks. We develop experimental systems to conduct this analysis, showing results with modern deep learning algorithms and recent overhead image data. Our results are compared to standard image quality measurements based on human visual perception, and we conclude not only that machine and human interpretability differ significantly, but that computer vision performance is largely self-consistent across a range of disparate conditions. This research is presented as a cornerstone for a new generation of sensor design systems which focus on computer algorithm performance instead of human visual perception.



### Deep Instance-Level Hard Negative Mining Model for Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1906.09681v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.09681v3)
- **Published**: 2019-06-24 01:00:05+00:00
- **Updated**: 2019-06-27 02:00:32+00:00
- **Authors**: Meng Li, Lin Wu, Arnold Wiliem, Kun Zhao, Teng Zhang, Brian C. Lovell
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Histopathology image analysis can be considered as a Multiple instance learning (MIL) problem, where the whole slide histopathology image (WSI) is regarded as a bag of instances (i.e, patches) and the task is to predict a single class label to the WSI. However, in many real-life applications such as computational pathology, discovering the key instances that trigger the bag label is of great interest because it provides reasons for the decision made by the system. In this paper, we propose a deep convolutional neural network (CNN) model that addresses the primary task of a bag classification on a WSI and also learns to identify the response of each instance to provide interpretable results to the final prediction. We incorporate the attention mechanism into the proposed model to operate the transformation of instances and learn attention weights to allow us to find key patches. To perform a balanced training, we introduce adaptive weighing in each training bag to explicitly adjust the weight distribution in order to concentrate more on the contribution of hard samples. Based on the learned attention weights, we further develop a solution to boost the classification performance by generating the bags with hard negative instances. We conduct extensive experiments on colon and breast cancer histopathology data and show that our framework achieves state-of-the-art performance.



### Refined-Segmentation R-CNN: A Two-stage Convolutional Neural Network for Punctate White Matter Lesion Segmentation in Preterm Infants
- **Arxiv ID**: http://arxiv.org/abs/1906.09684v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09684v2)
- **Published**: 2019-06-24 01:19:24+00:00
- **Updated**: 2019-07-01 02:25:57+00:00
- **Authors**: Yalong Liu, Jie Li, Ying Wang, Miaomiao Wang, Xianjun Li, Zhicheng Jiao, Jian Yang, Xingbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of punctate white matter lesion (PWML) in infantile brains by an automatic algorithm can reduce the potential risk of postnatal development. How to segment PWML effectively has become one of the active topics in medical image segmentation in recent years. In this paper, we construct an efficient two-stage PWML semantic segmentation network based on the characteristics of the lesion, called refined segmentation R-CNN (RS RCNN). We propose a heuristic RPN (H-RPN) which can utilize surrounding information around the PWMLs for heuristic segmentation. Also, we design a lightweight segmentation network to segment the lesion in a fast way. Densely connected conditional random field (DCRF) is used to optimize the segmentation results. We only use T1w MRIs to segment PWMLs. The result shows that our model can well segment the lesion of ordinary size or even pixel size. The Dice similarity coefficient reaches 0.6616, the sensitivity is 0.7069, the specificity is 0.9997, and the Hausdorff distance is 52.9130. The proposed method outperforms the state-of-the-art algorithm. (The code of this paper is available on https://github.com/YalongLiu/Refined-Segmentation-R-CNN)



### Bayesian Uncertainty Matching for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.09693v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09693v1)
- **Published**: 2019-06-24 02:57:22+00:00
- **Updated**: 2019-06-24 02:57:22+00:00
- **Authors**: Jun Wen, Nenggan Zheng, Junsong Yuan, Zhefeng Gong, Changyou Chen
- **Comment**: IJCAI-2019 Accepted
- **Journal**: None
- **Summary**: Domain adaptation is an important technique to alleviate performance degradation caused by domain shift, e.g., when training and test data come from different domains. Most existing deep adaptation methods focus on reducing domain shift by matching marginal feature distributions through deep transformations on the input features, due to the unavailability of target domain labels. We show that domain shift may still exist via label distribution shift at the classifier, thus deteriorating model performances. To alleviate this issue, we propose an approximate joint distribution matching scheme by exploiting prediction uncertainty. Specifically, we use a Bayesian neural network to quantify prediction uncertainty of a classifier. By imposing distribution matching on both features and labels (via uncertainty), label distribution mismatching in source and target data is effectively alleviated, encouraging the classifier to produce consistent predictions across domains. We also propose a few techniques to improve our method by adaptively reweighting domain adaptation loss to achieve nontrivial distribution matching and stable training. Comparisons with state of the art unsupervised domain adaptation methods on three popular benchmark datasets demonstrate the superiority of our approach, especially on the effectiveness of alleviating negative transfer.



### Dense Scale Network for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1906.09707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09707v1)
- **Published**: 2019-06-24 03:33:35+00:00
- **Updated**: 2019-06-24 03:33:35+00:00
- **Authors**: Feng Dai, Hao Liu, Yike Ma, Juan Cao, Qiang Zhao, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd counting has been widely studied by computer vision community in recent years. Due to the large scale variation, it remains to be a challenging task. Previous methods adopt either multi-column CNN or single-column CNN with multiple branches to deal with this problem. However, restricted by the number of columns or branches, these methods can only capture a few different scales and have limited capability. In this paper, we propose a simple but effective network called DSNet for crowd counting, which can be easily trained in an end-to-end fashion. The key component of our network is the dense dilated convolution block, in which each dilation layer is densely connected with the others to preserve information from continuously varied scales. The dilation rates in dilation layers are carefully selected to prevent the block from gridding artifacts. To further enlarge the range of scales covered by the network, we cascade three blocks and link them with dense residual connections. We also introduce a novel multi-scale density level consistency loss for performance improvement. To evaluate our method, we compare it with state-of-the-art algorithms on four crowd counting datasets (ShanghaiTech, UCF-QNRF, UCF_CC_50 and UCSD). Experimental results demonstrate that DSNet can achieve the best performance and make significant improvements on all the four datasets (30% on the UCF-QNRF and UCF_CC_50, and 20% on the others).



### Mixup of Feature Maps in a Hidden Layer for Training of Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.09739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09739v1)
- **Published**: 2019-06-24 06:10:17+00:00
- **Updated**: 2019-06-24 06:10:17+00:00
- **Authors**: Hideki Oki, Takio Kurita
- **Comment**: 11 pages, 5 figures
- **Journal**: Neural Information Processing 25th International Conference
  (ICONIP2018) Proceedings Part II
- **Summary**: The deep Convolutional Neural Network (CNN) became very popular as a fundamental technique for image classification and objects recognition. To improve the recognition accuracy for the more complex tasks, deeper networks have being introduced. However, the recognition accuracy of the trained deep CNN drastically decreases for the samples which are obtained from the outside regions of the training samples. To improve the generalization ability for such samples, Krizhevsky et al. proposed to generate additional samples through transformations from the existing samples and to make the training samples richer. This method is known as data augmentation. Hongyi Zhang et al. introduced data augmentation method called mixup which achieves state-of-the-art performance in various datasets. Mixup generates new samples by mixing two different training samples. Mixing of the two images is implemented with simple image morphing. In this paper, we propose to apply mixup to the feature maps in a hidden layer. To implement the mixup in the hidden layer we use the Siamese network or the triplet network architecture to mix feature maps. From the experimental comparison, it is observed that the mixup of the feature maps obtained from the first convolution layer is more effective than the original image mixup.



### Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel
- **Arxiv ID**: http://arxiv.org/abs/1906.09744v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09744v3)
- **Published**: 2019-06-24 06:49:04+00:00
- **Updated**: 2021-07-08 04:20:20+00:00
- **Authors**: Ye Zhu, Kai Ming Ting
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new insight into improving the performance of Stochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of Gaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects. First, the use of Isolation kernel in t-SNE overcomes the drawback of misrepresenting some structures in the data, which often occurs when Gaussian kernel is applied in t-SNE. This is because Gaussian kernel determines each local bandwidth based on one local point only, while Isolation kernel is derived directly from the data based on space partitioning. Second, the use of Isolation kernel yields a more efficient similarity computation because data-dependent Isolation kernel has only one parameter that needs to be tuned. In contrast, the use of data-independent Gaussian kernel increases the computational cost by determining n bandwidths for a dataset of n points. As the root cause of these deficiencies in t-SNE is Gaussian kernel, we show that simply replacing Gaussian kernel with Isolation kernel in t-SNE significantly improves the quality of the final visualisation output (without creating misrepresented structures) and removes one key obstacle that prevents t-SNE from processing large datasets. Moreover, Isolation kernel enables t-SNE to deal with large-scale datasets in less runtime without trading off accuracy, unlike existing methods in speeding up t-SNE.



### Resolution-invariant Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1906.09748v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09748v2)
- **Published**: 2019-06-24 07:03:20+00:00
- **Updated**: 2019-06-28 06:28:02+00:00
- **Authors**: Shunan Mao, Shiliang Zhang, Ming Yang
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: Exploiting resolution invariant representation is critical for person Re-Identification (ReID) in real applications, where the resolutions of captured person images may vary dramatically. This paper learns person representations robust to resolution variance through jointly training a Foreground-Focus Super-Resolution (FFSR) module and a Resolution-Invariant Feature Extractor (RIFE) by end-to-end CNN learning. FFSR upscales the person foreground using a fully convolutional auto-encoder with skip connections learned with a foreground focus training loss. RIFE adopts two feature extraction streams weighted by a dual-attention block to learn features for low and high resolution images, respectively. These two complementary modules are jointly trained, leading to a strong resolution invariant representation. We evaluate our methods on five datasets containing person images at a large range of resolutions, where our methods show substantial superiority to existing solutions. For instance, we achieve Rank-1 accuracy of 36.4% and 73.3% on CAVIAR and MLR-CUHK03, outperforming the state-of-the art by 2.9% and 2.6%, respectively.



### Cascade R-CNN: High Quality Object Detection and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.09756v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09756v1)
- **Published**: 2019-06-24 07:22:28+00:00
- **Updated**: 2019-06-24 07:22:28+00:00
- **Authors**: Zhaowei Cai, Nuno Vasconcelos
- **Comment**: extension of arXiv:1712.00726 "Cascade R-CNN: Delving into High
  Quality Object Detection"
- **Journal**: None
- **Summary**: In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its \textit{quality}. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN. To facilitate future research, two implementations are made available at \url{https://github.com/zhaoweicai/cascade-rcnn} (Caffe) and \url{https://github.com/zhaoweicai/Detectron-Cascade-RCNN} (Detectron).



### Improving CCTA based lesions' hemodynamic significance assessment by accounting for partial volume modeling in automatic coronary lumen segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.09763v1
- **DOI**: 10.1002/mp.12121
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1906.09763v1)
- **Published**: 2019-06-24 07:45:26+00:00
- **Updated**: 2019-06-24 07:45:26+00:00
- **Authors**: Moti Freiman, Hannes Nickisch, Sven Prevrhal, Holger Schmitt, Mani Vembar, Pál Maurovich-Horvat, Patrick Donnelly, Liran Goshen
- **Comment**: None
- **Journal**: Medical Physics 44: 1040-1049, 2017
- **Summary**: Purpose: The goal of this study was to assess the potential added benefit of accounting for partial volume effects (PVE) in an automatic coronary lumen segmentation algorithm from coronary computed tomography angiography (CCTA). Materials and methods: We assessed the potential added value of PVE integration as a part of the automatic coronary lumen segmentation algorithm by means of segmentation accuracy using the MICCAI 2012 challenge framework and by means of flow simulation overall accuracy, sensitivity, specificity, negative and positive predictive values and the receiver operated characteristic (ROC) area under the curve. We also evaluated the potential benefit of accounting for PVE in automatic segmentation for flow-simulation for lesions that were diagnosed as obstructive based on CCTA, which could have indicated a need for an invasive exam and revascularization. Results: Our segmentation algorithm improves the maximal surface distance error by ~39% compared to previously published method on the 18 datasets 50 from the MICCAI 2012 challenge with comparable Dice and mean surface distance. Results with and without accounting for PVE were comparable. In contrast, integrating PVE analysis into an automatic coronary lumen segmentation algorithm improved the flow simulation specificity from 0.6 to 0.68 with the same sensitivity of 0.83. Also, accounting for PVE improved the area under the ROC curve for detecting hemodynamically significant CAD from 0.76 to 0.8 compared to automatic segmentation without PVE analysis with invasive FFR threshold of 0.8 as the reference standard. The improvement in the AUC was statistically significant (N=76, Delong's test, p=0.012). Conclusion: Accounting for the partial volume effects in automatic coronary lumen segmentation algorithms has the potential to improve the accuracy of CCTA-based hemodynamic assessment of coronary artery lesions.



### Saliency Detection With Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.09806v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09806v1)
- **Published**: 2019-06-24 09:29:59+00:00
- **Updated**: 2019-06-24 09:29:59+00:00
- **Authors**: Hooman Misaghi, Reza Askari Moghadam, Ali Mahmoudi, Kurosh Madani
- **Comment**: None
- **Journal**: Proceedings of 61st Research World International Conference,
  Ottawa, Canada, 27th -28th March, 2019
- **Summary**: Saliency detection is an important task in image processing as it can solve many problems and it usually is the first step in for other processes. Convolutional neural networks have been proved to be very effective on several image processing tasks such as classification, segmentation, semantic colorization and object manipulation. Besides, using the weights of a pretrained networks is a common practice for enhancing the accuracy of a network. In this paper a fully convolutional neural network which uses a part of VGG-16 is proposed for saliency detection in images.



### ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.09826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09826v1)
- **Published**: 2019-06-24 10:05:32+00:00
- **Updated**: 2019-06-24 10:05:32+00:00
- **Authors**: Yu Wang, Quan Zhou, Xiaofu Wu
- **Comment**: 12 pages, 3 figures, 4 tables, submitted. Code can be found at
  https://github.com/xiaoyufenfei/ESNet
- **Journal**: None
- **Summary**: The recent years have witnessed great advances for semantic segmentation using deep convolutional neural networks (DCNNs). However, a large number of convolutional layers and feature channels lead to semantic segmentation as a computationally heavy task, which is disadvantage to the scenario with limited resources. In this paper, we design an efficient symmetric network, called (ESNet), to address this problem. The whole network has nearly symmetric architecture, which is mainly composed of a series of factorized convolution unit (FCU) and its parallel counterparts (PFCU). On one hand, the FCU adopts a widely-used 1D factorized convolution in residual layers. On the other hand, the parallel version employs a transform-split-transform-merge strategy in the designment of residual module, where the split branch adopts dilated convolutions with different rate to enlarge receptive field. Our model has nearly 1.6M parameters, and is able to be performed over 62 FPS on a single GTX 1080Ti GPU. The experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off for real-time semantic segmentation on CityScapes dataset.



### Interactive Optimization of Generative Image Modeling using Sequential Subspace Search and Content-based Guidance
- **Arxiv ID**: http://arxiv.org/abs/1906.09840v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.09840v3)
- **Published**: 2019-06-24 10:34:37+00:00
- **Updated**: 2020-08-29 09:05:11+00:00
- **Authors**: Toby Chong Long Hin, I-Chao Shen, Issei Sato, Takeo Igarashi
- **Comment**: 13 pages, Toby Chong Long Hin and I-Chao Shen contributed equally to
  the paper
- **Journal**: None
- **Summary**: Generative image modeling techniques such as GAN demonstrate highly convincing image generation result. However, user interaction is often necessary to obtain the desired results. Existing attempts add interactivity but require either tailored architectures or extra data. We present a human-in-the-optimization method that allows users to directly explore and search the latent vector space of generative image modeling. Our system provides multiple candidates by sampling the latent vector space, and the user selects the best blending weights within the subspace using multiple sliders. In addition, the user can express their intention through image editing tools. The system samples latent vectors based on inputs and presents new candidates to the user iteratively. An advantage of our formulation is that one can apply our method to arbitrary pre-trained model without developing specialized architecture or data. We demonstrate our method with various generative image modeling applications, and show superior performance in a comparative user study with prior art iGAN.



### Adversarial Multimodal Network for Movie Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1906.09844v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09844v2)
- **Published**: 2019-06-24 10:44:48+00:00
- **Updated**: 2020-03-12 07:13:55+00:00
- **Authors**: Zhaoquan Yuan, Siyuan Sun, Lixin Duan, Xiao Wu, Changsheng Xu
- **Comment**: We will revise the paper
- **Journal**: None
- **Summary**: Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, as inspired by generative adversarial networks, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions). Moreover, we introduce a self-attention mechanism to enforce the so-called consistency constraints in order to preserve the self-correlation of visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the MovieQA dataset show the effectiveness of our proposed AMN over other published state-of-the-art methods.



### Pose Estimation for Non-Cooperative Rendezvous Using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.09868v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.09868v1)
- **Published**: 2019-06-24 11:51:57+00:00
- **Updated**: 2019-06-24 11:51:57+00:00
- **Authors**: Sumant Sharma, Simone D'Amico
- **Comment**: Presented at AIAA/AAS Space Flight Mechanics Meeting January 2019
- **Journal**: None
- **Summary**: This work introduces the Spacecraft Pose Network (SPN) for on-board estimation of the pose, i.e., the relative position and attitude, of a known non-cooperative spacecraft using monocular vision. In contrast to other state-of-the-art pose estimation approaches for spaceborne applications, the SPN method does not require the formulation of hand-engineered features and only requires a single grayscale image to determine the pose of the spacecraft relative to the camera. The SPN method uses a Convolutional Neural Network (CNN) with three branches to solve for the pose. The first branch of the CNN bootstraps a state-of-the-art object detector to detect a 2D bounding box around the target spacecraft. The region inside the bounding box is then used by the other two branches of the CNN to determine the attitude by initially classifying the input region into discrete coarse attitude labels before regressing to a finer estimate. The SPN method then uses a novel Gauss-Newton algorithm to estimate the position by using the constraints imposed by the detected 2D bounding box and the estimated attitude. The secondary contribution of this work is the generation of the Spacecraft PosE Estimation Dataset (SPEED). SPEED consists of synthetic as well as actual camera images of a mock-up of the Tango spacecraft from the PRISMA mission. The synthetic images are created by fusing OpenGL-based renderings of the spacecraft's 3D model with actual images of the Earth captured by the Himawari-8 meteorological satellite. The actual camera images are created using a 7 degrees-of-freedom robotic arm, which positions and orients a vision-based sensor with respect to a full-scale mock-up of the Tango spacecraft. The SPN method, trained only on synthetic images, produces degree-level attitude error and cm-level position errors when evaluated on the actual camera images not used during training.



### Knowledge Amalgamation from Heterogeneous Networks by Common Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.10546v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.10546v1)
- **Published**: 2019-06-24 12:33:24+00:00
- **Updated**: 2019-06-24 12:33:24+00:00
- **Authors**: Sihui Luo, Xinchao Wang, Gongfan Fang, Yao Hu, Dapeng Tao, Mingli Song
- **Comment**: IJCAI 2019, 7 pages, the 28th International Joint Conference on
  Artificial Intelligence (IJCAI 2019)
- **Journal**: None
- **Summary**: An increasing number of well-trained deep networks have been released online by researchers and developers, enabling the community to reuse them in a plug-and-play way without accessing the training annotations. However, due to the large number of network variants, such public-available trained models are often of different architectures, each of which being tailored for a specific task or dataset. In this paper, we study a deep-model reusing task, where we are given as input pre-trained networks of heterogeneous architectures specializing in distinct tasks, as teacher models. We aim to learn a multitalented and light-weight student model that is able to grasp the integrated knowledge from all such heterogeneous-structure teachers, again without accessing any human annotation. To this end, we propose a common feature learning scheme, in which the features of all teachers are transformed into a common space and the student is enforced to imitate them all so as to amalgamate the intact knowledge. We test the proposed approach on a list of benchmarks and demonstrate that the learned student is able to achieve very promising performance, superior to those of the teachers in their specialized tasks.



### Channel-by-Channel Demosaicking Networks with Embedded Spectral Correlation
- **Arxiv ID**: http://arxiv.org/abs/1906.09884v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09884v3)
- **Published**: 2019-06-24 12:33:49+00:00
- **Updated**: 2020-04-22 07:31:17+00:00
- **Authors**: Niu Yan, Jihong Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: Demosaicking is standardly the first step in today's Image Signal Processing (ISP) pipeline of digital cameras. It reconstructs image RGB values from the spatially and spectrally sparse Color Filter Array (CFA) samples, which are the original raw data digitized from electrical signals. High quality and low cost demosaicking is not only necessary for photography, but also fundamental for many machine vision tasks. This paper proposes an accurate and fast demosaicking model based on Convolutional Neural Networks (CNN) for the Bayer CFA, which is the most popular color filter arrangement adopted by digital camera manufacturers. Observing that each channel has different estimation complexity, we reconstruct each channel by an individual sub-network. Moreover, instead of directly estimating the red and blue values, our model infers the green-red and green-blue color difference. This strategy allows recovering the most complex channel by a light weight network. Although the total size of our model is significantly smaller than the state of the art demosaicking networks, it achieves substantially higher performance in both demosaicking quality and computational cost, as validated by extensive experiments. Source code will be released along with paper publication.



### Deep Exemplar-based Video Colorization
- **Arxiv ID**: http://arxiv.org/abs/1906.09909v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.09909v1)
- **Published**: 2019-06-24 12:56:36+00:00
- **Updated**: 2019-06-24 12:56:36+00:00
- **Authors**: Bo Zhang, Mingming He, Jing Liao, Pedro V. Sander, Lu Yuan, Amine Bermak, Dong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents the first end-to-end network for exemplar-based video colorization. The main challenge is to achieve temporal consistency while remaining faithful to the reference style. To address this issue, we introduce a recurrent framework that unifies the semantic correspondence and color propagation steps. Both steps allow a provided reference image to guide the colorization of every frame, thus reducing accumulated propagation errors. Video frames are colorized in sequence based on the colorization history, and its coherency is further enforced by the temporal consistency loss. All of these components, learned end-to-end, help produce realistic videos with good temporal stability. Experiments show our result is superior to the state-of-the-art methods both quantitatively and qualitatively.



### Integrating Knowledge and Reasoning in Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1906.09954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.09954v1)
- **Published**: 2019-06-24 13:44:34+00:00
- **Updated**: 2019-06-24 13:44:34+00:00
- **Authors**: Somak Aditya, Yezhou Yang, Chitta Baral
- **Comment**: 8 pages, 2 figures
- **Journal**: IJCAI 2019
- **Summary**: Deep learning based data-driven approaches have been successfully applied in various image understanding applications ranging from object recognition, semantic segmentation to visual question answering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few representative reasoning mechanisms, knowledge integration methods and their corresponding image understanding applications developed by various groups of researchers, approaching the problem from a variety of angles. Furthermore, we discuss upon key efforts on integrating external knowledge with neural networks. Taking cues from these efforts, we conclude by discussing potential pathways to improve reasoning capabilities.



### A Comparative Review of Recent Kinect-based Action Recognition Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1906.09955v1
- **DOI**: 10.1109/TIP.2019.2925285
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09955v1)
- **Published**: 2019-06-24 13:45:21+00:00
- **Updated**: 2019-06-24 13:45:21+00:00
- **Authors**: Lei Wang, Du Q. Huynh, Piotr Koniusz
- **Comment**: Accepted by the IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Video-based human action recognition is currently one of the most active research areas in computer vision. Various research studies indicate that the performance of action recognition is highly dependent on the type of features being extracted and how the actions are represented. Since the release of the Kinect camera, a large number of Kinect-based human action recognition techniques have been proposed in the literature. However, there still does not exist a thorough comparison of these Kinect-based techniques under the grouping of feature types, such as handcrafted versus deep learning features and depth-based versus skeleton-based features. In this paper, we analyze and compare ten recent Kinect-based algorithms for both cross-subject action recognition and cross-view action recognition using six benchmark datasets. In addition, we have implemented and improved some of these techniques and included their variants in the comparison. Our experiments show that the majority of methods perform better on cross-subject action recognition than cross-view action recognition, that skeleton-based features are more robust for cross-view recognition than depth-based features, and that deep learning features are suitable for large datasets.



### Cross-Domain Conditional Generative Adversarial Networks for Stereoscopic Hyperrealism in Surgical Training
- **Arxiv ID**: http://arxiv.org/abs/1906.10011v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1906.10011v1)
- **Published**: 2019-06-24 15:05:07+00:00
- **Updated**: 2019-06-24 15:05:07+00:00
- **Authors**: Sandy Engelhardt, Lalith Sharan, Matthias Karck, Raffaele De Simone, Ivo Wolf
- **Comment**: accepted for MICCAI 2019
- **Journal**: None
- **Summary**: Phantoms for surgical training are able to mimic cutting and suturing properties and patient-individual shape of organs, but lack a realistic visual appearance that captures the heterogeneity of surgical scenes. In order to overcome this in endoscopic approaches, hyperrealistic concepts have been proposed to be used in an augmented reality-setting, which are based on deep image-to-image transformation methods. Such concepts are able to generate realistic representations of phantoms learned from real intraoperative endoscopic sequences. Conditioned on frames from the surgical training process, the learned models are able to generate impressive results by transforming unrealistic parts of the image (e.g.\ the uniform phantom texture is replaced by the more heterogeneous texture of the tissue). Image-to-image synthesis usually learns a mapping $G:X~\to~Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$. However, it does not necessarily force the generated images to be consistent and without artifacts. In the endoscopic image domain this can affect depth cues and stereo consistency of a stereo image pair, which ultimately impairs surgical vision. We propose a cross-domain conditional generative adversarial network approach (GAN) that aims to generate more consistent stereo pairs. The results show substantial improvements in depth perception and realism evaluated by 3 domain experts and 3 medical students on a 3D monitor over the baseline method. In 84 of 90 instances our proposed method was preferred or rated equal to the baseline.



### Who said that?: Audio-visual speaker diarisation of real-world meetings
- **Arxiv ID**: http://arxiv.org/abs/1906.10042v1
- **DOI**: None
- **Categories**: **cs.SD**, cs.CV, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1906.10042v1)
- **Published**: 2019-06-24 16:06:13+00:00
- **Updated**: 2019-06-24 16:06:13+00:00
- **Authors**: Joon Son Chung, Bong-Jin Lee, Icksang Han
- **Comment**: Accepted to Interspeech 2019
- **Journal**: None
- **Summary**: The goal of this work is to determine 'who spoke when' in real-world meetings. The method takes surround-view video and single or multi-channel audio as inputs, and generates robust diarisation outputs. To achieve this, we propose a novel iterative approach that first enrolls speaker models using audio-visual correspondence, then uses the enrolled models together with the visual information to determine the active speaker. We show strong quantitative and qualitative performance on a dataset of real-world meetings. The method is also evaluated on the public AMI meeting corpus, on which we demonstrate results that exceed all comparable methods. We also show that beamforming can be used together with the video to further improve the performance when multi-channel audio is available.



### Complex Signal Denoising and Interference Mitigation for Automotive Radar Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.10044v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.10044v2)
- **Published**: 2019-06-24 16:07:52+00:00
- **Updated**: 2019-06-25 11:07:24+00:00
- **Authors**: Johanna Rock, Mate Toth, Elmar Messner, Paul Meissner, Franz Pernkopf
- **Comment**: FUSION 2019; 8 pages
- **Journal**: None
- **Summary**: Driver assistance systems as well as autonomous cars have to rely on sensors to perceive their environment. A heterogeneous set of sensors is used to perform this task robustly. Among them, radar sensors are indispensable because of their range resolution and the possibility to directly measure velocity. Since more and more radar sensors are deployed on the streets, mutual interference must be dealt with. In the so far unregulated automotive radar frequency band, a sensor must be capable of detecting, or even mitigating the harmful effects of interference, which include a decreased detection sensitivity. In this paper, we address this issue with Convolutional Neural Networks (CNNs), which are state-of-the-art machine learning tools. We show that the ability of CNNs to find structured information in data while preserving local information enables superior denoising performance. To achieve this, CNN parameters are found using training with simulated data and integrated into the automotive radar signal processing chain. The presented method is compared with the state of the art, highlighting its promising performance. Hence, CNNs can be employed for interference mitigation as an alternative to conventional signal processing methods. Code and pre-trained models are available at https://github.com/johanna-rock/imRICnn.



### SurReal: Fréchet Mean and Distance Transform for Complex-Valued Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.10048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10048v1)
- **Published**: 2019-06-24 16:12:31+00:00
- **Updated**: 2019-06-24 16:12:31+00:00
- **Authors**: Rudrasis Chakraborty, Jiayun Wang, Stella X. Yu
- **Comment**: IEEE Computer Vision and Pattern Recognition Workshop on Perception
  Beyond the Visible Spectrum, Long Beach, California, 16 June 2019 Best Paper
  Award
- **Journal**: None
- **Summary**: We develop a novel deep learning architecture for naturally complex-valued data, which is often subject to complex scaling ambiguity. We treat each sample as a field in the space of complex numbers. With the polar form of a complex-valued number, the general group that acts in this space is the product of planar rotation and non-zero scaling. This perspective allows us to develop not only a novel convolution operator using weighted Fr\'echet mean (wFM) on a Riemannian manifold, but also a novel fully connected layer operator using the distance to the wFM, with natural equivariant properties to non-zero scaling and planar rotation for the former and invariance properties for the latter.   Compared to the baseline approach of learning real-valued neural network models on the two-channel real-valued representation of complex-valued data, our method achieves surreal performance on two publicly available complex-valued datasets: MSTAR on SAR images and RadioML on radio frequency signals. On MSTAR, at 8% of the baseline model size and with fewer than 45,000 parameters, our model improves the target classification accuracy from 94% to 98% on this highly imbalanced dataset. On RadioML, our model achieves comparable RF modulation classification accuracy at 10% of the baseline model size.



### Efficient and Effective Context-Based Convolutional Entropy Modeling for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1906.10057v2
- **DOI**: 10.1109/TIP.2020.2985225
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.10057v2)
- **Published**: 2019-06-24 16:26:03+00:00
- **Updated**: 2020-03-28 17:26:12+00:00
- **Authors**: Mu Li, Kede Ma, Jane You, David Zhang, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Precise estimation of the probabilistic structure of natural images plays an essential role in image compression. Despite the recent remarkable success of end-to-end optimized image compression, the latent codes are usually assumed to be fully statistically factorized in order to simplify entropy modeling. However, this assumption generally does not hold true and may hinder compression performance. Here we present context-based convolutional networks (CCNs) for efficient and effective entropy modeling. In particular, a 3D zigzag scanning order and a 3D code dividing technique are introduced to define proper coding contexts for parallel entropy decoding, both of which boil down to place translation-invariant binary masks on convolution filters of CCNs. We demonstrate the promise of CCNs for entropy modeling in both lossless and lossy image compression. For the former, we directly apply a CCN to the binarized representation of an image to compute the Bernoulli distribution of each code for entropy estimation. For the latter, the categorical distribution of each code is represented by a discretized mixture of Gaussian distributions, whose parameters are estimated by three CCNs. We then jointly optimize the CCN-based entropy model along with analysis and synthesis transforms for rate-distortion performance. Experiments on the Kodak and Tecnick datasets show that our methods powered by the proposed CCNs generally achieve comparable compression performance to the state-of-the-art while being much faster.



### Image to Images Translation for Multi-Task Organ Segmentation and Bone Suppression in Chest X-Ray Radiography
- **Arxiv ID**: http://arxiv.org/abs/1906.10089v2
- **DOI**: 10.1109/TMI.2020.2974159
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.10089v2)
- **Published**: 2019-06-24 17:12:31+00:00
- **Updated**: 2019-12-31 18:59:21+00:00
- **Authors**: Mohammad Eslami, Solale Tabarestani, Shadi Albarqouni, Ehsan Adeli, Nassir Navab, Malek Adjouadi
- **Comment**: None
- **Journal**: None
- **Summary**: Chest X-ray radiography is one of the earliest medical imaging technologies and remains one of the most widely-used for diagnosis, screening, and treatment follow up of diseases related to lungs and heart. The literature in this field of research reports many interesting studies dealing with the challenging tasks of bone suppression and organ segmentation but performed separately, limiting any learning that comes with the consolidation of parameters that could optimize both processes. This study, and for the first time, introduces a multitask deep learning model that generates simultaneously the bone-suppressed image and the organ-segmented image, enhancing the accuracy of tasks, minimizing the number of parameters needed by the model and optimizing the processing time, all by exploiting the interplay between the network parameters to benefit the performance of both tasks. The architectural design of this model, which relies on a conditional generative adversarial network, reveals the process on how the well-established pix2pix network (image-to-image network) is modified to fit the need for multitasking and extending it to the new image-to-images architecture. The developed source code of this multitask model is shared publicly on Github as the first attempt for providing the two-task pix2pix extension, a supervised/paired/aligned/registered image-to-images translation which would be useful in many multitask applications. Dilated convolutions are also used to improve the results through a more effective receptive field assessment. The comparison with state-of-the-art algorithms along with ablation study and a demonstration video are provided to evaluate efficacy and gauge the merits of the proposed approach.



### An Empirical Comparison of FAISS and FENSHSES for Nearest Neighbor Search in Hamming Space
- **Arxiv ID**: http://arxiv.org/abs/1906.10095v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.10095v2)
- **Published**: 2019-06-24 17:24:11+00:00
- **Updated**: 2019-07-28 20:46:33+00:00
- **Authors**: Cun Mu, Binwei Yang, Zheng Yan
- **Comment**: SIGIR eCom'19
- **Journal**: None
- **Summary**: In this paper, we compare the performances of FAISS and FENSHSES on nearest neighbor search in Hamming space--a fundamental task with ubiquitous applications in nowadays eCommerce. Comprehensive evaluations are made in terms of indexing speed, search latency and RAM consumption. This comparison is conducted towards a better understanding on trade-offs between nearest neighbor search systems implemented in main memory and the ones implemented in secondary memory, which is largely unaddressed in literature.



### Audio-Visual Kinship Verification
- **Arxiv ID**: http://arxiv.org/abs/1906.10096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10096v1)
- **Published**: 2019-06-24 17:24:56+00:00
- **Updated**: 2019-06-24 17:24:56+00:00
- **Authors**: Xiaoting Wu, Eric Granger, Xiaoyi Feng
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Visual kinship verification entails confirming whether or not two individuals in a given pair of images or videos share a hypothesized kin relation. As a generalized face verification task, visual kinship verification is particularly difficult with low-quality found Internet data. Due to uncontrolled variations in background, pose, facial expression, blur, illumination and occlusion, state-of-the-art methods fail to provide high level of recognition accuracy. As with many other visual recognition tasks, kinship verification may benefit from combining visual and audio signals. However, voice-based kinship verification has received very little prior attention. We hypothesize that the human voice contains kin-related cues that are complementary to visual cues. In this paper, we address, for the first time, the use of audio-visual information from face and voice modalities to perform kinship verification. We first propose a new multi-modal kinship dataset, called TALking KINship (TALKIN), that contains several pairs of Internet-quality video sequences. Using TALKIN, we then study the utility of various kinship verification methods including traditional local feature based methods, statistical methods and more recent deep learning approaches. Then, early and late fusion methods are evaluated on the TALKIN dataset for the study of kinship verification with both face and voice modalities. Finally, we propose a deep Siamese fusion network with contrastive loss for multi-modal fusion of kinship relations. Extensive experiments on the TALKIN dataset indicate that by combining face and voice modalities, the proposed Siamese network can provide a significantly higher level of accuracy compared to baseline uni-modal and multi-modal fusion techniques. Experimental results also indicate that audio (vocal) information is complementary (to facial information) and useful for kinship verification.



### LMVP: Video Predictor with Leaked Motion Information
- **Arxiv ID**: http://arxiv.org/abs/1906.10101v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.10101v1)
- **Published**: 2019-06-24 17:36:27+00:00
- **Updated**: 2019-06-24 17:36:27+00:00
- **Authors**: Dong Wang, Yitong Li, Wei Cao, Liqun Chen, Qi Wei, Lawrence Carin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Leaked Motion Video Predictor (LMVP) to predict future frames by capturing the spatial and temporal dependencies from given inputs. The motion is modeled by a newly proposed component, motion guider, which plays the role of both learner and teacher. Specifically, it {\em learns} the temporal features from real data and {\em guides} the generator to predict future frames. The spatial consistency in video is modeled by an adaptive filtering network. To further ensure the spatio-temporal consistency of the prediction, a discriminator is also adopted to distinguish the real and generated frames. Further, the discriminator leaks information to the motion guider and the generator to help the learning of motion. The proposed LMVP can effectively learn the static and temporal features in videos without the need for human labeling. Experiments on synthetic and real data demonstrate that LMVP can yield state-of-the-art results.



### Remote Estimation of Free-Flow Speeds
- **Arxiv ID**: http://arxiv.org/abs/1906.10104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10104v1)
- **Published**: 2019-06-24 17:41:46+00:00
- **Updated**: 2019-06-24 17:41:46+00:00
- **Authors**: Weilian Song, Tawfiq Salem, Hunter Blanton, Nathan Jacobs
- **Comment**: 4 pages, 4 figures, IGARSS 2019 camera-ready submission
- **Journal**: None
- **Summary**: We propose an automated method to estimate a road segment's free-flow speed from overhead imagery and road metadata. The free-flow speed of a road segment is the average observed vehicle speed in ideal conditions, without congestion or adverse weather. Standard practice for estimating free-flow speeds depends on several road attributes, including grade, curve, and width of the right of way. Unfortunately, many of these fine-grained labels are not always readily available and are costly to manually annotate. To compensate, our model uses a small, easy to obtain subset of road features along with aerial imagery to directly estimate free-flow speed with a deep convolutional neural network (CNN). We evaluate our approach on a large dataset, and demonstrate that using imagery alone performs nearly as well as the road features and that the combination of imagery with road features leads to the highest accuracy.



### CMRNet: Camera to LiDAR-Map Registration
- **Arxiv ID**: http://arxiv.org/abs/1906.10109v3
- **DOI**: 10.1109/ITSC.2019.8917470
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.10109v3)
- **Published**: 2019-06-24 17:53:28+00:00
- **Updated**: 2021-07-09 11:50:34+00:00
- **Authors**: Daniele Cattaneo, Matteo Vaghi, Augusto Luis Ballardini, Simone Fontana, Domenico Giorgio Sorrenti, Wolfram Burgard
- **Comment**: Accepted for presentation at IEEE ITSC2019
- **Journal**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC) pp.
  1283-1289
- **Summary**: In this paper we present CMRNet, a realtime approach based on a Convolutional Neural Network to localize an RGB image of a scene in a map built from LiDAR data. Our network is not trained in the working area, i.e. CMRNet does not learn the map. Instead it learns to match an image to the map. We validate our approach on the KITTI dataset, processing each frame independently without any tracking procedure. CMRNet achieves 0.27m and 1.07deg median localization accuracy on the sequence 00 of the odometry dataset, starting from a rough pose estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is the first CNN-based approach that learns to match images from a monocular camera to a given, preexisting 3D LiDAR-map.



### GANalyze: Toward Visual Definitions of Cognitive Image Properties
- **Arxiv ID**: http://arxiv.org/abs/1906.10112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10112v1)
- **Published**: 2019-06-24 17:56:29+00:00
- **Updated**: 2019-06-24 17:56:29+00:00
- **Authors**: Lore Goetschalckx, Alex Andonian, Aude Oliva, Phillip Isola
- **Comment**: 17 pages, 15 figures
- **Journal**: None
- **Summary**: We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability, aesthetics, and emotional valence. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more or less memorable? GANs allow us to generate a manifold of natural-looking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more or less memorable. The resulting ``visual definitions" surface image properties (like ``object size") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. Visit the GANalyze website at http://ganalyze.csail.mit.edu/.



### RUBi: Reducing Unimodal Biases in Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1906.10169v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.10169v2)
- **Published**: 2019-06-24 18:55:24+00:00
- **Updated**: 2020-03-23 11:25:27+00:00
- **Authors**: Remi Cadene, Corentin Dancette, Hedi Ben-younes, Matthieu Cord, Devi Parikh
- **Comment**: NeurIPS 2019
  http://papers.nips.cc/paper/8371-rubi-reducing-unimodal-biases-for-visual-question-answering
- **Journal**: Advances in Neural Information Processing Systems 2019 (pp.
  839-850)
- **Summary**: Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings.   We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training.   Our code is available: github.com/cdancette/rubi.bootstrap.pytorch



### Planning Robot Motion using Deep Visual Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.10182v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.10182v1)
- **Published**: 2019-06-24 19:17:34+00:00
- **Updated**: 2019-06-24 19:17:34+00:00
- **Authors**: Meenakshi Sarkar, Prabhu Pradhan, Debasish Ghose
- **Comment**: 7th ICAPS Workshop on Planning and Robotics (PlanRob), 2019
- **Journal**: None
- **Summary**: In this paper, we introduce a novel framework that can learn to make visual predictions about the motion of a robotic agent from raw video frames. Our proposed motion prediction network (PROM-Net) can learn in a completely unsupervised manner and efficiently predict up to 10 frames in the future. Moreover, unlike any other motion prediction models, it is lightweight and once trained it can be easily implemented on mobile platforms that have very limited computing capabilities. We have created a new robotic data set comprising LEGO Mindstorms moving along various trajectories in three different environments under different lighting conditions for testing and training the network. Finally, we introduce a framework that would use the predicted frames from the network as an input to a model predictive controller for motion planning in unknown dynamic environments with moving obstacles.



### A Deep Regression Model for Seed Identification in Prostate Brachytherapy
- **Arxiv ID**: http://arxiv.org/abs/1906.10183v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1906.10183v1)
- **Published**: 2019-06-24 19:17:37+00:00
- **Updated**: 2019-06-24 19:17:37+00:00
- **Authors**: Yading Yuan, Ren-Dih Sheu, Luke Fu, Yeh-Chi Lo
- **Comment**: Accepted for presentation in MICCAI 2019 (8 pages, 4 figures)
- **Journal**: None
- **Summary**: Post-implant dosimetry (PID) is an essential step of prostate brachytherapy that utilizes CT to image the prostate and allow the location and dose distribution of the radioactive seeds to be directly related to the actual prostate. However, it it a very challenging task to identify these seeds in CT images due to the severe metal artifacts and high-overlapped appearance when multiple seeds clustered together. In this paper, we propose an automatic and efficient algorithm based on 3D deep fully convolutional network for identifying implanted seeds in CT images. Our method models the seed localization task as a supervised regression problem that projects the input CT image to a map where each element represents the probability that the corresponding input voxel belongs to a seed. This deep regression model significantly suppresses image artifacts and makes the post-processing much easier and more controllable. The proposed method is validated on a large clinical database with 7820 seeds in 100 patients, in which 5534 seeds from 70 patients were used for model training and validation. Our method correctly detected 2150 of 2286 (94.1%) seeds in the 30 testing patients, yielding 16% improvement as compared to a widely-used commercial seed finder software (VariSeed, Varian, Palo Alto, CA).



### Efficient Multi-Domain Network Learning by Covariance Normalization
- **Arxiv ID**: http://arxiv.org/abs/1906.10267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10267v1)
- **Published**: 2019-06-24 23:42:55+00:00
- **Updated**: 2019-06-24 23:42:55+00:00
- **Authors**: Yunsheng Li, Nuno Vasconcelos
- **Comment**: 10 Pages Accepted by CVPR 2019
- **Journal**: None
- **Summary**: The problem of multi-domain learning of deep networks is considered. An adaptive layer is induced per target domain and a novel procedure, denoted covariance normalization (CovNorm), proposed to reduce its parameters. CovNorm is a data driven method of fairly simple implementation, requiring two principal component analyzes (PCA) and fine-tuning of a mini-adaptation layer. Nevertheless, it is shown, both theoretically and experimentally, to have several advantages over previous approaches, such as batch normalization or geometric matrix approximations. Furthermore, CovNorm can be deployed both when target datasets are available sequentially or simultaneously. Experiments show that, in both cases, it has performance comparable to a fully fine-tuned network, using as few as 0.13% of the corresponding parameters per target domain.



### Serif or Sans: Visual Font Analytics on Book Covers and Online Advertisements
- **Arxiv ID**: http://arxiv.org/abs/1906.10269v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.10269v2)
- **Published**: 2019-06-24 23:56:00+00:00
- **Updated**: 2019-06-29 10:56:58+00:00
- **Authors**: Yuto Shinahara, Takuro Karamatsu, Daisuke Harada, Kota Yamaguchi, Seiichi Uchida
- **Comment**: Accepted by ICDAR2019
- **Journal**: None
- **Summary**: In this paper, we conduct a large-scale study of font statistics in book covers and online advertisements. Through the statistical study, we try to understand how graphic designers relate fonts and content genres and identify the relationship between font styles, colors, and genres. We propose an automatic approach to extract font information from graphic designs by applying a sequence of character detection, style classification, and clustering techniques to the graphic designs. The extracted font information is accumulated together with genre information, such as romance or business, for further trend analysis. Through our unique empirical study, we show that the collected font statistics reveal interesting trends in terms of how typographic design represents the impression and the atmosphere of the content genres.



