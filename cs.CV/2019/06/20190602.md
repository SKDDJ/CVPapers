# Arxiv Papers in cs.CV on 2019-06-02
### Incremental Few-Shot Learning for Pedestrian Attribute Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.00330v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00330v2)
- **Published**: 2019-06-02 02:49:01+00:00
- **Updated**: 2019-06-24 12:42:01+00:00
- **Authors**: Liuyu Xiang, Xiaoming Jin, Guiguang Ding, Jungong Han, Leida Li
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: Pedestrian attribute recognition has received increasing attention due to its important role in video surveillance applications. However, most existing methods are designed for a fixed set of attributes. They are unable to handle the incremental few-shot learning scenario, i.e. adapting a well-trained model to newly added attributes with scarce data, which commonly exists in the real world. In this work, we present a meta learning based method to address this issue. The core of our framework is a meta architecture capable of disentangling multiple attribute information and generalizing rapidly to new coming attributes. By conducting extensive experiments on the benchmark dataset PETA and RAP under the incremental few-shot setting, we show that our method is able to perform the task with competitive performances and low resource requirements.



### NeuralDivergence: Exploring and Understanding Neural Networks by Comparing Activation Distributions
- **Arxiv ID**: http://arxiv.org/abs/1906.00332v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00332v1)
- **Published**: 2019-06-02 03:03:51+00:00
- **Updated**: 2019-06-02 03:03:51+00:00
- **Authors**: Haekyu Park, Fred Hohman, Duen Horng Chau
- **Comment**: Published in PacificVis2019, Poster
- **Journal**: None
- **Summary**: As deep neural networks are increasingly used in solving high-stake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.



### Adversarial Examples for Edge Detection: They Exist, and They Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.00335v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00335v1)
- **Published**: 2019-06-02 03:51:21+00:00
- **Updated**: 2019-06-02 03:51:21+00:00
- **Authors**: Christian Cosgrove, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have recently advanced the state of the art in many tasks including edge and object boundary detection. However, in this paper, we demonstrate that these edge detectors inherit a troubling property of neural networks: they can be fooled by adversarial examples. We show that adding small perturbations to an image causes HED, a CNN-based edge detection model, to fail to locate edges, to detect nonexistent edges, and even to hallucinate arbitrary configurations of edges. More surprisingly, we find that these adversarial examples transfer to other CNN-based vision models. In particular, attacks on edge detection result in significant drops in accuracy in models trained to perform unrelated, high-level tasks like image classification and semantic segmentation. Our code will be made public.



### Data Augmentation for Object Detection via Progressive and Selective Instance-Switching
- **Arxiv ID**: http://arxiv.org/abs/1906.00358v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00358v2)
- **Published**: 2019-06-02 07:31:36+00:00
- **Updated**: 2019-07-11 01:40:05+00:00
- **Authors**: Hao Wang, Qilong Wang, Fan Yang, Weiqi Zhang, Wangmeng Zuo
- **Comment**: None
- **Journal**: None
- **Summary**: Collection of massive well-annotated samples is effective in improving object detection performance but is extremely laborious and costly. Instead of data collection and annotation, the recently proposed Cut-Paste methods [12, 15] show the potential to augment training dataset by cutting foreground objects and pasting them on proper new backgrounds. However, existing Cut-Paste methods cannot guarantee synthetic images always precisely model visual context, and all of them require external datasets. To handle above issues, this paper proposes a simple yet effective instance-switching (IS) strategy, which generates new training data by switching instances of same class from different images. Our IS naturally preserves contextual coherence in the original images while requiring no external dataset. For guiding our IS to obtain better object performance, we explore issues of instance imbalance and class importance in datasets, which frequently occur and bring adverse effect on detection performance. To this end, we propose a novel Progressive and Selective Instance-Switching (PSIS) method to augment training data for object detection. The proposed PSIS enhances instance balance by combining selective re-sampling with a class-balanced loss, and considers class importance by progressively augmenting training dataset guided by detection performance. The experiments are conducted on the challenging MS COCO benchmark, and results demonstrate our PSIS brings clear improvement over various state-of-the-art detectors (e.g., Faster R-CNN, FPN, Mask R-CNN and SNIPER), showing the superiority and generality of our PSIS. Code and models are available at: https://github.com/Hwang64/PSIS.



### Iterative Path Reconstruction for Large-Scale Inertial Navigation on Smartphones
- **Arxiv ID**: http://arxiv.org/abs/1906.00360v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00360v1)
- **Published**: 2019-06-02 07:41:01+00:00
- **Updated**: 2019-06-02 07:41:01+00:00
- **Authors**: Santiago Cort√©s Reina, Yuxin Hou, Juho Kannala, Arno Solin
- **Comment**: To appear in Proceedings FUSION 2019
- **Journal**: None
- **Summary**: Modern smartphones have all the sensing capabilities required for accurate and robust navigation and tracking. In specific environments some data streams may be absent, less reliable, or flat out wrong. In particular, the GNSS signal can become flawed or silent inside buildings or in streets with tall buildings. In this application paper, we aim to advance the current state-of-the-art in motion estimation using inertial measurements in combination with partial GNSS data on standard smartphones. We show how iterative estimation methods help refine the positioning path estimates in retrospective use cases that can cover both fixed-interval and fixed-lag scenarios. We compare estimation results provided by global iterated Kalman filtering methods to those of a visual-inertial tracking scheme (Apple ARKit). The practical applicability is demonstrated on real-world use cases on empirical data acquired from both smartphones and tablet devices.



### Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network
- **Arxiv ID**: http://arxiv.org/abs/1906.00377v1
- **DOI**: 10.1007/978-3-030-11018-5_24
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00377v1)
- **Published**: 2019-06-02 10:02:39+00:00
- **Updated**: 2019-06-02 10:02:39+00:00
- **Authors**: Feng Mao, Xiang Wu, Hui Xue, Rong Zhang
- **Comment**: ECCV 2018
- **Journal**: ECCV 2018 Workshops pp 262-270
- **Summary**: High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation re ecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.



### Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1906.00378v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00378v1)
- **Published**: 2019-06-02 10:05:26+00:00
- **Updated**: 2019-06-02 10:05:26+00:00
- **Authors**: Shizhe Chen, Qin Jin, Alexander Hauptmann
- **Comment**: Accepted by AAAI 2019
- **Journal**: None
- **Summary**: Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.



### Generating Diverse High-Fidelity Images with VQ-VAE-2
- **Arxiv ID**: http://arxiv.org/abs/1906.00446v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00446v1)
- **Published**: 2019-06-02 16:46:42+00:00
- **Updated**: 2019-06-02 16:46:42+00:00
- **Authors**: Ali Razavi, Aaron van den Oord, Oriol Vinyals
- **Comment**: None
- **Journal**: None
- **Summary**: We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.



### On The Radon-Nikodym Spectral Approach With Optimal Clustering
- **Arxiv ID**: http://arxiv.org/abs/1906.00460v17
- **DOI**: 10.2139/ssrn.3398755
- **Categories**: **cs.LG**, cs.CV, cs.NA, math.NA, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00460v17)
- **Published**: 2019-06-02 17:57:08+00:00
- **Updated**: 2021-09-12 15:42:07+00:00
- **Authors**: Vladislav Gennadievich Malyshkin
- **Comment**: Relation to PCA variation expansion is added. Whereas a regular PCA
  variation expansion depends on attributes normalizing, the PCA variation
  expansion in the Lebesgue quadrature arXiv:1807.06007 basis is unique thus
  does not depend on attributes scale, moreover it is invariant relatively any
  non-degenerated linear transform of input vector components. Christoffel
  function solution to vector label
- **Journal**: None
- **Summary**: Problems of interpolation, classification, and clustering are considered. In the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle / \langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on input attributes, all the answers are obtained from a generalized eigenproblem $|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the interpolation problem is a regular Radon-Nikodym derivative. The solution to the classification problem requires prior and posterior probabilities that are obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian approach new observations change only outcome probabilities, in the Radon-Nikodym approach not only outcome probabilities but also the probability space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable feature of the approach: both the probabilities and the probability space are constructed from the data. The Lebesgue quadrature technique can be also applied to the optimal clustering problem. The problem is solved by constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing feature of the Radon-Nikodym approach is the knowledge of the invariant group: all the answers are invariant relatively any non-degenerated linear transform of input vector $\mathbf{x}$ components. A software product implementing the algorithms of interpolation, classification, and optimal clustering is available from the authors.



### Truncated Cauchy Non-negative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1906.00495v1
- **DOI**: 10.1109/TPAMI.2017.2777841
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00495v1)
- **Published**: 2019-06-02 22:21:30+00:00
- **Updated**: 2019-06-02 22:21:30+00:00
- **Authors**: Naiyang Guan, Tongliang Liu, Yangmuzi Zhang, Dacheng Tao, Larry S. Davis
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (IEEE T-PAMI), vol. 41, no. 1, pp. 246-259, Jan. 2019
- **Summary**: Non-negative matrix factorization (NMF) minimizes the Euclidean distance between the data matrix and its low rank approximation, and it fails when applied to corrupted data because the loss function is sensitive to outliers. In this paper, we propose a Truncated CauchyNMF loss that handle outliers by truncating large errors, and develop a Truncated CauchyNMF to robustly learn the subspace on noisy datasets contaminated by outliers. We theoretically analyze the robustness of Truncated CauchyNMF comparing with the competing models and theoretically prove that Truncated CauchyNMF has a generalization bound which converges at a rate of order $O(\sqrt{{\ln n}/{n}})$, where $n$ is the sample size. We evaluate Truncated CauchyNMF by image clustering on both simulated and real datasets. The experimental results on the datasets containing gross corruptions validate the effectiveness and robustness of Truncated CauchyNMF for learning robust subspaces.



