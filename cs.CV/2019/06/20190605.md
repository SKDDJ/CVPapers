# Arxiv Papers in cs.CV on 2019-06-05
### PI-Net: A Deep Learning Approach to Extract Topological Persistence Images
- **Arxiv ID**: http://arxiv.org/abs/1906.01769v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.AT
- **Links**: [PDF](http://arxiv.org/pdf/1906.01769v2)
- **Published**: 2019-06-05 00:54:06+00:00
- **Updated**: 2020-05-23 14:51:14+00:00
- **Authors**: Anirudh Som, Hongjun Choi, Karthikeyan Natesan Ramamurthy, Matthew Buman, Pavan Turaga
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Topological features such as persistence diagrams and their functional approximations like persistence images (PIs) have been showing substantial promise for machine learning and computer vision applications. This is greatly attributed to the robustness topological representations provide against different types of physical nuisance variables seen in real-world data, such as view-point, illumination, and more. However, key bottlenecks to their large scale adoption are computational expenditure and difficulty incorporating them in a differentiable architecture. We take an important step in this paper to mitigate these bottlenecks by proposing a novel one-step approach to generate PIs directly from the input data. We design two separate convolutional neural network architectures, one designed to take in multi-variate time series signals as input and another that accepts multi-channel images as input. We call these networks Signal PI-Net and Image PI-Net respectively. To the best of our knowledge, we are the first to propose the use of deep learning for computing topological features directly from data. We explore the use of the proposed PI-Net architectures on two applications: human activity recognition using tri-axial accelerometer sensor data and image classification. We demonstrate the ease of fusion of PIs in supervised deep learning architectures and speed up of several orders of magnitude for extracting PIs from data. Our code is available at https://github.com/anirudhsom/PI-Net.



### Learning to Compose and Reason with Language Tree Structures for Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1906.01784v1
- **DOI**: 10.1109/TPAMI.2019.2911066
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01784v1)
- **Published**: 2019-06-05 02:03:55+00:00
- **Updated**: 2019-06-05 02:03:55+00:00
- **Authors**: Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, Hanwang Zhang
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI)
- **Journal**: None
- **Summary**: Grounding natural language in images, such as localizing "the black dog on the left of the tree", is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained and compositional language space. However, existing solutions rely on the association between the holistic language features and visual features, while neglect the nature of compositional reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by sub-trees. RVG-TREE can be trained end-to-end by using the Straight-Through Gumbel-Softmax estimator that allows the gradients from the continuous score functions passing through the discrete tree construction. Experiments on several benchmarks show that our model achieves the state-of-the-art performance with more explainable reasoning.



### PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised Cross-View Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1906.01792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01792v1)
- **Published**: 2019-06-05 02:38:26+00:00
- **Updated**: 2019-06-05 02:38:26+00:00
- **Authors**: Chengyuan Zhang, Lei Zhu, Shichao Zhang
- **Comment**: 32 pages
- **Journal**: None
- **Summary**: Person re-identification (person Re-Id) aims to retrieve the pedestrian images of a same person that captured by disjoint and non-overlapping cameras. Lots of researchers recently focuse on this hot issue and propose deep learning based methods to enhance the recognition rate in a supervised or unsupervised manner. However, two limitations that cannot be ignored: firstly, compared with other image retrieval benchmarks, the size of existing person Re-Id datasets are far from meeting the requirement, which cannot provide sufficient pedestrian samples for the training of deep model; secondly, the samples in existing datasets do not have sufficient human motions or postures coverage to provide more priori knowledges for learning. In this paper, we introduce a novel unsupervised pose augmentation cross-view person Re-Id scheme called PAC-GAN to overcome these limitations. We firstly present the formal definition of cross-view pose augmentation and then propose the framework of PAC-GAN that is a novel conditional generative adversarial network (CGAN) based approach to improve the performance of unsupervised corss-view person Re-Id. Specifically, The pose generation model in PAC-GAN called CPG-Net is to generate enough quantity of pose-rich samples from original image and skeleton samples. The pose augmentation dataset is produced by combining the synthesized pose-rich samples with the original samples, which is fed into the corss-view person Re-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the CPG-Net to improve the quality of new generated samples. To the best of our knowledge, we are the first try to enhance the unsupervised cross-view person Re-Id by pose augmentation, and the results of extensive experiments show that the proposed scheme can combat the state-of-the-arts.



### Fully Automated Pancreas Segmentation with Two-stage 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01795v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01795v2)
- **Published**: 2019-06-05 02:48:24+00:00
- **Updated**: 2019-07-25 22:29:47+00:00
- **Authors**: Ningning Zhao, Nuo Tong, Dan Ruan, Ke Sheng
- **Comment**: This paper has been accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Due to the fact that pancreas is an abdominal organ with very large variations in shape and size, automatic and accurate pancreas segmentation can be challenging for medical image analysis. In this work, we proposed a fully automated two stage framework for pancreas segmentation based on convolutional neural networks (CNN). In the first stage, a U-Net is trained for the down-sampled 3D volume segmentation. Then a candidate region covering the pancreas is extracted from the estimated labels. Motivated by the superior performance reported by renowned region based CNN, in the second stage, another 3D U-Net is trained on the candidate region generated in the first stage. We evaluated the performance of the proposed method on the NIH computed tomography (CT) dataset, and verified its superiority over other state-of-the-art 2D and 3D approaches for pancreas segmentation in terms of dice-sorensen coefficient (DSC) accuracy in testing. The mean DSC of the proposed method is 85.99%.



### Two-Stream Region Convolutional 3D Network for Temporal Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.02182v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02182v1)
- **Published**: 2019-06-05 02:48:37+00:00
- **Updated**: 2019-06-05 02:48:37+00:00
- **Authors**: Huijuan Xu, Abir Das, Kate Saenko
- **Comment**: Published in TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1703.07814, 1710.08011
- **Journal**: None
- **Summary**: We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.



### One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.01796v2
- **DOI**: 10.1109/TIP.2020.2973510
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01796v2)
- **Published**: 2019-06-05 02:50:08+00:00
- **Updated**: 2020-02-08 03:34:20+00:00
- **Authors**: Chenhong Zhou, Changxing Ding, Xinchao Wang, Zhentai Lu, Dacheng Tao
- **Comment**: 14 pages, 7 figures, To appear in IEEE Transactions on Image
  Processing
- **Journal**: None
- **Summary**: Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws, we propose a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks and design a cross-task guided attention (CGA) module which can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.



### StarNet: Pedestrian Trajectory Prediction using Deep Neural Network in Star Topology
- **Arxiv ID**: http://arxiv.org/abs/1906.01797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01797v2)
- **Published**: 2019-06-05 03:04:51+00:00
- **Updated**: 2020-01-13 03:38:19+00:00
- **Authors**: Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia
- **Comment**: Accepted by the 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: Pedestrian trajectory prediction is crucial for many important applications. This problem is a great challenge because of complicated interactions among pedestrians. Previous methods model only the pairwise interactions between pedestrians, which not only oversimplifies the interactions among pedestrians but also is computationally inefficient. In this paper, we propose a novel model StarNet to deal with these issues. StarNet has a star topology which includes a unique hub network and multiple host networks. The hub network takes observed trajectories of all pedestrians to produce a comprehensive description of the interpersonal interactions. Then the host networks, each of which corresponds to one pedestrian, consult the description and predict future trajectories. The star topology gives StarNet two advantages over conventional models. First, StarNet is able to consider the collective influence among all pedestrians in the hub network, making more accurate predictions. Second, StarNet is computationally efficient since the number of host network is linear to the number of pedestrians. Experiments on multiple public datasets demonstrate that StarNet outperforms multiple state-of-the-arts by a large margin in terms of both accuracy and efficiency.



### Artifact Disentanglement Network for Unsupervised Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1906.01806v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01806v5)
- **Published**: 2019-06-05 03:27:02+00:00
- **Updated**: 2019-11-28 01:33:36+00:00
- **Authors**: Haofu Liao, Wei-An Lin, Jianbo Yuan, S. Kevin Zhou, Jiebo Luo
- **Comment**: This work is accepted to MICCAI 2019. An extended version can be
  found at arXiv:1908.01104
- **Journal**: None
- **Summary**: Current deep neural network based approaches to computed tomography (CT) metal artifact reduction (MAR) are supervised methods which rely heavily on synthesized data for training. However, as synthesized data may not perfectly simulate the underlying physical mechanisms of CT imaging, the supervised methods often generalize poorly to clinical applications. To address this problem, we propose, to the best of our knowledge, the first unsupervised learning approach to MAR. Specifically, we introduce a novel artifact disentanglement network that enables different forms of generations and regularizations between the artifact-affected and artifact-free image domains to support unsupervised learning. Extensive experiments show that our method significantly outperforms the existing unsupervised models for image-to-image translation problems, and achieves comparable performance to existing supervised models on a synthesized dataset. When applied to clinical datasets, our method achieves considerable improvements over the supervised models. The source code of this paper is publicly available at https://github.com/liaohaofu/adn.



### Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)
- **Arxiv ID**: http://arxiv.org/abs/1906.01815v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01815v1)
- **Published**: 2019-06-05 04:08:47+00:00
- **Updated**: 2019-06-05 04:08:47+00:00
- **Authors**: Santiago Castro, Devamanyu Hazarika, Verónica Pérez-Rosas, Roger Zimmermann, Rada Mihalcea, Soujanya Poria
- **Comment**: Accepted at ACL 2019
- **Journal**: None
- **Summary**: Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities. The full dataset is publicly available for use at https://github.com/soujanyaporia/MUStARD



### Infant Contact-less Non-Nutritive Sucking Pattern Quantification via Facial Gesture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1906.01821v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01821v1)
- **Published**: 2019-06-05 04:45:58+00:00
- **Updated**: 2019-06-05 04:45:58+00:00
- **Authors**: Xiaofei Huang, Alaina Martens, Emily Zimmerman, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: Non-nutritive sucking (NNS) is defined as the sucking action that occurs when a finger, pacifier, or other object is placed in the baby's mouth, but there is no nutrient delivered. In addition to providing a sense of safety, NNS even can be regarded as an indicator of infant's central nervous system development. The rich data, such as sucking frequency, the number of cycles, and their amplitude during baby's non-nutritive sucking is important clue for judging the brain development of infants or preterm infants. Nowadays most researchers are collecting NNS data by using some contact devices such as pressure transducers. However, such invasive contact will have a direct impact on the baby's natural sucking behavior, resulting in significant distortion in the collected data. Therefore, we propose a novel contact-less NNS data acquisition and quantification scheme, which leverages the facial landmarks tracking technology to extract the movement signals of baby's jaw from recorded baby's sucking video. Since completion of the sucking action requires a large amount of synchronous coordination and neural integration of the facial muscles and the cranial nerves, the facial muscle movement signals accompanying baby's sucking pacifier can indirectly replace the NNS signal. We have evaluated our method on videos collected from several infants during their NNS behaviors and we have achieved the quantified NNS patterns closely comparable to results from visual inspection as well as contact-based sensor readings.



### A Feature Transfer Enabled Multi-Task Deep Learning Model on Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1906.01828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01828v1)
- **Published**: 2019-06-05 05:11:33+00:00
- **Updated**: 2019-06-05 05:11:33+00:00
- **Authors**: Fei Gao, Hyunsoo Yoon, Teresa Wu, Xianghua Chu
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection, segmentation and classification are three common tasks in medical image analysis. Multi-task deep learning (MTL) tackles these three tasks jointly, which provides several advantages saving computing time and resources and improving robustness against overfitting. However, existing multitask deep models start with each task as an individual task and integrate parallelly conducted tasks at the end of the architecture with one cost function. Such architecture fails to take advantage of the combined power of the features from each individual task at an early stage of the training. In this research, we propose a new architecture, FTMTLNet, an MTL enabled by feature transferring. Traditional transfer learning deals with the same or similar task from different data sources (a.k.a. domain). The underlying assumption is that the knowledge gained from source domains may help the learning task on the target domain. Our proposed FTMTLNet utilizes the different tasks from the same domain. Considering features from the tasks are different views of the domain, the combined feature maps can be well exploited using knowledge from multiple views to enhance the generalizability. To evaluate the validity of the proposed approach, FTMTLNet is compared with models from literature including 8 classification models, 4 detection models and 3 segmentation models using a public full field digital mammogram dataset for breast cancer diagnosis. Experimental results show that the proposed FTMTLNet outperforms the competing models in classification and detection and has comparable results in segmentation.



### Detecting Kissing Scenes in a Database of Hollywood Films
- **Arxiv ID**: http://arxiv.org/abs/1906.01843v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.01843v1)
- **Published**: 2019-06-05 06:31:26+00:00
- **Updated**: 2019-06-05 06:31:26+00:00
- **Authors**: Amir Ziai
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting scene types in a movie can be very useful for application such as video editing, ratings assignment, and personalization. We propose a system for detecting kissing scenes in a movie. This system consists of two components. The first component is a binary classifier that predicts a binary label (i.e. kissing or not) given a features exctracted from both the still frames and audio waves of a one-second segment. The second component aggregates the binary labels for contiguous non-overlapping segments into a set of kissing scenes. We experimented with a variety of 2D and 3D convolutional architectures such as ResNet, DesnseNet, and VGGish and developed a highly accurate kissing detector that achieves a validation F1 score of 0.95 on a diverse database of Hollywood films ranging many genres and spanning multiple decades. The code for this project is available at http://github.com/amirziai/kissing-detector.



### Compact Approximation for Polynomial of Covariance Feature
- **Arxiv ID**: http://arxiv.org/abs/1906.01851v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01851v1)
- **Published**: 2019-06-05 06:46:58+00:00
- **Updated**: 2019-06-05 06:46:58+00:00
- **Authors**: Yusuke Mukuta, Tatsuaki Machida, Tatsuya Harada
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Covariance pooling is a feature pooling method with good classification accuracy. Because covariance features consist of second-order statistics, the scale of the feature elements are varied. Therefore, normalizing covariance features using a matrix square root affects the performance improvement. When pooling methods are applied to local features extracted from CNN models, the accuracy increases when the pooling function is back-propagatable and the feature-extraction model is learned in an end-to-end manner. Recently, the iterative polynomial approximation method for the matrix square root of a covariance feature was proposed, and resulted in a faster and more stable training than the methods based on singular-value decomposition. In this paper, we propose an extension of compact bilinear pooling, which is a compact approximation of the standard covariance feature, to the polynomials of the covariance feature. Subsequently, we apply the proposed approximation to the polynomial corresponding to the matrix square root to obtain a compact approximation for the square root of the covariance feature. Our method approximates a higher-dimensional polynomial of a covariance by the weighted sum of the approximate features corresponding to a pair of local features based on the similarity of the local features. We apply our method for standard fine-grained image recognition datasets and demonstrate that the proposed method shows comparable accuracy with fewer dimensions than the original feature.



### Invariant Feature Coding using Tensor Product Representation
- **Arxiv ID**: http://arxiv.org/abs/1906.01857v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01857v3)
- **Published**: 2019-06-05 07:15:17+00:00
- **Updated**: 2023-03-08 07:57:17+00:00
- **Authors**: Yusuke Mukuta, Tatsuya Harada
- **Comment**: 26 pages, 41 figures
- **Journal**: None
- **Summary**: In this study, a novel feature coding method that exploits invariance for transformations represented by a finite group of orthogonal matrices is proposed. We prove that the group-invariant feature vector contains sufficient discriminative information when learning a linear classifier using convex loss minimization. Based on this result, a novel feature model that explicitly consider group action is proposed for principal component analysis and k-means clustering, which are commonly used in most feature coding methods, and global feature functions. Although the global feature functions are in general complex nonlinear functions, the group action on this space can be easily calculated by constructing these functions as tensor-product representations of basic representations, resulting in an explicit form of invariant feature functions. The effectiveness of our method is demonstrated on several image datasets.



### AssemblyNet: A Novel Deep Decision-Making Process for Whole Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.01862v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1906.01862v1)
- **Published**: 2019-06-05 07:35:37+00:00
- **Updated**: 2019-06-05 07:35:37+00:00
- **Authors**: Pierrick Coupé, Boris Mansencal, Michaël Clément, Rémi Giraud, Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, José V. Manjon
- **Comment**: None
- **Journal**: None
- **Summary**: Whole brain segmentation using deep learning (DL) is a very challenging task since the number of anatomical labels is very high compared to the number of available training images. To address this problem, previous DL methods proposed to use a global convolution neural network (CNN) or few independent CNNs. In this paper, we present a novel ensemble method based on a large number of CNNs processing different overlapping brain areas. Inspired by parliamentary decision-making systems, we propose a framework called AssemblyNet, made of two "assemblies" of U-Nets. Such a parliamentary system is capable of dealing with complex decisions and reaching a consensus quickly. AssemblyNet introduces sharing of knowledge among neighboring U-Nets, an "amendment" procedure made by the second assembly at higher-resolution to refine the decision taken by the first one, and a final decision obtained by majority voting. When using the same 45 training images, AssemblyNet outperforms global U-Net by 28% in terms of the Dice metric, patch-based joint label fusion by 15% and SLANT-27 by 10%. Finally, AssemblyNet demonstrates high capacity to deal with limited training data to achieve whole brain segmentation in practical training and testing times.



### Farm land weed detection with region-based deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01885v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01885v1)
- **Published**: 2019-06-05 08:57:34+00:00
- **Updated**: 2019-06-05 08:57:34+00:00
- **Authors**: Mohammad Ibrahim Sarker, Hyongsuk Kim
- **Comment**: 7 Pages, Published in ICROS 2017 32nd Control Robot System Society
  Conference
- **Journal**: None
- **Summary**: Machine learning has become a major field of research in order to handle more and more complex image detection problems. Among the existing state-of-the-art CNN models, in this paper a region-based, fully convolutional network, for fast and accurate object detection has been proposed based on the experimental results. Among the region based networks, ResNet is regarded as the most recent CNN architecture which has obtained the best results at ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2015. Deep residual networks (ResNets) can make the training process faster and attain more accuracy compared to their equivalent conventional neural networks. Being motivated with such unique attributes of ResNet, this paper evaluates the performance of fine-tuned ResNet for object classification of our weeds dataset. The dataset of farm land weeds detection is insufficient to train such deep CNN models. To overcome this shortcoming, we perform dropout techniques along with deep residual network for reducing over-fitting problem as well as applying data augmentation with the proposed ResNet to achieve a significant outperforming result from our weeds dataset. We achieved better object detection performance with Region-based Fully Convolutional Networks (R-FCN) technique which is latched with our proposed ResNet-101.



### Weakly Supervised Object Detection with 2D and 3D Regression Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01891v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01891v4)
- **Published**: 2019-06-05 09:08:38+00:00
- **Updated**: 2020-03-19 18:30:35+00:00
- **Authors**: Florian Dubost, Hieab Adams, Pinar Yilmaz, Gerda Bortsova, Gijs van Tulder, M. Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen de Bruijne
- **Comment**: New formatting. A few changes in introduction, discussion and
  conclusion
- **Journal**: None
- **Summary**: Finding automatically multiple lesions in large images is a common problem in medical image analysis. Solving this problem can be challenging if, during optimization, the automated method cannot access information about the location of the lesions nor is given single examples of the lesions. We propose a new weakly supervised detection method using neural networks, that computes attention maps revealing the locations of brain lesions. These attention maps are computed using the last feature maps of a segmentation network optimized only with global image-level labels. The proposed method can generate attention maps at full input resolution without need for interpolation during preprocessing, which allows small lesions to appear in attention maps. For comparison, we modify state-of-the-art methods to compute attention maps for weakly supervised object detection, by using a global regression objective instead of the more conventional classification objective. This regression objective optimizes the number of occurrences of the target object in an image, e.g. the number of brain lesions in a scan, or the number of digits in an image. We study the behavior of the proposed method in MNIST-based detection datasets, and evaluate it for the challenging detection of enlarged perivascular spaces - a type of brain lesion - in a dataset of 2202 3D scans with point-wise annotations in the center of all lesions in four brain regions. In the brain dataset, the weakly supervised detection methods come close to the human intrarater agreement in each region. The proposed method reaches the best area under the curve in two out of four regions, and has the lowest number of false positive detections in all regions, while its average sensitivity over all regions is similar to that of the other best methods. The proposed method can facilitate epidemiological and clinical studies of enlarged perivascular spaces.



### A Robust Roll Angle Estimation Algorithm Based on Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/1906.01894v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01894v1)
- **Published**: 2019-06-05 09:11:51+00:00
- **Updated**: 2019-06-05 09:11:51+00:00
- **Authors**: Rui Fan, Lujia Wang, Ming Liu, Ioannis Pitas
- **Comment**: 5 pages, six figures, 2019 EUSIPCO
- **Journal**: None
- **Summary**: This paper presents a robust roll angle estimation algorithm, which is developed from our previously published work, where the roll angle was estimated from a dense disparity map by minimizing a global energy using golden section search algorithm. In this paper, to achieve greater computational efficiency, we utilize gradient descent to optimize the aforementioned global energy. The experimental results illustrate that the proposed roll angle estimation algorithm takes fewer iterations to achieve the same precision as the previous method.



### AI-Skin : Skin Disease Recognition based on Self-learning and Wide Data Collection through a Closed Loop Framework
- **Arxiv ID**: http://arxiv.org/abs/1906.01895v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01895v1)
- **Published**: 2019-06-05 09:13:29+00:00
- **Updated**: 2019-06-05 09:13:29+00:00
- **Authors**: Min Chen, Ping Zhou, Di Wu, Long Hu, Mohammad Mehedi Hassan, Atif Alamri
- **Comment**: None
- **Journal**: None
- **Summary**: There are a lot of hidden dangers in the change of human skin conditions, such as the sunburn caused by long-time exposure to ultraviolet radiation, which not only has aesthetic impact causing psychological depression and lack of self-confidence, but also may even be life-threatening due to skin canceration. Current skin disease researches adopt the auto-classification system for improving the accuracy rate of skin disease classification. However, the excessive dependence on the image sample database is unable to provide individualized diagnosis service for different population groups. To overcome this problem, a medical AI framework based on data width evolution and self-learning is put forward in this paper to provide skin disease medical service meeting the requirement of real time, extendibility and individualization. First, the wide collection of data in the close-loop information flow of user and remote medical data center is discussed. Next, a data set filter algorithm based on information entropy is given, to lighten the load of edge node and meanwhile improve the learning ability of remote cloud analysis model. In addition, the framework provides an external algorithm load module, which can be compatible with the application requirements according to the model selected. Three kinds of deep learning model, i.e. LeNet-5, AlexNet and VGG16, are loaded and compared, which have verified the universality of the algorithm load module. The experiment platform for the proposed real-time, individualized and extensible skin disease recognition system is built. And the system's computation and communication delay under the interaction scenario between tester and remote data center are analyzed. It is demonstrated that the system we put forward is reliable and effective.



### Corn leaf detection using Region based convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1906.01900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01900v1)
- **Published**: 2019-06-05 09:20:33+00:00
- **Updated**: 2019-06-05 09:20:33+00:00
- **Authors**: Mohammad Ibrahim Sarker, Heechan Yang, Hyongsuk Kim
- **Comment**: 3 pages, published in ICROS 2017 Conference
- **Journal**: None
- **Summary**: The field of machine learning has become an increasingly budding area of research as more efficient methods are needed in the quest to handle more complex image detection challenges. To solve the problems of agriculture is more and more important because food is the fundamental of life. However, the detection accuracy in recent corn field systems are still far away from the demands in practice due to a number of different weeds. This paper presents a model to handle the problem of corn leaf detection in given digital images collected from farm field. Based on results of experiments conducted with several state-of-the-art models adopted by CNN, a region-based method has been proposed as a faster and more accurate method of corn leaf detection. Being motivated with such unique attributes of ResNet, we combine it with region based network (such as faster rcnn), which is able to automatically detect corn leaf in heavy weeds occlusion. The method is evaluated on the dataset from farm and we make an annotation ourselves. Our proposed method achieves significantly outperform in corn detection system.



### Baby steps towards few-shot learning with multiple semantics
- **Arxiv ID**: http://arxiv.org/abs/1906.01905v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01905v2)
- **Published**: 2019-06-05 09:28:59+00:00
- **Updated**: 2020-01-22 12:11:47+00:00
- **Authors**: Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from one or few visual examples is one of the key capabilities of humans since early infancy, but is still a significant challenge for modern AI systems. While considerable progress has been achieved in few-shot learning from a few image examples, much less attention has been given to the verbal descriptions that are usually provided to infants when they are presented with a new object. In this paper, we focus on the role of additional semantics that can significantly facilitate few-shot visual learning. Building upon recent advances in few-shot learning with additional semantic information, we demonstrate that further improvements are possible by combining multiple and richer semantics (category labels, attributes, and natural language descriptions). Using these ideas, we offer the community new results on the popular miniImageNet and CUB few-shot benchmarks, comparing favorably to the previous state-of-the-art results for both visual only and visual plus semantics-based approaches. We also performed an ablation study investigating the components and design choices of our approach.



### Combining crowd-sourcing and deep learning to explore the meso-scale organization of shallow convection
- **Arxiv ID**: http://arxiv.org/abs/1906.01906v3
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01906v3)
- **Published**: 2019-06-05 09:35:19+00:00
- **Updated**: 2020-04-21 12:05:30+00:00
- **Authors**: Stephan Rasp, Hauke Schulz, Sandrine Bony, Bjorn Stevens
- **Comment**: None
- **Journal**: None
- **Summary**: Humans excel at detecting interesting patterns in images, for example those taken from satellites. This kind of anecdotal evidence can lead to the discovery of new phenomena. However, it is often difficult to gather enough data of subjective features for significant analysis. This paper presents an example of how two tools that have recently become accessible to a wide range of researchers, crowd-sourcing and deep learning, can be combined to explore satellite imagery at scale. In particular, the focus is on the organization of shallow cumulus convection in the trade wind regions. Shallow clouds play a large role in the Earth's radiation balance yet are poorly represented in climate models. For this project four subjective patterns of organization were defined: Sugar, Flower, Fish and Gravel. On cloud labeling days at two institutes, 67 scientists screened 10,000 satellite images on a crowd-sourcing platform and classified almost 50,000 mesoscale cloud clusters. This dataset is then used as a training dataset for deep learning algorithms that make it possible to automate the pattern detection and create global climatologies of the four patterns. Analysis of the geographical distribution and large-scale environmental conditions indicates that the four patterns have some overlap with established modes of organization, such as open and closed cellular convection, but also differ in important ways. The results and dataset from this project suggests promising research questions. Further, this study illustrates that crowd-sourcing and deep learning complement each other well for the exploration of image datasets.



### Towards Document Image Quality Assessment: A Text Line Based Framework and A Synthetic Text Line Image Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.01907v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01907v1)
- **Published**: 2019-06-05 09:40:34+00:00
- **Updated**: 2019-06-05 09:40:34+00:00
- **Authors**: Hongyu Li, Fan Zhu, Junhua Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Since the low quality of document images will greatly undermine the chances of success in automatic text recognition and analysis, it is necessary to assess the quality of document images uploaded in online business process, so as to reject those images of low quality. In this paper, we attempt to achieve document image quality assessment and our contributions are twofold. Firstly, since document image quality assessment is more interested in text, we propose a text line based framework to estimate document image quality, which is composed of three stages: text line detection, text line quality prediction, and overall quality assessment. Text line detection aims to find potential text lines with a detector. In the text line quality prediction stage, the quality score is computed for each text line with a CNN-based prediction model. The overall quality of document images is finally assessed with the ensemble of all text line quality. Secondly, to train the prediction model, a large-scale dataset, comprising 52,094 text line images, is synthesized with diverse attributes. For each text line image, a quality label is computed with a piece-wise function. To demonstrate the effectiveness of the proposed framework, comprehensive experiments are evaluated on two popular document image quality assessment benchmarks. Our framework significantly outperforms the state-of-the-art methods by large margins on the large and complicated dataset.



### Semi-supervised semantic segmentation needs strong, varied perturbations
- **Arxiv ID**: http://arxiv.org/abs/1906.01916v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01916v5)
- **Published**: 2019-06-05 10:09:27+00:00
- **Updated**: 2020-08-11 16:23:40+00:00
- **Authors**: Geoff French, Samuli Laine, Timo Aila, Michal Mackiewicz, Graham Finlayson
- **Comment**: 21 pages, 7 figures, accepted to BMVC 2020
- **Journal**: None
- **Summary**: Consistency regularization describes a class of approaches that have yielded ground breaking results in semi-supervised classification problems. Prior work has established the cluster assumption - under which the data distribution consists of uniform class clusters of samples separated by low density regions - as important to its success. We analyze the problem of semantic segmentation and find that its' distribution does not exhibit low density regions separating classes and offer this as an explanation for why semi-supervised segmentation is a challenging problem, with only a few reports of success. We then identify choice of augmentation as key to obtaining reliable performance without such low-density regions. We find that adapted variants of the recently proposed CutOut and CutMix augmentation techniques yield state-of-the-art semi-supervised semantic segmentation results in standard datasets. Furthermore, given its challenging nature we propose that semantic segmentation acts as an effective acid test for evaluating semi-supervised regularizers. Implementation at: https://github.com/Britefury/cutmix-semisup-seg.



### Prediction of Soil Moisture Content Based On Satellite Data and Sequence-to-Sequence Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.03697v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.03697v1)
- **Published**: 2019-06-05 11:03:17+00:00
- **Updated**: 2019-06-05 11:03:17+00:00
- **Authors**: Natalia Efremova, Dmitry Zausaev, Gleb Antipov
- **Comment**: Presented on NeurIPS 2018 WiML workshop
- **Journal**: None
- **Summary**: The main objective of this study is to combine remote sensing and machine learning to detect soil moisture content. Growing population and food consumption has led to the need to improve agricultural yield and to reduce wastage of natural resources. In this paper, we propose a neural network architecture, based on recent work by the research community, that can make a strong social impact and aid United Nations Sustainable Development Goal of Zero Hunger. The main aims here are to: improve efficiency of water usage; reduce dependence on irrigation; increase overall crop yield; minimise risk of crop loss due to drought and extreme weather conditions. We achieve this by applying satellite imagery, crop segmentation, soil classification and NDVI and soil moisture prediction on satellite data, ground truth and climate data records. By applying machine learning to sensor data and ground data, farm management systems can evolve into a real time AI enabled platform that can provide actionable recommendations and decision support tools to the farmers.



### Efficient, Lexicon-Free OCR using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.01969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01969v1)
- **Published**: 2019-06-05 12:21:54+00:00
- **Updated**: 2019-06-05 12:21:54+00:00
- **Authors**: Marcin Namysl, Iuliu Konya
- **Comment**: Accepted for presentation in the 15th International Conference on
  Document Analysis and Recognition (ICDAR 2019)
- **Journal**: None
- **Summary**: Contrary to popular belief, Optical Character Recognition (OCR) remains a challenging problem when text occurs in unconstrained environments, like natural scenes, due to geometrical distortions, complex backgrounds, and diverse fonts. In this paper, we present a segmentation-free OCR system that combines deep learning methods, synthetic training data generation, and data augmentation techniques. We render synthetic training data using large text corpora and over 2000 fonts. To simulate text occurring in complex natural scenes, we augment extracted samples with geometric distortions and with a proposed data augmentation technique - alpha-compositing with background textures. Our models employ a convolutional neural network encoder to extract features from text images. Inspired by the recent progress in neural machine translation and language modeling, we examine the capabilities of both recurrent and convolutional neural networks in modeling the interactions between input elements.



### Efficient Codebook and Factorization for Second Order Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.01972v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01972v1)
- **Published**: 2019-06-05 12:27:55+00:00
- **Updated**: 2019-06-05 12:27:55+00:00
- **Authors**: Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein
- **Comment**: Accepted at IEEE International Conference on Image Processing (ICIP)
  2019
- **Journal**: None
- **Summary**: Learning rich and compact representations is an open topic in many fields such as object recognition or image retrieval. Deep neural networks have made a major breakthrough during the last few years for these tasks but their representations are not necessary as rich as needed nor as compact as expected. To build richer representations, high order statistics have been exploited and have shown excellent performances, but they produce higher dimensional features. While this drawback has been partially addressed with factorization schemes, the original compactness of first order models has never been retrieved, or at the cost of a strong performance decrease. Our method, by jointly integrating codebook strategy to factorization scheme, is able to produce compact representations while keeping the second order performances with few additional parameters. This formulation leads to state-of-the-art results on three image retrieval datasets.



### Visual Confusion Label Tree For Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.02012v1
- **DOI**: 10.1109/ICME.2018.8486612
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02012v1)
- **Published**: 2019-06-05 13:06:11+00:00
- **Updated**: 2019-06-05 13:06:11+00:00
- **Authors**: Yuntao Liu, Yong Dou, Ruochun Jin, Rongchun Li
- **Comment**: 9 pages, 5 figures, conference
- **Journal**: 2018 IEEE International Conference on Multimedia and Expo (ICME)
- **Summary**: Convolution neural network models are widely used in image classification tasks. However, the running time of such models is so long that it is not the conforming to the strict real-time requirement of mobile devices. In order to optimize models and meet the requirement mentioned above, we propose a method that replaces the fully-connected layers of convolution neural network models with a tree classifier. Specifically, we construct a Visual Confusion Label Tree based on the output of the convolution neural network models, and use a multi-kernel SVM plus classifier with hierarchical constraints to train the tree classifier. Focusing on those confusion subsets instead of the entire set of categories makes the tree classifier more discriminative and the replacement of the fully-connected layers reduces the original running time. Experiments show that our tree classifier obtains a significant improvement over the state-of-the-art tree classifier by 4.3% and 2.4% in terms of top-1 accuracy on CIFAR-100 and ImageNet datasets respectively. Additionally, our method achieves 124x and 115x speedup ratio compared with fully-connected layers on AlexNet and VGG16 without accuracy decline.



### OctopusNet: A Deep Learning Segmentation Network for Multi-modal Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1906.02031v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02031v2)
- **Published**: 2019-06-05 13:47:12+00:00
- **Updated**: 2019-08-22 06:36:43+00:00
- **Authors**: Yu Chen, Jiawei Chen, Dong Wei, Yuexiang Li, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models, such as the fully convolutional network (FCN), have been widely used in 3D biomedical segmentation and achieved state-of-the-art performance. Multiple modalities are often used for disease diagnosis and quantification. Two approaches are widely used in the literature to fuse multiple modalities in the segmentation networks: early-fusion (which stacks multiple modalities as different input channels) and late-fusion (which fuses the segmentation results from different modalities at the very end). These fusion methods easily suffer from the cross-modal interference caused by the input modalities which have wide variations. To address the problem, we propose a novel deep learning architecture, namely OctopusNet, to better leverage and fuse the information contained in multi-modalities. The proposed framework employs a separate encoder for each modality for feature extraction and exploits a hyper-fusion decoder to fuse the extracted features while avoiding feature explosion. We evaluate the proposed OctopusNet on two publicly available datasets, i.e. ISLES-2018 and MRBrainS-2013. The experimental results show that our framework outperforms the commonly-used feature fusion approaches and yields the state-of-the-art segmentation accuracy.



### Multi-way Encoding for Robustness
- **Arxiv ID**: http://arxiv.org/abs/1906.02033v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02033v2)
- **Published**: 2019-06-05 13:51:11+00:00
- **Updated**: 2020-01-15 18:30:50+00:00
- **Authors**: Donghyun Kim, Sarah Adel Bargal, Jianming Zhang, Stan Sclaroff
- **Comment**: Accepted at WACV 2020
- **Journal**: None
- **Summary**: Deep models are state-of-the-art for many computer vision tasks including image classification and object detection. However, it has been shown that deep models are vulnerable to adversarial examples. We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping. We demonstrate that by leveraging a different output encoding, multi-way encoding, we decorrelate source and target models, making target models more secure. Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks. We present robustness for black-box and white-box attacks on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.



### A GLCM Embedded CNN Strategy for Computer-aided Diagnosis in Intracerebral Hemorrhage
- **Arxiv ID**: http://arxiv.org/abs/1906.02040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02040v1)
- **Published**: 2019-06-05 14:12:21+00:00
- **Updated**: 2019-06-05 14:12:21+00:00
- **Authors**: Yifan Hu, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided diagnosis (CADx) systems have been shown to assist radiologists by providing classifications of all kinds of medical images like Computed tomography (CT) and Magnetic resonance (MR). Currently, convolutional neural networks play an important role in CADx. However, since CNN model should have a square-like input, it is usually difficult to directly apply the CNN algorithms on the irregular segmentation region of interests (ROIs) where the radiologists are interested in. In this paper, we propose a new approach to construct the model by extracting and converting the information of the irregular region into a fixed-size Gray-Level Co-Occurrence Matrix (GLCM) and then utilize the GLCM as one input of our CNN model. In this way, as an useful implementary to the original CNN, a couple of GLCM-based features are also extracted by CNN. Meanwhile, the network will pay more attention to the important lesion area and achieve a higher accuracy in classification. Experiments are performed on three classification databases: Hemorrhage, BraTS18 and Cervix to validate the universality of our innovative model. In conclusion, the proposed framework outperforms the corresponding state-of-art algorithms on each database with both test losses and classification accuracy as the evaluation criteria.



### Single-Camera Basketball Tracker through Pose and Semantic Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/1906.02042v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02042v2)
- **Published**: 2019-06-05 14:18:07+00:00
- **Updated**: 2019-07-10 09:42:43+00:00
- **Authors**: Adrià Arbués-Sangüesa, Coloma Ballester, Gloria Haro
- **Comment**: Accepted in the International Conference on Artificial Intelligence
  in Sports 2019 (ICAIS)
- **Journal**: None
- **Summary**: Tracking sports players is a widely challenging scenario, specially in single-feed videos recorded in tight courts, where cluttering and occlusions cannot be avoided. This paper presents an analysis of several geometric and semantic visual features to detect and track basketball players. An ablation study is carried out and then used to remark that a robust tracker can be built with Deep Learning features, without the need of extracting contextual ones, such as proximity or color similarity, nor applying camera stabilization techniques. The presented tracker consists of: (1) a detection step, which uses a pretrained deep learning model to estimate the players pose, followed by (2) a tracking step, which leverages pose and semantic information from the output of a convolutional layer in a VGG network. Its performance is analyzed in terms of MOTA over a basketball dataset with more than 10k instances.



### On the use of Pairwise Distance Learning for Brain Signal Classification with Limited Observations
- **Arxiv ID**: http://arxiv.org/abs/1906.02076v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.SP, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02076v2)
- **Published**: 2019-06-05 15:36:57+00:00
- **Updated**: 2019-09-06 11:40:27+00:00
- **Authors**: David Calhas, Enrique Romero, Rui Henriques
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing access to brain signal data using electroencephalography creates new opportunities to study electrophysiological brain activity and perform ambulatory diagnoses of neuronal diseases. This work proposes a pairwise distance learning approach for Schizophrenia classification relying on the spectral properties of the signal. Given the limited number of observations (i.e. the case and/or control individuals) in clinical trials, we propose a Siamese neural network architecture to learn a discriminative feature space from pairwise combinations of observations per channel. In this way, the multivariate order of the signal is used as a form of data augmentation, further supporting the network generalization ability. Convolutional layers with parameters learned under a cosine contrastive loss are proposed to adequately explore spectral images derived from the brain signal. Results on a case-control population show that the features extracted using the proposed neural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy and sensitivity) against spectral features, thus suggesting the existence of non-trivial, discriminative electrophysiological brain patterns.



### Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.02112v4
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02112v4)
- **Published**: 2019-06-05 16:40:54+00:00
- **Updated**: 2019-07-09 17:51:21+00:00
- **Authors**: Pingchuan Ma, Stavros Petridis, Maja Pantic
- **Comment**: Accepted for publication at Interspeech 2019
- **Journal**: None
- **Summary**: Several audio-visual speech recognition models have been recently proposed which aim to improve the robustness over audio-only models in the presence of noise. However, almost all of them ignore the impact of the Lombard effect, i.e., the change in speaking style in noisy environments which aims to make speech more intelligible and affects both the acoustic characteristics of speech and the lip movements. In this paper, we investigate the impact of the Lombard effect in audio-visual speech recognition. To the best of our knowledge, this is the first work which does so using end-to-end deep architectures and presents results on unseen speakers. Our results show that properly modelling Lombard speech is always beneficial. Even if a relatively small amount of Lombard speech is added to the training set then the performance in a real scenario, where noisy Lombard speech is present, can be significantly improved. We also show that the standard approach followed in the literature, where a model is trained and tested on noisy plain speech, provides a correct estimate of the video-only performance and slightly underestimates the audio-visual performance. In case of audio-only approaches, performance is overestimated for SNRs higher than -3dB and underestimated for lower SNRs.



### Do Image Classifiers Generalize Across Time?
- **Arxiv ID**: http://arxiv.org/abs/1906.02168v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02168v3)
- **Published**: 2019-06-05 17:55:42+00:00
- **Updated**: 2019-12-09 17:30:11+00:00
- **Authors**: Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, Ludwig Schmidt
- **Comment**: 23 pages, 11 tables, 11 figures. Paper Website:
  https://modestyachts.github.io/natural-perturbations-website/
- **Journal**: None
- **Summary**: We study the robustness of image classifiers to temporal perturbations derived from videos. As part of this study, we construct two datasets, ImageNet-Vid-Robust and YTBB-Robust , containing a total 57,897 images grouped into 3,139 sets of perceptually similar images. Our datasets were derived from ImageNet-Vid and Youtube-BB respectively and thoroughly re-annotated by human experts for image similarity. We evaluate a diverse array of classifiers pre-trained on ImageNet and show a median classification accuracy drop of 16 and 10 on our two datasets. Additionally, we evaluate three detection models and show that natural perturbations induce both classification as well as localization errors, leading to a median drop in detection mAP of 14 points. Our analysis demonstrates that perturbations occurring naturally in videos pose a substantial and realistic challenge to deploying convolutional neural networks in environments that require both reliable and low-latency predictions



### Style Generator Inversion for Image Enhancement and Animation
- **Arxiv ID**: http://arxiv.org/abs/1906.11880v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11880v1)
- **Published**: 2019-06-05 17:58:28+00:00
- **Updated**: 2019-06-05 17:58:28+00:00
- **Authors**: Aviv Gabbay, Yedid Hoshen
- **Comment**: Project page: http://www.vision.huji.ac.il/style-image-prior
- **Journal**: None
- **Summary**: One of the main motivations for training high quality image generative models is their potential use as tools for image manipulation. Recently, generative adversarial networks (GANs) have been able to generate images of remarkable quality. Unfortunately, adversarially-trained unconditional generator networks have not been successful as image priors. One of the main requirements for a network to act as a generative image prior, is being able to generate every possible image from the target distribution. Adversarial learning often experiences mode-collapse, which manifests in generators that cannot generate some modes of the target distribution. Another requirement often not satisfied is invertibility i.e. having an efficient way of finding a valid input latent code given a required output image. In this work, we show that differently from earlier GANs, the very recently proposed style-generators are quite easy to invert. We use this important observation to propose style generators as general purpose image priors. We show that style generators outperform other GANs as well as Deep Image Prior as priors for image enhancement tasks. The latent space spanned by style-generators satisfies linear identity-pose relations. The latent space linearity, combined with invertibility, allows us to animate still facial images without supervision. Extensive experiments are performed to support the main contributions of this paper.



### Nail Polish Try-On: Realtime Semantic Segmentation of Small Objects for Native and Browser Smartphone AR Applications
- **Arxiv ID**: http://arxiv.org/abs/1906.02222v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02222v2)
- **Published**: 2019-06-05 18:04:58+00:00
- **Updated**: 2019-06-10 14:25:46+00:00
- **Authors**: Brendan Duke, Abdalla Ahmed, Edmund Phung, Irina Kezele, Parham Aarabi
- **Comment**: 4 pages, 3 figures. CVPRW 2019: Third Workshop on Computer Vision for
  AR/VR
- **Journal**: None
- **Summary**: We provide a system for semantic segmentation of small objects that enables nail polish try-on AR applications to run client-side in realtime in native and web mobile applications. By adjusting input resolution and neural network depth, our model design enables a smooth trade-off of performance and runtime, with the highest performance setting achieving~\num{94.5} mIoU at 29.8ms runtime in native applications on an iPad Pro. We also provide a postprocessing and rendering algorithm for nail polish try-on, which integrates with our semantic segmentation and fingernail base-tip direction predictions.



### Adaptation Across Extreme Variations using Unlabeled Domain Bridges
- **Arxiv ID**: http://arxiv.org/abs/1906.02238v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02238v2)
- **Published**: 2019-06-05 18:32:16+00:00
- **Updated**: 2020-08-24 23:38:03+00:00
- **Authors**: Shuyang Dai, Kihyuk Sohn, Yi-Hsuan Tsai, Lawrence Carin, Manmohan Chandraker
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle an unsupervised domain adaptation problem for which the domain discrepancy between labeled source and unlabeled target domains is large, due to many factors of inter and intra-domain variation. While deep domain adaptation methods have been realized by reducing the domain discrepancy, these are difficult to apply when domains are significantly unalike. In this work, we propose to decompose domain discrepancy into multiple but smaller, and thus easier to minimize, discrepancies by introducing unlabeled bridging domains that connect the source and target domains. We realize our proposal through an extension of the domain adversarial neural network with multiple discriminators, each of which accounts for reducing discrepancies between unlabeled (bridge, target) domains and a mix of all precedent domains including source. We validate the effectiveness of our method on several adaptation tasks including object recognition and semantic segmentation.



### Butterfly Transform: An Efficient FFT Based Neural Architecture Design
- **Arxiv ID**: http://arxiv.org/abs/1906.02256v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02256v2)
- **Published**: 2019-06-05 19:04:06+00:00
- **Updated**: 2020-04-16 23:28:09+00:00
- **Authors**: Keivan Alizadeh Vahid, Anish Prabhu, Ali Farhadi, Mohammad Rastegari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show that extending the butterfly operations from the FFT algorithm to a general Butterfly Transform (BFT) can be beneficial in building an efficient block structure for CNN designs. Pointwise convolutions, which we refer to as channel fusions, are the main computational bottleneck in the state-of-the-art efficient CNNs (e.g. MobileNets ). We introduce a set of criteria for channel fusion and prove that BFT yields an asymptotically optimal FLOP count with respect to these criteria. By replacing pointwise convolutions with BFT, we reduce the computational complexity of these layers from O(n^2) to O(n\log n) with respect to the number of channels. Our experimental evaluations show that our method results in significant accuracy gains across a wide range of network architectures, especially at low FLOP ranges. For example, BFT results in up to a 6.75% absolute Top-1 improvement for MobileNetV1, 4.4 \% for ShuffleNet V2 and 5.4% for MobileNetV3 on ImageNet under a similar number of FLOPS. Notably, ShuffleNet-V2+BFT outperforms state-of-the-art architecture search methods MNasNet, FBNet and MobilenetV3 in the low FLOP regime.



### Lightweight Real-time Makeup Try-on in Mobile Browsers with Tiny CNN Models for Facial Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.02260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02260v2)
- **Published**: 2019-06-05 19:16:17+00:00
- **Updated**: 2019-06-11 13:25:36+00:00
- **Authors**: TianXing Li, Zhi Yu, Edmund Phung, Brendan Duke, Irina Kezele, Parham Aarabi
- **Comment**: 4 pages, Third Workshop on Computer Vision for AR/VR
- **Journal**: None
- **Summary**: Recent works on convolutional neural networks (CNNs) for facial alignment have demonstrated unprecedented accuracy on a variety of large, publicly available datasets. However, the developed models are often both cumbersome and computationally expensive, and are not adapted to applications on resource restricted devices. In this work, we look into developing and training compact facial alignment models that feature fast inference speed and small deployment size, making them suitable for applications on the aforementioned category of devices. Our main contribution lies in designing such small models while maintaining high accuracy of facial alignment. The models we propose make use of light CNN architectures adapted to the facial alignment problem for accurate two-stage prediction of facial landmark coordinates from low-resolution output heatmaps. We further combine the developed facial tracker with a rendering method, and build a real-time makeup try-on demo that runs client-side in smartphone Web browsers. More results and demo are in our project page: http://research.modiface.com/makeup-try-on-cvprw2019/



### Learning Shape Representation on Sparse Point Clouds for Volumetric Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02281v1
- **DOI**: 10.1007/978-3-030-32245-8_31
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02281v1)
- **Published**: 2019-06-05 19:56:50+00:00
- **Updated**: 2019-06-05 19:56:50+00:00
- **Authors**: Fabian Balsiger, Yannick Soom, Olivier Scheidegger, Mauricio Reyes
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Volumetric image segmentation with convolutional neural networks (CNNs) encounters several challenges, which are specific to medical images. Among these challenges are large volumes of interest, high class imbalances, and difficulties in learning shape representations. To tackle these challenges, we propose to improve over traditional CNN-based volumetric image segmentation through point-wise classification of point clouds. The sparsity of point clouds allows processing of entire image volumes, balancing highly imbalanced segmentation problems, and explicitly learning an anatomical shape. We build upon PointCNN, a neural network proposed to process point clouds, and propose here to jointly encode shape and volumetric information within the point cloud in a compact and computationally effective manner. We demonstrate how this approach can then be used to refine CNN-based segmentation, which yields significantly improved results in our experiments on the difficult task of peripheral nerve segmentation from magnetic resonance neurography images. By synthetic experiments, we further show the capability of our approach in learning an explicit anatomical shape representation.



### Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels
- **Arxiv ID**: http://arxiv.org/abs/1906.02283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02283v1)
- **Published**: 2019-06-05 20:04:11+00:00
- **Updated**: 2019-06-05 20:04:11+00:00
- **Authors**: Martin Zlocha, Qi Dou, Ben Glocker
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Accurate, automated lesion detection in Computed Tomography (CT) is an important yet challenging task due to the large variation of lesion types, sizes, locations and appearances. Recent work on CT lesion detection employs two-stage region proposal based methods trained with centroid or bounding-box annotations. We propose a highly accurate and efficient one-stage lesion detector, by re-designing a RetinaNet to meet the particular challenges in medical imaging. Specifically, we optimize the anchor configurations using a differential evolution search algorithm. For training, we leverage the response evaluation criteria in solid tumors (RECIST) annotation which are measured in clinical routine. We incorporate dense masks from weak RECIST labels, obtained automatically using GrabCut, into the training objective, which in combination with other advancements yields new state-of-the-art performance. We evaluate our method on the public DeepLesion benchmark, consisting of 32,735 lesions across the body. Our one-stage detector achieves a sensitivity of 90.77% at 4 false positives per image, significantly outperforming the best reported methods by over 5%.



### Progressive-X: Efficient, Anytime, Multi-Model Fitting Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1906.02290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02290v1)
- **Published**: 2019-06-05 20:15:57+00:00
- **Updated**: 2019-06-05 20:15:57+00:00
- **Authors**: Daniel Barath, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: The Progressive-X algorithm, Prog-X in short, is proposed for geometric multi-model fitting. The method interleaves sampling and consolidation of the current data interpretation via repetitive hypothesis proposal, fast rejection, and integration of the new hypothesis into the kept instance set by labeling energy minimization. Due to exploring the data progressively, the method has several beneficial properties compared with the state-of-the-art. First, a clear criterion, adopted from RANSAC, controls the termination and stops the algorithm when the probability of finding a new model with a reasonable number of inliers falls below a threshold. Second, Prog-X is an any-time algorithm. Thus, whenever is interrupted, e.g. due to a time limit, the returned instances cover real and, likely, the most dominant ones. The method is superior to the state-of-the-art in terms of accuracy in both synthetic experiments and on publicly available real-world datasets for homography, two-view motion, and motion segmentation.



### Progressive NAPSAC: sampling from gradually growing neighborhoods
- **Arxiv ID**: http://arxiv.org/abs/1906.02295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02295v1)
- **Published**: 2019-06-05 20:28:18+00:00
- **Updated**: 2019-06-05 20:28:18+00:00
- **Authors**: Daniel Barath, Maksym Ivashechkin, Jiri Matas
- **Comment**: None
- **Journal**: None
- **Summary**: We propose Progressive NAPSAC, P-NAPSAC in short, which merges the advantages of local and global sampling by drawing samples from gradually growing neighborhoods. Exploiting the fact that nearby points are more likely to originate from the same geometric model, P-NAPSAC finds local structures earlier than global samplers. We show that the progressive spatial sampling in P-NAPSAC can be integrated with PROSAC sampling, which is applied to the first, location-defining, point. P-NAPSAC is embedded in USAC, a state-of-the-art robust estimation pipeline, which we further improve by implementing its local optimization as in Graph-Cut RANSAC. We call the resulting estimator USAC*. The method is tested on homography and fundamental matrix fitting on a total of 10,691 models from seven publicly available datasets. USAC* with P-NAPSAC outperforms reference methods in terms of speed on all problems.



### OutdoorSent: Sentiment Analysis of Urban Outdoor Images by Using Semantic and Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1906.02331v4
- **DOI**: 10.1145/3385186
- **Categories**: **cs.CV**, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1906.02331v4)
- **Published**: 2019-06-05 22:08:37+00:00
- **Updated**: 2020-02-27 22:25:02+00:00
- **Authors**: Wyverson B. de Oliveira, Leyza B. Dorini, Rodrigo Minetto, Thiago H. Silva
- **Comment**: Accepted on the ACM Transactions on Information Systems (TOIS)
- **Journal**: ACM Transactions on Information Systems (TOIS) 2020
- **Summary**: Opinion mining in outdoor images posted by users during different activities can provide valuable information to better understand urban areas. In this regard, we propose a framework to classify the sentiment of outdoor images shared by users on social networks. We compare the performance of state-of-the-art ConvNet architectures, and one specifically designed for sentiment analysis. We also evaluate how the merging of deep features and semantic information derived from the scene attributes can improve classification and cross-dataset generalization performance. The evaluation explores a novel dataset, namely OutdoorSent, and other datasets publicly available. We observe that the incorporation of knowledge about semantic attributes improves the accuracy of all ConvNet architectures studied. Besides, we found that exploring only images related to the context of the study, outdoor in our case, is recommended, i.e., indoor images were not significantly helpful. Furthermore, we demonstrated the applicability of our results in the city of Chicago, USA, showing that they can help to improve the knowledge of subjective characteristics of different areas of the city. For instance, particular areas of the city tend to concentrate more images of a specific class of sentiment, which are also correlated with median income, opening up opportunities in different fields.



### MNIST-C: A Robustness Benchmark for Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.02337v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02337v1)
- **Published**: 2019-06-05 22:23:43+00:00
- **Updated**: 2019-06-05 22:23:43+00:00
- **Authors**: Norman Mu, Justin Gilmer
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. Through several experiments and visualizations we demonstrate that our corruptions significantly degrade performance of state-of-the-art computer vision models while preserving the semantic content of the test images. In contrast to the popular notion of adversarial robustness, our model-agnostic corruptions do not seek worst-case performance but are instead designed to be broad and diverse, capturing multiple failure modes of modern models. In fact, we find that several previously published adversarial defenses significantly degrade robustness as measured by MNIST-C. We hope that our benchmark serves as a useful tool for future work in designing systems that are able to learn robust feature representations that capture the underlying semantics of the input.



### Anatomical Priors for Image Segmentation via Post-Processing with Denoising Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1906.02343v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02343v1)
- **Published**: 2019-06-05 22:41:11+00:00
- **Updated**: 2019-06-05 22:41:11+00:00
- **Authors**: Agostina J. Larrazabal, Cesar Martinez, Enzo Ferrante
- **Comment**: Accepted for publication in MICCAI 2019
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNN) proved to be highly accurate to perform anatomical segmentation of medical images. However, some of the most popular CNN architectures for image segmentation still rely on post-processing strategies (e.g. Conditional Random Fields) to incorporate connectivity constraints into the resulting masks. These post-processing steps are based on the assumption that objects are usually continuous and therefore nearby pixels should be assigned the same object label. Even if it is a valid assumption in general, these methods do not offer a straightforward way to incorporate more complex priors like convexity or arbitrary shape restrictions. In this work we propose Post-DAE, a post-processing method based on denoising autoencoders (DAE) trained using only segmentation masks. We learn a low-dimensional space of anatomically plausible segmentations, and use it as a post-processing step to impose shape constraints on the resulting masks obtained with arbitrary segmentation methods. Our approach is independent of image modality and intensity information since it employs only segmentation masks for training. This enables the use of anatomical segmentations that do not need to be paired with intensity images, making the approach very flexible. Our experimental results on anatomical segmentation of X-ray images show that Post-DAE can improve the quality of noisy and incorrect segmentation masks obtained with a variety of standard methods, by bringing them back to a feasible space, with almost no extra computational time.



### Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise
- **Arxiv ID**: http://arxiv.org/abs/1906.02355v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02355v1)
- **Published**: 2019-06-05 23:19:50+00:00
- **Updated**: 2019-06-05 23:19:50+00:00
- **Authors**: Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.



