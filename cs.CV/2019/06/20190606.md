# Arxiv Papers in cs.CV on 2019-06-06
### Context-Aware Visual Policy Network for Fine-Grained Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1906.02365v1
- **DOI**: 10.1109/TPAMI.2019.2909864
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02365v1)
- **Published**: 2019-06-06 00:37:33+00:00
- **Updated**: 2019-06-06 00:37:33+00:00
- **Authors**: Zheng-Jun Zha, Daqing Liu, Hanwang Zhang, Yongdong Zhang, Feng Wu
- **Comment**: Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI). Extended version of "Context-Aware Visual Policy
  Network for Sequence-Level Image Captioning", ACM MM 2018 (arXiv:1808.05864)
- **Journal**: None
- **Summary**: With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., "man riding horse") and visual comparisons (e.g., "small(er) cat"). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model -- CAVP and its subsequent language policy network -- can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.



### A database for face presentation attack using wax figure faces
- **Arxiv ID**: http://arxiv.org/abs/1906.11900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11900v1)
- **Published**: 2019-06-06 00:50:27+00:00
- **Updated**: 2019-06-06 00:50:27+00:00
- **Authors**: Shan Jia, Chuanbo Hu, Guodong Guo, Zhengquan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Compared to 2D face presentation attacks (e.g. printed photos and video replays), 3D type attacks are more challenging to face recognition systems (FRS) by presenting 3D characteristics or materials similar to real faces. Existing 3D face spoofing databases, however, mostly based on 3D masks, are restricted to small data size or poor authenticity due to the production difficulty and high cost. In this work, we introduce the first wax figure face database, WFFD, as one type of super-realistic 3D presentation attacks to spoof the FRS. This database consists of 2200 images with both real and wax figure faces (totally 4400 faces) with a high diversity from online collections. Experiments on this database first investigate the vulnerability of three popular FRS to this kind of new attack. Further, we evaluate the performance of several face presentation attack detection methods to show the attack abilities of this super-realistic face spoofing database.



### Omnidirectional Scene Text Detection with Sequential-free Box Discretization
- **Arxiv ID**: http://arxiv.org/abs/1906.02371v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02371v3)
- **Published**: 2019-06-06 01:13:02+00:00
- **Updated**: 2019-07-28 12:30:15+00:00
- **Authors**: Yuliang Liu, Sheng Zhang, Lianwen Jin, Lele Xie, Yaqiang Wu, Zhepeng Wang
- **Comment**: Accepted by IJCAI2019
- **Journal**: None
- **Summary**: Scene text in the wild is commonly presented with high variant characteristics. Using quadrilateral bounding box to localize the text instance is nearly indispensable for detection methods. However, recent researches reveal that introducing quadrilateral bounding box for scene text detection will bring a label confusion issue which is easily overlooked, and this issue may significantly undermine the detection performance. To address this issue, in this paper, we propose a novel method called Sequential-free Box Discretization (SBD) by discretizing the bounding box into key edges (KE) which can further derive more effective methods to improve detection performance. Experiments showed that the proposed method can outperform state-of-the-art methods in many popular scene text benchmarks, including ICDAR 2015, MLT, and MSRA-TD500. Ablation study also showed that simply integrating the SBD into Mask R-CNN framework, the detection performance can be substantially improved. Furthermore, an experiment on the general object dataset HRSC2016 (multi-oriented ships) showed that our method can outperform recent state-of-the-art methods by a large margin, demonstrating its powerful generalization ability. Source code: https://github.com/Yuliang-Liu/Box_Discretization_Network.



### Blockwise Based Detection of Local Defects
- **Arxiv ID**: http://arxiv.org/abs/1906.02374v1
- **DOI**: 10.2352/ISSN.2470-1173.2016.13.IQSP-207
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02374v1)
- **Published**: 2019-06-06 01:27:32+00:00
- **Updated**: 2019-06-06 01:27:32+00:00
- **Authors**: Xiaoyu Xiang, Renee Jessome, Eric Maggard, Yousun Bang, Minki Cho, Jan Allebach
- **Comment**: 7 pages, 13 figures, IS&T Electronic Imaging 2019 Proceedings
- **Journal**: Electronic Imaging Symposium Proceedings, Image Quality and System
  Performance XVI. Society for Imaging Science and Technology, 2019
- **Summary**: Print quality is an important criterion for a printer's performance. The detection, classification, and assessment of printing defects can reflect the printer's working status and help to locate mechanical problems inside. To handle all these questions, an efficient algorithm is needed to replace the traditionally visual checking method. In this paper, we focus on pages with local defects including gray spots and solid spots. We propose a coarse-to-fine method to detect local defects in a block-wise manner, and aggregate the blockwise attributes to generate the feature vector of the whole test page for a further ranking task. In the detection part, we first select candidate regions by thresholding a single feature. Then more detailed features of candidate blocks are calculated and sent to a decision tree that is previously trained on our training dataset. The final result is given by the decision tree model to control the false alarm rate while maintaining the required miss rate. Our algorithm is proved to be effective in detecting and classifying local defects compared with previous methods.



### Generative Model-Based Ischemic Stroke Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02392v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02392v1)
- **Published**: 2019-06-06 03:25:11+00:00
- **Updated**: 2019-06-06 03:25:11+00:00
- **Authors**: Tao Song
- **Comment**: None
- **Journal**: None
- **Summary**: CT perfusion (CTP) has been used to triage ischemic stroke patients in the early stage, because of its speed, availability, and lack of contraindications. Perfusion parameters including cerebral blood volume (CBV), cerebral blood flow (CBF), mean transit time (MTT) and time of peak (Tmax) could also be computed from CTP data. However, CTP data or the perfusion parameters, are ambiguous to locate the infarct core or tissue at risk (penumbra), which is normally confirmed by the follow-up Diffusion Weighted Imaging (DWI) or perfusion diffusion mismatch. In this paper, we propose a novel generative modelbased segmentation framework composed of an extractor, a generator and a segmentor for ischemic stroke lesion segmentation. First, an extractor is used to directly extract the representative feature images from the CTP feature images. Second, a generator is used to generate the clinical relevant DWI images using the output from the extractor and perfusion parameters. Finally, the segmentor is used to precisely segment the ischemic stroke lesion using the generated DWI from the generator. Meanwhile, a novel pixel-region loss function, generalized dice combined with weighted cross entropy, is used to handle data unbalance problem which is commonly encountered in medical image segmentation. All networks are trained end-to-end from scratch using the 2018 Ischemic Stroke Lesion Segmentation Challenge (ISLES) dataset and our method won the first place in the 2018 ischemic stroke lesions segmentation challenge in the test stage.



### Query-efficient Meta Attack to Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02398v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02398v3)
- **Published**: 2019-06-06 03:49:00+00:00
- **Updated**: 2020-02-15 03:51:45+00:00
- **Authors**: Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Black-box attack methods aim to infer suitable attack patterns to targeted DNN models by only using output feedback of the models and the corresponding input queries. However, due to lack of prior and inefficiency in leveraging the query and feedback information, existing methods are mostly query-intensive for obtaining effective attack patterns. In this work, we propose a meta attack approach that is capable of attacking a targeted model with much fewer queries. Its high queryefficiency stems from effective utilization of meta learning approaches in learning generalizable prior abstraction from the previously observed attack patterns and exploiting such prior to help infer attack patterns from only a few queries and outputs. Extensive experiments on MNIST, CIFAR10 and tiny-Imagenet demonstrate that our meta-attack method can remarkably reduce the number of model queries without sacrificing the attack performance. Besides, the obtained meta attacker is not restricted to a particular model but can be used easily with a fast adaptive ability to attack a variety of models.The code of our work is available at https://github.com/dydjw9/MetaAttack_ICLR2020/.



### Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02415v1)
- **Published**: 2019-06-06 05:01:04+00:00
- **Updated**: 2019-06-06 05:01:04+00:00
- **Authors**: Vinicius Ribeiro, Sandra Avila, Eduardo Valle
- **Comment**: 10 pages, 5 images
- **Journal**: None
- **Summary**: In this work, we explore the issue of the inter-annotator agreement for training and evaluating automated segmentation of skin lesions. We explore what different degrees of agreement represent, and how they affect different use cases for segmentation. We also evaluate how conditioning the ground truths using different (but very simple) algorithms may help to enhance agreement and may be appropriate for some use cases. The segmentation of skin lesions is a cornerstone task for automated skin lesion analysis, useful both as an end-result to locate/detect the lesions and as an ancillary task for lesion classification. Lesion segmentation, however, is a very challenging task, due not only to the challenge of image segmentation itself but also to the difficulty in obtaining properly annotated data. Detecting accurately the borders of lesions is challenging even for trained humans, since, for many lesions, those borders are fuzzy and ill-defined. Using lesions and annotations from the ISIC Archive, we estimate inter-annotator agreement for skin-lesion segmentation and propose several simple procedures that may help to improve inter-annotator agreement if used to condition the ground truths.



### Extreme Points Derived Confidence Map as a Cue For Class-Agnostic Segmentation Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.02421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02421v1)
- **Published**: 2019-06-06 05:19:22+00:00
- **Updated**: 2019-06-06 05:19:22+00:00
- **Authors**: Shadab Khan, Ahmed H. Shahin, Javier Villafruela, Jianbing Shen, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: To automate the process of segmenting an anatomy of interest, we can learn a model from previously annotated data. The learning-based approach uses annotations to train a model that tries to emulate the expert labeling on a new data set. While tremendous progress has been made using such approaches, labeling of medical images remains a time-consuming and expensive task. In this paper, we evaluate the utility of extreme points in learning to segment. Specifically, we propose a novel approach to compute a confidence map from extreme points that quantitatively encodes the priors derived from extreme points. We use the confidence map as a cue to train a deep neural network based on ResNet-101 and PSP module to develop a class-agnostic segmentation model that outperforms state-of-the-art method that employs extreme points as a cue. Further, we evaluate a realistic use-case by using our model to generate training data for supervised learning (U-Net) and observed that U-Net performs comparably when trained with either the generated data or the ground truth data. These findings suggest that models trained using cues can be used to generate reliable training data.



### Uncertainty-guided Continual Learning with Bayesian Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02425v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02425v2)
- **Published**: 2019-06-06 05:40:25+00:00
- **Updated**: 2020-02-20 01:08:22+00:00
- **Authors**: Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, Marcus Rohrbach
- **Comment**: Accepted at ICLR 2020
- **Journal**: None
- **Summary**: Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' \textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify \textit{what to remember} and \textit{what to change} as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.



### Salient Building Outline Enhancement and Extraction Using Iterative L0 Smoothing and Line Enhancing
- **Arxiv ID**: http://arxiv.org/abs/1906.02426v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.02426v1)
- **Published**: 2019-06-06 05:44:12+00:00
- **Updated**: 2019-06-06 05:44:12+00:00
- **Authors**: Cho-Ying Wu, Ulrich Neumann
- **Comment**: Accepted to ICIP 2019
- **Journal**: None
- **Summary**: In this paper, our goal is salient building outline enhancement and extraction from images taken from consumer cameras using L0 smoothing. We address weak outlines and over-smoothing problem. Weak outlines are often undetected by edge extractors or easily smoothed out. We propose an iterative method, including the smoothing cell and sharpening cell. In the smoothing cell, we iteratively enlarge the smoothing level of the L0 smoothing. In the sharpening cell, we use Hough Transform to extract lines, based on the assumption that salient outlines for buildings are usually straight, and enhance those extracted lines. Our goal is to enhance line structures and do the L0 smoothing simultaneously. Also, we propose to create building masks from semantic segmentation using an encoder-decoder network. The masks filter out irrelevant edges. We also provide an evaluation dataset on this task.



### Occluded Face Recognition Using Low-rank Regression with Generalized Gradient Direction
- **Arxiv ID**: http://arxiv.org/abs/1906.02429v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1906.02429v1)
- **Published**: 2019-06-06 05:58:36+00:00
- **Updated**: 2019-06-06 05:58:36+00:00
- **Authors**: Cho-Ying Wu, Jian-Jiun Ding
- **Comment**: None
- **Journal**: Pattern Recognition (PR), Elsevier, vol. 80, pp. 256-268, 2018
- **Summary**: In this paper, a very effective method to solve the contiguous face occlusion recognition problem is proposed. It utilizes the robust image gradient direction features together with a variety of mapping functions and adopts a hierarchical sparse and low-rank regression model. This model unites the sparse representation in dictionary learning and the low-rank representation on the error term that is usually messy in the gradient domain. We call it the "weak low-rankness" optimization problem, which can be efficiently solved by the framework of Alternating Direction Method of Multipliers (ADMM). The optimum of the error term has a similar weak low-rank structure as the reference error map and the recognition performance can be enhanced by leaps and bounds using weak low-rankness optimization. Extensive experiments are conducted on real-world disguise / occlusion data and synthesized contiguous occlusion data. These experiments show that the proposed gradient direction-based hierarchical adaptive sparse and low-rank (GD-HASLR) algorithm has the best performance compared to state-of-the-art methods, including popular convolutional neural network-based methods.



### ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1906.02467v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02467v1)
- **Published**: 2019-06-06 08:08:14+00:00
- **Updated**: 2019-06-06 08:08:14+00:00
- **Authors**: Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, Dacheng Tao
- **Comment**: Accepted at AAAI 2019
- **Journal**: None
- **Summary**: Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos. The dataset is available at https://github.com/MILVLG/activitynet-qa



### StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02470v1)
- **Published**: 2019-06-06 08:21:04+00:00
- **Updated**: 2019-06-06 08:21:04+00:00
- **Authors**: Jie An, Haoyi Xiong, Jinwen Ma, Jiebo Luo, Jun Huan
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) has been widely studied for designing discriminative deep learning models such as image classification, object detection, and semantic segmentation. As a large number of priors have been obtained through the manual design of architectures in the fields, NAS is usually considered as a supplement approach. In this paper, we have significantly expanded the application areas of NAS by performing an empirical study of NAS to search generative models, or specifically, auto-encoder based universal style transfer, which lacks systematic exploration, if any, from the architecture search aspect. In our work, we first designed a search space where common operators for image style transfer such as VGG-based encoders, whitening and coloring transforms (WCT), convolution kernels, instance normalization operators, and skip connections were searched in a combinatorial approach. With a simple yet effective parallel evolutionary NAS algorithm with multiple objectives, we derived the first group of end-to-end deep networks for universal photorealistic style transfer. Comparing to random search, a NAS method that is gaining popularity recently, we demonstrated that carefully designed search strategy leads to much better architecture design. Finally compared to existing universal style transfer networks for photorealistic rendering such as PhotoWCT that stacks multiple well-trained auto-encoders and WCT transforms in a non-end-to-end manner, the architectures designed by StyleNAS produce better style-transferred images with details preserving, using a tiny number of operators/parameters, and enjoying around 500x inference time speed-up.



### Image Synthesis with a Single (Robust) Classifier
- **Arxiv ID**: http://arxiv.org/abs/1906.09453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09453v2)
- **Published**: 2019-06-06 09:12:08+00:00
- **Updated**: 2019-08-08 15:47:42+00:00
- **Authors**: Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.



### Anytime Lane-Level Intersection Estimation Based on Trajectories of Other Traffic Participants
- **Arxiv ID**: http://arxiv.org/abs/1906.02495v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.02495v2)
- **Published**: 2019-06-06 09:39:41+00:00
- **Updated**: 2019-08-07 14:15:55+00:00
- **Authors**: Annika Meyer, Jonas Walter, Martin Lauer, Christoph Stiller
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating and understanding the current scene is an inevitable capability of automated vehicles. Usually, maps are used as prior for interpreting sensor measurements in order to drive safely and comfortably. Only few approaches take into account that maps might be outdated and lead to wrong assumptions on the environment. This work estimates a lane-level intersection topology without any map prior by observing the trajectories of other traffic participants.   We are able to deliver both a coarse lane-level topology as well as the lane course inside and outside of the intersection using Markov chain Monte Carlo sampling. The model is neither limited to a number of lanes or arms nor to the topology of the intersection.   We present our results on an evaluation set of 1000 simulated intersections and achieve 99.9% accuracy on the topology estimation that takes only 36ms, when utilizing tracked object detections. The precise lane course on these intersections is estimated with an error of 15cm on average after 140ms. Our approach shows a similar level of precision on 14 real-world intersections with 18cm average deviation on simple intersections and 27cm for more complex scenarios. Here the estimation takes only 113ms in total.



### Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.02497v2
- **DOI**: 10.1145/3331184.3331235
- **Categories**: **cs.IR**, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1906.02497v2)
- **Published**: 2019-06-06 09:45:58+00:00
- **Updated**: 2019-07-27 10:18:43+00:00
- **Authors**: Zhu Zhang, Zhijie Lin, Zhou Zhao, Zhenxin Xiao
- **Comment**: Accepted by SIGIR 2019 as a full paper
- **Journal**: SIGIR, 2019, pages 655-664
- **Summary**: Query-based moment retrieval aims to localize the most relevant moment in an untrimmed video according to the given natural language query. Existing works often only focus on one aspect of this emerging task, such as the query representation learning, video context modeling or multi-modal fusion, thus fail to develop a comprehensive system for further performance improvement. In this paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to consider multiple crucial factors for this challenging task, including (1) the syntactic structure of natural language queries; (2) long-range semantic dependencies in video context and (3) the sufficient cross-modal interaction. Specifically, we devise a syntactic GCN to leverage the syntactic structure of queries for fine-grained representation learning, propose a multi-head self-attention to capture long-range semantic dependencies from video context, and next employ a multi-stage cross-modal interaction to explore the potential relations of video and query contents. The extensive experiments demonstrate the effectiveness of our proposed method.



### Removing Rain in Videos: A Large-scale Database and A Two-stream ConvLSTM Approach
- **Arxiv ID**: http://arxiv.org/abs/1906.02526v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02526v1)
- **Published**: 2019-06-06 11:31:44+00:00
- **Updated**: 2019-06-06 11:31:44+00:00
- **Authors**: Tie Liu, Mai Xu, Zulin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Rain removal has recently attracted increasing research attention, as it is able to enhance the visibility of rain videos. However, the existing learning based rain removal approaches for videos suffer from insufficient training data, especially when applying deep learning to remove rain. In this paper, we establish a large-scale video database for rain removal (LasVR), which consists of 316 rain videos. Then, we observe from our database that there exist the temporal correlation of clean content and similar patterns of rain across video frames. According to these two observations, we propose a two-stream convolutional long- and short- term memory (ConvLSTM) approach for rain removal in videos. The first stream is composed of the subnet for rain detection, while the second stream is the subnet of rain removal that leverages the features from the rain detection subnet. Finally, the experimental results on both synthetic and real rain videos show the proposed approach performs better than other state-of-the-art approaches.



### Contextual Relabelling of Detected Objects
- **Arxiv ID**: http://arxiv.org/abs/1906.02534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.02534v1)
- **Published**: 2019-06-06 11:48:36+00:00
- **Updated**: 2019-06-06 11:48:36+00:00
- **Authors**: Faisal Alamri, Nicolas Pugeault
- **Comment**: Presented at the IEEE ICDL-Epirob'2019 conference, Oslo, Norway
- **Journal**: None
- **Summary**: Contextual information, such as the co-occurrence of objects and the spatial and relative size among objects provides deep and complex information about scenes. It also can play an important role in improving object detection. In this work, we present two contextual models (rescoring and re-labeling models) that leverage contextual information (16 contextual relationships are applied in this paper) to enhance the state-of-the-art RCNN-based object detection (Faster RCNN). We experimentally demonstrate that our models lead to enhancement in detection performance using the most common dataset used in this field (MSCOCO).



### STN-Homography: estimate homography parameters directly
- **Arxiv ID**: http://arxiv.org/abs/1906.02539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02539v1)
- **Published**: 2019-06-06 12:07:22+00:00
- **Updated**: 2019-06-06 12:07:22+00:00
- **Authors**: Qiang Zhou, Xin Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce the STN-Homography model to directly estimate the homography matrix between image pair. Different most CNN-based homography estimation methods which use an alternative 4-point homography parameterization, we use prove that, after coordinate normalization, the variance of elements of coordinate normalized $3\times3$ homography matrix is very small and suitable to be regressed well with CNN. Based on proposed STN-Homography, we use a hierarchical architecture which stacks several STN-Homography models and successively reduce the estimation error. Effectiveness of the proposed method is shown through experiments on MSCOCO dataset, in which it significantly outperforms the state-of-the-art. The average processing time of our hierarchical STN-Homography with 1 stage is only 4.87 ms on the GPU, and the processing time for hierarchical STN-Homography with 3 stages is 17.85 ms. The code will soon be open sourced.



### Hierarchical Bayesian myocardial perfusion quantification
- **Arxiv ID**: http://arxiv.org/abs/1906.02540v2
- **DOI**: 10.1016/j.media.2019.101611
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1906.02540v2)
- **Published**: 2019-06-06 12:07:52+00:00
- **Updated**: 2019-11-21 21:39:57+00:00
- **Authors**: Cian M. Scannell, Amedeo Chiribiri, Adriana D. M. Villa, Marcel Breeuwer, Jack Lee
- **Comment**: Published in Medical Image Analysis
- **Journal**: None
- **Summary**: Purpose: Tracer-kinetic models can be used for the quantitative assessment of contrast-enhanced MRI data. However, the model-fitting can produce unreliable results due to the limited data acquired and the high noise levels. Such problems are especially prevalent in myocardial perfusion MRI leading to the compromise of constrained numerical deconvolutions and segmental signal averaging being commonly used as alternatives to the more complex tracer-kinetic models. Methods: In this work, the use of hierarchical Bayesian inference for the parameter estimation is explored. It is shown that with Bayesian inference it is possible to reliably fit the two-compartment exchange model to perfusion data. The use of prior knowledge on the ranges of kinetic parameters and the fact that neighbouring voxels are likely to have similar kinetic properties combined with a Markov chain Monte Carlo based fitting procedure significantly improves the reliability of the perfusion estimates with compared to the traditional least-squares approach. The method is assessed using both simulated and patient data. Results: The average (standard deviation) normalised mean square error for the distinct noise realisations of a simulation phantom falls from 0.32 (0.55) with the least-squares fitting to 0.13 (0.2) using Bayesian inference. The assessment of the presence of coronary artery disease based purely on the quantitative MBF maps obtained using Bayesian inference matches the visual assessment in all 24 slices. When using the maps obtained by the least-squares fitting, a corresponding assessment is only achieved in 16/24 slices. Conclusion: Bayesian inference allows a reliable, fully automated and user-independent assessment of myocardial perfusion on a voxel-wise level using the two-compartment exchange model.



### Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video
- **Arxiv ID**: http://arxiv.org/abs/1906.02549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02549v1)
- **Published**: 2019-06-06 12:32:55+00:00
- **Updated**: 2019-06-06 12:32:55+00:00
- **Authors**: Zhenfang Chen, Lin Ma, Wenhan Luo, Kwan-Yee K. Wong
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train the proposed attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. Moreover, we also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Extensive experimental results demonstrate the superiority of our model over the baseline approaches.



### On the Effectiveness of Laser Speckle Contrast Imaging and Deep Neural Networks for Detecting Known and Unknown Fingerprint Presentation Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.02595v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02595v1)
- **Published**: 2019-06-06 13:58:44+00:00
- **Updated**: 2019-06-06 13:58:44+00:00
- **Authors**: Hengameh Mirzaalian, Mohamed Hussein, Wael Abd-Almageed
- **Comment**: International Conference on Biometrics (ICB 2019)
- **Journal**: None
- **Summary**: Fingerprint presentation attack detection (FPAD) is becoming an increasingly challenging problem due to the continuous advancement of attack techniques, which generate `realistic-looking' fake fingerprint presentations. Recently, laser speckle contrast imaging (LSCI) has been introduced as a new sensing modality for FPAD. LSCI has the interesting characteristic of capturing the blood flow under the skin surface. Toward studying the importance and effectiveness of LSCI for FPAD, we conduct a comprehensive study using different patch-based deep neural network architectures. Our studied architectures include 2D and 3D convolutional networks as well as a recurrent network using long short-term memory (LSTM) units. The study demonstrates that strong FPAD performance can be achieved using LSCI. We evaluate the different models over a new large dataset. The dataset consists of 3743 bona fide samples, collected from 335 unique subjects, and 218 presentation attack samples, including six different types of attacks. To examine the effect of changing the training and testing sets, we conduct a 3-fold cross validation evaluation. To examine the effect of the presence of an unseen attack, we apply a leave-one-attack out strategy. The FPAD classification results of the networks, which are separately optimized and tuned for the temporal and spatial patch-sizes, indicate that the best performance is achieved by LSTM.



### Scaling Autoregressive Video Models
- **Arxiv ID**: http://arxiv.org/abs/1906.02634v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02634v3)
- **Published**: 2019-06-06 15:06:21+00:00
- **Updated**: 2020-02-10 19:29:56+00:00
- **Authors**: Dirk Weissenborn, Oscar Täckström, Jakob Uszkoreit
- **Comment**: International Conference on Learning Representations (ICLR) 2020
- **Journal**: None
- **Summary**: Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models often attempt to address these issues by combining sometimes complex, usually video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple autoregressive video generation models based on a three-dimensional self-attention mechanism achieve competitive results across multiple metrics on popular benchmark datasets, for which they produce continuations of high fidelity and realism. We also present results from training our models on Kinetics, a large scale action recognition dataset comprised of YouTube videos exhibiting phenomena such as camera movement, complex object interactions and diverse human movement. While modeling these phenomena consistently remains elusive, we hope that our results, which include occasional realistic continuations encourage further research on comparatively complex, large scale datasets such as Kinetics.



### Does Object Recognition Work for Everyone?
- **Arxiv ID**: http://arxiv.org/abs/1906.02659v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02659v2)
- **Published**: 2019-06-06 16:00:18+00:00
- **Updated**: 2019-06-18 17:37:44+00:00
- **Authors**: Terrance DeVries, Ishan Misra, Changhan Wang, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: The paper analyzes the accuracy of publicly available object-recognition systems on a geographically diverse dataset. This dataset contains household items and was designed to have a more representative geographical coverage than commonly used image datasets in object recognition. We find that the systems perform relatively poorly on household items that commonly occur in countries with a low household income. Qualitative analyses suggest the drop in performance is primarily due to appearance differences within an object class (e.g., dish soap) and due to items appearing in a different context (e.g., toothbrushes appearing outside of bathrooms). The results of our study suggest that further work is needed to make object-recognition systems work equally well for people across different countries and income levels.



### Feature-level and Model-level Audiovisual Fusion for Emotion Recognition in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1906.02728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02728v1)
- **Published**: 2019-06-06 17:49:41+00:00
- **Updated**: 2019-06-06 17:49:41+00:00
- **Authors**: Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, Shizhong Han, Ping Liu, Min Chen, Yan Tong
- **Comment**: None
- **Journal**: None
- **Summary**: Emotion recognition plays an important role in human-computer interaction (HCI) and has been extensively studied for decades. Although tremendous improvements have been achieved for posed expressions, recognizing human emotions in "close-to-real-world" environments remains a challenge. In this paper, we proposed two strategies to fuse information extracted from different modalities, i.e., audio and visual. Specifically, we utilized LBP-TOP, an ensemble of CNNs, and a bi-directional LSTM (BLSTM) to extract features from the visual channel, and the OpenSmile toolkit to extract features from the audio channel. Two kinds of fusion methods, i,e., feature-level fusion and model-level fusion, were developed to utilize the information extracted from the two channels. Experimental results on the EmotiW2018 AFEW dataset have shown that the proposed fusion methods outperform the baseline methods significantly and achieve better or at least comparable performance compared with the state-of-the-art methods, where the model-level fusion performs better when one of the channels totally fails.



### 3D-RelNet: Joint Object and Relational Network for 3D Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.02729v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02729v3)
- **Published**: 2019-06-06 17:50:48+00:00
- **Updated**: 2020-03-05 04:10:25+00:00
- **Authors**: Nilesh Kulkarni, Ishan Misra, Shubham Tulsiani, Abhinav Gupta
- **Comment**: Project page: https://nileshkulkarni.github.io/relative3d/
- **Journal**: None
- **Summary**: We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets (SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods.



### Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02611v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02611v1)
- **Published**: 2019-06-06 17:54:24+00:00
- **Updated**: 2019-06-06 17:54:24+00:00
- **Authors**: Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D. Cubuk
- **Comment**: None
- **Journal**: None
- **Summary**: Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise(similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.



### Mesh R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1906.02739v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02739v2)
- **Published**: 2019-06-06 17:56:09+00:00
- **Updated**: 2020-01-25 21:56:36+00:00
- **Authors**: Georgia Gkioxari, Jitendra Malik, Justin Johnson
- **Comment**: None
- **Journal**: None
- **Summary**: Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.



### Object Pose Estimation in Robotics Revisited
- **Arxiv ID**: http://arxiv.org/abs/1906.02783v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02783v3)
- **Published**: 2019-06-06 19:27:48+00:00
- **Updated**: 2020-05-21 17:09:25+00:00
- **Authors**: Antti Hietanen, Jyrki Latokartano, Alessandro Foi, Roel Pieters, Ville Kyrki, Minna Lanz, Joni-Kristian Kämäräinen
- **Comment**: 29 pages, 8 figures
- **Journal**: None
- **Summary**: Vision based object grasping and manipulation in robotics require accurate estimation of object's 6D pose. The 6D pose estimation has received significant attention in computer vision community and multiple datasets and evaluation metrics have been proposed. However, the existing metrics measure how well two geometrical surfaces are aligned - ground truth vs. estimated pose - which does not directly measure how well a robot can perform the task with the given estimate. In this work we propose a probabilistic metric that directly measures success in robotic tasks. The evaluation metric is based on non-parametric probability density that is estimated from samples of a real physical setup. During the pose evaluation stage the physical setup is not needed. The evaluation metric is validated in controlled experiments and a new pose estimation dataset of industrial parts is introduced. The experimental results with the parts confirm that the proposed evaluation metric better reflects the true performance in robotics than the existing metrics.



### Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers
- **Arxiv ID**: http://arxiv.org/abs/1906.02792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02792v1)
- **Published**: 2019-06-06 19:59:56+00:00
- **Updated**: 2019-06-06 19:59:56+00:00
- **Authors**: Manjot Bilkhu, Siyang Wang, Tushar Dobhal
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: Video Captioning and Summarization have become very popular in the recent years due to advancements in Sequence Modelling, with the resurgence of Long-Short Term Memory networks (LSTMs) and introduction of Gated Recurrent Units (GRUs). Existing architectures extract spatio-temporal features using CNNs and utilize either GRUs or LSTMs to model dependencies with soft attention layers. These attention layers do help in attending to the most prominent features and improve upon the recurrent units, however, these models suffer from the inherent drawbacks of the recurrent units themselves. The introduction of the Transformer model has driven the Sequence Modelling field into a new direction. In this project, we implement a Transformer-based model for Video captioning, utilizing 3D CNN architectures like C3D and Two-stream I3D for video extraction. We also apply certain dimensionality reduction techniques so as to keep the overall size of the model within limits. We finally present our results on the MSVD and ActivityNet datasets for Single and Dense video captioning tasks respectively.



### Smart IoT Cameras for Crowd Analysis based on augmentation for automatic pedestrian detection, simulation and annotation
- **Arxiv ID**: http://arxiv.org/abs/1906.03994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03994v1)
- **Published**: 2019-06-06 20:43:21+00:00
- **Updated**: 2019-06-06 20:43:21+00:00
- **Authors**: Antoine Rimboux, Rob Dupre, Thomas Lagkas, Panagiotis Sarigiannidis, Paolo Remagnino, Vasileios Argyriou
- **Comment**: IoTI4 Workshop 2019. arXiv admin note: text overlap with
  arXiv:1707.02655
- **Journal**: None
- **Summary**: Smart video sensors for applications related to surveillance and security are IOT-based as they use Internet for various purposes. Such applications include crowd behaviour monitoring and advanced decision support systems operating and transmitting information over internet. The analysis of crowd and pedestrian behaviour is an important task for smart IoT cameras and in particular video processing. In order to provide related behavioural models, simulation and tracking approaches have been considered in the literature. In both cases ground truth is essential to train deep models and provide a meaningful quantitative evaluation. We propose a framework for crowd simulation and automatic data generation and annotation that supports multiple cameras and multiple targets. The proposed approach is based on synthetically generated human agents, augmented frames and compositing techniques combined with path finding and planning methods. A number of popular crowd and pedestrian data sets were used to validate the model, and scenarios related to annotation and simulation were considered.



### Scene and Environment Monitoring Using Aerial Imagery and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.02809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02809v1)
- **Published**: 2019-06-06 20:58:39+00:00
- **Updated**: 2019-06-06 20:58:39+00:00
- **Authors**: Mahdi Maktabdar Oghaz, Manzoor Razaak, Hamideh Kerdegari, Vasileios Argyriou, Paolo Remagnino
- **Comment**: 8
- **Journal**: IoTI4 Workshop 2019
- **Summary**: Unmanned Aerial vehicles (UAV) are a promising technology for smart farming related applications. Aerial monitoring of agriculture farms with UAV enables key decision-making pertaining to crop monitoring. Advancements in deep learning techniques have further enhanced the precision and reliability of aerial imagery based analysis. The capabilities to mount various kinds of sensors (RGB, spectral cameras) on UAV allows remote crop analysis applications such as vegetation classification and segmentation, crop counting, yield monitoring and prediction, crop mapping, weed detection, disease and nutrient deficiency detection and others. A significant amount of studies are found in the literature that explores UAV for smart farming applications. In this paper, a review of studies applying deep learning on UAV imagery for smart farming is presented. Based on the application, we have classified these studies into five major groups including: vegetation identification, classification and segmentation, crop counting and yield predictions, crop mapping, weed detection and crop disease and nutrient deficiency detection. An in depth critical analysis of each study is provided.



### V-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02817v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02817v2)
- **Published**: 2019-06-06 21:07:40+00:00
- **Updated**: 2019-08-04 22:04:33+00:00
- **Authors**: Zhuotun Zhu, Chenxi Liu, Dong Yang, Alan Yuille, Daguang Xu
- **Comment**: Accepted by 3DV, camera ready version. 8 pages, 4 figures and 5
  tables
- **Journal**: None
- **Summary**: Deep learning algorithms, in particular 2D and 3D fully convolutional neural networks (FCNs), have rapidly become the mainstream methodology for volumetric medical image segmentation. However, 2D convolutions cannot fully leverage the rich spatial information along the third axis, while 3D convolutions suffer from the demanding computation and high GPU memory consumption. In this paper, we propose to automatically search the network architecture tailoring to volumetric medical image segmentation problem. Concretely, we formulate the structure learning as differentiable neural architecture search, and let the network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon (MSD) Challenge. Our method, named V-NAS, consistently outperforms other state-of-the-arts on the segmentation task of both normal organ (NIH Pancreas) and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the power of chosen architecture. Moreover, the searched architecture on one dataset can be well generalized to other datasets, which demonstrates the robustness and practical use of our proposed method.



### Iterative Self-Learning: Semi-Supervised Improvement to Dataset Volumes and Model Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1906.02823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02823v1)
- **Published**: 2019-06-06 21:20:12+00:00
- **Updated**: 2019-06-06 21:20:12+00:00
- **Authors**: Robert Dupre, Jiri Fajtl, Vasileios Argyriou, Paolo Remagnin
- **Comment**: None
- **Journal**: CVPR'2019 workshop - Uncertainty and Robustness in Deep Visual
  Learning
- **Summary**: A novel semi-supervised learning technique is introduced based on a simple iterative learning cycle together with learned thresholding techniques and an ensemble decision support system. State-of-the-art model performance and increased training data volume are demonstrated, through the use of unlabelled data when training deeply learned classification models. Evaluation of the proposed approach is performed on commonly used datasets when evaluating semi-supervised learning techniques as well as a number of more challenging image classification datasets (CIFAR-100 and a 200 class subset of ImageNet).



### XRAI: Better Attributions Through Regions
- **Arxiv ID**: http://arxiv.org/abs/1906.02825v2
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02825v2)
- **Published**: 2019-06-06 21:21:39+00:00
- **Updated**: 2019-08-20 21:09:28+00:00
- **Authors**: Andrei Kapishnikov, Tolga Bolukbasi, Fernanda Viégas, Michael Terry
- **Comment**: None
- **Journal**: None
- **Summary**: Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.



### Learning Temporal Pose Estimation from Sparsely-Labeled Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.04016v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04016v3)
- **Published**: 2019-06-06 21:24:52+00:00
- **Updated**: 2019-12-11 07:39:26+00:00
- **Authors**: Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, Lorenzo Torresani
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.



### Data Extraction from Charts via Single Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.11906v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11906v1)
- **Published**: 2019-06-06 21:54:50+00:00
- **Updated**: 2019-06-06 21:54:50+00:00
- **Authors**: Xiaoyi Liu, Diego Klabjan, Patrick NBless
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic data extraction from charts is challenging for two reasons: there exist many relations among objects in a chart, which is not a common consideration in general computer vision problems; and different types of charts may not be processed by the same model. To address these problems, we propose a framework of a single deep neural network, which consists of object detection, text recognition and object matching modules. The framework handles both bar and pie charts, and it may also be extended to other types of charts by slight revisions and by augmenting the training data. Our model performs successfully on 79.4% of test simulated bar charts and 88.0% of test simulated pie charts, while for charts outside of the training domain it degrades for 57.5% and 62.3%, respectively.



### Detection and Tracking of Multiple Mice Using Part Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02831v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02831v3)
- **Published**: 2019-06-06 22:04:12+00:00
- **Updated**: 2022-03-25 14:19:49+00:00
- **Authors**: Zheheng Jiang, Zhihua Liu, Long Chen, Lei Tong, Xiangrong Zhang, Xiangyuan Lan, Danny Crookes, Ming-Hsuan Yang, Huiyu Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: The study of mouse social behaviours has been increasingly undertaken in neuroscience research. However, automated quantification of mouse behaviours from the videos of interacting mice is still a challenging problem, where object tracking plays a key role in locating mice in their living spaces. Artificial markers are often applied for multiple mice tracking, which are intrusive and consequently interfere with the movements of mice in a dynamic environment. In this paper, we propose a novel method to continuously track several mice and individual parts without requiring any specific tagging. Firstly, we propose an efficient and robust deep learning based mouse part detection scheme to generate part candidates. Subsequently, we propose a novel Bayesian Integer Linear Programming Model that jointly assigns the part candidates to individual targets with necessary geometric constraints whilst establishing pair-wise association between the detected parts. There is no publicly available dataset in the research community that provides a quantitative test-bed for the part detection and tracking of multiple mice, and we here introduce a new challenging Multi-Mice PartsTrack dataset that is made of complex behaviours and actions. Finally, we evaluate our proposed approach against several baselines on our new datasets, where the results show that our method outperforms the other state-of-the-art approaches in terms of accuracy.



### How to make a pizza: Learning a compositional layer-based GAN model
- **Arxiv ID**: http://arxiv.org/abs/1906.02839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02839v1)
- **Published**: 2019-06-06 23:22:31+00:00
- **Updated**: 2019-06-06 23:22:31+00:00
- **Authors**: Dim P. Papadopoulos, Youssef Tamaazousti, Ferda Ofli, Ingmar Weber, Antonio Torralba
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: A food recipe is an ordered set of instructions for preparing a particular dish. From a visual perspective, every instruction step can be seen as a way to change the visual appearance of the dish by adding extra objects (e.g., adding an ingredient) or changing the appearance of the existing ones (e.g., cooking the dish). In this paper, we aim to teach a machine how to make a pizza by building a generative model that mirrors this step-by-step procedure. To do so, we learn composable module operations which are able to either add or remove a particular ingredient. Each operator is designed as a Generative Adversarial Network (GAN). Given only weak image-level supervision, the operators are trained to generate a visual layer that needs to be added to or removed from the existing image. The proposed model is able to decompose an image into an ordered sequence of layers by applying sequentially in the right order the corresponding removing modules. Experimental results on synthetic and real pizza images demonstrate that our proposed model is able to: (1) segment pizza toppings in a weaklysupervised fashion, (2) remove them by revealing what is occluded underneath them (i.e., inpainting), and (3) infer the ordering of the toppings without any depth ordering supervision. Code, data, and models are available online.



