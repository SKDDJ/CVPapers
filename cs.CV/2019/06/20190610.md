# Arxiv Papers in cs.CV on 2019-06-10
### Zero-Shot Image Classification Using Coupled Dictionary Embedding
- **Arxiv ID**: http://arxiv.org/abs/1906.10509v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.10509v2)
- **Published**: 2019-06-10 01:35:36+00:00
- **Updated**: 2021-10-23 17:06:41+00:00
- **Authors**: Mohammad Rostami, Soheil Kolouri, Zak Murez, Yuri Owekcho, Eric Eaton, Kuyngnam Kim
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1709.03688
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is a framework to classify images belonging to unseen classes based on solely semantic information about these unseen classes. In this paper, we propose a new ZSL algorithm using coupled dictionary learning. The core idea is that the visual features and the semantic attributes of an image can share the same sparse representation in an intermediate space. We use images from seen classes and semantic attributes from seen and unseen classes to learn two dictionaries that can represent sparsely the visual and semantic feature vectors of an image. In the ZSL testing stage and in the absence of labeled data, images from unseen classes can be mapped into the attribute space by finding the joint sparse representation using solely the visual data. The image is then classified in the attribute space given semantic descriptions of unseen classes. We also provide an attribute-aware formulation to tackle domain shift and hubness problems in ZSL. Extensive experiments are provided to demonstrate the superior performance of our approach against the state of the art ZSL algorithms on benchmark ZSL datasets.



### Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping
- **Arxiv ID**: http://arxiv.org/abs/1906.03764v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03764v6)
- **Published**: 2019-06-10 01:53:42+00:00
- **Updated**: 2020-05-17 02:16:28+00:00
- **Authors**: Adam W. Harley, Shrinidhi K. Lakshmikanth, Fangyu Li, Xian Zhou, Hsiao-Yu Fish Tung, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: None
- **Summary**: Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature maps of the scene, by disentangling the scene content from the motion of the camera. The model also projects its 3D feature maps to novel viewpoints, to predict and match against target views. We propose contrastive prediction losses to replace the standard color regression loss, and show that this leads to better performance on complex photorealistic data. We show that the proposed model learns visual representations useful for (1) semi-supervised learning of 3D object detectors, and (2) unsupervised learning of 3D moving object detectors, by estimating the motion of the inferred 3D feature maps in videos of dynamic scenes. To the best of our knowledge, this is the first work that empirically shows view prediction to be a scalable self-supervised task beneficial to 3D object detection.



### BAGS: An automatic homework grading system using the pictures taken by smart phones
- **Arxiv ID**: http://arxiv.org/abs/1906.03767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03767v1)
- **Published**: 2019-06-10 02:21:28+00:00
- **Updated**: 2019-06-10 02:21:28+00:00
- **Authors**: Xiaoshuo Li, Tiezhu Yue, Xuanping Huang, Zhe Yang, Gang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Homework grading is critical to evaluate teaching quality and effect. However, it is usually time-consuming to grade the homework manually. In automatic homework grading scenario, many optical mark reader (OMR)-based solutions which require specific equipments have been proposed. Although many of them can achieve relatively high accuracy, they are less convenient for users. In contrast, with the popularity of smart phones, the automatic grading system which depends on the image photographed by phones becomes more available. In practice, due to different photographing angles or uneven papers, images may be distorted. Moreover, most of images are photographed under complex backgrounds, making answer areas detection more difficult. To solve these problems, we propose BAGS, an automatic homework grading system which can effectively locate and recognize handwritten answers. In BAGS, all the answers would be written above the answer area underlines (AAU), and we use two segmentation networks based on DeepLabv3+ to locate the answer areas. Then, we use the characters recognition part to recognize students' answers. Finally, the grading part is designed for the comparison between the recognized answers and the standard ones. In our test, BAGS correctly locates and recognizes the handwritten answers in 91% of total answer areas.



### BDNet: Bengali Handwritten Numeral Digit Recognition based on Densely connected Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.03786v5
- **DOI**: 10.1016/j.jksuci.2020.03.002
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03786v5)
- **Published**: 2019-06-10 03:31:58+00:00
- **Updated**: 2020-03-12 13:15:44+00:00
- **Authors**: A. Sufian, Anirudha Ghosh, Avijit Naskar, Farhana Sultana, Jaya Sil, M M Hafizur Rahman
- **Comment**: 23 pages, 11 figures, 7 tables, Accepted Manuscript. Journal of King
  Saud University - Computer and Information Sciences (2019)
- **Journal**: Journal of King Saud University - Computer and Information
  Sciences, Elsevier, Online, 2020
- **Summary**: Images of handwritten digits are different from natural images as the orientation of a digit, as well as similarity of features of different digits, makes confusion. On the other hand, deep convolutional neural networks are achieving huge success in computer vision problems, especially in image classification. BDNet is a densely connected deep convolutional neural network model used to classify (recognize) Bengali handwritten numeral digits. It is end-to-end trained using ISI Bengali handwritten numeral dataset. During training, untraditional data preprocessing and augmentation techniques are used so that the trained model works on a different dataset. The model has achieved the test accuracy of 99.775%(baseline was 99.40%) on the test dataset of ISI Bengali handwritten numerals. So, the BDNet model gives 62.5% error reduction compared to previous state-of-the-art models. Here we have also created a dataset of 1000 images of Bengali handwritten numerals to test the trained model, and it giving promising results. Codes, trained model and our own dataset are available at: {https://github.com/Sufianlab/BDNet}.



### Intriguing properties of adversarial training at scale
- **Arxiv ID**: http://arxiv.org/abs/1906.03787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03787v2)
- **Published**: 2019-06-10 03:41:52+00:00
- **Updated**: 2019-12-21 20:48:24+00:00
- **Authors**: Cihang Xie, Alan Yuille
- **Comment**: To appear in ICLR 2020
- **Journal**: None
- **Summary**: Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of adversarial training, which reveals two intriguing properties.   First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness.   Second, we study the role of network capacity. We find our so-called "deep" networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to "deep" networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale, i.e., ResNet-638.



### Fast Spatially-Varying Indoor Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.03799v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03799v1)
- **Published**: 2019-06-10 05:25:58+00:00
- **Updated**: 2019-06-10 05:25:58+00:00
- **Authors**: Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, Jean-François Lalonde
- **Comment**: CVPR19
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 6908-6917
- **Summary**: We propose a real-time method to estimate spatiallyvarying indoor lighting from a single RGB image. Given an image and a 2D location in that image, our CNN estimates a 5th order spherical harmonic representation of the lighting at the given location in less than 20ms on a laptop mobile graphics card. While existing approaches estimate a single, global lighting representation or require depth as input, our method reasons about local lighting without requiring any geometry information. We demonstrate, through quantitative experiments including a user study, that our results achieve lower lighting estimation errors and are preferred by users over the state-of-the-art. Our approach can be used directly for augmented reality applications, where a virtual object is relit realistically at any position in the scene in real-time.



### Learning to Segment Skin Lesions from Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/1906.03815v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03815v2)
- **Published**: 2019-06-10 06:51:21+00:00
- **Updated**: 2019-08-20 21:39:44+00:00
- **Authors**: Zahra Mirikharaji, Yiqi Yan, Ghassan Hamarneh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks have driven substantial advancements in the automatic understanding of images. Requiring a large collection of images and their associated annotations is one of the main bottlenecks limiting the adoption of deep networks. In the task of medical image segmentation, requiring pixel-level semantic annotations performed by human experts exacerbate this difficulty. This paper proposes a new framework to train a fully convolutional segmentation network from a large set of cheap unreliable annotations and a small set of expert-level clean annotations. We propose a spatially adaptive reweighting approach to treat clean and noisy pixel-level annotations commensurately in the loss function. We deploy a meta-learning approach to assign higher importance to pixels whose loss gradient direction is closer to those of clean data. Our experiments on training the network using segmentation ground truth corrupted with different levels of annotation noise show how spatial reweighting improves the robustness of deep networks to noisy annotations.



### HalalNet: A Deep Neural Network that Classifies the Halalness Slaughtered Chicken from their Images
- **Arxiv ID**: http://arxiv.org/abs/1906.11893v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11893v1)
- **Published**: 2019-06-10 06:55:14+00:00
- **Updated**: 2019-06-10 06:55:14+00:00
- **Authors**: A. Elfakharany, R. Yusof, N. Ismail, R. Arfa, M. Yunus
- **Comment**: Submitted in the International Conference on Artificial Intelligence
  and Robotics for Industrial Applications, AIR2018
- **Journal**: International Journal of Integrated Engineering, Vol. 11, no. 4,
  Sept. 2019,
  https://publisher.uthm.edu.my/ojs/index.php/ijie/article/view/4194
- **Summary**: Halal requirement in food is important for millions of Muslims worldwide especially for meat and chicken products, insuring that slaughter houses adhere to this requirement is a challenging task to do manually. In this paper a method is proposed that uses a camera that takes images of slaughtered chicken on the conveyor in a slaughter house, the images are then analyzed by a deep neural network to classify if the image is of a halal slaughtered chicken or not. However, traditional deep learning models require large amounts of data to train on, which in this case these amounts of data were challenging to collect especially the images of non-halal slaughtered chicken, hence this paper shows how the use of one shot learning [1] and transfer learning [2] can reach high accuracy on the few amounts of data that were available. The architecture used is based on the Siamese neural networks architecture which ranks the similarity between two inputs [3] while using the Xception network [4] as the twin networks. We call it HalalNet. This work was done as part of SYCUT (syriah compliant slaughtering system) which is a monitoring system that monitors the halalness of the slaughtered chicken in a slaughter house. The data used to train and validate HalalNet was collected from the Azain slaughtering site (Semenyih, Selangor, Malaysia) containing images of both halal and non-halal slaughtered chicken.



### Network Implosion: Effective Model Compression for ResNets via Static Layer Pruning and Retraining
- **Arxiv ID**: http://arxiv.org/abs/1906.03826v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03826v1)
- **Published**: 2019-06-10 07:53:18+00:00
- **Updated**: 2019-06-10 07:53:18+00:00
- **Authors**: Yasutoshi Ida, Yasuhiro Fujiwara
- **Comment**: Preprint of International Joint Conference on Neural Networks (IJCNN)
  2019
- **Journal**: None
- **Summary**: Residual Networks with convolutional layers are widely used in the field of machine learning. Since they effectively extract features from input data by stacking multiple layers, they can achieve high accuracy in many applications. However, the stacking of many layers raises their computation costs. To address this problem, we propose Network Implosion, it erases multiple layers from Residual Networks without degrading accuracy. Our key idea is to introduce a priority term that identifies the importance of a layer; we can select unimportant layers according to the priority and erase them after the training. In addition, we retrain the networks to avoid critical drops in accuracy after layer erasure. A theoretical assessment reveals that our erasure and retraining scheme can erase layers without accuracy drop, and achieve higher accuracy than is possible with training from scratch. Our experiments show that Network Implosion can, for classification on Cifar-10/100 and ImageNet, reduce the number of layers by 24.00 to 42.86 percent without any drop in accuracy.



### Synthesizing 3D Shapes from Silhouette Image Collections using Multi-projection Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.03841v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.03841v1)
- **Published**: 2019-06-10 08:45:35+00:00
- **Updated**: 2019-06-10 08:45:35+00:00
- **Authors**: Xiao Li, Yue Dong, Pieter Peers, Xin Tong
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a new weakly supervised learning-based method for generating novel category-specific 3D shapes from unoccluded image collections. Our method is weakly supervised and only requires silhouette annotations from unoccluded, category-specific objects. Our method does not require access to the object's 3D shape, multiple observations per object from different views, intra-image pixel-correspondences, or any view annotations. Key to our method is a novel multi-projection generative adversarial network (MP-GAN) that trains a 3D shape generator to be consistent with multiple 2D projections of the 3D shapes, and without direct access to these 3D shapes. This is achieved through multiple discriminators that encode the distribution of 2D projections of the 3D shapes seen from a different views. Additionally, to determine the view information for each silhouette image, we also train a view prediction network on visualizations of 3D shapes synthesized by the generator. We iteratively alternate between training the generator and training the view prediction network. We validate our multi-projection GAN on both synthetic and real image datasets. Furthermore, we also show that multi-projection GANs can aid in learning other high-dimensional distributions from lower dimensional training datasets, such as material-class specific spatially varying reflectance properties from images.



### Progressive Cluster Purification for Transductive Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.03847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03847v1)
- **Published**: 2019-06-10 08:55:37+00:00
- **Updated**: 2019-06-10 08:55:37+00:00
- **Authors**: Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to learn to generalize a classifier to novel classes with limited labeled data. Transductive inference that utilizes unlabeled test set to deal with low-data problem has been employed for few-shot learning in recent literature. Yet, these methods do not explicitly exploit the manifold structures of semantic clusters, which is inefficient for transductive inference. In this paper, we propose a novel Progressive Cluster Purification (PCP) method for transductive few-shot learning. The PCP can progressively purify the cluster by exploring the semantic interdependency in the individual cluster space. Specifically, the PCP consists of two-level operations: inter-class classification and intra-class transduction. The inter-class classification partitions all the test samples into several clusters by comparing the test samples with the prototypes. The intra-class transduction effectively explores trustworthy test samples for each cluster by modeling data relations within a cluster as well as among different clusters. Then, it refines the prototypes to better represent the real distribution of semantic clusters. The refined prototypes are used to remeasure all the test instances and purify each cluster. Furthermore, the inter-class classification and the intra-class transduction are extremely flexible to be repeated several times to progressively purify the clusters. Experimental results are provided on two datasets: miniImageNet dataset and tieredImageNet dataset. The comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods on both datasets.



### DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions
- **Arxiv ID**: http://arxiv.org/abs/1906.03853v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03853v2)
- **Published**: 2019-06-10 09:13:02+00:00
- **Updated**: 2019-06-11 18:10:45+00:00
- **Authors**: Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, Shuran Song
- **Comment**: RSS 2019. Project page: http://zhenjiaxu.com/DensePhysNet
- **Journal**: None
- **Summary**: We study the problem of learning physical object representations for robot manipulation. Understanding object physics is critical for successful object manipulation, but also challenging because physical object properties can rarely be inferred from the object's static appearance. In this paper, we propose DensePhysNet, a system that actively executes a sequence of dynamic interactions (e.g., sliding and colliding), and uses a deep predictive model over its visual observations to learn dense, pixel-wise representations that reflect the physical properties of observed objects. Our experiments in both simulation and real settings demonstrate that the learned representations carry rich physical information, and can directly be used to decode physical object properties such as friction and mass. The use of dense representation enables DensePhysNet to generalize well to novel scenes with more objects than in training. With knowledge of object physics, the learned representation also leads to more accurate and efficient manipulation in downstream tasks than the state-of-the-art.



### UniDual: A Unified Model for Image and Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1906.03857v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03857v2)
- **Published**: 2019-06-10 09:15:52+00:00
- **Updated**: 2019-06-12 07:30:33+00:00
- **Authors**: Yufei Wang, Du Tran, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: Although a video is effectively a sequence of images, visual perception systems typically model images and videos separately, thus failing to exploit the correlation and the synergy provided by these two media. While a few prior research efforts have explored the benefits of leveraging still-image datasets for video analysis, or vice-versa, most of these attempts have been limited to pretraining a model on one type of visual modality and then adapting it via finetuning on the other modality. In contrast, in this paper we introduce a framework that enables joint training of a unified model on mixed collections of image and video examples spanning different tasks. The key ingredient in our architecture design is a new network block, which we name UniDual. It consists of a shared 2D spatial convolution followed by two parallel point-wise convolutional layers, one devoted to images and the other one used for videos. For video input, the point-wise filtering implements a temporal convolution. For image input, it performs a pixel-wise nonlinear transformation. Repeated stacking of such blocks gives rise to a network where images and videos undergo partially distinct execution pathways, unified by spatial convolutions (capturing commonalities in visual appearance) but separated by point-wise operations (modeling patterns specific to each modality). Extensive experiments on Kinetics and ImageNet demonstrate that our UniDual model jointly trained on these datasets yields substantial accuracy gains for both tasks, compared to 1) training separate models, 2) traditional multi-task learning and 3) the conventional framework of pretraining-followed-by-finetuning. On Kinetics, the UniDual architecture applied to a state-of-the-art video backbone model (R(2+1)D-152) yields an additional video@1 accuracy gain of 1.5%.



### Few-Shot Learning with Per-Sample Rich Supervision
- **Arxiv ID**: http://arxiv.org/abs/1906.03859v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03859v1)
- **Published**: 2019-06-10 09:17:30+00:00
- **Updated**: 2019-06-10 09:17:30+00:00
- **Authors**: Roman Visotsky, Yuval Atzmon, Gal Chechik
- **Comment**: Accepted to AGI 2019 (oral)
- **Journal**: None
- **Summary**: Learning with few samples is a major challenge for parameter-rich models like deep networks. In contrast, people learn complex new concepts even from very few examples, suggesting that the sample complexity of learning can often be reduced. Many approaches to few-shot learning build on transferring a representation from well-sampled classes, or using meta learning to favor architectures that can learn with few samples. Unfortunately, such approaches often struggle when learning in an online way or with non-stationary data streams. Here we describe a new approach to learn with fewer samples, by using additional information that is provided per sample. Specifically, we show how the sample complexity can be reduced by providing semantic information about the relevance of features per sample, like information about the presence of objects in a scene or confidence of detecting attributes in an image. We provide an improved generalization error bound for this case. We cast the problem of using per-sample feature relevance by using a new ellipsoid-margin loss, and develop an online algorithm that minimizes this loss effectively. Empirical evaluation on two machine vision benchmarks for scene classification and fine-grain bird classification demonstrate the benefits of this approach for few-shot learning.



### From Data Quality to Model Quality: an Exploratory Study on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.11882v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11882v1)
- **Published**: 2019-06-10 09:19:32+00:00
- **Updated**: 2019-06-10 09:19:32+00:00
- **Authors**: Tianxing He, Shengcheng Yu, Ziyuan Wang, Jieqiong Li, Zhenyu Chen
- **Comment**: 4pages
- **Journal**: None
- **Summary**: Nowadays, people strive to improve the accuracy of deep learning models. However, very little work has focused on the quality of data sets. In fact, data quality determines model quality. Therefore, it is important for us to make research on how data quality affects on model quality. In this paper, we mainly consider four aspects of data quality, including Dataset Equilibrium, Dataset Size, Quality of Label, Dataset Contamination. We deign experiment on MNIST and Cifar-10 and try to find out the influence the four aspects make on model quality. Experimental results show that four aspects all have decisive impact on the quality of models. It means that decrease in data quality in these aspects will reduce the accuracy of model.



### Scale Steerable Filters for Locally Scale-Invariant Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.03861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03861v1)
- **Published**: 2019-06-10 09:25:38+00:00
- **Updated**: 2019-06-10 09:25:38+00:00
- **Authors**: Rohan Ghosh, Anupam K. Gupta
- **Comment**: Accepted as a Spotlight talk to ICML Workshop on Theoretical Physics
  for Deep Learning, 2019
- **Journal**: None
- **Summary**: Augmenting transformation knowledge onto a convolutional neural network's weights has often yielded significant improvements in performance. For rotational transformation augmentation, an important element to recent approaches has been the use of a steerable basis i.e. the circular harmonics. Here, we propose a scale-steerable filter basis for the locally scale-invariant CNN, denoted as log-radial harmonics. By replacing the kernels in the locally scale-invariant CNN \cite{lsi_cnn} with scale-steered kernels, significant improvements in performance can be observed on the MNIST-Scale and FMNIST-Scale datasets. Training with a scale-steerable basis results in filters which show meaningful structure, and feature maps demonstrate which demonstrate visibly higher spatial-structure preservation of input. Furthermore, the proposed scale-steerable CNN shows on-par generalization to global affine transformation estimation methods such as Spatial Transformers, in response to test-time data distortions.



### An Image Clustering Auto-Encoder Based on Predefined Evenly-Distributed Class Centroids and MMD Distance
- **Arxiv ID**: http://arxiv.org/abs/1906.03905v2
- **DOI**: 10.1007/s11063-020-10194-y
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03905v2)
- **Published**: 2019-06-10 11:28:15+00:00
- **Updated**: 2020-01-04 07:29:28+00:00
- **Authors**: Qiuyu Zhu, Zhengyong Wang
- **Comment**: Accepted by Neural Processing Letters
- **Journal**: None
- **Summary**: In this paper, we propose a novel, effective and simpler end-to-end image clustering auto-encoder algorithm: ICAE. The algorithm uses PEDCC (Predefined Evenly-Distributed Class Centroids) as the clustering centers, which ensures the inter-class distance of latent features is maximal, and adds data distribution constraint, data augmentation constraint, auto-encoder reconstruction constraint and Sobel smooth constraint to improve the clustering performance. Specifically, we perform one-to-one data augmentation to learn the more effective features. The data and the augmented data are simultaneously input into the autoencoder to obtain latent features and the augmented latent features whose similarity are constrained by an augmentation loss. Then, making use of the maximum mean discrepancy distance (MMD), we combine the latent features and augmented latent features to make their distribution close to the PEDCC distribution (uniform distribution between classes, Dirac distribution within the class) to further learn clustering-oriented features. At the same time, the MSE of the original input image and reconstructed image is used as reconstruction constraint, and the Sobel smooth loss to build generalization constraint to improve the generalization ability. Finally, extensive experiments on three common datasets MNIST, Fashion-MNIST, COIL20 are conducted. The experimental results show that the algorithm has achieved the best clustering results so far. In addition, we can use the predefined PEDCC class centers, and the decoder to clearly generate the samples of each class. The code can be downloaded at https://github.com/zyWang-Power/Clustering!



### Automatic Segmentation of Vestibular Schwannoma from T2-Weighted MRI by Deep Spatial Attention with Hardness-Weighted Loss
- **Arxiv ID**: http://arxiv.org/abs/1906.03906v1
- **DOI**: 10.1007/978-3-030-32245-8_30
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03906v1)
- **Published**: 2019-06-10 11:28:22+00:00
- **Updated**: 2019-06-10 11:28:22+00:00
- **Authors**: Guotai Wang, Jonathan Shapey, Wenqi Li, Reuben Dorent, Alex Demitriadis, Sotirios Bisdas, Ian Paddick, Robert Bradford, Sebastien Ourselin, Tom Vercauteren
- **Comment**: 9 pages, 4 figures, submitted to MICCAI
- **Journal**: None
- **Summary**: Automatic segmentation of vestibular schwannoma (VS) tumors from magnetic resonance imaging (MRI) would facilitate efficient and accurate volume measurement to guide patient management and improve clinical workflow. The accuracy and robustness is challenged by low contrast, small target region and low through-plane resolution. We introduce a 2.5D convolutional neural network (CNN) able to exploit the different in-plane and through-plane resolutions encountered in standard of care imaging protocols. We use an attention module to enable the CNN to focus on the small target and propose a supervision on the learning of attention maps for more accurate segmentation. Additionally, we propose a hardness-weighted Dice loss function that gives higher weights to harder voxels to boost the training of CNNs. Experiments with ablation studies on the VS tumor segmentation task show that: 1) the proposed 2.5D CNN outperforms its 2D and 3D counterparts, 2) our supervised attention mechanism outperforms unsupervised attention, 3) the voxel-level hardness-weighted Dice loss can improve the performance of CNNs. Our method achieved an average Dice score and ASSD of 0.87 and 0.43~mm respectively. This will facilitate patient management decisions in clinical practice.



### The role of ego vision in view-invariant action recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.03918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03918v1)
- **Published**: 2019-06-10 11:49:52+00:00
- **Updated**: 2019-06-10 11:49:52+00:00
- **Authors**: Gaurvi Goyal, Nicoletta Noceti, Francesca Odone, Alessandra Sciutti
- **Comment**: Accepted for presentation at EPIC@CVPR2019 workshop
- **Journal**: None
- **Summary**: Analysis and interpretation of egocentric video data is becoming more and more important with the increasing availability and use of wearable cameras. Exploring and fully understanding affinities and differences between ego and allo (or third-person) vision is paramount for the design of effective methods to process, analyse and interpret egocentric data. In addition, a deeper understanding of ego-vision and its peculiarities may enable new research perspectives in which first person viewpoints can act either as a mean for easily acquiring large amounts of data to be employed in general-purpose recognition systems, and as a challenging test-bed to assess the usability of techniques specifically tailored to deal with allocentric vision on more challenging settings. Our work, with an eye to cognitive science findings, leverages transfer learning in Convolutional Neural Networks to demonstrate capabilities and limitations of an implicitly learnt view-invariant representation in the specific case of action recognition.



### Generation of Multimodal Justification Using Visual Word Constraint Model for Explainable Computer-Aided Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1906.03922v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03922v1)
- **Published**: 2019-06-10 12:07:58+00:00
- **Updated**: 2019-06-10 12:07:58+00:00
- **Authors**: Hyebin Lee, Seong Tae Kim, Yong Man Ro
- **Comment**: None
- **Journal**: None
- **Summary**: The ambiguity of the decision-making process has been pointed out as the main obstacle to applying the deep learning-based method in a practical way in spite of its outstanding performance. Interpretability could guarantee the confidence of deep learning system, therefore it is particularly important in the medical field. In this study, a novel deep network is proposed to explain the diagnostic decision with visual pointing map and diagnostic sentence justifying result simultaneously. For the purpose of increasing the accuracy of sentence generation, a visual word constraint model is devised in training justification generator. To verify the proposed method, comparative experiments were conducted on the problem of the diagnosis of breast masses. Experimental results demonstrated that the proposed deep network could explain diagnosis more accurately with various textual justifications.



### CVPR19 Tracking and Detection Challenge: How crowded can it get?
- **Arxiv ID**: http://arxiv.org/abs/1906.04567v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04567v1)
- **Published**: 2019-06-10 12:35:01+00:00
- **Updated**: 2019-06-10 12:35:01+00:00
- **Authors**: Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal-Taixe
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1603.00831,
  arXiv:1504.01942
- **Journal**: None
- **Summary**: Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research.   The benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal to establish a standardized evaluation of multiple object tracking methods. The challenge focuses on multiple people tracking, since pedestrians are well studied in the tracking community, and precise tracking and detection has high practical relevance. Since the first release, MOT15, MOT16 and MOT17 have tremendously contributed to the community by introducing a clean dataset and precise framework to benchmark multi-object trackers. In this paper, we present our CVPR19 benchmark, consisting of 8 new sequences depicting very crowded challenging scenes. The benchmark will be presented at the 4th BMTT MOT Challenge Workshop at the Computer Vision and Pattern Recognition Conference (CVPR) 2019, and will evaluate the state-of-the-art in multiple object tracking whend handling extremely crowded scenarios.



### Team JL Solution to Google Landmark Recognition 2019
- **Arxiv ID**: http://arxiv.org/abs/1906.11874v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11874v1)
- **Published**: 2019-06-10 12:48:26+00:00
- **Updated**: 2019-06-10 12:48:26+00:00
- **Authors**: Yinzheng Gu, Chuanpeng Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe our solution to the Google Landmark Recognition 2019 Challenge held on Kaggle. Due to the large number of classes, noisy data, imbalanced class sizes, and the presence of a significant amount of distractors in the test set, our method is based mainly on retrieval techniques with both global and local CNN approaches. Our full pipeline, after ensembling the models and applying several steps of re-ranking strategies, scores 0.37606 GAP on the private leaderboard which won the 1st place in the competition.



### Deep Learning-Based Classification Of the Defective Pistachios Via Deep Autoencoder Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.11878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.11878v1)
- **Published**: 2019-06-10 13:02:50+00:00
- **Updated**: 2019-06-10 13:02:50+00:00
- **Authors**: Mehdi Abbaszadeh, Aliakbar Rahimifard, Mohammadali Eftekhari, Hossein Ghayoumi Zadeh, Ali Fayazi, Ali Dini, Mostafa Danaeian
- **Comment**: None
- **Journal**: None
- **Summary**: Pistachio nut is mainly consumed as raw, salted or roasted because of its high nutritional properties and favorable taste. Pistachio nuts with shell and kernel defects, besides not being acceptable for a consumer, are also prone to insects damage, mold decay, and aflatoxin contamination. In this research, a deep learning-based imaging algorithm was developed to improve the sorting of nuts with shell and kernel defects that indicate the risk of aflatoxin contamination, such as dark stains, oily stains, adhering hull, fungal decay and Aspergillus molds. This paper presents an unsupervised learning method to classify defective and unpleasant pistachios based on deep Auto-encoder neural networks. The testing of the designed neural network on a validation dataset showed that nuts having dark stain, oily stain or adhering hull with an accuracy of 80.3% can be distinguished from normal nuts. Due to the limited memory available in the HPC of university, the results are reasonable and justifiable.



### Robust Classification with Sparse Representation Fusion on Diverse Data Subsets
- **Arxiv ID**: http://arxiv.org/abs/1906.11885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11885v1)
- **Published**: 2019-06-10 13:22:54+00:00
- **Updated**: 2019-06-10 13:22:54+00:00
- **Authors**: Chun-Mei Feng, Yong Xu, Zuoyong Li, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse Representation (SR) techniques encode the test samples into a sparse linear combination of all training samples and then classify the test samples into the class with the minimum residual. The classification of SR techniques depends on the representation capability on the test samples. However, most of these models view the representation problem of the test samples as a deterministic problem, ignoring the uncertainty of the representation. The uncertainty is caused by two factors, random noise in the samples and the intrinsic randomness of the sample set, which means that if we capture a group of samples, the obtained set of samples will be different in different conditions. In this paper, we propose a novel method based upon Collaborative Representation that is a special instance of SR and has closed-form solution. It performs Sparse Representation Fusion based on the Diverse Subset of training samples (SRFDS), which reduces the impact of randomness of the sample set and enhances the robustness of classification results. The proposed method is suitable for multiple types of data and has no requirement on the pattern type of the tasks. In addition, SRFDS not only preserves a closed-form solution but also greatly improves the classification performance. Promising results on various datasets serve as the evidence of better performance of SRFDS than other SR-based methods. The Matlab code of SRFDS will be accessible at http://www.yongxu.org/lunwen.html.



### E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles
- **Arxiv ID**: http://arxiv.org/abs/1906.03973v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.03973v2)
- **Published**: 2019-06-10 13:40:37+00:00
- **Updated**: 2019-06-11 08:58:35+00:00
- **Authors**: Markus Kettunen, Erik Härkönen, Jaakko Lehtinen
- **Comment**: Code and supplemental material available at
  https://github.com/mkettune/elpips/
- **Journal**: None
- **Summary**: It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of "perceptual convexity" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.



### 2nd Place and 2nd Place Solution to Kaggle Landmark Recognition andRetrieval Competition 2019
- **Arxiv ID**: http://arxiv.org/abs/1906.03990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03990v2)
- **Published**: 2019-06-10 14:04:37+00:00
- **Updated**: 2019-06-16 12:42:03+00:00
- **Authors**: Kaibing Chen, Cheng Cui, Yuning Du, Xianglong Meng, Hui Ren
- **Comment**: None
- **Journal**: None
- **Summary**: We present a retrieval based system for landmark retrieval and recognition challenge.There are five parts in retrieval competition system, including feature extraction and matching to get candidates queue; database augmentation and query extension searching; reranking from recognition results and local feature matching. In recognition challenge including: landmark and non-landmark recognition, multiple recognition results voting and reranking using combination of recognition and retrieval results. All of models trained and predicted by PaddlePaddle framework. Using our method, we achieved 2nd place in the Google Landmark Recognition 2019 and 2nd place in the Google Landmark Retrieval 2019 on kaggle. The source code is available at here.



### Detecting Clues for Skill Levels and Machine Operation Difficulty from Egocentric Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.04002v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04002v1)
- **Published**: 2019-06-10 14:18:34+00:00
- **Updated**: 2019-06-10 14:18:34+00:00
- **Authors**: Longfei Chen, Yuichi Nakamura, Kazuaki Kondo
- **Comment**: Accepted for presentation at EPIC@CVPR2019 workshop
- **Journal**: None
- **Summary**: With respect to machine operation tasks, the experiences from different skill level operators, especially novices, can provide worthy understanding about the manner in which they perceive the operational environment and formulate knowledge to deal with various operation situations. In this study, we describe the operator's behaviors by utilizing the relations among their head, hand, and operation location (hotspot) during the operation. A total of 40 experiences associated with a sewing machine operation task performed by amateur operators was recorded via a head-mounted RGB-D camera. We examined important features of operational behaviors in different skill level operators and confirmed their correlation to the difficulties of the operation steps. The result shows that the pure-gazing behavior is significantly reduced when the operator's skill improved. Moreover, the hand-approaching duration and the frequency of attention movement before operation are strongly correlated to the operational difficulty in such machine operating environments.



### Landslide Geohazard Assessment With Convolutional Neural Networks Using Sentinel-2 Imagery Data
- **Arxiv ID**: http://arxiv.org/abs/1906.06151v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06151v1)
- **Published**: 2019-06-10 15:22:54+00:00
- **Updated**: 2019-06-10 15:22:54+00:00
- **Authors**: Silvia L. Ullo, Maximillian S. Langenkamp, Tuomas P. Oikarinen, Maria P. Del Rosso, Alessandro Sebastianelli, Federica Piccirillo, Stefania Sica
- **Comment**: 4 pages, 3 figures, 1 table, accepted to 2019 IEEE IGARSS Conference
  that will be held in Japan next July
- **Journal**: None
- **Summary**: In this paper, the authors aim to combine the latest state of the art models in image recognition with the best publicly available satellite images to create a system for landslide risk mitigation. We focus first on landslide detection and further propose a similar system to be used for prediction. Such models are valuable as they could easily be scaled up to provide data for hazard evaluation, as satellite imagery becomes increasingly available. The goal is to use satellite images and correlated data to enrich the public repository of data and guide disaster relief efforts for locating precise areas where landslides have occurred. Different image augmentation methods are used to increase diversity in the chosen dataset and create more robust classification. The resulting outputs are then fed into variants of 3-D convolutional neural networks. A review of the current literature indicates there is no research using CNNs (Convolutional Neural Networks) and freely available satellite imagery for classifying landslide risk. The model has shown to be ultimately able to achieve a significantly better than baseline accuracy.



### Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.04087v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04087v2)
- **Published**: 2019-06-10 16:00:06+00:00
- **Updated**: 2019-06-11 08:32:16+00:00
- **Authors**: Kohei Ozaki, Shuhei Yokoo
- **Comment**: Technical report for Second Landmark Recognition Workshop to be held
  at CVPR 2019 (Long Beach, CA), June 16th, 2019
- **Journal**: None
- **Summary**: The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset characterized by a large magnitude of noisiness and diversity. We present a novel landmark retrieval/recognition system, robust to a noisy and diverse dataset, by our team, smlyaka. Our approach is based on deep convolutional neural networks with metric learning, trained by cosine-softmax based losses. Deep metric learning methods are usually sensitive to noise, and it could hinder to learn a reliable metric. To address this issue, we develop an automated data cleaning system. Besides, we devise a discriminative re-ranking method to address the diversity of the dataset for landmark retrieval. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle.



### Making CNNs for Video Parsing Accessible
- **Arxiv ID**: http://arxiv.org/abs/1906.11877v1
- **DOI**: 10.1145/3337722.3337755
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11877v1)
- **Published**: 2019-06-10 16:00:40+00:00
- **Updated**: 2019-06-10 16:00:40+00:00
- **Authors**: Zijin Luo, Matthew Guzdial, Mark Riedl
- **Comment**: 11 pages, 6 figures, Foundations of Digital Games 2018
- **Journal**: None
- **Summary**: The ability to extract sequences of game events for high-resolution e-sport games has traditionally required access to the game's engine. This serves as a barrier to groups who don't possess this access. It is possible to apply deep learning to derive these logs from gameplay video, but it requires computational power that serves as an additional barrier. These groups would benefit from access to these logs, such as small e-sport tournament organizers who could better visualize gameplay to inform both audience and commentators. In this paper we present a combined solution to reduce the required computational resources and time to apply a convolutional neural network (CNN) to extract events from e-sport gameplay videos. This solution consists of techniques to train a CNN faster and methods to execute predictions more quickly. This expands the types of machines capable of training and running these models, which in turn extends access to extracting game logs with this approach. We evaluate the approaches in the domain of DOTA2, one of the most popular e-sports. Our results demonstrate our approach outperforms standard backpropagation baselines.



### Human-Machine Collaboration for Fast Land Cover Mapping
- **Arxiv ID**: http://arxiv.org/abs/1906.04176v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04176v3)
- **Published**: 2019-06-10 16:04:57+00:00
- **Updated**: 2019-11-14 19:30:03+00:00
- **Authors**: Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias, Andi Peng, Dan Morris, Bistra Dilkina, Nebojsa Jojic
- **Comment**: To appear in AAAI 2020
- **Journal**: None
- **Summary**: We propose incorporating human labelers in a model fine-tuning system that provides immediate user feedback. In our framework, human labelers can interactively query model predictions on unlabeled data, choose which data to label, and see the resulting effect on the model's predictions. This bi-directional feedback loop allows humans to learn how the model responds to new data. Our hypothesis is that this rich feedback allows human labelers to create mental models that enable them to better choose which biases to introduce to the model. We compare human-selected points to points selected using standard active learning methods. We further investigate how the fine-tuning methodology impacts the human labelers' performance. We implement this framework for fine-tuning high-resolution land cover segmentation models. Specifically, we fine-tune a deep neural network -- trained to segment high-resolution aerial imagery into different land cover classes in Maryland, USA -- to a new spatial area in New York, USA. The tight loop turns the algorithm and the human operator into a hybrid system that can produce land cover maps of a large area much more efficiently than the traditional workflows. Our framework has applications in geospatial machine learning settings where there is a practically limitless supply of unlabeled data, of which only a small fraction can feasibly be labeled through human efforts.



### Global Context for Convolutional Pose Machines
- **Arxiv ID**: http://arxiv.org/abs/1906.04104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04104v1)
- **Published**: 2019-06-10 16:24:04+00:00
- **Updated**: 2019-06-10 16:24:04+00:00
- **Authors**: Daniil Osokin
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Pose Machine is a popular neural network architecture for articulated pose estimation. In this work we explore its empirical receptive field and realize, that it can be enhanced with integration of a global context. To do so U-shaped context module is proposed and compared with the pyramid pooling and atrous spatial pyramid pooling modules, which are often used in semantic segmentation domain. The proposed neural network achieves state-of-the-art accuracy with 87.9% PCKh for single-person pose estimation on the Look Into Person dataset. A smaller version of this network runs more than 160 frames per second while being just 2.9% less accurate. Generalization of the proposed approach is tested on the MPII benchmark and shown, that it faster than hourglass-based networks, while provides similar accuracy. The code is available at https://github.com/opencv/openvino_training_extensions/tree/develop/pytorch_toolkit/human_pose_estimation .



### Quantification and Analysis of Layer-wise and Pixel-wise Information Discarding
- **Arxiv ID**: http://arxiv.org/abs/1906.04109v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04109v2)
- **Published**: 2019-06-10 16:33:03+00:00
- **Updated**: 2022-06-13 16:25:18+00:00
- **Authors**: Haotian Ma, Hao Zhang, Fan Zhou, Yinqing Zhang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a method to explain how the information of each input variable is gradually discarded during the forward propagation in a deep neural network (DNN), which provides new perspectives to explain DNNs. We define two types of entropy-based metrics, i.e. (1) the discarding of pixel-wise information used in the forward propagation, and (2) the uncertainty of the input reconstruction, to measure input information contained by a specific layer from two perspectives. Unlike previous attribution metrics, the proposed metrics ensure the fairness of comparisons between different layers of different DNNs. We can use these metrics to analyze the efficiency of information processing in DNNs, which exhibits strong connections to the performance of DNNs. We analyze information discarding in a pixel-wise manner, which is different from the information bottleneck theory measuring feature information w.r.t. the sample distribution. Experiments have shown the effectiveness of our metrics in analyzing classic DNNs and explaining existing deep-learning techniques.



### A New Ratio Image Based CNN Algorithm For SAR Despeckling
- **Arxiv ID**: http://arxiv.org/abs/1906.04111v1
- **DOI**: 10.1109/IGARSS.2019.8899245
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04111v1)
- **Published**: 2019-06-10 16:38:57+00:00
- **Updated**: 2019-06-10 16:38:57+00:00
- **Authors**: Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio
- **Comment**: None
- **Journal**: IGARSS 2019 - 2019 IEEE International Geoscience and Remote
  Sensing Symposium, Yokohama, Japan, 2019, pp. 9494-9497
- **Summary**: In SAR domain many application like classification, detection and segmentation are impaired by speckle. Hence, despeckling of SAR images is the key for scene understanding. Usually despeckling filters face the trade-off of speckle suppression and information preservation. In the last years deep learning solutions for speckle reduction have been proposed. One the biggest issue for these methods is how to train a network given the lack of a reference. In this work we proposed a convolutional neural network based solution trained on simulated data. We propose the use of a cost function taking into account both spatial and statistical properties. The aim is two fold: overcome the trade-off between speckle suppression and details suppression; find a suitable cost function for despeckling in unsupervised learning. The algorithm is validated on both real and simulated data, showing interesting performances.



### Fast Hierarchical Neural Network for Feature Learning on Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1906.04117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04117v1)
- **Published**: 2019-06-10 16:49:40+00:00
- **Updated**: 2019-06-10 16:49:40+00:00
- **Authors**: Can Chen, Luca Zanotti Fragonara, Antonios Tsourdos
- **Comment**: None
- **Journal**: None
- **Summary**: The analyses relying on 3D point clouds are an utterly complex task, often involving million of points, but also requiring computationally efficient algorithms because of many real-time applications; e.g. autonomous vehicle. However, point clouds are intrinsically irregular and the points are sparsely distributed in a non-Euclidean space, which normally requires point-wise processing to achieve high performances. Although shared filter matrices and pooling layers in convolutional neural networks (CNNs) are capable of reducing the dimensionality of the problem and extracting high-level information simultaneously, grids and highly regular data format are required as input. In order to balance model performance and complexity, we introduce a novel neural network architecture exploiting local features from a manually subsampled point set. In our network, a recursive farthest point sampling method is firstly applied to efficiently cover the entire point set. Successively, we employ the k-nearest neighbours (knn) algorithm to gather local neighbourhood for each group of the subsampled points. Finally, a multiple layer perceptron (MLP) is applied on the subsampled points and edges that connect corresponding point and neighbours to extract local features. The architecture has been tested for both shape classification and segmentation using the ModelNet40 and ShapeNet part datasets, in order to show that the network achieves the best trade-off in terms of competitive performance when compared to other state-of-the-art algorithms.



### Patch Transformer for Multi-tagging Whole Slide Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1906.04151v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04151v3)
- **Published**: 2019-06-10 17:43:52+00:00
- **Updated**: 2019-07-04 13:29:54+00:00
- **Authors**: Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, Jiebo Luo
- **Comment**: To appear at the International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI-19)
- **Journal**: None
- **Summary**: Automated whole slide image (WSI) tagging has become a growing demand due to the increasing volume and diversity of WSIs collected nowadays in histopathology. Various methods have been studied to classify WSIs with single tags but none of them focuses on labeling WSIs with multiple tags. To this end, we propose a novel end-to-end trainable deep neural network named Patch Transformer which can effectively predict multiple slide-level tags from WSI patches based on both the correlations and the uniqueness between the tags. Specifically, the proposed method learns patch characteristics considering 1) patch-wise relations through a patch transformation module and 2) tag-wise uniqueness for each tagging task through a multi-tag attention module. Extensive experiments on a large and diverse dataset consisting of 4,920 WSIs prove the effectiveness of the proposed model.



### Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in A Triadic Interaction
- **Arxiv ID**: http://arxiv.org/abs/1906.04158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.04158v1)
- **Published**: 2019-06-10 17:56:03+00:00
- **Updated**: 2019-06-10 17:56:03+00:00
- **Authors**: Hanbyul Joo, Tomas Simon, Mina Cikara, Yaser Sheikh
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a new research task and a dataset to understand human social interactions via computational methods, to ultimately endow machines with the ability to encode and decode a broad channel of social signals humans use. This research direction is essential to make a machine that genuinely communicates with humans, which we call Social Artificial Intelligence. We first formulate the "social signal prediction" problem as a way to model the dynamics of social signals exchanged among interacting individuals in a data-driven way. We then present a new 3D motion capture dataset to explore this problem, where the broad spectrum of social signals (3D body, face, and hand motions) are captured in a triadic social interaction scenario. Baseline approaches to predict speaking status, social formation, and body gestures of interacting individuals are presented in the defined social prediction framework.



### Learning Individual Styles of Conversational Gesture
- **Arxiv ID**: http://arxiv.org/abs/1906.04160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1906.04160v1)
- **Published**: 2019-06-10 17:58:08+00:00
- **Updated**: 2019-06-10 17:58:08+00:00
- **Authors**: Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, Jitendra Malik
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Human speech is often accompanied by hand and arm gestures. Given audio speech input, we generate plausible gestures to go along with the sound. Specifically, we perform cross-modal translation from "in-the-wild'' monologue speech of a single speaker to their hand and arm motion. We train on unlabeled videos for which we only have noisy pseudo ground truth from an automatic pose detection system. Our proposed model significantly outperforms baseline methods in a quantitative comparison. To support research toward obtaining a computational understanding of the relationship between gesture and speech, we release a large video dataset of person-specific gestures. The project website with video, code and data can be found at http://people.eecs.berkeley.edu/~shiry/speech2gesture .



### Self-Supervised Exploration via Disagreement
- **Arxiv ID**: http://arxiv.org/abs/1906.04161v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04161v1)
- **Published**: 2019-06-10 17:58:32+00:00
- **Updated**: 2019-06-10 17:58:32+00:00
- **Authors**: Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta
- **Comment**: Accepted at ICML 2019. Website at
  https://pathak22.github.io/exploration-by-disagreement/
- **Journal**: None
- **Summary**: Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/



### End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans
- **Arxiv ID**: http://arxiv.org/abs/1906.04201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04201v1)
- **Published**: 2019-06-10 18:01:42+00:00
- **Updated**: 2019-06-10 18:01:42+00:00
- **Authors**: Armen Avetisyan, Angela Dai, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by $19.04\%$ for CAD model alignment to scans, with $\approx 250\times$ faster runtime than previous data-driven approaches.



### FASTER Recurrent Networks for Efficient Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.04226v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04226v2)
- **Published**: 2019-06-10 18:54:00+00:00
- **Updated**: 2019-09-08 17:10:01+00:00
- **Authors**: Linchao Zhu, Laura Sevilla-Lara, Du Tran, Matt Feiszli, Yi Yang, Heng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Typical video classification methods often divide a video into short clips, do inference on each clip independently, then aggregate the clip-level predictions to generate the video-level results. However, processing visually similar clips independently ignores the temporal structure of the video sequence, and increases the computational cost at inference time. In this paper, we propose a novel framework named FASTER, i.e., Feature Aggregation for Spatio-TEmporal Redundancy. FASTER aims to leverage the redundancy between neighboring clips and reduce the computational cost by learning to aggregate the predictions from models of different complexities. The FASTER framework can integrate high quality representations from expensive models to capture subtle motion information and lightweight representations from cheap models to cover scene changes in the video. A new recurrent network (i.e., FAST-GRU) is designed to aggregate the mixture of different representations. Compared with existing approaches, FASTER can reduce the FLOPs by over 10x? while maintaining the state-of-the-art accuracy across popular datasets, such as Kinetics, UCF-101 and HMDB-51.



### Alzheimer's Disease Brain MRI Classification: Challenges and Insights
- **Arxiv ID**: http://arxiv.org/abs/1906.04231v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04231v1)
- **Published**: 2019-06-10 19:04:00+00:00
- **Updated**: 2019-06-10 19:04:00+00:00
- **Authors**: Yi Ren Fung, Ziqiang Guan, Ritesh Kumar, Joie Yeahuay Wu, Madalina Fiterau
- **Comment**: 5 pages, 2 figures, IJCAI ARIAL workshop paper
- **Journal**: None
- **Summary**: In recent years, many papers have reported state-of-the-art performance on Alzheimer's Disease classification with MRI scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset using convolutional neural networks. However, we discover that when we split that data into training and testing sets at the subject level, we are not able to obtain similar performance, bringing the validity of many of the previous studies into question. Furthermore, we point out that previous works use different subsets of the ADNI data, making comparison across similar works tricky. In this study, we present the results of three splitting methods, discuss the motivations behind their validity, and report our results using all of the available subjects.



### BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour Extraction
- **Arxiv ID**: http://arxiv.org/abs/1906.04232v1
- **DOI**: 10.1121/1.5137212
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1906.04232v1)
- **Published**: 2019-06-10 19:04:09+00:00
- **Updated**: 2019-06-10 19:04:09+00:00
- **Authors**: M. Hamed Mozaffari, Won-Sook Lee
- **Comment**: 23 pages, 15 figures, 10 tables
- **Journal**: BowNet: Dilated convolutional neural network for ultrasound tongue
  contour extraction, 2019, The Journal of the Acoustical Society of America,
  pages 2940-2941, volume 146, number 4
- **Summary**: Ultrasound imaging is safe, relatively affordable, and capable of real-time performance. One application of this technology is to visualize and to characterize human tongue shape and motion during a real-time speech to study healthy or impaired speech production. Due to the noisy nature of ultrasound images with low-contrast characteristic, it might require expertise for non-expert users to recognize organ shape such as tongue surface (dorsum). To alleviate this difficulty for quantitative analysis of tongue shape and motion, tongue surface can be extracted, tracked, and visualized instead of the whole tongue region. Delineating the tongue surface from each frame is a cumbersome, subjective, and error-prone task. Furthermore, the rapidity and complexity of tongue gestures have made it a challenging task, and manual segmentation is not a feasible solution for real-time applications. Employing the power of state-of-the-art deep neural network models and training techniques, it is feasible to implement new fully-automatic, accurate, and robust segmentation methods with the capability of real-time performance, applicable for tracking of the tongue contours during the speech. This paper presents two novel deep neural network models named BowNet and wBowNet benefits from the ability of global prediction of decoding-encoding models, with integrated multi-scale contextual information, and capability of full-resolution (local) extraction of dilated convolutions. Experimental results using several ultrasound tongue image datasets revealed that the combination of both localization and globalization searching could improve prediction result significantly. Assessment of BowNet models using both qualitatively and quantitatively studies showed them outstanding achievements in terms of accuracy and robustness in comparison with similar techniques.



### Identifying Visible Actions in Lifestyle Vlogs
- **Arxiv ID**: http://arxiv.org/abs/1906.04236v1
- **DOI**: 10.18653/v1/P19-1643
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04236v1)
- **Published**: 2019-06-10 19:11:01+00:00
- **Updated**: 2019-06-10 19:11:01+00:00
- **Authors**: Oana Ignat, Laura Burdick, Jia Deng, Rada Mihalcea
- **Comment**: Accepted at ACL 2019
- **Journal**: None
- **Summary**: We consider the task of identifying human actions visible in online videos. We focus on the widely spread genre of lifestyle vlogs, which consist of videos of people performing actions while verbally describing them. Our goal is to identify if actions mentioned in the speech description of a video are visually present. We construct a dataset with crowdsourced manual annotations of visible actions, and introduce a multimodal algorithm that leverages information derived from visual and linguistic clues to automatically infer which actions are visible in a video. We demonstrate that our multimodal algorithm outperforms algorithms based only on one modality at a time.



### SymNet: Symmetrical Filters in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04252v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04252v1)
- **Published**: 2019-06-10 19:55:03+00:00
- **Updated**: 2019-06-10 19:55:03+00:00
- **Authors**: Gregory Dzhezyan, Hubert Cecotti
- **Comment**: Copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: Symmetry is present in nature and science. In image processing, kernels for spatial filtering possess some symmetry (e.g. Sobel operators, Gaussian, Laplacian). Convolutional layers in artificial feed-forward neural networks have typically considered the kernel weights without any constraint. In this paper, we propose to investigate the impact of a symmetry constraint in convolutional layers for image classification tasks, taking our inspiration from the processes involved in the primary visual cortex and common image processing techniques. The goal is to assess the extent to which it is possible to enforce symmetrical constraints on the filters throughout the training process of a convolutional neural network (CNN) by modifying the weight update preformed during the backpropagation algorithm and to evaluate the change in performance. The main hypothesis of this paper is that the symmetrical constraint reduces the number of free parameters in the network, and it is able to achieve near identical performance to the modern methodology of training. In particular, we address the following cases: x/y-axis symmetry, point reflection, and anti-point reflection. The performance has been evaluated on four databases of images. The results support the conclusion that while random weights offer more freedom to the model, the symmetry constraint provides a similar level of performance while decreasing substantially the number of free parameters in the model. Such an approach can be valuable in phase-sensitive applications that require a linear phase property throughout the feature extraction process.



### Transport Triggered Array Processor for Vision Applications
- **Arxiv ID**: http://arxiv.org/abs/1906.04258v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04258v1)
- **Published**: 2019-06-10 20:07:16+00:00
- **Updated**: 2019-06-10 20:07:16+00:00
- **Authors**: Mehdi Safarpour, Ilkka Hautala, Miguel Bordallo Lopez, Olli Silven
- **Comment**: None
- **Journal**: None
- **Summary**: Low-level sensory data processing in many Internet-of-Things (IoT) devices pursue energy efficiency by utilizing sleep modes or slowing the clocking to the minimum. To curb the share of stand-by power dissipation in those designs, near-threshold/sub-threshold operational points or ultra-low-leakage processes in fabrication are employed. Those limit the clocking rates significantly, reducing the computing throughputs of individual processing cores. In this contribution we explore compensating for the performance loss of operating in near-threshold region (Vdd =0.6V) through massive parallelization. Benefits of near-threshold operation and massive parallelism are optimum energy consumption per instruction operation and minimized memory roundtrips, respectively. The Processing Elements (PE) of the design are based on Transport Triggered Architecture. The fine grained programmable parallel solution allows for fast and efficient computation of learnable low-level features (e.g. local binary descriptors and convolutions). Other operations, including Max-pooling have also been implemented. The programmable design achieves excellent energy efficiency for Local Binary Patterns computations.



### Semantic-guided Encoder Feature Learning for Blurry Boundary Delineation
- **Arxiv ID**: http://arxiv.org/abs/1906.04306v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04306v1)
- **Published**: 2019-06-10 22:31:08+00:00
- **Updated**: 2019-06-10 22:31:08+00:00
- **Authors**: Dong Nie, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Encoder-decoder architectures are widely adopted for medical image segmentation tasks. With the lateral skip connection, the models can obtain and fuse both semantic and resolution information in deep layers to achieve more accurate segmentation performance. However, in many applications (e.g., blurry boundary images), these models often cannot precisely locate complex boundaries and segment tiny isolated parts. To solve this challenging problem, we firstly analyze why simple skip connections are not enough to help accurately locate indistinct boundaries and argue that it is due to the fuzzy information in the skip connection provided in the encoder layers. Then we propose a semantic-guided encoder feature learning strategy to learn both high resolution and rich semantic encoder features so that we can more accurately locate the blurry boundaries, which can also enhance the network by selectively learning discriminative features. Besides, we further propose a soft contour constraint mechanism to model the blurry boundary detection. Experimental results on real clinical datasets show that our proposed method can achieve state-of-the-art segmentation accuracy, especially for the blurry regions. Further analysis also indicates that our proposed network components indeed contribute to the improvement of performance. Experiments on additional datasets validate the generalization ability of our proposed method.



### Online Object Representations with Contrastive Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.04312v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.04312v1)
- **Published**: 2019-06-10 22:43:20+00:00
- **Updated**: 2019-06-10 22:43:20+00:00
- **Authors**: Sören Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, Pierre Sermanet
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose a self-supervised approach for learning representations of objects from monocular videos and demonstrate it is particularly useful in situated settings such as robotics. The main contributions of this paper are: 1) a self-supervising objective trained with contrastive learning that can discover and disentangle object attributes from video without using any labels; 2) we leverage object self-supervision for online adaptation: the longer our online model looks at objects in a video, the lower the object identification error, while the offline baseline remains with a large fixed error; 3) to explore the possibilities of a system entirely free of human supervision, we let a robot collect its own data, train on this data with our self-supervise scheme, and then show the robot can point to objects similar to the one presented in front of it, demonstrating generalization of object attributes. An interesting and perhaps surprising finding of this approach is that given a limited set of objects, object correspondences will naturally emerge when using contrastive learning without requiring explicit positive pairs. Videos illustrating online object adaptation and robotic pointing are available at: https://online-objects.github.io/.



### Adaptively Preconditioned Stochastic Gradient Langevin Dynamics
- **Arxiv ID**: http://arxiv.org/abs/1906.04324v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04324v2)
- **Published**: 2019-06-10 23:38:54+00:00
- **Updated**: 2019-06-12 11:50:58+00:00
- **Authors**: Chandrasekaran Anirudh Bhardwaj
- **Comment**: International Conference on Machine Learning (ICML) 2019 Workshop on
  Understanding and Improving Generalization in Deep Learning
- **Journal**: None
- **Summary**: Stochastic Gradient Langevin Dynamics infuses isotropic gradient noise to SGD to help navigate pathological curvature in the loss landscape for deep networks. Isotropic nature of the noise leads to poor scaling, and adaptive methods based on higher order curvature information such as Fisher Scoring have been proposed to precondition the noise in order to achieve better convergence. In this paper, we describe an adaptive method to estimate the parameters of the noise and conduct experiments on well-known model architectures to show that the adaptively preconditioned SGLD method achieves convergence with the speed of adaptive first order methods such as Adam, AdaGrad etc. and achieves generalization equivalent of SGD in the test set.



