# Arxiv Papers in cs.CV on 2019-06-28
### Densely Residual Laplacian Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1906.12021v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.12021v2)
- **Published**: 2019-06-28 02:32:44+00:00
- **Updated**: 2019-07-01 05:31:05+00:00
- **Authors**: Saeed Anwar, Nick Barnes
- **Comment**: None
- **Journal**: None
- **Summary**: Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.



### Learning from Web Data with Self-Organizing Memory Module
- **Arxiv ID**: http://arxiv.org/abs/1906.12028v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.12028v5)
- **Published**: 2019-06-28 03:29:01+00:00
- **Updated**: 2020-03-11 17:08:27+00:00
- **Authors**: Yi Tu, Li Niu, Junjie Chen, Dawei Cheng, Liqing Zhang
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: Learning from web data has attracted lots of research interest in recent years. However, crawled web images usually have two types of noises, label noise and background noise, which induce extra difficulties in utilizing them effectively. Most existing methods either rely on human supervision or ignore the background noise. In this paper, we propose a novel method, which is capable of handling these two types of noises together, without the supervision of clean images in the training stage. Particularly, we formulate our method under the framework of multi-instance learning by grouping ROIs (i.e., images and their region proposals) from the same category into bags. ROIs in each bag are assigned with different weights based on the representative/discriminative scores of their nearest clusters, in which the clusters and their scores are obtained via our designed memory module. Our memory module could be naturally integrated with the classification module, leading to an end-to-end trainable system. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our method.



### Deep Multi-Task Learning for Anomalous Driving Detection Using CAN Bus Scalar Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1907.00749v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00749v1)
- **Published**: 2019-06-28 04:36:47+00:00
- **Updated**: 2019-06-28 04:36:47+00:00
- **Authors**: Vidyasagar Sadhu, Teruhisa Misu, Dario Pompili
- **Comment**: IROS 2019, 8 pages
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), pp. 1-8
- **Summary**: Corner cases are the main bottlenecks when applying Artificial Intelligence (AI) systems to safety-critical applications. An AI system should be intelligent enough to detect such situations so that system developers can prepare for subsequent planning. In this paper, we propose semi-supervised anomaly detection considering the imbalance of normal situations. In particular, driving data consists of multiple positive/normal situations (e.g., right turn, going straight), some of which (e.g., U-turn) could be as rare as anomalous situations. Existing machine learning based anomaly detection approaches do not fare sufficiently well when applied to such imbalanced data. In this paper, we present a novel multi-task learning based approach that leverages domain-knowledge (maneuver labels) for anomaly detection in driving data. We evaluate the proposed approach both quantitatively and qualitatively on 150 hours of real-world driving data and show improved performance over baseline approaches.



### Background Subtraction using Adaptive Singular Value Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1906.12064v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.12064v1)
- **Published**: 2019-06-28 07:17:14+00:00
- **Updated**: 2019-06-28 07:17:14+00:00
- **Authors**: Günther Reitberger, Tomas Sauer
- **Comment**: None
- **Journal**: None
- **Summary**: An important task when processing sensor data is to distinguish relevant from irrelevant data. This paper describes a method for an iterative singular value decomposition that maintains a model of the background via singular vectors spanning a subspace of the image space, thus providing a way to determine the amount of new information contained in an incoming frame. We update the singular vectors spanning the background space in a computationally efficient manner and provide the ability to perform block-wise updates, leading to a fast and robust adaptive SVD computation. The effects of those two properties and the success of the overall method to perform a state of the art background subtraction are shown in both qualitative and quantitative evaluations.



### An algorithm for the selection of route dependent orientation information
- **Arxiv ID**: http://arxiv.org/abs/1907.05289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05289v1)
- **Published**: 2019-06-28 07:43:45+00:00
- **Updated**: 2019-06-28 07:43:45+00:00
- **Authors**: Heinrich Löwen, Angela Schwering
- **Comment**: None
- **Journal**: None
- **Summary**: Landmarks are important features of spatial cognition. Landmarks are naturally included in human route descriptions and in the past algorithms were developed to select the most salient landmarks at decision points and automatically incorporate them in route instructions. Moreover, it was shown that human route descriptions contain a significant amount of orientation information and that these orientation information support the acquisition of survey knowledge. Thus, there is a need to extend the landmarks selection in order to automatically select orientation information. In this work we present an algorithm for the computational selection of route dependent orientation information, which extends previous algorithms and includes a salience evaluation of orientation information for any location along the route. We implemented the algorithm and demonstrate the functionality on the basis of OpenStreetMap data.



### A linear method for camera pair self-calibration and multi-view reconstruction with geometrically verified correspondences
- **Arxiv ID**: http://arxiv.org/abs/1906.12075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.12075v1)
- **Published**: 2019-06-28 07:45:19+00:00
- **Updated**: 2019-06-28 07:45:19+00:00
- **Authors**: Nikos Melanitis, Petros Maragos
- **Comment**: None
- **Journal**: None
- **Summary**: We examine 3D reconstruction of architectural scenes in unordered sets of uncalibrated images. We introduce a linear method to self-calibrate and find the metric reconstruction of a camera pair. We assume unknown and different focal lengths but otherwise known internal camera parameters and a known projective reconstruction of the camera pair. We recover two possible camera configurations in space and use the Cheirality condition, that all 3D scene points are in front of both cameras, to disambiguate the solution. We show in two Theorems, first that the two solutions are in mirror positions and then the relations between their viewing directions. Our new method performs on par (median rotation error $\Delta R = 3.49^{\circ}$) with the standard approach of Kruppa equations ($\Delta R = 3.77^{\circ}$) for self-calibration and 5-Point algorithm for calibrated metric reconstruction of a camera pair. We reject erroneous image correspondences by introducing a method to examine whether point correspondences appear in the same order along $x, y$ image axes in image pairs. We evaluate this method by its precision and recall and show that it improves the robustness of point matches in architectural and general scenes. Finally, we integrate all the introduced methods to a 3D reconstruction pipeline. We utilize the numerous camera pair metric recontructions using rotation-averaging algorithms and a novel method to average focal length estimates.



### Voxel-FPN: multi-scale voxel feature aggregation in 3D object detection from point clouds
- **Arxiv ID**: http://arxiv.org/abs/1907.05286v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.05286v2)
- **Published**: 2019-06-28 09:49:10+00:00
- **Updated**: 2019-07-16 08:22:44+00:00
- **Authors**: Bei Wang, Jianping An, Jiayan Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in point cloud data is one of the key components in computer vision systems, especially for autonomous driving applications. In this work, we present Voxel-FPN, a novel one-stage 3D object detector that utilizes raw data from LIDAR sensors only. The core framework consists of an encoder network and a corresponding decoder followed by a region proposal network. Encoder extracts multi-scale voxel information in a bottom-up manner while decoder fuses multiple feature maps from various scales in a top-down way. Extensive experiments show that the proposed method has better performance on extracting features from point data and demonstrates its superiority over some baselines on the challenging KITTI-3D benchmark, obtaining good performance on both speed and accuracy in real-world scenarios.



### Fully automatic computer-aided mass detection and segmentation via pseudo-color mammograms and Mask R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1906.12118v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.12118v2)
- **Published**: 2019-06-28 09:58:25+00:00
- **Updated**: 2019-10-19 06:42:40+00:00
- **Authors**: Hang Min, Devin Wilson, Yinhuang Huang, Siyu Liu, Stuart Crozier, Andrew P Bradley, Shekhar S. Chandra
- **Comment**: None
- **Journal**: None
- **Summary**: Mammographic mass detection and segmentation are usually performed as serial and separate tasks, with segmentation often only performed on manually confirmed true positive detections in previous studies. We propose a fully-integrated computer-aided detection (CAD) system for simultaneous mammographic mass detection and segmentation without user intervention. The proposed CAD only consists of a pseudo-color image generation and a mass detection-segmentation stage based on Mask R-CNN. Grayscale mammograms are transformed into pseudo-color images based on multi-scale morphological sifting where mass-like patterns are enhanced to improve the performance of Mask R-CNN. Transfer learning with the Mask R-CNN is then adopted to simultaneously detect and segment masses on the pseudo-color images. Evaluated on the public dataset INbreast, the method outperforms the state-of-the-art methods by achieving an average true positive rate of 0.90 at 0.9 false positive per image and an average Dice similarity index of 0.88 for mass segmentation.



### Place recognition in gardens by learning visual representations: data set and benchmark analysis
- **Arxiv ID**: http://arxiv.org/abs/1906.12151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.12151v1)
- **Published**: 2019-06-28 12:01:55+00:00
- **Updated**: 2019-06-28 12:01:55+00:00
- **Authors**: Maria Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov
- **Comment**: Accepted for the 18th International Conference on Computer Analysis
  of Images and Patterns
- **Journal**: None
- **Summary**: Visual place recognition is an important component of systems for camera localization and loop closure detection. It concerns the recognition of a previously visited place based on visual cues only. Although it is a widely studied problem for indoor and urban environments, the recent use of robots for automation of agricultural and gardening tasks has created new problems, due to the challenging appearance of garden-like environments. Garden scenes predominantly contain green colors, as well as repetitive patterns and textures. The lack of available data recorded in gardens and natural environments makes the improvement of visual localization algorithms difficult. In this paper we propose an extended version of the TB-Places data set, which is designed for testing algorithms for visual place recognition. It contains images with ground truth camera pose recorded in real gardens in different seasons, with varying light conditions. We constructed and released a ground truth for all possible pairs of images, indicating whether they depict the same place or not. We present the results of a benchmark analysis of methods based on convolutional neural networks for holistic image description and place recognition. We train existing networks (i.e. ResNet, DenseNet and VGG NetVLAD) as backbone of a two-way architecture with a contrastive loss function. The results that we obtained demonstrate that learning garden-tailored representations contribute to an improvement of performance, although the generalization capabilities are limited.



### Open-Ended Long-Form Video Question Answering via Hierarchical Convolutional Self-Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.12158v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.12158v1)
- **Published**: 2019-06-28 12:23:37+00:00
- **Updated**: 2019-06-28 12:23:37+00:00
- **Authors**: Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song, Xiaofei He
- **Comment**: Accepted by IJCAI 2019 as a poster paper
- **Journal**: None
- **Summary**: Open-ended video question answering aims to automatically generate the natural-language answer from referenced video contents according to the given question. Currently, most existing approaches focus on short-form video question answering with multi-modal recurrent encoder-decoder networks. Although these works have achieved promising performance, they may still be ineffectively applied to long-form video question answering due to the lack of long-range dependency modeling and the suffering from the heavy computational cost. To tackle these problems, we propose a fast Hierarchical Convolutional Self-Attention encoder-decoder network(HCSA). Concretely, we first develop a hierarchical convolutional self-attention encoder to efficiently model long-form video contents, which builds the hierarchical structure for video sequences and captures question-aware long-range dependencies from video context. We then devise a multi-scale attentive decoder to incorporate multi-layer video representations for answer generation, which avoids the information missing of the top encoder layer. The extensive experiments show the effectiveness and efficiency of our method.



### Localizing Unseen Activities in Video via Image Query
- **Arxiv ID**: http://arxiv.org/abs/1906.12165v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1906.12165v1)
- **Published**: 2019-06-28 12:32:41+00:00
- **Updated**: 2019-06-28 12:32:41+00:00
- **Authors**: Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song, Deng Cai
- **Comment**: Accepted by IJCAI 2019 as a poster paper
- **Journal**: None
- **Summary**: Action localization in untrimmed videos is an important topic in the field of video understanding. However, existing action localization methods are restricted to a pre-defined set of actions and cannot localize unseen activities. Thus, we consider a new task to localize unseen activities in videos via image queries, named Image-Based Activity Localization. This task faces three inherent challenges: (1) how to eliminate the influence of semantically inessential contents in image queries; (2) how to deal with the fuzzy localization of inaccurate image queries; (3) how to determine the precise boundaries of target segments. We then propose a novel self-attention interaction localizer to retrieve unseen activities in an end-to-end fashion. Specifically, we first devise a region self-attention method with relative position encoding to learn fine-grained image region representations. Then, we employ a local transformer encoder to build multi-step fusion and reasoning of image and video contents. We next adopt an order-sensitive localizer to directly retrieve the target segment. Furthermore, we construct a new dataset ActivityIBAL by reorganizing the ActivityNet dataset. The extensive experiments show the effectiveness of our method.



### Accurate Retinal Vessel Segmentation via Octave Convolution Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.12193v8
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.12193v8)
- **Published**: 2019-06-28 13:07:36+00:00
- **Updated**: 2020-09-23 03:04:25+00:00
- **Authors**: Zhun Fan, Jiajie Mo, Benzhang Qiu, Wenji Li, Guijie Zhu, Chong Li, Jianye Hu, Yibiao Rong, Xinjian Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal vessel segmentation is a crucial step in diagnosing and screening various diseases, including diabetes, ophthalmologic diseases, and cardiovascular diseases. In this paper, we propose an effective and efficient method for vessel segmentation in color fundus images using encoder-decoder based octave convolution networks. Compared with other convolution networks utilizing standard convolution for feature extraction, the proposed method utilizes octave convolutions and octave transposed convolutions for learning multiple-spatial-frequency features, thus can better capture retinal vasculatures with varying sizes and shapes. To provide the network the capability of learning how to decode multifrequency features, we extend octave convolution and propose a new operation named octave transposed convolution. A novel architecture of convolutional neural network, named as Octave UNet integrating both octave convolutions and octave transposed convolutions is proposed based on the encoder-decoder architecture of UNet, which can generate high resolution vessel segmentation in one single forward feeding without post-processing steps. Comprehensive experimental results demonstrate that the proposed Octave UNet outperforms the baseline UNet achieving better or comparable performance to the state-of-the-art methods with fast processing speed. Specifically, the proposed method achieves 0.9664 / 0.9713 / 0.9759 / 0.9698 accuracy, 0.8374 / 0.8664 / 0.8670 / 0.8076 sensitivity, 0.9790 / 0.9798 / 0.9840 / 0.9831 specificity, 0.8127 / 0.8191 / 0.8313 / 0.7963 F1 score, and 0.9835 / 0.9875 / 0.9905 / 0.9845 Area Under Receiver Operating Characteristic curve, on DRIVE, STARE, CHASE_DB1, and HRF datasets, respectively.



### Adversarial optimization for joint registration and segmentation in prostate CT radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/1906.12223v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.12223v1)
- **Published**: 2019-06-28 13:55:03+00:00
- **Updated**: 2019-06-28 13:55:03+00:00
- **Authors**: Mohamed S. Elmahdy, Jelmer M. Wolterink, Hessam Sokooti, Ivana Išgum, Marius Staring
- **Comment**: Accepted to MICCAI 2019, 13-17 Oct 2019, Shenzhen, China
- **Journal**: None
- **Summary**: Joint image registration and segmentation has long been an active area of research in medical imaging. Here, we reformulate this problem in a deep learning setting using adversarial learning. We consider the case in which fixed and moving images as well as their segmentations are available for training, while segmentations are not available during testing; a common scenario in radiotherapy. The proposed framework consists of a 3D end-to-end generator network that estimates the deformation vector field (DVF) between fixed and moving images in an unsupervised fashion and applies this DVF to the moving image and its segmentation. A discriminator network is trained to evaluate how well the moving image and segmentation align with the fixed image and segmentation. The proposed network was trained and evaluated on follow-up prostate CT scans for image-guided radiotherapy, where the planning CT contours are propagated to the daily CT images using the estimated DVF. A quantitative comparison with conventional registration using \texttt{elastix} showed that the proposed method improved performance and substantially reduced computation time, thus enabling real-time contour propagation necessary for online-adaptive radiotherapy.



### Modelling Airway Geometry as Stock Market Data using Bayesian Changepoint Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.12225v2
- **DOI**: 10.1007/978-3-030-32692-0_40
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.12225v2)
- **Published**: 2019-06-28 13:56:58+00:00
- **Updated**: 2019-10-27 18:09:20+00:00
- **Authors**: Kin Quan, Ryutaro Tanno, Michael Duong, Arjun Nair, Rebecca Shipley, Mark Jones, Christopher Brereton, John Hurst, David Hawkes, Joseph Jacob
- **Comment**: 14 pages, 7 figures, Accepted to The 10th International Workshop on
  Machine Learning in Medical Imaging (MLMI 2019). In conjunction with MICCAI
  2019, Shenzhen, China
- **Journal**: In Lecture Notes in Computer Science, vol 11861. (2019) Springer,
  Cham
- **Summary**: Numerous lung diseases, such as idiopathic pulmonary fibrosis (IPF), exhibit dilation of the airways. Accurate measurement of dilatation enables assessment of the progression of disease. Unfortunately the combination of image noise and airway bifurcations causes high variability in the profiles of cross-sectional areas, rendering the identification of affected regions very difficult. Here we introduce a noise-robust method for automatically detecting the location of progressive airway dilatation given two profiles of the same airway acquired at different time points. We propose a probabilistic model of abrupt relative variations between profiles and perform inference via Reversible Jump Markov Chain Monte Carlo sampling. We demonstrate the efficacy of the proposed method on two datasets; (i) images of healthy airways with simulated dilatation; (ii) pairs of real images of IPF-affected airways acquired at 1 year intervals. Our model is able to detect the starting location of airway dilatation with an accuracy of 2.5mm on simulated data. The experiments on the IPF dataset display reasonable agreement with radiologists. We can compute a relative change in airway volume that may be useful for quantifying IPF disease progression. The code is available at https://github.com/quan14/Modelling_Airway_Geometry_as_Stock_Market_Data



### A Regularized Convolutional Neural Network for Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.05287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.05287v1)
- **Published**: 2019-06-28 14:45:17+00:00
- **Updated**: 2019-06-28 14:45:17+00:00
- **Authors**: Fan Jia, Jun Liu, Xue-cheng Tai
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) show outstanding performance in many image processing problems, such as image recognition, object detection and image segmentation. Semantic segmentation is a very challenging task that requires recognizing, understanding what's in the image in pixel level. Though the state of the art has been greatly improved by CNNs, there is no explicit connections between prediction of neighbouring pixels. That is, spatial regularity of the segmented objects is still a problem for CNNs. In this paper, we propose a method to add spatial regularization to the segmented objects. In our method, the spatial regularization such as total variation (TV) can be easily integrated into CNN network. It can help CNN find a better local optimum and make the segmentation results more robust to noise. We apply our proposed method to Unet and Segnet, which are well established CNNs for image segmentation, and test them on WBC, CamVid and SUN-RGBD datasets, respectively. The results show that the regularized networks not only could provide better segmentation results with regularization effect than the original ones but also have certain robustness to noise.



### PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/1906.12320v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.12320v3)
- **Published**: 2019-06-28 17:25:54+00:00
- **Updated**: 2019-09-02 12:11:36+00:00
- **Authors**: Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, Bharath Hariharan
- **Comment**: Published in ICCV 2019
- **Journal**: None
- **Summary**: As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code will be available at https://github.com/stevenygd/PointFlow.



### Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1906.12340v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.12340v2)
- **Published**: 2019-06-28 17:44:00+00:00
- **Updated**: 2019-10-29 17:57:27+00:00
- **Authors**: Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song
- **Comment**: NeurIPS 2019; code and data available at
  https://github.com/hendrycks/ss-ood
- **Journal**: None
- **Summary**: Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.



### Classification of glomerular hypercellularity using convolutional features and support vector machine
- **Arxiv ID**: http://arxiv.org/abs/1907.00028v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00028v1)
- **Published**: 2019-06-28 18:29:45+00:00
- **Updated**: 2019-06-28 18:29:45+00:00
- **Authors**: Paulo Chagas, Luiz Souza, Ikaro Araújo, Nayze Aldeman, Angelo Duarte, Michele Angelo, Washington LC dos-Santos, Luciano Oliveira
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: Glomeruli are histological structures of the kidney cortex formed by interwoven blood capillaries, and are responsible for blood filtration. Glomerular lesions impair kidney filtration capability, leading to protein loss and metabolic waste retention. An example of lesion is the glomerular hypercellularity, which is characterized by an increase in the number of cell nuclei in different areas of the glomeruli. Glomerular hypercellularity is a frequent lesion present in different kidney diseases. Automatic detection of glomerular hypercellularity would accelerate the screening of scanned histological slides for the lesion, enhancing clinical diagnosis. Having this in mind, we propose a new approach for classification of hypercellularity in human kidney images. Our proposed method introduces a novel architecture of a convolutional neural network (CNN) along with a support vector machine, achieving near perfect average results with the FIOCRUZ data set in a binary classification (lesion or normal). Our deep-based classifier outperformed the state-of-the-art results on the same data set. Additionally, classification of hypercellularity sub-lesions was also performed, considering mesangial, endocapilar and both lesions; in this multi-classification task, our proposed method just failed in 4\% of the cases. To the best of our knowledge, this is the first study on deep learning over a data set of glomerular hypercellularity images of human kidney.



### Explainable Anatomical Shape Analysis through Deep Hierarchical Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1907.00058v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.00058v2)
- **Published**: 2019-06-28 19:58:08+00:00
- **Updated**: 2020-01-04 13:54:23+00:00
- **Authors**: Carlo Biffi, Juan J. Cerrolaza, Giacomo Tarroni, Wenjia Bai, Antonio de Marvao, Ozan Oktay, Christian Ledig, Loic Le Folgoc, Konstantinos Kamnitsas, Georgia Doumou, Jinming Duan, Sanjay K. Prasad, Stuart A. Cook, Declan P. O'Regan, Daniel Rueckert
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
  (TMI)
- **Journal**: None
- **Summary**: Quantification of anatomical shape changes currently relies on scalar global indexes which are largely insensitive to regional or asymmetric modifications. Accurate assessment of pathology-driven anatomical remodeling is a crucial step for the diagnosis and treatment of many conditions. Deep learning approaches have recently achieved wide success in the analysis of medical images, but they lack interpretability in the feature extraction and decision processes. In this work, we propose a new interpretable deep learning model for shape analysis. In particular, we exploit deep generative networks to model a population of anatomical segmentations through a hierarchy of conditional latent variables. At the highest level of this hierarchy, a two-dimensional latent space is simultaneously optimised to discriminate distinct clinical conditions, enabling the direct visualisation of the classification space. Moreover, the anatomical variability encoded by this discriminative latent space can be visualised in the segmentation space thanks to the generative properties of the model, making the classification task transparent. This approach yielded high accuracy in the categorisation of healthy and remodelled left ventricles when tested on unseen segmentations from our own multi-centre dataset as well as in an external validation set, and on hippocampi from healthy controls and patients with Alzheimer's disease when tested on ADNI data. More importantly, it enabled the visualisation in three-dimensions of both global and regional anatomical features which better discriminate between the conditions under exam. The proposed approach scales effectively to large populations, facilitating high-throughput analysis of normal anatomy and pathology in large-scale studies of volumetric imaging.



### On Reducing Negative Jacobian Determinant of the Deformation Predicted by Deep Registration Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.00068v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00068v1)
- **Published**: 2019-06-28 20:42:12+00:00
- **Updated**: 2019-06-28 20:42:12+00:00
- **Authors**: Dongyang Kuang
- **Comment**: None
- **Journal**: None
- **Summary**: Image registration is a fundamental step in medical image analysis. Ideally, the transformation that registers one image to another should be a diffeomorphism that is both invertible and smooth. Traditional methods like geodesic shooting approach the problem via differential geometry, with theoretical guarantees that the resulting transformation will be smooth and invertible. Most previous research using unsupervised deep neural networks for registration have used a local smoothness constraint (typically, a spatial variation loss) to address the smoothness issue. These networks usually produce non-invertible transformations with ``folding'' in multiple voxel locations, indicated by a negative determinant of the Jacobian matrix of the transformation. While using a loss function that specifically penalizes the folding is a straightforward solution, this usually requires carefully tuning the regularization strength, especially when there are also other losses. In this paper we address this problem from a different angle, by investigating possible training mechanisms that will help the network avoid negative Jacobians and produce smoother deformations. We contribute two independent ideas in this direction. Both ideas greatly reduce the number of folding locations in the predicted deformation, without making changes to the hyperparameters or the architecture used in the existing baseline registration network.



### A 1d convolutional network for leaf and time series classification
- **Arxiv ID**: http://arxiv.org/abs/1907.00069v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00069v2)
- **Published**: 2019-06-28 20:44:16+00:00
- **Updated**: 2020-09-17 03:06:49+00:00
- **Authors**: Dongyang Kuang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a 1d convolutional neural network is designed for classification tasks of plant leaves. This network based classifier is analyzed in two directions. In the forward direction, the proposed network can be used in two ways: a classifier and an automatic feature extractor. As a classifier, it takes the simple centroid contour distance curve as the single feature and achieves comparable performance with state-of-art methods that usually require multiple extracted features. As a feature extractor, it produces nearly linear separable features, hence can be used together with other classifiers such as support vector machines to provide better performance. The proposed network adopts simple 1d input and is generally applicable for other tasks such as classifying one dimensional time series in an end-to-end fashion without changes. Experiments on some benchmark datasets show this architecture can provide classification accuracies that are comparable or higher than some existing methods. In the backward direction, methods like gradient-weighted class activation mapping and maximum activation map of neurons in the classification layer with respect to inputs are performed to help investigate and further validate that hidden signatures helping trigger the trained classifier's specific decisions can be human interpretable. Code for the paper is available at https://github.com/dykuang/Leaf_Project.



### Robustness Guarantees for Deep Neural Networks on Videos
- **Arxiv ID**: http://arxiv.org/abs/1907.00098v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00098v3)
- **Published**: 2019-06-28 22:27:17+00:00
- **Updated**: 2020-04-03 18:07:06+00:00
- **Authors**: Min Wu, Marta Kwiatkowska
- **Comment**: To appear in 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Journal**: None
- **Summary**: The widespread adoption of deep learning models places demands on their robustness. In this paper, we consider the robustness of deep neural networks on videos, which comprise both the spatial features of individual frames extracted by a convolutional neural network and the temporal dynamics between adjacent frames captured by a recurrent neural network. To measure robustness, we study the maximum safe radius problem, which computes the minimum distance from the optical flow sequence obtained from a given input to that of an adversarial example in the neighbourhood of the input. We demonstrate that, under the assumption of Lipschitz continuity, the problem can be approximated using finite optimisation via discretising the optical flow space, and the approximation has provable guarantees. We then show that the finite optimisation problem can be solved by utilising a two-player turn-based game in a cooperative setting, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. We employ an anytime approach to solve the game, in the sense of approximating the value of the game by monotonically improving its upper and lower bounds. We exploit a gradient-based search algorithm to compute the upper bounds, and the admissible A* algorithm to update the lower bounds. Finally, we evaluate our framework on the UCF101 video dataset.



