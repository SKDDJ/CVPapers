# Arxiv Papers in cs.CV on 2019-06-15
### Efficient Neural Network Approaches for Leather Defect Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.06446v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06446v1)
- **Published**: 2019-06-15 01:12:02+00:00
- **Updated**: 2019-06-15 01:12:02+00:00
- **Authors**: Sze-Teng Liong, Y. S. Gan, Kun-Hong Liu, Tran Quang Binh, Cong Tue Le, Chien An Wu, Cheng-Yan Yang, Yen-Chang Huang
- **Comment**: 15 pages, 10 Tables, 8 figures
- **Journal**: None
- **Summary**: Genuine leather, such as the hides of cows, crocodiles, lizards and goats usually contain natural and artificial defects, like holes, fly bites, tick marks, veining, cuts, wrinkles and others. A traditional solution to identify the defects is by manual defect inspection, which involves skilled experts. It is time consuming and may incur a high error rate and results in low productivity. This paper presents a series of automatic image processing processes to perform the classification of leather defects by adopting deep learning approaches. Particularly, the leather images are first partitioned into small patches,then it undergoes a pre-processing technique, namely the Canny edge detection to enhance defect visualization. Next, artificial neural network (ANN) and convolutional neural network (CNN) are employed to extract the rich image features. The best classification result achieved is 80.3 %, evaluated on a data set that consists of 2000 samples. In addition, the performance metrics such as confusion matrix and Receiver Operating Characteristic (ROC) are reported to demonstrate the efficiency of the method proposed.



### High Speed and High Dynamic Range Video with an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1906.07165v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07165v1)
- **Published**: 2019-06-15 04:56:14+00:00
- **Updated**: 2019-06-15 04:56:14+00:00
- **Authors**: Henri Rebecq, RenÃ© Ranftl, Vladlen Koltun, Davide Scaramuzza
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.08298
- **Journal**: None
- **Summary**: Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous "events" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality (> 20%), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos (> 5,000 frames per second) of high-speed phenomena (e.g. a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. We also demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data.



### Speeding up VP9 Intra Encoder with Hierarchical Deep Learning Based Partition Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.06476v2
- **DOI**: 10.1109/TIP.2020.3011270
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06476v2)
- **Published**: 2019-06-15 05:50:25+00:00
- **Updated**: 2020-07-19 21:38:51+00:00
- **Authors**: Somdyuti Paul, Andrey Norkin, Alan C. Bovik
- **Comment**: None
- **Journal**: None
- **Summary**: In VP9 video codec, the sizes of blocks are decided during encoding by recursively partitioning 64$\times$64 superblocks using rate-distortion optimization (RDO). This process is computationally intensive because of the combinatorial search space of possible partitions of a superblock. Here, we propose a deep learning based alternative framework to predict the intra-mode superblock partitions in the form of a four-level partition tree, using a hierarchical fully convolutional network (H-FCN). We created a large database of VP9 superblocks and the corresponding partitions to train an H-FCN model, which was subsequently integrated with the VP9 encoder to reduce the intra-mode encoding time. The experimental results establish that our approach speeds up intra-mode encoding by 69.7% on average, at the expense of a 1.71% increase in the Bjontegaard-Delta bitrate (BD-rate). While VP9 provides several built-in speed levels which are designed to provide faster encoding at the expense of decreased rate-distortion performance, we find that our model is able to outperform the fastest recommended speed level of the reference VP9 encoder for the good quality intra encoding configuration, in terms of both speedup and BD-rate.



### Visual Context-aware Convolution Filters for Transformation-invariant Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.09986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09986v1)
- **Published**: 2019-06-15 05:53:16+00:00
- **Updated**: 2019-06-15 05:53:16+00:00
- **Authors**: Suraj Tripathi, Abhay Kumar, Chirag Singh
- **Comment**: Under-Review
- **Journal**: None
- **Summary**: We propose a novel visual context-aware filter generation module which incorporates contextual information present in images into Convolutional Neural Networks (CNNs). In contrast to traditional CNNs, we do not employ the same set of learned convolution filters for all input image instances. Our proposed input-conditioned convolution filters when combined with techniques inspired by Multi-instance learning and max-pooling, results in a transformation-invariant neural network. We investigated the performance of our proposed framework on three MNIST variations, which covers both rotation and scaling variance, and achieved 1.13% error on MNIST-rot-12k, 1.12% error on Half-rotated MNIST and 0.68% error on Scaling MNIST, which is significantly better than the state-of-the-art results. We make use of visualization to further prove the effectiveness of our visual context-aware convolution filters. Our proposed visual context-aware convolution filter generation framework can also serve as a plugin for any CNN based architecture and enhance its modeling capacity.



### RECAL: Reuse of Established CNN classifer Apropos unsupervised Learning paradigm
- **Arxiv ID**: http://arxiv.org/abs/1906.06480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06480v1)
- **Published**: 2019-06-15 06:46:04+00:00
- **Updated**: 2019-06-15 06:46:04+00:00
- **Authors**: Jayasree Saha, Jayanta Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, clustering with deep network framework has attracted attention of several researchers in the computer vision community. Deep framework gains extensive attention due to its efficiency and scalability towards large-scale and high-dimensional data. In this paper, we transform supervised CNN classifier architecture into an unsupervised clustering model, called RECAL, which jointly learns discriminative embedding subspace and cluster labels. RECAL is made up of feature extraction layers which are convolutional, followed by unsupervised classifier layers which is fully connected. A multinomial logistic regression function (softmax) stacked on top of classifier layers. We train this network using stochastic gradient descent (SGD) optimizer. However, the successful implementation of our model is revolved around the design of loss function. Our loss function uses the heuristics that true partitioning entails lower entropy given that the class distribution is not heavily skewed. This is a trade-off between the situations of "skewed distribution" and "low-entropy". To handle this, we have proposed classification entropy and class entropy which are the two components of our loss function. In this approach, size of the mini-batch should be kept high. Experimental results indicate the consistent and competitive behavior of our model for clustering well-known digit, multi-viewed object and face datasets. Morever, we use this model to generate unsupervised patch segmentation for multi-spectral LISS-IV images. We observe that it is able to distinguish built-up area, wet land, vegetation and waterbody from the underlying scene.



### Accelerating temporal action proposal generation via high performance computing
- **Arxiv ID**: http://arxiv.org/abs/1906.06496v4
- **DOI**: 10.1007/s11704-021-0173-7
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1906.06496v4)
- **Published**: 2019-06-15 08:35:34+00:00
- **Updated**: 2020-04-24 06:35:10+00:00
- **Authors**: Tian Wang, Shiye Lei, Youyou Jiang, Choi Chang, Hichem Snoussi, Guangcun Shan
- **Comment**: 11 pages, 12 figures
- **Journal**: Frontiers of Computer Science volume 16, Article number: 164317
  (2022)
- **Summary**: Temporal action recognition always depends on temporal action proposal generation to hypothesize actions and algorithms usually need to process very long video sequences and output the starting and ending times of each potential action in each video suffering from high computation cost. To address this, based on boundary sensitive network we propose a new temporal convolution network called Multipath Temporal ConvNet (MTN), which consists of two parts i.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high performance ring parallel architecture based on Message Passing Interface (MPI) is further introduced into temporal action proposal generation, which is a reliable communication protocol, in order to respond to the requirements of large memory occupation and a large number of videos. Remarkably, the total data transmission is reduced by adding a connection between multiple computing load in the newly developed architecture. It is found that, compared to the traditional Parameter Server architecture, our parallel architecture has higher efficiency on temporal action detection task with multiple GPUs, which is suitable for dealing with the tasks of temporal action proposal generation, especially for large datasets of millions of videos. We conduct experiments on ActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art temporal action detection methods with high recall and high temporal precision. In addition, a time metric is further proposed here to evaluate the speed performance in the distributed training process.



### PVRED: A Position-Velocity Recurrent Encoder-Decoder for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.06514v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06514v2)
- **Published**: 2019-06-15 09:59:30+00:00
- **Updated**: 2021-06-13 01:42:05+00:00
- **Authors**: Hongsong Wang, Jian Dong, Bin Cheng, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction, which aims to predict future human poses given past poses, has recently seen increased interest. Many recent approaches are based on Recurrent Neural Networks (RNN) which model human poses with exponential maps. These approaches neglect the pose velocity as well as temporal relation of different poses, and tend to converge to the mean pose or fail to generate natural-looking poses. We therefore propose a novel Position-Velocity Recurrent Encoder-Decoder (PVRED) for human motion prediction, which makes full use of pose velocities and temporal positional information. A temporal position embedding method is presented and a Position-Velocity RNN (PVRNN) is proposed. We also emphasize the benefits of quaternion parameterization of poses and design a novel trainable Quaternion Transformation (QT) layer, which is combined with a robust loss function during training. We provide quantitative results for both short-term prediction in the future 0.5 seconds and long-term prediction in the future 0.5 to 1 seconds. Experiments on several benchmarks show that our approach considerably outperforms the state-of-the-art methods. In addition, qualitative visualizations in the future 4 seconds show that our approach could predict future human-like and meaningful poses in very long time horizons. Code is publicly available on GitHub: \textcolor{red}{https://github.com/hongsong-wang/PVRNN}.



### Delving into 3D Action Anticipation from Streaming Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.06521v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06521v2)
- **Published**: 2019-06-15 10:30:29+00:00
- **Updated**: 2023-06-15 00:09:45+00:00
- **Authors**: Hongsong Wang, Jiashi Feng
- **Comment**: We will modify this method
- **Journal**: None
- **Summary**: Action anticipation, which aims to recognize the action with a partial observation, becomes increasingly popular due to a wide range of applications. In this paper, we investigate the problem of 3D action anticipation from streaming videos with the target of understanding best practices for solving this problem. We first introduce several complementary evaluation metrics and present a basic model based on frame-wise action classification. To achieve better performance, we then investigate two important factors, i.e., the length of the training clip and clip sampling method. We also explore multi-task learning strategies by incorporating auxiliary information from two aspects: the full action representation and the class-agnostic action label. Our comprehensive experiments uncover the best practices for 3D action anticipation, and accordingly we propose a novel method with a multi-task loss. The proposed method considerably outperforms the recent methods and exhibits the state-of-the-art performance on standard benchmarks.



### MV-C3D: A Spatial Correlated Multi-View 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.06538v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06538v1)
- **Published**: 2019-06-15 12:00:05+00:00
- **Updated**: 2019-06-15 12:00:05+00:00
- **Authors**: Qi Xuan, Fuxian Li, Yi Liu, Yun Xiang
- **Comment**: 11 pages, 11 figures
- **Journal**: None
- **Summary**: As the development of deep neural networks, 3D object recognition is becoming increasingly popular in computer vision community. Many multi-view based methods are proposed to improve the category recognition accuracy. These approaches mainly rely on multi-view images which are rendered with the whole circumference. In real-world applications, however, 3D objects are mostly observed from partial viewpoints in a less range. Therefore, we propose a multi-view based 3D convolutional neural network, which takes only part of contiguous multi-view images as input and can still maintain high accuracy. Moreover, our model takes these view images as a joint variable to better learn spatially correlated features using 3D convolution and 3D max-pooling layers. Experimental results on ModelNet10 and ModelNet40 datasets show that our MV-C3D technique can achieve outstanding performance with multi-view images which are captured from partial angles with less range. The results on 3D rotated real image dataset MIRO further demonstrate that MV-C3D is more adaptable in real-world scenarios. The classification accuracy can be further improved with the increasing number of view images.



### Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era
- **Arxiv ID**: http://arxiv.org/abs/1906.06543v3
- **DOI**: 10.1109/TPAMI.2019.2954885
- **Categories**: **cs.CV**, cs.CG, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.06543v3)
- **Published**: 2019-06-15 12:35:05+00:00
- **Updated**: 2019-11-01 14:01:31+00:00
- **Authors**: Xian-Feng Han, Hamid Laga, Mohammed Bennamoun
- **Comment**: arXiv admin note: text overlap with arXiv:1806.06098,
  arXiv:1712.06584, arXiv:1804.10975 by other authors
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  Nov. 2019
- **Summary**: 3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.



### Mask Based Unsupervised Content Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.06558v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06558v2)
- **Published**: 2019-06-15 13:15:51+00:00
- **Updated**: 2020-01-13 12:04:01+00:00
- **Authors**: Ron Mokady, Sagie Benaim, Lior Wolf, Amit Bermano
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables weakly-supervised semantic segmentation of the separate part of each domain, where only class labels are provided. Our code is publicly available at https://github.com/rmokady/mbu-content-tansfer.



### Single Image Super-resolution via Dense Blended Attention Generative Adversarial Network for Clinical Diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1906.06575v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06575v4)
- **Published**: 2019-06-15 15:13:41+00:00
- **Updated**: 2020-02-24 03:55:33+00:00
- **Authors**: Kewen Liu, Yuan Ma, Hongxia Xiong, Zejun Yan, Zhijun Zhou, Chaoyang Liu, Panpan Fang, Xiaojun Li, Yalei Chen
- **Comment**: We abandoned this paper due to its limitation only applied on medical
  images, please view our lastest work at arXiv:1911.03464
- **Journal**: None
- **Summary**: During training phase, more connections (e.g. channel concatenation in last layer of DenseNet) means more occupied GPU memory and lower GPU utilization, requiring more training time. The increase of training time is also not conducive to launch application of SR algorithms. This's why we abandoned DenseNet as basic network. Futhermore, we abandoned this paper due to its limitation only applied on medical images. Please view our lastest work applied on general images at arXiv:1911.03464.



### EXTD: Extremely Tiny Face Detector via Iterative Filter Reuse
- **Arxiv ID**: http://arxiv.org/abs/1906.06579v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06579v2)
- **Published**: 2019-06-15 15:53:41+00:00
- **Updated**: 2019-06-23 12:41:13+00:00
- **Authors**: YoungJoon Yoo, Dongyoon Han, Sangdoo Yun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a new multi-scale face detector having an extremely tiny number of parameters (EXTD),less than 0.1 million, as well as achieving comparable performance to deep heavy detectors. While existing multi-scale face detectors extract feature maps with different scales from a single backbone network, our method generates the feature maps by iteratively reusing a shared lightweight and shallow backbone network. This iterative sharing of the backbone network significantly reduces the number of parameters, and also provides the abstract image semantics captured from the higher stage of the network layers to the lower-level feature map. The proposed idea is employed by various model architectures and evaluated by extensive experiments. From the experiments from WIDER FACE dataset, we show that the proposed face detector can handle faces with various scale and conditions, and achieved comparable performance to the more massive face detectors that few hundreds and tens times heavier in model size and floating point operations.



### IMP: Instance Mask Projection for High Accuracy Semantic Segmentation of Things
- **Arxiv ID**: http://arxiv.org/abs/1906.06597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06597v1)
- **Published**: 2019-06-15 17:58:07+00:00
- **Updated**: 2019-06-15 17:58:07+00:00
- **Authors**: Cheng-Yang Fu, Tamara L. Berg, Alexander C. Berg
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a new operator, called Instance Mask Projection (IMP), which projects a predicted Instance Segmentation as a new feature for semantic segmentation. It also supports back propagation so is trainable end-to-end. Our experiments show the effectiveness of IMP on both Clothing Parsing (with complex layering, large deformations, and non-convex objects), and on Street Scene Segmentation (with many overlapping instances and small objects). On the Varied Clothing Parsing dataset (VCP), we show instance mask projection can improve 3 points on mIOU from a state-of-the-art Panoptic FPN segmentation approach. On the ModaNet clothing parsing dataset, we show a dramatic improvement of 20.4% absolutely compared to existing baseline semantic segmentation results. In addition, the instance mask projection operator works well on other (non-clothing) datasets, providing an improvement of 3 points in mIOU on Thing classes of Cityscapes, a self-driving dataset, on top of a state-of-the-art approach.



### A Statistical View on Synthetic Aperture Imaging for Occlusion Removal
- **Arxiv ID**: http://arxiv.org/abs/1906.06600v1
- **DOI**: 10.1109/JSEN.2019.2922731
- **Categories**: **cs.GR**, cs.CV, eess.IV, I.4.1; I.4.3
- **Links**: [PDF](http://arxiv.org/pdf/1906.06600v1)
- **Published**: 2019-06-15 18:28:23+00:00
- **Updated**: 2019-06-15 18:28:23+00:00
- **Authors**: Indrajit Kurmi, David C. Schedl, Oliver Bimber
- **Comment**: 10 pages, 11 figures, IEEE Sensors Jounral (accepted)
- **Journal**: None
- **Summary**: Synthetic apertures find applications in many fields, such as radar, radio telescopes, microscopy, sonar, ultrasound, LiDAR, and optical imaging. They approximate the signal of a single hypothetical wide aperture sensor with either an array of static small aperture sensors or a single moving small aperture sensor. Common sense in synthetic aperture sampling is that a dense sampling pattern within a wide aperture is required to reconstruct a clear signal. In this article we show that there exists practical limits to both, synthetic aperture size and number of samples for the application of occlusion removal. This leads to an understanding on how to design synthetic aperture sampling patterns and sensors in a most optimal and practically efficient way. We apply our findings to airborne optical sectioning which uses camera drones and synthetic aperture imaging to computationally remove occluding vegetation or trees for inspecting ground surfaces.



### 4D X-Ray CT Reconstruction using Multi-Slice Fusion
- **Arxiv ID**: http://arxiv.org/abs/1906.06601v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.06601v1)
- **Published**: 2019-06-15 18:37:56+00:00
- **Updated**: 2019-06-15 18:37:56+00:00
- **Authors**: Soumendu Majee, Thilo Balke, Craig A. J. Kemp, Gregery T. Buzzard, Charles A. Bouman
- **Comment**: 8 pages, 8 figures, IEEE International Conference on Computational
  Photography 2019, Tokyo
- **Journal**: None
- **Summary**: There is an increasing need to reconstruct objects in four or more dimensions corresponding to space, time and other independent parameters. The best 4D reconstruction algorithms use regularized iterative reconstruction approaches such as model based iterative reconstruction (MBIR), which depends critically on the quality of the prior modeling. Recently, Plug-and-Play methods have been shown to be an effective way to incorporate advanced prior models using state-of-the-art denoising algorithms designed to remove additive white Gaussian noise (AWGN). However, state-of-the-art denoising algorithms such as BM4D and deep convolutional neural networks (CNNs) are primarily available for 2D and sometimes 3D images. In particular, CNNs are difficult and computationally expensive to implement in four or more dimensions, and training may be impossible if there is no associated high-dimensional training data.   In this paper, we present Multi-Slice Fusion, a novel algorithm for 4D and higher-dimensional reconstruction, based on the fusion of multiple low-dimensional denoisers. Our approach uses multi-agent consensus equilibrium (MACE), an extension of Plug-and-Play, as a framework for integrating the multiple lower-dimensional prior models. We apply our method to the problem of 4D cone-beam X-ray CT reconstruction for Non Destructive Evaluation (NDE) of moving parts. This is done by solving the MACE equations using lower-dimensional CNN denoisers implemented in parallel on a heterogeneous cluster. Results on experimental CT data demonstrate that Multi-Slice Fusion can substantially improve the quality of reconstructions relative to traditional 4D priors, while also being practical to implement and train.



### How To Train Your Deep Multi-Object Tracker
- **Arxiv ID**: http://arxiv.org/abs/1906.06618v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06618v3)
- **Published**: 2019-06-15 21:34:30+00:00
- **Updated**: 2020-04-23 14:00:36+00:00
- **Authors**: Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taixe, Xavier Alameda-Pineda
- **Comment**: 14 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: The recent trend in vision-based multi-object tracking (MOT) is heading towards leveraging the representational power of deep learning to jointly learn to detect and track objects. However, existing methods train only certain sub-modules using loss functions that often do not correlate with established tracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and Precision (MOTP). As these measures are not differentiable, the choice of appropriate loss functions for end-to-end training of multi-object tracking methods is still an open research problem. In this paper, we bridge this gap by proposing a differentiable proxy of MOTA and MOTP, which we combine in a loss function suitable for end-to-end training of deep multi-object trackers. As a key ingredient, we propose a Deep Hungarian Net (DHN) module that approximates the Hungarian matching algorithm. DHN allows estimating the correspondence between object tracks and ground truth objects to compute differentiable proxies of MOTA and MOTP, which are in turn used to optimize deep trackers directly. We experimentally demonstrate that the proposed differentiable framework improves the performance of existing multi-object trackers, and we establish a new state of the art on the MOTChallenge benchmark. Our code is publicly available from https://github.com/yihongXU/deepMOT.



### Generating Diverse and Informative Natural Language Fashion Feedback
- **Arxiv ID**: http://arxiv.org/abs/1906.06619v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06619v1)
- **Published**: 2019-06-15 21:39:34+00:00
- **Updated**: 2019-06-15 21:39:34+00:00
- **Authors**: Gil Sadeh, Lior Fritz, Gabi Shalev, Eduard Oks
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in multi-modal vision and language tasks enable a new set of applications. In this paper, we consider the task of generating natural language fashion feedback on outfit images. We collect a unique dataset, which contains outfit images and corresponding positive and constructive fashion feedback. We treat each feedback type separately, and train deep generative encoder-decoder models with visual attention, similar to the standard image captioning pipeline. Following this approach, the generated sentences tend to be too general and non-informative. We propose an alternative decoding technique based on the Maximum Mutual Information objective function, which leads to more diverse and detailed responses. We evaluate our model with common language metrics, and also show human evaluation results. This technology is applied within the ``Alexa, how do I look?'' feature, publicly available in Echo Look devices.



### Joint Visual-Textual Embedding for Multimodal Style Search
- **Arxiv ID**: http://arxiv.org/abs/1906.06620v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06620v1)
- **Published**: 2019-06-15 21:50:31+00:00
- **Updated**: 2019-06-15 21:50:31+00:00
- **Authors**: Gil Sadeh, Lior Fritz, Gabi Shalev, Eduard Oks
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a multimodal visual-textual search refinement method for fashion garments. Existing search engines do not enable intuitive, interactive, refinement of retrieved results based on the properties of a particular product. We propose a method to retrieve similar items, based on a query item image and textual refinement properties. We believe this method can be leveraged to solve many real-life customer scenarios, in which a similar item in a different color, pattern, length or style is desired. We employ a joint embedding training scheme in which product images and their catalog textual metadata are mapped closely in a shared space. This joint visual-textual embedding space enables manipulating catalog images semantically, based on textual refinement requirements. We propose a new training objective function, Mini-Batch Match Retrieval, and demonstrate its superiority over the commonly used triplet loss. Additionally, we demonstrate the feasibility of adding an attribute extraction module, trained on the same catalog data, and demonstrate how to integrate it within the multimodal search to boost its performance. We introduce an evaluation protocol with an associated benchmark, and compare several approaches.



### Scalable Model Compression by Entropy Penalized Reparameterization
- **Arxiv ID**: http://arxiv.org/abs/1906.06624v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06624v3)
- **Published**: 2019-06-15 22:46:33+00:00
- **Updated**: 2020-02-16 17:51:13+00:00
- **Authors**: Deniz Oktay, Johannes BallÃ©, Saurabh Singh, Abhinav Shrivastava
- **Comment**: Published in ICLR 2020
- **Journal**: None
- **Summary**: We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a "latent" space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate--accuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art model compression can be achieved in a scalable and general way without requiring complex procedures such as multi-stage training.



### REMAP: Multi-layer entropy-guided pooling of dense CNN features for image retrieval
- **Arxiv ID**: http://arxiv.org/abs/1906.06626v1
- **DOI**: 10.1109/TIP.2019.2917234
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.06626v1)
- **Published**: 2019-06-15 23:02:49+00:00
- **Updated**: 2019-06-15 23:02:49+00:00
- **Authors**: Syed Sameed Husain, Miroslaw Bober
- **Comment**: Submitted to IEEE Trans. Image Processing on 24 May 2018, published
  22 May 2019
- **Journal**: IEEE Transactions on Image Processing, Early Access 22 May 2019
- **Summary**: This paper addresses the problem of very large-scale image retrieval, focusing on improving its accuracy and robustness. We target enhanced robustness of search to factors such as variations in illumination, object appearance and scale, partial occlusions, and cluttered backgrounds - particularly important when search is performed across very large datasets with significant variability. We propose a novel CNN-based global descriptor, called REMAP, which learns and aggregates a hierarchy of deep features from multiple CNN layers, and is trained end-to-end with a triplet loss. REMAP explicitly learns discriminative features which are mutually-supportive and complementary at various semantic levels of visual abstraction. These dense local features are max-pooled spatially at each layer, within multi-scale overlapping regions, before aggregation into a single image-level descriptor. To identify the semantically useful regions and layers for retrieval, we propose to measure the information gain of each region and layer using KL-divergence. Our system effectively learns during training how useful various regions and layers are and weights them accordingly. We show that such relative entropy-guided aggregation outperforms classical CNN-based aggregation controlled by SGD. The entire framework is trained in an end-to-end fashion, outperforming the latest state-of-the-art results. On image retrieval datasets Holidays, Oxford and MPEG, the REMAP descriptor achieves mAP of 95.5%, 91.5%, and 80.1% respectively, outperforming any results published to date. REMAP also formed the core of the winning submission to the Google Landmark Retrieval Challenge on Kaggle.



### Representation Quality Of Neural Networks Links To Adversarial Attacks and Defences
- **Arxiv ID**: http://arxiv.org/abs/1906.06627v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.06627v5)
- **Published**: 2019-06-15 23:32:33+00:00
- **Updated**: 2020-07-16 14:49:14+00:00
- **Authors**: Shashank Kotyan, Danilo Vasconcellos Vargas, Moe Matsuki
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks have been shown vulnerable to a variety of adversarial algorithms. A crucial step to understanding the rationale for this lack of robustness is to assess the potential of the neural networks' representation to encode the existing features. Here, we propose a method to understand the representation quality of the neural networks using a novel test based on Zero-Shot Learning, entitled Raw Zero-Shot. The principal idea is that, if an algorithm learns rich features, such features should be able to interpret "unknown" classes as an aggregate of previously learned features. This is because unknown classes usually share several regular features with recognised classes, given the features learned are general enough. We further introduce two metrics to assess these learned features to interpret unknown classes. One is based on inter-cluster validation technique (Davies-Bouldin Index), and the other is based on the distance to an approximated ground-truth. Experiments suggest that adversarial defences improve the representation of the classifiers, further suggesting that to improve the robustness of the classifiers, one has to improve the representation quality also. Experiments also reveal a strong association (a high Pearson Correlation and low p-value) between the metrics and adversarial attacks. Interestingly, the results indicate that dynamic routing networks such as CapsNet have better representation while current deeper neural networks are trading off representation quality for accuracy.   Code available at http://bit.ly/RepresentationMetrics.



