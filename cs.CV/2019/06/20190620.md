# Arxiv Papers in cs.CV on 2019-06-20
### Improving the robustness of ImageNet classifiers using elements of human visual cognition
- **Arxiv ID**: http://arxiv.org/abs/1906.08416v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08416v2)
- **Published**: 2019-06-20 02:28:19+00:00
- **Updated**: 2020-02-10 16:29:21+00:00
- **Authors**: A. Emin Orhan, Brenden M. Lake
- **Comment**: v2 involves reformating and stylistic changes
- **Journal**: None
- **Summary**: We investigate the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale. As reported in previous work, we show that an explicit episodic memory improves the robustness of image recognition models against small-norm adversarial perturbations under some threat models. It does not, however, improve the robustness against more natural, and typically larger, perturbations. Learning more robust features during training appears to be necessary for robustness in this second sense. We show that features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) do not only improve the robustness against natural perturbations, but when used in conjunction with an episodic memory, they also provide additional robustness against adversarial perturbations. Finally, we address three important design choices for the episodic memory: memory size, dimensionality of the memories and the retrieval method. We show that to make the episodic memory more compact, it is preferable to reduce the number of memories by clustering them, instead of reducing their dimensionality.



### Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects
- **Arxiv ID**: http://arxiv.org/abs/1906.08430v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08430v1)
- **Published**: 2019-06-20 03:28:09+00:00
- **Updated**: 2019-06-20 03:28:09+00:00
- **Authors**: Gabriel Grand, Yonatan Belinkov
- **Comment**: In Proceedings of the 2nd Workshop on Shortcomings in Vision and
  Language (SiVL) at NAACL-HLT 2019
- **Journal**: None
- **Summary**: Visual question answering (VQA) models have been shown to over-rely on linguistic biases in VQA datasets, answering questions "blindly" without considering visual context. Adversarial regularization (AdvReg) aims to address this issue via an adversary sub-network that encourages the main model to learn a bias-free representation of the question. In this work, we investigate the strengths and shortcomings of AdvReg with the goal of better understanding how it affects inference in VQA models. Despite achieving a new state-of-the-art on VQA-CP, we find that AdvReg yields several undesirable side-effects, including unstable gradients and sharply reduced performance on in-domain examples. We demonstrate that gradual introduction of regularization during training helps to alleviate, but not completely solve, these issues. Through error analyses, we observe that AdvReg improves generalization to binary questions, but impairs performance on questions with heterogeneous answer distributions. Qualitatively, we also find that regularized models tend to over-rely on visual features, while ignoring important linguistic cues in the question. Our results suggest that AdvReg requires further refinement before it can be considered a viable bias mitigation technique for VQA.



### Nested Network with Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1906.08462v1
- **DOI**: 10.1109/TGRS.2019.2925070
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08462v1)
- **Published**: 2019-06-20 06:57:13+00:00
- **Updated**: 2019-06-20 06:57:13+00:00
- **Authors**: Chongyi Li, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong
- **Comment**: 11 pages, 8 figures, has been accepted by TGRS
- **Journal**: None
- **Summary**: Arising from the various object types and scales, diverse imaging orientations, and cluttered backgrounds in optical remote sensing image (RSI), it is difficult to directly extend the success of salient object detection for nature scene image to the optical RSI. In this paper, we propose an end-to-end deep network called LV-Net based on the shape of network architecture, which detects salient objects from optical RSIs in a purely data-driven fashion. The proposed LV-Net consists of two key modules, i.e., a two-stream pyramid module (L-shaped module) and an encoder-decoder module with nested connections (V-shaped module). Specifically, the L-shaped module extracts a set of complementary information hierarchically by using a two-stream pyramid structure, which is beneficial to perceiving the diverse scales and local details of salient objects. The V-shaped module gradually integrates encoder detail features with decoder semantic features through nested connections, which aims at suppressing the cluttered backgrounds and highlighting the salient objects. In addition, we construct the first publicly available optical RSI dataset for salient object detection, including 800 images with varying spatial resolutions, diverse saliency types, and pixel-wise ground truth. Experiments on this benchmark dataset demonstrate that the proposed method outperforms the state-of-the-art salient object detection methods both qualitatively and quantitatively.



### GAN-Knowledge Distillation for one-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.08467v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08467v4)
- **Published**: 2019-06-20 07:10:06+00:00
- **Updated**: 2019-07-04 02:44:51+00:00
- **Authors**: Wei Hong, Jin ke Yu Fan Zong
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks have a significant improvement in the accuracy of Object detection. As convolutional neural networks become deeper, the accuracy of detection is also obviously improved, and more floating-point calculations are needed. Many researchers use the knowledge distillation method to improve the accuracy of student networks by transferring knowledge from a deeper and larger teachers network to a small student network, in object detection. Most methods of knowledge distillation need to designed complex cost functions and they are aimed at the two-stage object detection algorithm. This paper proposes a clean and effective knowledge distillation method for the one-stage object detection. The feature maps generated by teacher network and student network are used as true samples and fake samples respectively, and generate adversarial training for both to improve the performance of the student network in one-stage object detection.



### Predicting Motion of Vulnerable Road Users using High-Definition Maps and Efficient ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1906.08469v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08469v2)
- **Published**: 2019-06-20 07:16:16+00:00
- **Updated**: 2020-06-11 06:54:12+00:00
- **Authors**: Fang-Chieh Chou, Tsung-Han Lin, Henggang Cui, Vladan Radosavljevic, Thi Nguyen, Tzu-Kuo Huang, Matthew Niedoba, Jeff Schneider, Nemanja Djuric
- **Comment**: Accepted for publication at IEEE Intelligent Vehicles Symposium (IV)
  2020
- **Journal**: None
- **Summary**: Following detection and tracking of traffic actors, prediction of their future motion is the next critical component of a self-driving vehicle (SDV) technology, allowing the SDV to operate safely and efficiently in its environment. This is particularly important when it comes to vulnerable road users (VRUs), such as pedestrians and bicyclists. These actors need to be handled with special care due to an increased risk of injury, as well as the fact that their behavior is less predictable than that of motorized actors. To address this issue, in the current study we present a deep learning-based method for predicting VRU movement, where we rasterize high-definition maps and actor's surroundings into a bird's-eye view image used as an input to deep convolutional networks. In addition, we propose a fast architecture suitable for real-time inference, and perform an ablation study of various rasterization approaches to find the optimal choice for accurate prediction. The results strongly indicate benefits of using the proposed approach for motion prediction of VRUs, both in terms of accuracy and latency.



### PointNLM: Point Nonlocal-Means for vegetation segmentation based on middle echo point clouds
- **Arxiv ID**: http://arxiv.org/abs/1906.08476v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08476v2)
- **Published**: 2019-06-20 07:34:05+00:00
- **Updated**: 2019-09-19 04:50:13+00:00
- **Authors**: Jonathan Li, Rongren Wu, Yiping Chen, Qing Zhu, Zhipeng Luo, Cheng Wang
- **Comment**: There are some syntax errors that need to be fixed
- **Journal**: None
- **Summary**: Middle-echo, which covers one or a few corresponding points, is a specific type of 3D point cloud acquired by a multi-echo laser scanner. In this paper, we propose a novel approach for automatic segmentation of trees that leverages middle-echo information from LiDAR point clouds. First, using a convolution classification method, the proposed type of point clouds reflected by the middle echoes are identified from all point clouds. The middle-echo point clouds are distinguished from the first and last echoes. Hence, the crown positions of the trees are quickly detected from the huge number of point clouds. Second, to accurately extract trees from all point clouds, we propose a 3D deep learning network, PointNLM, to semantically segment tree crowns. PointNLM captures the long-range relationship between the point clouds via a non-local branch and extracts high-level features via max-pooling applied to unordered points. The whole framework is evaluated using the Semantic 3D reduced-test set. The IoU of tree point cloud segmentation reached 0.864. In addition, the semantic segmentation network was tested using the Paris-Lille-3D dataset. The average IoU outperformed several other popular methods. The experimental results indicate that the proposed algorithm provides an excellent solution for vegetation segmentation from LiDAR point clouds.



### Efficient two step optimization for large embedded deformation graph based SLAM
- **Arxiv ID**: http://arxiv.org/abs/1906.08477v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08477v2)
- **Published**: 2019-06-20 07:36:16+00:00
- **Updated**: 2020-03-24 03:30:20+00:00
- **Authors**: Jingwei Song, Fang Bai, Liang Zhao, Shoudong Huang, Rong Xiong
- **Comment**: This work is accepted by ICRA2020 (2020 International Conference on
  Robotics and Automation) 7 pages 8 figures
- **Journal**: None
- **Summary**: Embedded deformation nodes based formulation has been widely applied in deformable geometry and graphical problems. Though being promising in stereo (or RGBD) sensor based SLAM applications, it remains challenging to keep constant speed in deformation nodes parameter estimation when model grows larger. In practice, the processing time grows rapidly in accordance with the expansion of maps. In this paper, we propose an approach to decouple nodes of deformation graph in large scale dense deformable SLAM and keep the estimation time to be constant. We observe that only partial deformable nodes in the graph are connected to visible points. Based on this fact, sparsity of original Hessian matrix is utilized to split parameter estimation in two independent steps. With this new technique, we achieve faster parameter estimation with amortized computation complexity reduced from O(n^2) to closing O(1). As a result, the computation cost barely increases as the map keeps growing. Based on our strategy, computational bottleneck in large scale embedded deformation graph based applications will be greatly mitigated. The effectiveness is validated by experiments, featuring large scale deformation scenarios.



### A Segmentation-Oriented Inter-Class Transfer Method: Application to Retinal Vessel Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.08501v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68-06
- **Links**: [PDF](http://arxiv.org/pdf/1906.08501v1)
- **Published**: 2019-06-20 08:39:38+00:00
- **Updated**: 2019-06-20 08:39:38+00:00
- **Authors**: Chengzhi Shi, Jihong Liu, Dali Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Retinal vessel segmentation, as a principal nonintrusive diagnose method for ophthalmology diseases or diabetics, suffers from data scarcity due to requiring pixel-wise labels. In this paper, we proposed a convenient patch-based two-stage transfer method. First, based on the information bottleneck theory, we insert one dimensionality-reduced layer for task-specific feature space. Next, the semi-supervised clustering is conducted to select instances, from different sources databases, possessing similarities in the feature space. Surprisingly, we empirically demonstrate that images from different classes possessing similarities contribute to better performance than some same-class instances. The proposed framework achieved an accuracy of 97%, 96.8%, and 96.77% on DRIVE, STARE, and HRF respectively, outperforming current methods and independent human observers (DRIVE (96.37%) and STARE (93.39%)).



### Multiple-Identity Image Attacks Against Face-based Identity Verification
- **Arxiv ID**: http://arxiv.org/abs/1906.08507v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.08507v1)
- **Published**: 2019-06-20 08:58:22+00:00
- **Updated**: 2019-06-20 08:58:22+00:00
- **Authors**: Jerone T. A. Andrews, Thomas Tanay, Lewis D. Griffin
- **Comment**: 13 pages, 7 figures
- **Journal**: None
- **Summary**: Facial verification systems are vulnerable to poisoning attacks that make use of multiple-identity images (MIIs)---face images stored in a database that resemble multiple persons, such that novel images of any of the constituent persons are verified as matching the identity of the MII. Research on this mode of attack has focused on defence by detection, with no explanation as to why the vulnerability exists. New quantitative results are presented that support an explanation in terms of the geometry of the representations spaces used by the verification systems. In the spherical geometry of those spaces, the angular distance distributions of matching and non-matching pairs of face representations are only modestly separated, approximately centred at 90 and 40-60 degrees, respectively. This is sufficient for open-set verification on normal data but provides an opportunity for MII attacks. Our analysis considers ideal MII algorithms, demonstrating that, if realisable, they would deliver faces roughly 45 degrees from their constituent faces, thus classed as matching them. We study the performance of three methods for MII generation---gallery search, image space morphing, and representation space inversion---and show that the latter two realise the ideal well enough to produce effective attacks, while the former could succeed but only with an implausibly large gallery to search. Gallery search and inversion MIIs depend on having access to a facial comparator, for optimisation, but our results show that these attacks can still be effective when attacking disparate comparators, thus securing a deployed comparator is an insufficient defence.



### From Zero-Shot Learning to Cold-Start Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1906.08511v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1906.08511v2)
- **Published**: 2019-06-20 09:13:17+00:00
- **Updated**: 2019-06-21 02:52:11+00:00
- **Authors**: Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, Zi Huang
- **Comment**: Accepted to AAAI 2019. Codes are available at
  https://github.com/lijin118/LLAE
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) and cold-start recommendation (CSR) are two challenging problems in computer vision and recommender system, respectively. In general, they are independently investigated in different communities. This paper, however, reveals that ZSL and CSR are two extensions of the same intension. Both of them, for instance, attempt to predict unseen classes and involve two spaces, one for direct feature representation and the other for supplementary description. Yet there is no existing approach which addresses CSR from the ZSL perspective. This work, for the first time, formulates CSR as a ZSL problem, and a tailor-made ZSL method is proposed to handle CSR. Specifically, we propose a Low-rank Linear Auto-Encoder (LLAE), which challenges three cruxes, i.e., domain shift, spurious correlations and computing efficiency, in this paper. LLAE consists of two parts, a low-rank encoder maps user behavior into user attributes and a symmetric decoder reconstructs user behavior from user attributes. Extensive experiments on both ZSL and CSR tasks verify that the proposed method is a win-win formulation, i.e., not only can CSR be handled by ZSL models with a significant performance improvement compared with several conventional state-of-the-art methods, but the consideration of CSR can benefit ZSL as well.



### Deep Eyedentification: Biometric Identification using Micro-Movements of the Eye
- **Arxiv ID**: http://arxiv.org/abs/1906.11889v5
- **DOI**: 10.1007/978-3-030-46147-8_18
- **Categories**: **cs.CV**, cs.CL, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11889v5)
- **Published**: 2019-06-20 10:36:40+00:00
- **Updated**: 2020-05-05 08:30:44+00:00
- **Authors**: Lena A. Jäger, Silvia Makowski, Paul Prasse, Sascha Liehr, Maximilian Seidler, Tobias Scheffer
- **Comment**: None
- **Journal**: In: U. Brefeld et al. (Eds.): Machine Learning and Knowledge
  Discovery in Databases, ECML PKDD 2019, LNCS 11907, pp. 299-314, Springer
  Nature, Switzerland, 2020
- **Summary**: We study involuntary micro-movements of the eye for biometric identification. While prior studies extract lower-frequency macro-movements from the output of video-based eye-tracking systems and engineer explicit features of these macro-movements, we develop a deep convolutional architecture that processes the raw eye-tracking signal. Compared to prior work, the network attains a lower error rate by one order of magnitude and is faster by two orders of magnitude: it identifies users accurately within seconds.



### vireoJD-MM at Activity Detection in Extended Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.08547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08547v1)
- **Published**: 2019-06-20 10:43:00+00:00
- **Updated**: 2019-06-20 10:43:00+00:00
- **Authors**: Fuchen Long, Qi Cai, Zhaofan Qiu, Zhijian Hou, Yingwei Pan, Ting Yao, Chong-Wah Ngo
- **Comment**: None
- **Journal**: None
- **Summary**: This notebook paper presents an overview and comparative analysis of our system designed for activity detection in extended videos (ActEV-PC) in ActivityNet Challenge 2019. Specifically, we exploit person/vehicle detections in spatial level and action localization in temporal level for action detection in surveillance videos. The mechanism of different tubelet generation and model decomposition methods are studied as well. The detection results are finally predicted by late fusing the results from each component.



### On Physical Adversarial Patches for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.11897v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11897v1)
- **Published**: 2019-06-20 11:04:57+00:00
- **Updated**: 2019-06-20 11:04:57+00:00
- **Authors**: Mark Lee, Zico Kolter
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we demonstrate a physical adversarial patch attack against object detectors, notably the YOLOv3 detector. Unlike previous work on physical object detection attacks, which required the patch to overlap with the objects being misclassified or avoiding detection, we show that a properly designed patch can suppress virtually all the detected objects in the image. That is, we can place the patch anywhere in the image, causing all existing objects in the image to be missed entirely by the detector, even those far away from the patch itself. This in turn opens up new lines of physical attacks against object detection systems, which require no modification of the objects in a scene. A demo of the system can be found at https://youtu.be/WXnQjbZ1e7Y.



### Effective degrees of freedom for surface finish defect detection and classification
- **Arxiv ID**: http://arxiv.org/abs/1906.11904v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11904v1)
- **Published**: 2019-06-20 11:13:52+00:00
- **Updated**: 2019-06-20 11:13:52+00:00
- **Authors**: Natalya Pya Arnqvist, Blaise Ngendangenzwa, Eric Lindahl, Leif Nilsson, Jun Yu
- **Comment**: 17 pages, 12 figures, 3 tables
- **Journal**: None
- **Summary**: One of the primary concerns of product quality control in the automotive industry is an automated detection of defects of small sizes on specular car body surfaces. A new statistical learning approach is presented for surface finish defect detection based on spline smoothing method for feature extraction and $k$-nearest neighbour probabilistic classifier. Since the surfaces are specular, structured lightning reflection technique is applied for image acquisition. Reduced rank cubic regression splines are used to smooth the pixel values while the effective degrees of freedom of the obtained smooths serve as components of the feature vector. A key advantage of the approach is that it allows reaching near zero misclassification error rate when applying standard learning classifiers. We also propose probability based performance evaluation metrics as alternatives to the conventional metrics. The usage of those provides the means for uncertainty estimation of the predictive performance of a classifier. Experimental classification results on the images obtained from the pilot system located at Volvo GTO Cab plant in Ume{\aa}, Sweden, show that the proposed approach is much more efficient than the compared methods.



### Pattern Spotting in Historical Documents Using Convolutional Models
- **Arxiv ID**: http://arxiv.org/abs/1906.08580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08580v1)
- **Published**: 2019-06-20 12:39:35+00:00
- **Updated**: 2019-06-20 12:39:35+00:00
- **Authors**: Ignacio Úbeda, Jose M. Saavedra, Stéphane Nicolas, Caroline Petitjean, Laurent Heutte
- **Comment**: 6 pages, 9 figures
- **Journal**: None
- **Summary**: Pattern spotting consists of searching in a collection of historical document images for occurrences of a graphical object using an image query. Contrary to object detection, no prior information nor predefined class is given about the query so training a model of the object is not feasible. In this paper, a convolutional neural network approach is proposed to tackle this problem. We use RetinaNet as a feature extractor to obtain multiscale embeddings of the regions of the documents and also for the queries. Experiments conducted on the DocExplore dataset show that our proposal is better at locating patterns and requires less storage for indexing images than the state-of-the-art system, but fails at retrieving some pages containing multiple instances of the query.



### BGrowth: an efficient approach for the segmentation of vertebral compression fractures in magnetic resonance imaging
- **Arxiv ID**: http://arxiv.org/abs/1906.08620v2
- **DOI**: 10.1145/3297280.3299728
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08620v2)
- **Published**: 2019-06-20 13:51:27+00:00
- **Updated**: 2019-06-25 01:17:16+00:00
- **Authors**: Jonathan S. Ramos, Carolina Y. V. Watanabe, Marcello H. Nogueira-Barbosa, Agma J. M. Traina
- **Comment**: This is a pre-print of an article published in Symposium on Applied
  Computing. The final authenticated version is available online at
  https://doi.org/10.1145/3297280.3299728
- **Journal**: The 34th ACM/SIGAPP Symposium on Applied Computing (SAC2019)
- **Summary**: Segmentation of medical images is a critical issue: several process of analysis and classification rely on this segmentation. With the growing number of people presenting back pain and problems related to it, the automatic or semi-automatic segmentation of fractured vertebral bodies became a challenging task. In general, those fractures present several regions with non-homogeneous intensities and the dark regions are quite similar to the structures nearby. Aimed at overriding this challenge, in this paper we present a semi-automatic segmentation method, called Balanced Growth (BGrowth). The experimental results on a dataset with 102 crushed and 89 normal vertebrae show that our approach significantly outperforms well-known methods from the literature. We have achieved an accuracy up to 95% while keeping acceptable processing time performance, that is equivalent to the state-of-the-artmethods. Moreover, BGrowth presents the best results even with a rough (sloppy) manual annotation (seed points).



### 3D Instance Segmentation via Multi-Task Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.08650v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08650v2)
- **Published**: 2019-06-20 14:14:16+00:00
- **Updated**: 2019-11-01 01:07:14+00:00
- **Authors**: Jean Lahoud, Bernard Ghanem, Marc Pollefeys, Martin R. Oswald
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for instance label segmentation of dense 3D voxel grids. We target volumetric scene representations, which have been acquired with depth sensors or multi-view stereo methods and which have been processed with semantic 3D reconstruction or scene completion methods. The main task is to learn shape information about individual object instances in order to accurately separate them, including connected and incompletely scanned objects. We solve the 3D instance-labeling problem with a multi-task learning strategy. The first goal is to learn an abstract feature embedding, which groups voxels with the same instance label close to each other while separating clusters with different instance labels from each other. The second goal is to learn instance information by densely estimating directional information of the instance's center of mass for each voxel. This is particularly useful to find instance boundaries in the clustering post-processing step, as well as, for scoring the segmentation quality for the first goal. Both synthetic and real-world experiments demonstrate the viability and merits of our approach. In fact, it achieves state-of-the-art performance on the ScanNet 3D instance segmentation benchmark.



### Homogeneous Vector Capsules Enable Adaptive Gradient Descent in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08676v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.08676v2)
- **Published**: 2019-06-20 14:54:14+00:00
- **Updated**: 2021-01-27 01:55:50+00:00
- **Authors**: Adam Byerly, Tatiana Kalganova
- **Comment**: 14 pages, 5 figures, 7 tables
- **Journal**: None
- **Summary**: Capsules are the name given by Geoffrey Hinton to vector-valued neurons. Neural networks traditionally produce a scalar value for an activated neuron. Capsules, on the other hand, produce a vector of values, which Hinton argues correspond to a single, composite feature wherein the values of the components of the vectors indicate properties of the feature such as transformation or contrast. We present a new way of parameterizing and training capsules that we refer to as homogeneous vector capsules (HVCs). We demonstrate, experimentally, that altering a convolutional neural network (CNN) to use HVCs can achieve superior classification accuracy without increasing the number of parameters or operations in its architecture as compared to a CNN using a single final fully connected layer. Additionally, the introduction of HVCs enables the use of adaptive gradient descent, reducing the dependence a model's achievable accuracy has on the finely tuned hyperparameters of a non-adaptive optimizer. We demonstrate our method and results using two neural network architectures. First, a very simple monolithic CNN that when using HVCs achieved a 63% improvement in top-1 classification accuracy and a 35% improvement in top-5 classification accuracy over the baseline architecture. Second, with the CNN architecture referred to as Inception v3 that achieved similar accuracies both with and without HVCs. Additionally, the simple monolithic CNN when using HVCs showed no overfitting after more than 300 epochs whereas the baseline showed overfitting after 30 epochs. We use the ImageNet ILSVRC 2012 classification challenge dataset with both networks.



### The Limited Multi-Label Projection Layer
- **Arxiv ID**: http://arxiv.org/abs/1906.08707v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08707v3)
- **Published**: 2019-06-20 15:51:24+00:00
- **Updated**: 2019-10-14 17:53:34+00:00
- **Authors**: Brandon Amos, Vladlen Koltun, J. Zico Kolter
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the Limited Multi-Label (LML) projection layer as a new primitive operation for end-to-end learning systems. The LML layer provides a probabilistic way of modeling multi-label predictions limited to having exactly k labels. We derive efficient forward and backward passes for this layer and show how the layer can be used to optimize the top-k recall for multi-label tasks with incomplete label information. We evaluate LML layers on top-k CIFAR-100 classification and scene graph generation. We show that LML layers add a negligible amount of computational overhead, strictly improve the model's representational capacity, and improve accuracy. We also revisit the truncated top-k entropy method as a competitive baseline for top-k classification.



### Reversible Privacy Preservation using Multi-level Encryption and Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1906.08713v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1906.08713v1)
- **Published**: 2019-06-20 15:58:53+00:00
- **Updated**: 2019-06-20 15:58:53+00:00
- **Authors**: Mehmet Yamac, Mete Ahishali, Nikolaos Passalis, Jenni Raitoharju, Bulent Sankur, Moncef Gabbouj
- **Comment**: 5 pages, submitted/accepted, EUSIPCO 2019
- **Journal**: None
- **Summary**: Security monitoring via ubiquitous cameras and their more extended in intelligent buildings stand to gain from advances in signal processing and machine learning. While these innovative and ground-breaking applications can be considered as a boon, at the same time they raise significant privacy concerns. In fact, recent GDPR (General Data Protection Regulation) legislation has highlighted and become an incentive for privacy-preserving solutions. Typical privacy-preserving video monitoring schemes address these concerns by either anonymizing the sensitive data. However, these approaches suffer from some limitations, since they are usually non-reversible, do not provide multiple levels of decryption and computationally costly. In this paper, we provide a novel privacy-preserving method, which is reversible, supports de-identification at multiple privacy levels, and can efficiently perform data acquisition, encryption and data hiding by combining multi-level encryption with compressive sensing. The effectiveness of the proposed approach in protecting the identity of the users has been validated using the goodness of reconstruction quality and strong anonymization of the faces.



### Clustering and Classification Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08714v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08714v1)
- **Published**: 2019-06-20 15:59:22+00:00
- **Updated**: 2019-06-20 15:59:22+00:00
- **Authors**: Jin-mo Choi
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: In this paper, we will describe a network architecture that demonstrates high performance on various sizes of datasets. To do this, we will perform an architecture search by dividing the fully connected layer into three levels in the existing network architecture. The first step is to learn existing CNN layer and existing fully connected layer for 1 epoch. The second step is clustering similar classes by applying L1 distance to the result of Softmax. The third step is to reclassify using clustering class masks. We accomplished the result of state-of-the-art by performing the above three steps sequentially or recursively. The technology recorded an error of 11.56% on Cifar-100.



### Deep-Learning-Based Aerial Image Classification for Emergency Response Applications Using Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1906.08716v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.08716v1)
- **Published**: 2019-06-20 16:03:32+00:00
- **Updated**: 2019-06-20 16:03:32+00:00
- **Authors**: Christos Kyrkou, Theocharis Theocharides
- **Comment**: CVPR International Workshop on Computer Vision for UAVs
  (UAVision2019), 16 June 2019
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs), equipped with camera sensors can facilitate enhanced situational awareness for many emergency response and disaster management applications since they are capable of operating in remote and difficult to access areas. In addition, by utilizing an embedded platform and deep learning UAVs can autonomously monitor a disaster stricken area, analyze the image in real-time and alert in the presence of various calamities such as collapsed buildings, flood, or fire in order to faster mitigate their effects on the environment and on human population. To this end, this paper focuses on the automated aerial scene classification of disaster events from on-board a UAV. Specifically, a dedicated Aerial Image Database for Emergency Response (AIDER) applications is introduced and a comparative analysis of existing approaches is performed. Through this analysis a lightweight convolutional neural network (CNN) architecture is developed, capable of running efficiently on an embedded platform achieving ~3x higher performance compared to existing models with minimal memory requirements with less than 2% accuracy drop compared to the state-of-the-art. These preliminary results provide a solid basis for further experimentation towards real-time aerial image classification for emergency response applications using UAVs.



### We Need No Pixels: Video Manipulation Detection Using Stream Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1906.08743v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08743v1)
- **Published**: 2019-06-20 16:42:06+00:00
- **Updated**: 2019-06-20 16:42:06+00:00
- **Authors**: David Güera, Sriram Baireddy, Paolo Bestagini, Stefano Tubaro, Edward J. Delp
- **Comment**: 7 pages, 6 figures, presented at the ICML 2019 Worksop on Synthetic
  Realities: Deep Learning for Detecting AudioVisual Fakes
- **Journal**: None
- **Summary**: Manipulating video content is easier than ever. Due to the misuse potential of manipulated content, multiple detection techniques that analyze the pixel data from the videos have been proposed. However, clever manipulators should also carefully forge the metadata and auxiliary header information, which is harder to do for videos than images. In this paper, we propose to identify forged videos by analyzing their multimedia stream descriptors with simple binary classifiers, completely avoiding the pixel space. Using well-known datasets, our results show that this scalable approach can achieve a high manipulation detection score if the manipulators have not done a careful data sanitization of the multimedia stream descriptors.



### Let's Take This Online: Adapting Scene Coordinate Regression Network Predictions for Online RGB-D Camera Relocalisation
- **Arxiv ID**: http://arxiv.org/abs/1906.08744v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.08744v1)
- **Published**: 2019-06-20 16:44:16+00:00
- **Updated**: 2019-06-20 16:44:16+00:00
- **Authors**: Tommaso Cavallari, Luca Bertinetto, Jishnu Mukhoti, Philip Torr, Stuart Golodetz
- **Comment**: Tommaso Cavallari and Stuart Golodetz contributed equally to this
  paper
- **Journal**: None
- **Summary**: Many applications require a camera to be relocalised online, without expensive offline training on the target scene. Whilst both keyframe and sparse keypoint matching methods can be used online, the former often fail away from the training trajectory, and the latter can struggle in textureless regions. By contrast, scene coordinate regression (SCoRe) methods generalise to novel poses and can leverage dense correspondences to improve robustness, and recent work has shown how to adapt SCoRe forests between scenes, allowing their state-of-the-art performance to be leveraged online. However, because they use features hand-crafted for indoor use, they do not generalise well to harder outdoor scenes. Whilst replacing the forest with a neural network and learning suitable features for outdoor use is possible, the techniques used to adapt forests between scenes are unfortunately harder to transfer to a network context. In this paper, we address this by proposing a novel way of leveraging a network trained on one scene to predict points in another scene. Our approach replaces the appearance clustering performed by the branching structure of a regression forest with a two-step process that first uses the network to predict points in the original scene, and then uses these predicted points to look up clusters of points from the new scene. We show experimentally that our online approach achieves state-of-the-art performance on both the 7-Scenes and Cambridge Landmarks datasets, whilst running in under 300ms, making it highly effective in live scenarios.



### Progressive Gradient Pruning for Classification, Detection and DomainAdaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.08746v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08746v4)
- **Published**: 2019-06-20 16:46:16+00:00
- **Updated**: 2020-02-25 18:42:05+00:00
- **Authors**: Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Louis-Antoine Blais-Morin, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: Although deep neural networks (NNs) have achievedstate-of-the-art accuracy in many visual recognition tasks,the growing computational complexity and energy con-sumption of networks remains an issue, especially for ap-plications on platforms with limited resources and requir-ing real-time processing. Filter pruning techniques haverecently shown promising results for the compression andacceleration of convolutional NNs (CNNs). However, thesetechniques involve numerous steps and complex optimisa-tions because some only prune after training CNNs, whileothers prune from scratch during training by integratingsparsity constraints or modifying the loss function.In this paper we propose a new Progressive GradientPruning (PGP) technique for iterative filter pruning dur-ing training. In contrast to previous progressive pruningtechniques, it relies on a novel filter selection criterion thatmeasures the change in filter weights, uses a new hard andsoft pruning strategy and effectively adapts momentum ten-sors during the backward propagation pass. Experimentalresults obtained after training various CNNs on image datafor classification, object detection and domain adaptationbenchmarks indicate that the PGP technique can achievea better trade-off between classification accuracy and net-work (time and memory) complexity than PSFP and otherstate-of-the-art filter pruning techniques.



### Learning the Sampling Pattern for MRI
- **Arxiv ID**: http://arxiv.org/abs/1906.08754v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1906.08754v2)
- **Published**: 2019-06-20 17:13:45+00:00
- **Updated**: 2020-06-22 00:03:35+00:00
- **Authors**: Ferdia Sherry, Martin Benning, Juan Carlos De los Reyes, Martin J. Graves, Georg Maierhofer, Guy Williams, Carola-Bibiane Schönlieb, Matthias J. Ehrhardt
- **Comment**: The main document is 12 pages, the supporting document is 2 pages and
  attached at the end of the main document
- **Journal**: None
- **Summary**: The discovery of the theory of compressed sensing brought the realisation that many inverse problems can be solved even when measurements are "incomplete". This is particularly interesting in magnetic resonance imaging (MRI), where long acquisition times can limit its use. In this work, we consider the problem of learning a sparse sampling pattern that can be used to optimally balance acquisition time versus quality of the reconstructed image. We use a supervised learning approach, making the assumption that our training data is representative enough of new data acquisitions. We demonstrate that this is indeed the case, even if the training data consists of just 7 training pairs of measurements and ground-truth images; with a training set of brain images of size 192 by 192, for instance, one of the learned patterns samples only 35% of k-space, however results in reconstructions with mean SSIM 0.914 on a test set of similar images. The proposed framework is general enough to learn arbitrary sampling patterns, including common patterns such as Cartesian, spiral and radial sampling.



### Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors
- **Arxiv ID**: http://arxiv.org/abs/1906.08763v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08763v2)
- **Published**: 2019-06-20 17:34:49+00:00
- **Updated**: 2020-03-27 18:29:58+00:00
- **Authors**: Gauri Jagatap, Chinmay Hegde
- **Comment**: NeurIPS 2019 version with few modifications
- **Journal**: NeurIPS 2019
- **Summary**: Deep neural networks as image priors have been recently introduced for problems such as denoising, super-resolution and inpainting with promising performance gains over hand-crafted image priors such as sparsity and low-rank. Unlike learned generative priors they do not require any training over large datasets. However, few theoretical guarantees exist in the scope of using untrained neural network priors for inverse imaging problems. We explore new applications and theory for untrained neural network priors. Specifically, we consider the problem of solving linear inverse problems, such as compressive sensing, as well as non-linear problems, such as compressive phase retrieval. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present a projected gradient descent scheme that can be used for both compressive sensing and phase retrieval and provide rigorous theoretical guarantees for its convergence. We also show both theoretically as well as empirically that with deep network priors, one can achieve better compression rates for the same image quality compared to hand crafted priors.



### Learning to segment microscopy images with lazy labels
- **Arxiv ID**: http://arxiv.org/abs/1906.12177v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.12177v2)
- **Published**: 2019-06-20 17:38:54+00:00
- **Updated**: 2020-09-10 10:08:44+00:00
- **Authors**: Rihuan Ke, Aurélie Bugeau, Nicolas Papadakis, Peter Schuetz, Carola-Bibiane Schönlieb
- **Comment**: None
- **Journal**: None
- **Summary**: The need for labour intensive pixel-wise annotation is a major limitation of many fully supervised learning methods for segmenting bioimages that can contain numerous object instances with thin separations. In this paper, we introduce a deep convolutional neural network for microscopy image segmentation. Annotation issues are circumvented by letting the network being trainable on coarse labels combined with only a very small number of images with pixel-wise annotations. We call this new labelling strategy `lazy' labels. Image segmentation is stratified into three connected tasks: rough inner region detection, object separation and pixel-wise segmentation. These tasks are learned in an end-to-end multi-task learning framework. The method is demonstrated on two microscopy datasets, where we show that the model gives accurate segmentation results even if exact boundary labels are missing for a majority of annotated data. It brings more flexibility and efficiency for training deep neural networks that are data hungry and is applicable to biomedical images with poor contrast at the object boundaries or with diverse textures and repeated patterns.



### Understanding More about Human and Machine Attention in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08764v3
- **DOI**: None
- **Categories**: **cs.CV**, 62H35
- **Links**: [PDF](http://arxiv.org/pdf/1906.08764v3)
- **Published**: 2019-06-20 17:41:57+00:00
- **Updated**: 2020-07-06 05:13:58+00:00
- **Authors**: Qiuxia Lai, Salman Khan, Yongwei Nie, Jianbing Shen, Hanqiu Sun, Ling Shao
- **Comment**: Q. Lai, S. Khan, Y. Nie, J. Shen, H. Sun, L. Shao, TMM, in press,
  2020
- **Journal**: None
- **Summary**: Human visual system can selectively attend to parts of a scene for quick perception, a biological mechanism known as Human attention. Inspired by this, recent deep learning models encode attention mechanisms to focus on the most task-relevant parts of the input signal for further processing, which is called Machine/Neural/Artificial attention. Understanding the relation between human and machine attention is important for interpreting and designing neural networks. Many works claim that the attention mechanism offers an extra dimension of interpretability by explaining where the neural networks look. However, recent studies demonstrate that artificial attention maps do not always coincide with common intuition. In view of these conflicting evidence, here we make a systematic study on using artificial attention and human attention in neural network design. With three example computer vision tasks, diverse representative backbones, and famous architectures, corresponding real human gaze data, and systematically conducted large-scale quantitative studies, we quantify the consistency between artificial attention and human visual attention and offer novel insights into existing artificial attention mechanisms by giving preliminary answers to several key questions related to human and artificial attention mechanisms. Overall results demonstrate that human attention can benchmark the meaningful `ground-truth' in attention-driven tasks, where the more the artificial attention is close to human attention, the better the performance; for higher-level vision tasks, it is case-by-case. It would be advisable for attention-driven tasks to explicitly force a better alignment between artificial and human attention to boost the performance; such alignment would also improve the network explainability for higher-level computer vision tasks.



### Automated crater shape retrieval using weakly-supervised deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.08826v3
- **DOI**: 10.1016/j.icarus.2020.113749
- **Categories**: **astro-ph.EP**, astro-ph.IM, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.08826v3)
- **Published**: 2019-06-20 20:04:08+00:00
- **Updated**: 2020-03-11 16:34:45+00:00
- **Authors**: Mohamad Ali-Dib, Kristen Menou, Alan P. Jackson, Chenchong Zhu, Noah Hammond
- **Comment**: 59 pages, 13 figures, Accepted for publication in Icarus
- **Journal**: None
- **Summary**: Crater ellipticity determination is a complex and time consuming task that so far has evaded successful automation. We train a state of the art computer vision algorithm to identify craters in Lunar digital elevation maps and retrieve their sizes and 2D shapes. The computational backbone of the model is MaskRCNN, an "instance segmentation" general framework that detects craters in an image while simultaneously producing a mask for each crater that traces its outer rim. Our post-processing pipeline then finds the closest fitting ellipse to these masks, allowing us to retrieve the crater ellipticities. Our model is able to correctly identify 87\% of known craters in the longitude range we hid from the network during training and validation (test set), while predicting thousands of additional craters not present in our training data. Manual validation of a subset of these "new" craters indicates that a majority of them are real, which we take as an indicator of the strength of our model in learning to identify craters, despite incomplete training data. The crater size, ellipticity, and depth distributions predicted by our model are consistent with human-generated results. The model allows us to perform a large scale search for differences in crater diameter and shape distributions between the lunar highlands and maria, and we exclude any such differences with a high statistical significance. The predicted test set catalogue and trained model are available here: https://github.com/malidib/Craters_MaskRCNN/.



### Informative Image Captioning with External Sources of Information
- **Arxiv ID**: http://arxiv.org/abs/1906.08876v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08876v1)
- **Published**: 2019-06-20 21:51:48+00:00
- **Updated**: 2019-06-20 21:51:48+00:00
- **Authors**: Sanqiang Zhao, Piyush Sharma, Tomer Levinboim, Radu Soricut
- **Comment**: None
- **Journal**: None
- **Summary**: An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important "informativeness" dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.



### SGANVO: Unsupervised Deep Visual Odometry and Depth Estimation with Stacked Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08889v1
- **DOI**: 10.1109/LRA.2019.2925555
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08889v1)
- **Published**: 2019-06-20 22:51:10+00:00
- **Updated**: 2019-06-20 22:51:10+00:00
- **Authors**: Tuo Feng, Dongbing Gu
- **Comment**: 7 pages, 4 figures,
- **Journal**: None
- **Summary**: Recently end-to-end unsupervised deep learning methods have achieved an effect beyond geometric methods for visual depth and ego-motion estimation tasks. These data-based learning methods perform more robustly and accurately in some of the challenging scenes. The encoder-decoder network has been widely used in the depth estimation and the RCNN has brought significant improvements in the ego-motion estimation. Furthermore, the latest use of Generative Adversarial Nets(GANs) in depth and ego-motion estimation has demonstrated that the estimation could be further improved by generating pictures in the game learning process. This paper proposes a novel unsupervised network system for visual depth and ego-motion estimation: Stacked Generative Adversarial Network(SGANVO). It consists of a stack of GAN layers, of which the lowest layer estimates the depth and ego-motion while the higher layers estimate the spatial features. It can also capture the temporal dynamic due to the use of a recurrent representation across the layers. See Fig.1 for details. We select the most commonly used KITTI [1] data set for evaluation. The evaluation results show that our proposed method can produce better or comparable results in depth and ego-motion estimation.



### Predicting Future Opioid Incidences Today
- **Arxiv ID**: http://arxiv.org/abs/1906.08891v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08891v1)
- **Published**: 2019-06-20 22:53:18+00:00
- **Updated**: 2019-06-20 22:53:18+00:00
- **Authors**: Sandipan Choudhuri, Kaustav Basu, Kevin Thomas, Arunabha Sen
- **Comment**: None
- **Journal**: None
- **Summary**: According to the Center of Disease Control (CDC), the Opioid epidemic has claimed more than 72,000 lives in the US in 2017 alone. In spite of various efforts at the local, state and federal level, the impact of the epidemic is becoming progressively worse, as evidenced by the fact that the number of Opioid related deaths increased by 12.5\% between 2016 and 2017. Predictive analytics can play an important role in combating the epidemic by providing decision making tools to stakeholders at multiple levels - from health care professionals to policy makers to first responders. Generating Opioid incidence heat maps from past data, aid these stakeholders to visualize the profound impact of the Opioid epidemic. Such post-fact creation of the heat map provides only retrospective information, and as a result, may not be as useful for preventive action in the current or future time-frames. In this paper, we present a novel deep neural architecture, which learns subtle spatio-temporal variations in Opioid incidences data and accurately predicts future heat maps. We evaluated the efficacy of our model on two open source datasets- (i) The Cincinnati Heroin Overdose dataset, and (ii) Connecticut Drug Related Death Dataset.



