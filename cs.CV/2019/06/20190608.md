# Arxiv Papers in cs.CV on 2019-06-08
### TransNet: A deep network for fast detection of common shot transitions
- **Arxiv ID**: http://arxiv.org/abs/1906.03363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03363v1)
- **Published**: 2019-06-08 00:45:18+00:00
- **Updated**: 2019-06-08 00:45:18+00:00
- **Authors**: Tomáš Souček, Jaroslav Moravec, Jakub Lokoč
- **Comment**: None
- **Journal**: None
- **Summary**: Shot boundary detection (SBD) is an important first step in many video processing applications. This paper presents a simple modular convolutional neural network architecture that achieves state-of-the-art results on the RAI dataset with well above real-time inference speed even on a single mediocre GPU. The network employs dilated convolutions and operates just on small resized frames. The training process employed randomly generated transitions using selected shots from the TRECVID IACC.3 dataset. The code and a selected trained network will be available at https://github.com/soCzech/TransNet.



### Global Semantic Description of Objects based on Prototype Theory
- **Arxiv ID**: http://arxiv.org/abs/1906.03365v4
- **DOI**: 10.1016/j.imavis.2021.104249
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.5.1
- **Links**: [PDF](http://arxiv.org/pdf/1906.03365v4)
- **Published**: 2019-06-08 01:02:02+00:00
- **Updated**: 2021-06-20 01:22:02+00:00
- **Authors**: Omar Vidal Pino, Erickson Rangel Nascimento, Mario Fernando Montenegro Campos
- **Comment**: Content: 24 pages (22 + 2 reference) with 15 Figures and 3 Tables. In
  the future, a new version will be updated with other experiments and results
  (and a journal reference if applicable)
- **Journal**: None
- **Summary**: In this paper, we introduce a novel semantic description approach inspired on Prototype Theory foundations. We propose a Computational Prototype Model (CPM) that encodes and stores the central semantic meaning of objects category: the semantic prototype. Also, we introduce a Prototype-based Description Model that encodes the semantic meaning of an object while describing its features using our CPM model. Our description method uses semantic prototypes computed by CNN-classifications models to create discriminative signatures that describe an object highlighting its most distinctive features within the category. Our experiments show that: i) our CPM model (semantic prototype + distance metric) is able to describe the internal semantic structure of objects categories; ii) our semantic distance metric can be understood as the object visual typicality score within a category; iii) our descriptor encoding is semantically interpretable and significantly outperforms other image global encodings in clustering and classification tasks.



### S-ConvNet: A Shallow Convolutional Neural Network Architecture for Neuromuscular Activity Recognition Using Instantaneous High-Density Surface EMG Images
- **Arxiv ID**: http://arxiv.org/abs/1906.03381v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.AI, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03381v1)
- **Published**: 2019-06-08 03:32:57+00:00
- **Updated**: 2019-06-08 03:32:57+00:00
- **Authors**: Md. Rabiul Islam, Daniel Massicotte, Francois Nougarou, Philippe Massicotte, Wei-Ping Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: The concept of neuromuscular activity recognition using instantaneous high-density surface electromyography (HD-sEMG) images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the existing approaches employed a very large deep convolutional neural network (ConvNet) architecture and complex training schemes for HD-sEMG image recognition, which requires the network architecture to be pre-trained on a very large-scale labeled training dataset, as a result, it makes computationally very expensive. To overcome this problem, we propose S-ConvNet and All-ConvNet models, a simple yet efficient framework for learning instantaneous HD-sEMG images from scratch for neuromuscular activity recognition. Without using any pre-trained models, our proposed S-ConvNet and All-ConvNet demonstrate very competitive recognition accuracy to the more complex state of the art for neuromuscular activity recognition based on instantaneous HD-sEMG images, while using a ~ 12 x smaller dataset and reducing learning parameters to a large extent. The experimental results proved that the S-ConvNet and All-ConvNet are highly effective for learning discriminative features for instantaneous HD-sEMG image recognition especially in the data and high-end resource constrained scenarios.



### A New Compensatory Genetic Algorithm-Based Method for Effective Compressed Multi-function Convolutional Neural Network Model Selection with Multi-Objective Optimization
- **Arxiv ID**: http://arxiv.org/abs/1906.11912v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11912v1)
- **Published**: 2019-06-08 04:09:19+00:00
- **Updated**: 2019-06-08 04:09:19+00:00
- **Authors**: Luna M. Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, there have been many popular Convolutional Neural Networks (CNNs), such as Google's Inception-V4, that have performed very well for various image classification problems. These commonly used CNN models usually use the same activation function, such as RELU, for all neurons in the convolutional layers; they are "Single-function CNNs." However, SCNNs may not always be optimal. Thus, a "Multi-function CNN" (MCNN), which uses different activation functions for different neurons, has been shown to outperform a SCNN. Also, CNNs typically have very large architectures that use a lot of memory and need a lot of data in order to be trained well. As a result, they tend to have very high training and prediction times too. An important research problem is how to automatically and efficiently find the best CNN with both high classification performance and compact architecture with high training and prediction speeds, small power usage, and small memory size for any image classification problem. It is very useful to intelligently find an effective, fast, energy-efficient, and memory-efficient "Compressed Multi-function CNN" (CMCNN) from a large number of candidate MCNNs. A new compensatory algorithm using a new genetic algorithm (GA) is created to find the best CMCNN with an ideal compensation between performance and architecture size. The optimal CMCNN has the best performance and the smallest architecture size. Simulations using the CIFAR10 dataset showed that the new compensatory algorithm could find CMCNNs that could outperform non-compressed MCNNs in terms of classification performance (F1-score), speed, power usage, and memory usage. Other effective, fast, power-efficient, and memory-efficient CMCNNs based on popular CNN architectures will be developed for image classification problems in important real-world applications, such as brain informatics and biomedical imaging.



### A Coarse-to-Fine Framework for Learned Color Enhancement with Non-Local Attention
- **Arxiv ID**: http://arxiv.org/abs/1906.03404v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03404v2)
- **Published**: 2019-06-08 07:16:01+00:00
- **Updated**: 2019-07-20 08:15:10+00:00
- **Authors**: Chaowei Shan, Zhizheng Zhang, Zhibo Chen
- **Comment**: To appear in ICIP19
- **Journal**: None
- **Summary**: Automatic color enhancement is aimed to adaptively adjust photos to expected styles and tones. For current learned methods in this field, global harmonious perception and local details are hard to be well-considered in a single model simultaneously. To address this problem, we propose a coarse-to-fine framework with non-local attention for color enhancement in this paper. Within our framework, we propose to divide enhancement process into channel-wise enhancement and pixel-wise refinement performed by two cascaded Convolutional Neural Networks (CNNs). In channel-wise enhancement, our model predicts a global linear mapping for RGB channels of input images to perform global style adjustment. In pixel-wise refinement, we learn a refining mapping using residual learning for local adjustment. Further, we adopt a non-local attention block to capture the long-range dependencies from global information for subsequent fine-grained local refinement. We evaluate our proposed framework on the commonly using benchmark and conduct sufficient experiments to demonstrate each technical component within it.



### Defending Against Universal Attacks Through Selective Feature Regeneration
- **Arxiv ID**: http://arxiv.org/abs/1906.03444v4
- **DOI**: 10.1109/CVPR42600.2020.00079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03444v4)
- **Published**: 2019-06-08 12:18:13+00:00
- **Updated**: 2020-06-11 02:40:33+00:00
- **Authors**: Tejas Borkar, Felix Heide, Lina Karam
- **Comment**: CVPR 2020. Code:
  https://github.com/tsborkar/Selective-feature-regeneration Webpage:
  https://www.cs.princeton.edu/~fheide/SelectiveFeatureRegeneration/
- **Journal**: CVPR 2020
- **Summary**: Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Specifically, image-agnostic (universal adversarial) perturbations added to any image can fool a target network into making erroneous predictions. Departing from existing defense strategies that work mostly in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal perturbations. Our approach identifies pre-trained convolutional features that are most vulnerable to adversarial noise and deploys trainable feature regeneration units which transform these DNN filter activations into resilient features that are robust to universal perturbations. Regenerating only the top 50% adversarially susceptible activations in at most 6 DNN layers and leaving all remaining DNN activations unchanged, we outperform existing defense strategies across different network architectures by more than 10% in restored accuracy. We show that without any additional modification, our defense trained on ImageNet with one type of universal attack examples effectively defends against other types of unseen universal attacks.



### Strategies to architect AI Safety: Defense to guard AI from Adversaries
- **Arxiv ID**: http://arxiv.org/abs/1906.03466v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CR, cs.CV, I.2.0
- **Links**: [PDF](http://arxiv.org/pdf/1906.03466v1)
- **Published**: 2019-06-08 14:34:47+00:00
- **Updated**: 2019-06-08 14:34:47+00:00
- **Authors**: Rajagopal. A, Nirmala. V
- **Comment**: None
- **Journal**: None
- **Summary**: The impact of designing for security of AI is critical for humanity in the AI era. With humans increasingly becoming dependent upon AI, there is a need for neural networks that work reliably, inspite of Adversarial attacks. The vision for Safe and secure AI for popular use is achievable. To achieve safety of AI, this paper explores strategies and a novel deep learning architecture. To guard AI from adversaries, paper explores combination of 3 strategies:   1. Introduce randomness at inference time to hide the representation learning from adversaries.   2. Detect presence of adversaries by analyzing the sequence of inferences.   3. Exploit visual similarity.   To realize these strategies, this paper designs a novel architecture, Dynamic Neural Defense, DND. This defense has 3 deep learning architectural features:   1. By hiding the way a neural network learns from exploratory attacks using a random computation graph, DND evades attack.   2. By analyzing input sequence to cloud AI inference engine with LSTM, DND detects attack sequence.   3. By inferring with visual similar inputs generated by VAE, any AI defended by DND approach does not succumb to hackers.   Thus, a roadmap to develop reliable, safe and secure AI is presented.



### 3DFPN-HS$^2$: 3D Feature Pyramid Network Based High Sensitivity and Specificity Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.03467v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03467v2)
- **Published**: 2019-06-08 14:35:33+00:00
- **Updated**: 2019-06-11 04:05:12+00:00
- **Authors**: Jingya Liu, Liangliang Cao, Oguz Akin, Yingli Tian
- **Comment**: 8 pages, 3 figures. Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Accurate detection of pulmonary nodules with high sensitivity and specificity is essential for automatic lung cancer diagnosis from CT scans. Although many deep learning-based algorithms make great progress for improving the accuracy of nodule detection, the high false positive rate is still a challenging problem which limited the automatic diagnosis in routine clinical practice. In this paper, we propose a novel pulmonary nodule detection framework based on a 3D Feature Pyramid Network (3DFPN) to improve the sensitivity of nodule detection by employing multi-scale features to increase the resolution of nodules, as well as a parallel top-down path to transit the high-level semantic features to complement low-level general features. Furthermore, a High Sensitivity and Specificity (HS$^2$) network is introduced to eliminate the falsely detected nodule candidates by tracking the appearance changes in continuous CT slices of each nodule candidate. The proposed framework is evaluated on the public Lung Nodule Analysis (LUNA16) challenge dataset. Our method is able to accurately detect lung nodules at high sensitivity and specificity and achieves $90.4\%$ sensitivity with 1/8 false positive per scan which outperforms the state-of-the-art results $15.6\%$.



### Neurogeometry of perception: isotropic and anisotropic aspects
- **Arxiv ID**: http://arxiv.org/abs/1906.03495v1
- **DOI**: 10.1007/s10516-019-09426-1
- **Categories**: **cs.CV**, math.AP
- **Links**: [PDF](http://arxiv.org/pdf/1906.03495v1)
- **Published**: 2019-06-08 18:19:16+00:00
- **Updated**: 2019-06-08 18:19:16+00:00
- **Authors**: Giovanna Citti, Alessandro Sarti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we first recall the definition of geometical model of the visual cortex, focusing in particular on the geometrical properties of horizontal cortical connectivity. Then we recognize that histograms of edges - co-occurrences are not isotropic distributed, and are strongly biased in horizontal and vertical directions of the stimulus. Finally we introduce a new model of non isotropic cortical connectivity modeled on the histogram of edges - co-occurrences. Using this kernel we are able to justify oblique phenomena comparable with experimental findings.



### Attending to Discriminative Certainty for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.03502v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03502v2)
- **Published**: 2019-06-08 19:04:38+00:00
- **Updated**: 2019-09-16 14:51:38+00:00
- **Authors**: Vinod Kumar Kurmi, Shanu Kumar, Vinay P Namboodiri
- **Comment**: CVPR 2019 Accepted, Project: https://delta-lab-iitk.github.io/CADA/
- **Journal**: None
- **Summary**: In this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specify focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.



### Outlier Exposure with Confidence Control for Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.03509v4
- **DOI**: 10.1016/j.neucom.2021.02.007
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03509v4)
- **Published**: 2019-06-08 19:30:24+00:00
- **Updated**: 2021-02-02 22:31:14+00:00
- **Authors**: Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, Jiamian Wang
- **Comment**: Accepted as a Journal paper at Neurocomputing. PyTorch code available
  at https://github.com/nazim1021/OOD-detection-using-OECC
- **Journal**: Neurocomputing 441 (2021) 138-150
- **Summary**: Deep neural networks have achieved great success in classification tasks during the last years. However, one major problem to the path towards artificial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classification algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to efficiently detect out-of-distribution (OOD) examples without compromising much of its classification accuracy on the test examples from known classes. We propose a novel loss function that gives rise to a novel method, Outlier Exposure with Confidence Control (OECC), which achieves superior results in OOD detection with OE both on image and text classification tasks without requiring access to OOD samples. Additionally, we experimentally show that the combination of OECC with state-of-the-art post-training OOD detection methods, like the Mahalanobis Detector (MD) and the Gramian Matrices (GM) methods, further improves their performance in the OOD detection task, demonstrating the potential of combining training and post-training methods for OOD detection.



### DiCENet: Dimension-wise Convolutions for Efficient Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.03516v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03516v3)
- **Published**: 2019-06-08 20:17:06+00:00
- **Updated**: 2020-11-30 06:27:08+00:00
- **Authors**: Sachin Mehta, Hannaneh Hajishirzi, Mohammad Rastegari
- **Comment**: Accepted at IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)
- **Journal**: None
- **Summary**: We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4% higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet. Our source code in PyTorch is open-source and is available at https://github.com/sacmehta/EdgeNets/



### Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.03525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03525v1)
- **Published**: 2019-06-08 21:30:02+00:00
- **Updated**: 2019-06-08 21:30:02+00:00
- **Authors**: Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, Jian Yang
- **Comment**: 10 pages, 9 figures, CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel Pattern-Affinitive Propagation (PAP) framework to jointly predict depth, surface normal and semantic segmentation. The motivation behind it comes from the statistic observation that pattern-affinitive pairs recur much frequently across different tasks as well as within a task. Thus, we can conduct two types of propagations, cross-task propagation and task-specific propagation, to adaptively diffuse those similar patterns. The former integrates cross-task affinity patterns to adapt to each task therein through the calculation on non-local relationships. Next the latter performs an iterative diffusion in the feature space so that the cross-task affinity patterns can be widely-spread within the task. Accordingly, the learning of each task can be regularized and boosted by the complementary task-level affinities. Extensive experiments demonstrate the effectiveness and the superiority of our method on the joint three tasks. Meanwhile, we achieve the state-of-the-art or competitive results on the three related datasets, NYUD-v2, SUN-RGBD and KITTI.



### Structure from Motion for Panorama-Style Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.03539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03539v1)
- **Published**: 2019-06-08 23:29:45+00:00
- **Updated**: 2019-06-08 23:29:45+00:00
- **Authors**: Chris Sweeney, Aleksander Holynski, Brian Curless, Steve M Seitz
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel Structure from Motion pipeline that is capable of reconstructing accurate camera poses for panorama-style video capture without prior camera intrinsic calibration. While panorama-style capture is common and convenient, previous reconstruction methods fail to obtain accurate reconstructions due to the rotation-dominant motion and small baseline between views. Our method is built on the assumption that the camera motion approximately corresponds to motion on a sphere, and we introduce three novel relative pose methods to estimate the fundamental matrix and camera distortion for spherical motion. These solvers are efficient and robust, and provide an excellent initialization for bundle adjustment. A soft prior on the camera poses is used to discourage large deviations from the spherical motion assumption when performing bundle adjustment, which allows cameras to remain properly constrained for optimization in the absence of well-triangulated 3D points. To validate the effectiveness of the proposed method we evaluate our approach on both synthetic and real-world data, and demonstrate that camera poses are accurate enough for multiview stereo.



