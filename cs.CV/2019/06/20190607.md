# Arxiv Papers in cs.CV on 2019-06-07
### Multi-scale self-guided attention for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.02849v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02849v3)
- **Published**: 2019-06-07 00:54:05+00:00
- **Updated**: 2020-02-14 20:23:08+00:00
- **Authors**: Ashish Sinha, Jose Dolz
- **Comment**: None
- **Journal**: None
- **Summary**: Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation, standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder architectures, leads to a redundant use of information, where similar low-level features are extracted multiple times at multiple scales. Second, long-range feature dependencies are not efficiently modeled, resulting in non-optimal discriminative feature representations associated with each semantic class. In this paper we attempt to overcome these limitations with the proposed architecture, by capturing richer contextual dependencies based on the use of guided self-attention mechanisms. This approach is able to integrate local features with their corresponding global dependencies, as well as highlight interdependent channel maps in an adaptive manner. Further, the additional loss between different modules guides the attention mechanisms to neglect irrelevant information and focus on more discriminant regions of the image by emphasizing relevant feature associations. We evaluate the proposed model in the context of semantic segmentation on three different datasets: abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments support the importance of these attention modules in the proposed architecture. In addition, compared to other state-of-the-art segmentation networks our model yields better segmentation performance, increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates the efficiency of our approach to generate precise and reliable automatic segmentations of medical images. Our code is made publicly available at https://github.com/sinAshish/Multi-Scale-Attention



### Figure Captioning with Reasoning and Sequence-Level Training
- **Arxiv ID**: http://arxiv.org/abs/1906.02850v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.02850v1)
- **Published**: 2019-06-07 00:54:53+00:00
- **Updated**: 2019-06-07 00:54:53+00:00
- **Authors**: Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan Rossi, Razvan Bunescu
- **Comment**: None
- **Journal**: None
- **Summary**: Figures, such as bar charts, pie charts, and line plots, are widely used to convey important information in a concise format. They are usually human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning where the goal is to automatically generate a natural language description of the figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. First, we introduce a new dataset for figure captioning, FigCAP, based on FigureQA. Second, we propose two novel attention mechanisms. To achieve accurate generation of labels in figures, we propose Label Maps Attention. To model the relations between figure labels, we propose Relation Maps Attention. Third, we use sequence-level training with reinforcement learning in order to directly optimizes evaluation metrics, which alleviates the exposure bias issue and further improves the models in generating long captions. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for the automatic captioning of vast repositories of figures.



### Recognizing American Sign Language Manual Signs from RGB-D Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.02851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02851v1)
- **Published**: 2019-06-07 00:56:11+00:00
- **Updated**: 2019-06-07 00:56:11+00:00
- **Authors**: Longlong Jing, Elahe Vahdani, Matt Huenerfauth, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a 3D Convolutional Neural Network (3DCNN) based multi-stream framework to recognize American Sign Language (ASL) manual signs (consisting of movements of the hands, as well as non-manual face movements in some cases) in real-time from RGB-D videos, by fusing multimodality features including hand gestures, facial expressions, and body poses from multi-channels (RGB, depth, motion, and skeleton joints). To learn the overall temporal dynamics in a video, a proxy video is generated by selecting a subset of frames for each video which are then used to train the proposed 3DCNN model. We collect a new ASL dataset, ASL-100-RGBD, which contains 42 RGB-D videos captured by a Microsoft Kinect V2 camera, each of 100 ASL manual signs, including RGB channel, depth maps, skeleton joints, face features, and HDface. The dataset is fully annotated for each semantic region (i.e. the time duration of each word that the human signer performs). Our proposed method achieves 92.88 accuracy for recognizing 100 ASL words in our newly collected ASL-100-RGBD dataset. The effectiveness of our framework for recognizing hand gestures from RGB-D videos is further demonstrated on the Chalearn IsoGD dataset and achieves 76 accuracy which is 5.51 higher than the state-of-the-art work in terms of average fusion by using only 5 channels instead of 12 channels in the previous work.



### Does Generative Face Completion Help Face Recognition?
- **Arxiv ID**: http://arxiv.org/abs/1906.02858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02858v1)
- **Published**: 2019-06-07 01:48:28+00:00
- **Updated**: 2019-06-07 01:48:28+00:00
- **Authors**: Joe Mathai, Iacopo Masi, Wael AbdAlmageed
- **Comment**: In Proceedings Of IAPR International Conference On Biometrics 2019
  (ICB'19)
- **Journal**: None
- **Summary**: Face occlusions, covering either the majority or discriminative parts of the face, can break facial perception and produce a drastic loss of information. Biometric systems such as recent deep face recognition models are not immune to obstructions or other objects covering parts of the face. While most of the current face recognition methods are not optimized to handle occlusions, there have been a few attempts to improve robustness directly in the training stage. Unlike those, we propose to study the effect of generative face completion on the recognition. We offer a face completion encoder-decoder, based on a convolutional operator with a gating mechanism, trained with an ample set of face occlusions. To systematically evaluate the impact of realistic occlusions on recognition, we propose to play the occlusion game: we render 3D objects onto different face parts, providing precious knowledge of what the impact is of effectively removing those occlusions. Extensive experiments on the Labeled Faces in the Wild (LFW), and its more difficult variant LFW-BLUFR, testify that face completion is able to partially restore face perception in machine vision systems for improved recognition.



### Risky Action Recognition in Lane Change Video Clips using Deep Spatiotemporal Networks with Segmentation Mask Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.02859v2
- **DOI**: 10.1109/ITSC.2019.8917362
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02859v2)
- **Published**: 2019-06-07 01:53:23+00:00
- **Updated**: 2020-02-02 16:01:54+00:00
- **Authors**: Ekim Yurtsever, Yongkang Liu, Jacob Lambert, Chiyomi Miyajima, Eijiro Takeuchi, Kazuya Takeda, John H. L. Hansen
- **Comment**: 8 pages, 3 figures, 1 table. The code is open-source
- **Journal**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand, 2019, pp. 3100-3107
- **Summary**: Advanced driver assistance and automated driving systems rely on risk estimation modules to predict and avoid dangerous situations. Current methods use expensive sensor setups and complex processing pipeline, limiting their availability and robustness. To address these issues, we introduce a novel deep learning based action recognition framework for classifying dangerous lane change behavior in short video clips captured by a monocular camera. We designed a deep spatiotemporal classification network that uses pre-trained state-of-the-art instance segmentation network Mask R-CNN as its spatial feature extractor for this task. The Long-Short Term Memory (LSTM) and shallower final classification layers of the proposed method were trained on a semi-naturalistic lane change dataset with annotated risk labels. A comprehensive comparison of state-of-the-art feature extractors was carried out to find the best network layout and training strategy. The best result, with a 0.937 AUC score, was obtained with the proposed network. Our code and trained models are available open-source.



### Deep Spherical Quantization for Image Search
- **Arxiv ID**: http://arxiv.org/abs/1906.02865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02865v1)
- **Published**: 2019-06-07 02:21:16+00:00
- **Updated**: 2019-06-07 02:21:16+00:00
- **Authors**: Sepehr Eghbali, Ladan Tahvildari
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing methods, which encode high-dimensional images with compact discrete codes, have been widely applied to enhance large-scale image retrieval. In this paper, we put forward Deep Spherical Quantization (DSQ), a novel method to make deep convolutional neural networks generate supervised and compact binary codes for efficient image search. Our approach simultaneously learns a mapping that transforms the input images into a low-dimensional discriminative space, and quantizes the transformed data points using multi-codebook quantization. To eliminate the negative effect of norm variance on codebook learning, we force the network to L_2 normalize the extracted features and then quantize the resulting vectors using a new supervised quantization technique specifically designed for points lying on a unit hypersphere. Furthermore, we introduce an easy-to-implement extension of our quantization technique that enforces sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its sparse variant can generate semantically separable compact binary codes outperforming many state-of-the-art image retrieval methods on three benchmarks.



### Seeing Behind Things: Extending Semantic Segmentation to Occluded Regions
- **Arxiv ID**: http://arxiv.org/abs/1906.02885v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02885v2)
- **Published**: 2019-06-07 03:30:10+00:00
- **Updated**: 2019-09-17 07:26:21+00:00
- **Authors**: Pulak Purkait, Christopher Zach, Ian Reid
- **Comment**: None
- **Journal**: IROS 2019
- **Summary**: Semantic segmentation and instance level segmentation made substantial progress in recent years due to the emergence of deep neural networks (DNNs). A number of deep architectures with Convolution Neural Networks (CNNs) were proposed that surpass the traditional machine learning approaches for segmentation by a large margin. These architectures predict the directly observable semantic category of each pixel by usually optimizing a cross entropy loss. In this work we push the limit of semantic segmentation towards predicting semantic labels of directly visible as well as occluded objects or objects parts, where the network's input is a single depth image. We group the semantic categories into one background and multiple foreground object groups, and we propose a modification of the standard cross-entropy loss to cope with the settings. In our experiments we demonstrate that a CNN trained by minimizing the proposed loss is able to predict semantic categories for visible and occluded object parts without requiring to increase the network size (compared to a standard segmentation task). The results are validated on a newly generated dataset (augmented from SUNCG) dataset.



### Visually Grounded Neural Syntax Acquisition
- **Arxiv ID**: http://arxiv.org/abs/1906.02890v2
- **DOI**: 10.18653/v1/P19-1180
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02890v2)
- **Published**: 2019-06-07 04:03:53+00:00
- **Updated**: 2019-09-24 18:29:51+00:00
- **Authors**: Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu
- **Comment**: ACL 2019. Project page:
  https://ttic.uchicago.edu/~freda/project/vgnsl/
- **Journal**: None
- **Summary**: We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches.



### Towards Non-I.I.D. Image Classification: A Dataset and Baselines
- **Arxiv ID**: http://arxiv.org/abs/1906.02899v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02899v3)
- **Published**: 2019-06-07 05:07:07+00:00
- **Updated**: 2019-08-14 09:18:14+00:00
- **Authors**: Yue He, Zheyan Shen, Peng Cui
- **Comment**: None
- **Journal**: None
- **Summary**: I.I.D. hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing instable performances of these models. In literature, however, the Non-I.I.D. image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO, which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with ConvNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings.



### Decompose-and-Integrate Learning for Multi-class Segmentation in Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1906.02901v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02901v1)
- **Published**: 2019-06-07 05:10:35+00:00
- **Updated**: 2019-06-07 05:10:35+00:00
- **Authors**: Yizhe Zhang, Michael T. C. Ying, Danny Z. Chen
- **Comment**: To appear in MICCAI 2019
- **Journal**: None
- **Summary**: Segmentation maps of medical images annotated by medical experts contain rich spatial information. In this paper, we propose to decompose annotation maps to learn disentangled and richer feature transforms for segmentation problems in medical images. Our new scheme consists of two main stages: decompose and integrate. Decompose: by annotation map decomposition, the original segmentation problem is decomposed into multiple segmentation sub-problems; these new segmentation sub-problems are modeled by training multiple deep learning modules, each with its own set of feature transforms. Integrate: a procedure summarizes the solutions of the modules in the previous stage; a final solution is then formed for the original segmentation problem. Multiple ways of annotation map decomposition are presented and a new end-to-end trainable K-to-1 deep network framework is developed for implementing our proposed "decompose-and-integrate" learning scheme. In experiments, we demonstrate that our decompose-and-integrate segmentation, utilizing state-of-the-art fully convolutional networks (e.g., DenseVoxNet in 3D and CUMedNet in 2D), improves segmentation performance on multiple 3D and 2D datasets. Ablation study confirms the effectiveness of our proposed learning scheme for medical images.



### AutoGrow: Automatic Layer Growing in Deep Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02909v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1906.02909v5)
- **Published**: 2019-06-07 05:54:41+00:00
- **Updated**: 2020-06-15 18:09:02+00:00
- **Authors**: Wei Wen, Feng Yan, Yiran Chen, Hai Li
- **Comment**: KDD 2020
- **Journal**: None
- **Summary**: Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN. Our code is available at https://github.com/wenwei202/autogrow.



### Two-Stage Peer-Regularized Feature Recombination for Arbitrary Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.02913v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02913v3)
- **Published**: 2019-06-07 06:14:07+00:00
- **Updated**: 2020-04-11 20:05:06+00:00
- **Authors**: Jan Svoboda, Asha Anoosheh, Christian Osendorfer, Jonathan Masci
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a neural style transfer model to generate a stylized image conditioning on a set of examples describing the desired style. The proposed solution produces high-quality images even in the zero-shot setting and allows for more freedom in changes to the content geometry. This is made possible by introducing a novel Two-Stage Peer-Regularization Layer that recombines style and content in latent space by means of a custom graph convolutional layer. Contrary to the vast majority of existing solutions, our model does not depend on any pre-trained networks for computing perceptual losses and can be trained fully end-to-end thanks to a new set of cyclic losses that operate directly in latent space and not on the RGB images. An extensive ablation study confirms the usefulness of the proposed losses and of the Two-Stage Peer-Regularization Layer, with qualitative results that are competitive with respect to the current state of the art using a single model for all presented styles. This opens the door to more abstract and artistic neural image generation scenarios, along with simpler deployment of the model.



### EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1906.02919v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02919v3)
- **Published**: 2019-06-07 06:27:46+00:00
- **Updated**: 2020-03-02 00:33:09+00:00
- **Authors**: Nitin J. Sanket, Chethan M. Parameshwara, Chahat Deep Singh, Ashwin V. Kuruttukulam, Cornelia Fermüller, Davide Scaramuzza, Yiannis Aloimonos
- **Comment**: 15 pages, 16 figures, Code and Video can be found at:
  https://prg.cs.umd.edu/EVDodgeNet
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  2020
- **Summary**: Dynamic obstacle avoidance on quadrotors requires low latency. A class of sensors that are particularly suitable for such scenarios are event cameras. In this paper, we present a deep learning -- based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and on-board computation. Our approach uses a series of shallow neural networks for estimating both the ego-motion and the motion of independently moving objects. The networks are trained in simulation and directly transfer to the real world without any fine-tuning or retraining. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with obstacles of different shapes and sizes, achieving an overall success rate of 70% including objects of unknown shape and a low light testing scenario. To our knowledge, this is the first deep learning -- based solution to the problem of dynamic obstacle avoidance using event cameras on a quadrotor. Finally, we also extend our work to the pursuit task by merely reversing the control policy, proving that our navigation stack can cater to different scenarios.



### PseudoEdgeNet: Nuclei Segmentation only with Point Annotations
- **Arxiv ID**: http://arxiv.org/abs/1906.02924v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.02924v2)
- **Published**: 2019-06-07 06:55:45+00:00
- **Updated**: 2019-07-22 10:08:04+00:00
- **Authors**: Inwan Yoo, Donggeun Yoo, Kyunghyun Paeng
- **Comment**: MICCAI 2019 accepted
- **Journal**: None
- **Summary**: Nuclei segmentation is one of the important tasks for whole slide image analysis in digital pathology. With the drastic advance of deep learning, recent deep networks have demonstrated successful performance of the nuclei segmentation task. However, a major bottleneck to achieving good performance is the cost for annotation. A large network requires a large number of segmentation masks, and this annotation task is given to pathologists, not the public. In this paper, we propose a weakly supervised nuclei segmentation method, which requires only point annotations for training. This method can scale to large training set as marking a point of a nucleus is much cheaper than the fine segmentation mask. To this end, we introduce a novel auxiliary network, called PseudoEdgeNet, which guides the segmentation network to recognize nuclei edges even without edge annotations. We evaluate our method with two public datasets, and the results demonstrate that the method consistently outperforms other weakly supervised methods.



### Key Ingredients of Self-Driving Cars
- **Arxiv ID**: http://arxiv.org/abs/1906.02939v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1906.02939v2)
- **Published**: 2019-06-07 07:41:27+00:00
- **Updated**: 2019-08-10 04:23:01+00:00
- **Authors**: Rui Fan, Jianhao Jiao, Haoyang Ye, Yang Yu, Ioannis Pitas, Ming Liu
- **Comment**: 5 pages, 2 figures, EUSIPCO 2019 Satellite Workshop: Signal
  Processing, Computer Vision and Deep Learning for Autonomous Systems
- **Journal**: None
- **Summary**: Over the past decade, many research articles have been published in the area of autonomous driving. However, most of them focus only on a specific technological area, such as visual environment perception, vehicle control, etc. Furthermore, due to fast advances in the self-driving car technology, such articles become obsolete very fast. In this paper, we give a brief but comprehensive overview on key ingredients of autonomous cars (ACs), including driving automation levels, AC sensors, AC software, open source datasets, industry leaders, AC applications and existing challenges.



### Selfie: Self-supervised Pretraining for Image Embedding
- **Arxiv ID**: http://arxiv.org/abs/1906.02940v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02940v3)
- **Published**: 2019-06-07 07:47:24+00:00
- **Updated**: 2019-07-27 08:03:46+00:00
- **Authors**: Trieu H. Trinh, Minh-Thang Luong, Quoc V. Le
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a pretraining technique called Selfie, which stands for SELFie supervised Image Embedding. Selfie generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other "distractor" patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selfie includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate Selfie on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying amounts of labeled data, from 5% to 100% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 x 224 with 60 examples per class (5%), our method improves the mean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across different runs.



### Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.02944v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.02944v5)
- **Published**: 2019-06-07 08:07:05+00:00
- **Updated**: 2021-06-27 02:17:15+00:00
- **Authors**: Han-Jia Ye, Hexiang Hu, De-Chuan Zhan
- **Comment**: Accepted by IJCV; The code is available at
  https://github.com/Sha-Lab/aCASTLE
- **Journal**: None
- **Summary**: Object recognition in the real-world requires handling long-tailed or even open-ended data. An ideal visual system needs to recognize the populated head visual concepts reliably and meanwhile efficiently learn about emerging new tail categories with a few training instances. Class-balanced many-shot learning and few-shot learning tackle one side of this problem, by either learning strong classifiers for head or learning to learn few-shot classifiers for the tail. In this paper, we investigate the problem of generalized few-shot learning (GFSL) -- a model during the deployment is required to learn about tail categories with few shots and simultaneously classify the head classes. We propose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that learns how to synthesize calibrated few-shot classifiers in addition to the multi-class classifiers of head classes with a shared neural dictionary, shedding light upon the inductive GFSL. Furthermore, we propose an adaptive version of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the incoming tail training examples, yielding a framework that allows effective backward knowledge transfer. As a consequence, ACASTLE can handle GFSL with classes from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate superior performances than existing GFSL algorithms and strong baselines on MiniImageNet as well as TieredImageNet datasets. More interestingly, they outperform previous state-of-the-art methods when evaluated with standard few-shot learning criteria.



### Deep Learning based Cephalometric Landmark Identification using Landmark-dependent Multi-scale Patches
- **Arxiv ID**: http://arxiv.org/abs/1906.02961v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02961v1)
- **Published**: 2019-06-07 08:44:00+00:00
- **Updated**: 2019-06-07 08:44:00+00:00
- **Authors**: Chonho Lee, Chihiro Tanikawa, Jae-Yeon Lim, Takashi Yamashiro
- **Comment**: None
- **Journal**: None
- **Summary**: A deep neural network based cephalometric landmark identification model is proposed. Two neural networks, named patch classification and point estimation, are trained by multi-scale image patches cropped from 935 Cephalograms (of Japanese young patients), whose size and orientation vary based on landmark-dependent criteria examined by orthodontists. The proposed model identifies both 22 hard and 11 soft tissue landmarks. In order to evaluate the proposed model, (i) landmark estimation accuracy by Euclidean distance error between true and estimated values, and (ii) success rate that the estimated landmark was located within the corresponding norm using confidence ellipse, are computed. The proposed model successfully identified hard tissue landmarks within the error range of 1.32 - 3.5 mm and with a mean success rate of 96.4%, and soft tissue landmarks with the error range of 1.16 - 4.37 mm and with a mean success rate of 75.2%. We verify that considering the landmark-dependent size and orientation of patches helps improve the estimation accuracy.



### An Artificial Intelligence-Based System for Nutrient Intake Assessment of Hospitalised Patients
- **Arxiv ID**: http://arxiv.org/abs/1906.02990v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02990v2)
- **Published**: 2019-06-07 09:54:14+00:00
- **Updated**: 2019-06-12 09:11:47+00:00
- **Authors**: Ya Lu, Thomai Stathopoulou, Maria F. Vasiloglou, Stergios Christodoulidis, Beat Blum, Thomas Walser, Vinzenz Meier, Zeno Stanga, Stavroula G. Mougiakakou
- **Comment**: EMBC2019
- **Journal**: None
- **Summary**: Regular nutrient intake monitoring in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition (DRM). Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve the data accuracy and reduce both the participant burden and the health costs. In this paper, we propose a novel system based on artificial intelligence to accurately estimate nutrient intake, by simply processing RGB depth image pairs captured before and after a meal consumption. For the development and evaluation of the system, a dedicated and new database of images and recipes of 322 meals was assembled, coupled to data annotation using innovative strategies. With this database, a system was developed that employed a novel multi-task neural network and an algorithm for 3D surface construction. This allowed sequential semantic food segmentation and estimation of the volume of the consumed food, and permitted fully automatic estimation of nutrient intake for each food type with a 15% estimation error.



### Deep Angular Embedding and Feature Correlation Attention for Breast MRI Cancer Analysis
- **Arxiv ID**: http://arxiv.org/abs/1906.02999v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.02999v1)
- **Published**: 2019-06-07 10:23:06+00:00
- **Updated**: 2019-06-07 10:23:06+00:00
- **Authors**: Luyang Luo, Hao Chen, Xi Wang, Qi Dou, Huangjin Lin, Juan Zhou, Gongjie Li, Pheng-Ann Heng
- **Comment**: Accepted by MICCAI 2019
- **Journal**: None
- **Summary**: Accurate and automatic analysis of breast MRI plays an important role in early diagnosis and successful treatment planning for breast cancer. Due to the heterogeneity nature, accurate diagnosis of tumors remains a challenging task. In this paper, we propose to identify breast tumor in MRI by Cosine Margin Sigmoid Loss (CMSL) with deep learning (DL) and localize possible cancer lesion by COrrelation Attention Map (COAM) based on the learned features. The CMSL embeds tumor features onto a hypersphere and imposes a decision margin through cosine constraints. In this way, the DL model could learn more separable inter-class features and more compact intra-class features in the angular space. Furthermore, we utilize the correlations among feature vectors to generate attention maps that could accurately localize cancer candidates with only image-level label. We build the largest breast cancer dataset involving 10,290 DCE-MRI scan volumes for developing and evaluating the proposed methods. The model driven by CMSL achieved classification accuracy of 0.855 and AUC of 0.902 on the testing set, with sensitivity and specificity of 0.857 and 0.852, respectively, outperforming other competitive methods overall. In addition, the proposed COAM accomplished more accurate localization of the cancer center compared with other state-of-the-art weakly supervised localization method.



### Visual Person Understanding through Multi-Task and Multi-Dataset Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.03019v1
- **DOI**: 10.1007/978-3-030-33676-9_39
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03019v1)
- **Published**: 2019-06-07 11:24:41+00:00
- **Updated**: 2019-06-07 11:24:41+00:00
- **Authors**: Kilian Pfeiffer, Alexander Hermans, István Sárándi, Mark Weber, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of learning a single model for person re-identification, attribute classification, body part segmentation, and pose estimation. With predictions for these tasks we gain a more holistic understanding of persons, which is valuable for many applications. This is a classical multi-task learning problem. However, no dataset exists that these tasks could be jointly learned from. Hence several datasets need to be combined during training, which in other contexts has often led to reduced performance in the past. We extensively evaluate how the different task and datasets influence each other and how different degrees of parameter sharing between the tasks affect performance. Our final model matches or outperforms its single-task counterparts without creating significant computational overhead, rendering it highly interesting for resource-constrained scenarios such as mobile robotics.



### Context-driven Active and Incremental Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.03033v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03033v1)
- **Published**: 2019-06-07 12:06:34+00:00
- **Updated**: 2019-06-07 12:06:34+00:00
- **Authors**: Gabriele Civitarese, Riccardo Presotto, Claudio Bettini
- **Comment**: None
- **Journal**: None
- **Summary**: Human activity recognition based on mobile device sensor data has been an active research area in mobile and pervasive computing for several years. While the majority of the proposed techniques are based on supervised learning, semi-supervised approaches are being considered to significantly reduce the size of the training set required to initialize the recognition model. These approaches usually apply self-training or active learning to incrementally refine the model, but their effectiveness seems to be limited to a restricted set of physical activities. We claim that the context which surrounds the user (e.g., semantic location, proximity to transportation routes, time of the day) combined with common knowledge about the relationship between this context and human activities could be effective in significantly increasing the set of recognized activities including those that are difficult to discriminate only considering inertial sensors, and the ones that are highly context-dependent. In this paper, we propose CAVIAR, a novel hybrid semi-supervised and knowledge-based system for real-time activity recognition. Our method applies semantic reasoning to context data to refine the prediction of a semi-supervised classifier. The context-refined predictions are used as new labeled samples to update the classifier combining self-training and active learning techniques. Results on a real dataset obtained from 26 subjects show the effectiveness of the context-aware approach both on the recognition rates and on the number of queries to the subjects generated by the active learning module. In order to evaluate the impact of context reasoning, we also compare CAVIAR with a purely statistical version, considering features computed on context data as part of the machine learning process.



### A Generative Framework for Zero-Shot Learning with Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.03038v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.03038v3)
- **Published**: 2019-06-07 12:11:22+00:00
- **Updated**: 2020-02-22 18:49:21+00:00
- **Authors**: Varun Khare, Divyat Mahajan, Homanga Bharadhwaj, Vinay Verma, Piyush Rai
- **Comment**: Proceedings of Winter Conference on Applications of Computer Vision
  (WACV) 2020
- **Journal**: None
- **Summary**: We present a domain adaptation based generative framework for zero-shot learning. Our framework addresses the problem of domain shift between the seen and unseen class distributions in zero-shot learning and minimizes the shift by developing a generative model trained via adversarial domain adaptation. Our approach is based on end-to-end learning of the class distributions of seen classes and unseen classes. To enable the model to learn the class distributions of unseen classes, we parameterize these class distributions in terms of the class attribute information (which is available for both seen and unseen classes). This provides a very simple way to learn the class distribution of any unseen class, given only its class attribute information, and no labeled training data. Training this model with adversarial domain adaptation further provides robustness against the distribution mismatch between the data from seen and unseen classes. Our approach also provides a novel way for training neural net based classifiers to overcome the hubness problem in zero-shot learning. Through a comprehensive set of experiments, we show that our model yields superior accuracies as compared to various state-of-the-art zero shot learning models, on a variety of benchmark datasets. Code for the experiments is available at github.com/vkkhare/ZSL-ADA



### Coherent Point Drift Networks: Unsupervised Learning of Non-Rigid Point Set Registration
- **Arxiv ID**: http://arxiv.org/abs/1906.03039v5
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03039v5)
- **Published**: 2019-06-07 12:12:22+00:00
- **Updated**: 2019-07-28 17:43:16+00:00
- **Authors**: Lingjing Wang, Xiang Li, Jianchun Chen, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Given new pairs of source and target point sets, standard point set registration methods often repeatedly conduct the independent iterative search of desired geometric transformation to align the source point set with the target one. This limits their use in applications to handle the real-time point set registration with large volume dataset. This paper presents a novel method, named coherent point drift networks (CPD-Net), for the unsupervised learning of geometric transformation towards real-time non-rigid point set registration. In contrast to previous efforts (e.g. coherent point drift), CPD-Net can learn displacement field function to estimate geometric transformation from a training dataset, consequently, to predict the desired geometric transformation for the alignment of previously unseen pairs without any additional iterative optimization process. Furthermore, CPD-Net leverages the power of deep neural networks to fit an arbitrary function, that adaptively accommodates different levels of complexity of the desired geometric transformation. Particularly, CPD-Net is proved with a theoretical guarantee to learn a continuous displacement vector function that could further avoid imposing additional parametric smoothness constraint as in previous works. Our experiments verify the impressive performance of CPD-Net for non-rigid point set registration on various 2D/3D datasets, even in the presence of significant displacement noise, outliers, and missing points. Our code will be available at https://github.com/nyummvc/CPD-Net.



### HPILN: A feature learning framework for cross-modality person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1906.03142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03142v2)
- **Published**: 2019-06-07 14:55:47+00:00
- **Updated**: 2019-08-14 08:06:40+00:00
- **Authors**: Jian-Wu Lin, Hao Li
- **Comment**: None
- **Journal**: None
- **Summary**: Most video surveillance systems use both RGB and infrared cameras, making it a vital technique to re-identify a person cross the RGB and infrared modalities. This task can be challenging due to both the cross-modality variations caused by heterogeneous images in RGB and infrared, and the intra-modality variations caused by the heterogeneous human poses, camera views, light brightness, etc. To meet these challenges a novel feature learning framework, HPILN, is proposed. In the framework existing single-modality re-identification models are modified to fit for the cross-modality scenario, following which specifically designed hard pentaplet loss and identity loss are used to improve the performance of the modified cross-modality re-identification models. Based on the benchmark of the SYSU-MM01 dataset, extensive experiments have been conducted, which show that the proposed method outperforms all existing methods in terms of Cumulative Match Characteristic curve (CMC) and Mean Average Precision (MAP).



### A deep learning approach for automated detection of geographic atrophy from color fundus photographs
- **Arxiv ID**: http://arxiv.org/abs/1906.03153v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03153v1)
- **Published**: 2019-06-07 15:12:38+00:00
- **Updated**: 2019-06-07 15:12:38+00:00
- **Authors**: Tiarnan D. Keenan, Shazia Dharssi, Yifan Peng, Qingyu Chen, Elvira Agrón, Wai T. Wong, Zhiyong Lu, Emily Y. Chew
- **Comment**: Accepted for publication in Ophthalmology
- **Journal**: None
- **Summary**: Purpose: To assess the utility of deep learning in the detection of geographic atrophy (GA) from color fundus photographs; secondary aim to explore potential utility in detecting central GA (CGA). Design: A deep learning model was developed to detect the presence of GA in color fundus photographs, and two additional models to detect CGA in different scenarios. Participants: 59,812 color fundus photographs from longitudinal follow up of 4,582 participants in the AREDS dataset. Gold standard labels were from human expert reading center graders using a standardized protocol. Methods: A deep learning model was trained to use color fundus photographs to predict GA presence from a population of eyes with no AMD to advanced AMD. A second model was trained to predict CGA presence from the same population. A third model was trained to predict CGA presence from the subset of eyes with GA. For training and testing, 5-fold cross-validation was employed. For comparison with human clinician performance, model performance was compared with that of 88 retinal specialists. Results: The deep learning models (GA detection, CGA detection from all eyes, and centrality detection from GA eyes) had AUC of 0.933-0.976, 0.939-0.976, and 0.827-0.888, respectively. The GA detection model had accuracy, sensitivity, specificity, and precision of 0.965, 0.692, 0.978, and 0.584, respectively. The CGA detection model had equivalent values of 0.966, 0.763, 0.971, and 0.394. The centrality detection model had equivalent values of 0.762, 0.782, 0.729, and 0.799. Conclusions: A deep learning model demonstrated high accuracy for the automated detection of GA. The AUC was non-inferior to that of human retinal specialists. Deep learning approaches may also be applied to the identification of CGA. The code and pretrained models are publicly available at https://github.com/ncbi-nlp/DeepSeeNet.



### Ego-Pose Estimation and Forecasting as Real-Time PD Control
- **Arxiv ID**: http://arxiv.org/abs/1906.03173v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.03173v2)
- **Published**: 2019-06-07 15:39:21+00:00
- **Updated**: 2019-08-04 21:21:33+00:00
- **Authors**: Ye Yuan, Kris Kitani
- **Comment**: ICCV 2019; Webpage: https://www.ye-yuan.com/ego-pose; Video:
  https://youtu.be/968IIDZeWE0
- **Journal**: None
- **Summary**: We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.



### Multimodal End-to-End Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1906.03199v2
- **DOI**: 10.1109/TITS.2020.3013234
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03199v2)
- **Published**: 2019-06-07 16:08:19+00:00
- **Updated**: 2020-10-25 10:42:37+00:00
- **Authors**: Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, Antonio M. López
- **Comment**: The paper has been accepted by IEEE Transactions on Intelligent
  Transportation Systems 2020
- **Journal**: None
- **Summary**: A crucial component of an autonomous vehicle (AV) is the artificial intelligence (AI) is able to drive towards a desired destination. Today, there are different paradigms addressing the development of AI drivers. On the one hand, we find modular pipelines, which divide the driving task into sub-tasks such as perception and maneuver planning and control. On the other hand, we find end-to-end driving approaches that try to learn a direct mapping from input raw sensor data to vehicle control signals. The later are relatively less studied, but are gaining popularity since they are less demanding in terms of sensor data annotation. This paper focuses on end-to-end autonomous driving. So far, most proposals relying on this paradigm assume RGB images as input sensor data. However, AVs will not be equipped only with cameras, but also with active sensors providing accurate depth information (e.g., LiDARs). Accordingly, this paper analyses whether combining RGB and depth modalities, i.e. using RGBD data, produces better end-to-end AI drivers than relying on a single modality. We consider multimodality based on early, mid and late fusion schemes, both in multisensory and single-sensor (monocular depth estimation) settings. Using the CARLA simulator and conditional imitation learning (CIL), we show how, indeed, early fusion multimodality outperforms single-modality.



### Extracting Visual Knowledge from the Internet: Making Sense of Image Data
- **Arxiv ID**: http://arxiv.org/abs/1906.03219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1906.03219v1)
- **Published**: 2019-06-07 16:35:33+00:00
- **Updated**: 2019-06-07 16:35:33+00:00
- **Authors**: Yazhou Yao, Jian Zhang, Xiansheng Hua, Fumin Shen, Zhenmin Tang
- **Comment**: Accepted by International Conference on MultiMedia Modeling, 2016
  (MMM)
- **Journal**: None
- **Summary**: Recent successes in visual recognition can be primarily attributed to feature representation, learning algorithms, and the ever-increasing size of labeled training data. Extensive research has been devoted to the first two, but much less attention has been paid to the third. Due to the high cost of manual labeling, the size of recent efforts such as ImageNet is still relatively small in respect to daily applications. In this work, we mainly focus on how to automatically generate identifying image data for a given visual concept on a vast scale. With the generated image data, we can train a robust recognition model for the given concept. We evaluate the proposed webly supervised approach on the benchmark Pascal VOC 2007 dataset and the results demonstrates the superiority of our proposed approach in image data collection.



### Recurrent Registration Neural Networks for Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1906.09988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09988v1)
- **Published**: 2019-06-07 16:44:53+00:00
- **Updated**: 2019-06-07 16:44:53+00:00
- **Authors**: Robin Sandkühler, Simon Andermatt, Grzegorz Bauman, Sylvia Nyilas, Christoph Jud, Philippe C. Cattin
- **Comment**: None
- **Journal**: None
- **Summary**: Parametric spatial transformation models have been successfully applied to image registration tasks. In such models, the transformation of interest is parameterized by a fixed set of basis functions as for example B-splines. Each basis function is located on a fixed regular grid position among the image domain, because the transformation of interest is not known in advance. As a consequence, not all basis functions will necessarily contribute to the final transformation which results in a non-compact representation of the transformation. We reformulate the pairwise registration problem as a recursive sequence of successive alignments. For each element in the sequence, a local deformation defined by its position, shape, and weight is computed by our recurrent registration neural network. The sum of all local deformations yield the final spatial alignment of both images. Formulating the registration problem in this way allows the network to detect non-aligned regions in the images and to learn how to locally refine the registration properly. In contrast to current non-sequence-based registration methods, our approach iteratively applies local spatial deformations to the images until the desired registration accuracy is achieved. We trained our network on 2D magnetic resonance images of the lung and compared our method to a standard parametric B-spline registration. The experiments show, that our method performs on par for the accuracy but yields a more compact representation of the transformation. Furthermore, we achieve a speedup of around 15 compared to the B-spline registration.



### Evolving Losses for Unlabeled Video Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.03248v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03248v1)
- **Published**: 2019-06-07 17:22:54+00:00
- **Updated**: 2019-06-07 17:22:54+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo
- **Comment**: Non-archival abstract for CVPR Workshop on Learning from Unlabeled
  Videos
- **Journal**: None
- **Summary**: We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.



### TensorNetwork for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.06329v1
- **DOI**: None
- **Categories**: **cs.LG**, cond-mat.str-el, cs.CV, physics.comp-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.06329v1)
- **Published**: 2019-06-07 17:58:31+00:00
- **Updated**: 2019-06-07 17:58:31+00:00
- **Authors**: Stavros Efthymiou, Jack Hidary, Stefan Leichenauer
- **Comment**: 9 pages, 8 figures. All code can be found at
  https://github.com/google/tensornetwork
- **Journal**: None
- **Summary**: We demonstrate the use of tensor networks for image classification with the TensorNetwork open source library. We explain in detail the encoding of image data into a matrix product state form, and describe how to contract the network in a way that is parallelizable and well-suited to automatic gradients for optimization. Applying the technique to the MNIST and Fashion-MNIST datasets we find out-of-the-box performance of 98% and 88% accuracy, respectively, using the same tensor network architecture. The TensorNetwork library allows us to seamlessly move from CPU to GPU hardware, and we see a factor of more than 10 improvement in computational speed using a GPU.



### Deep Robust Single Image Depth Estimation Neural Network Using Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1906.03279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03279v1)
- **Published**: 2019-06-07 18:08:34+00:00
- **Updated**: 2019-06-07 18:08:34+00:00
- **Authors**: Haoyu Ren, Mostafa El-khamy, Jungwon Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Single image depth estimation (SIDE) plays a crucial role in 3D computer vision. In this paper, we propose a two-stage robust SIDE framework that can perform blind SIDE for both indoor and outdoor scenes. At the first stage, the scene understanding module will categorize the RGB image into different depth-ranges. We introduce two different scene understanding modules based on scene classification and coarse depth estimation respectively. At the second stage, SIDE networks trained by the images of specific depth-range are applied to obtain an accurate depth map. In order to improve the accuracy, we further design a multi-task encoding-decoding SIDE network DS-SIDENet based on depthwise separable convolutions. DS-SIDENet is optimized to minimize both depth classification and depth regression losses. This improves the accuracy compared to a single-task SIDE network. Experimental results demonstrate that training DS-SIDENet on an individual dataset such as NYU achieves competitive performance to the state-of-art methods with much better efficiency. Ours proposed robust SIDE framework also shows good performance for the ScanNet indoor images and KITTI outdoor images simultaneously. It achieves the top performance compared to the Robust Vision Challenge (ROB) 2018 submissions.



### PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding Module for Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.03299v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.03299v2)
- **Published**: 2019-06-07 19:06:24+00:00
- **Updated**: 2019-09-30 04:04:47+00:00
- **Authors**: Kang Zhiheng, Li Ning
- **Comment**: Accepted for presentation at ICONIP2019
- **Journal**: None
- **Summary**: With the tide of artificial intelligence, we try to apply deep learning to understand 3D data. Point cloud is an important 3D data structure, which can accurately and directly reflect the real world. In this paper, we propose a simple and effective network, which is named PyramNet, suites for point cloud object classification and semantic segmentation in 3D scene. We design two new operators: Graph Embedding Module(GEM) and Pyramid Attention Network(PAN). Specifically, GEM projects point cloud onto the graph and practices the covariance matrix to explore the relationship between points, so as to improve the local feature expression ability of the model. PAN assigns some strong semantic features to each point to retain fine geometric features as much as possible. Furthermore, we provide extensive evaluation and analysis for the effectiveness of PyramNet. Empirically, we evaluate our model on ModelNet40, ShapeNet and S3DIS.



### HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips
- **Arxiv ID**: http://arxiv.org/abs/1906.03327v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03327v2)
- **Published**: 2019-06-07 20:48:19+00:00
- **Updated**: 2019-07-31 13:47:49+00:00
- **Authors**: Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/.



### DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04569v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04569v1)
- **Published**: 2019-06-07 20:51:52+00:00
- **Updated**: 2019-06-07 20:51:52+00:00
- **Authors**: Aryan Mobiny, Hien V. Nguyen, Supratik Moulik, Naveen Garg, Carol C. Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have achieved state-of-the-art performances in many important domains, including medical diagnosis, security, and autonomous driving. In these domains where safety is highly critical, an erroneous decision can result in serious consequences. While a perfect prediction accuracy is not always achievable, recent work on Bayesian deep networks shows that it is possible to know when DNNs are more likely to make mistakes. Knowing what DNNs do not know is desirable to increase the safety of deep learning technology in sensitive applications. Bayesian neural networks attempt to address this challenge. However, traditional approaches are computationally intractable and do not scale well to large, complex neural network architectures. In this paper, we develop a theoretical framework to approximate Bayesian inference for DNNs by imposing a Bernoulli distribution on the model weights. This method, called MC-DropConnect, gives us a tool to represent the model uncertainty with little change in the overall model structure or computational cost. We extensively validate the proposed algorithm on multiple network architectures and datasets for classification and semantic segmentation tasks. We also propose new metrics to quantify the uncertainty estimates. This enables an objective comparison between MC-DropConnect and prior approaches. Our empirical results demonstrate that the proposed framework yields significant improvement in both prediction accuracy and uncertainty estimation quality compared to the state of the art.



### Detecting the Starting Frame of Actions in Video
- **Arxiv ID**: http://arxiv.org/abs/1906.03340v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03340v2)
- **Published**: 2019-06-07 21:48:09+00:00
- **Updated**: 2020-01-17 20:11:54+00:00
- **Authors**: Iljung S. Kwak, Jian-Zhong Guo, Adam Hantman, David Kriegman, Kristin Branson
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we address the problem of precisely localizing key frames of an action, for example, the precise time that a pitcher releases a baseball, or the precise time that a crowd begins to applaud. Key frame localization is a largely overlooked and important action-recognition problem, for example in the field of neuroscience, in which we would like to understand the neural activity that produces the start of a bout of an action. To address this problem, we introduce a novel structured loss function that properly weights the types of errors that matter in such applications: it more heavily penalizes extra and missed action start detections over small misalignments. Our structured loss is based on the best matching between predicted and labeled action starts. We train recurrent neural networks (RNNs) to minimize differentiable approximations of this loss. To evaluate these methods, we introduce the Mouse Reach Dataset, a large, annotated video dataset of mice performing a sequence of actions. The dataset was collected and labeled by experts for the purpose of neuroscience research. On this dataset, we demonstrate that our method outperforms related approaches and baseline methods using an unstructured loss.



### When Unseen Domain Generalization is Unnecessary? Rethinking Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.03347v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.03347v2)
- **Published**: 2019-06-07 22:25:06+00:00
- **Updated**: 2019-06-12 14:18:26+00:00
- **Authors**: Ling Zhang, Xiaosong Wang, Dong Yang, Thomas Sanford, Stephanie Harmon, Baris Turkbey, Holger Roth, Andriy Myronenko, Daguang Xu, Ziyue Xu
- **Comment**: 9 pages, 3 figure
- **Journal**: None
- **Summary**: Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However, in clinically realistic environments, such methods have marginal performance due to differences in image domains, including different imaging protocols, device vendors and patient populations. Here we consider the problem of domain generalization, when a model is trained once, and its performance generalizes to unseen domains. Intuitively, within a specific medical imaging modality the domain differences are smaller relative to natural images domain variability. We rethink data augmentation for medical 3D images and propose a deep stacked transformations (DST) approach for domain generalization. Specifically, a series of n stacked transformations are applied to each image in each mini-batch during network training to account for the contribution of domain-specific shifts in medical images. We comprehensively evaluate our method on three tasks: segmentation of whole prostate from 3D MRI, left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate that when trained on a small source dataset, (i) on average, DST models on unseen datasets degrade only by 11% (Dice score change), compared to the conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%); (ii) when evaluation on the same domain, DST is also better albeit only marginally. (iii) When training on large-sized data, DST on unseen domains reaches performance of state-of-the-art fully supervised models. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of robust deep segmentation models for clinical deployment.



### Video Modeling with Correlation Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.03349v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03349v2)
- **Published**: 2019-06-07 22:36:00+00:00
- **Updated**: 2020-05-27 00:13:12+00:00
- **Authors**: Heng Wang, Du Tran, Lorenzo Torresani, Matt Feiszli
- **Comment**: None
- **Journal**: None
- **Summary**: Motion is a salient cue to recognize actions in video. Modern action recognition models leverage motion information either explicitly by using optical flow as input or implicitly by means of 3D convolutional filters that simultaneously capture appearance and motion information. This paper proposes an alternative approach based on a learnable correlation operator that can be used to establish frame-toframe matches over convolutional feature maps in the different layers of the network. The proposed architecture enables the fusion of this explicit temporal matching information with traditional appearance cues captured by 2D convolution. Our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We empirically demonstrate that correlation networks produce strong results on a variety of video datasets, and outperform the state of the art on four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48 and Sports1M.



### Learning Physics-guided Face Relighting under Directional Light
- **Arxiv ID**: http://arxiv.org/abs/1906.03355v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.03355v2)
- **Published**: 2019-06-07 23:16:34+00:00
- **Updated**: 2020-04-19 12:33:08+00:00
- **Authors**: Thomas Nestmeyer, Jean-François Lalonde, Iain Matthews, Andreas M. Lehrmann
- **Comment**: CVPR 2020 (Oral)
- **Journal**: None
- **Summary**: Relighting is an essential step in realistically transferring objects from a captured image into another environment. For example, authentic telepresence in Augmented Reality requires faces to be displayed and relit consistent with the observer's scene lighting. We investigate end-to-end deep learning architectures that both de-light and relight an image of a human face. Our model decomposes the input image into intrinsic components according to a diffuse physics-based image formation model. We enable non-diffuse effects including cast shadows and specular highlights by predicting a residual correction to the diffuse render. To train and evaluate our model, we collected a portrait database of 21 subjects with various expressions and poses. Each sample is captured in a controlled light stage setup with 32 individual light sources. Our method creates precise and believable relighting results and generalizes to complex illumination conditions and challenging poses, including when the subject is not looking straight at the camera.



### Unsupervised Feature Learning with K-means and An Ensemble of Deep Convolutional Neural Networks for Medical Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.03359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.03359v1)
- **Published**: 2019-06-07 23:52:26+00:00
- **Updated**: 2019-06-07 23:52:26+00:00
- **Authors**: Euijoon Ahn, Ashnil Kumar, Dagan Feng, Michael Fulham, Jinman Kim
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Medical image analysis using supervised deep learning methods remains problematic because of the reliance of deep learning methods on large amounts of labelled training data. Although medical imaging data repositories continue to expand there has not been a commensurate increase in the amount of annotated data. Hence, we propose a new unsupervised feature learning method that learns feature representations to then differentiate dissimilar medical images using an ensemble of different convolutional neural networks (CNNs) and K-means clustering. It jointly learns feature representations and clustering assignments in an end-to-end fashion. We tested our approach on a public medical dataset and show its accuracy was better than state-of-the-art unsupervised feature learning methods and comparable to state-of-the-art supervised CNNs. Our findings suggest that our method could be used to tackle the issue of the large volume of unlabelled data in medical imaging repositories.



