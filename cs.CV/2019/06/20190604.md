# Arxiv Papers in cs.CV on 2019-06-04
### Style Transfer With Adaptation to the Central Objects of the Scene
- **Arxiv ID**: http://arxiv.org/abs/1906.01134v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1906.01134v1)
- **Published**: 2019-06-04 00:07:46+00:00
- **Updated**: 2019-06-04 00:07:46+00:00
- **Authors**: Alexey Schekalev, Victor Kitov
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer is a problem of rendering image with some content in the style of another image, for example a family photo in the style of a painting of some famous artist. The drawback of classical style transfer algorithm is that it imposes style uniformly on all parts of the content image, which perturbs central objects on the content image, such as faces or text, and makes them unrecognizable. This work proposes a novel style transfer algorithm which automatically detects central objects on the content image, generates spatial importance mask and imposes style non-uniformly: central objects are stylized less to preserve their recognizability and other parts of the image are stylized as usual to preserve the style. Three methods of automatic central object detection are proposed and evaluated qualitatively and via a user evaluation study. Both comparisons demonstrate higher quality of stylization compared to the classical style transfer method.



### A Natural Language-Inspired Multi-label Video Streaming Traffic Classification Method Based on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.02679v1
- **DOI**: 10.1007/s11760-020-01844-8
- **Categories**: **eess.SP**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.02679v1)
- **Published**: 2019-06-04 00:21:21+00:00
- **Updated**: 2019-06-04 00:21:21+00:00
- **Authors**: Yan Shi, Dezhi Feng, Subir Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a deep-learning based traffic classification method for identifying multiple streaming video sources at the same time within an encrypted tunnel. The work defines a novel feature inspired by Natural Language Processing (NLP) that allows existing NLP techniques to help the traffic classification. The feature extraction method is described, and a large dataset containing video streaming and web traffic is created to verify its effectiveness. Results are obtained by applying several NLP methods to show that the proposed method performs well on both binary and multilabel traffic classification problems. We also show the ability to achieve zero-shot learning with the proposed method.



### Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1906.01140v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.01140v2)
- **Published**: 2019-06-04 00:33:56+00:00
- **Updated**: 2019-09-05 06:47:05+00:00
- **Authors**: Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, Niki Trigoni
- **Comment**: NeurIPS 2019 Spotlight. Code and data are available at
  https://github.com/Yang7879/3D-BoNet
- **Journal**: None
- **Summary**: We propose a novel, conceptually simple and general framework for instance segmentation on 3D point clouds. Our method, called 3D-BoNet, follows the simple design philosophy of per-point multilayer perceptrons (MLPs). The framework directly regresses 3D bounding boxes for all instances in a point cloud, while simultaneously predicting a point-level mask for each instance. It consists of a backbone network followed by two parallel network branches for 1) bounding box regression and 2) point mask prediction. 3D-BoNet is single-stage, anchor-free and end-to-end trainable. Moreover, it is remarkably computationally efficient as, unlike existing approaches, it does not require any post-processing steps such as non-maximum suppression, feature sampling, clustering or voting. Extensive experiments show that our approach surpasses existing work on both ScanNet and S3DIS datasets while being approximately 10x more computationally efficient. Comprehensive ablation studies demonstrate the effectiveness of our design.



### Transfer Learning with intelligent training data selection for prediction of Alzheimer's Disease
- **Arxiv ID**: http://arxiv.org/abs/1906.01160v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01160v1)
- **Published**: 2019-06-04 02:23:59+00:00
- **Updated**: 2019-06-04 02:23:59+00:00
- **Authors**: Naimul Mefraz Khan, Marcia Hon, Nabila Abraham
- **Comment**: Accepted to IEEE Access
- **Journal**: None
- **Summary**: Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI through machine learning has been a subject of intense research in recent years. Recent success of deep learning in computer vision has progressed such research further. However, common limitations with such algorithms are reliance on a large number of training images, and requirement of careful optimization of the architecture of deep networks. In this paper, we attempt solving these issues with transfer learning, where the state-of-the-art VGG architecture is initialized with pre-trained weights from large benchmark datasets consisting of natural images. The network is then fine-tuned with layer-wise tuning, where only a pre-defined group of layers are trained on MRI images. To shrink the training data size, we employ image entropy to select the most informative slices. Through experimentation on the ADNI dataset, we show that with training size of 10 to 20 times smaller than the other contemporary methods, we reach state-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC classification problems, with a 4% and a 7% increase in accuracy over the state-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide detailed analysis of the effect of the intelligent training data selection method, changing the training size, and changing the number of layers to be fine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate how the proposed model focuses on discriminative image regions that are neuropathologically relevant, and can help the healthcare practitioner in interpreting the model's decision making process.



### Interpretable Neural Network Decoupling
- **Arxiv ID**: http://arxiv.org/abs/1906.01166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01166v2)
- **Published**: 2019-06-04 02:40:38+00:00
- **Updated**: 2020-08-25 13:22:34+00:00
- **Authors**: Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan, Yongjian Wu, Feiyue Huang, Ling Shao
- **Comment**: 20 pages, 12 figures
- **Journal**: None
- **Summary**: The remarkable performance of convolutional neural networks (CNNs) is entangled with their huge number of uninterpretable parameters, which has become the bottleneck limiting the exploitation of their full potential. Towards network interpretation, previous endeavors mainly resort to the single filter analysis, which however ignores the relationship between filters. In this paper, we propose a novel architecture decoupling method to interpret the network from a perspective of investigating its calculation paths. More specifically, we introduce a novel architecture controlling module in each layer to encode the network architecture by a vector. By maximizing the mutual information between the vectors and input images, the module is trained to select specific filters to distill a unique calculation path for each input. Furthermore, to improve the interpretability and compactness of the decoupled network, the output of each layer is encoded to align the architecture encoding vector with the constraint of sparsity regularization. Unlike conventional pixel-level or filter-level network interpretation methods, we propose a path-level analysis to explore the relationship between the combination of filter and semantic concepts, which is more suitable to interpret the working rationale of the decoupled network. Extensive experiments show that the decoupled network achieves several applications, i.e., network interpretation, network acceleration, and adversarial samples detection.



### Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1906.01984v1
- **DOI**: 10.1016/j.neucom.2019.03.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01984v1)
- **Published**: 2019-06-04 03:17:30+00:00
- **Updated**: 2019-06-04 03:17:30+00:00
- **Authors**: Xianxu Hou, Ke Sun, Linlin Shen, Guoping Qiu
- **Comment**: Accepted in Neurocomputing, 2019. arXiv admin note: text overlap with
  arXiv:1610.00291
- **Journal**: None
- **Summary**: We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistent principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement a generative adversarial training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperforms state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that our method can learn powerful embeddings of input face images, which can be used to achieve facial attribute manipulation. Moreover we propose a multi-view feature extraction strategy to extract effective image representations, which can be used to achieve state of the art performance in facial attribute prediction.



### Triangulation Learning Network: from Monocular to Stereo 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.01193v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01193v1)
- **Published**: 2019-06-04 04:50:46+00:00
- **Updated**: 2019-06-04 04:50:46+00:00
- **Authors**: Zengyi Qin, Jinglu Wang, Yan Lu
- **Comment**: 9 pages, accepted by CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we study the problem of 3D object detection from stereo images, in which the key challenge is how to effectively utilize stereo information. Different from previous methods using pixel-level depth maps, we propose employing 3D anchors to explicitly construct object-level correspondences between the regions of interest in stereo images, from which the deep neural network learns to detect and triangulate the targeted object in 3D space. We also introduce a cost-efficient channel reweighting strategy that enhances representational features and weakens noisy signals to facilitate the learning process. All of these are flexibly integrated into a solid baseline detector that uses monocular images. We demonstrate that both the monocular baseline and the stereo triangulation learning network outperform the prior state-of-the-arts in 3D object detection and localization on the challenging KITTI dataset.



### A Strong and Robust Baseline for Text-Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1906.01205v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01205v1)
- **Published**: 2019-06-04 05:42:58+00:00
- **Updated**: 2019-06-04 05:42:58+00:00
- **Authors**: Fangyu Liu, Rongtian Ye
- **Comment**: 6 pages (excluding references); 2019 ACL Student Research Workshop
  (to appear)
- **Journal**: None
- **Summary**: We review the current schemes of text-image matching models and propose improvements for both training and inference. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off: a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all $K$-most hardest samples are taken into account, tolerating \emph{pseudo} negatives and outliers. Second, we advocate the use of Inverted Softmax (\textsc{Is}) and Cross-modal Local Scaling (\textsc{Csls}) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin.



### Content Adaptive Optimization for Neural Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1906.01223v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01223v2)
- **Published**: 2019-06-04 06:39:50+00:00
- **Updated**: 2019-06-05 07:21:47+00:00
- **Authors**: Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, Christopher Schroers
- **Comment**: CVPR Workshop and Challenge on Learned Image Compression (2019)
- **Journal**: None
- **Summary**: The field of neural image compression has witnessed exciting progress as recently proposed architectures already surpass the established transform coding based approaches. While, so far, research has mainly focused on architecture and model improvements, in this work we explore content adaptive optimization. To this end, we introduce an iterative procedure which adapts the latent representation to the specific content we wish to compress while keeping the parameters of the network and the predictive model fixed. Our experiments show that this allows for an overall increase in rate-distortion performance, independently of the specific architecture used. Furthermore, we also evaluate this strategy in the context of adapting a pretrained network to other content that is different in visual appearance or resolution. Here, our experiments show that our adaptation strategy can largely close the gap as compared to models specifically trained for the given content while having the benefit that no additional data in the form of model parameter updates has to be transmitted.



### Visual Diagnosis of Dermatological Disorders: Human and Machine Performance
- **Arxiv ID**: http://arxiv.org/abs/1906.01256v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01256v1)
- **Published**: 2019-06-04 08:06:50+00:00
- **Updated**: 2019-06-04 08:06:50+00:00
- **Authors**: Jeremy Kawahara, Ghassan Hamarneh
- **Comment**: 15 pages, 7 figures, 5 tables
- **Journal**: None
- **Summary**: Skin conditions are a global health concern, ranking the fourth highest cause of nonfatal disease burden when measured as years lost due to disability. As diagnosing, or classifying, skin diseases can help determine effective treatment, dermatologists have extensively researched how to diagnose conditions from a patient's history and the lesion's visual appearance. Computer vision researchers are attempting to encode this diagnostic ability into machines, and several recent studies report machine level performance comparable with dermatologists.   This report reviews machine approaches to classify skin images and consider their performance when compared to human dermatologists. Following an overview of common image modalities, dermatologists' diagnostic approaches and common tasks, and publicly available datasets, we discuss approaches to machine skin lesion classification. We then review works that directly compare human and machine performance. Finally, this report addresses the limitations and sources of errors in image-based skin disease diagnosis, applicable to both machines and dermatologists in a teledermatology setting.



### Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.01258v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01258v1)
- **Published**: 2019-06-04 08:14:22+00:00
- **Updated**: 2019-06-04 08:14:22+00:00
- **Authors**: Massimiliano Mancini, Hakan Karaoguz, Elisa Ricci, Patric Jensfelt, Barbara Caputo
- **Comment**: ICRA 2019
- **Journal**: None
- **Summary**: While today's robots are able to perform sophisticated tasks, they can only act on objects they have been trained to recognize. This is a severe limitation: any robot will inevitably see new objects in unconstrained settings, and thus will always have visual knowledge gaps. However, standard visual modules are usually built on a limited set of classes and are based on the strong prior that an object must belong to one of those classes. Identifying whether an instance does not belong to the set of known categories (i.e. open set recognition), only partially tackles this problem, as a truly autonomous agent should be able not only to detect what it does not know, but also to extend dynamically its knowledge about the world. We contribute to this challenge with a deep learning architecture that can dynamically update its known classes in an end-to-end fashion. The proposed deep network, based on a deep extension of a non-parametric model, detects whether a perceived object belongs to the set of categories known by the system and learns it without the need to retrain the whole system from scratch. Annotated images about the new category can be provided by an 'oracle' (i.e. human supervision), or by autonomous mining of the Web. Experiments on two different databases and on a robot platform demonstrate the promise of our approach.



### Learning Deep Image Priors for Blind Image Denoising
- **Arxiv ID**: http://arxiv.org/abs/1906.01259v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01259v1)
- **Published**: 2019-06-04 08:16:01+00:00
- **Updated**: 2019-06-04 08:16:01+00:00
- **Authors**: Xianxu Hou, Hongming Luo, Jingxin Liu, Bolei Xu, Ke Sun, Yuanhao Gong, Bozhi Liu, Guoping Qiu
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising is the process of removing noise from noisy images, which is an image domain transferring task, i.e., from a single or several noise level domains to a photo-realistic domain. In this paper, we propose an effective image denoising method by learning two image priors from the perspective of domain alignment. We tackle the domain alignment on two levels. 1) the feature-level prior is to learn domain-invariant features for corrupted images with different level noise; 2) the pixel-level prior is used to push the denoised images to the natural image manifold. The two image priors are based on $\mathcal{H}$-divergence theory and implemented by learning classifiers in adversarial training manners. We evaluate our approach on multiple datasets. The results demonstrate the effectiveness of our approach for robust image denoising on both synthetic and real-world noisy images. Furthermore, we show that the feature-level prior is capable of alleviating the discrepancy between different level noise. It can be used to improve the blind denoising performance in terms of distortion measures (PSNR and SSIM), while pixel-level prior can effectively improve the perceptual quality to ensure the realistic outputs, which is further validated by subjective evaluation.



### Evaluation of an AI system for the automated detection of glaucoma from stereoscopic optic disc photographs: the European Optic Disc Assessment Study
- **Arxiv ID**: http://arxiv.org/abs/1906.01272v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01272v1)
- **Published**: 2019-06-04 08:40:03+00:00
- **Updated**: 2019-06-04 08:40:03+00:00
- **Authors**: Thomas W. Rogers, Nicolas Jaccard, Francis Carbonaro, Hans G. Lemij, Koenraad A. Vermeer, Nicolaas J. Reus, Sameer Trikha
- **Comment**: 24 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Objectives: To evaluate the performance of a deep learning based Artificial Intelligence (AI) software for detection of glaucoma from stereoscopic optic disc photographs, and to compare this performance to the performance of a large cohort of ophthalmologists and optometrists.   Methods: A retrospective study evaluating the diagnostic performance of an AI software (Pegasus v1.0, Visulytix Ltd., London UK) and comparing it to that of 243 European ophthalmologists and 208 British optometrists, as determined in previous studies, for the detection of glaucomatous optic neuropathy from 94 scanned stereoscopic photographic slides scanned into digital format.   Results: Pegasus was able to detect glaucomatous optic neuropathy with an accuracy of 83.4% (95% CI: 77.5-89.2). This is comparable to an average ophthalmologist accuracy of 80.5% (95% CI: 67.2-93.8) and average optometrist accuracy of 80% (95% CI: 67-88) on the same images. In addition, the AI system had an intra-observer agreement (Cohen's Kappa, $\kappa$) of 0.74 (95% CI: 0.63-0.85), compared to 0.70 (range: -0.13-1.00; 95% CI: 0.67-0.73) and 0.71 (range: 0.08-1.00) for ophthalmologists and optometrists, respectively. There was no statistically significant difference between the performance of the deep learning system and ophthalmologists or optometrists. There was no statistically significant difference between the performance of the deep learning system and ophthalmologists or optometrists.   Conclusion: The AI system obtained a diagnostic performance and repeatability comparable to that of the ophthalmologists and optometrists. We conclude that deep learning based AI systems, such as Pegasus, demonstrate significant promise in the assisted detection of glaucomatous optic neuropathy.



### Classifying logistic vehicles in cities using Deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.11895v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11895v1)
- **Published**: 2019-06-04 09:05:20+00:00
- **Updated**: 2019-06-04 09:05:20+00:00
- **Authors**: Salma Benslimane, Simon Tamayo, Arnaud de La Fortelle
- **Comment**: None
- **Journal**: World Conference on Transport Research, May 2019, Mumbai, India
- **Summary**: Rapid growth in delivery and freight transportation is increasing in urban areas; as a result the use of delivery trucks and light commercial vehicles is evolving. Major cities can use traffic counting as a tool to monitor the presence of delivery vehicles in order to implement intelligent city planning measures. Classical methods for counting vehicles use mechanical, electromagnetic or pneumatic sensors, but these devices are costly, difficult to implement and only detect the presence of vehicles without giving information about their category, model or trajectory. This paper proposes a Deep Learning tool for classifying vehicles in a given image while considering different categories of logistic vehicles, namely: light-duty, medium-duty and heavy-duty vehicles. The proposed approach yields two main contributions: first we developed an architecture to create an annotated and balanced database of logistic vehicles, reducing manual annotation efforts. Second, we built a classifier that accurately classifies the logistic vehicles passing through a given road. The results of this work are: first, a database of 72 000 images for 4 vehicles classes; and second two retrained convolutional neural networks (InceptionV3 and MobileNetV2) capable of classifying vehicles with accuracies over 90%.



### Information Competing Process for Learning Diversified Representations
- **Arxiv ID**: http://arxiv.org/abs/1906.01288v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01288v3)
- **Published**: 2019-06-04 09:10:43+00:00
- **Updated**: 2019-11-28 04:17:10+00:00
- **Authors**: Jie Hu, Rongrong Ji, ShengChuan Zhang, Xiaoshuai Sun, Qixiang Ye, Chia-Wen Lin, Qi Tian
- **Comment**: Accept as a NeurIPS 2019 paper
- **Journal**: None
- **Summary**: Learning representations with diversified information remains as an open problem. Towards learning diversified representations, a new approach, termed Information Competing Process (ICP), is proposed in this paper. Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task. By fusing representation parts learned competitively under different conditions, ICP facilitates obtaining diversified representations which contain rich information. Experiments on image classification and image reconstruction tasks demonstrate the great potential of ICP to learn discriminative and disentangled representations in both supervised and self-supervised learning settings.



### Relational Reasoning using Prior Knowledge for Visual Captioning
- **Arxiv ID**: http://arxiv.org/abs/1906.01290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01290v1)
- **Published**: 2019-06-04 09:15:54+00:00
- **Updated**: 2019-06-04 09:15:54+00:00
- **Authors**: Jingyi Hou, Xinxiao Wu, Yayun Qi, Wentian Zhao, Jiebo Luo, Yunde Jia
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting relationships among objects has achieved remarkable progress in interpreting images or videos by natural language. Most existing methods resort to first detecting objects and their relationships, and then generating textual descriptions, which heavily depends on pre-trained detectors and leads to performance drop when facing problems of heavy occlusion, tiny-size objects and long-tail in object detection. In addition, the separate procedure of detecting and captioning results in semantic inconsistency between the pre-defined object/relation categories and the target lexical words. We exploit prior human commonsense knowledge for reasoning relationships between objects without any pre-trained detectors and reaching semantic coherency within one image or video in captioning. The prior knowledge (e.g., in the form of knowledge graph) provides commonsense semantic correlation and constraint between objects that are not explicit in the image and video, serving as useful guidance to build semantic graph for sentence generation. Particularly, we present a joint reasoning method that incorporates 1) commonsense reasoning for embedding image or video regions into semantic space to build semantic graph and 2) relational reasoning for encoding semantic graph to generate sentences. Extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark validate the superiority of our method on leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.



### Optimal Unsupervised Domain Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.01292v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01292v1)
- **Published**: 2019-06-04 09:19:47+00:00
- **Updated**: 2019-06-04 09:19:47+00:00
- **Authors**: Emmanuel de Bézenac, Ibrahim Ayed, Patrick Gallinari
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method.



### Vision-Based Autonomous UAV Navigation and Landing for Urban Search and Rescue
- **Arxiv ID**: http://arxiv.org/abs/1906.01304v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01304v2)
- **Published**: 2019-06-04 09:54:38+00:00
- **Updated**: 2019-09-03 11:03:34+00:00
- **Authors**: Mayank Mittal, Rohit Mohan, Wolfram Burgard, Abhinav Valada
- **Comment**: Accepted for publication in the proceedings of the International
  Symposium on Robotics Research (ISRR) 2019
- **Journal**: None
- **Summary**: Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving technology that can enable identification of survivors under collapsed buildings in the aftermath of natural disasters such as earthquakes or gas explosions. However, these UAVs have to be able to autonomously navigate in disaster struck environments and land on debris piles in order to accurately locate the survivors. This problem is extremely challenging as pre-existing maps cannot be leveraged for navigation due to structural changes that may have occurred. Furthermore, existing landing site detection algorithms are not suitable to identify safe landing regions on debris piles. In this work, we present a computationally efficient system for autonomous UAV navigation and landing that does not require any prior knowledge about the environment. We propose a novel landing site detection algorithm that computes costmaps based on several hazard factors including terrain flatness, steepness, depth accuracy, and energy consumption information. We also introduce a first-of-a-kind synthetic dataset of over 1.2 million images of collapsed buildings with groundtruth depth, surface normals, semantics and camera pose information. We demonstrate the efficacy of our system using experiments from a city scale hyperrealistic simulation environment and in real-world scenarios with collapsed buildings.



### Towards better Validity: Dispersion based Clustering for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1906.01308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01308v1)
- **Published**: 2019-06-04 10:03:08+00:00
- **Updated**: 2019-06-04 10:03:08+00:00
- **Authors**: Guodong Ding, Salman Khan, Zhenmin Tang, Jian Zhang, Fatih Porikli
- **Comment**: 10 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Person re-identification aims to establish the correct identity correspondences of a person moving through a non-overlapping multi-camera installation. Recent advances based on deep learning models for this task mainly focus on supervised learning scenarios where accurate annotations are assumed to be available for each setup. Annotating large scale datasets for person re-identification is demanding and burdensome, which renders the deployment of such supervised approaches to real-world applications infeasible. Therefore, it is necessary to train models without explicit supervision in an autonomous manner. In this paper, we propose an elegant and practical clustering approach for unsupervised person re-identification based on the cluster validity consideration. Concretely, we explore a fundamental concept in statistics, namely \emph{dispersion}, to achieve a robust clustering criterion. Dispersion reflects the compactness of a cluster when employed at the intra-cluster level and reveals the separation when measured at the inter-cluster level. With this insight, we design a novel Dispersion-based Clustering (DBC) approach which can discover the underlying patterns in data. This approach considers a wider context of sample-level pairwise relationships to achieve a robust cluster affinity assessment which handles the complications may arise due to prevalent imbalanced data distributions. Additionally, our solution can automatically prioritize standalone data points and prevents inferior clustering. Our extensive experimental analysis on image and video re-identification benchmarks demonstrate that our method outperforms the state-of-the-art unsupervised methods by a significant margin. Code is available at https://github.com/gddingcs/Dispersion-based-Clustering.git.



### Example-Guided Style Consistent Image Synthesis from Semantic Labeling
- **Arxiv ID**: http://arxiv.org/abs/1906.01314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01314v2)
- **Published**: 2019-06-04 10:09:47+00:00
- **Updated**: 2019-06-28 01:31:01+00:00
- **Authors**: Miao Wang, Guo-Ye Yang, Ruilong Li, Run-Ze Liang, Song-Hai Zhang, Peter. M. Hall, Shi-Min Hu
- **Comment**: CVPR 2019 - Code and data - https://github.com/cxjyxxme/pix2pixSC
- **Journal**: None
- **Summary**: Example-guided image synthesis aims to synthesize an image from a semantic label map and an exemplary image indicating style. We use the term "style" in this problem to refer to implicit characteristics of images, for example: in portraits "style" includes gender, racial identity, age, hairstyle; in full body pictures it includes clothing; in street scenes, it refers to weather and time of day and such like. A semantic label map in these cases indicates facial expression, full body pose, or scene segmentation. We propose a solution to the example-guided image synthesis problem using conditional generative adversarial networks with style consistency. Our key contributions are (i) a novel style consistency discriminator to determine whether a pair of images are consistent in style; (ii) an adaptive semantic consistency loss; and (iii) a training data sampling strategy, for synthesizing style-consistent results to the exemplar.



### Color Constancy Convolutional Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1906.01340v1
- **DOI**: 10.1109/SSCI44817.2019.9002684
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01340v1)
- **Published**: 2019-06-04 10:56:32+00:00
- **Updated**: 2019-06-04 10:56:32+00:00
- **Authors**: Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen, Moncef Gabbouj
- **Comment**: 6 pages, 1 figure, 3 tables
- **Journal**: 2019 IEEE Symposium Series on Computational Intelligence (SSCI)
- **Summary**: In this paper, we study the importance of pre-training for the generalization capability in the color constancy problem. We propose two novel approaches based on convolutional autoencoders: an unsupervised pre-training algorithm using a fine-tuned encoder and a semi-supervised pre-training algorithm using a novel composite-loss function. This enables us to solve the data scarcity problem and achieve competitive, to the state-of-the-art, results while requiring much fewer parameters on ColorChecker RECommended dataset. We further study the over-fitting phenomenon on the recently introduced version of INTEL-TUT Dataset for Camera Invariant Color Constancy Research, which has both field and non-field scenes acquired by three different camera models.



### Face Parsing with RoI Tanh-Warping
- **Arxiv ID**: http://arxiv.org/abs/1906.01342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01342v1)
- **Published**: 2019-06-04 11:01:17+00:00
- **Updated**: 2019-06-04 11:01:17+00:00
- **Authors**: Jinpeng Lin, Hao Yang, Dong Chen, Ming Zeng, Fang Wen, Lu Yuan
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Face parsing computes pixel-wise label maps for different semantic components (e.g., hair, mouth, eyes) from face images. Existing face parsing literature have illustrated significant advantages by focusing on individual regions of interest (RoIs) for faces and facial components. However, the traditional crop-and-resize focusing mechanism ignores all contextual area outside the RoIs, and thus is not suitable when the component area is unpredictable, e.g. hair. Inspired by the physiological vision system of human, we propose a novel RoI Tanh-warping operator that combines the central vision and the peripheral vision together. It addresses the dilemma between a limited sized RoI for focusing and an unpredictable area of surrounding context for peripheral information. To this end, we propose a novel hybrid convolutional neural network for face parsing. It uses hierarchical local based method for inner facial components and global methods for outer facial components. The whole framework is simple and principled, and can be trained end-to-end. To facilitate future research of face parsing, we also manually relabel the training data of the HELEN dataset and will make it public. Experiments on both HELEN and LFW-PL benchmarks demonstrate that our method surpasses state-of-the-art methods.



### Exploiting Offset-guided Network for Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.01344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01344v1)
- **Published**: 2019-06-04 11:05:24+00:00
- **Updated**: 2019-06-04 11:05:24+00:00
- **Authors**: Rui Zhang, Zheng Zhu, Peng Li, Rui Wu, Chaoxu Guo, Guan Huang, Hailun Xia
- **Comment**: To appear in CVPR-2019 VUHCS Workshop
- **Journal**: None
- **Summary**: Human pose estimation has witnessed a significant advance thanks to the development of deep learning. Recent human pose estimation approaches tend to directly predict the location heatmaps, which causes quantization errors and inevitably deteriorates the performance within the reduced network output. Aim at solving it, we revisit the heatmap-offset aggregation method and propose the Offset-guided Network (OGN) with an intuitive but effective fusion strategy for both two-stages pose estimation and Mask R-CNN. For two-stages pose estimation, a greedy box generation strategy is also proposed to keep more necessary candidates while performing person detection. For mask R-CNN, ratio-consistent is adopted to improve the generalization ability of the network. State-of-the-art results on COCO and PoseTrack dataset verify the effectiveness of our offset-guided pose estimation and tracking.



### End-to-End Learning of Geometric Deformations of Feature Maps for Virtual Try-On
- **Arxiv ID**: http://arxiv.org/abs/1906.01347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01347v2)
- **Published**: 2019-06-04 11:09:36+00:00
- **Updated**: 2019-06-10 20:13:13+00:00
- **Authors**: Thibaut Issenhuth, Jérémie Mary, Clément Calauzènes
- **Comment**: None
- **Journal**: None
- **Summary**: The 2D virtual try-on task has recently attracted a lot of interest from the research community, for its direct potential applications in online shopping as well as for its inherent and non-addressed scientific challenges. This task requires to fit an in-shop cloth image on the image of a person. It is highly challenging because it requires to warp the cloth on the target person while preserving its patterns and characteristics, and to compose the item with the person in a realistic manner. Current state-of-the-art models generate images with visible artifacts, due either to a pixel-level composition step or to the geometric transformation. In this paper, we propose WUTON: a Warping U-net for a Virtual Try-On system. It is a siamese U-net generator whose skip connections are geometrically transformed by a convolutional geometric matcher. The whole architecture is trained end-to-end with a multi-task loss including an adversarial one. This enables our network to generate and use realistic spatial transformations of the cloth to synthesize images of high visual quality. The proposed architecture can be trained end-to-end and allows us to advance towards a detail-preserving and photo-realistic 2D virtual try-on system. Our method outperforms the current state-of-the-art with visual results as well as with the Learned Perceptual Image Similarity (LPIPS) metric.



### State-aware Re-identification Feature for Multi-target Multi-camera Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.01357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01357v1)
- **Published**: 2019-06-04 11:43:02+00:00
- **Updated**: 2019-06-04 11:43:02+00:00
- **Authors**: Peng Li, Jiabin Zhang, Zheng Zhu, Yanwei Li, Lu Jiang, Guan Huang
- **Comment**: To appear in CVPR-2019 TRMTMCT Workshop
- **Journal**: None
- **Summary**: Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories from videos captured by a set of cameras. Recently, the tracking performance of MTMCT is significantly enhanced with the employment of re-identification (Re-ID) model. However, the appearance feature usually becomes unreliable due to the occlusion and orientation variance of the targets. Directly applying Re-ID model in MTMCT will encounter the problem of identity switches (IDS) and tracklet fragment caused by occlusion. To solve these problems, we propose a novel tracking framework in this paper. In this framework, the occlusion status and orientation information are utilized in Re-ID model with human pose information considered. In addition, the tracklet association using the proposed fused tracking feature is adopted to handle the fragment problem. The proposed tracker achieves 81.3\% IDF1 on the multiple-camera hard sequence, which outperforms all other reference methods by a large margin.



### KarNet: An Efficient Boolean Function Simplifier
- **Arxiv ID**: http://arxiv.org/abs/1906.01363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01363v1)
- **Published**: 2019-06-04 11:55:22+00:00
- **Updated**: 2019-06-04 11:55:22+00:00
- **Authors**: Shanka Subhra Mondal, Abhilash Nandy, Ritesh Agrawal, Debashis Sen
- **Comment**: 5 pages, 8 figures
- **Journal**: None
- **Summary**: Many approaches such as Quine-McCluskey algorithm, Karnaugh map solving, Petrick's method and McBoole's method have been devised to simplify Boolean expressions in order to optimize hardware implementation of digital circuits. However, the algorithmic implementations of these methods are hard-coded and also their computation time is proportional to the number of minterms involved in the expression. In this paper, we propose KarNet, where the ability of Convolutional Neural Networks to model relationships between various cell locations and values by capturing spatial dependencies is exploited to solve Karnaugh maps. In order to do so, a Karnaugh map is represented as an image signal, where each cell is considered as a pixel. Experimental results show that the computation time of KarNet is independent of the number of minterms and is of the order of one-hundredth to one-tenth that of the rule-based methods. KarNet being a learned system is found to achieve nearly a hundred percent accuracy, precision, and recall. We train KarNet to solve four variable Karnaugh maps and also show that a similar method can be applied on Karnaugh maps with more variables. Finally, we show a way to build a fully accurate and computationally fast system using KarNet.



### What, Where and How to Transfer in SAR Target Recognition Based on Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1906.01379v1
- **DOI**: 10.1109/TGRS.2019.2947634
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01379v1)
- **Published**: 2019-06-04 12:41:30+00:00
- **Updated**: 2019-06-04 12:41:30+00:00
- **Authors**: Zhongling Huang, Zongxu Pan, Bin Lei
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing 2019
- **Summary**: Deep convolutional neural networks (DCNNs) have attracted much attention in remote sensing recently. Compared with the large-scale annotated dataset in natural images, the lack of labeled data in remote sensing becomes an obstacle to train a deep network very well, especially in SAR image interpretation. Transfer learning provides an effective way to solve this problem by borrowing the knowledge from the source task to the target task. In optical remote sensing application, a prevalent mechanism is to fine-tune on an existing model pre-trained with a large-scale natural image dataset, such as ImageNet. However, this scheme does not achieve satisfactory performance for SAR application because of the prominent discrepancy between SAR and optical images. In this paper, we attempt to discuss three issues that are seldom studied before in detail: (1) what network and source tasks are better to transfer to SAR targets, (2) in which layer are transferred features more generic to SAR targets and (3) how to transfer effectively to SAR targets recognition. Based on the analysis, a transitive transfer method via multi-source data with domain adaptation is proposed in this paper to decrease the discrepancy between the source data and SAR targets. Several experiments are conducted on OpenSARShip. The results indicate that the universal conclusions about transfer learning in natural images cannot be completely applied to SAR targets, and the analysis of what and where to transfer in SAR target recognition is helpful to decide how to transfer more effectively.



### Semi- and Weakly-supervised Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1906.01399v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01399v1)
- **Published**: 2019-06-04 13:12:28+00:00
- **Updated**: 2019-06-04 13:12:28+00:00
- **Authors**: Norimichi Ukita, Yusuke Uematsu
- **Comment**: Revised preprint submitted to CVIU
- **Journal**: None
- **Summary**: For human pose estimation in still images, this paper proposes three semi- and weakly-supervised learning schemes. While recent advances of convolutional neural networks improve human pose estimation using supervised training data, our focus is to explore the semi- and weakly-supervised schemes. Our proposed schemes initially learn conventional model(s) for pose estimation from a small amount of standard training images with human pose annotations. For the first semi-supervised learning scheme, this conventional pose model detects candidate poses in training images with no human annotation. From these candidate poses, only true-positives are selected by a classifier using a pose feature representing the configuration of all body parts. The accuracies of these candidate pose estimation and true-positive pose selection are improved by action labels provided to these images in our second and third learning schemes, which are semi- and weakly-supervised learning. While the first and second learning schemes select only poses that are similar to those in the supervised training data, the third scheme selects more true-positive poses that are significantly different from any supervised poses. This pose selection is achieved by pose clustering using outlier pose detection with Dirichlet process mixtures and the Bayes factor. The proposed schemes are validated with large-scale human pose datasets.



### Active Object Manipulation Facilitates Visual Object Learning: An Egocentric Vision Study
- **Arxiv ID**: http://arxiv.org/abs/1906.01415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01415v1)
- **Published**: 2019-06-04 13:32:28+00:00
- **Updated**: 2019-06-04 13:32:28+00:00
- **Authors**: Satoshi Tsutsui, Dian Zhi, Md Alimoor Reza, David Crandall, Chen Yu
- **Comment**: Accepted at 2019 CVPR Workshop on Egocentric Perception, Interaction
  and Computing (EPIC)
- **Journal**: None
- **Summary**: Inspired by the remarkable ability of the infant visual learning system, a recent study collected first-person images from children to analyze the `training data' that they receive. We conduct a follow-up study that investigates two additional directions. First, given that infants can quickly learn to recognize a new object without much supervision (i.e. few-shot learning), we limit the number of training images. Second, we investigate how children control the supervision signals they receive during learning based on hand manipulation of objects. Our experimental results suggest that supervision with hand manipulation is better than without hands, and the trend is consistent even when a small number of images is available.



### Selective Style Transfer for Text
- **Arxiv ID**: http://arxiv.org/abs/1906.01466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01466v1)
- **Published**: 2019-06-04 14:16:19+00:00
- **Updated**: 2019-06-04 14:16:19+00:00
- **Authors**: Raul Gomez, Ali Furkan Biten, Lluis Gomez, Jaume Gibert, Marçal Rusiñol, Dimosthenis Karatzas
- **Comment**: Accepted in ICDAR 2019
- **Journal**: None
- **Summary**: This paper explores the possibilities of image style transfer applied to text maintaining the original transcriptions. Results on different text domains (scene text, machine printed text and handwritten text) and cross modal results demonstrate that this is feasible, and open different research lines. Furthermore, two architectures for selective style transfer, which means transferring style to only desired image pixels, are proposed. Finally, scene text selective style transfer is evaluated as a data augmentation technique to expand scene text detection datasets, resulting in a boost of text detectors performance. Our implementation of the described models is publicly available.



### What do AI algorithms actually learn? - On false structures in deep learning
- **Arxiv ID**: http://arxiv.org/abs/1906.01478v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01478v1)
- **Published**: 2019-06-04 14:35:32+00:00
- **Updated**: 2019-06-04 14:35:32+00:00
- **Authors**: Laura Thesing, Vegard Antun, Anders C. Hansen
- **Comment**: None
- **Journal**: None
- **Summary**: There are two big unsolved mathematical questions in artificial intelligence (AI): (1) Why is deep learning so successful in classification problems and (2) why are neural nets based on deep learning at the same time universally unstable, where the instabilities make the networks vulnerable to adversarial attacks. We present a solution to these questions that can be summed up in two words; false structures. Indeed, deep learning does not learn the original structures that humans use when recognising images (cats have whiskers, paws, fur, pointy ears, etc), but rather different false structures that correlate with the original structure and hence yield the success. However, the false structure, unlike the original structure, is unstable. The false structure is simpler than the original structure, hence easier to learn with less data and the numerical algorithm used in the training will more easily converge to the neural network that captures the false structure. We formally define the concept of false structures and formulate the solution as a conjecture. Given that trained neural networks always are computed with approximations, this conjecture can only be established through a combination of theoretical and computational results similar to how one establishes a postulate in theoretical physics (e.g. the speed of light is constant). Establishing the conjecture fully will require a vast research program characterising the false structures. We provide the foundations for such a program establishing the existence of the false structures in practice. Finally, we discuss the far reaching consequences the existence of the false structures has on state-of-the-art AI and Smale's 18th problem.



### Automatic Health Problem Detection from Gait Videos Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01480v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01480v2)
- **Published**: 2019-06-04 14:37:00+00:00
- **Updated**: 2020-01-27 13:47:56+00:00
- **Authors**: Rahil Mehrizi, Xi Peng, Shaoting Zhang, Ruisong Liao, Kang Li
- **Comment**: The claims in the conclusion are disproportionate to the merits of
  the research
- **Journal**: None
- **Summary**: The aim of this study is developing an automatic system for detection of gait-related health problems using Deep Neural Networks (DNNs). The proposed system takes a video of patients as the input and estimates their 3D body pose using a DNN based method. Our code is publicly available at https://github.com/rmehrizi/multi-view-pose-estimation. The resulting 3D body pose time series are then analyzed in a classifier, which classifies input gait videos into four different groups including Healthy, with Parkinsons disease, Post Stroke patient, and with orthopedic problems. The proposed system removes the requirement of complex and heavy equipment and large laboratory space, and makes the system practical for home use. Moreover, it does not need domain knowledge for feature engineering since it is capable of extracting semantic and high level features from the input data. The experimental results showed the classification accuracy of 56% to 96% for different groups. Furthermore, only 1 out of 25 healthy subjects were misclassified (False positive), and only 1 out of 70 patients were classified as a healthy subject (False negative). This study presents a starting point toward a powerful tool for automatic classification of gait disorders and can be used as a basis for future applications of Deep Learning in clinical gait analysis. Since the system uses digital cameras as the only required equipment, it can be employed in domestic environment of patients and elderly people for consistent gait monitoring and early detection of gait alterations.



### Constructing Energy-efficient Mixed-precision Neural Networks through Principal Component Analysis for Edge Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1906.01493v2
- **DOI**: 10.1038/s42256-019-0134-0
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.01493v2)
- **Published**: 2019-06-04 15:02:30+00:00
- **Updated**: 2019-12-03 02:05:07+00:00
- **Authors**: Indranil Chakraborty, Deboleena Roy, Isha Garg, Aayush Ankit, Kaushik Roy
- **Comment**: 14 pages, 4 figures, 8 tables
- **Journal**: Nature Machine Intelligence, 2, 43-55 (2020)
- **Summary**: The `Internet of Things' has brought increased demand for AI-based edge computing in applications ranging from healthcare monitoring systems to autonomous vehicles. Quantization is a powerful tool to address the growing computational cost of such applications, and yields significant compression over full-precision networks. However, quantization can result in substantial loss of performance for complex image classification tasks. To address this, we propose a Principal Component Analysis (PCA) driven methodology to identify the important layers of a binary network, and design mixed-precision networks. The proposed Hybrid-Net achieves a more than 10% improvement in classification accuracy over binary networks such as XNOR-Net for ResNet and VGG architectures on CIFAR-100 and ImageNet datasets while still achieving up to 94% of the energy-efficiency of XNOR-Nets. This work furthers the feasibility of using highly compressed neural networks for energy-efficient neural computing in edge devices.



### Text-based Editing of Talking-head Video
- **Arxiv ID**: http://arxiv.org/abs/1906.01524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01524v1)
- **Published**: 2019-06-04 15:35:16+00:00
- **Updated**: 2019-06-04 15:35:16+00:00
- **Authors**: Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, Maneesh Agrawala
- **Comment**: A version with higher resolution images can be downloaded from the
  authors' website
- **Journal**: None
- **Summary**: Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.



### Cross-Domain Cascaded Deep Feature Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.01526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01526v1)
- **Published**: 2019-06-04 15:37:09+00:00
- **Updated**: 2019-06-04 15:37:09+00:00
- **Authors**: Oren Katzir, Dani Lischinski, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years we have witnessed tremendous progress in unpaired image-to-image translation methods, propelled by the emergence of DNNs and adversarial training strategies. However, most existing methods focus on transfer of style and appearance, rather than on shape translation. The latter task is challenging, due to its intricate non-local nature, which calls for additional supervision. We mitigate this by descending the deep layers of a pre-trained network, where the deep features contain more semantics, and applying the translation from and between these deep features. Specifically, we leverage VGG, which is a classification network, pre-trained with large-scale semantic supervision. Our translation is performed in a cascaded, deep-to-shallow, fashion, along the deep feature hierarchy: we first translate between the deepest layers that encode the higher-level semantic content of the image, proceeding to translate the shallower layers, conditioned on the deeper ones. We show that our method is able to translate between different domains, which exhibit significantly different shapes. We evaluate our method both qualitatively and quantitatively and compare it to state-of-the-art image-to-image translation methods. Our code and trained models will be made available.



### Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy
- **Arxiv ID**: http://arxiv.org/abs/1906.01529v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01529v6)
- **Published**: 2019-06-04 15:40:53+00:00
- **Updated**: 2020-12-29 11:49:06+00:00
- **Authors**: Zhengwei Wang, Qi She, Tomas E. Ward
- **Comment**: Accepted by ACM Computing Surveys, 23 November 2020
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are: (1) the generation of high quality images, (2) diversity of image generation, and (3) stable training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state of the art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress towards addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress towards important computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Code related to GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.



### The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue
- **Arxiv ID**: http://arxiv.org/abs/1906.01530v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01530v2)
- **Published**: 2019-06-04 15:41:32+00:00
- **Updated**: 2019-06-26 17:36:47+00:00
- **Authors**: Janosch Haber, Tim Baumgärtner, Ece Takmaz, Lieke Gelderloos, Elia Bruni, Raquel Fernández
- **Comment**: Updates 26-06-2019: Changed caption sizes to comply with the ACL
  style guidelines and corrected some references
- **Journal**: None
- **Summary**: This paper introduces the PhotoBook dataset, a large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation. Taking inspiration from seminal work on dialogue analysis, we propose a data-collection task formulated as a collaborative game prompting two online participants to refer to images utilising both their visual context as well as previously established referring expressions. We provide a detailed description of the task setup and a thorough analysis of the 2,500 dialogues collected. To further illustrate the novel features of the dataset, we propose a baseline model for reference resolution which uses a simple method to take into account shared information accumulated in a reference chain. Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction.



### Visual Tree Convolutional Neural Network in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.01536v1
- **DOI**: 10.1109/ICPR.2018.8546126
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01536v1)
- **Published**: 2019-06-04 15:49:57+00:00
- **Updated**: 2019-06-04 15:49:57+00:00
- **Authors**: Yuntao Liu, Yong Dou, Ruochun Jin, Peng Qiao
- **Comment**: 7 pages, 2 figures, conference
- **Journal**: 2018 24th International Conference on Pattern Recognition (ICPR)
- **Summary**: In image classification, Convolutional Neural Network(CNN) models have achieved high performance with the rapid development in deep learning. However, some categories in the image datasets are more difficult to distinguished than others. Improving the classification accuracy on these confused categories is benefit to the overall performance. In this paper, we build a Confusion Visual Tree(CVT) based on the confused semantic level information to identify the confused categories. With the information provided by the CVT, we can lead the CNN training procedure to pay more attention on these confused categories. Therefore, we propose Visual Tree Convolutional Neural Networks(VT-CNN) based on the original deep CNN embedded with our CVT. We evaluate our VT-CNN model on the benchmark datasets CIFAR-10 and CIFAR-100. In our experiments, we build up 3 different VT-CNN models and they obtain improvement over their based CNN models by 1.36%, 0.89% and 0.64%, respectively.



### Localization in Aerial Imagery with Grid Maps using LocGAN
- **Arxiv ID**: http://arxiv.org/abs/1906.01540v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01540v2)
- **Published**: 2019-06-04 15:53:20+00:00
- **Updated**: 2019-07-16 14:48:16+00:00
- **Authors**: Haohao Hu, Junyi Zhu, Sascha Wirges, Martin Lauer
- **Comment**: 8 pages, 21 figures
- **Journal**: None
- **Summary**: In this work, we present LocGAN, our localization approach based on a geo-referenced aerial imagery and LiDAR grid maps. Currently, most self-localization approaches relate the current sensor observations to a map generated from previously acquired data. Unfortunately, this data is not always available and the generated maps are usually sensor setup specific. Global Navigation Satellite Systems (GNSS) can overcome this problem. However, they are not always reliable especially in urban areas due to multi-path and shadowing effects. Since aerial imagery is usually available, we can use it as prior information. To match aerial images with grid maps, we use conditional Generative Adversarial Networks (cGANs) which transform aerial images to the grid map domain. The transformation between the predicted and measured grid map is estimated using a localization network (LocNet). Given the geo-referenced aerial image transformation the vehicle pose can be estimated. Evaluations performed on the data recorded in region Karlsruhe, Germany show that our LocGAN approach provides reliable global localization results.



### Natural Vocabulary Emerges from Free-Form Annotations
- **Arxiv ID**: http://arxiv.org/abs/1906.01542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01542v1)
- **Published**: 2019-06-04 15:57:46+00:00
- **Updated**: 2019-06-04 15:57:46+00:00
- **Authors**: Jordi Pont-Tuset, Michael Gygli, Vittorio Ferrari
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for annotating object classes using free-form text written by undirected and untrained annotators. Free-form labeling is natural for annotators, they intuitively provide very specific and exhaustive labels, and no training stage is necessary. We first collect 729 labels on 15k images using 124 different annotators. Then we automatically enrich the structure of these free-form annotations by discovering a natural vocabulary of 4020 classes within them. This vocabulary represents the natural distribution of objects well and is learned directly from data, instead of being an educated guess done before collecting any labels. Hence, the natural vocabulary emerges from a large mass of free-form annotations. To do so, we (i) map the raw input strings to entities in an ontology of physical objects (which gives them an unambiguous meaning); and (ii) leverage inter-annotator co-occurrences, as well as biases and knowledge specific to individual annotators. Finally, we also automatically extract natural vocabularies of reduced size that have high object coverage while remaining specific. These reduced vocabularies represent the natural distribution of objects much better than commonly used predefined vocabularies. Moreover, they feature more uniform sample distribution over classes.



### Companion Surface of Danger Cylinder and its Role in Solution Variation of P3P Problem
- **Arxiv ID**: http://arxiv.org/abs/1906.08598v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08598v1)
- **Published**: 2019-06-04 16:09:23+00:00
- **Updated**: 2019-06-04 16:09:23+00:00
- **Authors**: Bo wang, Hao Hu, Caixia Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditionally the danger cylinder is intimately related to the solution stability in P3P problem. In this work, we show that the danger cylinder is also closely related to the multiple-solution phenomenon. More specifically, we show when the optical center lies on the danger cylinder, of the 3 possible P3P solutions, i.e., one double solution, and two other solutions, the optical center of the double solution still lies on the danger cylinder, but the optical centers of the other two solutions no longer lie on the danger cylinder. And when the optical center moves on the danger cylinder, accordingly the optical centers of the two other solutions of the corresponding P3P problem form a new surface, characterized by a polynomial equation of degree 12 in the optical center coordinates, called the Companion Surface of Danger Cylinder (CSDC). That means the danger cylinder always has a companion surface. For the significance of CSDC, we show that when the optical center passes through the CSDC, the number of solutions of P3P problem must change by 2. That means CSDC acts as a delimitating surface of the P3P solution space. These new findings shed some new lights on the P3P multi-solution phenomenon, an important issue in PnP study.



### Learning Rotation Adaptive Correlation Filters in Robust Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.01551v1
- **DOI**: 10.1007/978-3-030-20890-5_41
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01551v1)
- **Published**: 2019-06-04 16:10:49+00:00
- **Updated**: 2019-06-04 16:10:49+00:00
- **Authors**: Litu Rout, Priya Mariam Raju, Deepak Mishra, Rama Krishna Sai Subrahmanyam Gorthi
- **Comment**: Published in ACCV 2018
- **Journal**: ACCV: Asian Conference on Computer Vision 2018
- **Summary**: Visual object tracking is one of the major challenges in the field of computer vision. Correlation Filter (CF) trackers are one of the most widely used categories in tracking. Though numerous tracking algorithms based on CFs are available today, most of them fail to efficiently detect the object in an unconstrained environment with dynamically changing object appearance. In order to tackle such challenges, the existing strategies often rely on a particular set of algorithms. Here, we propose a robust framework that offers the provision to incorporate illumination and rotation invariance in the standard Discriminative Correlation Filter (DCF) formulation. We also supervise the detection stage of DCF trackers by eliminating false positives in the convolution response map. Further, we demonstrate the impact of displacement consistency on CF trackers. The generality and efficiency of the proposed framework is illustrated by integrating our contributions into two state-of-the-art CF trackers: SRDCF and ECO. As per the comprehensive experiments on the VOT2016 dataset, our top trackers show substantial improvement of 14.7% and 6.41% in robustness, 11.4% and 1.71% in Average Expected Overlap (AEO) over the baseline SRDCF and ECO, respectively.



### Disentangling neural mechanisms for perceptual grouping
- **Arxiv ID**: http://arxiv.org/abs/1906.01558v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.01558v2)
- **Published**: 2019-06-04 16:21:46+00:00
- **Updated**: 2020-10-28 15:53:55+00:00
- **Authors**: Junkyung Kim, Drew Linsley, Kalpit Thakkar, Thomas Serre
- **Comment**: Published in ICLR 2020
- **Journal**: None
- **Summary**: Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level "Gestalt" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on bottom-up connections. Horizontal connections resolve straining on tasks with Gestalt cues by supporting incremental grouping, whereas top-down connections rescue learning on tasks with high-level object cues by modifying coarse predictions about the position of the target object. Our findings dissociate the computational roles of bottom-up, horizontal and top-down connectivity, and demonstrate how a model featuring all of these interactions can more flexibly learn to form perceptual groups.



### GAMMA: A General Agent Motion Model for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1906.01566v6
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01566v6)
- **Published**: 2019-06-04 16:33:36+00:00
- **Updated**: 2022-04-26 12:43:18+00:00
- **Authors**: Yuanfu Luo, Panpan Cai, Yiyuan Lee, David Hsu
- **Comment**: This paper is accepted by IEEE RA-L
- **Journal**: None
- **Summary**: This paper presents GAMMA, a general motion prediction model that enables large-scale real-time simulation and planning for autonomous driving. GAMMA models heterogeneous, interactive traffic agents. They operate under diverse road conditions, with various geometric and kinematic constraints. GAMMA treats the prediction task as constrained optimization in traffic agents' velocity space. The objective is to optimize an agent's driving performance, while obeying all the constraints resulting from the agent's kinematics, collision avoidance with other agents, and the environmental context. Further, GAMMA explicitly conditions the prediction on human behavioral states as parameters of the optimization model, in order to account for versatile human behaviors. We evaluated GAMMA on a set of real-world benchmark datasets. The results show that GAMMA achieves high prediction accuracy on both homogeneous and heterogeneous traffic datasets, with sub-millisecond execution time. Further, the computational efficiency and the flexibility of GAMMA enable (i) simulation of mixed urban traffic at many locations worldwide and (ii) planning for autonomous driving in dense traffic with uncertain driver behaviors, both in real-time. The open-source code of GAMMA is available online.



### Photo-Geometric Autoencoding to Learn 3D Objects from Unlabelled Images
- **Arxiv ID**: http://arxiv.org/abs/1906.01568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01568v1)
- **Published**: 2019-06-04 16:36:07+00:00
- **Updated**: 2019-06-04 16:36:07+00:00
- **Authors**: Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi
- **Comment**: Appendix included, 17 pages. Project page:
  https://elliottwu.com/projects/unsup3d
- **Journal**: None
- **Summary**: We show that generative models can be used to capture visual geometry constraints statistically. We use this fact to infer the 3D shape of object categories from raw single-view images. Differently from prior work, we use no external supervision, nor do we use multiple views or videos of the objects. We achieve this by a simple reconstruction task, exploiting the symmetry of the objects' shape and albedo. Specifically, given a single image of the object seen from an arbitrary viewpoint, our model predicts a symmetric canonical view, the corresponding 3D shape and a viewpoint transformation, and trains with the goal of reconstructing the input view, resembling an auto-encoder. Our experiments show that this method can recover the 3D shape of human faces, cat faces, and cars from single view images, without supervision. On benchmarks, we demonstrate superior accuracy compared to other methods that use supervision at the level of 2D image correspondences.



### Dominant Set Clustering and Pooling for Multi-View 3D Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.01592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01592v1)
- **Published**: 2019-06-04 17:09:23+00:00
- **Updated**: 2019-06-04 17:09:23+00:00
- **Authors**: Chu Wang, Marcello Pelillo, Kaleem Siddiqi
- **Comment**: British Machine Vision Conference 2017
- **Journal**: None
- **Summary**: View based strategies for 3D object recognition have proven to be very successful. The state-of-the-art methods now achieve over 90% correct category level recognition performance on appearance images. We improve upon these methods by introducing a view clustering and pooling layer based on dominant sets. The key idea is to pool information from views which are similar and thus belong to the same cluster. The pooled feature vectors are then fed as inputs to the same layer, in a recurrent fashion. This recurrent clustering and pooling module, when inserted in an off-the-shelf pretrained CNN, boosts performance for multi-view 3D object recognition, achieving a new state of the art test set recognition accuracy of 93.8% on the ModelNet 40 database. We also explore a fast approximate learning strategy for our cluster-pooling CNN, which, while sacrificing end-to-end learning, greatly improves its training efficiency with only a slight reduction of recognition accuracy to 93.3%. Our implementation is available at https://github.com/fate3439/dscnn.



### Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations
- **Arxiv ID**: http://arxiv.org/abs/1906.01618v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2.10; I.4.5; I.4.8; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1906.01618v2)
- **Published**: 2019-06-04 17:53:11+00:00
- **Updated**: 2020-01-28 23:06:36+00:00
- **Authors**: Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein
- **Comment**: Video: https://youtu.be/6vMEBWD8O20 Project page:
  https://vsitzmann.github.io/srns/
- **Journal**: None
- **Summary**: Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.



### Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.01620v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01620v3)
- **Published**: 2019-06-04 17:54:20+00:00
- **Updated**: 2020-04-07 12:49:00+00:00
- **Authors**: Fredrik K. Gustafsson, Martin Danelljan, Thomas B. Schön
- **Comment**: CVPR Workshops 2020. Code is available at
  https://github.com/fregu856/evaluating_bdl
- **Journal**: None
- **Summary**: While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for example in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is specifically designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the first properly extensive and conclusive comparison of the two current state-of-the-art scalable methods: ensembling and MC-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates. Code is available at https://github.com/fregu856/evaluating_bdl.



### Traffic Light Recognition Using Deep Learning and Prior Maps for Autonomous Cars
- **Arxiv ID**: http://arxiv.org/abs/1906.11886v1
- **DOI**: 10.1109/IJCNN.2019.8851927
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11886v1)
- **Published**: 2019-06-04 18:05:25+00:00
- **Updated**: 2019-06-04 18:05:25+00:00
- **Authors**: Lucas C. Possatti, Rânik Guidolini, Vinicius B. Cardoso, Rodrigo F. Berriel, Thiago M. Paixão, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos
- **Comment**: Accepted in 2019 International Joint Conference on Neural Networks
  (IJCNN)
- **Journal**: None
- **Summary**: Autonomous terrestrial vehicles must be capable of perceiving traffic lights and recognizing their current states to share the streets with human drivers. Most of the time, human drivers can easily identify the relevant traffic lights. To deal with this issue, a common solution for autonomous cars is to integrate recognition with prior maps. However, additional solution is required for the detection and recognition of the traffic light. Deep learning techniques have showed great performance and power of generalization including traffic related problems. Motivated by the advances in deep learning, some recent works leveraged some state-of-the-art deep detectors to locate (and further recognize) traffic lights from 2D camera images. However, none of them combine the power of the deep learning-based detectors with prior maps to recognize the state of the relevant traffic lights. Based on that, this work proposes to integrate the power of deep learning-based detection with the prior maps used by our car platform IARA (acronym for Intelligent Autonomous Robotic Automobile) to recognize the relevant traffic lights of predefined routes. The process is divided in two phases: an offline phase for map construction and traffic lights annotation; and an online phase for traffic light recognition and identification of the relevant ones. The proposed system was evaluated on five test cases (routes) in the city of Vit\'oria, each case being composed of a video sequence and a prior map with the relevant traffic lights for the route. Results showed that the proposed technique is able to correctly identify the relevant traffic light along the trajectory.



### 4-D Scene Alignment in Surveillance Video
- **Arxiv ID**: http://arxiv.org/abs/1906.01675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.01675v2)
- **Published**: 2019-06-04 18:39:20+00:00
- **Updated**: 2019-06-06 14:16:19+00:00
- **Authors**: Robert Wagner, Daniel Crispell, Patrick Feeney, Joe Mundy
- **Comment**: None
- **Journal**: None
- **Summary**: Designing robust activity detectors for fixed camera surveillance video requires knowledge of the 3-D scene. This paper presents an automatic camera calibration process that provides a mechanism to reason about the spatial proximity between objects at different times. It combines a CNN-based camera pose estimator with a vertical scale provided by pedestrian observations to establish the 4-D scene geometry. Unlike some previous methods, the people do not need to be tracked nor do the head and feet need to be explicitly detected. It is robust to individual height variations and camera parameter estimation errors.



### DVDnet: A Fast Network for Deep Video Denoising
- **Arxiv ID**: http://arxiv.org/abs/1906.11890v1
- **DOI**: 10.1109/ICIP.2019.8803136
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11890v1)
- **Published**: 2019-06-04 21:06:33+00:00
- **Updated**: 2019-06-04 21:06:33+00:00
- **Authors**: Matias Tassano, Julie Delon, Thomas Veit
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a state-of-the-art video denoising algorithm based on a convolutional neural network architecture. Previous neural network based approaches to video denoising have been unsuccessful as their performance cannot compete with the performance of patch-based methods. However, our approach outperforms other patch-based competitors with significantly lower computing times. In contrast to other existing neural network denoisers, our algorithm exhibits several desirable properties such as a small memory footprint, and the ability to handle a wide range of noise levels with a single network model. The combination between its denoising performance and lower computational load makes this algorithm attractive for practical denoising applications. We compare our method with different state-of-art algorithms, both visually and with respect to objective quality metrics. The experiments show that our algorithm compares favorably to other state-of-art methods. Video examples, code and models are publicly available at \url{https://github.com/m-tassano/dvdnet}.



### Geo-Aware Networks for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.01737v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01737v2)
- **Published**: 2019-06-04 21:53:07+00:00
- **Updated**: 2019-09-04 21:56:58+00:00
- **Authors**: Grace Chu, Brian Potetz, Weijun Wang, Andrew Howard, Yang Song, Fernando Brucher, Thomas Leung, Hartwig Adam
- **Comment**: ICCVW 2019
- **Journal**: None
- **Summary**: Fine-grained recognition distinguishes among categories with subtle visual differences. In order to differentiate between these challenging visual categories, it is helpful to leverage additional information. Geolocation is a rich source of additional information that can be used to improve fine-grained classification accuracy, but has been understudied. Our contributions to this field are twofold. First, to the best of our knowledge, this is the first paper which systematically examined various ways of incorporating geolocation information into fine-grained image classification through the use of geolocation priors, post-processing or feature modulation. Secondly, to overcome the situation where no fine-grained dataset has complete geolocation information, we release two fine-grained datasets with geolocation by providing complementary information to existing popular datasets - iNaturalist and YFCC100M. By leveraging geolocation information we improve top-1 accuracy in iNaturalist from 70.1% to 79.0% for a strong baseline image-only model. Comparing several models, we found that best performance was achieved by a post-processing model that consumed the output of the image-only baseline alongside geolocation. However, for a resource-constrained model (MobileNetV2), performance was better with a feature modulation model that trains jointly over pixels and geolocation: accuracy increased from 59.6% to 72.2%. Our work makes a strong case for incorporating geolocation information in fine-grained recognition models for both server and on-device.



### An Introduction to Deep Morphological Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01751v2)
- **Published**: 2019-06-04 23:13:10+00:00
- **Updated**: 2021-07-09 18:33:01+00:00
- **Authors**: Keiller Nogueira, Jocelyn Chanussot, Mauro Dalla Mura, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: The recent impressive results of deep learning-based methods on computer vision applications brought fresh air to the research and industrial community. This success is mainly due to the process that allows those methods to learn data-driven features, generally based upon linear operations. However, in some scenarios, such operations do not have a good performance because of their inherited process that blurs edges, losing notions of corners, borders, and geometry of objects. Overcoming this, non-linear operations, such as morphological ones, may preserve such properties of the objects, being preferable and even state-of-the-art in some applications. Encouraged by this, in this work, we propose a novel network, called Deep Morphological Network (DeepMorphNet), capable of doing non-linear morphological operations while performing the feature learning process by optimizing the structuring elements. The DeepMorphNets can be trained and optimized end-to-end using traditional existing techniques commonly employed in the training of deep learning approaches. A systematic evaluation of the proposed algorithm is conducted using two synthetic and two traditional image classification datasets. Results show that the proposed DeepMorphNets is a promising technique that can learn distinct features when compared to the ones learned by current deep learning methods.



