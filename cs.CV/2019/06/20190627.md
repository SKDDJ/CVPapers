# Arxiv Papers in cs.CV on 2019-06-27
### Emergence of Exploratory Look-Around Behaviors through Active Observation Completion
- **Arxiv ID**: http://arxiv.org/abs/1906.11407v1
- **DOI**: 10.1126/scirobotics.aaw6326
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.11407v1)
- **Published**: 2019-06-27 01:39:51+00:00
- **Updated**: 2019-06-27 01:39:51+00:00
- **Authors**: Santhosh K. Ramakrishnan, Dinesh Jayaraman, Kristen Grauman
- **Comment**: Main paper 7 figures, supplementary 6 figures. Published in Science
  Robotics 2019
- **Journal**: None
- **Summary**: Standard computer vision systems assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is a major challenge in itself. We address the problem of learning to look around: how can an agent learn to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for reducing its uncertainty about the unobserved portions of its environment. Specifically, the agent is trained to select a short sequence of glimpses after which it must infer the appearance of its full environment. To address the challenge of sparse rewards, we further introduce sidekick policy learning, which exploits the asymmetry in observability between training and test time. The proposed methods learn observation policies that not only perform the completion task for which they are trained, but also generalize to exhibit useful "look-around" behavior for a range of active perception tasks.



### Few-Shot Video Classification via Temporal Alignment
- **Arxiv ID**: http://arxiv.org/abs/1906.11415v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11415v1)
- **Published**: 2019-06-27 03:04:16+00:00
- **Updated**: 2019-06-27 03:04:16+00:00
- **Authors**: Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing interest in learning a model which could recognize novel classes with only a few labeled examples. In this paper, we propose Temporal Alignment Module (TAM), a novel few-shot learning framework that can learn to classify a previous unseen video. While most previous works neglect long-term temporal ordering information, our proposed model explicitly leverages the temporal ordering information in video data through temporal alignment. This leads to strong data-efficiency for few-shot learning. In concrete, TAM calculates the distance value of query video with respect to novel class proxies by averaging the per frame distances along its alignment path. We introduce continuous relaxation to TAM so the model can be learned in an end-to-end fashion to directly optimize the few-shot learning objective. We evaluate TAM on two challenging real-world datasets, Kinetics and Something-Something-V2, and show that our model leads to significant improvement of few-shot video classification over a wide range of competitive baselines.



### Clustering by the way of atomic fission
- **Arxiv ID**: http://arxiv.org/abs/1906.11416v1
- **DOI**: 10.1109/ACCESS.2020.2987345
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11416v1)
- **Published**: 2019-06-27 03:05:13+00:00
- **Updated**: 2019-06-27 03:05:13+00:00
- **Authors**: Shizhan Lu
- **Comment**: 9 pages, 3 figures
- **Journal**: IEEE ACCESS 2020
- **Summary**: Cluster analysis which focuses on the grouping and categorization of similar elements is widely used in various fields of research. Inspired by the phenomenon of atomic fission, a novel density-based clustering algorithm is proposed in this paper, called fission clustering (FC). It focuses on mining the dense families of a dataset and utilizes the information of the distance matrix to fissure clustering dataset into subsets. When we face the dataset which has a few points surround the dense families of clusters, K-nearest neighbors local density indicator is applied to distinguish and remove the points of sparse areas so as to obtain a dense subset that is constituted by the dense families of clusters. A number of frequently-used datasets were used to test the performance of this clustering approach, and to compare the results with those of algorithms. The proposed algorithm is found to outperform other algorithms in speed and accuracy.



### Reconstructing Perceived Images from Brain Activity by Visually-guided Cognitive Representation and Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.12181v2
- **DOI**: None
- **Categories**: **cs.CV**, 97R40
- **Links**: [PDF](http://arxiv.org/pdf/1906.12181v2)
- **Published**: 2019-06-27 03:08:24+00:00
- **Updated**: 2019-10-22 15:03:48+00:00
- **Authors**: Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, Xinbo Gao
- **Comment**: 11 pages, 7 figures, 1 Table and Reference
- **Journal**: None
- **Summary**: Reconstructing visual stimulus (image) only from human brain activity measured with functional Magnetic Resonance Imaging (fMRI) is a significant and meaningful task in Human-AI collaboration. However, the inconsistent distribution and representation between fMRI signals and visual images cause the heterogeneity gap. Moreover, the fMRI data is often extremely high-dimensional and contains a lot of visually-irrelevant information. Existing methods generally suffer from these issues so that a satisfactory reconstruction is still challenging. In this paper, we show that it is possible to overcome these challenges by learning visually-guided cognitive latent representations from the fMRI signals, and inversely decoding them to the image stimuli. The resulting framework is called Dual-Variational Autoencoder/ Generative Adversarial Network (D-VAE/GAN), which combines the advantages of adversarial representation learning with knowledge distillation. In addition, we introduce a novel three-stage learning approach which enables the (cognitive) encoder to gradually distill useful knowledge from the paired (visual) encoder during the learning process. Extensive experimental results on both artificial and natural images have demonstrated that our method could achieve surprisingly good results and outperform all other alternatives.



### ELKPPNet: An Edge-aware Neural Network with Large Kernel Pyramid Pooling for Learning Discriminative Features in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.11428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11428v1)
- **Published**: 2019-06-27 03:58:45+00:00
- **Updated**: 2019-06-27 03:58:45+00:00
- **Authors**: Xianwei Zheng, Linxi Huan, Hanjiang Xiong, Jianya Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation has been a hot topic across diverse research fields. Along with the success of deep convolutional neural networks, semantic segmentation has made great achievements and improvements, in terms of both urban scene parsing and indoor semantic segmentation. However, most of the state-of-the-art models are still faced with a challenge in discriminative feature learning, which limits the ability of a model to detect multi-scale objects and to guarantee semantic consistency inside one object or distinguish different adjacent objects with similar appearance. In this paper, a practical and efficient edge-aware neural network is presented for semantic segmentation. This end-to-end trainable engine consists of a new encoder-decoder network, a large kernel spatial pyramid pooling (LKPP) block, and an edge-aware loss function. The encoder-decoder network was designed as a balanced structure to narrow the semantic and resolution gaps in multi-level feature aggregation, while the LKPP block was constructed with a densely expanding receptive field for multi-scale feature extraction and fusion. Furthermore, the new powerful edge-aware loss function is proposed to refine the boundaries directly from the semantic segmentation prediction for more robust and discriminative features. The effectiveness of the proposed model was demonstrated using Cityscapes, CamVid, and NYUDv2 benchmark datasets. The performance of the two structures and the edge-aware loss function in ELKPPNet was validated on the Cityscapes dataset, while the complete ELKPPNet was evaluated on the CamVid and NYUDv2 datasets. A comparative analysis with the state-of-the-art methods under the same conditions confirmed the superiority of the proposed algorithm.



### DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints
- **Arxiv ID**: http://arxiv.org/abs/1906.11435v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11435v2)
- **Published**: 2019-06-27 04:56:12+00:00
- **Updated**: 2019-06-28 05:54:46+00:00
- **Authors**: Liming Han, Yimin Lin, Guoguang Du, Shiguo Lian
- **Comment**: Accepted by IROS 2019, demo video:
  https://www.youtube.com/watch?v=fMeqCcpBCdM&feature=youtu.be
- **Journal**: None
- **Summary**: This paper presents an self-supervised deep learning network for monocular visual inertial odometry (named DeepVIO). DeepVIO provides absolute trajectory estimation by directly merging 2D optical flow feature (OFF) and Inertial Measurement Unit (IMU) data. Specifically, it firstly estimates the depth and dense 3D point cloud of each scene by using stereo sequences, and then obtains 3D geometric constraints including 3D optical flow and 6-DoF pose as supervisory signals. Note that such 3D optical flow shows robustness and accuracy to dynamic objects and textureless environments. In DeepVIO training, 2D optical flow network is constrained by the projection of its corresponding 3D optical flow, and LSTM-style IMU preintegration network and the fusion network are learned by minimizing the loss functions from ego-motion constraints. Furthermore, we employ an IMU status update scheme to improve IMU pose estimation through updating the additional gyroscope and accelerometer bias. The experimental results on KITTI and EuRoC datasets show that DeepVIO outperforms state-of-the-art learning based methods in terms of accuracy and data adaptability. Compared to the traditional methods, DeepVIO reduces the impacts of inaccurate Camera-IMU calibrations, unsynchronized and missing data.



### Hard Pixel Mining for Depth Privileged Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.11437v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11437v5)
- **Published**: 2019-06-27 05:01:04+00:00
- **Updated**: 2020-03-11 01:28:53+00:00
- **Authors**: Zhangxuan Gu, Li Niu, Haohua Zhao, Liqing Zhang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Semantic segmentation has achieved remarkable progress but remains challenging due to the complex scene, object occlusion, and so on. Some research works have attempted to use extra information such as a depth map to help RGB based semantic segmentation because the depth map could provide complementary geometric cues. However, due to the inaccessibility of depth sensors, depth information is usually unavailable for the test images. In this paper, we leverage only the depth of training images as the privileged information to mine the hard pixels in semantic segmentation, in which depth information is only available for training images but not available for test images. Specifically, we propose a novel Loss Weight Module, which outputs a loss weight map by employing two depth-related measurements of hard pixels: Depth Prediction Error and Depthaware Segmentation Error. The loss weight map is then applied to segmentation loss, with the goal of learning a more robust model by paying more attention to the hard pixels. Besides, we also explore a curriculum learning strategy based on the loss weight map. Meanwhile, to fully mine the hard pixels on different scales, we apply our loss weight module to multi-scale side outputs. Our hard pixels mining method achieves the state-of-the-art results on two benchmark datasets, and even outperforms the methods which need depth input during testing.



### Region Refinement Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.11443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11443v2)
- **Published**: 2019-06-27 05:45:44+00:00
- **Updated**: 2022-10-09 14:34:33+00:00
- **Authors**: Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Jiaze Wang, Ruiyu Li, Xiaoyong Shen, Jiaya Jia
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Albeit intensively studied, false prediction and unclear boundaries are still major issues of salient object detection. In this paper, we propose a Region Refinement Network (RRN), which recurrently filters redundant information and explicitly models boundary information for saliency detection. Different from existing refinement methods, we propose a Region Refinement Module (RRM) that optimizes salient region prediction by incorporating supervised attention masks in the intermediate refinement stages. The module only brings a minor increase in model size and yet significantly reduces false predictions from the background. To further refine boundary areas, we propose a Boundary Refinement Loss (BRL) that adds extra supervision for better distinguishing foreground from background. BRL is parameter free and easy to train. We further observe that BRL helps retain the integrity in prediction by refining the boundary. Extensive experiments on saliency detection datasets show that our refinement module and loss bring significant improvement to the baseline and can be easily applied to different frameworks. We also demonstrate that our proposed model generalizes well to portrait segmentation and shadow detection tasks.



### Teaching DNNs to design fast fashion
- **Arxiv ID**: http://arxiv.org/abs/1906.12159v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.12159v2)
- **Published**: 2019-06-27 06:48:45+00:00
- **Updated**: 2019-07-03 06:30:09+00:00
- **Authors**: Abhinav Ravi, Arun Patro, Vikram Garg, Anoop Kolar Rajagopal, Aruna Rajan, Rajdeep Hazra Banerjee
- **Comment**: 8 pages, 9 figures, KDD conference
- **Journal**: None
- **Summary**: $ $"Fast Fashion" spearheads the biggest disruption in fashion that enabled to engineer resilient supply chains to quickly respond to changing fashion trends. The conventional design process in commercial manufacturing is often fed through "trends" or prevailing modes of dressing around the world that indicate sudden interest in a new form of expression, cyclic patterns, and popular modes of expression for a given time frame. In this work, we propose a fully automated system to explore, detect, and finally synthesize trends in fashion into design elements by designing representative prototypes of apparel given time series signals generated from social media feeds. Our system is envisioned to be the first step in design of Fast Fashion where the production cycle for clothes from design inception to manufacturing is meant to be rapid and responsive to current "trends". It also works to reduce wastage in fashion production by taking in customer feedback on sellability at the time of design generation. We also provide an interface wherein the designers can play with multiple trending styles in fashion and visualize designs as interpolations of elements of these styles. We aim to aid the creative process through generating interesting and inspiring combinations for a designer to mull by running them through her key customers.



### Automatic Colon Polyp Detection using Region based Deep CNN and Post Learning Approaches
- **Arxiv ID**: http://arxiv.org/abs/1906.11463v1
- **DOI**: 10.1109/ACCESS.2018.2856402
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.11463v1)
- **Published**: 2019-06-27 07:18:44+00:00
- **Updated**: 2019-06-27 07:18:44+00:00
- **Authors**: Younghak Shin, Hemin Ali Qadir, Lars Aabakken, Jacob Bergsland, Ilangko Balasingham
- **Comment**: 9 pages
- **Journal**: IEEE Access 6 (2018): 40950-40962
- **Summary**: Automatic detection of colonic polyps is still an unsolved problem due to the large variation of polyps in terms of shape, texture, size, and color, and the existence of various polyp-like mimics during colonoscopy. In this study, we apply a recent region based convolutional neural network (CNN) approach for the automatic detection of polyps in images and videos obtained from colonoscopy examinations. We use a deep-CNN model (Inception Resnet) as a transfer learning scheme in the detection system. To overcome the polyp detection obstacles and the small number of polyp images, we examine image augmentation strategies for training deep networks. We further propose two efficient post-learning methods such as, automatic false positive learning and off-line learning, both of which can be incorporated with the region based detection system for reliable polyp detection. Using the large size of colonoscopy databases, experimental results demonstrate that the suggested detection systems show better performance compared to other systems in the literature. Furthermore, we show improved detection performance using the proposed post-learning schemes for colonoscopy videos.



### Loss Switching Fusion with Similarity Search for Video Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.11465v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11465v1)
- **Published**: 2019-06-27 07:20:09+00:00
- **Updated**: 2019-06-27 07:20:09+00:00
- **Authors**: Lei Wang, Du Q. Huynh, Moussa Reda Mansour
- **Comment**: Accepted by ICIP 2019
- **Journal**: None
- **Summary**: From video streaming to security and surveillance applications, video data play an important role in our daily living today. However, managing a large amount of video data and retrieving the most useful information for the user remain a challenging task. In this paper, we propose a novel video classification system that would benefit the scene understanding task. We define our classification problem as classifying background and foreground motions using the same feature representation for outdoor scenes. This means that the feature representation needs to be robust enough and adaptable to different classification tasks. We propose a lightweight Loss Switching Fusion Network (LSFNet) for the fusion of spatiotemporal descriptors and a similarity search scheme with soft voting to boost the classification performance. The proposed system has a variety of potential applications such as content-based video clustering, video filtering, etc. Evaluation results on two private industry datasets show that our system is robust in both classifying different background motions and detecting human motions from these background motions.



### Abnormal Colon Polyp Image Synthesis Using Conditional Adversarial Networks for Improved Detection Performance
- **Arxiv ID**: http://arxiv.org/abs/1906.11467v1
- **DOI**: 10.1109/ACCESS.2018.2872717
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11467v1)
- **Published**: 2019-06-27 07:26:07+00:00
- **Updated**: 2019-06-27 07:26:07+00:00
- **Authors**: Younghak Shin, Hemin Ali Qadir, Ilangko Balasingham
- **Comment**: 10 pages
- **Journal**: IEEE Access 6 (2018): 56007-56017
- **Summary**: One of the major obstacles in automatic polyp detection during colonoscopy is the lack of labeled polyp training images. In this paper, we propose a framework of conditional adversarial networks to increase the number of training samples by generating synthetic polyp images. Using a normal binary form of polyp mask which represents only the polyp position as an input conditioned image, realistic polyp image generation is a difficult task in a generative adversarial networks approach. We propose an edge filtering-based combined input conditioned image to train our proposed networks. This enables realistic polyp image generations while maintaining the original structures of the colonoscopy image frames. More importantly, our proposed framework generates synthetic polyp images from normal colonoscopy images which have the advantage of being relatively easy to obtain. The network architecture is based on the use of multiple dilated convolutions in each encoding part of our generator network to consider large receptive fields and avoid many contractions of a feature map size. An image resizing with convolution for upsampling in the decoding layers is considered to prevent artifacts on generated images. We show that the generated polyp images are not only qualitatively realistic but also help to improve polyp detection performance.



### Automatically Extract the Semi-transparent Motion-blurred Hand from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1906.11470v1
- **DOI**: 10.1109/LSP.2019.2939754
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11470v1)
- **Published**: 2019-06-27 07:32:53+00:00
- **Updated**: 2019-06-27 07:32:53+00:00
- **Authors**: Xiaomei Zhao, Yihong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: When we use video chat, video game, or other video applications, motion-blurred hands often appear. Accurately extracting these hands is very useful for video editing and behavior analysis. However, existing motion-blurred object extraction methods either need user interactions, such as user supplied trimaps and scribbles, or need additional information, such as background images. In this paper, a novel method which can automatically extract the semi-transparent motion-blurred hand just according to the original RGB image is proposed. The proposed method separates the extraction task into two subtasks: alpha matte prediction and foreground prediction. These two subtasks are implemented by Xception based encoder-decoder networks. The extracted motion-blurred hand images can be calculated by multiplying the predicted alpha mattes and foreground images. Experiments on synthetic and real datasets show that the proposed method has promising performance.



### A Convolutional Decoder for Point Clouds using Adaptive Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/1906.11478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.11478v1)
- **Published**: 2019-06-27 07:53:02+00:00
- **Updated**: 2019-06-27 07:53:02+00:00
- **Authors**: Isaak Lim, Moritz Ibing, Leif Kobbelt
- **Comment**: Symposium on Geometry Processing 2019
- **Journal**: Computer Graphics Forum 38 (5), 2019
- **Summary**: Automatic synthesis of high quality 3D shapes is an ongoing and challenging area of research. While several data-driven methods have been proposed that make use of neural networks to generate 3D shapes, none of them reach the level of quality that deep learning synthesis approaches for images provide. In this work we present a method for a convolutional point cloud decoder/generator that makes use of recent advances in the domain of image synthesis. Namely, we use Adaptive Instance Normalization and offer an intuition on why it can improve training. Furthermore, we propose extensions to the minimization of the commonly used Chamfer distance for auto-encoding point clouds. In addition, we show that careful sampling is important both for the input geometry and in our point cloud generation process to improve results. The results are evaluated in an auto-encoding setup to offer both qualitative and quantitative analysis. The proposed decoder is validated by an extensive ablation study and is able to outperform current state of the art results in a number of experiments. We show the applicability of our method in the fields of point cloud upsampling, single view reconstruction, and shape synthesis.



### Change Detection in Multi-temporal VHR Images Based on Deep Siamese Multi-scale Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.11479v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11479v2)
- **Published**: 2019-06-27 07:53:41+00:00
- **Updated**: 2020-07-10 13:43:03+00:00
- **Authors**: Hongruixuan Chen, Chen Wu, Bo Du, Liangpei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Very-high-resolution (VHR) images can provide abundant ground details and spatial geometric information. Change detection in multi-temporal VHR images plays a significant role in urban expansion and area internal change analysis. Nevertheless, traditional change detection methods can neither take full advantage of spatial context information nor cope with the complex internal heterogeneity of VHR images. In this paper, a powerful feature extraction model entitled multi-scale feature convolution unit (MFCU) is adopted for change detection in multi-temporal VHR images. MFCU can extract multi-scale spatial-spectral features in the same layer. Based on the unit two novel deep siamese convolutional neural networks, called as deep siamese multi-scale convolutional network (DSMS-CN) and deep siamese multi-scale fully convolutional network (DSMS-FCN), are designed for unsupervised and supervised change detection, respectively. For unsupervised change detection, an automatic pre-classification is implemented to obtain reliable training samples, then DSMS-CN fits the statistical distribution of changed and unchanged areas from selected training samples through MFCU modules and deep siamese architecture. For supervised change detection, the end-to-end deep fully convolutional network DSMS-FCN is trained in any size of multi-temporal VHR images, and directly outputs the binary change map. In addition, for the purpose of solving the inaccurate localization problem, the fully connected conditional random field (FC-CRF) is combined with DSMS-FCN to refine the results. The experimental results with challenging data sets confirm that the two proposed architectures perform better than the state-of-the-art methods.



### Automated Segmentation of Hip and Thigh Muscles in Metal Artifact-Contaminated CT using Convolutional Neural Network-Enhanced Normalized Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1906.11484v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1906.11484v1)
- **Published**: 2019-06-27 08:06:59+00:00
- **Updated**: 2019-06-27 08:06:59+00:00
- **Authors**: Mitsuki Sakamoto, Yuta Hiasa, Yoshito Otake, Masaki Takao, Yuki Suzuki, Nobuhiko Sugano, Yoshinobu Sato
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: In total hip arthroplasty, analysis of postoperative medical images is important to evaluate surgical outcome. Since Computed Tomography (CT) is most prevalent modality in orthopedic surgery, we aimed at the analysis of CT image. In this work, we focus on the metal artifact in postoperative CT caused by the metallic implant, which reduces the accuracy of segmentation especially in the vicinity of the implant. Our goal was to develop an automated segmentation method of the bones and muscles in the postoperative CT images. We propose a method that combines Normalized Metal Artifact Reduction (NMAR), which is one of the state-of-the-art metal artifact reduction methods, and a Convolutional Neural Network-based segmentation using two U-net architectures. The first U-net refines the result of NMAR and the muscle segmentation is performed by the second U-net. We conducted experiments using simulated images of 20 patients and real images of three patients to evaluate the segmentation accuracy of 19 muscles. In simulation study, the proposed method showed statistically significant improvement (p<0.05) in the average symmetric surface distance (ASD) metric for 14 muscles out of 19 muscles and the average ASD of all muscles from 1.17 +/- 0.543 mm (mean +/- std over all patients) to 1.10 +/- 0.509 mm over our previous method. The real image study using the manual trace of gluteus maximus and medius muscles showed ASD of 1.32 +/- 0.25 mm. Our future work includes training of a network in an end-to-end manner for both the metal artifact reduction and muscle segmentation.



### On the notion of number in humans and machines
- **Arxiv ID**: http://arxiv.org/abs/1906.12213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.12213v1)
- **Published**: 2019-06-27 09:26:22+00:00
- **Updated**: 2019-06-27 09:26:22+00:00
- **Authors**: Norbert Bátfai, Dávid Papp, Gergő Bogacsovics, Máté Szabó, Viktor Szilárd Simkó, Márió Bersenszki, Gergely Szabó, Lajos Kovács, Ferencz Kovács, Erik Szilveszter Varga
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we performed two types of software experiments to study the numerosity classification (subitizing) in humans and machines. Experiments focus on a particular kind of task is referred to as Semantic MNIST or simply SMNIST where the numerosity of objects placed in an image must be determined. The experiments called SMNIST for Humans are intended to measure the capacity of the Object File System in humans. In this type of experiment the measurement result is in well agreement with the value known from the cognitive psychology literature. The experiments called SMNIST for Machines serve similar purposes but they investigate existing, well known (but originally developed for other purpose) and under development deep learning computer programs. These measurement results can be interpreted similar to the results from SMNIST for Humans. The main thesis of this paper can be formulated as follows: in machines the image classification artificial neural networks can learn to distinguish numerosities with better accuracy when these numerosities are smaller than the capacity of OFS in humans. Finally, we outline a conceptual framework to investigate the notion of number in humans and machines.



### Effective Rotation-invariant Point CNN with Spherical Harmonics kernels
- **Arxiv ID**: http://arxiv.org/abs/1906.11555v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11555v2)
- **Published**: 2019-06-27 11:30:49+00:00
- **Updated**: 2019-09-10 14:53:54+00:00
- **Authors**: Adrien Poulenard, Marie-Julie Rakotosaona, Yann Ponty, Maks Ovsjanikov
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel rotation invariant architecture operating directly on point cloud data. We demonstrate how rotation invariance can be injected into a recently proposed point-based PCNN architecture, at all layers of the network, achieving invariance to both global shape transformations, and to local rotations on the level of patches or parts, useful when dealing with non-rigid objects. We achieve this by employing a spherical harmonics based kernel at different layers of the network, which is guaranteed to be invariant to rigid motions. We also introduce a more efficient pooling operation for PCNN using space-partitioning data-structures. This results in a flexible, simple and efficient architecture that achieves accurate results on challenging shape analysis tasks including classification and segmentation, without requiring data-augmentation, typically employed by non-invariant approaches.



### A New Benchmark Dataset for Texture Image Analysis and Surface Defect Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.11561v1
- **DOI**: 10.13140/RG.2.2.33612.46722
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11561v1)
- **Published**: 2019-06-27 11:36:29+00:00
- **Updated**: 2019-06-27 11:36:29+00:00
- **Authors**: Shervan Fekri-Ershad
- **Comment**: Texture Image Analysis, Benchmark Texture Dataset, Feature
  Extraction, Surface Defect Detection, Image Processing, Texture
  Classification, Visual Inspection Systems
- **Journal**: None
- **Summary**: Texture analysis plays an important role in many image processing applications to describe the image content or objects. On the other hand, visual surface defect detection is a highly research field in the computer vision. Surface defect refers to abnormalities in the texture of the surface. So, in this paper a dual purpose benchmark dataset is proposed for texture image analysis and surface defect detection titled stone texture image (STI dataset). The proposed benchmark dataset consist of 4 different class of stone texture images. The proposed benchmark dataset have some unique properties to make it very near to real applications. Local rotation, different zoom rates, unbalanced classes, variation of textures in size are some properties of the proposed dataset. In the result part, some descriptors are applied on this dataset to evaluate the proposed STI dataset in comparison with other state-of-the-art datasets.



### A PolSAR Scattering Power Factorization Framework and Novel Roll-Invariant Parameters Based Unsupervised Classification Scheme Using a Geodesic Distance
- **Arxiv ID**: http://arxiv.org/abs/1906.11577v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11577v1)
- **Published**: 2019-06-27 12:06:00+00:00
- **Updated**: 2019-06-27 12:06:00+00:00
- **Authors**: Debanshu Ratha, Eric Pottier, Avik Bhattacharya, Alejandro C. Frery
- **Comment**: Submitted to IEEE Transactions on Geoscience and Remote Sensing
- **Journal**: None
- **Summary**: We propose a generic Scattering Power Factorization Framework (SPFF) for Polarimetric Synthetic Aperture Radar (PolSAR) data to directly obtain $N$ scattering power components along with a residue power component for each pixel. Each scattering power component is factorized into similarity (or dissimilarity) using elementary targets and a generalized random volume model. The similarity measure is derived using a geodesic distance between pairs of $4\times4$ real Kennaugh matrices. In standard model-based decomposition schemes, the $3\times3$ Hermitian positive semi-definite covariance (or coherency) matrix is expressed as a weighted linear combination of scattering targets following a fixed hierarchical process. In contrast, under the proposed framework, a convex splitting of unity is performed to obtain the weights while preserving the dominance of the scattering components. The product of the total power (Span) with these weights provides the non-negative scattering power components. Furthermore, the framework along the geodesic distance is effectively used to obtain specific roll-invariant parameters which are then utilized to design an unsupervised classification scheme. The SPFF, the roll invariant parameters, and the classification results are assessed using C-band RADARSAT-2 and L-band ALOS-2 images of San Francisco.



### A shallow residual neural network to predict the visual cortex response
- **Arxiv ID**: http://arxiv.org/abs/1906.11578v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.11578v1)
- **Published**: 2019-06-27 12:06:55+00:00
- **Updated**: 2019-06-27 12:06:55+00:00
- **Authors**: Anne-Ruth José Meijer, Arnoud Visser
- **Comment**: 3 pages, 5 figures
- **Journal**: None
- **Summary**: Understanding how the visual cortex of the human brain really works is still an open problem for science today. A better understanding of natural intelligence could also benefit object-recognition algorithms based on convolutional neural networks. In this paper we demonstrate the asset of using a shallow residual neural network for this task. The benefit of this approach is that earlier stages of the network can be accurately trained, which allows us to add more layers at the earlier stage. With this additional layer the prediction of the visual brain activity improves from $10.4\%$ (block 1) to $15.53\%$ (last fully connected layer). By training the network for more than 10 epochs this improvement can become even larger.



### CaDIS: Cataract Dataset for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.11586v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11586v7)
- **Published**: 2019-06-27 12:24:03+00:00
- **Updated**: 2022-02-22 15:25:41+00:00
- **Authors**: Maria Grammatikopoulou, Evangello Flouty, Abdolrahim Kadkhodamohammadi, Gwenol'e Quellec, Andre Chow, Jean Nehme, Imanol Luengo, Danail Stoyanov
- **Comment**: None
- **Journal**: None
- **Summary**: Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labelled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos complementing the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts-semantic-segmentation2020.grand-challenge.org/.



### Curriculum Learning for Deep Generative Models with Clustering
- **Arxiv ID**: http://arxiv.org/abs/1906.11594v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11594v2)
- **Published**: 2019-06-27 12:44:09+00:00
- **Updated**: 2019-09-26 03:04:58+00:00
- **Authors**: Deli Zhao, Jiapeng Zhu, Zhenfang Guo, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Training generative models like Generative Adversarial Network (GAN) is challenging for noisy data. A novel curriculum learning algorithm pertaining to clustering is proposed to address this issue in this paper. The curriculum construction is based on the centrality of underlying clusters in data points. The data points of high centrality takes priority of being fed into generative models during training. To make our algorithm scalable to large-scale data, the active set is devised, in the sense that every round of training proceeds only on an active subset containing a small fraction of already trained data and the incremental data of lower centrality. Moreover, the geometric analysis is presented to interpret the necessity of cluster curriculum for generative models. The experiments on cat and human-face data validate that our algorithm is able to learn the optimal generative models (e.g. ProGAN) with respect to specified quality metrics for noisy data. An interesting finding is that the optimal cluster curriculum is closely related to the critical point of the geometric percolation process formulated in the paper.



### Dealing with Topological Information within a Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1906.11600v1
- **DOI**: 10.1007/978-3-030-01449-0_39
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11600v1)
- **Published**: 2019-06-27 13:05:48+00:00
- **Updated**: 2019-06-27 13:05:48+00:00
- **Authors**: Etienne Decencière, Santiago Velasco-Forero, Fu Min, Juanjuan Chen, Hélène Burdin, Gervais Gauthier, Bruno Laÿ, Thomas Bornschloegl, Thérèse Baldeweck
- **Comment**: International Conference on Advanced Concepts for Intelligent Vision
  Systems (ACIVS 2018)
- **Journal**: Advanced Concepts for Intelligent Vision Systems. ACIVS 2018.
  Lecture Notes in Computer Science, vol 11182. Springer, Cham
- **Summary**: A fully convolutional neural network has a receptive field of limited size and therefore cannot exploit global information, such as topological information. A solution is proposed in this paper to solve this problem, based on pre-processing with a geodesic operator. It is applied to the segmentation of histological images of pigmented reconstructed epidermis acquired via Whole Slide Imaging.



### Mind2Mind : transfer learning for GANs
- **Arxiv ID**: http://arxiv.org/abs/1906.11613v2
- **DOI**: 10.1007/978-3-030-80209-7
- **Categories**: **cs.LG**, cs.CV, stat.ML, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1906.11613v2)
- **Published**: 2019-06-27 13:19:29+00:00
- **Updated**: 2020-10-20 13:15:27+00:00
- **Authors**: Yaël Frégier, Jean-Baptiste Gouray
- **Comment**: 10 pages, 7 figures
- **Journal**: International Conference on Geometric Science of Information
  Springer 2021
- **Summary**: Training generative adversarial networks (GANs) on high quality (HQ) images involves important computing resources. This requirement represents a bottleneck for the development of applications of GANs. We propose a transfer learning technique for GANs that significantly reduces training time. Our approach consists of freezing the low-level layers of both the critic and generator of the original GAN. We assume an autoencoder constraint in order to ensure the compatibility of the internal representations of the critic and the generator. This assumption explains the gain in training time as it enables us to bypass the low-level layers during the forward and backward passes. We compare our method to baselines and observe a significant acceleration of the training. It can reach two orders of magnitude on HQ datasets when compared with StyleGAN. We prove rigorously, within the framework of optimal transport, a theorem ensuring the convergence of the learning of the transferred GAN. We moreover provide a precise bound for the convergence of the training in terms of the distance between the source and target dataset.



### SpliceRadar: A Learned Method For Blind Image Forensics
- **Arxiv ID**: http://arxiv.org/abs/1906.11663v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11663v1)
- **Published**: 2019-06-27 14:06:58+00:00
- **Updated**: 2019-06-27 14:06:58+00:00
- **Authors**: Aurobrata Ghosh, Zheng Zhong, Terrance E Boult, Maneesh Singh
- **Comment**: CVPR 2019, Workshop on Media Forensics, 8 pages
- **Journal**: None
- **Summary**: Detection and localization of image manipulations like splices are gaining in importance with the easy accessibility of image editing softwares. While detection generates a verdict for an image it provides no insight into the manipulation. Localization helps explain a positive detection by identifying the pixels of the image which have been tampered. We propose a deep learning based method for splice localization without prior knowledge of a test image's camera-model. It comprises a novel approach for learning rich filters and for suppressing image-edges. Additionally, we train our model on a surrogate task of camera model identification, which allows us to leverage large and widely available, unmanipulated, camera-tagged image databases. During inference, we assume that the spliced and host regions come from different camera-models and we segment these regions using a Gaussian-mixture model. Experiments on three test databases demonstrate results on par with and above the state-of-the-art and a good generalization ability to unknown datasets.



### Evolving Robust Neural Architectures to Defend from Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.11667v3
- **DOI**: None
- **Categories**: **cs.NE**, cs.AI, cs.CR, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11667v3)
- **Published**: 2019-06-27 14:12:52+00:00
- **Updated**: 2020-07-16 13:34:50+00:00
- **Authors**: Shashank Kotyan, Danilo Vasconcellos Vargas
- **Comment**: Pre-print of the published article in Proceedings of the Workshop on
  Artificial Intelligence Safety 2020, co-located with the 29th International
  Joint Conference on Artificial Intelligence and the 17th Pacific Rim
  International Conference on Artificial Intelligence (IJCAI-PRICAI 2020)
- **Journal**: None
- **Summary**: Neural networks are prone to misclassify slightly modified input images. Recently, many defences have been proposed, but none have improved the robustness of neural networks consistently. Here, we propose to use adversarial attacks as a function evaluation to search for neural architectures that can resist such attacks automatically. Experiments on neural architecture search algorithms from the literature show that although accurate, they are not able to find robust architectures. A significant reason for this lies in their limited search space. By creating a novel neural architecture search with options for dense layers to connect with convolution layers and vice-versa as well as the addition of concatenation layers in the search, we were able to evolve an architecture that is inherently accurate on adversarial samples. Interestingly, this inherent robustness of the evolved architecture rivals state-of-the-art defences such as adversarial training while being trained only on the non-adversarial samples. Moreover, the evolved architecture makes use of some peculiar traits which might be useful for developing even more robust ones. Thus, the results here confirm that more robust architectures exist as well as opens up a new realm of feasibilities for the development and exploration of neural networks.   Code available at http://bit.ly/RobustArchitectureSearch.



### Adversarial Pixel-Level Generation of Semantic Images
- **Arxiv ID**: http://arxiv.org/abs/1906.12195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.12195v1)
- **Published**: 2019-06-27 14:25:11+00:00
- **Updated**: 2019-06-27 14:25:11+00:00
- **Authors**: Emanuele Ghelfi, Paolo Galeone, Michele De Simoni, Federico Di Mattia
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have obtained extraordinary success in the generation of realistic images, a domain where a lower pixel-level accuracy is acceptable. We study the problem, not yet tackled in the literature, of generating semantic images starting from a prior distribution. Intuitively this problem can be approached using standard methods and architectures. However, a better-suited approach is needed to avoid generating blurry, hallucinated and thus unusable images since tasks like semantic segmentation require pixel-level exactness. In this work, we present a novel architecture for learning to generate pixel-level accurate semantic images, namely Semantic Generative Adversarial Networks (SemGANs). The experimental evaluation shows that our architecture outperforms standard ones from both a quantitative and a qualitative point of view in many semantic image generation tasks.



### GASP, a generalized framework for agglomerative clustering of signed graphs and its application to Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.11713v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11713v2)
- **Published**: 2019-06-27 15:00:16+00:00
- **Updated**: 2022-06-03 12:41:35+00:00
- **Authors**: Alberto Bailoni, Constantin Pape, Nathan Hütsch, Steffen Wolf, Thorsten Beier, Anna Kreshuk, Fred A. Hamprecht
- **Comment**: Published in CVPR 2022
- **Journal**: None
- **Summary**: We propose a theoretical framework that generalizes simple and fast algorithms for hierarchical agglomerative clustering to weighted graphs with both attractive and repulsive interactions between the nodes. This framework defines GASP, a Generalized Algorithm for Signed graph Partitioning, and allows us to explore many combinations of different linkage criteria and cannot-link constraints. We prove the equivalence of existing clustering methods to some of those combinations and introduce new algorithms for combinations that have not been studied before. We study both theoretical and empirical properties of these combinations and prove that some of these define an ultrametric on the graph. We conduct a systematic comparison of various instantiations of GASP on a large variety of both synthetic and existing signed clustering problems, in terms of accuracy but also efficiency and robustness to noise. Lastly, we show that some of the algorithms included in our framework, when combined with the predictions from a CNN model, result in a simple bottom-up instance segmentation pipeline. Going all the way from pixels to final segments with a simple procedure, we achieve state-of-the-art accuracy on the CREMI 2016 EM segmentation benchmark without requiring domain-specific superpixels.



### Using Intuition from Empirical Properties to Simplify Adversarial Training Defense
- **Arxiv ID**: http://arxiv.org/abs/1906.11729v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11729v1)
- **Published**: 2019-06-27 15:22:56+00:00
- **Updated**: 2019-06-27 15:22:56+00:00
- **Authors**: Guanxiong Liu, Issa Khalil, Abdallah Khreishah
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the surprisingly good representation power of complex distributions, neural network (NN) classifiers are widely used in many tasks which include natural language processing, computer vision and cyber security. In recent works, people noticed the existence of adversarial examples. These adversarial examples break the NN classifiers' underlying assumption that the environment is attack free and can easily mislead fully trained NN classifier without noticeable changes. Among defensive methods, adversarial training is a popular choice. However, original adversarial training with single-step adversarial examples (Single-Adv) can not defend against iterative adversarial examples. Although adversarial training with iterative adversarial examples (Iter-Adv) can defend against iterative adversarial examples, it consumes too much computational power and hence is not scalable. In this paper, we analyze Iter-Adv techniques and identify two of their empirical properties. Based on these properties, we propose modifications which enhance Single-Adv to perform competitively as Iter-Adv. Through preliminary evaluation, we show that the proposed method enhances the test accuracy of state-of-the-art (SOTA) Single-Adv defensive method against iterative adversarial examples by up to 16.93% while reducing its training cost by 28.75%.



### Demystifying Inter-Class Disentanglement
- **Arxiv ID**: http://arxiv.org/abs/1906.11796v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11796v3)
- **Published**: 2019-06-27 16:58:26+00:00
- **Updated**: 2020-02-18 18:56:58+00:00
- **Authors**: Aviv Gabbay, Yedid Hoshen
- **Comment**: ICLR 2020. Project page: http://www.vision.huji.ac.il/lord
- **Journal**: None
- **Summary**: Learning to disentangle the hidden factors of variations within a set of observations is a key task for artificial intelligence. We present a unified formulation for class and content disentanglement and use it to illustrate the limitations of current methods. We therefore introduce LORD, a novel method based on Latent Optimization for Representation Disentanglement. We find that latent optimization, along with an asymmetric noise regularization, is superior to amortized inference for achieving disentangled representations. In extensive experiments, our method is shown to achieve better disentanglement performance than both adversarial and non-adversarial methods that use the same level of supervision. We further introduce a clustering-based approach for extending our method for settings that exhibit in-class variation with promising results on the task of domain translation.



### More chemical detection through less sampling: amplifying chemical signals in hyperspectral data cubes through compressive sensing
- **Arxiv ID**: http://arxiv.org/abs/1906.11818v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1906.11818v1)
- **Published**: 2019-06-27 17:56:28+00:00
- **Updated**: 2019-06-27 17:56:28+00:00
- **Authors**: Henry Kvinge, Elin Farnell, Julia R. Dupuis, Michael Kirby, Chris Peterson, Elizabeth C. Schundler
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Compressive sensing (CS) is a method of sampling which permits some classes of signals to be reconstructed with high accuracy even when they were under-sampled. In this paper we explore a phenomenon in which bandwise CS sampling of a hyperspectral data cube followed by reconstruction can actually result in amplification of chemical signals contained in the cube. Perhaps most surprisingly, chemical signal amplification generally seems to increase as the level of sampling decreases. In some examples, the chemical signal is significantly stronger in a data cube reconstructed from 10% CS sampling than it is in the raw, 100% sampled data cube. We explore this phenomenon in two real-world datasets including the Physical Sciences Inc. Fabry-P\'{e}rot interferometer sensor multispectral dataset and the Johns Hopkins Applied Physics Lab FTIR-based longwave infrared sensor hyperspectral dataset. Each of these datasets contains the release of a chemical simulant, such as glacial acetic acid, triethyl phospate, and sulfur hexafluoride, and in all cases we use the adaptive coherence estimator (ACE) to detect a target signal in the hyperspectral data cube. We end the paper by suggesting some theoretical justifications for why chemical signals would be amplified in CS sampled and reconstructed hyperspectral data cubes and discuss some practical implications.



### HEMELB Acceleration and Visualization for Cerebral Aneurysms
- **Arxiv ID**: http://arxiv.org/abs/1906.11925v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.11925v1)
- **Published**: 2019-06-27 19:23:57+00:00
- **Updated**: 2019-06-27 19:23:57+00:00
- **Authors**: Sahar Soheilian Esfahani, Xiaojun Zhai, Minsi Chen, Abbes Amira, Faycal Bensaali, Julien AbiNahed, Sarada Dakua, Georges Younes, Robin A. Richardson, Peter V. Coveney
- **Comment**: None
- **Journal**: None
- **Summary**: A weakness in the wall of a cerebral artery causing a dilation or ballooning of the blood vessel is known as a cerebral aneurysm. Optimal treatment requires fast and accurate diagnosis of the aneurysm. HemeLB is a fluid dynamics solver for complex geometries developed to provide neurosurgeons with information related to the flow of blood in and around aneurysms. On a cost efficient platform, HemeLB could be employed in hospitals to provide surgeons with the simulation results in real-time. In this work, we developed an improved version of HemeLB for GPU implementation and result visualization. A visualization platform for smooth interaction with end users is also presented. Finally, a comprehensive evaluation of this implementation is reported. The results demonstrate that the proposed implementation achieves a maximum performance of 15,168,964 site updates per second, and is capable of speeding up HemeLB for deployment in hospitals and clinical investigations.



### Homography from two orientation- and scale-covariant features
- **Arxiv ID**: http://arxiv.org/abs/1906.11927v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11927v1)
- **Published**: 2019-06-27 19:34:08+00:00
- **Updated**: 2019-06-27 19:34:08+00:00
- **Authors**: Daniel Barath, Zuzana Kukelova
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a geometric interpretation of the angles and scales which the orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two new general constraints are derived on the scales and rotations which can be used in any geometric model estimation tasks. Using these formulas, two new constraints on homography estimation are introduced. Exploiting the derived equations, a solver for estimating the homography from the minimal number of two correspondences is proposed. Also, it is shown how the normalization of the point correspondences affects the rotation and scale parameters, thus achieving numerically stable results. Due to requiring merely two feature pairs, robust estimators, e.g. RANSAC, do significantly fewer iterations than by using the four-point algorithm. When using covariant features, e.g. SIFT, the information about the scale and orientation is given at no cost. The proposed homography estimation method is tested in a synthetic environment and on publicly available real-world datasets.



### Datasets for Face and Object Detection in Fisheye Images
- **Arxiv ID**: http://arxiv.org/abs/1906.11942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.11942v1)
- **Published**: 2019-06-27 20:21:45+00:00
- **Updated**: 2019-06-27 20:21:45+00:00
- **Authors**: Jianglin Fu, Ivan V. Bajic, Rodney G. Vaughan
- **Comment**: None
- **Journal**: None
- **Summary**: We present two new fisheye image datasets for training face and object detection models: VOC-360 and Wider-360. The fisheye images are created by post-processing regular images collected from two well-known datasets, VOC2012 and Wider Face, using a model for mapping regular to fisheye images implemented in Matlab. VOC-360 contains 39,575 fisheye images for object detection, segmentation, and classification. Wider-360 contains 63,897 fisheye images for face detection. These datasets will be useful for developing face and object detectors as well as segmentation modules for fisheye images while the efforts to collect and manually annotate true fisheye images are underway.



### Supervise Thyself: Examining Self-Supervised Representations in Interactive Environments
- **Arxiv ID**: http://arxiv.org/abs/1906.11951v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11951v1)
- **Published**: 2019-06-27 20:38:47+00:00
- **Updated**: 2019-06-27 20:38:47+00:00
- **Authors**: Evan Racah, Christopher Pal
- **Comment**: Accepted to the 2019 ICML Workshop on Self-Supervised Learning
- **Journal**: None
- **Summary**: Self-supervised methods, wherein an agent learns representations solely by observing the results of its actions, become crucial in environments which do not provide a dense reward signal or have labels. In most cases, such methods are used for pretraining or auxiliary tasks for "downstream" tasks, such as control, exploration, or imitation learning. However, it is not clear which method's representations best capture meaningful features of the environment, and which are best suited for which types of environments. We present a small-scale study of self-supervised methods on two visual environments: Flappy Bird and Sonic The Hedgehog. In particular, we quantitatively evaluate the representations learned from these tasks in two contexts: a) the extent to which the representations capture true state information of the agent and b) how generalizable these representations are to novel situations, like new levels and textures. Lastly, we evaluate these self-supervised features by visualizing which parts of the environment they focus on. Our results show that the utility of the representations is highly dependent on the visuals and dynamics of the environment.



### Studying the Impact of Mood on Identifying Smartphone Users
- **Arxiv ID**: http://arxiv.org/abs/1906.11960v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.11960v1)
- **Published**: 2019-06-27 20:55:16+00:00
- **Updated**: 2019-06-27 20:55:16+00:00
- **Authors**: Khadija Zanna, Sayde King, Tempestt Neal, Shaun Canavan
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the identification of smartphone users when certain samples collected while the subject felt happy, upset or stressed were absent or present. We employ data from 19 subjects using the StudentLife dataset, a dataset collected by researchers at Dartmouth College that was originally collected to correlate behaviors characterized by smartphone usage patterns with changes in stress and academic performance. Although many previous works on behavioral biometrics have implied that mood is a source of intra-person variation which may impact biometric performance, our results contradict this assumption. Our findings show that performance worsens when removing samples that were generated when subjects may be happy, upset, or stressed. Thus, there is no indication that mood negatively impacts performance. However, we do find that changes existing in smartphone usage patterns may correlate with mood, including changes in locking, audio, location, calling, homescreen, and e-mail habits. Thus, we show that while mood is a source of intra-person variation, it may be an inaccurate assumption that biometric systems (particularly, mobile biometrics) are likely influenced by mood.



### A Utility-Preserving GAN for Face Obscuration
- **Arxiv ID**: http://arxiv.org/abs/1906.11979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11979v1)
- **Published**: 2019-06-27 22:01:27+00:00
- **Updated**: 2019-06-27 22:01:27+00:00
- **Authors**: Hanxiang Hao, David Güera, Amy R. Reibman, Edward J. Delp
- **Comment**: 6 pages, 5 figures, presented at the ICML 2019 Worksop on Synthetic
  Realities: Deep Learning for Detecting AudioVisual Fakes
- **Journal**: None
- **Summary**: From TV news to Google StreetView, face obscuration has been used for privacy protection. Due to recent advances in the field of deep learning, obscuration methods such as Gaussian blurring and pixelation are not guaranteed to conceal identity. In this paper, we propose a utility-preserving generative model, UP-GAN, that is able to provide an effective face obscuration, while preserving facial utility. By utility-preserving we mean preserving facial features that do not reveal identity, such as age, gender, skin tone, pose, and expression. We show that the proposed method achieves the best performance in terms of obscuration and utility preservation.



### Optimizing CNN-based Hyperspectral Image Classification on FPGAs
- **Arxiv ID**: http://arxiv.org/abs/1906.11834v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11834v1)
- **Published**: 2019-06-27 22:05:22+00:00
- **Updated**: 2019-06-27 22:05:22+00:00
- **Authors**: Shuanglong Liu, Ringo S. W. Chu, Xiwei Wang, Wayne Luk
- **Comment**: This article is accepted for publication at ARC'2019
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) classification has been widely adopted in applications involving remote sensing imagery analysis which require high classification accuracy and real-time processing speed. Methods based on Convolutional neural networks (CNNs) have been proven to achieve state-of-the-art accuracy in classifying HSIs. However, CNN models are often too computationally intensive to achieve real-time response due to the high dimensional nature of HSI, compared to traditional methods such as Support Vector Machines (SVMs). Besides, previous CNN models used in HSI are not specially designed for efficient implementation on embedded devices such as FPGAs. This paper proposes a novel CNN-based algorithm for HSI classification which takes into account hardware efficiency. A customized architecture which enables the proposed algorithm to be mapped effectively onto FPGA resources is then proposed to support real-time on-board classification with low power consumption. Implementation results show that our proposed accelerator on a Xilinx Zynq 706 FPGA board achieves more than 70x faster than an Intel 8-core Xeon CPU and 3x faster than an NVIDIA GeForce 1080 GPU. Compared to previous SVM-based FPGA accelerators, we achieve comparable processing speed but provide a much higher classification accuracy.



### Convolution Based Spectral Partitioning Architecture for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.11981v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11981v1)
- **Published**: 2019-06-27 22:06:15+00:00
- **Updated**: 2019-06-27 22:06:15+00:00
- **Authors**: Ringo S. W. Chu, Ho-Cheung Ng, Xiwei Wang, Wayne Luk
- **Comment**: Accepted for publication in IGARSS'2019
- **Journal**: None
- **Summary**: Hyperspectral images (HSIs) can distinguish materials with high number of spectral bands, which is widely adopted in remote sensing applications and benefits in high accuracy land cover classifications. However, HSIs processing are tangled with the problem of high dimensionality and limited amount of labelled data. To address these challenges, this paper proposes a deep learning architecture using three dimensional convolutional neural networks with spectral partitioning to perform effective feature extraction. We conduct experiments using Indian Pines and Salinas scenes acquired by NASA Airborne Visible/Infra-Red Imaging Spectrometer. In comparison to prior results, our architecture shows competitive performance for classification results over current methods.



### BTEL: A Binary Tree Encoding Approach for Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1906.11992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.11992v1)
- **Published**: 2019-06-27 23:27:06+00:00
- **Updated**: 2019-06-27 23:27:06+00:00
- **Authors**: Huu Le, Tuan Hoang, Michael Milford
- **Comment**: Accepted to IROS 2019
- **Journal**: None
- **Summary**: Visual localization algorithms have achieved significant improvements in performance thanks to recent advances in camera technology and vision-based techniques. However, there remains one critical caveat: all current approaches that are based on image retrieval currently scale at best linearly with the size of the environment with respect to both storage, and consequentially in most approaches, query time. This limitation severely curtails the capability of autonomous systems in a wide range of compute, power, storage, size, weight or cost constrained applications such as drones. In this work, we present a novel binary tree encoding approach for visual localization which can serve as an alternative for existing quantization and indexing techniques. The proposed tree structure allows us to derive a compressed training scheme that achieves sub-linearity in both required storage and inference time. The encoding memory can be easily configured to satisfy different storage constraints. Moreover, our approach is amenable to an optional sequence filtering mechanism to further improve the localization results, while maintaining the same amount of storage. Our system is entirely agnostic to the front-end descriptors, allowing it to be used on top of recent state-of-the-art image representations. Experimental results show that the proposed method significantly outperforms state-of-the-art approaches under limited storage constraints.



