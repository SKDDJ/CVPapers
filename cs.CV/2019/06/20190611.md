# Arxiv Papers in cs.CV on 2019-06-11
### Inferring 3D Shapes from Image Collections using Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04910v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04910v1)
- **Published**: 2019-06-11 00:28:19+00:00
- **Updated**: 2019-06-11 00:28:19+00:00
- **Authors**: Matheus Gadelha, Aartika Rai, Subhransu Maji, Rui Wang
- **Comment**: Source code: https://github.com/matheusgadelha/PrGAN . arXiv admin
  note: substantial text overlap with arXiv:1612.05872
- **Journal**: None
- **Summary**: We investigate the problem of learning a probabilistic distribution over three-dimensional shapes given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach called projective generative adversarial network (PrGAN) trains a deep generative model of 3D shapes whose projections (or renderings) match the distributions of the provided 2D distribution. The addition of a differentiable projection module allows us to infer the underlying 3D shape distribution without access to any explicit 3D or viewpoint annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained directly on 3D data. %for a number of shape categoriesincluding chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage of our model is that it estimates 3D shape, viewpoint, and generates novel views from an input image in a completely unsupervised manner. We further investigate how the generative models can be improved if additional information such as depth, viewpoint or part segmentations is available at training time. To this end, we present new differentiable projection operators that can be used by PrGAN to learn better 3D generative models. Our experiments show that our method can successfully leverage extra visual cues to create more diverse and accurate shapes.



### Multiscale Nakagami parametric imaging for improved liver tumor localization
- **Arxiv ID**: http://arxiv.org/abs/1906.04333v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1906.04333v1)
- **Published**: 2019-06-11 00:48:45+00:00
- **Updated**: 2019-06-11 00:48:45+00:00
- **Authors**: Omar S. Al-Kadi
- **Comment**: IEEE International Conference on Image Processing (ICIP), USA, pp.
  3384-3388, 2016
- **Journal**: None
- **Summary**: Effective ultrasound tissue characterization is usually hindered by complex tissue structures. The interlacing of speckle patterns complicates the correct estimation of backscatter distribution parameters. Nakagami parametric imaging based on localized shape parameter mapping can model different backscattering conditions. However, performance of the constructed Nakagami image depends on the sensitivity of the estimation method to the backscattered statistics and scale of analysis. Using a fixed focal region of interest in estimating the Nakagami parametric image would increase estimation variance. In this work, localized Nakagami parameters are estimated adaptively by means of maximum likelihood estimation on a multiscale basis. The varying size kernel integrates the goodness-of-fit of the backscattering distribution parameters at multiple scales for more stable parameter estimation. Results show improved quantitative visualization of changes in tissue specular reflections, suggesting a potential approach for improving tumor localization in low contrast ultrasound images.



### FAMED-Net: A Fast and Accurate Multi-scale End-to-end Dehazing Network
- **Arxiv ID**: http://arxiv.org/abs/1906.04334v2
- **DOI**: 10.1109/TIP.2019.2922837
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04334v2)
- **Published**: 2019-06-11 00:51:40+00:00
- **Updated**: 2019-07-07 06:22:31+00:00
- **Authors**: Jing Zhang, Dacheng Tao
- **Comment**: 13 pages, 9 figures, To appear in IEEE Transactions on Image
  Processing. The code is available at https://github.com/chaimi2013/FAMED-Net
- **Journal**: None
- **Summary**: Single image dehazing is a critical image pre-processing step for subsequent high-level computer vision tasks. However, it remains challenging due to its ill-posed nature. Existing dehazing models tend to suffer from model overcomplexity and computational inefficiency or have limited representation capacity. To tackle these challenges, here we propose a fast and accurate multi-scale end-to-end dehazing network called FAMED-Net, which comprises encoders at three scales and a fusion module to efficiently and directly learn the haze-free image. Each encoder consists of cascaded and densely connected point-wise convolutional layers and pooling layers. Since no larger convolutional kernels are used and features are reused layer-by-layer, FAMED-Net is lightweight and computationally efficient. Thorough empirical studies on public synthetic datasets (including RESIDE) and real-world hazy images demonstrate the superiority of FAMED-Net over other representative state-of-the-art models with respect to model complexity, computational efficiency, restoration accuracy, and cross-set generalization. The code will be made publicly available.



### SALT: Subspace Alignment as an Auxiliary Learning Task for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.04338v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04338v2)
- **Published**: 2019-06-11 01:20:12+00:00
- **Updated**: 2019-12-19 02:10:38+00:00
- **Authors**: Kowshik Thopalli, Jayaraman J. Thiagarajan, Rushil Anirudh, Pavan Turaga
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to transfer and adapt knowledge learned from a labeled source domain to an unlabeled target domain. Key components of unsupervised domain adaptation include: (a) maximizing performance on the target, and (b) aligning the source and target domains. Traditionally, these tasks have either been considered as separate, or assumed to be implicitly addressed together with high-capacity feature extractors. When considered separately, alignment is usually viewed as a problem of aligning data distributions, either through geometric approaches such as subspace alignment or through distributional alignment such as optimal transport. This paper represents a hybrid approach, where we assume simplified data geometry in the form of subspaces, and consider alignment as an auxiliary task to the primary task of maximizing performance on the source. The alignment is made rather simple by leveraging tractable data geometry in the form of subspaces. We synergistically allow certain parameters derived from the closed-form auxiliary solution, to be affected by gradients from the primary task. The proposed approach represents a unique fusion of geometric and model-based alignment with gradients from a data-driven primary task. Our approach termed SALT, is a simple framework that achieves comparable or sometimes outperforms state-of-the-art on multiple standard benchmarks.



### Hybrid Function Sparse Representation towards Image Super Resolution
- **Arxiv ID**: http://arxiv.org/abs/1906.04363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04363v1)
- **Published**: 2019-06-11 03:04:45+00:00
- **Updated**: 2019-06-11 03:04:45+00:00
- **Authors**: Junyi Bian, Baojun Lin, Ke Zhang
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: Sparse representation with training-based dictionary has been shown successful on super resolution(SR) but still have some limitations. Based on the idea of making the magnification of function curve without losing its fidelity, we proposed a function based dictionary on sparse representation for super resolution, called hybrid function sparse representation (HFSR). The dictionary we designed is directly generated by preset hybrid functions without additional training, which can be scaled to any size as is required due to its scalable property. We mixed approximated Heaviside function (AHF), sine function and DCT function as the dictionary. Multi-scale refinement is then proposed to utilize the scalable property of the dictionary to improve the results. In addition, a reconstruct strategy is adopted to deal with the overlaps. The experiments on Set14 SR dataset show that our method has an excellent performance particularly with regards to images containing rich details and contexts compared with non-learning based state-of-the art methods.



### Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1906.04375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04375v1)
- **Published**: 2019-06-11 03:35:25+00:00
- **Updated**: 2019-06-11 03:35:25+00:00
- **Authors**: Junchao Zhang, Yuxin Peng
- **Comment**: 10 pages, accepted by IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: Video captioning aims to automatically generate natural language descriptions of video content, which has drawn a lot of attention recent years. Generating accurate and fine-grained captions needs to not only understand the global content of video, but also capture the detailed object information. Meanwhile, video representations have great impact on the quality of generated captions. Thus, it is important for video captioning to capture salient objects with their detailed temporal dynamics, and represent them using discriminative spatio-temporal representations. In this paper, we propose a new video captioning approach based on object-aware aggregation with bidirectional temporal graph (OA-BTG), which captures detailed temporal dynamics for salient objects in video, and learns discriminative spatio-temporal representations by performing object-aware local feature aggregation on detected object regions. The main novelties and advantages are: (1) Bidirectional temporal graph: A bidirectional temporal graph is constructed along and reversely along the temporal order, which provides complementary ways to capture the temporal trajectories for each salient object. (2) Object-aware aggregation: Learnable VLAD (Vector of Locally Aggregated Descriptors) models are constructed on object temporal trajectories and global frame sequence, which performs object-aware aggregation to learn discriminative representations. A hierarchical attention mechanism is also developed to distinguish different contributions of multiple objects. Experiments on two widely-used datasets demonstrate our OA-BTG achieves state-of-the-art performance in terms of BLEU@4, METEOR and CIDEr metrics.



### Recognizing License Plates in Real-Time
- **Arxiv ID**: http://arxiv.org/abs/1906.04376v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04376v6)
- **Published**: 2019-06-11 03:45:49+00:00
- **Updated**: 2022-11-02 16:04:38+00:00
- **Authors**: Xuewen Yang, Xin Wang
- **Comment**: License Plate Detection and Recognition, Computer Vision, Supervised
  Learning
- **Journal**: None
- **Summary**: License plate detection and recognition (LPDR) is of growing importance for enabling intelligent transportation and ensuring the security and safety of the cities. However, LPDR faces a big challenge in a practical environment. The license plates can have extremely diverse sizes, fonts and colors, and the plate images are usually of poor quality caused by skewed capturing angles, uneven lighting, occlusion, and blurring. In applications such as surveillance, it often requires fast processing. To enable real-time and accurate license plate recognition, in this work, we propose a set of techniques: 1) a contour reconstruction method along with edge-detection to quickly detect the candidate plates; 2) a simple zero-one-alternation scheme to effectively remove the fake top and bottom borders around plates to facilitate more accurate segmentation of characters on plates; 3) a set of techniques to augment the training data, incorporate SIFT features into the CNN network, and exploit transfer learning to obtain the initial parameters for more effective training; and 4) a two-phase verification procedure to determine the correct plate at low cost, a statistical filtering in the plate detection stage to quickly remove unwanted candidates, and the accurate CR results after the CR process to perform further plate verification without additional processing. We implement a complete LPDR system based on our algorithms. The experimental results demonstrate that our system can accurately recognize license plate in real-time. Additionally, it works robustly under various levels of illumination and noise, and in the presence of car movement. Compared to peer schemes, our system is not only among the most accurate ones but is also the fastest, and can be easily applied to other scenarios.



### PAN: Projective Adversarial Network for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.04378v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04378v1)
- **Published**: 2019-06-11 03:51:09+00:00
- **Updated**: 2019-06-11 03:51:09+00:00
- **Authors**: Naji Khosravan, Aliasghar Mortazi, Michael Wallace, Ulas Bagci
- **Comment**: Accepted for presentation in MICCAI 2019
- **Journal**: None
- **Summary**: Adversarial learning has been proven to be effective for capturing long-range and high-level label consistencies in semantic segmentation. Unique to medical imaging, capturing 3D semantics in an effective yet computationally efficient way remains an open problem. In this study, we address this computational burden by proposing a novel projective adversarial network, called PAN, which incorporates high-level 3D information through 2D projections. Furthermore, we introduce an attention module into our framework that helps for a selective integration of global information directly from our segmentor to our adversarial network. For the clinical application we chose pancreas segmentation from CT scans. Our proposed framework achieved state-of-the-art performance without adding to the complexity of the segmentor.



### Band Attention Convolutional Networks For Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.04379v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04379v1)
- **Published**: 2019-06-11 03:56:20+00:00
- **Updated**: 2019-06-11 03:56:20+00:00
- **Authors**: Hongwei Dong, Lamei Zhang, Bin Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Redundancy and noise exist in the bands of hyperspectral images (HSIs). Thus, it is a good property to be able to select suitable parts from hundreds of input bands for HSIs classification methods. In this letter, a band attention module (BAM) is proposed to implement the deep learning based HSIs classification with the capacity of band selection or weighting. The proposed BAM can be seen as a plug-and-play complementary component of the existing classification networks which fully considers the adverse effects caused by the redundancy of the bands when using convolutional neural networks (CNNs) for HSIs classification. Unlike most of deep learning methods used in HSIs, the band attention module which is customized according to the characteristics of hyperspectral images is embedded in the ordinary CNNs for better performance. At the same time, unlike classical band selection or weighting methods, the proposed method achieves the end-to-end training instead of the separated stages. Experiments are carried out on two HSI benchmark datasets. Compared to some classical and advanced deep learning methods, numerical simulations under different evaluation criteria show that the proposed method have good performance. Last but not least, some advanced CNNs are combined with the proposed BAM for better performance.



### Different Approaches for Human Activity Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1906.05074v1
- **DOI**: 10.1016/j.jnca.2020.102738
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.05074v1)
- **Published**: 2019-06-11 04:12:15+00:00
- **Updated**: 2019-06-11 04:12:15+00:00
- **Authors**: Zawar Hussain, Michael Sheng, Wei Emma Zhang
- **Comment**: 28
- **Journal**: None
- **Summary**: Human activity recognition has gained importance in recent years due to its applications in various fields such as health, security and surveillance, entertainment, and intelligent environments. A significant amount of work has been done on human activity recognition and researchers have leveraged different approaches, such as wearable, object-tagged, and device-free, to recognize human activities. In this article, we present a comprehensive survey of the work conducted over the period 2010-2018 in various areas of human activity recognition with main focus on device-free solutions. The device-free approach is becoming very popular due to the fact that the subject is not required to carry anything, instead, the environment is tagged with devices to capture the required information. We propose a new taxonomy for categorizing the research work conducted in the field of activity recognition and divide the existing literature into three sub-areas: action-based, motion-based, and interaction-based. We further divide these areas into ten different sub-topics and present the latest research work in these sub-topics. Unlike previous surveys which focus only on one type of activities, to the best of our knowledge, we cover all the sub-areas in activity recognition and provide a comparison of the latest research work in these sub-areas. Specifically, we discuss the key attributes and design approaches for the work presented. Then we provide extensive analysis based on 10 important metrics, to give the reader, a complete overview of the state-of-the-art techniques and trends in different sub-areas of human activity recognition. In the end, we discuss open research issues and provide future research directions in the field of human activity recognition.



### Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks
- **Arxiv ID**: http://arxiv.org/abs/1906.04392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.04392v1)
- **Published**: 2019-06-11 04:55:18+00:00
- **Updated**: 2019-06-11 04:55:18+00:00
- **Authors**: Ziang Yan, Yiwen Guo, Changshui Zhang
- **Comment**: 10 pages + 3 pages supplementary material
- **Journal**: None
- **Summary**: Unlike the white-box counterparts that are widely studied and readily accessible, adversarial examples in black-box settings are generally more Herculean on account of the difficulty of estimating gradients. Many methods achieve the task by issuing numerous queries to target classification systems, which makes the whole procedure costly and suspicious to the systems. In this paper, we aim at reducing the query complexity of black-box attacks in this category. We propose to exploit gradients of a few reference models which arguably span some promising search subspaces. Experimental results show that, in comparison with the state-of-the-arts, our method can gain up to 2x and 4x reductions in the requisite mean and medium numbers of queries with much lower failure rates even if the reference models are trained on a small and inadequate dataset disjoint to the one for training the victim model. Code and models for reproducing our results will be made publicly available.



### Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1906.04402v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04402v2)
- **Published**: 2019-06-11 05:48:32+00:00
- **Updated**: 2019-07-17 04:49:18+00:00
- **Authors**: Yale Song, Mohammad Soleymani
- **Comment**: CVPR 2019. Includes supplementary material. Have updated results on
  TGIF and MRW
- **Journal**: None
- **Summary**: Visual-semantic embedding aims to find a shared latent space where related visual and textual instances are close to each other. Most current methods learn injective embedding functions that map an instance to a single point in the shared space. Unfortunately, injective embedding cannot effectively handle polysemous instances with multiple possible meanings; at best, it would find an average representation of different meanings. This hinders its use in real-world scenarios where individual instances and their cross-modal associations are often ambiguous. In this work, we introduce Polysemous Instance Embedding Networks (PIE-Nets) that compute multiple and diverse representations of an instance by combining global context with locally-guided features via multi-head self-attention and residual learning. To learn visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in the multiple instance learning framework. Most existing work on cross-modal retrieval focuses on image-text data. Here, we also tackle a more challenging case of video-text retrieval. To facilitate further research in video-text retrieval, we release a new dataset of 50K video-sentence pairs collected from social media, dubbed MRW (my reaction when). We demonstrate our approach on both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our new MRW dataset.



### iProStruct2D: Identifying protein structural classes by deep learning via 2D representations
- **Arxiv ID**: http://arxiv.org/abs/1906.04407v1
- **DOI**: 10.1016/j.eswa.2019.113019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04407v1)
- **Published**: 2019-06-11 06:26:13+00:00
- **Updated**: 2019-06-11 06:26:13+00:00
- **Authors**: Loris Nanni, Alessandra Lumini, Federica Pasquali, Sheryl Brahnam
- **Comment**: 9 pages, 3 figures, 4 tables
- **Journal**: Expert Systems With Applications 2020, 142, (March), 113019
- **Summary**: In this paper we address the problem of protein classification starting from a multi-view 2D representation of proteins. From each 3D protein structure, a large set of 2D projections is generated using the protein visualization software Jmol. This set of multi-view 2D representations includes 13 different types of protein visualizations that emphasize specific properties of protein structure (e.g., a backbone visualization that displays the backbone structure of the protein as a trace of the C{\alpha} atom). Each type of representation is used to train a different Convolutional Neural Network (CNN), and the fusion of these CNNs is shown to be able to exploit the diversity of different types of representations to improve classification performance. In addition, several multi-view projections are obtained by uniformly rotating the protein structure around its central X, Y, and Z viewing axes to produce 125 images. This approach can be considered a data augmentation method for improving the performance of the classifier and can be used in both the training and the testing phases. Experimental evaluation of the proposed approach on two datasets demonstrates the strength of the proposed method with respect to the other state-of-the-art approaches. The MATLAB code used in this paper is available at https://github.com/LorisNanni.



### Few-Shot Point Cloud Region Annotation with Human in the Loop
- **Arxiv ID**: http://arxiv.org/abs/1906.04409v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04409v1)
- **Published**: 2019-06-11 06:34:13+00:00
- **Updated**: 2019-06-11 06:34:13+00:00
- **Authors**: Siddhant Jain, Sowmya Munukutla, David Held
- **Comment**: presented at 2019 ICML Workshop on Human in the Loop Learning (HILL
  2019), Long Beach, USA
- **Journal**: None
- **Summary**: We propose a point cloud annotation framework that employs human-in-loop learning to enable the creation of large point cloud datasets with per-point annotations. Sparse labels from a human annotator are iteratively propagated to generate a full segmentation of the network by fine-tuning a pre-trained model of an allied task via a few-shot learning paradigm. We show that the proposed framework significantly reduces the amount of human interaction needed in annotating point clouds, without sacrificing on the quality of the annotations. Our experiments also suggest the suitability of the framework in annotating large datasets by noting a reduction in human interaction as the number of full annotations completed by the system increases. Finally, we demonstrate the flexibility of the framework to support multiple different annotations of the same point cloud enabling the creation of datasets with different granularities of annotation.



### Deep learning analysis of coronary arteries in cardiac CT angiography for detection of patients requiring invasive coronary angiography
- **Arxiv ID**: http://arxiv.org/abs/1906.04419v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04419v2)
- **Published**: 2019-06-11 07:38:26+00:00
- **Updated**: 2019-11-10 13:23:46+00:00
- **Authors**: Majd Zreik, Robbert W. van Hamersvelt, Nadieh Khalili, Jelmer M. Wolterink, Michiel Voskuil, Max A. Viergever, Tim Leiner, Ivana Išgum
- **Comment**: This work has been accepted to IEEE TMI for publication
- **Journal**: None
- **Summary**: In patients with obstructive coronary artery disease, the functional significance of a coronary artery stenosis needs to be determined to guide treatment. This is typically established through fractional flow reserve (FFR) measurement, performed during invasive coronary angiography (ICA). We present a method for automatic and non-invasive detection of patients requiring ICA, employing deep unsupervised analysis of complete coronary arteries in cardiac CT angiography (CCTA) images. We retrospectively collected CCTA scans of 187 patients, 137 of them underwent invasive FFR measurement in 192 different coronary arteries. These FFR measurements served as a reference standard for the functional significance of the coronary stenosis. The centerlines of the coronary arteries were extracted and used to reconstruct straightened multi-planar reformatted (MPR) volumes. To automatically identify arteries with functionally significant stenosis that require ICA, each MPR volume was encoded into a fixed number of encodings using two disjoint 3D and 1D convolutional autoencoders performing spatial and sequential encodings, respectively. Thereafter, these encodings were employed to classify arteries using a support vector machine classifier. The detection of coronary arteries requiring invasive evaluation, evaluated using repeated cross-validation experiments, resulted in an area under the receiver operating characteristic curve of $0.81 \pm 0.02$ on the artery-level, and $0.87 \pm 0.02$ on the patient-level. The results demonstrate the feasibility of automatic non-invasive detection of patients that require ICA and possibly subsequent coronary artery intervention. This could potentially reduce the number of patients that unnecessarily undergo ICA.



### NAS-FCOS: Fast Neural Architecture Search for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.04423v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04423v4)
- **Published**: 2019-06-11 07:55:41+00:00
- **Updated**: 2020-02-25 08:05:47+00:00
- **Authors**: Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen, Yanning Zhang
- **Comment**: 9 pages, 9 figures, accepted by CVPR-2020
- **Journal**: None
- **Summary**: The success of deep neural networks relies on significant architecture engineering. Recently neural architecture search (NAS) has emerged as a promise to greatly reduce manual effort in network design by automatically searching for optimal architectures, although typically such algorithms need an excessive amount of computational resources, e.g., a few thousand GPU-days. To date, on challenging vision tasks such as object detection, NAS, especially fast versions of NAS, is less studied. Here we propose to search for the decoder structure of object detectors with search efficiency being taken into consideration. To be more specific, we aim to efficiently search for the feature pyramid network (FPN) as well as the prediction head of a simple anchor-free object detector, namely FCOS, using a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms and strategies for evaluating network quality, we are able to efficiently search a top-performing detection architecture within 4 days using 8 V100 GPUs. The discovered architecture surpasses state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS for object detection.



### On the Vector Space in Photoplethysmography Imaging
- **Arxiv ID**: http://arxiv.org/abs/1906.04431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04431v1)
- **Published**: 2019-06-11 08:08:10+00:00
- **Updated**: 2019-06-11 08:08:10+00:00
- **Authors**: Christian S. Pilz, Vladimir Blazek, Steffen Leonhardt
- **Comment**: None
- **Journal**: None
- **Summary**: We study the vector space of visible wavelength intensities from face videos widely used as input features in Photoplethysmography Imaging (PPGI). Based upon theoretical principles of Group invariance in the Euclidean space we derive a change of the topology where the corresponding distance between successive measurements is defined as geodesic on a Riemannian manifold. This lower dimensional embedding of the sensor signal unifies the invariance properties with respect to translation of the features as discussed by several former approaches. The resulting operator acts implicit on the feature space without requiring any kind of prior knowledge and does not need parameter tuning. The resulting feature's time varying quasi-periodic shaping naturally occurs in form of the canonical state space representation according to the known Diffusion process of blood volume changes. The computational complexity is low and the implementation becomes fairly simple. During experiments the operator achieved robust and competitive estimation performance of heart rate from face videos on two public databases.



### A Novel Cost Function for Despeckling using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04441v1
- **DOI**: 10.1109/JURSE.2019.8809042
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04441v1)
- **Published**: 2019-06-11 08:42:58+00:00
- **Updated**: 2019-06-11 08:42:58+00:00
- **Authors**: Giampaolo Ferraioli, Vito Pascazio, Sergio Vitale
- **Comment**: Accepted on JURSE 2019 - Joint Urban Remote Sensing Event
- **Journal**: 2019 Joint Urban Remote Sensing Event (JURSE), Vannes, France,
  2019, pp. 1-4
- **Summary**: Removing speckle noise from SAR images is still an open issue. It is well know that the interpretation of SAR images is very challenging and despeckling algorithms are necessary to improve the ability of extracting information. An urban environment makes this task more heavy due to different structures and to different objects scale. Following the recent spread of deep learning methods related to several remote sensing applications, in this work a convolutional neural networks based algorithm for despeckling is proposed. The network is trained on simulated SAR data. The paper is mainly focused on the implementation of a cost function that takes account of both spatial consistency of image and statistical properties of noise.



### Single Image Blind Deblurring Using Multi-Scale Latent Structure Prior
- **Arxiv ID**: http://arxiv.org/abs/1906.04442v1
- **DOI**: 10.1109/TCSVT.2019.2919159
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04442v1)
- **Published**: 2019-06-11 08:43:47+00:00
- **Updated**: 2019-06-11 08:43:47+00:00
- **Authors**: Yuanchao Bai, Huizhu Jia, Ming Jiang, Xianming Liu, Xiaodong Xie, Wen Gao
- **Comment**: To appear in IEEE Transactions on Circuits and Systems for Video
  Technology, 2019; Image downsampling makes a good prior for fast blind image
  deblurring
- **Journal**: None
- **Summary**: Blind image deblurring is a challenging problem in computer vision, which aims to restore both the blur kernel and the latent sharp image from only a blurry observation. Inspired by the prevalent self-example prior in image super-resolution, in this paper, we observe that a coarse enough image down-sampled from a blurry observation is approximately a low-resolution version of the latent sharp image. We prove this phenomenon theoretically and define the coarse enough image as a latent structure prior of the unknown sharp image. Starting from this prior, we propose to restore sharp images from the coarsest scale to the finest scale on a blurry image pyramid, and progressively update the prior image using the newly restored sharp image. These coarse-to-fine priors are referred to as \textit{Multi-Scale Latent Structures} (MSLS). Leveraging the MSLS prior, our algorithm comprises two phases: 1) we first preliminarily restore sharp images in the coarse scales; 2) we then apply a refinement process in the finest scale to obtain the final deblurred image. In each scale, to achieve lower computational complexity, we alternately perform a sharp image reconstruction with fast local self-example matching, an accelerated kernel estimation with error compensation, and a fast non-blind image deblurring, instead of computing any computationally expensive non-convex priors. We further extend the proposed algorithm to solve more challenging non-uniform blind image deblurring problem. Extensive experiments demonstrate that our algorithm achieves competitive results against the state-of-the-art methods with much faster running speed.



### Bag of Color Features For Color Constancy
- **Arxiv ID**: http://arxiv.org/abs/1906.04445v1
- **DOI**: 10.1109/TIP.2020.3004921
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04445v1)
- **Published**: 2019-06-11 08:47:49+00:00
- **Updated**: 2019-06-11 08:47:49+00:00
- **Authors**: Firas Laakom, Nikolaos Passalis, Jenni Raitoharju, Jarno Nikkanen, Anastasios Tefas, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: 12 pages, 5 figures, 6 tables
- **Journal**: IEEE Transactions on Image Processing ( Volume: 29 ) 2020
- **Summary**: In this paper, we propose a novel color constancy approach, called Bag of Color Features (BoCF), building upon Bag-of-Features pooling. The proposed method substantially reduces the number of parameters needed for illumination estimation. At the same time, the proposed method is consistent with the color constancy assumption stating that global spatial information is not relevant for illumination estimation and local information ( edges, etc.) is sufficient. Furthermore, BoCF is consistent with color constancy statistical approaches and can be interpreted as a learning-based generalization of many statistical approaches. To further improve the illumination estimation accuracy, we propose a novel attention mechanism for the BoCF model with two variants based on self-attention. BoCF approach and its variants achieve competitive, compared to the state of the art, results while requiring much fewer parameters on three benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.



### Compressed Sensing MRI via a Multi-scale Dilated Residual Convolution Network
- **Arxiv ID**: http://arxiv.org/abs/1906.05251v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05251v1)
- **Published**: 2019-06-11 09:05:57+00:00
- **Updated**: 2019-06-11 09:05:57+00:00
- **Authors**: Yuxiang Dai, Peixian Zhuang
- **Comment**: 27 pages and 13 figures
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) reconstruction is an active inverse problem which can be addressed by conventional compressed sensing (CS) MRI algorithms that exploit the sparse nature of MRI in an iterative optimization-based manner. However, two main drawbacks of iterative optimization-based CSMRI methods are time-consuming and are limited in model capacity. Meanwhile, one main challenge for recent deep learning-based CSMRI is the trade-off between model performance and network size. To address the above issues, we develop a new multi-scale dilated network for MRI reconstruction with high speed and outstanding performance. Comparing to convolutional kernels with same receptive fields, dilated convolutions reduce network parameters with smaller kernels and expand receptive fields of kernels to obtain almost same information. To maintain the abundance of features, we present global and local residual learnings to extract more image edges and details. Then we utilize concatenation layers to fuse multi-scale features and residual learnings for better reconstruction. Compared with several non-deep and deep learning CSMRI algorithms, the proposed method yields better reconstruction accuracy and noticeable visual improvements. In addition, we perform the noisy setting to verify the model stability, and then extend the proposed model on a MRI super-resolution task.



### TW-SMNet: Deep Multitask Learning of Tele-Wide Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1906.04463v1
- **DOI**: 10.1109/ACCESS.2020.3029085
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04463v1)
- **Published**: 2019-06-11 09:46:26+00:00
- **Updated**: 2019-06-11 09:46:26+00:00
- **Authors**: Mostafa El-Khamy, Haoyu Ren, Xianzhi Du, Jungwon Lee
- **Comment**: None
- **Journal**: Multitask Deep Neural Networks for Tele-Wide Stereo Matching, IEEE
  Access, 2020
- **Summary**: In this paper, we introduce the problem of estimating the real world depth of elements in a scene captured by two cameras with different field of views, where the first field of view (FOV) is a Wide FOV (WFOV) captured by a wide angle lens, and the second FOV is contained in the first FOV and is captured by a tele zoom lens. We refer to the problem of estimating the inverse depth for the union of FOVs, while leveraging the stereo information in the overlapping FOV, as Tele-Wide Stereo Matching (TW-SM). We propose different deep learning solutions to the TW-SM problem. Since the disparity is proportional to the inverse depth, we train stereo matching disparity estimation (SMDE) networks to estimate the disparity for the union WFOV. We further propose an end-to-end deep multitask tele-wide stereo matching neural network (MT-TW-SMNet), which simultaneously learns the SMDE task for the overlapped Tele FOV and the single image inverse depth estimation (SIDE) task for the WFOV. Moreover, we design multiple methods for the fusion of the SMDE and SIDE networks. We evaluate the performance of TW-SM on the popular KITTI and SceneFlow stereo datasets, and demonstrate its practicality by synthesizing the Bokeh effect on the WFOV from a tele-wide stereo image pair.



### Relationship-Embedded Representation Learning for Grounding Referring Expressions
- **Arxiv ID**: http://arxiv.org/abs/1906.04464v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.04464v3)
- **Published**: 2019-06-11 09:47:26+00:00
- **Updated**: 2020-04-19 11:04:35+00:00
- **Authors**: Sibei Yang, Guanbin Li, Yizhou Yu
- **Comment**: This paper is going to appear in TPAMI. Code is available at
  https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models
- **Journal**: None
- **Summary**: Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models



### Explicit Disentanglement of Appearance and Perspective in Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1906.11881v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.11881v2)
- **Published**: 2019-06-11 10:24:03+00:00
- **Updated**: 2019-11-13 07:18:04+00:00
- **Authors**: Nicki Skafte Detlefsen, Søren Hauberg
- **Comment**: 9 main pages + 2 pages references + 8 pages of supplementary material
- **Journal**: None
- **Summary**: Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.



### Solving Large-Scale 0-1 Knapsack Problems and its Application to Point Cloud Resampling
- **Arxiv ID**: http://arxiv.org/abs/1906.05929v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.05929v1)
- **Published**: 2019-06-11 10:56:18+00:00
- **Updated**: 2019-06-11 10:56:18+00:00
- **Authors**: Duanshun Li, Jing Liu, Noseong Park, Dongeun Lee, Giridhar Ramachandran, Ali Seyedmazloom, Kookjin Lee, Chen Feng, Vadim Sokolov, Rajesh Ganesan
- **Comment**: None
- **Journal**: None
- **Summary**: 0-1 knapsack is of fundamental importance in computer science, business, operations research, etc. In this paper, we present a deep learning technique-based method to solve large-scale 0-1 knapsack problems where the number of products (items) is large and/or the values of products are not necessarily predetermined but decided by an external value assignment function during the optimization process. Our solution is greatly inspired by the method of Lagrange multiplier and some recent adoptions of game theory to deep learning. After formally defining our proposed method based on them, we develop an adaptive gradient ascent method to stabilize its optimization process. In our experiments, the presented method solves all the large-scale benchmark KP instances in a minute whereas existing methods show fluctuating runtime. We also show that our method can be used for other applications, including but not limited to the point cloud resampling.



### Labeling, Cutting, Grouping: an Efficient Text Line Segmentation Method for Medieval Manuscripts
- **Arxiv ID**: http://arxiv.org/abs/1906.11894v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.11894v2)
- **Published**: 2019-06-11 11:06:43+00:00
- **Updated**: 2019-07-01 11:28:34+00:00
- **Authors**: Michele Alberti, Lars Vögtlin, Vinaychandran Pondenkandath, Mathias Seuret, Rolf Ingold, Marcus Liwicki
- **Comment**: None
- **Journal**: 2019 15th IAPR International Conference on Document Analysis and
  Recognition (ICDAR), Sydney, Australia
- **Summary**: This paper introduces a new way for text-line extraction by integrating deep-learning based pre-classification and state-of-the-art segmentation methods. Text-line extraction in complex handwritten documents poses a significant challenge, even to the most modern computer vision algorithms. Historical manuscripts are a particularly hard class of documents as they present several forms of noise, such as degradation, bleed-through, interlinear glosses, and elaborated scripts. In this work, we propose a novel method which uses semantic segmentation at pixel level as intermediate task, followed by a text-line extraction step. We measured the performance of our method on a recent dataset of challenging medieval manuscripts and surpassed state-of-the-art results by reducing the error by 80.7%. Furthermore, we demonstrate the effectiveness of our approach on various other datasets written in different scripts. Hence, our contribution is two-fold. First, we demonstrate that semantic pixel segmentation can be used as strong denoising pre-processing step before performing text line extraction. Second, we introduce a novel, simple and robust algorithm that leverages the high-quality semantic segmentation to achieve a text-line extraction performance of 99.42% line IU on a challenging dataset.



### Simultaneously Learning Architectures and Features of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04505v1)
- **Published**: 2019-06-11 11:49:10+00:00
- **Updated**: 2019-06-11 11:49:10+00:00
- **Authors**: Tinghuai Wang, Lixin Fan, Huiling Wang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method which simultaneously learns the number of filters and network features repeatedly over multiple epochs. We propose a novel pruning loss to explicitly enforces the optimizer to focus on promising candidate filters while suppressing contributions of less relevant ones. In the meanwhile, we further propose to enforce the diversities between filters and this diversity-based regularization term improves the trade-off between model sizes and accuracies. It turns out the interplay between architecture and feature optimizations improves the final compressed models, and the proposed method is compared favorably to existing methods, in terms of both models sizes and accuracies for a wide range of applications including image classification, image compression and audio classification.



### BasisConv: A method for compressed representation and learning in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1906.04509v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04509v1)
- **Published**: 2019-06-11 12:07:48+00:00
- **Updated**: 2019-06-11 12:07:48+00:00
- **Authors**: Muhammad Tayyab, Abhijit Mahalanobis
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that Convolutional Neural Networks (CNNs) have significant redundancy in their filter weights. Various methods have been proposed in the literature to compress trained CNNs. These include techniques like pruning weights, filter quantization and representing filters in terms of a basis functions. Our approach falls in this latter class of strategies, but is distinct in that that we show both compressed learning and representation can be achieved without significant modifications of popular CNN architectures. Specifically, any convolution layer of the CNN is easily replaced by two successive convolution layers: the first is a set of fixed filters (that represent the knowledge space of the entire layer and do not change), which is followed by a layer of one-dimensional filters (that represent the learned knowledge in this space). For the pre-trained networks, the fixed layer is just the truncated eigen-decompositions of the original filters. The 1D filters are initialized as the weights of linear combination, but are fine-tuned to recover any performance loss due to the truncation. For training networks from scratch, we use a set of random orthogonal fixed filters (that never change), and learn the 1D weight vector directly from the labeled data. Our method substantially reduces i) the number of learnable parameters during training, and ii) the number of multiplication operations and filter storage requirements during implementation. It does so without requiring any special operators in the convolution layer, and extends to all known popular CNN architectures. We apply our method to four well known network architectures trained with three different data sets. Results show a consistent reduction in i) the number of operations by up to a factor of 5, and ii) number of learnable parameters by up to a factor of 18, with less than 3% drop in performance on the CIFAR100 dataset.



### Learning robust visual representations using data augmentation invariance
- **Arxiv ID**: http://arxiv.org/abs/1906.04547v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04547v1)
- **Published**: 2019-06-11 13:03:19+00:00
- **Updated**: 2019-06-11 13:03:19+00:00
- **Authors**: Alex Hernández-García, Peter König, Tim C. Kietzmann
- **Comment**: 6 pages, 2 figures, work in progress
- **Journal**: None
- **Summary**: Deep convolutional neural networks trained for image object categorization have shown remarkable similarities with representations found across the primate ventral visual stream. Yet, artificial and biological networks still exhibit important differences. Here we investigate one such property: increasing invariance to identity-preserving image transformations found along the ventral stream. Despite theoretical evidence that invariance should emerge naturally from the optimization process, we present empirical evidence that the activations of convolutional neural networks trained for object categorization are not robust to identity-preserving image transformations commonly used in data augmentation. As a solution, we propose data augmentation invariance, an unsupervised learning objective which improves the robustness of the learned representations by promoting the similarity between the activations of augmented image samples. Our results show that this approach is a simple, yet effective and efficient (10 % increase in training time) way of increasing the invariance of the models while obtaining similar categorization performance.



### Anomaly Detection in High Performance Computers: A Vicinity Perspective
- **Arxiv ID**: http://arxiv.org/abs/1906.04550v1
- **DOI**: 10.1109/ispdc.2019.00024
- **Categories**: **cs.DC**, cs.CV, cs.SY, eess.SY, 97R99
- **Links**: [PDF](http://arxiv.org/pdf/1906.04550v1)
- **Published**: 2019-06-11 13:06:02+00:00
- **Updated**: 2019-06-11 13:06:02+00:00
- **Authors**: Siavash Ghiasvand, Florina M. Ciorba
- **Comment**: 9 pages, Submitted to the 18th IEEE International Symposium on
  Parallel and Distributed Computing
- **Journal**: None
- **Summary**: In response to the demand for higher computational power, the number of computing nodes in high performance computers (HPC) increases rapidly. Exascale HPC systems are expected to arrive by 2020. With drastic increase in the number of HPC system components, it is expected to observe a sudden increase in the number of failures which, consequently, poses a threat to the continuous operation of the HPC systems. Detecting failures as early as possible and, ideally, predicting them, is a necessary step to avoid interruptions in HPC systems operation. Anomaly detection is a well-known general purpose approach for failure detection, in computing systems. The majority of existing methods are designed for specific architectures, require adjustments on the computing systems hardware and software, need excessive information, or pose a threat to users' and systems' privacy. This work proposes a node failure detection mechanism based on a vicinity-based statistical anomaly detection approach using passively collected and anonymized system log entries. Application of the proposed approach on system logs collected over 8 months indicates an anomaly detection precision between 62% to 81%.



### Challenges in Time-Stamp Aware Anomaly Detection in Traffic Videos
- **Arxiv ID**: http://arxiv.org/abs/1906.04574v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04574v1)
- **Published**: 2019-06-11 13:23:04+00:00
- **Updated**: 2019-06-11 13:23:04+00:00
- **Authors**: Kuldeep Marotirao Biradar, Ayushi Gupta, Murari Mandal, Santosh Kumar Vipparthi
- **Comment**: IEEE Computer Vision and Pattern Recognition Workshops (CVPRW-2019)
- **Journal**: IEEE Computer Vision and Pattern Recognition Workshops
  (CVPRW-2019)
- **Summary**: Time-stamp aware anomaly detection in traffic videos is an essential task for the advancement of the intelligent transportation system. Anomaly detection in videos is a challenging problem due to sparse occurrence of anomalous events, inconsistent behavior of a different type of anomalies and imbalanced available data for normal and abnormal scenarios. In this paper, we present a three-stage pipeline to learn the motion patterns in videos to detect a visual anomaly. First, the background is estimated from recent history frames to identify the motionless objects. This background image is used to localize the normal/abnormal behavior within the frame. Further, we detect an object of interest in the estimated background and categorize it into anomaly based on a time-stamp aware anomaly detection algorithm. We also discuss the challenges faced in improving performance over the unseen test data for traffic anomaly detection. Experiments are conducted over Track 3 of NVIDIA AI city challenge 2019. The results show the effectiveness of the proposed method in detecting time-stamp aware anomalies in traffic/road videos.



### Joint Subspace Recovery and Enhanced Locality Driven Robust Flexible Discriminative Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.04598v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04598v1)
- **Published**: 2019-06-11 13:49:06+00:00
- **Updated**: 2019-06-11 13:49:06+00:00
- **Authors**: Zhao Zhang, Jiahuan Ren, Weiming Jiang, Zheng Zhang, Richang Hong, Shuicheng Yan, Meng Wang
- **Comment**: Accepted by IEEE TCSVT 2019
- **Journal**: None
- **Summary**: We propose a joint subspace recovery and enhanced locality based robust flexible label consistent dictionary learning method called Robust Flexible Discriminative Dictionary Learning (RFDDL). RFDDL mainly improves the data representation and classification abilities by enhancing the robust property to sparse errors and encoding the locality, reconstruction error and label consistency more accurately. First, for the robustness to noise and sparse errors in data and atoms, RFDDL aims at recovering the underlying clean data and clean atom subspaces jointly, and then performs DL and encodes the locality in the recovered subspaces. Second, to enable the data sampled from a nonlinear manifold to be handled potentially and obtain the accurate reconstruction by avoiding the overfitting, RFDDL minimizes the reconstruction error in a flexible manner. Third, to encode the label consistency accurately, RFDDL involves a discriminative flexible sparse code error to encourage the coefficients to be soft. Fourth, to encode the locality well, RFDDL defines the Laplacian matrix over recovered atoms, includes label information of atoms in terms of intra-class compactness and inter-class separation, and associates with group sparse codes and classifier to obtain the accurate discriminative locality-constrained coefficients and classifier. Extensive results on public databases show the effectiveness of our RFDDL.



### Mimic and Fool: A Task Agnostic Adversarial Attack
- **Arxiv ID**: http://arxiv.org/abs/1906.04606v2
- **DOI**: 10.1109/TNNLS.2020.2984972
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04606v2)
- **Published**: 2019-06-11 13:56:12+00:00
- **Updated**: 2020-04-12 18:37:42+00:00
- **Authors**: Akshay Chaturvedi, Utpal Garain
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems (2020)
- **Summary**: At present, adversarial attacks are designed in a task-specific fashion. However, for downstream computer vision tasks such as image captioning, image segmentation etc., the current deep learning systems use an image classifier like VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this in mind, we propose Mimic and Fool, a task agnostic adversarial attack. Given a feature extractor, the proposed attack finds an adversarial image which can mimic the image feature of the original image. This ensures that the two images give the same (or similar) output regardless of the task. We randomly select 1000 MSCOCO validation images for experimentation. We perform experiments on two image captioning models, Show and Tell, Show Attend and Tell and one VQA model, namely, end-to-end neural module network (N2NMN). The proposed attack achieves success rate of 74.0%, 81.0% and 87.1% for Show and Tell, Show Attend and Tell and N2NMN respectively. We also propose a slight modification to our attack to generate natural-looking adversarial images. In addition, we also show the applicability of the proposed attack for invertible architecture. Since Mimic and Fool only requires information about the feature extractor of the model, it can be considered as a gray-box attack.



### On Stabilizing Generative Adversarial Training with Noise
- **Arxiv ID**: http://arxiv.org/abs/1906.04612v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04612v2)
- **Published**: 2019-06-11 14:16:04+00:00
- **Updated**: 2019-09-17 14:30:26+00:00
- **Authors**: Simon Jenni, Paolo Favaro
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.



### Scale Invariant Fully Convolutional Network: Detecting Hands Efficiently
- **Arxiv ID**: http://arxiv.org/abs/1906.04634v1
- **DOI**: 10.1609/aaai.v33i01.33014344
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04634v1)
- **Published**: 2019-06-11 14:52:08+00:00
- **Updated**: 2019-06-11 14:52:08+00:00
- **Authors**: Dan Liu, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Feiyue Huang, Siwei Lyu
- **Comment**: Accepted to AAAI2019
- **Journal**: None
- **Summary**: Existing hand detection methods usually follow the pipeline of multiple stages with high computation cost, i.e., feature extraction, region proposal, bounding box regression, and additional layers for rotated region detection. In this paper, we propose a new Scale Invariant Fully Convolutional Network (SIFCN) trained in an end-to-end fashion to detect hands efficiently. Specifically, we merge the feature maps from high to low layers in an iterative way, which handles different scales of hands better with less time overhead comparing to concatenating them simply. Moreover, we develop the Complementary Weighted Fusion (CWF) block to make full use of the distinctive features among multiple layers to achieve scale invariance. To deal with rotated hand detection, we present the rotation map to get rid of complex rotation and derotation layers. Besides, we design the multi-scale loss scheme to accelerate the training process significantly by adding supervision to the intermediate layers of the network. Compared with the state-of-the-art methods, our algorithm shows comparable accuracy and runs a 4.23 times faster speed on the VIVA dataset and achieves better average precision on Oxford hand detection dataset at a speed of 62.5 fps.



### `Project & Excite' Modules for Segmentation of Volumetric Medical Scans
- **Arxiv ID**: http://arxiv.org/abs/1906.04649v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04649v2)
- **Published**: 2019-06-11 15:21:33+00:00
- **Updated**: 2019-06-12 09:21:34+00:00
- **Authors**: Anne-Marie Rickmann, Abhijit Guha Roy, Ignacio Sarasua, Nassir Navab, Christian Wachinger
- **Comment**: Accepted for International Conference on Medical Image Computing and
  Computer Assisted Intervention (MICCAI) 2019
- **Journal**: None
- **Summary**: Fully Convolutional Neural Networks (F-CNNs) achieve state-of-the-art performance for image segmentation in medical imaging. Recently, squeeze and excitation (SE) modules and variations thereof have been introduced to recalibrate feature maps channel- and spatial-wise, which can boost performance while only minimally increasing model complexity. So far, the development of SE has focused on 2D images. In this paper, we propose `Project & Excite' (PE) modules that base upon the ideas of SE and extend them to operating on 3D volumetric images. `Project & Excite' does not perform global average pooling, but squeezes feature maps along different slices of a tensor separately to retain more spatial information that is subsequently used in the excitation step. We demonstrate that PE modules can be easily integrated in 3D U-Net, boosting performance by 5% Dice points, while only increasing the model complexity by 2%. We evaluate the PE module on two challenging tasks, whole-brain segmentation of MRI scans and whole-body segmentation of CT scans. Code: https://github.com/ai-med/squeeze_and_excitation



### Gated CRF Loss for Weakly Supervised Semantic Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.04651v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04651v2)
- **Published**: 2019-06-11 15:23:01+00:00
- **Updated**: 2019-10-30 16:45:43+00:00
- **Authors**: Anton Obukhov, Stamatios Georgoulis, Dengxin Dai, Luc Van Gool
- **Comment**: A portion of the reported numbers are incorrect, along with a few
  statements
- **Journal**: None
- **Summary**: State-of-the-art approaches for semantic segmentation rely on deep convolutional neural networks trained on fully annotated datasets, that have been shown to be notoriously expensive to collect, both in terms of time and money. To remedy this situation, weakly supervised methods leverage other forms of supervision that require substantially less annotation effort, but they typically present an inability to predict precise object boundaries due to approximate nature of the supervisory signals in those regions. While great progress has been made in improving the performance, many of these weakly supervised methods are highly tailored to their own specific settings. This raises challenges in reusing algorithms and making steady progress. In this paper, we intentionally avoid such practices when tackling weakly supervised semantic segmentation. In particular, we train standard neural networks with partial cross-entropy loss function for the labeled pixels and our proposed Gated CRF loss for the unlabeled pixels. The Gated CRF loss is designed to deliver several important assets: 1) it enables flexibility in the kernel construction to mask out influence from undesired pixel positions; 2) it offloads learning contextual relations to CNN and concentrates on semantic boundaries; 3) it does not rely on high-dimensional filtering and thus has a simple implementation. Throughout the paper we present the advantages of the loss function, analyze several aspects of weakly supervised training, and show that our `purist' approach achieves state-of-the-art performance for both click-based and scribble-based annotations.



### On Single Source Robustness in Deep Fusion Models
- **Arxiv ID**: http://arxiv.org/abs/1906.04691v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04691v2)
- **Published**: 2019-06-11 16:47:56+00:00
- **Updated**: 2019-10-16 05:32:13+00:00
- **Authors**: Taewan Kim, Joydeep Ghosh
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: Algorithms that fuse multiple input sources benefit from both complementary and shared information. Shared information may provide robustness against faulty or noisy inputs, which is indispensable for safety-critical applications like self-driving cars. We investigate learning fusion algorithms that are robust against noise added to a single source. We first demonstrate that robustness against single source noise is not guaranteed in a linear fusion model. Motivated by this discovery, two possible approaches are proposed to increase robustness: a carefully designed loss with corresponding training algorithms for deep fusion models, and a simple convolutional fusion layer that has a structural advantage in dealing with noise. Experimental results show that both training algorithms and our fusion layer make a deep fusion-based 3D object detector robust against noise applied to a single source, while preserving the original performance on clean data.



### Deep Visual Re-Identification with Confidence
- **Arxiv ID**: http://arxiv.org/abs/1906.04692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04692v2)
- **Published**: 2019-06-11 16:49:27+00:00
- **Updated**: 2020-09-10 08:53:43+00:00
- **Authors**: George Adaimi, Sven Kreiss, Alexandre Alahi
- **Comment**: Show improvements on vehicle Re-ID datasets; Methods Clarified
- **Journal**: None
- **Summary**: Transportation systems often rely on understanding the flow of vehicles or pedestrian. From traffic monitoring at the city scale, to commuters in train terminals, recent progress in sensing technology make it possible to use cameras to better understand the demand, i.e., better track moving agents (e.g., vehicles and pedestrians). Whether the cameras are mounted on drones, vehicles, or fixed in the built environments, they inevitably remain scatter. We need to develop the technology to re-identify the same agents across images captured from non-overlapping field-of-views, referred to as the visual re-identification task. State-of-the-art methods learn a neural network based representation trained with the cross-entropy loss function. We argue that such loss function is not suited for the visual re-identification task hence propose to model confidence in the representation learning framework. We show the impact of our confidence-based learning framework with three methods: label smoothing, confidence penalty, and deep variational information bottleneck. They all show a boost in performance validating our claim. Our contribution is generic to any agent of interest, i.e., vehicles or pedestrians, and outperform highly specialized state-of-the-art methods across 5 datasets. The source code and models are shared towards an open science mission.



### Generative adversarial network for segmentation of motion affected neonatal brain MRI
- **Arxiv ID**: http://arxiv.org/abs/1906.04704v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04704v1)
- **Published**: 2019-06-11 17:11:14+00:00
- **Updated**: 2019-06-11 17:11:14+00:00
- **Authors**: N. Khalili, E. Turk, M. Zreik, M. A. Viergever, M. J. N. L. Benders, I. Isgum
- **Comment**: Accepted in Medical Image Computing and Computer Assisted
  Intervention 2019
- **Journal**: None
- **Summary**: Automatic neonatal brain tissue segmentation in preterm born infants is a prerequisite for evaluation of brain development. However, automatic segmentation is often hampered by motion artifacts caused by infant head movements during image acquisition. Methods have been developed to remove or minimize these artifacts during image reconstruction using frequency domain data. However, frequency domain data might not always be available. Hence, in this study we propose a method for removing motion artifacts from the already reconstructed MR scans. The method employs a generative adversarial network trained with a cycle consistency loss to transform slices affected by motion into slices without motion artifacts, and vice versa. In the experiments 40 T2-weighted coronal MR scans of preterm born infants imaged at 30 weeks postmenstrual age were used. All images contained slices affected by motion artifacts hampering automatic tissue segmentation. To evaluate whether correction allows more accurate image segmentation, the images were segmented into 8 tissue classes: cerebellum, myelinated white matter, basal ganglia and thalami, ventricular cerebrospinal fluid, white matter, brain stem, cortical gray matter, and extracerebral cerebrospinal fluid. Images corrected for motion and corresponding segmentations were qualitatively evaluated using 5-point Likert scale. Before the correction of motion artifacts, median image quality and quality of corresponding automatic segmentations were assigned grade 2 (poor) and 3 (moderate), respectively. After correction of motion artifacts, both improved to grades 3 and 4, respectively. The results indicate that correction of motion artifacts in the image space using the proposed approach allows accurate segmentation of brain tissue classes in slices affected by motion artifacts.



### Automatic brain tissue segmentation in fetal MRI using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04713v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.04713v1)
- **Published**: 2019-06-11 17:28:25+00:00
- **Updated**: 2019-06-11 17:28:25+00:00
- **Authors**: N. Khalili, N. Lessmann, E. Turk, N. Claessens, R. de Heus, T. Kolk, M. A. Viergever, M. J. N. L. Benders, I. Isgum
- **Comment**: Published in Magnetic Resonance Imaging, 2019
- **Journal**: None
- **Summary**: MR images of fetuses allow clinicians to detect brain abnormalities in an early stage of development. The cornerstone of volumetric and morphologic analysis in fetal MRI is segmentation of the fetal brain into different tissue classes. Manual segmentation is cumbersome and time consuming, hence automatic segmentation could substantially simplify the procedure. However, automatic brain tissue segmentation in these scans is challenging owing to artifacts including intensity inhomogeneity, caused in particular by spontaneous fetal movements during the scan. Unlike methods that estimate the bias field to remove intensity inhomogeneity as a preprocessing step to segmentation, we propose to perform segmentation using a convolutional neural network that exploits images with synthetically introduced intensity inhomogeneity as data augmentation. The method first uses a CNN to extract the intracranial volume. Thereafter, another CNN with the same architecture is employed to segment the extracted volume into seven brain tissue classes: cerebellum, basal ganglia and thalami, ventricular cerebrospinal fluid, white matter, brain stem, cortical gray matter and extracerebral cerebrospinal fluid. To make the method applicable to slices showing intensity inhomogeneity artifacts, the training data was augmented by applying a combination of linear gradients with random offsets and orientations to image slices without artifacts.



### Deep Neural Networks for Surface Segmentation Meet Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1906.04714v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04714v2)
- **Published**: 2019-06-11 17:30:27+00:00
- **Updated**: 2019-09-23 20:15:41+00:00
- **Authors**: Leixin Zhou, Zisha Zhong, Abhay Shah, Bensheng Qiu, John Buatti, Xiaodong Wu
- **Comment**: 10 pages. Submitted to IEEE TMI
- **Journal**: None
- **Summary**: Automated surface segmentation is important and challenging in many medical image analysis applications. Recent deep learning based methods have been developed for various object segmentation tasks. Most of them are a classification based approach (e.g., U-net), which predicts the probability of being target object or background for each voxel. One problem of those methods is lacking of topology guarantee for segmented objects, and usually post processing is needed to infer the boundary surface of the object. In this paper, a novel model based on 3-D convolutional neural networks (CNNs) and Conditional Random Fields (CRFs) is proposed to tackle the surface segmentation problem with end-to-end training. To the best of our knowledge, this is the first study to apply a 3-D neural network with a CRFs model for direct surface segmentation. Experiments carried out on NCI-ISBI 2013 MR prostate dataset and Medical Segmentation Decathlon Spleen dataset demonstrated promising segmentation results.



### Data-Free Quantization Through Weight Equalization and Bias Correction
- **Arxiv ID**: http://arxiv.org/abs/1906.04721v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.04721v3)
- **Published**: 2019-06-11 17:47:51+00:00
- **Updated**: 2019-11-25 15:00:11+00:00
- **Authors**: Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling
- **Comment**: ICCV 2019
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019
- **Summary**: We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.



### Recurrent U-Net for Resource-Constrained Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.04913v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04913v1)
- **Published**: 2019-06-11 17:52:30+00:00
- **Updated**: 2019-06-11 17:52:30+00:00
- **Authors**: Wei Wang, Kaicheng Yu, Joachim Hugonot, Pascal Fua, Mathieu Salzmann
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1811.10914
- **Journal**: None
- **Summary**: State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.



### Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene Layouts
- **Arxiv ID**: http://arxiv.org/abs/1906.04725v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04725v1)
- **Published**: 2019-06-11 17:55:37+00:00
- **Updated**: 2019-06-11 17:55:37+00:00
- **Authors**: Zhile Ren, Erik B. Sudderth
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI)
- **Journal**: None
- **Summary**: We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes. We first propose a clouds of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image gradients. To better represent the 3D visual styles of large objects and provide contextual cues to improve the detection of small objects, we introduce latent support surfaces. We then propose a "Manhattan voxel" representation which better captures the 3D room layout geometry of common indoor environments. Effective classification rules are learned via a latent structured prediction framework. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses that exceed the state-of-the-art on the SUN RGB-D database.



### Shapes and Context: In-the-Wild Image Synthesis & Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1906.04728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04728v1)
- **Published**: 2019-06-11 17:56:26+00:00
- **Updated**: 2019-06-11 17:56:26+00:00
- **Authors**: Aayush Bansal, Yaser Sheikh, Deva Ramanan
- **Comment**: Project Page: http://www.cs.cmu.edu/~aayushb/OpenShapes/
- **Journal**: CVPR 2019
- **Summary**: We introduce a data-driven approach for interactively synthesizing in-the-wild images from semantic label maps. Our approach is dramatically different from recent work in this space, in that we make use of no learning. Instead, our approach uses simple but classic tools for matching scene context, shapes, and parts to a stored library of exemplars. Though simple, this approach has several notable advantages over recent work: (1) because nothing is learned, it is not limited to specific training data distributions (such as cityscapes, facades, or faces); (2) it can synthesize arbitrarily high-resolution images, limited only by the resolution of the exemplar library; (3) by appropriately composing shapes and parts, it can generate an exponentially large set of viable candidate output images (that can say, be interactively searched by a user). We present results on the diverse COCO dataset, significantly outperforming learning-based approaches on standard image synthesis metrics. Finally, we explore user-interaction and user-controllability, demonstrating that our system can be used as a platform for user-driven content creation.



### Joint 3D Localization and Classification of Space Debris using a Multispectral Rotating Point Spread Function
- **Arxiv ID**: http://arxiv.org/abs/1906.04749v1
- **DOI**: 10.1364/AO.58.008598
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1906.04749v1)
- **Published**: 2019-06-11 18:00:42+00:00
- **Updated**: 2019-06-11 18:00:42+00:00
- **Authors**: Chao Wang, Grey Ballard, Robert Plemmons, Sudhakar Prasad
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: We consider the problem of joint three-dimensional (3D) localization and material classification of unresolved space debris using a multispectral rotating point spread function (RPSF). The use of RPSF allows one to estimate the 3D locations of point sources from their rotated images acquired by a single 2D sensor array, since the amount of rotation of each source image about its x, y location depends on its axial distance z. Using multi-spectral images, with one RPSF per spectral band, we are able not only to localize the 3D positions of the space debris but also classify their material composition. We propose a three-stage method for achieving joint localization and classification. In Stage 1, we adopt an optimization scheme for localization in which the spectral signature of each material is assumed to be uniform, which significantly improves efficiency and yields better localization results than possible with a single spectral band. In Stage 2, we estimate the spectral signature and refine the localization result via an alternating approach. We process classification in the final stage. Both Poisson noise and Gaussian noise models are considered, and the implementation of each is discussed. Numerical tests using multispectral data from NASA show the efficiency of our three-stage approach and illustrate the improvement of point source localization and spectral classification from using multiple bands over a single band.



### Suppressing Model Overfitting for Image Super-Resolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.04809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04809v1)
- **Published**: 2019-06-11 20:41:49+00:00
- **Updated**: 2019-06-11 20:41:49+00:00
- **Authors**: Ruicheng Feng, Jinjin Gu, Yu Qiao, Chao Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Large deep networks have demonstrated competitive performance in single image super-resolution (SISR), with a huge volume of data involved. However, in real-world scenarios, due to the limited accessible training pairs, large models exhibit undesirable behaviors such as overfitting and memorization. To suppress model overfitting and further enjoy the merits of large model capacity, we thoroughly investigate generic approaches for supplying additional training data pairs. In particular, we introduce a simple learning principle MixUp to train networks on interpolations of sample pairs, which encourages networks to support linear behavior in-between training samples. In addition, we propose a data synthesis method with learned degradation, enabling models to use extra high-quality images with higher content diversity. This strategy proves to be successful in reducing biases of data. By combining these components -- MixUp and synthetic training data, large models can be trained without overfitting under very limited data samples and achieve satisfactory generalization performance. Our method won the second place in NTIRE2019 Real SR Challenge.



### Weakly-supervised Compositional FeatureAggregation for Few-shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.04833v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04833v1)
- **Published**: 2019-06-11 21:34:09+00:00
- **Updated**: 2019-06-11 21:34:09+00:00
- **Authors**: Ping Hu, Ximeng Sun, Kate Saenko, Stan Sclaroff
- **Comment**: None
- **Journal**: None
- **Summary**: Learning from a few examples is a challenging task for machine learning. While recent progress has been made for this problem, most of the existing methods ignore the compositionality in visual concept representation (e.g. objects are built from parts or composed of semantic attributes), which is key to the human ability to easily learn from a small number of examples. To enhance the few-shot learning models with compositionality, in this paper we present the simple yet powerful Compositional Feature Aggregation (CFA) module as a weakly-supervised regularization for deep networks. Given the deep feature maps extracted from the input, our CFA module first disentangles the feature space into disjoint semantic subspaces that model different attributes, and then bilinearly aggregates the local features within each of these subspaces. CFA explicitly regularizes the representation with both semantic and spatial compositionality to produce discriminative representations for few-shot recognition tasks. Moreover, our method does not need any supervision for attributes and object parts during training, thus can be conveniently plugged into existing models for end-to-end optimization while keeping the model size and computation cost nearly the same. Extensive experiments on few-shot image classification and action recognition tasks demonstrate that our method provides substantial improvements over recent state-of-the-art methods.



### Edge-Direct Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1906.04838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.04838v1)
- **Published**: 2019-06-11 21:53:49+00:00
- **Updated**: 2019-06-11 21:53:49+00:00
- **Authors**: Kevin Christensen, Martial Hebert
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.



### Task-Aware Feature Generation for Zero-Shot Compositional Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.04854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.04854v2)
- **Published**: 2019-06-11 23:00:43+00:00
- **Updated**: 2020-03-23 23:57:41+00:00
- **Authors**: Xin Wang, Fisher Yu, Trevor Darrell, Joseph E. Gonzalez
- **Comment**: 17 pages, 9 figures; substantial content updates with additional
  experiments
- **Journal**: None
- **Summary**: Visual concepts (e.g., red apple, big elephant) are often semantically compositional and each element of the compositions can be reused to construct novel concepts (e.g., red elephant). Compositional feature synthesis, which generates image feature distributions exploiting the semantic compositionality, is a promising approach to sample-efficient model generalization. In this work, we propose a task-aware feature generation (TFG) framework for compositional learning, which generates features of novel visual concepts by transferring knowledge from previously seen concepts. These synthetic features are then used to train a classifier to recognize novel concepts in a zero-shot manner. Our novel TFG design injects task-conditioned noise layer-by-layer, producing task-relevant variation at each level. We find the proposed generator design improves classification accuracy and sample efficiency. Our model establishes a new state of the art on three zero-shot compositional learning (ZSCL) benchmarks, outperforming the previous discriminative models by a large margin. Our model improves the performance of the prior arts by over 2x in the generalized ZSCL setting.



