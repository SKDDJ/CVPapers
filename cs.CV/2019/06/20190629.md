# Arxiv Papers in cs.CV on 2019-06-29
### RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1907.00135v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00135v2)
- **Published**: 2019-06-29 02:51:29+00:00
- **Updated**: 2019-09-16 13:03:50+00:00
- **Authors**: Liuyuan Deng, Ming Yang, Tianyi Li, Yuesheng He, Chunxiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: RGB-D semantic segmentation methods conventionally use two independent encoders to extract features from the RGB and depth data. However, there lacks an effective fusion mechanism to bridge the encoders, for the purpose of fully exploiting the complementary information from multiple modalities. This paper proposes a novel bottom-up interactive fusion structure to model the interdependencies between the encoders. The structure introduces an interaction stream to interconnect the encoders. The interaction stream not only progressively aggregates modality-specific features from the encoders but also computes complementary features for them. To instantiate this structure, the paper proposes a residual fusion block (RFB) to formulate the interdependences of the encoders. The RFB consists of two residual units and one fusion unit with gate mechanism. It learns complementary features for the modality-specific encoders and extracts modality-specific features as well as cross-modal features. Based on the RFB, the paper presents the deep multimodal networks for RGB-D semantic segmentation called RFBNet. The experiments on two datasets demonstrate the effectiveness of modeling the interdependencies and that the RFBNet achieved state-of-the-art performance.



### Improved ICH classification using task-dependent learning
- **Arxiv ID**: http://arxiv.org/abs/1907.00148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00148v1)
- **Published**: 2019-06-29 05:26:24+00:00
- **Updated**: 2019-06-29 05:26:24+00:00
- **Authors**: Amir Bar, Michal Mauda, Yoni Turner, Michal Safadi, Eldad Elnekave
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI) 2019
- **Journal**: None
- **Summary**: Head CT is one of the most commonly performed imaging studied in the Emergency Department setting and Intracranial hemorrhage (ICH) is among the most critical and timesensitive findings to be detected on Head CT. We present BloodNet, a deep learning architecture designed for optimal triaging of Head CTs, with the goal of decreasing the time from CT acquisition to accurate ICH detection. The BloodNet architecture incorporates dependency between the otherwise independent tasks of segmentation and classification, achieving improved classification results. AUCs of 0.9493 and 0.9566 are reported on held out positive-enriched and randomly sampled sets comprised of over 1400 studies acquired from over 10 different hospitals. These results are comparable to previously reported results with smaller number of tagged studies.



### Progressive Fashion Attribute Extraction
- **Arxiv ID**: http://arxiv.org/abs/1907.00157v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1907.00157v1)
- **Published**: 2019-06-29 06:51:34+00:00
- **Updated**: 2019-06-29 06:51:34+00:00
- **Authors**: Sandeep Singh Adhikari, Sukhneer Singh, Anoop Rajagopal, Aruna Rajan
- **Comment**: 6 pages, 6 figures, AI for fashion : KDD 2019 Workshop, August 2019,
  Anchorage, Alaska - USA
- **Journal**: None
- **Summary**: Extracting fashion attributes from images of people wearing clothing/fashion accessories is a very hard multi-class classification problem. Most often, even catalogues of fashion do not have all the fine-grained attributes tagged due to prohibitive cost of annotation. Using images of fashion articles, running multi-class attribute extraction with a single model for all kinds of attributes (neck design detailing, sleeves detailing, etc) requires classifiers that are robust to missing and ambiguously labelled data. In this work, we propose a progressive training approach for such multi-class classification, where weights learnt from an attribute are fine tuned for another attribute of the same fashion article (say, dresses). We branch networks for each attributes from a base network progressively during training. While it may have many labels, an image doesn't need to have all possible labels for fashion articles present in it. We also compare our approach to multi-label classification, and demonstrate improvements over overall classification accuracies using our approach.



### Frame attention networks for facial expression recognition in videos
- **Arxiv ID**: http://arxiv.org/abs/1907.00193v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1907.00193v2)
- **Published**: 2019-06-29 12:11:44+00:00
- **Updated**: 2019-09-12 07:21:44+00:00
- **Authors**: Debin Meng, Xiaojiang Peng, Kai Wang, Yu Qiao
- **Comment**: Accepted by ICIP 2019
- **Journal**: None
- **Summary**: The video-based facial expression recognition aims to classify a given video into several basic emotions. How to integrate facial features of individual frames is crucial for this task. In this paper, we propose the Frame Attention Networks (FAN), to automatically highlight some discriminative frames in an end-to-end framework. The network takes a video with a variable number of face images as its input and produces a fixed-dimension representation. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which embeds face images into feature vectors. The frame attention module learns multiple attention weights which are used to adaptively aggregate the feature vectors to form a single discriminative video representation. We conduct extensive experiments on CK+ and AFEW8.0 datasets. Our proposed FAN shows superior performance compared to other CNN based methods and achieves state-of-the-art performance on CK+.



### High Sensitivity Snapshot Spectrometer Based on Deep Network Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1907.00209v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00209v1)
- **Published**: 2019-06-29 14:20:05+00:00
- **Updated**: 2019-06-29 14:20:05+00:00
- **Authors**: XiaoYu Chen, Xu Wang, Lianfa Bai, Jing Han, Zhuang Zhao
- **Comment**: 16 pages, 13 figures and 2 tables
- **Journal**: None
- **Summary**: In this paper, we present a convolution neural network based method to recover the light intensity distribution from the overlapped dispersive spectra instead of adding an extra light path to capture it directly for the first time. Then, we construct a single-path sub-Hadamard snapshot spectrometer based on our previous dual-path snapshot spectrometer. In the proposed single-path spectrometer, we use the reconstructed light intensity as the original light intensity and recover high signal-to-noise ratio spectra successfully. Compared with dual-path snapshot spectrometer, the network based single-path spectrometer has a more compact structure and maintains snapshot and high sensitivity. Abundant simulated and experimental results have demonstrated that the proposed method can obtain a better reconstructed signal-to-noise ratio spectrum than the dual-path sub-Hadamard spectrometer because of its higher light throughput.



### Robust Linear Discriminant Analysis Using Ratio Minimization of L1,2-Norms
- **Arxiv ID**: http://arxiv.org/abs/1907.00211v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00211v1)
- **Published**: 2019-06-29 14:39:27+00:00
- **Updated**: 2019-06-29 14:39:27+00:00
- **Authors**: Feiping Nie, Hua Wang, Zheng Wang, Heng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: As one of the most popular linear subspace learning methods, the Linear Discriminant Analysis (LDA) method has been widely studied in machine learning community and applied to many scientific applications. Traditional LDA minimizes the ratio of squared L2-norms, which is sensitive to outliers. In recent research, many L1-norm based robust Principle Component Analysis methods were proposed to improve the robustness to outliers. However, due to the difficulty of L1-norm ratio optimization, so far there is no existing work to utilize sparsity-inducing norms for LDA objective. In this paper, we propose a novel robust linear discriminant analysis method based on the L1,2-norm ratio minimization. Minimizing the L1,2-norm ratio is a much more challenging problem than the traditional methods, and there is no existing optimization algorithm to solve such non-smooth terms ratio problem. We derive a new efficient algorithm to solve this challenging problem, and provide a theoretical analysis on the convergence of our algorithm. The proposed algorithm is easy to implement, and converges fast in practice. Extensive experiments on both synthetic data and nine real benchmark data sets show the effectiveness of the proposed robust LDA method.



### Learning Where to Look While Tracking Instruments in Robot-assisted Surgery
- **Arxiv ID**: http://arxiv.org/abs/1907.00214v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00214v1)
- **Published**: 2019-06-29 14:56:09+00:00
- **Updated**: 2019-06-29 14:56:09+00:00
- **Authors**: Mobarakol Islam, Yueyuan Li, Hongliang Ren
- **Comment**: This work is accepted in MICCAI 2019
- **Journal**: None
- **Summary**: Directing of the task-specific attention while tracking instrument in surgery holds great potential in robot-assisted intervention. For this purpose, we propose an end-to-end trainable multitask learning (MTL) model for real-time surgical instrument segmentation and attention prediction. Our model is designed with a weight-shared encoder and two task-oriented decoders and optimized for the joint tasks. We introduce batch-Wasserstein (bW) loss and construct a soft attention module to refine the distinctive visual region for efficient saliency learning. For multitask optimization, it is always challenging to obtain convergence of both tasks in the same epoch. We deal with this problem by adopting `poly' loss weight and two phases of training. We further propose a novel way to generate task-aware saliency map and scanpath of the instruments on MICCAI robotic instrument segmentation dataset. Compared to the state of the art segmentation and saliency models, our model outperforms most of the evaluation metrics.



### Non-destructive three-dimensional measurement of hand vein based on self-supervised network
- **Arxiv ID**: http://arxiv.org/abs/1907.00215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00215v1)
- **Published**: 2019-06-29 15:01:16+00:00
- **Updated**: 2019-06-29 15:01:16+00:00
- **Authors**: Xiaoyu Chen, Qixin Wang, Jinzhou Ge, Yi Zhang, Jing Han
- **Comment**: 10 pages, 11 figures, 3 tables
- **Journal**: None
- **Summary**: At present, supervised stereo methods based on deep neural network have achieved impressive results. However, in some scenarios, accurate three-dimensional labels are inaccessible for supervised training. In this paper, a self-supervised network is proposed for binocular disparity matching (SDMNet), which computes dense disparity maps from stereo image pairs without disparity labels: In the self-supervised training, we match the stereo images densely to approximate the disparity maps and use them to warp the left and right images to estimate the right and left images; we build the loss function between estimated images and original images for self-supervised training, which adopts perceptual loss to help improve the quality of disparity maps in both detail and structure. Then, we use SDMNet to obtain disparities of hand vein. SDMNet has achieved excellent results on KITTI 2012, KITTI 2015, simulated vein dataset and real vein dataset, outperforming many state-of-the-art supervised matching methods.



### Predicting Social Perception from Faces: A Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1907.00217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00217v1)
- **Published**: 2019-06-29 15:05:51+00:00
- **Updated**: 2019-06-29 15:05:51+00:00
- **Authors**: U. Messer, S. Fausser
- **Comment**: None
- **Journal**: None
- **Summary**: Warmth and competence represent the fundamental traits in social judgment that determine emotional reactions and behavioral intentions towards social targets. This research investigates whether an algorithm can learn visual representations of social categorization and accurately predict human perceivers' impressions of warmth and competence in face images. In addition, this research unravels which areas of a face are important for the classification of warmth and competence. We use Deep Convolutional Neural Networks to extract features from face images and the Gradient-weighted Class Activation Mapping (Grad CAM) method to understand the importance of face regions for the classification. Given a single face image the trained algorithm could correctly predict warmth impressions with an accuracy of about 90% and competence impressions with an accuracy of about 80%. The findings have implications for the automated processing of faces and the design of artificial characters.



### Evaluating Local Geometric Feature Representations for 3D Rigid Data Matching
- **Arxiv ID**: http://arxiv.org/abs/1907.00233v1
- **DOI**: 10.1109/TIP.2019.2959236
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1907.00233v1)
- **Published**: 2019-06-29 16:26:40+00:00
- **Updated**: 2019-06-29 16:26:40+00:00
- **Authors**: Jiaqi Yang, Siwen Quan, Peng Wang, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Local geometric descriptors remain an essential component for 3D rigid data matching and fusion. The devise of a rotational invariant local geometric descriptor usually consists of two steps: local reference frame (LRF) construction and feature representation. Existing evaluation efforts have mainly been paid on the LRF or the overall descriptor, yet the quantitative comparison of feature representations remains unexplored. This paper fills this gap by comprehensively evaluating nine state-of-the-art local geometric feature representations. Our evaluation is on the ground that ground-truth LRFs are leveraged such that the ranking of tested feature representations are more convincing as opposed to existing studies. The experiments are deployed on six standard datasets with various application scenarios (shape retrieval, point cloud registration, and object recognition) and data modalities (LiDAR, Kinect, and Space Time) as well as perturbations including Gaussian noise, shot noise, data decimation, clutter, occlusion, and limited overlap. The evaluated terms cover the major concerns for a feature representation, e.g., distinctiveness, robustness, compactness, and efficiency. The outcomes present interesting findings that may shed new light on this community and provide complementary perspectives to existing evaluations on the topic of local geometric feature description. A summary of evaluated methods regarding their peculiarities is also presented to guide real-world applications and new descriptor crafting.



### Dissecting Pruned Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1907.00262v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00262v1)
- **Published**: 2019-06-29 19:27:57+00:00
- **Updated**: 2019-06-29 19:27:57+00:00
- **Authors**: Jonathan Frankle, David Bau
- **Comment**: None
- **Journal**: None
- **Summary**: Pruning is a standard technique for removing unnecessary structure from a neural network to reduce its storage footprint, computational demands, or energy consumption. Pruning can reduce the parameter-counts of many state-of-the-art neural networks by an order of magnitude without compromising accuracy, meaning these networks contain a vast amount of unnecessary structure. In this paper, we study the relationship between pruning and interpretability. Namely, we consider the effect of removing unnecessary structure on the number of hidden units that learn disentangled representations of human-recognizable concepts as identified by network dissection. We aim to evaluate how the interpretability of pruned neural networks changes as they are compressed. We find that pruning has no detrimental effect on this measure of interpretability until so few parameters remain that accuracy beings to drop. Resnet-50 models trained on ImageNet maintain the same number of interpretable concepts and units until more than 90% of parameters have been pruned.



### Learning to Generate Synthetic 3D Training Data through Hybrid Gradient
- **Arxiv ID**: http://arxiv.org/abs/1907.00267v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1907.00267v2)
- **Published**: 2019-06-29 19:34:19+00:00
- **Updated**: 2020-04-25 18:42:03+00:00
- **Authors**: Dawei Yang, Jia Deng
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: Synthetic images rendered by graphics engines are a promising source for training deep networks. However, it is challenging to ensure that they can help train a network to perform well on real images, because a graphics-based generation pipeline requires numerous design decisions such as the selection of 3D shapes and the placement of the camera. In this work, we propose a new method that optimizes the generation of 3D training data based on what we call "hybrid gradient". We parametrize the design decisions as a real vector, and combine the approximate gradient and the analytical gradient to obtain the hybrid gradient of the network performance with respect to this vector. We evaluate our approach on the task of estimating surface normal, depth or intrinsic decomposition from a single image. Experiments on standard benchmarks show that our approach can outperform the prior state of the art on optimizing the generation of 3D training data, particularly in terms of computational efficiency.



### DuDoNet: Dual Domain Network for CT Metal Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1907.00273v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00273v1)
- **Published**: 2019-06-29 20:23:01+00:00
- **Updated**: 2019-06-29 20:23:01+00:00
- **Authors**: Wei-An Lin, Haofu Liao, Cheng Peng, Xiaohang Sun, Jingdan Zhang, Jiebo Luo, Rama Chellappa, Shaohua Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Computed tomography (CT) is an imaging modality widely used for medical diagnosis and treatment. CT images are often corrupted by undesirable artifacts when metallic implants are carried by patients, which creates the problem of metal artifact reduction (MAR). Existing methods for reducing the artifacts due to metallic implants are inadequate for two main reasons. First, metal artifacts are structured and non-local so that simple image domain enhancement approaches would not suffice. Second, the MAR approaches which attempt to reduce metal artifacts in the X-ray projection (sinogram) domain inevitably lead to severe secondary artifact due to sinogram inconsistency. To overcome these difficulties, we propose an end-to-end trainable Dual Domain Network (DuDoNet) to simultaneously restore sinogram consistency and enhance CT images. The linkage between the sigogram and image domains is a novel Radon inversion layer that allows the gradients to back-propagate from the image domain to the sinogram domain during training. Extensive experiments show that our method achieves significant improvements over other single domain MAR approaches. To the best of our knowledge, it is the first end-to-end dual-domain network for MAR.



### NetTailor: Tuning the Architecture, Not Just the Weights
- **Arxiv ID**: http://arxiv.org/abs/1907.00274v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1907.00274v1)
- **Published**: 2019-06-29 20:32:58+00:00
- **Updated**: 2019-06-29 20:32:58+00:00
- **Authors**: Pedro Morgado, Nuno Vasconcelos
- **Comment**: None
- **Journal**: CVF/IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019
- **Summary**: Real-world applications of object recognition often require the solution of multiple tasks in a single platform. Under the standard paradigm of network fine-tuning, an entirely new CNN is learned per task, and the final network size is independent of task complexity. This is wasteful, since simple tasks require smaller networks than more complex tasks, and limits the number of tasks that can be solved simultaneously. To address these problems, we propose a transfer learning procedure, denoted NetTailor, in which layers of a pre-trained CNN are used as universal blocks that can be combined with small task-specific layers to generate new networks. Besides minimizing classification error, the new network is trained to mimic the internal activations of a strong unconstrained CNN, and minimize its complexity by the combination of 1) a soft-attention mechanism over blocks and 2) complexity regularization constraints. In this way, NetTailor can adapt the network architecture, not just its weights, to the target task. Experiments show that networks adapted to simple tasks, such as character or traffic sign recognition, become significantly smaller than those adapted to hard tasks, such as fine-grained recognition. More importantly, due to the modular nature of the procedure, this reduction in network complexity is achieved without compromise of either parameter sharing across tasks, or classification accuracy.



### Stereo relative pose from line and point feature triplets
- **Arxiv ID**: http://arxiv.org/abs/1907.00276v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.00276v1)
- **Published**: 2019-06-29 20:34:35+00:00
- **Updated**: 2019-06-29 20:34:35+00:00
- **Authors**: Alexander Vakhitov, Victor Lempitsky, Yinqiang Zheng
- **Comment**: European Conference on Computer Vision 2018. Project page:
  https://alexandervakhitov.github.io/sego/
- **Journal**: None
- **Summary**: Stereo relative pose problem lies at the core of stereo visual odometry systems that are used in many applications. In this work, we present two minimal solvers for the stereo relative pose. We specifically consider the case when a minimal set consists of three point or line features and each of them has three known projections on two stereo cameras. We validate the importance of this formulation for practical purposes in our experiments with motion estimation. We then present a complete classification of minimal cases with three point or line correspondences each having three projections, and present two new solvers that can handle all such cases. We demonstrate a considerable effect from the integration of the new solvers into a visual SLAM system.



### Improving 3D U-Net for Brain Tumor Segmentation by Utilizing Lesion Prior
- **Arxiv ID**: http://arxiv.org/abs/1907.00281v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00281v3)
- **Published**: 2019-06-29 21:29:21+00:00
- **Updated**: 2020-02-20 02:28:06+00:00
- **Authors**: Po-Yu Kao, Jefferson W. Chen, B. S. Manjunath
- **Comment**: 5 pages, 4 figures, 1 table, LNCS format
- **Journal**: None
- **Summary**: We propose a novel, simple and effective method to integrate lesion prior and a 3D U-Net for improving brain tumor segmentation. First, we utilize the ground-truth brain tumor lesions from a group of patients to generate the heatmaps of different types of lesions. These heatmaps are used to create the volume-of-interest (VOI) map which contains prior information about brain tumor lesions. The VOI map is then integrated with the multimodal MR images and input to a 3D U-Net for segmentation. The proposed method is evaluated on a public benchmark dataset, and the experimental results show that the proposed feature fusion method achieves an improvement over the baseline methods. In addition, our proposed method also achieves a competitive performance compared to state-of-the-art methods.



### SLAM Endoscopy enhanced by adversarial depth prediction
- **Arxiv ID**: http://arxiv.org/abs/1907.00283v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1907.00283v1)
- **Published**: 2019-06-29 21:42:42+00:00
- **Updated**: 2019-06-29 21:42:42+00:00
- **Authors**: Richard J. Chen, Taylor L. Bobrow, Thomas Athey, Faisal Mahmood, Nicholas J. Durr
- **Comment**: None
- **Journal**: None
- **Summary**: Medical endoscopy remains a challenging application for simultaneous localization and mapping (SLAM) due to the sparsity of image features and size constraints that prevent direct depth-sensing. We present a SLAM approach that incorporates depth predictions made by an adversarially-trained convolutional neural network (CNN) applied to monocular endoscopy images. The depth network is trained with synthetic images of a simple colon model, and then fine-tuned with domain-randomized, photorealistic images rendered from computed tomography measurements of human colons. Each image is paired with an error-free depth map for supervised adversarial learning. Monocular RGB images are then fused with corresponding depth predictions, enabling dense reconstruction and mosaicing as an endoscope is advanced through the gastrointestinal tract. Our preliminary results demonstrate that incorporating monocular depth estimation into a SLAM architecture can enable dense reconstruction of endoscopic scenes.



### Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction with Joint Projection-Sinogram Correction
- **Arxiv ID**: http://arxiv.org/abs/1907.00294v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1907.00294v4)
- **Published**: 2019-06-29 23:30:10+00:00
- **Updated**: 2022-03-23 16:18:11+00:00
- **Authors**: Haofu Liao, Wei-An Lin, Zhimin Huo, Levon Vogelsang, William J. Sehnert, S. Kevin Zhou, Jiebo Luo
- **Comment**: This paper is accepted to MICCAI 2019
- **Journal**: None
- **Summary**: A conventional approach to computed tomography (CT) or cone beam CT (CBCT) metal artifact reduction is to replace the X-ray projection data within the metal trace with synthesized data. However, existing projection or sinogram completion methods cannot always produce anatomically consistent information to fill the metal trace, and thus, when the metallic implant is large, significant secondary artifacts are often introduced. In this work, we propose to replace metal artifact affected regions with anatomically consistent content through joint projection-sinogram correction as well as adversarial learning. To handle the metallic implants of diverse shapes and large sizes, we also propose a novel mask pyramid network that enforces the mask information across the network's encoding layers and a mask fusion loss that reduces early saturation of adversarial training. Our experimental results show that the proposed projection-sinogram correction designs are effective and our method recovers information from the metal traces better than the state-of-the-art methods.



