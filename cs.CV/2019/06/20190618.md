# Arxiv Papers in cs.CV on 2019-06-18
### DeepView: View Synthesis with Learned Gradient Descent
- **Arxiv ID**: http://arxiv.org/abs/1906.07316v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07316v1)
- **Published**: 2019-06-18 00:29:27+00:00
- **Updated**: 2019-06-18 00:29:27+00:00
- **Authors**: John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker
- **Comment**: See https://augmentedperception.github.io/deepview/ for more results,
  video and an interactive viewer
- **Journal**: None
- **Summary**: We present a novel approach to view synthesis using multiplane images (MPIs). Building on recent advances in learned gradient descent, our algorithm generates an MPI from a set of sparse camera viewpoints. The resulting method incorporates occlusion reasoning, improving performance on challenging scene features such as object boundaries, lighting reflections, thin structures, and scenes with high depth complexity. We show that our method achieves high-quality, state-of-the-art results on two datasets: the Kalantari light field dataset, and a new camera array dataset, Spaces, which we make publicly available.



### Boosting CNN beyond Label in Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1906.07330v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07330v1)
- **Published**: 2019-06-18 01:21:40+00:00
- **Updated**: 2019-06-18 01:21:40+00:00
- **Authors**: Eunju Cha, Jaeduck Jang, Junho Lee, Eunha Lee, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNN) have been extensively used for inverse problems. However, their prediction error for unseen test data is difficult to estimate a priori since the neural networks are trained using only selected data and their architecture are largely considered a blackbox. This poses a fundamental challenge to neural networks for unsupervised learning or improvement beyond the label. In this paper, we show that the recent unsupervised learning methods such as Noise2Noise, Stein's unbiased risk estimator (SURE)-based denoiser, and Noise2Void are closely related to each other in their formulation of an unbiased estimator of the prediction error, but each of them are associated with its own limitations. Based on these observations, we provide a novel boosting estimator for the prediction error. In particular, by employing combinatorial convolutional frame representation of encoder-decoder CNN and synergistically combining it with the batch normalization, we provide a close form formulation for the unbiased estimator of the prediction error that can be minimized for neural network training beyond the label. Experimental results show that the resulting algorithm, what we call Noise2Boosting, provides consistent improvement in various inverse problems under both supervised and unsupervised learning setting.



### Learning Personalized Attribute Preference via Multi-task AUC Optimization
- **Arxiv ID**: http://arxiv.org/abs/1906.07341v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07341v1)
- **Published**: 2019-06-18 02:14:36+00:00
- **Updated**: 2019-06-18 02:14:36+00:00
- **Authors**: Zhiyong Yang, Qianqian Xu, Xiaochun Cao, Qingming Huang
- **Comment**: AAAI2019 oral
- **Journal**: AAAI2019 oral
- **Summary**: Traditionally, most of the existing attribute learning methods are trained based on the consensus of annotations aggregated from a limited number of annotators. However, the consensus might fail in settings, especially when a wide spectrum of annotators with different interests and comprehension about the attribute words are involved. In this paper, we develop a novel multi-task method to understand and predict personalized attribute annotations. Regarding the attribute preference learning for each annotator as a specific task, we first propose a multi-level task parameter decomposition to capture the evolution from a highly popular opinion of the mass to highly personalized choices that are special for each person. Meanwhile, for personalized learning methods, ranking prediction is much more important than accurate classification. This motivates us to employ an Area Under ROC Curve (AUC) based loss function to improve our model. On top of the AUC-based loss, we propose an efficient method to evaluate the loss and gradients. Theoretically, we propose a novel closed-form solution for one of our non-convex subproblem, which leads to provable convergence behaviors. Furthermore, we also provide a generalization bound to guarantee a reasonable performance. Finally, empirical analysis consistently speaks to the efficacy of our proposed method.



### Cardiac Segmentation from LGE MRI Using Deep Neural Network Incorporating Shape and Spatial Priors
- **Arxiv ID**: http://arxiv.org/abs/1906.07347v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07347v2)
- **Published**: 2019-06-18 02:36:34+00:00
- **Updated**: 2019-06-25 08:25:38+00:00
- **Authors**: Qian Yue, Xinzhe Luo, Qing Ye, Lingchao Xu, Xiahai Zhuang
- **Comment**: Accepted for publication in MICCAI 2019
- **Journal**: None
- **Summary**: Cardiac segmentation from late gadolinium enhancement MRI is an important task in clinics to identify and evaluate the infarction of myocardium. The automatic segmentation is however still challenging, due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this paper, we propose a new method, based on deep neural networks (DNN), for fully automatic segmentation. The proposed network, referred to as SRSCN, comprises a shape reconstruction neural network (SRNN) and a spatial constraint network (SCN). SRNN aims to maintain a realistic shape of the resulting segmentation. It can be pre-trained by a set of label images, and then be embedded into a unified loss function as a regularization term. Hence, no manually designed feature is needed. Furthermore, SCN incorporates the spatial information of the 2D slices. It is formulated and trained with the segmentation network via the multi-task learning strategy. We evaluated the proposed method using 45 patients and compared with two state-of-the-art regularization schemes, i.e., the anatomically constraint neural network and the adversarial neural network. The results show that the proposed SRSCN outperformed the conventional schemes, and obtained a Dice score of 0.758(std=0.227) for myocardial segmentation, which compares with 0.757(std=0.083) from the inter-observer variations.



### Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense Tracking
- **Arxiv ID**: http://arxiv.org/abs/1906.07357v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07357v1)
- **Published**: 2019-06-18 03:05:47+00:00
- **Updated**: 2019-06-18 03:05:47+00:00
- **Authors**: Wentao Zhu, Yufang Huang, Mani A Vannan, Shizhen Liu, Daguang Xu, Wei Fan, Zhen Qian, Xiaohui Xie
- **Comment**: Blood tracking video: https://youtu.be/pEA6ZmtTNuQ Muscle tracking
  video: https://youtu.be/NvLrCaqCiAE
- **Journal**: None
- **Summary**: Echocardiography has become routinely used in the diagnosis of cardiomyopathy and abnormal cardiac blood flow. However, manually measuring myocardial motion and cardiac blood flow from echocardiogram is time-consuming and error-prone. Computer algorithms that can automatically track and quantify myocardial motion and cardiac blood flow are highly sought after, but have not been very successful due to noise and high variability of echocardiography. In this work, we propose a neural multi-scale self-supervised registration (NMSR) method for automated myocardial and cardiac blood flow dense tracking. NMSR incorporates two novel components: 1) utilizing a deep neural net to parameterize the velocity field between two image frames, and 2) optimizing the parameters of the neural net in a sequential multi-scale fashion to account for large variations within the velocity field. Experiments demonstrate that NMSR yields significantly better registration accuracy than state-of-the-art methods, such as advanced normalization tools (ANTs) and VoxelMorph, for both myocardial and cardiac blood flow dense tracking. Our approach promises to provide a fully automated method for fast and accurate analyses of echocardiograms.



### A sparse annotation strategy based on attention-guided active learning for 3D medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.07367v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07367v1)
- **Published**: 2019-06-18 04:09:46+00:00
- **Updated**: 2019-06-18 04:09:46+00:00
- **Authors**: Zhenxi Zhang, Jie Li, Zhusi Zhong, Zhicheng Jiao, Xinbo Gao
- **Comment**: None
- **Journal**: None
- **Summary**: 3D image segmentation is one of the most important and ubiquitous problems in medical image processing. It provides detailed quantitative analysis for accurate disease diagnosis, abnormal detection, and classification. Currently deep learning algorithms are widely used in medical image segmentation, most algorithms trained models with full annotated datasets. However, obtaining medical image datasets is very difficult and expensive, and full annotation of 3D medical image is a monotonous and time-consuming work. Partially labelling informative slices in 3D images will be a great relief of manual annotation. Sample selection strategies based on active learning have been proposed in the field of 2D image, but few strategies focus on 3D images. In this paper, we propose a sparse annotation strategy based on attention-guided active learning for 3D medical image segmentation. Attention mechanism is used to improve segmentation accuracy and estimate the segmentation accuracy of each slice. The comparative experiments with three different strategies using datasets from the developing human connectome project (dHCP) show that, our strategy only needs 15% to 20% annotated slices in brain extraction task and 30% to 35% annotated slices in tissue segmentation task to achieve comparative results as full annotation.



### Neural Illumination: Lighting Prediction for Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1906.07370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07370v1)
- **Published**: 2019-06-18 04:17:16+00:00
- **Updated**: 2019-06-18 04:17:16+00:00
- **Authors**: Shuran Song, Thomas Funkhouser
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the task of estimating the light arriving from all directions to a 3D point observed at a selected pixel in an RGB image. This task is challenging because it requires predicting a mapping from a partial scene observation by a camera to a complete illumination map for a selected position, which depends on the 3D location of the selection, the distribution of unobserved light sources, the occlusions caused by scene geometry, etc. Previous methods attempt to learn this complex mapping directly using a single black-box neural network, which often fails to estimate high-frequency lighting details for scenes with complicated 3D geometry. Instead, we propose "Neural Illumination" a new approach that decomposes illumination prediction into several simpler differentiable sub-tasks: 1) geometry estimation, 2) scene completion, and 3) LDR-to-HDR estimation. The advantage of this approach is that the sub-tasks are relatively easy to learn and can be trained with direct supervision, while the whole pipeline is fully differentiable and can be fine-tuned with end-to-end supervision. Experiments show that our approach performs significantly better quantitatively and qualitatively than prior work.



### A Conditional Random Field Model for Context Aware Cloud Detection in Sky Images
- **Arxiv ID**: http://arxiv.org/abs/1906.07383v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07383v1)
- **Published**: 2019-06-18 05:26:06+00:00
- **Updated**: 2019-06-18 05:26:06+00:00
- **Authors**: Vijai T. Jayadevan, Jeffrey J. Rodriguez, Alexander D. Cronin
- **Comment**: This was part of Vijai Jayadevan's MS thesis which was completed in
  Dec 2013. This particular chapter remained unpublished till now
- **Journal**: None
- **Summary**: A conditional random field (CRF) model for cloud detection in ground based sky images is presented. We show that very high cloud detection accuracy can be achieved by combining a discriminative classifier and a higher order clique potential in a CRF framework. The image is first divided into homogeneous regions using a mean shift clustering algorithm and then a CRF model is defined over these regions. The various parameters involved are estimated using training data and the inference is performed using Iterated Conditional Modes (ICM) algorithm. We demonstrate how taking spatial context into account can boost the accuracy. We present qualitative and quantitative results to prove the superior performance of this framework in comparison with other state of the art methods applied for cloud detection.



### Active Scene Understanding via Online Semantic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1906.07409v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07409v2)
- **Published**: 2019-06-18 07:15:27+00:00
- **Updated**: 2022-01-13 14:07:43+00:00
- **Authors**: Lintao Zheng, Chenyang Zhu, Jiazhao Zhang, Hang Zhao, Hui Huang, Matthias Niessner, Kai Xu
- **Comment**: PG 2019
- **Journal**: None
- **Summary**: We propose a novel approach to robot-operated active understanding of unknown indoor scenes, based on online RGBD reconstruction with semantic segmentation. In our method, the exploratory robot scanning is both driven by and targeting at the recognition and segmentation of semantic objects from the scene. Our algorithm is built on top of the volumetric depth fusion framework (e.g., KinectFusion) and performs real-time voxel-based semantic labeling over the online reconstructed volume. The robot is guided by an online estimated discrete viewing score field (VSF) parameterized over the 3D space of 2D location and azimuth rotation. VSF stores for each grid the score of the corresponding view, which measures how much it reduces the uncertainty (entropy) of both geometric reconstruction and semantic labeling. Based on VSF, we select the next best views (NBV) as the target for each time step. We then jointly optimize the traverse path and camera trajectory between two adjacent NBVs, through maximizing the integral viewing score (information gain) along path and trajectory. Through extensive evaluation, we show that our method achieves efficient and accurate online scene parsing during exploratory scanning.



### Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
- **Arxiv ID**: http://arxiv.org/abs/1906.07413v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07413v2)
- **Published**: 2019-06-18 07:21:18+00:00
- **Updated**: 2019-10-27 23:31:52+00:00
- **Authors**: Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma
- **Comment**: to appear in NeurIPS 2019
- **Journal**: None
- **Summary**: Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains.



### Using colorization as a tool for automatic makeup suggestion
- **Arxiv ID**: http://arxiv.org/abs/1906.07421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07421v1)
- **Published**: 2019-06-18 07:45:43+00:00
- **Updated**: 2019-06-18 07:45:43+00:00
- **Authors**: Shreyank Narayana Gowda
- **Comment**: None
- **Journal**: None
- **Summary**: Colorization is the method of converting an image in grayscale to a fully color image. There are multiple methods to do the same. Old school methods used machine learning algorithms and optimization techniques to suggest possible colors to use. With advances in the field of deep learning, colorization results have improved consistently with improvements in deep learning architectures. The latest development in the field of deep learning is the emergence of generative adversarial networks (GANs) which is used to generate information and not just predict or classify. As part of this report, 2 architectures of recent papers are reproduced along with a novel architecture being suggested for general colorization. Following this, we propose the use of colorization by generating makeup suggestions automatically on a face. To do this, a dataset consisting of 1000 images has been created. When an image of a person without makeup is sent to the model, the model first converts the image to grayscale and then passes it through the suggested GAN model. The output is a generated makeup suggestion. To develop this model, we need to tweak the general colorization model to deal only with faces of people.



### Locality Preserving Joint Transfer for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1906.07441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07441v1)
- **Published**: 2019-06-18 08:34:40+00:00
- **Updated**: 2019-06-18 08:34:40+00:00
- **Authors**: Li Jingjing, Jing Mengmeng, Lu Ke, Zhu Lei, Shen Heng Tao
- **Comment**: Accepted to IEEE TIP 2019
- **Journal**: None
- **Summary**: Domain adaptation aims to leverage knowledge from a well-labeled source domain to a poorly-labeled target domain. A majority of existing works transfer the knowledge at either feature level or sample level. Recent researches reveal that both of the paradigms are essentially important, and optimizing one of them can reinforce the other. Inspired by this, we propose a novel approach to jointly exploit feature adaptation with distribution matching and sample adaptation with landmark selection. During the knowledge transfer, we also take the local consistency between samples into consideration, so that the manifold structures of samples can be preserved. At last, we deploy label propagation to predict the categories of new instances. Notably, our approach is suitable for both homogeneous and heterogeneous domain adaptation by learning domain-specific projections. Extensive experiments on five open benchmarks, which consist of both standard and large-scale datasets, verify that our approach can significantly outperform not only conventional approaches but also end-to-end deep models. The experiments also demonstrate that we can leverage handcrafted features to promote the accuracy on deep features by heterogeneous adaptation.



### An unsupervised approach to Geographical Knowledge Discovery using street level and street network images
- **Arxiv ID**: http://arxiv.org/abs/1906.11907v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.11907v2)
- **Published**: 2019-06-18 09:33:09+00:00
- **Updated**: 2019-10-11 13:27:35+00:00
- **Authors**: Stephen Law, Mateo Neira
- **Comment**: SigSpatial 2019 GeoAI
- **Journal**: None
- **Summary**: Recent researches have shown the increasing use of machine learn-ing methods in geography and urban analytics, primarily to extract features and patterns from spatial and temporal data using a supervised approach. Researches integrating geographical processes in machine learning models and the use of unsupervised approacheson geographical data for knowledge discovery had been sparse. This research contributes to the ladder, where we show how latent variables learned from unsupervised learning methods on urbanimages can be used for geographic knowledge discovery. In particular, we propose a simple approach called Convolutional-PCA(ConvPCA) which are applied on both street level and street network images to find a set of uncorrelated and ordered visual latentcomponents. The approach allows for meaningful explanations using a combination of geographical and generative visualisations to explore the latent space, and to show how the learned representation can be used to predict urban characteristics such as streetquality and street network attributes. The research also finds that the visual components from the ConvPCA model achieves similaraccuracy when compared to less interpretable dimension reduction techniques.



### Deep Multicameral Decoding for Localizing Unoccluded Object Instances from a Single RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1906.07480v3
- **DOI**: 10.1007/s11263-020-01323-0
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07480v3)
- **Published**: 2019-06-18 10:20:17+00:00
- **Updated**: 2020-04-15 08:46:37+00:00
- **Authors**: Matthieu Grard, Emmanuel Dellandréa, Liming Chen
- **Comment**: International Journal of Computer Vision, Springer Verlag, 2020,
  Special Issue on Deep Learning for Robotic Vision
- **Journal**: None
- **Summary**: Occlusion-aware instance-sensitive segmentation is a complex task generally split into region-based segmentations, by approximating instances as their bounding box. We address the showcase scenario of dense homogeneous layouts in which this approximation does not hold. In this scenario, outlining unoccluded instances by decoding a deep encoder becomes difficult, due to the translation invariance of convolutional layers and the lack of complexity in the decoder. We therefore propose a multicameral design composed of subtask-specific lightweight decoder and encoder-decoder units, coupled in cascade to encourage subtask-specific feature reuse and enforce a learning path within the decoding process. Furthermore, the state-of-the-art datasets for occlusion-aware instance segmentation contain real images with few instances and occlusions mostly due to objects occluding the background, unlike dense object layouts. We thus also introduce a synthetic dataset of dense homogeneous object layouts, namely Mikado, which extensibly contains more instances and inter-instance occlusions per image than these public datasets. Our extensive experiments on Mikado and public datasets show that ordinal multiscale units within the decoding process prove more effective than state-of-the-art design patterns for capturing position-sensitive representations. We also show that Mikado is plausible with respect to real-world problems, in the sense that it enables the learning of performance-enhancing representations transferable to real images, while drastically reducing the need of hand-made annotations for finetuning. The proposed dataset will be made publicly available.



### A One-step Pruning-recovery Framework for Acceleration of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.07488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07488v1)
- **Published**: 2019-06-18 10:40:55+00:00
- **Updated**: 2019-06-18 10:40:55+00:00
- **Authors**: Dong Wang, Lei Zhou, Xiao Bai, Jun Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Acceleration of convolutional neural network has received increasing attention during the past several years. Among various acceleration techniques, filter pruning has its inherent merit by effectively reducing the number of convolution filters. However, most filter pruning methods resort to tedious and time-consuming layer-by-layer pruning-recovery strategy to avoid a significant drop of accuracy. In this paper, we present an efficient filter pruning framework to solve this problem. Our method accelerates the network in one-step pruning-recovery manner with a novel optimization objective function, which achieves higher accuracy with much less cost compared with existing pruning methods. Furthermore, our method allows network compression with global filter pruning. Given a global pruning rate, it can adaptively determine the pruning rate for each single convolutional layer, while these rates are often set as hyper-parameters in previous approaches. Evaluated on VGG-16 and ResNet-50 using ImageNet, our approach outperforms several state-of-the-art methods with less accuracy drop under the same and even much fewer floating-point operations (FLOPs).



### Deep Learning Enhanced Extended Depth-of-Field for Thick Blood-Film Malaria High-Throughput Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1906.07496v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07496v1)
- **Published**: 2019-06-18 11:12:44+00:00
- **Updated**: 2019-06-18 11:12:44+00:00
- **Authors**: Petru Manescu, Lydia Neary- Zajiczek, Michael J. Shaw, Muna Elmi, Remy Claveau, Vijay Pawar, John Shawe-Taylor, Iasonas Kokkinos, Mandayam A. Srinivasan, Ikeoluwa Lagunju, Olugbemiro Sodeinde, Biobele J. Brown, Delmiro Fernandez-Reyes
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Fast accurate diagnosis of malaria is still a global health challenge for which automated digital-pathology approaches could provide scalable solutions amenable to be deployed in low-to-middle income countries. Here we address the problem of Extended Depth-of-Field (EDoF) in thick blood film microscopy for rapid automated malaria diagnosis. High magnification oil-objectives (100x) with large numerical aperture are usually preferred to resolve the fine structural details that help separate true parasites from distractors. However, such objectives have a very limited depth-of-field requiring the acquisition of a series of images at different focal planes per field of view (FOV). Current EDoF techniques based on multi-scale decompositions are time consuming and therefore not suited for high-throughput analysis of specimens. To overcome this challenge, we developed a new deep learning method based on Convolutional Neural Networks (EDoF-CNN) that is able to rapidly perform the extended depth-of-field while also enhancing the spatial resolution of the resulting fused image. We evaluated our approach using simulated low-resolution z-stacks from Giemsa-stained thick blood smears from patients presenting with Plasmodium falciparum malaria. The EDoF-CNN allows speed-up of our digital-pathology acquisition platform and significantly improves the quality of the EDoF compared to the traditional multi-scaled approaches when applied to lower resolution stacks corresponding to acquisitions with fewer focal planes, large camera pixel binning or lower magnification objectives (larger FOV). We use the parasite detection accuracy of a deep learning model on the EDoFs as a concrete, task-specific measure of performance of this approach.



### Impoved RPN for Single Targets Detection based on the Anchor Mask Net
- **Arxiv ID**: http://arxiv.org/abs/1906.07527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07527v1)
- **Published**: 2019-06-18 12:32:17+00:00
- **Updated**: 2019-06-18 12:32:17+00:00
- **Authors**: Mingjie Li, Youqian Feng, Zhonghai Yin, Cheng Zhou, Fanghao Dong
- **Comment**: 10 pages,9 Figure
- **Journal**: None
- **Summary**: Common target detection is usually based on single frame images, which is vulnerable to affected by the similar targets in the image and not applicable to video images. In this paper , anchor mask is proposed to add the prior knowledge for target detection and an anchor mask net is designed to impove the RPN performance for single target detection. Tested in the VOT2016, the model perform better.



### Locate, Size and Count: Accurately Resolving People in Dense Crowds via Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.07538v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07538v3)
- **Published**: 2019-06-18 12:58:07+00:00
- **Updated**: 2020-02-15 16:44:11+00:00
- **Authors**: Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, R. Venkatesh Babu
- **Comment**: Accepted in T-PAMI, 2020. Code available at :
  https://github.com/val-iisc/lsc-cnn
- **Journal**: None
- **Summary**: We introduce a detection framework for dense crowd counting and eliminate the need for the prevalent density regression paradigm. Typical counting models predict crowd density for an image as opposed to detecting every person. These regression methods, in general, fail to localize persons accurate enough for most applications other than counting. Hence, we adopt an architecture that locates every person in the crowd, sizes the spotted heads with bounding box and then counts them. Compared to normal object or face detectors, there exist certain unique challenges in designing such a detection system. Some of them are direct consequences of the huge diversity in dense crowds along with the need to predict boxes contiguously. We solve these issues and develop our LSC-CNN model, which can reliably detect heads of people across sparse to dense crowds. LSC-CNN employs a multi-column architecture with top-down feedback processing to better resolve persons and produce refined predictions at multiple resolutions. Interestingly, the proposed training regime requires only point head annotation, but can estimate approximate size information of heads. We show that LSC-CNN not only has superior localization than existing density regressors, but outperforms in counting as well. The code for our approach is available at https://github.com/val-iisc/lsc-cnn.



### Differentiable probabilistic models of scientific imaging with the Fourier slice theorem
- **Arxiv ID**: http://arxiv.org/abs/1906.07582v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.07582v2)
- **Published**: 2019-06-18 13:49:06+00:00
- **Updated**: 2019-06-20 15:20:54+00:00
- **Authors**: Karen Ullrich, Rianne van den Berg, Marcus Brubaker, David Fleet, Max Welling
- **Comment**: accepted to UAI 2019
- **Journal**: None
- **Summary**: Scientific imaging techniques such as optical and electron microscopy and computed tomography (CT) scanning are used to study the 3D structure of an object through 2D observations. These observations are related to the original 3D object through orthogonal integral projections. For common 3D reconstruction algorithms, computational efficiency requires the modeling of the 3D structures to take place in Fourier space by applying the Fourier slice theorem. At present, it is unclear how to differentiate through the projection operator, and hence current learning algorithms can not rely on gradient based methods to optimize 3D structure models. In this paper we show how back-propagation through the projection operator in Fourier space can be achieved. We demonstrate the validity of the approach with experiments on 3D reconstruction of proteins. We further extend our approach to learning probabilistic models of 3D objects. This allows us to predict regions of low sampling rates or estimate noise. A higher sample efficiency can be reached by utilizing the learned uncertainties of the 3D structure as an unsupervised estimate of the model fit. Finally, we demonstrate how the reconstruction algorithm can be extended with an amortized inference scheme on unknown attributes such as object pose. Through empirical studies we show that joint inference of the 3D structure and the object pose becomes more difficult when the ground truth object contains more symmetries. Due to the presence of for instance (approximate) rotational symmetries, the pose estimation can easily get stuck in local optima, inhibiting a fine-grained high-quality estimate of the 3D structure.



### Learning with Average Precision: Training Image Retrieval with a Listwise Loss
- **Arxiv ID**: http://arxiv.org/abs/1906.07589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07589v1)
- **Published**: 2019-06-18 13:59:34+00:00
- **Updated**: 2019-06-18 13:59:34+00:00
- **Authors**: Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar Roberto de Souza
- **Comment**: None
- **Journal**: None
- **Summary**: Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g. special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at https://europe.naverlabs.com/Deep-Image-Retrieval/



### 3D Geometric salient patterns analysis on 3D meshes
- **Arxiv ID**: http://arxiv.org/abs/1906.07645v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1906.07645v1)
- **Published**: 2019-06-18 15:43:14+00:00
- **Updated**: 2019-06-18 15:43:14+00:00
- **Authors**: Alice Othmani, Fakhri Torkhani, Jean-Marie Favreau
- **Comment**: None
- **Journal**: None
- **Summary**: Pattern analysis is a wide domain that has wide applicability in many fields. In fact, texture analysis is one of those fields, since the texture is defined as a set of repetitive or quasi-repetitive patterns. Despite its importance in analyzing 3D meshes, geometric texture analysis is less studied by geometry processing community. This paper presents a new efficient approach for geometric texture analysis on 3D triangular meshes. The proposed method is a scale-aware approach that takes as input a 3D mesh and a user-scale. It provides, as a result, a similarity-based clustering of texels in meaningful classes. Experimental results of the proposed algorithm are presented for both real-world and synthetic meshes within various textures. Furthermore, the efficiency of the proposed approach was experimentally demonstrated under mesh simplification and noise addition on the mesh surface. In this paper, we present a practical application for semantic annotation of 3D geometric salient texels.



### Weakly Supervised Clustering by Exploiting Unique Class Count
- **Arxiv ID**: http://arxiv.org/abs/1906.07647v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07647v2)
- **Published**: 2019-06-18 15:44:54+00:00
- **Updated**: 2020-01-25 08:47:04+00:00
- **Authors**: Mustafa Umit Oner, Hwee Kuan Lee, Wing-Kin Sung
- **Comment**: Published as a conference paper at ICLR 2020
- **Journal**: None
- **Summary**: A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count ($ucc$), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect $ucc$ classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based $ucc$ classifier and experimentally shown that the clustering performance of our framework with our weakly supervised $ucc$ classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model.



### ADA-Tucker: Compressing Deep Neural Networks via Adaptive Dimension Adjustment Tucker Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1906.07671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07671v1)
- **Published**: 2019-06-18 16:19:04+00:00
- **Updated**: 2019-06-18 16:19:04+00:00
- **Authors**: Zhisheng Zhong, Fangyin Wei, Zhouchen Lin, Chao Zhang
- **Comment**: 25 pages, 12 figures
- **Journal**: Elsevier, Neural Networks, Volume 110, Feb. 2019, Pages 104-115
- **Summary**: Despite the recent success of deep learning models in numerous applications, their widespread use on mobile devices is seriously impeded by storage and computational requirements. In this paper, we propose a novel network compression method called Adaptive Dimension Adjustment Tucker decomposition (ADA-Tucker). With learnable core tensors and transformation matrices, ADA-Tucker performs Tucker decomposition of arbitrary-order tensors. Furthermore, we propose that weight tensors in networks with proper order and balanced dimension are easier to be compressed. Therefore, the high flexibility in decomposition choice distinguishes ADA-Tucker from all previous low-rank models. To compress more, we further extend the model to Shared Core ADA-Tucker (SCADA-Tucker) by defining a shared core tensor for all layers. Our methods require no overhead of recording indices of non-zero elements. Without loss of accuracy, our methods reduce the storage of LeNet-5 and LeNet-300 by ratios of 691 times and 233 times, respectively, significantly outperforming state of the art. The effectiveness of our methods is also evaluated on other three benchmarks (CIFAR-10, SVHN, ILSVRC12) and modern newly deep networks (ResNet, Wide-ResNet).



### Weather Influence and Classification with Automotive Lidar Sensors
- **Arxiv ID**: http://arxiv.org/abs/1906.07675v1
- **DOI**: 10.1109/IVS.2019.8814205
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07675v1)
- **Published**: 2019-06-18 16:29:03+00:00
- **Updated**: 2019-06-18 16:29:03+00:00
- **Authors**: Robin Heinzler, Philipp Schindler, Jürgen Seekircher, Werner Ritter, Wilhelm Stork
- **Comment**: 8 pages, will be published in the IEEE IV 2019 Proceedings
- **Journal**: 2019 IEEE Intelligent Vehicles Symposium (IV)
- **Summary**: Lidar sensors are often used in mobile robots and autonomous vehicles to complement camera, radar and ultrasonic sensors for environment perception. Typically, perception algorithms are trained to only detect moving and static objects as well as ground estimation, but intentionally ignore weather effects to reduce false detections. In this work, we present an in-depth analysis of automotive lidar performance under harsh weather conditions, i.e. heavy rain and dense fog. An extensive data set has been recorded for various fog and rain conditions, which is the basis for the conducted in-depth analysis of the point cloud under changing environmental conditions. In addition, we introduce a novel approach to detect and classify rain or fog with lidar sensors only and achieve an mean union over intersection of 97.14 % for a data set in controlled environments. The analysis of weather influences on the performance of lidar sensors and the weather detection are important steps towards improving safety levels for autonomous driving in adverse weather conditions by providing reliable information to adapt vehicle behavior.



### Multiclass segmentation as multitask learning for drusen segmentation in retinal optical coherence tomography
- **Arxiv ID**: http://arxiv.org/abs/1906.07679v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07679v2)
- **Published**: 2019-06-18 16:45:46+00:00
- **Updated**: 2019-07-24 13:41:06+00:00
- **Authors**: Rhona Asgari, José Ignacio Orlando, Sebastian Waldstein, Ferdinand Schlanitz, Magdalena Baratsits, Ursula Schmidt-Erfurth, Hrvoje Bogunović
- **Comment**: Accepted for publication in MICCAI 2019
- **Journal**: None
- **Summary**: Automated drusen segmentation in retinal optical coherence tomography (OCT) scans is relevant for understanding age-related macular degeneration (AMD) risk and progression. This task is usually performed by segmenting the top/bottom anatomical interfaces that define drusen, the outer boundary of the retinal pigment epithelium (OBRPE) and the Bruch's membrane (BM), respectively. In this paper we propose a novel multi-decoder architecture that tackles drusen segmentation as a multitask problem. Instead of training a multiclass model for OBRPE/BM segmentation, we use one decoder per target class and an extra one aiming for the area between the layers. We also introduce connections between each class-specific branch and the additional decoder to increase the regularization effect of this surrogate task. We validated our approach on private/public data sets with 166 early/intermediate AMD Spectralis, and 200 AMD and control Bioptigen OCT volumes, respectively. Our method consistently outperformed several baselines in both layer and drusen segmentation evaluations.



### Expressing Visual Relationships via Language
- **Arxiv ID**: http://arxiv.org/abs/1906.07689v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.07689v2)
- **Published**: 2019-06-18 17:01:21+00:00
- **Updated**: 2019-06-19 02:49:11+00:00
- **Authors**: Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, Mohit Bansal
- **Comment**: ACL 2019 (11 pages)
- **Journal**: None
- **Summary**: Describing images with text is a fundamental problem in vision-language research. Current studies in this domain mostly focus on single image captioning. However, in various real applications (e.g., image editing, difference interpretation, and retrieval), generating relational captions for two images, can also be very useful. This important problem has not been explored mostly due to lack of datasets and effective models. To push forward the research in this direction, we first introduce a new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. We then propose a new relational speaker model based on an encoder-decoder architecture with static relational attention and sequential multi-head attention. We also extend the model with dynamic relational attention, which calculates visual alignment while decoding. Our models are evaluated on our newly collected and two public datasets consisting of image pairs annotated with relationship sentences. Experimental results, based on both automatic and human evaluation, demonstrate that our model outperforms all baselines and existing methods on all the datasets.



### Neural Volumes: Learning Dynamic Renderable Volumes from Images
- **Arxiv ID**: http://arxiv.org/abs/1906.07751v1
- **DOI**: 10.1145/3306346.3323020
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07751v1)
- **Published**: 2019-06-18 18:21:46+00:00
- **Updated**: 2019-06-18 18:21:46+00:00
- **Authors**: Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh
- **Comment**: Accepted to SIGGRAPH 2019
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65
- **Summary**: Modeling and rendering of dynamic scenes is challenging, as natural scenes often contain complex phenomena such as thin structures, evolving topology, translucency, scattering, occlusion, and biological motion. Mesh-based reconstruction and tracking often fail in these cases, and other approaches (e.g., light field video) typically rely on constrained viewing conditions, which limit interactivity. We circumvent these difficulties by presenting a learning-based approach to representing dynamic objects inspired by the integral projection model used in tomographic imaging. The approach is supervised directly from 2D images in a multi-view capture setting and does not require explicit reconstruction or tracking of the object. Our method has two primary components: an encoder-decoder network that transforms input images into a 3D volume representation, and a differentiable ray-marching operation that enables end-to-end training. By virtue of its 3D representation, our construction extrapolates better to novel viewpoints compared to screen-space rendering techniques. The encoder-decoder architecture learns a latent representation of a dynamic scene that enables us to produce novel content sequences not seen during training. To overcome memory limitations of voxel-based representations, we learn a dynamic irregular grid structure implemented with a warp field during ray-marching. This structure greatly improves the apparent resolution and reduces grid-like artifacts and jagged motion. Finally, we demonstrate how to incorporate surface-based representations into our volumetric-learning framework for applications where the highest resolution is required, using facial performance capture as a case in point.



### Tumor Saliency Estimation for Breast Ultrasound Images via Breast Anatomy Modeling
- **Arxiv ID**: http://arxiv.org/abs/1906.07760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07760v1)
- **Published**: 2019-06-18 18:35:01+00:00
- **Updated**: 2019-06-18 18:35:01+00:00
- **Authors**: Fei Xu, Yingtao Zhang, Min Xian, H. D. Cheng, Boyu Zhang, Jianrui Ding, Chunping Ning, Ying Wang
- **Comment**: submitted a journal paper
- **Journal**: None
- **Summary**: Tumor saliency estimation aims to localize tumors by modeling the visual stimuli in medical images. However, it is a challenging task for breast ultrasound due to the complicated anatomic structure of the breast and poor image quality; and existing saliency estimation approaches only model generic visual stimuli, e.g., local and global contrast, location, and feature correlation, and achieve poor performance for tumor saliency estimation. In this paper, we propose a novel optimization model to estimate tumor saliency by utilizing breast anatomy. First, we model breast anatomy and decompose breast ultrasound image into layers using Neutro-Connectedness; then utilize the layers to generate the foreground and background maps; and finally propose a novel objective function to estimate the tumor saliency by integrating the foreground map, background map, adaptive center bias, and region-based correlation cues. The extensive experiments demonstrate that the proposed approach obtains more accurate foreground and background maps with the assistance of the breast anatomy; especially, for the images having large or small tumors; meanwhile, the new objective function can handle the images without tumors. The newly proposed method achieves state-of-the-art performance when compared to eight tumor saliency estimation approaches using two breast ultrasound datasets.



### Crop Lodging Prediction from UAV-Acquired Images of Wheat and Canola using a DCNN Augmented with Handcrafted Texture Features
- **Arxiv ID**: http://arxiv.org/abs/1906.07771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07771v1)
- **Published**: 2019-06-18 19:11:38+00:00
- **Updated**: 2019-06-18 19:11:38+00:00
- **Authors**: Sara Mardanisamani, Farhad Maleki, Sara Hosseinzadeh Kassani, Sajith Rajapaksa, Hema Duddu, Menglu Wang, Steve Shirtliffe, Seungbum Ryu, Anique Josuttes, Ti Zhang, Sally Vail, Curtis Pozniak, Isobel Parkin, Ian Stavness, Mark Eramian
- **Comment**: None
- **Journal**: None
- **Summary**: Lodging, the permanent bending over of food crops, leads to poor plant growth and development. Consequently, lodging results in reduced crop quality, lowers crop yield, and makes harvesting difficult. Plant breeders routinely evaluate several thousand breeding lines, and therefore, automatic lodging detection and prediction is of great value aid in selection. In this paper, we propose a deep convolutional neural network (DCNN) architecture for lodging classification using five spectral channel orthomosaic images from canola and wheat breeding trials. Also, using transfer learning, we trained 10 lodging detection models using well-established deep convolutional neural network architectures. Our proposed model outperforms the state-of-the-art lodging detection methods in the literature that use only handcrafted features. In comparison to 10 DCNN lodging detection models, our proposed model achieves comparable results while having a substantially lower number of parameters. This makes the proposed model suitable for applications such as real-time classification using inexpensive hardware for high-throughput phenotyping pipelines. The GitHub repository at https://github.com/FarhadMaleki/LodgedNet contains code and models.



### Quantifying and Leveraging Classification Uncertainty for Chest Radiograph Assessment
- **Arxiv ID**: http://arxiv.org/abs/1906.07775v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07775v1)
- **Published**: 2019-06-18 19:21:44+00:00
- **Updated**: 2019-06-18 19:21:44+00:00
- **Authors**: Florin C. Ghesu, Bogdan Georgescu, Eli Gibson, Sebastian Guendel, Mannudeep K. Kalra, Ramandeep Singh, Subba R. Digumarthy, Sasa Grbic, Dorin Comaniciu
- **Comment**: Accepted for presentation at MICCAI 2019
- **Journal**: None
- **Summary**: The interpretation of chest radiographs is an essential task for the detection of thoracic diseases and abnormalities. However, it is a challenging problem with high inter-rater variability and inherent ambiguity due to inconclusive evidence in the data, limited data quality or subjective definitions of disease appearance. Current deep learning solutions for chest radiograph abnormality classification are typically limited to providing probabilistic predictions, relying on the capacity of learning models to adapt to the high degree of label noise and become robust to the enumerated causal factors. In practice, however, this leads to overconfident systems with poor generalization on unseen data. To account for this, we propose an automatic system that learns not only the probabilistic estimate on the presence of an abnormality, but also an explicit uncertainty measure which captures the confidence of the system in the predicted output. We argue that explicitly learning the classification uncertainty as an orthogonal measure to the predicted output, is essential to account for the inherent variability characteristic of this data. Experiments were conducted on two datasets of chest radiographs of over 85,000 patients. Sample rejection based on the predicted uncertainty can significantly improve the ROC-AUC, e.g., by 8% to 0.91 with an expected rejection rate of under 25%. Eliminating training samples using uncertainty-driven bootstrapping, enables a significant increase in robustness and accuracy. In addition, we present a multi-reader study showing that the predictive uncertainty is indicative of reader errors.



### SEN12MS -- A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion
- **Arxiv ID**: http://arxiv.org/abs/1906.07789v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.07789v1)
- **Published**: 2019-06-18 20:21:41+00:00
- **Updated**: 2019-06-18 20:21:41+00:00
- **Authors**: Michael Schmitt, Lloyd Haydn Hughes, Chunping Qiu, Xiao Xiang Zhu
- **Comment**: accepted for publication in the ISPRS Annals of the Photogrammetry,
  Remote Sensing and Spatial Information Sciences (online from September 2019)
- **Journal**: None
- **Summary**: The availability of curated large-scale training data is a crucial factor for the development of well-generalizing deep learning methods for the extraction of geoinformation from multi-sensor remote sensing imagery. While quite some datasets have already been published by the community, most of them suffer from rather strong limitations, e.g. regarding spatial coverage, diversity or simply number of available samples. Exploiting the freely available data acquired by the Sentinel satellites of the Copernicus program implemented by the European Space Agency, as well as the cloud computing facilities of Google Earth Engine, we provide a dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps. With all patches being fully georeferenced at a 10 m ground sampling distance and covering all inhabited continents during all meteorological seasons, we expect the dataset to support the community in developing sophisticated deep learning-based approaches for common tasks such as scene classification or semantic segmentation for land cover mapping.



### Image Super Resolution via Bilinear Pooling: Application to Confocal Endomicroscopy
- **Arxiv ID**: http://arxiv.org/abs/1906.07802v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07802v2)
- **Published**: 2019-06-18 20:39:37+00:00
- **Updated**: 2019-07-23 17:29:46+00:00
- **Authors**: Saeed Izadi, Darren Sutton, Ghassan Hamarneh
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Recent developments in image acquisition literature have miniaturized the confocal laser endomicroscopes to improve usability and flexibility of the apparatus in actual clinical settings. However, miniaturized devices collect less light and have fewer optical components, resulting in pixelation artifacts and low resolution images. Owing to the strength of deep networks, many supervised methods known as super resolution have achieved considerable success in restoring low resolution images by generating the missing high frequency details. In this work, we propose a novel attention mechanism that, for the first time, combines 1st- and 2nd-order statistics for pooling operation, in the spatial and channel-wise dimensions. We compare the efficacy of our method to 11 other existing single image super resolution techniques that compensate for the reduction in image quality caused by the necessity of endomicroscope miniaturization. All evaluations are carried out on three publicly available datasets. Experimental results show that our method can produce competitive results against state-of-the-art in terms of PSNR, SSIM, and IFC metrics. Additionally, our proposed method contains small number of parameters, which makes it lightweight and fast for real-time applications.



### Key Instance Selection for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.07851v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.07851v2)
- **Published**: 2019-06-18 23:49:22+00:00
- **Updated**: 2019-07-26 07:39:17+00:00
- **Authors**: Donghyeon Cho, Sungeun Hong, Sungil Kang, Jiwon Kim
- **Comment**: Ranked 3rd in 'Unsupervised DAVIS Challenge' (CVPR 2019)
- **Journal**: None
- **Summary**: This paper proposes key instance selection based on video saliency covering objectness and dynamics for unsupervised video object segmentation (UVOS). Our method takes frames sequentially and extracts object proposals with corresponding masks for each frame. We link objects according to their similarity until the M-th frame and then assign them unique IDs (i.e., instances). Similarity measure takes into account multiple properties such as ReID descriptor, expected trajectory, and semantic co-segmentation result. After M-th frame, we select K IDs based on video saliency and frequency of appearance; then only these key IDs are tracked through the remaining frames. Thanks to these technical contributions, our results are ranked third on the leaderboard of UVOS DAVIS challenge.



