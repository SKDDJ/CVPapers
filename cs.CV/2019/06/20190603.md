# Arxiv Papers in cs.CV on 2019-06-03
### Generating Question Relevant Captions to Aid Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1906.00513v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1906.00513v3)
- **Published**: 2019-06-03 00:42:08+00:00
- **Updated**: 2020-01-03 19:12:39+00:00
- **Authors**: Jialin Wu, Zeyuan Hu, Raymond J. Mooney
- **Comment**: ACL 2019 camera-ready
- **Journal**: None
- **Summary**: Visual question answering (VQA) and image captioning require a shared body of general knowledge connecting language and vision. We present a novel approach to improve VQA performance that exploits this connection by jointly generating captions that are targeted to help answer a specific visual question. The model is trained using an existing caption dataset by automatically determining question-relevant captions using an online gradient-based method. Experimental results on the VQA v2 challenge demonstrates that our approach obtains state-of-the-art VQA performance (e.g. 68.4% on the Test-standard set using a single model) by simultaneously generating question-relevant captions.



### 3D Magic Mirror: Automatic Video to 3D Caricature Translation
- **Arxiv ID**: http://arxiv.org/abs/1906.00544v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00544v1)
- **Published**: 2019-06-03 03:12:29+00:00
- **Updated**: 2019-06-03 03:12:29+00:00
- **Authors**: Yudong Guo, Luo Jiang, Lin Cai, Juyong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Caricature is an abstraction of a real person which distorts or exaggerates certain features, but still retains a likeness. While most existing works focus on 3D caricature reconstruction from 2D caricatures or translating 2D photos to 2D caricatures, this paper presents a real-time and automatic algorithm for creating expressive 3D caricatures with caricature style texture map from 2D photos or videos. To solve this challenging ill-posed reconstruction problem and cross-domain translation problem, we first reconstruct the 3D face shape for each frame, and then translate 3D face shape from normal style to caricature style by a novel identity and expression preserving VAE-CycleGAN. Based on a labeling formulation, the caricature texture map is constructed from a set of multi-view caricature images generated by CariGANs. The effectiveness and efficiency of our method are demonstrated by comparison with baseline implementations. The perceptual study shows that the 3D caricatures generated by our method meet people's expectations of 3D caricature style.



### Rethinking Loss Design for Large-scale 3D Shape Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1906.00546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00546v1)
- **Published**: 2019-06-03 03:17:22+00:00
- **Updated**: 2019-06-03 03:17:22+00:00
- **Authors**: Zhaoqun Li, Cheng Xu, Biao Leng
- **Comment**: Accepted by IJCAI2019
- **Journal**: None
- **Summary**: Learning discriminative shape representations is a crucial issue for large-scale 3D shape retrieval. In this paper, we propose the Collaborative Inner Product Loss (CIP Loss) to obtain ideal shape embedding that discriminative among different categories and clustered within the same class. Utilizing simple inner product operation, CIP loss explicitly enforces the features of the same class to be clustered in a linear subspace, while inter-class subspaces are constrained to be at least orthogonal. Compared to previous metric loss functions, CIP loss could provide more clear geometric interpretation for the embedding than Euclidean margin, and is easy to implement without normalization operation referring to cosine margin. Moreover, our proposed loss term can combine with other commonly used loss functions and can be easily plugged into existing off-the-shelf architectures. Extensive experiments conducted on the two public 3D object retrieval datasets, ModelNet and ShapeNetCore 55, demonstrate the effectiveness of our proposal, and our method has achieved state-of-the-art results on both datasets.



### Learning to Self-Train for Semi-Supervised Few-Shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1906.00562v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00562v2)
- **Published**: 2019-06-03 04:09:32+00:00
- **Updated**: 2019-09-29 04:43:05+00:00
- **Authors**: Xinzhe Li, Qianru Sun, Yaoyao Liu, Shibao Zheng, Qin Zhou, Tat-Seng Chua, Bernt Schiele
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Journal**: None
- **Summary**: Few-shot classification (FSC) is challenging due to the scarcity of labeled training data (e.g. only one labeled data point per class). Meta-learning has shown to achieve promising results by learning to initialize a classification model for FSC. In this paper we propose a novel semi-supervised meta-learning method called learning to self-train (LST) that leverages unlabeled data and specifically meta-learns how to cherry-pick and label such unsupervised data to further improve performance. To this end, we train the LST model through a large number of semi-supervised few-shot tasks. On each task, we train a few-shot model to predict pseudo labels for unlabeled data, and then iterate the self-training steps on labeled and pseudo-labeled data with each step followed by fine-tuning. We additionally learn a soft weighting network (SWN) to optimize the self-training weights of pseudo labels so that better ones can contribute more to gradient descent optimization. We evaluate our LST method on two ImageNet benchmarks for semi-supervised few-shot classification and achieve large improvements over the state-of-the-art method. Code is at https://github.com/xinzheli1217/learning-to-self-train.



### Discovering Neural Wirings
- **Arxiv ID**: http://arxiv.org/abs/1906.00586v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.00586v5)
- **Published**: 2019-06-03 05:58:33+00:00
- **Updated**: 2019-11-17 02:54:50+00:00
- **Authors**: Mitchell Wortsman, Ali Farhadi, Mohammad Rastegari
- **Comment**: NeurIPS 2019 Camera Ready
- **Journal**: None
- **Summary**: The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training -- as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1we boost the ImageNet accuracy by 10% at ~41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery with a combinatorial number of subnetworks. Code and pretrained models are available at https://github.com/allenai/dnw while additional visualizations may be found at https://mitchellnw.github.io/blog/2019/dnw/.



### Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.01452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01452v1)
- **Published**: 2019-06-03 06:04:00+00:00
- **Updated**: 2019-06-03 06:04:00+00:00
- **Authors**: Wei Zhang, Bairui Wang, Lin Ma, Wei Liu
- **Comment**: Accepted by TPAMI. arXiv admin note: substantial text overlap with
  arXiv:1803.11438
- **Journal**: None
- **Summary**: In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.



### Panoptic Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1906.00590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00590v1)
- **Published**: 2019-06-03 06:18:30+00:00
- **Updated**: 2019-06-03 06:18:30+00:00
- **Authors**: Yuan Hu, Yingtian Zou, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Pursuing more complete and coherent scene understanding towards realistic vision applications drives edge detection from category-agnostic to category-aware semantic level. However, finer delineation of instance-level boundaries still remains unexcavated. In this work, we address a new finer-grained task, termed panoptic edge detection (PED), which aims at predicting semantic-level boundaries for stuff categories and instance-level boundaries for instance categories, in order to provide more comprehensive and unified scene understanding from the perspective of edges.We then propose a versatile framework, Panoptic Edge Network (PEN), which aggregates different tasks of object detection, semantic and instance edge detection into a single holistic network with multiple branches. Based on the same feature representation, the semantic edge branch produces semantic-level boundaries for all categories and the object detection branch generates instance proposals. Conditioned on the prior information from these two branches, the instance edge branch aims at instantiating edge predictions for instance categories. Besides, we also devise a Panoptic Dual F-measure (F2) metric for the new PED task to uniformly measure edge prediction quality for both stuff and instances. By joint end-to-end training, the proposed PEN framework outperforms all competitive baselines on Cityscapes and ADE20K datasets.



### RF-Net: An End-to-End Image Matching Network based on Receptive Field
- **Arxiv ID**: http://arxiv.org/abs/1906.00604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00604v1)
- **Published**: 2019-06-03 07:11:48+00:00
- **Updated**: 2019-06-03 07:11:48+00:00
- **Authors**: Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, Jonathan Li, Chenglu Wen, Ming Cheng, Zijian He
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: This paper proposes a new end-to-end trainable matching network based on receptive field, RF-Net, to compute sparse correspondence between images. Building end-to-end trainable matching framework is desirable and challenging. The very recent approach, LF-Net, successfully embeds the entire feature extraction pipeline into a jointly trainable pipeline, and produces the state-of-the-art matching results. This paper introduces two modifications to the structure of LF-Net. First, we propose to construct receptive feature maps, which lead to more effective keypoint detection. Second, we introduce a general loss function term, neighbor mask, to facilitate training patch selection. This results in improved stability in descriptor training. We trained RF-Net on the open dataset HPatches, and compared it with other methods on multiple benchmark datasets. Experiments show that RF-Net outperforms existing state-of-the-art methods.



### Perceptual Embedding Consistency for Seamless Reconstruction of Tilewise Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.00617v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00617v1)
- **Published**: 2019-06-03 07:57:13+00:00
- **Updated**: 2019-06-03 07:57:13+00:00
- **Authors**: Amal Lahiani, Nassir Navab, Shadi Albarqouni, Eldad Klaiman
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer is a field with growing interest and use cases in deep learning. Recent work has shown Generative Adversarial Networks(GANs) can be used to create realistic images of virtually stained slide images in digital pathology with clinically validated interpretability. Digital pathology images are typically of extremely high resolution, making tilewise analysis necessary for deep learning applications. It has been shown that image generators with instance normalization can cause a tiling artifact when a large image is reconstructed from the tilewise analysis. We introduce a novel perceptual embedding consistency loss significantly reducing the tiling artifact created in the reconstructed whole slide image (WSI). We validate our results by comparing virtually stained slide images with consecutive real stained tissue slide images. We also demonstrate that our model is more robust to contrast, color and brightness perturbations by running comparative sensitivity analysis tests.



### Deep Face Recognition Model Compression via Knowledge Transfer and Distillation
- **Arxiv ID**: http://arxiv.org/abs/1906.00619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00619v1)
- **Published**: 2019-06-03 07:59:53+00:00
- **Updated**: 2019-06-03 07:59:53+00:00
- **Authors**: Jayashree Karlekar, Jiashi Feng, Zi Sian Wong, Sugiri Pranata
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Fully convolutional networks (FCNs) have become de facto tool to achieve very high-level performance for many vision and non-vision tasks in general and face recognition in particular. Such high-level accuracies are normally obtained by very deep networks or their ensemble. However, deploying such high performing models to resource constraint devices or real-time applications is challenging. In this paper, we present a novel model compression approach based on student-teacher paradigm for face recognition applications. The proposed approach consists of training teacher FCN at bigger image resolution while student FCNs are trained at lower image resolutions than that of teacher FCN. We explored three different approaches to train student FCNs: knowledge transfer (KT), knowledge distillation (KD) and their combination. Experimental evaluation on LFW and IJB-C datasets demonstrate comparable improvements in accuracies with these approaches. Training low-resolution student FCNs from higher resolution teacher offer fourfold advantage of accelerated training, accelerated inference, reduced memory requirements and improved accuracies. We evaluated all models on IJB-C dataset and achieved state-of-the-art results on this benchmark. The teacher network and some student networks even achieved Top-1 performance on IJB-C dataset. The proposed approach is simple and hardware friendly, thus enables the deployment of high performing face recognition deep models to resource constraint devices.



### Computing Valid p-values for Image Segmentation by Selective Inference
- **Arxiv ID**: http://arxiv.org/abs/1906.00629v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, math.ST, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1906.00629v2)
- **Published**: 2019-06-03 08:27:27+00:00
- **Updated**: 2019-12-09 10:23:18+00:00
- **Authors**: Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani, Ichiro Takeuchi
- **Comment**: 20 pages, 8 figures
- **Journal**: None
- **Summary**: Image segmentation is one of the most fundamental tasks of computer vision. In many practical applications, it is essential to properly evaluate the reliability of individual segmentation results. In this study, we propose a novel framework to provide the statistical significance of segmentation results in the form of p-values. Specifically, we consider a statistical hypothesis test for determining the difference between the object and the background regions. This problem is challenging because the difference can be deceptively large (called segmentation bias) due to the adaptation of the segmentation algorithm to the data. To overcome this difficulty, we introduce a statistical approach called selective inference, and develop a framework to compute valid p-values in which the segmentation bias is properly accounted for. Although the proposed framework is potentially applicable to various segmentation algorithms, we focus in this paper on graph cut-based and threshold-based segmentation algorithms, and develop two specific methods to compute valid p-values for the segmentation results obtained by these algorithms. We prove the theoretical validity of these two methods and demonstrate their practicality by applying them to segmentation problems for medical images.



### How Much Does Audio Matter to Recognize Egocentric Object Interactions?
- **Arxiv ID**: http://arxiv.org/abs/1906.00634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.00634v1)
- **Published**: 2019-06-03 08:40:49+00:00
- **Updated**: 2019-06-03 08:40:49+00:00
- **Authors**: Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli
- **Comment**: Accepted for presentation at EPIC@CVPR2019 workshop
- **Journal**: None
- **Summary**: Sounds are an important source of information on our daily interactions with objects. For instance, a significant amount of people can discern the temperature of water that it is being poured just by using the sense of hearing. However, only a few works have explored the use of audio for the classification of object interactions in conjunction with vision or as single modality. In this preliminary work, we propose an audio model for egocentric action recognition and explore its usefulness on the parts of the problem (noun, verb, and action classification). Our model achieves a competitive result in terms of verb classification (34.26% accuracy) on a standard benchmark with respect to vision-based state of the art systems, using a comparatively lighter architecture.



### Robust copy-move forgery detection by false alarms control
- **Arxiv ID**: http://arxiv.org/abs/1906.00649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00649v1)
- **Published**: 2019-06-03 09:09:48+00:00
- **Updated**: 2019-06-03 09:09:48+00:00
- **Authors**: Thibaud Ehret
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting reliably copy-move forgeries is difficult because images do contain similar objects. The question is: how to discard natural image self-similarities while still detecting copy-moved parts as being "unnaturally similar"? Copy-move may have been performed after a rotation, a change of scale and followed by JPEG compression or the addition of noise. For this reason, we base our method on SIFT, which provides sparse keypoints with scale, rotation and illumination invariant descriptors. To discriminate natural descriptor matches from artificial ones, we introduce an a contrario method which gives theoretical guarantees on the number of false alarms. We validate our method on several databases. Being fully unsupervised it can be integrated into any generic automated image tampering detection pipeline.



### Probabilistic Noise2Void: Unsupervised Content-Aware Denoising
- **Arxiv ID**: http://arxiv.org/abs/1906.00651v2
- **DOI**: 10.3389/fcomp.2020.00005
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00651v2)
- **Published**: 2019-06-03 09:13:52+00:00
- **Updated**: 2019-06-04 08:25:30+00:00
- **Authors**: Alexander Krull, Tomas Vicar, Florian Jug
- **Comment**: 8 pages, 2 figures
- **Journal**: None
- **Summary**: Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods such as Noise2Void~(N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present 'Probabilistic Noise2Void' (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods.



### A Closed-form Solution to Universal Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1906.00668v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00668v2)
- **Published**: 2019-06-03 09:48:14+00:00
- **Updated**: 2019-08-14 02:47:49+00:00
- **Authors**: Ming Lu, Hao Zhao, Anbang Yao, Yurong Chen, Feng Xu, Li Zhang
- **Comment**: Accepted to ICCV 2019. Code is available at
  https://github.com/lu-m13/OptimalStyleTransfer
- **Journal**: None
- **Summary**: Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer. Code is available at https://github.com/lu-m13/OptimalStyleTransfer.



### Deeply-supervised Knowledge Synergy
- **Arxiv ID**: http://arxiv.org/abs/1906.00675v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.00675v2)
- **Published**: 2019-06-03 09:54:26+00:00
- **Updated**: 2019-06-04 02:45:53+00:00
- **Authors**: Dawei Sun, Anbang Yao, Aojun Zhou, Hao Zhao
- **Comment**: Added supplementary materials, and the code is available at
  https://github.com/sundw2014/DKS
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become deeper and more complicated compared with the pioneering AlexNet. However, current prevailing training scheme follows the previous way of adding supervision to the last layer of the network only and propagating error information up layer-by-layer. In this paper, we propose Deeply-supervised Knowledge Synergy (DKS), a new method aiming to train CNNs with improved generalization ability for image classification tasks without introducing extra computational cost during inference. Inspired by the deeply-supervised learning scheme, we first append auxiliary supervision branches on top of certain intermediate network layers. While properly using auxiliary supervision can improve model accuracy to some degree, we go one step further to explore the possibility of utilizing the probabilistic knowledge dynamically learnt by the classifiers connected to the backbone network as a new regularization to improve the training. A novel synergy loss, which considers pairwise knowledge matching among all supervision branches, is presented. Intriguingly, it enables dense pairwise knowledge matching operations in both top-down and bottom-up directions at each training iteration, resembling a dynamic synergy process for the same task. We evaluate DKS on image classification datasets using state-of-the-art CNN architectures, and show that the models trained with it are consistently better than the corresponding counterparts. For instance, on the ImageNet classification benchmark, our ResNet-152 model outperforms the baseline model with a 1.47% margin in Top-1 accuracy. Code is available at https://github.com/sundw2014/DKS.



### An Adaptive Training-less System for Anomaly Detection in Crowd Scenes
- **Arxiv ID**: http://arxiv.org/abs/1906.00705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00705v1)
- **Published**: 2019-06-03 11:04:20+00:00
- **Updated**: 2019-06-03 11:04:20+00:00
- **Authors**: Arindam Sikdar, Ananda S. Chowdhury
- **Comment**: 29 pages, 13 figures
- **Journal**: None
- **Summary**: Anomaly detection in crowd videos has become a popular area of research for the computer vision community. Several existing methods generally perform a prior training about the scene with or without the use of labeled data. However, it is difficult to always guarantee the availability of prior data, especially, for scenarios like remote area surveillance. To address such challenge, we propose an adaptive training-less system capable of detecting anomaly on-the-fly while dynamically estimating and adjusting response based on certain parameters. This makes our system both training-less and adaptive in nature. Our pipeline consists of three main components, namely, adaptive 3D-DCT model for multi-object detection-based association, local motion structure description through saliency modulated optic flow, and anomaly detection based on earth movers distance (EMD). The proposed model, despite being training-free, is found to achieve comparable performance with several state-of-the-art methods on the publicly available UCSD, UMN, CHUK-Avenue and ShanghaiTech datasets.



### cGANs with Conditional Convolution Layer
- **Arxiv ID**: http://arxiv.org/abs/1906.00709v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00709v2)
- **Published**: 2019-06-03 11:15:51+00:00
- **Updated**: 2020-04-08 10:09:15+00:00
- **Authors**: Min-Cheol Sagong, Yong-Goo Shin, Yoon-Jae Yeo, Seung Park, Sung-Jea Ko
- **Comment**: Submitted to IEEE Trans. Neural Networks and Learning Systems (TNNLS)
- **Journal**: None
- **Summary**: Conditional generative adversarial networks (cGANs) have been widely researched to generate class conditional images using a single generator. However, in the conventional cGANs techniques, it is still challenging for the generator to learn condition-specific features, since a standard convolutional layer with the same weights is used regardless of the condition. In this paper, we propose a novel convolution layer, called the conditional convolution layer, which directly generates different feature maps by employing the weights which are adjusted depending on the conditions. More specifically, in each conditional convolution layer, the weights are conditioned in a simple but effective way through filter-wise scaling and channel-wise shifting operations. In contrast to the conventional methods, the proposed method with a single generator can effectively handle condition-specific characteristics. The experimental results on CIFAR, LSUN and ImageNet datasets show that the generator with the proposed conditional convolution layer achieves a higher quality of conditional image generation than that with the standard convolution layer.



### Masked Non-Autoregressive Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1906.00717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00717v1)
- **Published**: 2019-06-03 11:34:41+00:00
- **Updated**: 2019-06-03 11:34:41+00:00
- **Authors**: Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: None
- **Journal**: None
- **Summary**: Existing captioning models often adopt the encoder-decoder architecture, where the decoder uses autoregressive decoding to generate captions, such that each token is generated sequentially given the preceding generated tokens. However, autoregressive decoding results in issues such as sequential error accumulation, slow generation, improper semantics and lack of diversity. Non-autoregressive decoding has been proposed to tackle slow generation for neural machine translation but suffers from multimodality problem due to the indirect modeling of the target distribution. In this paper, we propose masked non-autoregressive decoding to tackle the issues of both autoregressive decoding and non-autoregressive decoding. In masked non-autoregressive decoding, we mask several kinds of ratios of the input sequences during training, and generate captions parallelly in several stages from a totally masked sequence to a totally non-masked sequence in a compositional manner during inference. Experimentally our proposed model can preserve semantic content more effectively and can generate more diverse captions.



### Separate In Latent Space: Unsupervised Single Image Layer Separation
- **Arxiv ID**: http://arxiv.org/abs/1906.00734v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00734v3)
- **Published**: 2019-06-03 12:17:07+00:00
- **Updated**: 2019-09-07 07:56:52+00:00
- **Authors**: Yunfei Liu, Feng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Many real world vision tasks, such as reflection removal from a transparent surface and intrinsic image decomposition, can be modeled as single image layer separation. However, this problem is highly ill-posed, requiring accurately aligned and hard to collect triplet data to train the CNN models. To address this problem, this paper proposes an unsupervised method that requires no ground truth data triplet in training. At the core of the method are two assumptions about data distributions in the latent spaces of different layers, based on which a novel unsupervised layer separation pipeline can be derived. Then the method can be constructed based on the GANs framework with self-supervision and cycle consistency constraints, etc. Experimental results demonstrate its successfulness in outperforming existing unsupervised methods in both synthetic and real world tasks. The method also shows its ability to solve a more challenging multi-layer separation task.



### Deep Feature Learning from a Hospital-Scale Chest X-ray Dataset with Application to TB Detection on a Small-Scale Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.00768v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00768v1)
- **Published**: 2019-06-03 13:03:15+00:00
- **Updated**: 2019-06-03 13:03:15+00:00
- **Authors**: Ophir Gozes, Hayit Greenspan
- **Comment**: None
- **Journal**: None
- **Summary**: The use of ImageNet pre-trained networks is becoming widespread in the medical imaging community. It enables training on small datasets, commonly available in medical imaging tasks. The recent emergence of a large Chest X-ray dataset opened the possibility for learning features that are specific to the X-ray analysis task. In this work, we demonstrate that the features learned allow for better classification results for the problem of Tuberculosis detection and enable generalization to an unseen dataset. To accomplish the task of feature learning, we train a DenseNet-121 CNN on 112K images from the ChestXray14 dataset which includes labels of 14 common thoracic pathologies. In addition to the pathology labels, we incorporate metadata which is available in the dataset: Patient Positioning, Gender and Patient Age. We term this architecture MetaChexNet. As a by-product of the feature learning, we demonstrate state of the art performance on the task of patient Age \& Gender estimation using CNN's. Finally, we show the features learned using ChestXray14 allow for better transfer learning on small-scale datasets for Tuberculosis.



### An Information-Theoretical Approach to the Information Capacity and Cost-Effectiveness Evaluation of Color Palettes
- **Arxiv ID**: http://arxiv.org/abs/1906.02567v1
- **DOI**: 10.12988/ijco.2017.759
- **Categories**: **cs.HC**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1906.02567v1)
- **Published**: 2019-06-03 13:38:01+00:00
- **Updated**: 2019-06-03 13:38:01+00:00
- **Authors**: R. Tanju Sirmen, B. Burak Ustundag
- **Comment**: 9 pages
- **Journal**: International Journal of Computing and Optimization, Vol.4, 2017,
  No.1, pp. 43-51
- **Summary**: Colors are used as effective tools of representing and transferring information. Number of colors in a palette is the direct arbiter of the information conveying capacity. Yet it should be well elaborated, since increasing the entropy by adding colors comes with its cost on decoding. Despite the possible effects upon diverse applications, a methodology for cost-effectiveness evaluation of palettes seems deficient. In this work, this need is being addressed from an information-theoretical perspective, via the articulated metrics and formulae. Besides, the proposed metrics are computed for some developed and known palettes, and observed results are evaluated.



### DualDis: Dual-Branch Disentangling with Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.00804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00804v1)
- **Published**: 2019-06-03 13:42:59+00:00
- **Updated**: 2019-06-03 13:42:59+00:00
- **Authors**: Thomas Robert, Nicolas Thome, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, disentangling techniques aim at improving latent representations of images by modeling factors of variation. In this paper, we propose DualDis, a new auto-encoder-based framework that disentangles and linearizes class and attribute information. This is achieved thanks to a two-branch architecture forcing the separation of the two kinds of information, accompanied by a decoder for image reconstruction and generation. To effectively separate the information, we propose to use a combination of regular and adversarial classifiers to guide the two branches in specializing for class and attribute information respectively. We also investigate the possibility of using semi-supervised learning for an effective disentangling even using few labels. We leverage the linearization property of the latent spaces for semantic image editing and generation of new images. We validate our approach on CelebA, Yale-B and NORB by measuring the efficiency of information separation via classification metrics, visual image manipulation and data augmentation.



### GazeCorrection:Self-Guided Eye Manipulation in the wild using Self-Supervised Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.00805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1906.00805v1)
- **Published**: 2019-06-03 13:43:01+00:00
- **Updated**: 2019-06-03 13:43:01+00:00
- **Authors**: Jichao Zhang, Meng Sun, Jingjing Chen, Hao Tang, Yan Yan, Xueying Qin, Nicu Sebe
- **Comment**: None
- **Journal**: None
- **Summary**: Gaze correction aims to redirect the person's gaze into the camera by manipulating the eye region, and it can be considered as a specific image resynthesis problem. Gaze correction has a wide range of applications in real life, such as taking a picture with staring at the camera. In this paper, we propose a novel method that is based on the inpainting model to learn from the face image to fill in the missing eye regions with new contents representing corrected eye gaze. Moreover, our model does not require the training dataset labeled with the specific head pose and eye angle information, thus, the training data is easy to collect. To retain the identity information of the eye region in the original input, we propose a self-guided pretrained model to learn the angle-invariance feature. Experiments show our model achieves very compelling gaze-corrected results in the wild dataset which is collected from the website and will be introduced in details. Code is available at https://github.com/zhangqianhui/GazeCorrection.



### Zero-Shot Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.00817v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00817v2)
- **Published**: 2019-06-03 13:53:00+00:00
- **Updated**: 2019-11-18 11:10:40+00:00
- **Authors**: Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, Patrick Pérez
- **Comment**: NeurIPS 2019 (accepted)
- **Journal**: None
- **Summary**: Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called "generalized" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.



### Self-supervised Body Image Acquisition Using a Deep Neural Network for Sensorimotor Prediction
- **Arxiv ID**: http://arxiv.org/abs/1906.00825v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00825v1)
- **Published**: 2019-06-03 14:10:17+00:00
- **Updated**: 2019-06-03 14:10:17+00:00
- **Authors**: Alban Laflaquière, Verena V. Hafner
- **Comment**: 6 pages, 7 figures, submitted to ICDL-Epirob 2019
- **Journal**: None
- **Summary**: This work investigates how a naive agent can acquire its own body image in a self-supervised way, based on the predictability of its sensorimotor experience. Our working hypothesis is that, due to its temporal stability, an agent's body produces more consistent sensory experiences than the environment, which exhibits a greater variability. Given its motor experience, an agent can thus reliably predict what appearance its body should have. This intrinsic predictability can be used to automatically isolate the body image from the rest of the environment. We propose a two-branches deconvolutional neural network to predict the visual sensory state associated with an input motor state, as well as the prediction error associated with this input. We train the network on a dataset of first-person images collected with a simulated Pepper robot, and show how the network outputs can be used to automatically isolate its visible arm from the rest of the environment. Finally, the quality of the body image produced by the network is evaluated.



### From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots
- **Arxiv ID**: http://arxiv.org/abs/1906.00872v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00872v1)
- **Published**: 2019-06-03 15:28:48+00:00
- **Updated**: 2019-06-03 15:28:48+00:00
- **Authors**: Shizhe Chen, Qin Jin, Jianlong Fu
- **Comment**: Accepted by IJCAI 2019
- **Journal**: None
- **Summary**: The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behavior, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image-pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach significantly outperforms other state-of-the-art methods.



### A new nonlocal forward model for diffuse optical tomography
- **Arxiv ID**: http://arxiv.org/abs/1906.00882v1
- **DOI**: None
- **Categories**: **physics.comp-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00882v1)
- **Published**: 2019-06-03 15:40:51+00:00
- **Updated**: 2019-06-03 15:40:51+00:00
- **Authors**: Wenqi Lu, Jinming Duan, Joshua Deepak Veesa, Iain B. Styles
- **Comment**: 7 pages, 9 figures
- **Journal**: None
- **Summary**: The forward model in diffuse optical tomography (DOT) describes how light propagates through a turbid medium. It is often approximated by a diffusion equation (DE) that is numerically discretized by the classical finite element method (FEM). We propose a nonlocal diffusion equation (NDE) as a new forward model for DOT, the discretization of which is carried out with an efficient graph-based numerical method (GNM). To quantitatively evaluate the new forward model, we first conduct experiments on a homogeneous slab, where the numerical accuracy of both NDE and DE is compared against the existing analytical solution. We further evaluate NDE by comparing its image reconstruction performance (inverse problem) to that of DE. Our experiments show that NDE is quantitatively comparable to DE and is up to 64% faster due to the efficient graph-based representation that can be implemented identically for geometries in different dimensions.



### Fashion Editing with Adversarial Parsing Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.00884v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.00884v2)
- **Published**: 2019-06-03 15:43:33+00:00
- **Updated**: 2019-09-28 16:47:46+00:00
- **Authors**: Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Zhenyu Xie, Bowen Wu, Ziqi Zhang, Xiaohui Shen, Jian Yin
- **Comment**: 22 pages, 18 figures
- **Journal**: None
- **Summary**: Interactive fashion image manipulation, which enables users to edit images with sketches and color strokes, is an interesting research problem with great application value. Existing works often treat it as a general inpainting task and do not fully leverage the semantic structural information in fashion images. Moreover, they directly utilize conventional convolution and normalization layers to restore the incomplete image, which tends to wash away the sketch and color information. In this paper, we propose a novel Fashion Editing Generative Adversarial Network (FE-GAN), which is capable of manipulating fashion images by free-form sketches and sparse color strokes. FE-GAN consists of two modules: 1) a free-form parsing network that learns to control the human parsing generation by manipulating sketch and color; 2) a parsing-aware inpainting network that renders detailed textures with semantic guidance from the human parsing map. A new attention normalization layer is further applied at multiple scales in the decoder of the inpainting network to enhance the quality of the synthesized image. Extensive experiments on high-resolution fashion image datasets demonstrate that the proposed method significantly outperforms the state-of-the-art methods on image manipulation.



### Automated Steel Bar Counting and Center Localization with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.00891v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1906.00891v2)
- **Published**: 2019-06-03 15:49:27+00:00
- **Updated**: 2019-06-19 08:02:47+00:00
- **Authors**: Zhun Fan, Jiewei Lu, Benzhang Qiu, Tao Jiang, Kang An, Alex Noel Josephraj, Chuliang Wei
- **Comment**: Ready to submit IEEE Transactions on Industrial Informatics
- **Journal**: None
- **Summary**: Automated steel bar counting and center localization plays an important role in the factory automation of steel bars. Traditional methods only focus on steel bar counting and their performances are often limited by complex industrial environments. Convolutional neural network (CNN), which has great capability to deal with complex tasks in challenging environments, is applied in this work. A framework called CNN-DC is proposed to achieve automated steel bar counting and center localization simultaneously. The proposed framework CNN-DC first detects the candidate center points with a deep CNN. Then an effective clustering algorithm named as Distance Clustering(DC) is proposed to cluster the candidate center points and locate the true centers of steel bars. The proposed CNN-DC can achieve 99.26% accuracy for steel bar counting and 4.1% center offset for center localization on the established steel bar dataset, which demonstrates that the proposed CNN-DC can perform well on automated steel bar counting and center localization. Code is made publicly available at: https://github.com/BenzhangQiu/Steel-bar-Detection.



### The iMet Collection 2019 Challenge Dataset
- **Arxiv ID**: http://arxiv.org/abs/1906.00901v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00901v2)
- **Published**: 2019-06-03 16:10:14+00:00
- **Updated**: 2019-06-04 02:37:29+00:00
- **Authors**: Chenyang Zhang, Christine Kaeser-Chen, Grace Vesom, Jennie Choi, Maria Kessler, Serge Belongie
- **Comment**: 3 pages, 4 figures
- **Journal**: None
- **Summary**: Existing computer vision technologies in artwork recognition focus mainly on instance retrieval or coarse-grained attribute classification. In this work, we present a novel dataset for fine-grained artwork attribute recognition. The images in the dataset are professional photographs of classic artworks from the Metropolitan Museum of Art, and annotations are curated and verified by world-class museum experts. In addition, we also present the iMet Collection 2019 Challenge as part of the FGVC6 workshop. Through the competition, we aim to spur the enthusiasm of the fine-grained visual recognition research community and advance the state-of-the-art in digital curation of museum collections.



### eSLAM: An Energy-Efficient Accelerator for Real-Time ORB-SLAM on FPGA Platform
- **Arxiv ID**: http://arxiv.org/abs/1906.05096v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.05096v1)
- **Published**: 2019-06-03 16:30:06+00:00
- **Updated**: 2019-06-03 16:30:06+00:00
- **Authors**: Runze Liu, Jianlei Yang, Yiran Chen, Weisheng Zhao
- **Comment**: to appear in DAC 2019
- **Journal**: None
- **Summary**: Simultaneous Localization and Mapping (SLAM) is a critical task for autonomous navigation. However, due to the computational complexity of SLAM algorithms, it is very difficult to achieve real-time implementation on low-power platforms.We propose an energy efficient architecture for real-time ORB (Oriented-FAST and Rotated- BRIEF) based visual SLAM system by accelerating the most time consuming stages of feature extraction and matching on FPGA platform.Moreover, the original ORB descriptor pattern is reformed as a rotational symmetric manner which is much more hardware friendly. Optimizations including rescheduling and parallelizing are further utilized to improve the throughput and reduce the memory footprint. Compared with Intel i7 and ARM Cortex-A9 CPUs on TUM dataset, our FPGA realization achieves up to 3X and 31X frame rate improvement, as well as up to 71X and 25X energy efficiency improvement, respectively.



### 3D Appearance Super-Resolution with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.00925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.00925v2)
- **Published**: 2019-06-03 16:51:35+00:00
- **Updated**: 2019-06-04 12:52:53+00:00
- **Authors**: Yawei Li, Vagia Tsiminaki, Radu Timofte, Marc Pollefeys, Luc van Gool
- **Comment**: In CVPR 2019. Github papge:
  https://github.com/ofsoundof/3D_Appearance_SR
- **Journal**: None
- **Summary**: We tackle the problem of retrieving high-resolution (HR) texture maps of objects that are captured from multiple view points. In the multi-view case, model-based super-resolution (SR) methods have been recently proved to recover high quality texture maps. On the other hand, the advent of deep learning-based methods has already a significant impact on the problem of video and image SR. Yet, a deep learning-based approach to super-resolve the appearance of 3D objects is still missing. The main limitation of exploiting the power of deep learning techniques in the multi-view case is the lack of data. We introduce a 3D appearance SR (3DASR) dataset based on the existing ETH3D [42], SyB3R [31], MiddleBury, and our Collection of 3D scenes from TUM [21], Fountain [51] and Relief [53]. We provide the high- and low-resolution texture maps, the 3D geometric model, images and projection matrices. We exploit the power of 2D learning-based SR methods and design networks suitable for the 3D multi-view case. We incorporate the geometric information by introducing normal maps and further improve the learning process. Experimental results demonstrate that our proposed networks successfully incorporate the 3D geometric information and super-resolve the texture maps.



### Y-GAN: A Generative Adversarial Network for Depthmap Estimation from Multi-camera Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/1906.00932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00932v1)
- **Published**: 2019-06-03 17:11:18+00:00
- **Updated**: 2019-06-03 17:11:18+00:00
- **Authors**: Miguel Alonso Jr
- **Comment**: Accepted for Presentation at the ICML 2019 LatinX in AI Research
  Workshop
- **Journal**: None
- **Summary**: Depth perception is a key component for autonomous systems that interact in the real world, such as delivery robots, warehouse robots, and self-driving cars. Tasks in autonomous robotics such as 3D object recognition, simultaneous localization and mapping (SLAM), path planning and navigation, require some form of 3D spatial information. Depth perception is a long-standing research problem in computer vision and robotics and has had a long history. Many approaches using deep learning, ranging from structure from motion, shape-from-X, monocular, binocular, and multi-view stereo, have yielded acceptable results. However, there are several shortcomings of these methods such as requiring expensive hardware, needing supervised training data, no ground truth data for comparison, and disregard for occlusion. In order to address these shortcomings, this work proposes a new deep convolutional generative adversarial network architecture, called Y-GAN, that uses data from three cameras to estimate a depth map for each frame in a multi-camera video stream.



### Big-Data Clustering: K-Means or K-Indicators?
- **Arxiv ID**: http://arxiv.org/abs/1906.00938v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, math.OC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.00938v1)
- **Published**: 2019-06-03 17:30:24+00:00
- **Updated**: 2019-06-03 17:30:24+00:00
- **Authors**: Feiyu Chen, Yuchen Yang, Liwei Xu, Taiping Zhang, Yin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: The K-means algorithm is arguably the most popular data clustering method, commonly applied to processed datasets in some "feature spaces", as is in spectral clustering. Highly sensitive to initializations, however, K-means encounters a scalability bottleneck with respect to the number of clusters K as this number grows in big data applications. In this work, we promote a closely related model called K-indicators model and construct an efficient, semi-convex-relaxation algorithm that requires no randomized initializations. We present extensive empirical results to show advantages of the new algorithm when K is large. In particular, using the new algorithm to start the K-means algorithm, without any replication, can significantly outperform the standard K-means with a large number of currently state-of-the-art random replications.



### Adversarial Robustness as a Prior for Learned Representations
- **Arxiv ID**: http://arxiv.org/abs/1906.00945v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1906.00945v2)
- **Published**: 2019-06-03 17:55:20+00:00
- **Updated**: 2019-09-27 17:39:54+00:00
- **Authors**: Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Aleksander Madry
- **Comment**: None
- **Journal**: None
- **Summary**: An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .



### Comparing two- and three-view Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.01003v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U99, I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/1906.01003v1)
- **Published**: 2019-06-03 18:11:52+00:00
- **Updated**: 2019-06-03 18:11:52+00:00
- **Authors**: Zsolt Levente Kucsván
- **Comment**: None
- **Journal**: None
- **Summary**: To reconstruct the points in three dimensional space, we need at least two images. In this paper we compared two different methods: the first uses only two images, the second one uses three. During the research we measured how camera resolution, camera angles and camera distances influence the number of reconstructed points and the dispersion of them. The paper presents that using the two-view method, we can reconstruct significantly more points than using the other one, but the dispersion of points is smaller if we use the three-view method. Taking into consideration the different camera settings, we can say that both the two- and three-view method behaves the same, and the best parameters are also the same for both methods.



### Frontal Low-rank Random Tensors for Fine-grained Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.01004v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01004v2)
- **Published**: 2019-06-03 18:12:26+00:00
- **Updated**: 2020-04-06 14:42:57+00:00
- **Authors**: Yan Zhang, Krikamol Muandet, Qianli Ma, Heiko Neumann, Siyu Tang
- **Comment**: 19 pages (4 pages appendix), 3 figures. Revised theories and models,
  new experiments
- **Journal**: None
- **Summary**: Fine-grained action segmentation in long untrimmed videos is an important task for many applications such as surveillance, robotics, and human-computer interaction. To understand subtle and precise actions within a long time period, second-order information (e.g. feature covariance) or higher is reported to be effective in the literature. However, extracting such high-order information is considerably non-trivial. In particular, the dimensionality increases exponentially with the information order, and hence gaining more representation power also increases the computational cost and the risk of overfitting. In this paper, we propose an approach to representing high-order information for temporal action segmentation via a simple yet effective bilinear form. Specifically, our contributions are: (1) From the multilinear perspective, we derive a bilinear form of low complexity, assuming that the three-way tensor has low-rank frontal slices. (2) Rather than learning the tensor entries from data, we sample the entries from different underlying distributions, and prove that the underlying distribution influences the information order. (3) We employed our bilinear form as an intermediate layer in state-of-the-art deep neural networks, enabling to represent high-order information in complex deep models effectively and efficiently. Our experimental results demonstrate that the proposed bilinear form outperforms the previous state-of-the-art methods on the challenging temporal action segmentation task. One can see our project page for data, model and code: \url{https://vlg.inf.ethz.ch/projects/BilinearTCN/}.



### Mining YouTube - A dataset for learning fine-grained action concepts from webly supervised video data
- **Arxiv ID**: http://arxiv.org/abs/1906.01012v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1906.01012v1)
- **Published**: 2019-06-03 18:18:01+00:00
- **Updated**: 2019-06-03 18:18:01+00:00
- **Authors**: Hilde Kuehne, Ahsan Iqbal, Alexander Richard, Juergen Gall
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: Action recognition is so far mainly focusing on the problem of classification of hand selected preclipped actions and reaching impressive results in this field. But with the performance even ceiling on current datasets, it also appears that the next steps in the field will have to go beyond this fully supervised classification. One way to overcome those problems is to move towards less restricted scenarios. In this context we present a large-scale real-world dataset designed to evaluate learning techniques for human action recognition beyond hand-crafted datasets. To this end we put the process of collecting data on its feet again and start with the annotation of a test set of 250 cooking videos. The training data is then gathered by searching for the respective annotated classes within the subtitles of freely available videos. The uniqueness of the dataset is attributed to the fact that the whole process of collecting the data and training does not involve any human intervention. To address the problem of semantic inconsistencies that arise with this kind of training data, we further propose a semantical hierarchical structure for the mined classes.



### A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1906.01028v1
- **DOI**: 10.1109/TPAMI.2018.2884469
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1906.01028v1)
- **Published**: 2019-06-03 19:12:29+00:00
- **Updated**: 2019-06-03 19:12:29+00:00
- **Authors**: Hilde Kuehne, Alexander Richard, Juergen Gall
- **Comment**: 15 pages, preprint for IEEE TPAMI
  https://ieeexplore.ieee.org/document/8585084 (open access). arXiv admin note:
  substantial text overlap with arXiv:1703.08132
- **Journal**: None
- **Summary**: Action recognition has become a rapidly developing research field within the last decade. But with the increasing demand for large scale data, the need of hand annotated data for the training becomes more and more impractical. One way to avoid frame-based human annotation is the use of action order information to learn the respective action classes. In this context, we propose a hierarchical approach to address the problem of weakly supervised learning of human actions from ordered action labels by structuring recognition in a coarse-to-fine manner. Given a set of videos and an ordered list of the occurring actions, the task is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. We address this problem by combining a framewise RNN model with a coarse probabilistic inference. This combination allows for the temporal alignment of long sequences and thus, for an iterative training of both elements. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes as well as by the introduction of a regularizing length prior. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.



### Grounded Human-Object Interaction Hotspots from Video (Extended Abstract)
- **Arxiv ID**: http://arxiv.org/abs/1906.01963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01963v1)
- **Published**: 2019-06-03 19:12:55+00:00
- **Updated**: 2019-06-03 19:12:55+00:00
- **Authors**: Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1812.04558
- **Journal**: None
- **Summary**: Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction "hotspots" directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating how an object would be manipulated in a potential interaction, even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories. Project page: http://vision.cs.utexas.edu/projects/interaction-hotspots/



### Correctness Verification of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.01030v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.01030v3)
- **Published**: 2019-06-03 19:13:24+00:00
- **Updated**: 2022-08-23 19:01:30+00:00
- **Authors**: Yichen Yang, Martin Rinard
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel framework for specifying and verifying correctness globally for neural networks on perception tasks. Most previous works on neural network verification for perception tasks focus on robustness verification. Unlike robustness verification, which aims to verify that the prediction of a network is stable in some local regions around labelled points, our framework provides a way to specify correctness globally in the whole target input space and verify that the network is correct for all target inputs (or find the regions where the network is not correct). We provide a specification through 1) a state space consisting of all relevant states of the world and 2) an observation process that produces neural network inputs from the states of the world. Tiling the state and input spaces with a finite number of tiles, obtaining ground truth bounds from the state tiles and network output bounds from the input tiles, then comparing the ground truth and network output bounds delivers an upper bound on the network output error for any inputs of interest. The presented framework also enables detecting illegal inputs -- inputs that are not contained in (or close to) the target input space as defined by the state space and observation process (the neural network is not designed to work on them), so that we can flag when we don't have guarantees. Results from two case studies highlight the ability of our technique to verify error bounds over the whole target input space and show how the error bounds vary over the state and input spaces.



### Resolving Overlapping Convex Objects in Silhouette Images by Concavity Analysis and Gaussian Process
- **Arxiv ID**: http://arxiv.org/abs/1906.01049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1906.01049v1)
- **Published**: 2019-06-03 19:55:26+00:00
- **Updated**: 2019-06-03 19:55:26+00:00
- **Authors**: Sahar Zafari, Mariia Murashkina, Tuomas Eerola, Jouni Sampo, Heikki Kälviäinen, Heikki Haario
- **Comment**: 16 pages, 11 Figures
- **Journal**: None
- **Summary**: Segmentation of overlapping convex objects has various applications, for example, in nanoparticles and cell imaging. Often the segmentation method has to rely purely on edges between the background and foreground making the analyzed images essentially silhouette images. Therefore, to segment the objects, the method needs to be able to resolve the overlaps between multiple objects by utilizing prior information about the shape of the objects. This paper introduces a novel method for segmentation of clustered partially overlapping convex objects in silhouette images. The proposed method involves three main steps: pre-processing, contour evidence extraction, and contour estimation. Contour evidence extraction starts by recovering contour segments from a binarized image by detecting concave points. After this, the contour segments which belong to the same objects are grouped. The grouping is formulated as a combinatorial optimization problem and solved using the branch and bound algorithm. Finally, the full contours of the objects are estimated by a Gaussian process regression method. The experiments on a challenging dataset consisting of nanoparticles demonstrate that the proposed method outperforms three current state-of-art approaches in overlapping convex objects segmentation. The method relies only on edge information and can be applied to any segmentation problems where the objects are partially overlapping and have a convex shape.



### A Curated Image Parameter Dataset from Solar Dynamics Observatory Mission
- **Arxiv ID**: http://arxiv.org/abs/1906.01062v1
- **DOI**: 10.3847/1538-4365/ab253a
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1906.01062v1)
- **Published**: 2019-06-03 20:17:44+00:00
- **Updated**: 2019-06-03 20:17:44+00:00
- **Authors**: Azim Ahmadzadeh, Dustin J. Kempton, Rafal A. Angryk
- **Comment**: Accepted to The Astrophysical Journal Supplement Series, 2019, 29
  pages
- **Journal**: None
- **Summary**: We provide a large image parameter dataset extracted from the Solar Dynamics Observatory (SDO) mission's AIA instrument, for the period of January 2011 through the current date, with the cadence of six minutes, for nine wavelength channels. The volume of the dataset for each year is just short of 1 TiB. Towards achieving better results in the region classification of active regions and coronal holes, we improve upon the performance of a set of ten image parameters, through an in depth evaluation of various assumptions that are necessary for calculation of these image parameters. Then, where possible, a method for finding an appropriate settings for the parameter calculations was devised, as well as a validation task to show our improved results. In addition, we include comparisons of JP2 and FITS image formats using supervised classification models, by tuning the parameters specific to the format of the images from which they are extracted, and specific to each wavelength. The results of these comparisons show that utilizing JP2 images, which are significantly smaller files, is not detrimental to the region classification task that these parameters were originally intended for. Finally, we compute the tuned parameters on the AIA images and provide a public API (http://dmlab.cs.gsu.edu/dmlabapi) to access the dataset. This dataset can be used in a range of studies on AIA images, such as content-based image retrieval or tracking of solar events, where dimensionality reduction on the images is necessary for feasibility of the tasks.



### An Adaptive Random Path Selection Approach for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.01120v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.01120v3)
- **Published**: 2019-06-03 23:32:06+00:00
- **Updated**: 2020-01-24 07:09:38+00:00
- **Authors**: Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, Ming-Hsuan Yang
- **Comment**: Extended version of Random Path Selection for Incremental Learning,
  published at NeurIPS 2019
- **Journal**: None
- **Summary**: In a conventional supervised learning setting, a machine learning model has access to examples of all object classes that are desired to be recognized during the inference stage. This results in a fixed model that lacks the flexibility to adapt to new learning tasks. In practical settings, learning tasks often arrive in a sequence and the models must continually learn to increment their previously acquired knowledge. Existing incremental learning approaches fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called Adaptive RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing between tasks. We introduce a new network capacity measure that enables us to automatically switch paths if the already used resources are saturated. Since the proposed path-reuse strategy ensures forward knowledge transfer, our approach is efficient and has considerably less computation overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the Adaptive RPS-Net method surpasses the state-of-the-art performance for incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.



### Depth-Aware Arbitrary Style Transfer Using Instance Normalization
- **Arxiv ID**: http://arxiv.org/abs/1906.01123v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68T45, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1906.01123v2)
- **Published**: 2019-06-03 23:39:25+00:00
- **Updated**: 2020-07-08 17:53:11+00:00
- **Authors**: Victor Kitov, Konstantin Kozlovtsev, Margarita Mishustina
- **Comment**: Replacement of the previous version due to the following
  improvements: depth estimation methods comparison added, better depth
  estimation network used, transformation to proximity map added with offset
  and contrast parameters. Dependency on these parameters shown, comparison of
  AdaIN and proposed method added, user evaluation study completely remade for
  improved version of the proposed method
- **Journal**: None
- **Summary**: Style transfer is the process of rendering one image with some content in the style of another image, representing the style. Recent studies of Liu et al. (2017) show that traditional style transfer methods of Gatys et al. (2016) and Johnson et al. (2016) fail to reproduce the depth of the content image, which is critical for human perception. They suggest to preserve the depth map by additional regularizer in the optimized loss function, forcing preservation of the depth map. However these traditional methods are either computationally inefficient or require training a separate neural network for each style. AdaIN method of Huang et al. (2017) allows efficient transferring of arbitrary style without training a separate model but is not able to reproduce the depth map of the content image. We propose an extension to this method, allowing depth map preservation by applying variable stylization strength. Qualitative analysis and results of user evaluation study indicate that the proposed method provides better stylizations, compared to the original AdaIN style transfer method.



