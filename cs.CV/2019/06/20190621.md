# Arxiv Papers in cs.CV on 2019-06-21
### Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions
- **Arxiv ID**: http://arxiv.org/abs/1906.08945v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.08945v1)
- **Published**: 2019-06-21 04:35:14+00:00
- **Updated**: 2019-06-21 04:35:14+00:00
- **Authors**: Joey Hong, Benjamin Sapp, James Philbin
- **Comment**: Accepted at CVPR 2019
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019, pp. 8454-8462
- **Summary**: We focus on the problem of predicting future states of entities in complex, real-world driving scenarios. Previous research has used low-level signals to predict short time horizons, and has not addressed how to leverage key assets relied upon heavily by industry self-driving systems: (1) large 3D perception efforts which provide highly accurate 3D states of agents with rich attributes, and (2) detailed and accurate semantic maps of the environment (lanes, traffic lights, crosswalks, etc). We present a unified representation which encodes such high-level semantic information in a spatial grid, allowing the use of deep convolutional models to fuse complex scene context. This enables learning entity-entity and entity-environment interactions with simple, feed-forward computations in each timestep within an overall temporal model of an agent's behavior. We propose different ways of modelling the future as a distribution over future states using standard supervised learning. We introduce a novel dataset providing industry-grade rich perception and semantic inputs, and empirically show we can effectively learn fundamentals of driving behavior.



### Pixel-Accurate Depth Evaluation in Realistic Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1906.08953v2
- **DOI**: 10.1109/3DV.2019.00020
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08953v2)
- **Published**: 2019-06-21 05:35:36+00:00
- **Updated**: 2019-08-16 16:28:04+00:00
- **Authors**: Tobias Gruber, Mario Bijelic, Felix Heide, Werner Ritter, Klaus Dietmayer
- **Comment**: 3DV 2019
- **Journal**: Published in: 2019 International Conference on 3D Vision (3DV)
- **Summary**: This work introduces an evaluation benchmark for depth estimation and completion using high-resolution depth measurements with angular resolution of up to 25" (arcsecond), akin to a 50 megapixel camera with per-pixel depth available. Existing datasets, such as the KITTI benchmark, provide only sparse reference measurements with an order of magnitude lower angular resolution - these sparse measurements are treated as ground truth by existing depth estimation methods. We propose an evaluation methodology in four characteristic automotive scenarios recorded in varying weather conditions (day, night, fog, rain). As a result, our benchmark allows us to evaluate the robustness of depth sensing methods in adverse weather and different driving conditions. Using the proposed evaluation data, we demonstrate that current stereo approaches provide significantly more stable depth estimates than monocular methods and lidar completion in adverse weather. Data and code are available at https://github.com/gruberto/PixelAccurateDepthBenchmark.git.



### FBK-HUPBA Submission to the EPIC-Kitchens 2019 Action Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1906.08960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08960v1)
- **Published**: 2019-06-21 05:59:58+00:00
- **Updated**: 2019-06-21 05:59:58+00:00
- **Authors**: Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz
- **Comment**: Ranked 3rd in the EPIC-Kitchens 2019 action recognition challenge,
  held as part of CVPR 2019
- **Journal**: None
- **Summary**: In this report we describe the technical details of our submission to the EPIC-Kitchens 2019 action recognition challenge. To participate in the challenge we have developed a number of CNN-LSTA [3] and HF-TSN [2] variants, and submitted predictions from an ensemble compiled out of these two model families. Our submission, visible on the public leaderboard with team name FBK-HUPBA, achieved a top-1 action recognition accuracy of 35.54% on S1 setting, and 20.25% on S2 setting.



### Filter Early, Match Late: Improving Network-Based Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1906.12176v1
- **DOI**: 10.1109/iros40897.2019.8967783
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1906.12176v1)
- **Published**: 2019-06-21 06:12:27+00:00
- **Updated**: 2019-06-21 06:12:27+00:00
- **Authors**: Stephen Hausler, Adam Jacobson, Michael Milford
- **Comment**: Pre-print version of article which will be presented at IROS 2019
- **Journal**: None
- **Summary**: CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training.



### Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1906.08967v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.08967v3)
- **Published**: 2019-06-21 06:18:13+00:00
- **Updated**: 2020-03-15 22:46:11+00:00
- **Authors**: Yiqi Zhong, Cho-Ying Wu, Suya You, Ulrich Neumann
- **Comment**: NeurIPS 2019. Code link https://github.com/choyingw/CFCNet
- **Journal**: None
- **Summary**: In this paper, we propose our Correlation For Completion Network (CFCNet), an end-to-end deep learning model that uses the correlation between two data sources to perform sparse depth completion. CFCNet learns to capture, to the largest extent, the semantically correlated features between RGB and depth information. Through pairs of image pixels and the visible measurements in a sparse depth map, CFCNet facilitates feature-level mutual transformation of different data sources. Such a transformation enables CFCNet to predict features and reconstruct data of missing depth measurements according to their corresponding, transformed RGB features. We extend canonical correlation analysis to a 2D domain and formulate it as one of our training objectives (i.e. 2d deep canonical correlation, or "2D2CCA loss"). Extensive experiments validate the ability and flexibility of our CFCNet compared to the state-of-the-art methods on both indoor and outdoor scenes with different real-life sparse patterns. Codes are available at: https://github.com/choyingw/CFCNet.



### A Fourier Perspective on Model Robustness in Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1906.08988v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.08988v3)
- **Published**: 2019-06-21 07:31:26+00:00
- **Updated**: 2020-09-17 01:30:25+00:00
- **Authors**: Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin D. Cubuk, Justin Gilmer
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.



### Data-Efficient Learning for Sim-to-Real Robotic Grasping using Deep Point Cloud Prediction Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.08989v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.08989v1)
- **Published**: 2019-06-21 07:36:29+00:00
- **Updated**: 2019-06-21 07:36:29+00:00
- **Authors**: Xinchen Yan, Mohi Khansari, Jasmine Hsu, Yuanzheng Gong, Yunfei Bai, SÃ¶ren Pirk, Honglak Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Training a deep network policy for robot manipulation is notoriously costly and time consuming as it depends on collecting a significant amount of real world data. To work well in the real world, the policy needs to see many instances of the task, including various object arrangements in the scene as well as variations in object geometry, texture, material, and environmental illumination.   In this paper, we propose a method that learns to perform table-top instance grasping of a wide variety of objects while using no real world grasping data, outperforming the baseline using 2.5D shape by 10%. Our method learns 3D point cloud of object, and use that to train a domain-invariant grasping policy. We formulate the learning process as a two-step procedure: 1) Learning a domain-invariant 3D shape representation of objects from about 76K episodes in simulation and about 530 episodes in the real world, where each episode lasts less than a minute and 2) Learning a critic grasping policy in simulation only based on the 3D shape representation from step 1. Our real world data collection in step 1 is both cheaper and faster compared to existing approaches as it only requires taking multiple snapshots of the scene using a RGBD camera. Finally, the learned 3D representation is not specific to grasping, and can potentially be used in other interaction tasks.



### Acute Lymphoblastic Leukemia Classification from Microscopic Images using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.09020v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09020v2)
- **Published**: 2019-06-21 09:13:45+00:00
- **Updated**: 2020-04-01 08:01:39+00:00
- **Authors**: Jonas Prellberg, Oliver Kramer
- **Comment**: None
- **Journal**: None
- **Summary**: Examining blood microscopic images for leukemia is necessary when expensive equipment for flow cytometry is unavailable. Automated systems can ease the burden on medical experts for performing this examination and may be especially helpful to quickly screen a large number of patients. We present a simple, yet effective classification approach using a ResNeXt convolutional neural network with Squeeze-and-Excitation modules. The approach was evaluated in the C-NMC online challenge and achieves a weighted F1-score of 88.91% on the test set. Code is available at https://github.com/jprellberg/isbi2019cancer



### Backpropagation-Friendly Eigendecomposition
- **Arxiv ID**: http://arxiv.org/abs/1906.09023v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09023v2)
- **Published**: 2019-06-21 09:17:14+00:00
- **Updated**: 2019-06-27 07:42:43+00:00
- **Authors**: Wei Wang, Zheng Dang, Yinlin Hu, Pascal Fua, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full. In this paper, we introduce a numerically stable and differentiable approach to leveraging eigenvectors in deep networks. It can handle large matrices without requiring to split them. We demonstrate the better robustness of our approach over standard ED and PI for ZCA whitening, an alternative to batch normalization, and for PCA denoising, which we introduce as a new normalization strategy for deep networks, aiming to further denoise the network's features.



### Automatic Intracranial Brain Segmentation from Computed Tomography Head Images
- **Arxiv ID**: http://arxiv.org/abs/1906.09726v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09726v1)
- **Published**: 2019-06-21 09:29:27+00:00
- **Updated**: 2019-06-21 09:29:27+00:00
- **Authors**: Bhavya Ajani
- **Comment**: None
- **Journal**: None
- **Summary**: Fast and automatic algorithm to segment Brain (intracranial region) from computed tomography (CT) head images using combination of HU thresholding, identification of intracranial voxels through ray intersection with cranium, special binary erosion and connected components per slice.



### Evolution Attack On Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1906.09072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09072v1)
- **Published**: 2019-06-21 11:22:03+00:00
- **Updated**: 2019-06-21 11:22:03+00:00
- **Authors**: YiGui Luo, RuiJia Yang, Wei Sha, WeiYi Ding, YouTeng Sun, YiSi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Many studies have been done to prove the vulnerability of neural networks to adversarial example. A trained and well-behaved model can be fooled by a visually imperceptible perturbation, i.e., an originally correctly classified image could be misclassified after a slight perturbation. In this paper, we propose a black-box strategy to attack such networks using an evolution algorithm. First, we formalize the generation of an adversarial example into the optimization problem of perturbations that represent the noise added to an original image at each pixel. To solve this optimization problem in a black-box way, we find that an evolution algorithm perfectly meets our requirement since it can work without any gradient information. Therefore, we test various evolution algorithms, including a simple genetic algorithm, a parameter-exploring policy gradient, an OpenAI evolution strategy, and a covariance matrix adaptive evolution strategy. Experimental results show that a covariance matrix adaptive evolution Strategy performs best in this optimization problem. Additionally, we also perform several experiments to explore the effect of different regularizations on improving the quality of an adversarial example.



### Fully Decoupled Neural Network Learning Using Delayed Gradients
- **Arxiv ID**: http://arxiv.org/abs/1906.09108v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09108v3)
- **Published**: 2019-06-21 13:02:35+00:00
- **Updated**: 2019-11-22 07:01:47+00:00
- **Authors**: Huiping Zhuang, Yi Wang, Qinglai Liu, Shuai Zhang, Zhiping Lin
- **Comment**: None
- **Journal**: None
- **Summary**: Training neural networks with back-propagation (BP) requires a sequential passing of activations and gradients, which forces the network modules to work in a synchronous fashion. This has been recognized as the lockings (i.e., the forward, backward and update lockings) inherited from the BP. In this paper, we propose a fully decoupled training scheme using delayed gradients (FDG) to break all these lockings. The FDG splits a neural network into multiple modules and trains them independently and asynchronously using different workers (e.g., GPUs). We also introduce a gradient shrinking process to reduce the stale gradient effect caused by the delayed gradients. In addition, we prove that the proposed FDG algorithm guarantees a statistical convergence during training. Experiments are conducted by training deep convolutional neural networks to perform classification tasks on benchmark datasets, showing comparable or better results against the state-of-the-art methods as well as the BP in terms of both generalization and acceleration abilities. In particular, we show that the FDG is also able to train very wide networks (e.g., WRN-28-10) and extremely deep networks (e.g., ResNet-1202). Code is available at https://github.com/ZHUANGHP/FDG.



### A Multitask Network for Localization and Recognition of Text in Images
- **Arxiv ID**: http://arxiv.org/abs/1906.09266v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09266v1)
- **Published**: 2019-06-21 17:25:45+00:00
- **Updated**: 2019-06-21 17:25:45+00:00
- **Authors**: Mohammad Reza Sarshogh, Keegan E. Hines
- **Comment**: ICDAR 2019
- **Journal**: None
- **Summary**: We present an end-to-end trainable multi-task network that addresses the problem of lexicon-free text extraction from complex documents. This network simultaneously solves the problems of text localization and text recognition and text segments are identified with no post-processing, cropping, or word grouping. A convolutional backbone and Feature Pyramid Network are combined to provide a shared representation that benefits each of three model heads: text localization, classification, and text recognition. To improve recognition accuracy, we describe a dynamic pooling mechanism that retains high-resolution information across all RoIs. For text recognition, we propose a convolutional mechanism with attention which out-performs more common recurrent architectures. Our model is evaluated against benchmark datasets and comparable methods and achieves high performance in challenging regimes of non-traditional OCR.



### Hiding Faces in Plain Sight: Disrupting AI Face Synthesis with Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1906.09288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09288v1)
- **Published**: 2019-06-21 18:29:57+00:00
- **Updated**: 2019-06-21 18:29:57+00:00
- **Authors**: Yuezun Li, Xin Yang, Baoyuan Wu, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent years have seen fast development in synthesizing realistic human faces using AI technologies. Such fake faces can be weaponized to cause negative personal and social impact. In this work, we develop technologies to defend individuals from becoming victims of recent AI synthesized fake videos by sabotaging would-be training data. This is achieved by disrupting deep neural network (DNN) based face detection method with specially designed imperceptible adversarial perturbations to reduce the quality of the detected faces. We describe attacking schemes under white-box, gray-box and black-box settings, each with decreasing information about the DNN based face detectors. We empirically show the effectiveness of our methods in disrupting state-of-the-art DNN based face detectors on several datasets.



### Are you really looking at me? A Feature-Extraction Framework for Estimating Interpersonal Eye Gaze from Conventional Video
- **Arxiv ID**: http://arxiv.org/abs/1906.12175v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1906.12175v2)
- **Published**: 2019-06-21 19:23:31+00:00
- **Updated**: 2020-01-17 20:32:21+00:00
- **Authors**: Minh Tran, Taylan Sen, Kurtis Haut, Mohammad Rafayet Ali, Mohammed Ehsan Hoque
- **Comment**: None
- **Journal**: None
- **Summary**: Despite a revolution in the pervasiveness of video cameras in our daily lives, one of the most meaningful forms of nonverbal affective communication, interpersonal eye gaze, i.e. eye gaze relative to a conversation partner, is not available from common video. We introduce the Interpersonal-Calibrating Eye-gaze Encoder (ICE), which automatically extracts interpersonal gaze from video recordings without specialized hardware and without prior knowledge of participant locations. Leveraging the intuition that individuals spend a large portion of a conversation looking at each other enables the ICE dynamic clustering algorithm to extract interpersonal gaze. We validate ICE in both video chat using an objective metric with an infrared gaze tracker (F1=0.846, N=8), as well as in face-to-face communication with expert-rated evaluations of eye contact (r= 0.37, N=170). We then use ICE to analyze behavior in two different, yet important affective communication domains: interrogation-based deception detection, and communication skill assessment in speed dating. We find that honest witnesses break interpersonal gaze contact and look down more often than deceptive witnesses when answering questions (p=0.004, d=0.79). In predicting expert communication skill ratings in speed dating videos, we demonstrate that interpersonal gaze alone has more predictive power than facial expressions.



### Adversarial Examples to Fool Iris Recognition Systems
- **Arxiv ID**: http://arxiv.org/abs/1906.09300v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1906.09300v2)
- **Published**: 2019-06-21 19:30:41+00:00
- **Updated**: 2019-07-18 21:09:35+00:00
- **Authors**: Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: 2019 International Conference on Biometrics (ICB 2019)
- **Journal**: None
- **Summary**: Adversarial examples have recently proven to be able to fool deep learning methods by adding carefully crafted small perturbation to the input space image. In this paper, we study the possibility of generating adversarial examples for code-based iris recognition systems. Since generating adversarial examples requires back-propagation of the adversarial loss, conventional filter bank-based iris-code generation frameworks cannot be employed in such a setup. Therefore, to compensate for this shortcoming, we propose to train a deep auto-encoder surrogate network to mimic the conventional iris code generation procedure. This trained surrogate network is then deployed to generate the adversarial examples using the iterative gradient sign method algorithm. We consider non-targeted and targeted attacks through three attack scenarios. Considering these attacks, we study the possibility of fooling an iris recognition system in white-box and black-box frameworks.



### A Cyclically-Trained Adversarial Network for Invariant Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1906.09313v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09313v2)
- **Published**: 2019-06-21 20:43:16+00:00
- **Updated**: 2020-04-16 16:14:12+00:00
- **Authors**: Jiawei Chen, Janusz Konrad, Prakash Ishwar
- **Comment**: None
- **Journal**: None
- **Summary**: Recent studies show that deep neural networks are vulnerable to adversarial examples which can be generated via certain types of transformations. Being robust to a desired family of adversarial attacks is then equivalent to being invariant to a family of transformations. Learning invariant representations then naturally emerges as an important goal to achieve which we explore in this paper within specific application contexts. Specifically, we propose a cyclically-trained adversarial network to learn a mapping from image space to latent representation space and back such that the latent representation is invariant to a specified factor of variation (e.g., identity). The learned mapping assures that the synthesized image is not only realistic, but has the same values for unspecified factors (e.g., pose and illumination) as the original image and a desired value of the specified factor. Unlike disentangled representation learning, which requires two latent spaces, one for specified and another for unspecified factors, invariant representation learning needs only one such space. We encourage invariance to a specified factor by applying adversarial training using a variational autoencoder in the image space as opposed to the latent space. We strengthen this invariance by introducing a cyclic training process (forward and backward cycle). We also propose a new method to evaluate conditional generative networks. It compares how well different factors of variation can be predicted from the synthesized, as opposed to real, images. In quantitative terms, our approach attains state-of-the-art performance in experiments spanning three datasets with factors such as identity, pose, illumination or style. Our method produces sharp, high-quality synthetic images with little visible artefacts compared to previous approaches.



### Building a Benchmark Dataset and Classifiers for Sentence-Level Findings in AP Chest X-rays
- **Arxiv ID**: http://arxiv.org/abs/1906.09336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1906.09336v1)
- **Published**: 2019-06-21 21:34:57+00:00
- **Updated**: 2019-06-21 21:34:57+00:00
- **Authors**: Tanveer Syeda-Mahmood, Hassan M. Ahmad, Nadeem Ansari, Yaniv Gur, Satyananda Kashyap, Alexandros Karargyris, Mehdi Moradi, Anup Pillai, Karthik Sheshadri, Weiting Wang, Ken C. L. Wong, Joy T. Wu
- **Comment**: This paper was accepted by the IEEE International Symposium on
  Biomedical Imaging (ISBI) 2019
- **Journal**: None
- **Summary**: Chest X-rays are the most common diagnostic exams in emergency rooms and hospitals. There has been a surge of work on automatic interpretation of chest X-rays using deep learning approaches after the availability of large open source chest X-ray dataset from NIH. However, the labels are not sufficiently rich and descriptive for training classification tools. Further, it does not adequately address the findings seen in Chest X-rays taken in anterior-posterior (AP) view which also depict the placement of devices such as central vascular lines and tubes. In this paper, we present a new chest X-ray benchmark database of 73 rich sentence-level descriptors of findings seen in AP chest X-rays. We describe our method of obtaining these findings through a semi-automated ground truth generation process from crowdsourcing of clinician annotations. We also present results of building classifiers for these findings that show that such higher granularity labels can also be learned through the framework of deep learning classifiers.



### Boosting the rule-out accuracy of deep disease detection using class weight modifiers
- **Arxiv ID**: http://arxiv.org/abs/1906.09354v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1906.09354v1)
- **Published**: 2019-06-21 23:55:36+00:00
- **Updated**: 2019-06-21 23:55:36+00:00
- **Authors**: Alexandros Karargyris, Ken C. L. Wong, Joy T. Wu, Mehdi Moradi, Tanveer Syeda-Mahmood
- **Comment**: This paper was accepted by the IEEE International Symposium on
  Biomedical Imaging (ISBI) 2019
- **Journal**: None
- **Summary**: In many screening applications, the primary goal of a radiologist or assisting artificial intelligence is to rule out certain findings. The classifiers built for such applications are often trained on large datasets that derive labels from clinical notes written for patients. While the quality of the positive findings described in these notes is often reliable, lack of the mention of a finding does not always rule out the presence of it. This happens because radiologists comment on the patient in the context of the exam, for example focusing on trauma as opposed to chronic disease at emergency rooms. However, this disease finding ambiguity can affect the performance of algorithms. Hence it is critical to model the ambiguity during training. We propose a scheme to apply reasonable class weight modifiers to our loss function for the no mention cases during training. We experiment with two different deep neural network architectures and show that the proposed method results in a large improvement in the performance of the classifiers, specially on negated findings. The baseline performance of a custom-made dilated block network proposed in this paper shows an improvement in comparison with baseline DenseNet-201, while both architectures benefit from the new proposed loss function weighting scheme. Over 200,000 chest X-ray images and three highly common diseases, along with their negated counterparts, are included in this study.



