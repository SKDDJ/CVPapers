# Arxiv Papers in cs.CV on 2019-04-21
### Neural Architecture Search for Deep Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09523v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09523v2)
- **Published**: 2019-04-21 01:05:41+00:00
- **Updated**: 2019-04-26 09:54:47+00:00
- **Authors**: Ning Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: By the widespread popularity of electronic devices, the emergence of biometric technology has brought significant convenience to user authentication compared with the traditional password and mode unlocking. Among many biological characteristics, the face is a universal and irreplaceable feature that does not need too much cooperation and can significantly improve the user's experience at the same time. Face recognition is one of the main functions of electronic equipment propaganda. Hence it's virtually worth researching in computer vision. Previous work in this field has focused on two directions: converting loss function to improve recognition accuracy in traditional deep convolution neural networks (Resnet); combining the latest loss function with the lightweight system (MobileNet) to reduce network size at the minimal expense of accuracy. But none of these has changed the network structure. With the development of AutoML, neural architecture search (NAS) has shown excellent performance in the benchmark of image classification. In this paper, we integrate NAS technology into face recognition to customize a more suitable network. We quote the framework of neural architecture search which trains child and controller network alternately. At the same time, we mutate NAS by incorporating evaluation latency into rewards of reinforcement learning and utilize policy gradient algorithm to search the architecture automatically with the most classical cross-entropy loss. The network architectures we searched out have got state-of-the-art accuracy in the large-scale face dataset, which achieves 98.77% top-1 in MS-Celeb-1M and 99.89% in LFW with relatively small network size. To the best of our knowledge, this proposal is the first attempt to use NAS to solve the problem of Deep Face Recognition and achieve the best results in this domain.



### Metric Learning for Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1904.09524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09524v1)
- **Published**: 2019-04-21 01:07:44+00:00
- **Updated**: 2019-04-21 01:07:44+00:00
- **Authors**: Marc Niethammer, Roland Kwitt, Francois-Xavier Vialard
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Image registration is a key technique in medical image analysis to estimate deformations between image pairs. A good deformation model is important for high-quality estimates. However, most existing approaches use ad-hoc deformation models chosen for mathematical convenience rather than to capture observed data variation. Recent deep learning approaches learn deformation models directly from data. However, they provide limited control over the spatial regularity of transformations. Instead of learning the entire registration approach, we learn a spatially-adaptive regularizer within a registration model. This allows controlling the desired level of regularity and preserving structural properties of a registration model. For example, diffeomorphic transformations can be attained. Our approach is a radical departure from existing deep learning approaches to image registration by embedding a deep learning model in an optimization-based registration algorithm to parameterize and data-adapt the registration model itself.



### Automatic Temporally Coherent Video Colorization
- **Arxiv ID**: http://arxiv.org/abs/1904.09527v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09527v1)
- **Published**: 2019-04-21 01:50:22+00:00
- **Updated**: 2019-04-21 01:50:22+00:00
- **Authors**: Harrish Thasarathan, Kamyar Nazeri, Mehran Ebrahimi
- **Comment**: None
- **Journal**: None
- **Summary**: Greyscale image colorization for applications in image restoration has seen significant improvements in recent years. Many of these techniques that use learning-based methods struggle to effectively colorize sparse inputs. With the consistent growth of the anime industry, the ability to colorize sparse input such as line art can reduce significant cost and redundant work for production studios by eliminating the in-between frame colorization process. Simply using existing methods yields inconsistent colors between related frames resulting in a flicker effect in the final video. In order to successfully automate key areas of large-scale anime production, the colorization of line arts must be temporally consistent between frames. This paper proposes a method to colorize line art frames in an adversarial setting, to create temporally coherent video of large anime by improving existing image to image translation methods. We show that by adding an extra condition to the generator and discriminator, we can effectively create temporally consistent video sequences from anime line arts. Code and models available at: https://github.com/Harry-Thasarathan/TCVC



### Web Based Brain Volume Calculation for Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/1904.09977v1
- **DOI**: 10.1109/IEMBS.2008.4649380
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09977v1)
- **Published**: 2019-04-21 01:58:04+00:00
- **Updated**: 2019-04-21 01:58:04+00:00
- **Authors**: Kevin Karsch, Brian Grinstead, Qing He, Ye Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Brain volume calculations are crucial in modern medical research, especially in the study of neurodevelopmental disorders. In this paper, we present an algorithm for calculating two classifications of brain volume, total brain volume (TBV) and intracranial volume (ICV). Our algorithm takes MRI data as input, performs several preprocessing and intermediate steps, and then returns each of the two calculated volumes. To simplify this process and make our algorithm publicly accessible to anyone, we have created a web-based interface that allows users to upload their own MRI data and calculate the TBV and ICV for the given data. This interface provides a simple and efficient method for calculating these two classifications of brain volume, and it also removes the need for the user to download or install any applications.



### A Fast, Semi-Automatic Brain Structure Segmentation Algorithm for Magnetic Resonance Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.09978v1
- **DOI**: 10.1109/BIBM.2009.40
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09978v1)
- **Published**: 2019-04-21 01:59:46+00:00
- **Updated**: 2019-04-21 01:59:46+00:00
- **Authors**: Kevin Karsch, Qing He, Ye Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image segmentation has become an essential technique in clinical and research-oriented applications. Because manual segmentation methods are tedious, and fully automatic segmentation lacks the flexibility of human intervention or correction, semi-automatic methods have become the preferred type of medical image segmentation. We present a hybrid, semi-automatic segmentation method in 3D that integrates both region-based and boundary-based procedures. Our method differs from previous hybrid methods in that we perform region-based and boundary-based approaches separately, which allows for more efficient segmentation. A region-based technique is used to generate an initial seed contour that roughly represents the boundary of a target brain structure, alleviating the local minima problem in the subsequent model deformation phase. The contour is deformed under a unique force equation independent of image edges. Experiments on MRI data show that this method can achieve high accuracy and efficiency primarily due to the unique seed initialization technique.



### 3G structure for image caption generation
- **Arxiv ID**: http://arxiv.org/abs/1904.09544v1
- **DOI**: 10.1016/j.neucom.2018.10.059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09544v1)
- **Published**: 2019-04-21 05:43:01+00:00
- **Updated**: 2019-04-21 05:43:01+00:00
- **Authors**: Aihong Yuan, Xuelong Li, Xiaoqiang Lu
- **Comment**: 35 pages, 7 figures, magazine
- **Journal**: Neurocomputing 330: 17-28 (2019)
- **Summary**: It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. 1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. 2) The gated recurrent neural network (RNN) is used as the sentence generator. 3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.



### DeepCaps: Going Deeper with Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09546v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09546v1)
- **Published**: 2019-04-21 06:31:30+00:00
- **Updated**: 2019-04-21 06:31:30+00:00
- **Authors**: Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Suranga Seneviratne, Ranga Rodrigo
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule Network is a promising concept in deep learning, yet its true potential is not fully realized thus far, providing sub-par performance on several key benchmark datasets with complex data. Drawing intuition from the success achieved by Convolutional Neural Networks (CNNs) by going deeper, we introduce DeepCaps1, a deep capsule network architecture which uses a novel 3D convolution based dynamic routing algorithm. With DeepCaps, we surpass the state-of-the-art results in the capsule network domain on CIFAR10, SVHN and Fashion MNIST, while achieving a 68% reduction in the number of parameters. Further, we propose a class-independent decoder network, which strengthens the use of reconstruction loss as a regularization term. This leads to an interesting property of the decoder, which allows us to identify and control the physical attributes of the images represented by the instantiation parameters.



### Complete Scene Reconstruction by Merging Images and Laser Scans
- **Arxiv ID**: http://arxiv.org/abs/1904.09568v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09568v4)
- **Published**: 2019-04-21 08:56:07+00:00
- **Updated**: 2019-09-18 08:29:27+00:00
- **Authors**: Xiang Gao, Shuhan Shen, Lingjie Zhu, Tianxin Shi, Zhiheng Wang, Zhanyi Hu
- **Comment**: This manuscript has been accepted by IEEE TCSVT
- **Journal**: None
- **Summary**: Image based modeling and laser scanning are two commonly used approaches in large-scale architectural scene reconstruction nowadays. In order to generate a complete scene reconstruction, an effective way is to completely cover the scene using ground and aerial images, supplemented by laser scanning on certain regions with low texture and complicated structure. Thus, the key issue is to accurately calibrate cameras and register laser scans in a unified framework. To this end, we proposed a three-step pipeline for complete scene reconstruction by merging images and laser scans. First, images are captured around the architecture in a multi-view and multi-scale way and are feed into a structure-from-motion (SfM) pipeline to generate SfM points. Then, based on the SfM result, the laser scanning locations are automatically planned by considering textural richness, structural complexity of the scene and spatial layout of the laser scans. Finally, the images and laser scans are accurately merged in a coarse-to-fine manner. Experimental evaluations on two ancient Chinese architecture datasets demonstrate the effectiveness of our proposed complete scene reconstruction pipeline.



### A Simple Pooling-Based Design for Real-Time Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09569v1)
- **Published**: 2019-04-21 09:22:25+00:00
- **Updated**: 2019-04-21 09:22:25+00:00
- **Authors**: Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, Jianmin Jiang
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: We solve the problem of salient object detection by investigating how to expand the role of pooling in convolutional neural networks. Based on the U-shape architecture, we first build a global guidance module (GGM) upon the bottom-up pathway, aiming at providing layers at different feature levels the location information of potential salient objects. We further design a feature aggregation module (FAM) to make the coarse-level semantic information well fused with the fine-level features from the top-down pathway. By adding FAMs after the fusion operations in the top-down pathway, coarse-level features from the GGM can be seamlessly merged with features at various scales. These two pooling-based modules allow the high-level semantic features to be progressively refined, yielding detail enriched saliency maps. Experiment results show that our proposed approach can more accurately locate the salient objects with sharpened details and hence substantially improve the performance compared to the previous state-of-the-arts. Our approach is fast as well and can run at a speed of more than 30 FPS when processing a $300 \times 400$ image. Code can be found at http://mmcheng.net/poolnet/.



### TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.09571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09571v1)
- **Published**: 2019-04-21 09:42:10+00:00
- **Updated**: 2019-04-21 09:42:10+00:00
- **Authors**: Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy
- **Comment**: Accepted to CVPR 2019. Project page:
  https://wywu.github.io/projects/TGaGa/TGaGa.html
- **Journal**: None
- **Summary**: Unsupervised image-to-image translation aims at learning a mapping between two visual domains. However, learning a translation across large geometry variations always ends up with failure. In this work, we present a novel disentangle-and-translate framework to tackle the complex objects image-to-image translation task. Instead of learning the mapping on the image space directly, we disentangle image space into a Cartesian product of the appearance and the geometry latent spaces. Specifically, we first introduce a geometry prior loss and a conditional VAE loss to encourage the network to learn independent but complementary representations. The translation is then built on appearance and geometry space separately. Extensive experiments demonstrate the superior performance of our method to other state-of-the-art approaches, especially in the challenging near-rigid and non-rigid objects translation tasks. In addition, by taking different exemplars as the appearance references, our method also supports multimodal translation. Project page: https://wywu.github.io/projects/TGaGa/TGaGa.html



### MiniMax Entropy Network: Learning Category-Invariant Features for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1904.09601v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09601v3)
- **Published**: 2019-04-21 13:39:29+00:00
- **Updated**: 2022-10-17 02:02:22+00:00
- **Authors**: Chaofan Tao, Fengmao Lv, Lixin Duan, Min Wu
- **Comment**: 8 pages, 6 figures
- **Journal**: None
- **Summary**: How to effectively learn from unlabeled data from the target domain is crucial for domain adaptation, as it helps reduce the large performance gap due to domain shift or distribution change. In this paper, we propose an easy-to-implement method dubbed MiniMax Entropy Networks (MMEN) based on adversarial learning. Unlike most existing approaches which employ a generator to deal with domain difference, MMEN focuses on learning the categorical information from unlabeled target samples with the help of labeled source samples. Specifically, we set an unfair multi-class classifier named categorical discriminator, which classifies source samples accurately but be confused about the categories of target samples. The generator learns a common subspace that aligns the unlabeled samples based on the target pseudo-labels. For MMEN, we also provide theoretical explanations to show that the learning of feature alignment reduces domain mismatch at the category level. Experimental results on various benchmark datasets demonstrate the effectiveness of our method over existing state-of-the-art baselines.



### TiK-means: $K$-means clustering for skewed groups
- **Arxiv ID**: http://arxiv.org/abs/1904.09609v1
- **DOI**: 10.1002/sam11416
- **Categories**: **stat.ML**, astro-ph.HE, cs.CV, cs.LG, stat.AP, stat.ME
- **Links**: [PDF](http://arxiv.org/pdf/1904.09609v1)
- **Published**: 2019-04-21 14:32:42+00:00
- **Updated**: 2019-04-21 14:32:42+00:00
- **Authors**: Nicholas S. Berry, Ranjan Maitra
- **Comment**: 15 pages, 6 figures, to appear in Statistical Analysis and Data
  Mining - The ASA Data Science Journal
- **Journal**: Statistical Analysis and Data Mining -- The ASA Data Science
  Journal, 2019, volume 12, number 3, pages 223-233
- **Summary**: The $K$-means algorithm is extended to allow for partitioning of skewed groups. Our algorithm is called TiK-Means and contributes a $K$-means type algorithm that assigns observations to groups while estimating their skewness-transformation parameters. The resulting groups and transformation reveal general-structured clusters that can be explained by inverting the estimated transformation. Further, a modification of the jump statistic chooses the number of groups. Our algorithm is evaluated on simulated and real-life datasets and then applied to a long-standing astronomical dispute regarding the distinct kinds of gamma ray bursts.



### Deep Metric Learning Beyond Binary Supervision
- **Arxiv ID**: http://arxiv.org/abs/1904.09626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09626v1)
- **Published**: 2019-04-21 17:02:56+00:00
- **Updated**: 2019-04-21 17:02:56+00:00
- **Authors**: Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, Suha Kwak
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Metric Learning for visual similarity has mostly adopted binary supervision indicating whether a pair of images are of the same class or not. Such a binary indicator covers only a limited subset of image relations, and is not sufficient to represent semantic similarity between images described by continuous and/or structured labels such as object poses, image captions, and scene graphs. Motivated by this, we present a novel method for deep metric learning using continuous labels. First, we propose a new triplet loss that allows distance ratios in the label space to be preserved in the learned metric space. The proposed loss thus enables our model to learn the degree of similarity rather than just the order. Furthermore, we design a triplet mining strategy adapted to metric learning with continuous labels. We address three different image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions, and demonstrate the superior performance of our approach compared to previous methods.



### Beyond Explainability: Leveraging Interpretability for Improved Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.09633v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.09633v1)
- **Published**: 2019-04-21 17:32:03+00:00
- **Updated**: 2019-04-21 17:32:03+00:00
- **Authors**: Devinder Kumar, Ibrahim Ben-Daya, Kanav Vats, Jeffery Feng, Graham Taylor and, Alexander Wong
- **Comment**: CVPR 2019 XAI Workshop accepted
- **Journal**: None
- **Summary**: In this study, we propose the leveraging of interpretability for tasks beyond purely the purpose of explainability. In particular, this study puts forward a novel strategy for leveraging gradient-based interpretability in the realm of adversarial examples, where we use insights gained to aid adversarial learning. More specifically, we introduce the concept of spatially constrained one-pixel adversarial perturbations, where we guide the learning of such adversarial perturbations towards more susceptible areas identified via gradient-based interpretability. Experimental results using different benchmark datasets show that such a spatially constrained one-pixel adversarial perturbation strategy can noticeably improve the speed of convergence as well as produce successful attacks that were also visually difficult to perceive, thus illustrating an effective use of interpretability methods for tasks outside of the purpose of purely explainability.



### Mesh Learning Using Persistent Homology on the Laplacian Eigenfunctions
- **Arxiv ID**: http://arxiv.org/abs/1904.09639v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09639v2)
- **Published**: 2019-04-21 18:18:40+00:00
- **Updated**: 2019-04-23 22:12:39+00:00
- **Authors**: Yunhao Zhang, Haowen Liu, Paul Rosen, Mustafa Hajij
- **Comment**: None
- **Journal**: None
- **Summary**: We use persistent homology along with the eigenfunctions of the Laplacian to study similarity amongst triangulated 2-manifolds. Our method relies on studying the lower-star filtration induced by the eigenfunctions of the Laplacian. This gives us a shape descriptor that inherits the rich information encoded in the eigenfunctions of the Laplacian. Moreover, the similarity between these descriptors can be easily computed using tools that are readily available in Topological Data Analysis. We provide experiments to illustrate the effectiveness of the proposed method.



### Probabilistic Face Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1904.09658v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09658v4)
- **Published**: 2019-04-21 21:08:00+00:00
- **Updated**: 2019-08-07 05:24:06+00:00
- **Authors**: Yichun Shi, Anil K. Jain
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.



### An image structure model for exact edge detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09659v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, 68U10, I.4.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1904.09659v1)
- **Published**: 2019-04-21 21:09:47+00:00
- **Updated**: 2019-04-21 21:09:47+00:00
- **Authors**: Alessandro Dal Palu'
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a new model for single channel images low-level interpretation. The image is decomposed into a graph which captures a complete set of structural features. The description allows to accurately identify every edge location and its correct connectivity. The key features of the method are: vector description of the edges, subpixel precision, and parallelism of the underlying algorithm. The methodology outperforms classical and state of the art edge detectors at both conceptual and experimental levels. It also enables graph based algorithms for higher-level feature extraction. Any image processing pipeline can benefit from such results: e.g., controlled denoising, edge preserving filtering, upsampling, compression, vector and graph based pattern matching, neural network training.



### Deep Hough Voting for 3D Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.09664v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09664v2)
- **Published**: 2019-04-21 21:36:36+00:00
- **Updated**: 2019-08-22 20:54:40+00:00
- **Authors**: Charles R. Qi, Or Litany, Kaiming He, Leonidas J. Guibas
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.



### State Classification of Cooking Objects Using a VGG CNN
- **Arxiv ID**: http://arxiv.org/abs/1904.12613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12613v1)
- **Published**: 2019-04-21 23:32:53+00:00
- **Updated**: 2019-04-21 23:32:53+00:00
- **Authors**: Kyle Mott
- **Comment**: 5 Pages, 4 Figures
- **Journal**: None
- **Summary**: In machine learning, it is very important for a robot to know the state of an object and recognize particular desired states. This is an image classification problem that can be solved using a convolutional neural network. In this paper, we will discuss the use of a VGG convolutional neural network to recognize those states of cooking objects. We will discuss the uses of activation functions, optimizers, data augmentation, layer additions, and other different versions of architectures. The results of this paper will be used to identify alternatives to the VGG convolutional neural network to improve accuracy.



