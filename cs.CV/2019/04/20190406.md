# Arxiv Papers in cs.CV on 2019-04-06
### 360 Panorama Synthesis from a Sparse Set of Images with Unknown Field of View
- **Arxiv ID**: http://arxiv.org/abs/1904.03326v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03326v4)
- **Published**: 2019-04-06 01:00:58+00:00
- **Updated**: 2019-12-22 13:07:36+00:00
- **Authors**: Julius Surya Sumantri, In Kyu Park
- **Comment**: Presented in WACV 2020
- **Journal**: None
- **Summary**: 360 images represent scenes captured in all possible viewing directions and enable viewers to navigate freely around the scene thereby providing an immersive experience. Conversely, conventional images represent scenes in a single viewing direction with a small or limited field of view (FOV). As a result, only certain parts of the scenes are observed, and valuable information about the surroundings is lost. In this paper, a learning-based approach that reconstructs the scene in 360 x 180 from a sparse set of conventional images (typically 4 images) is proposed. The proposed approach first estimates the FOV of input images relative to the panorama. The estimated FOV is then used as the prior for synthesizing a high-resolution 360 panoramic output. The proposed method overcomes the difficulty of learning-based approach in synthesizing high resolution images (up to 512$\times$1024). Experimental results demonstrate that the proposed method produces 360 panorama with reasonable quality. Results also show that the proposed method outperforms the alternative method and can be generalized for non-panoramic scenes and images captured by a smartphone camera.



### Artificial Intelligence for Pediatric Ophthalmology
- **Arxiv ID**: http://arxiv.org/abs/1904.08796v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08796v1)
- **Published**: 2019-04-06 01:47:47+00:00
- **Updated**: 2019-04-06 01:47:47+00:00
- **Authors**: Julia E. Reid, Eric Eaton
- **Comment**: None
- **Journal**: None
- **Summary**: PURPOSE OF REVIEW: Despite the impressive results of recent artificial intelligence (AI) applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques. This article discusses the unique needs of pediatric ophthalmology patients and how AI techniques can address these challenges, surveys recent applications of AI to pediatric ophthalmology, and discusses future directions in the field.   RECENT FINDINGS: The most significant advances involve the automated detection of retinopathy of prematurity (ROP), yielding results that rival experts. Machine learning (ML) has also been successfully applied to the classification of pediatric cataracts, prediction of post-operative complications following cataract surgery, detection of strabismus and refractive error, prediction of future high myopia, and diagnosis of reading disability via eye tracking. In addition, ML techniques have been used for the study of visual development, vessel segmentation in pediatric fundus images, and ophthalmic image synthesis.   SUMMARY: AI applications could significantly benefit clinical care for pediatric ophthalmology patients by optimizing disease detection and grading, broadening access to care, furthering scientific discovery, and improving clinical efficiency. These methods need to match or surpass physician performance in clinical trials before deployment with patients. Due to widespread use of closed-access data sets and software implementations, it is difficult to directly compare the performance of these approaches, and reproducibility is poor. Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence, deep learning



### Semantic Graph Convolutional Networks for 3D Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1904.03345v3
- **DOI**: 10.1109/CVPR.2019.00354
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03345v3)
- **Published**: 2019-04-06 02:52:02+00:00
- **Updated**: 2020-03-08 21:56:21+00:00
- **Authors**: Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris N. Metaxas
- **Comment**: In CVPR 2019 (13 pages including supplementary material). The code
  can be found at https://github.com/garyzhao/SemGCN
- **Journal**: None
- **Summary**: In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D human pose regression. Our formulation is intuitive and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.



### Progressive Pose Attention Transfer for Person Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.03349v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03349v3)
- **Published**: 2019-04-06 03:10:40+00:00
- **Updated**: 2019-05-13 06:45:54+00:00
- **Authors**: Zhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, Xiang Bai
- **Comment**: To appear in CVPR 2019, oral presentation (21 pages, 15 figures
  including the supplementary materials)
- **Journal**: None
- **Summary**: This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency. Codes and models are available at: https://github.com/tengteng95/Pose-Transfer.git.



### 3D Dilated Multi-Fiber Network for Real-time Brain Tumor Segmentation in MRI
- **Arxiv ID**: http://arxiv.org/abs/1904.03355v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03355v5)
- **Published**: 2019-04-06 03:55:42+00:00
- **Updated**: 2019-09-21 02:19:07+00:00
- **Authors**: Chen Chen, Xiaopeng Liu, Meng Ding, Junfeng Zheng, Jiangyun Li
- **Comment**: None
- **Journal**: None
- **Summary**: Brain tumor segmentation plays a pivotal role in medical image processing. In this work, we aim to segment brain MRI volumes. 3D convolution neural networks (CNN) such as 3D U-Net and V-Net employing 3D convolutions to capture the correlation between adjacent slices have achieved impressive segmentation results. However, these 3D CNN architectures come with high computational overheads due to multiple layers of 3D convolutions, which may make these models prohibitive for practical large-scale applications. To this end, we propose a highly efficient 3D CNN to achieve real-time dense volumetric segmentation. The network leverages the 3D multi-fiber unit which consists of an ensemble of lightweight 3D convolutional networks to significantly reduce the computational cost. Moreover, 3D dilated convolutions are used to build multi-scale feature representations. Extensive experimental results on the BraTS-2018 challenge dataset show that the proposed architecture greatly reduces computation cost while maintaining high accuracy for brain tumor segmentation. The source code can be found at https://github.com/China-LiuXiaopeng/BraTS-DMFNet



### BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.03358v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03358v1)
- **Published**: 2019-04-06 04:21:23+00:00
- **Updated**: 2019-04-06 04:21:23+00:00
- **Authors**: Wanhua Li, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, Qi Tian
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Age estimation is an important yet very challenging problem in computer vision. Existing methods for age estimation usually apply a divide-and-conquer strategy to deal with heterogeneous data caused by the non-stationary aging process. However, the facial aging process is also a continuous process, and the continuity relationship between different components has not been effectively exploited. In this paper, we propose BridgeNet for age estimation, which aims to mine the continuous relation between age labels effectively. The proposed BridgeNet consists of local regressors and gating networks. Local regressors partition the data space into multiple overlapping subspaces to tackle heterogeneous data and gating networks learn continuity aware weights for the results of local regressors by employing the proposed bridge-tree structure, which introduces bridge connections into tree models to enforce the similarity between neighbor nodes. Moreover, these two components of BridgeNet can be jointly learned in an end-to-end way. We show experimental results on the MORPH II, FG-NET and Chalearn LAP 2015 datasets and find that BridgeNet outperforms the state-of-the-art methods.



### Towards Locally Consistent Object Counting with Constrained Multi-stage Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03373v1)
- **Published**: 2019-04-06 06:21:07+00:00
- **Updated**: 2019-04-06 06:21:07+00:00
- **Authors**: Muming Zhao, Jian Zhang, Chongyang Zhang, Wenjun Zhang
- **Comment**: Accepted by ACCV2018
- **Journal**: None
- **Summary**: High-density object counting in surveillance scenes is challenging mainly due to the drastic variation of object scales. The prevalence of deep learning has largely boosted the object counting accuracy on several benchmark datasets. However, does the global counts really count? Armed with this question we dive into the predicted density map whose summation over the whole regions reports the global counts for more in-depth analysis. We observe that the object density map generated by most existing methods usually lacks of local consistency, i.e., counting errors in local regions exist unexpectedly even though the global count seems to well match with the ground-truth. Towards this problem, in this paper we propose a constrained multi-stage Convolutional Neural Networks (CNNs) to jointly pursue locally consistent density map from two aspects. Different from most existing methods that mainly rely on the multi-column architectures of plain CNNs, we exploit a stacking formulation of plain CNNs. Benefited from the internal multi-stage learning process, the feature map could be repeatedly refined, allowing the density map to approach the ground-truth density distribution. For further refinement of the density map, we also propose a grid loss function. With finer local-region-based supervisions, the underlying model is constrained to generate locally consistent density values to minimize the training errors considering both the global and local counts accuracy. Experiments on two widely-tested object counting benchmarks with overall significant results compared with state-of-the-art methods demonstrate the effectiveness of our approach.



### Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling
- **Arxiv ID**: http://arxiv.org/abs/1904.03375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03375v1)
- **Published**: 2019-04-06 06:25:41+00:00
- **Updated**: 2019-04-06 06:25:41+00:00
- **Authors**: Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, Qi Tian
- **Comment**: CVPR'2019
- **Journal**: None
- **Summary**: Geometric deep learning is increasingly important thanks to the popularity of 3D sensors. Inspired by the recent advances in NLP domain, the self-attention transformer is introduced to consume the point clouds. We develop Point Attention Transformers (PATs), using a parameter-efficient Group Shuffle Attention (GSA) to replace the costly Multi-Head Attention. We demonstrate its ability to process size-varying inputs, and prove its permutation equivariance. Besides, prior work uses heuristics dependence on the input data (e.g., Furthest Point Sampling) to hierarchically select subsets of input points. Thereby, we for the first time propose an end-to-end learnable and task-agnostic sampling operation, named Gumbel Subset Sampling (GSS), to select a representative subset of input points. Equipped with Gumbel-Softmax, it produces a "soft" continuous subset in training phase, and a "hard" discrete subset in test phase. By selecting representative subsets in a hierarchical fashion, the networks learn a stronger representation of the input sets with lower computation cost. Experiments on classification and segmentation benchmarks show the effectiveness and efficiency of our methods. Furthermore, we propose a novel application, to process event camera stream as point clouds, and achieve a state-of-the-art performance on DVS128 Gesture Dataset.



### Blind Super-Resolution With Iterative Kernel Correction
- **Arxiv ID**: http://arxiv.org/abs/1904.03377v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03377v2)
- **Published**: 2019-04-06 06:51:23+00:00
- **Updated**: 2019-05-29 09:08:31+00:00
- **Authors**: Jinjin Gu, Hannan Lu, Wangmeng Zuo, Chao Dong
- **Comment**: Accepted by CVPR 2019. The Table 1 in the CVF camera ready version is
  corrected in this pre-print version
- **Journal**: None
- **Summary**: Deep learning based methods have dominated super-resolution (SR) field due to their remarkable performance in terms of effectiveness and efficiency. Most of these methods assume that the blur kernel during downsampling is predefined/known (e.g., bicubic). However, the blur kernels involved in real applications are complicated and unknown, resulting in severe performance drop for the advanced SR methods. In this paper, we propose an Iterative Kernel Correction (IKC) method for blur kernel estimation in blind SR problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular artifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme -- IKC that achieves better results than direct kernel estimation. We further propose an effective SR network architecture using spatial feature transform (SFT) layers to handle multiple blur kernels, named SFTMD. Extensive experiments on synthetic and real-world images show that the proposed IKC method with SFTMD can provide visually favorable SR results and the state-of-the-art performance in blind SR problem.



### Camera Lens Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.03378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03378v1)
- **Published**: 2019-04-06 07:04:07+00:00
- **Updated**: 2019-04-06 07:04:07+00:00
- **Authors**: Chang Chen, Zhiwei Xiong, Xinmei Tian, Zheng-Jun Zha, Feng Wu
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Existing methods for single image super-resolution (SR) are typically evaluated with synthetic degradation models such as bicubic or Gaussian downsampling. In this paper, we investigate SR from the perspective of camera lenses, named as CameraSR, which aims to alleviate the intrinsic tradeoff between resolution (R) and field-of-view (V) in realistic imaging systems. Specifically, we view the R-V degradation as a latent model in the SR process and learn to reverse it with realistic low- and high-resolution image pairs. To obtain the paired images, we propose two novel data acquisition strategies for two representative imaging systems (i.e., DSLR and smartphone cameras), respectively. Based on the obtained City100 dataset, we quantitatively analyze the performance of commonly-used synthetic degradation models, and demonstrate the superiority of CameraSR as a practical solution to boost the performance of existing SR methods. Moreover, CameraSR can be readily generalized to different content and devices, which serves as an advanced digital zoom tool in realistic imaging systems. Codes and datasets are available at https://github.com/ngchc/CameraSR.



### Unsupervised Person Image Generation with Semantic Parsing Transformation
- **Arxiv ID**: http://arxiv.org/abs/1904.03379v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03379v2)
- **Published**: 2019-04-06 07:19:03+00:00
- **Updated**: 2019-04-18 13:12:53+00:00
- **Authors**: Sijie Song, Wei Zhang, Jiaying Liu, Tao Mei
- **Comment**: Accepted to CVPR 2019 (Oral). Our project is available at
  https://github.com/SijieSong/person_generation_spt
- **Journal**: None
- **Summary**: In this paper, we address unsupervised pose-guided person image generation, which is known challenging due to non-rigid deformation. Unlike previous methods learning a rock-hard direct mapping between human bodies, we propose a new pathway to decompose the hard mapping into two more accessible subtasks, namely, semantic parsing transformation and appearance generation. Firstly, a semantic generative network is proposed to transform between semantic parsing maps, in order to simplify the non-rigid deformation learning. Secondly, an appearance generative network learns to synthesize semantic-aware textures. Thirdly, we demonstrate that training our framework in an end-to-end manner further refines the semantic maps and final results accordingly. Our method is generalizable to other semantic-aware person image generation tasks, eg, clothing texture transfer and controlled image manipulation. Experimental results demonstrate the superiority of our method on DeepFashion and Market-1501 datasets, especially in keeping the clothing attributes and better body shapes.



### Visualization of Convolutional Neural Networks for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.03380v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03380v1)
- **Published**: 2019-04-06 07:29:50+00:00
- **Updated**: 2019-04-06 07:29:50+00:00
- **Authors**: Junjie Hu, Yan Zhang, Takayuki Okatani
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and outdoor scene datasets. The results provide several findings that help exploration of the above question.



### Re-Identification Supervised Texture Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.03385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03385v1)
- **Published**: 2019-04-06 08:10:33+00:00
- **Updated**: 2019-04-06 08:10:33+00:00
- **Authors**: Jian Wang, Yunshan Zhong, Yachun Li, Chi Zhang, Yichen Wei
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: The estimation of 3D human body pose and shape from a single image has been extensively studied in recent years. However, the texture generation problem has not been fully discussed. In this paper, we propose an end-to-end learning strategy to generate textures of human bodies under the supervision of person re-identification. We render the synthetic images with textures extracted from the inputs and maximize the similarity between the rendered and input images by using the re-identification network as the perceptual metrics. Experiment results on pedestrian images show that our model can generate the texture from a single image and demonstrate that our textures are of higher quality than those generated by other available methods. Furthermore, we extend the application scope to other categories and explore the possible utilization of our generated textures.



### KNN and ANN-based Recognition of Handwritten Pashto Letters using Zoning Features
- **Arxiv ID**: http://arxiv.org/abs/1904.03391v2
- **DOI**: 10.14569/IJACSA.2018.091070
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03391v2)
- **Published**: 2019-04-06 09:10:55+00:00
- **Updated**: 2019-06-08 15:49:26+00:00
- **Authors**: Sulaiman Khan, Hazrat Ali, Zahid Ullah, Nasru Minallah, Shahid Maqsood, Abdul Hafeez
- **Comment**: (IJACSA) International Journal of Advanced Computer Science and
  Applications,
- **Journal**: International Journal of Advanced Computer Science and
  Applications (IJACSA), 9(10), June 2018
- **Summary**: This paper presents a recognition system for handwritten Pashto letters. However, handwritten character recognition is a challenging task. These letters not only differ in shape and style but also vary among individuals. The recognition becomes further daunting due to the lack of standard datasets for inscribed Pashto letters. In this work, we have designed a database of moderate size, which encompasses a total of 4488 images, stemming from 102 distinguishing samples for each of the 44 letters in Pashto. The recognition framework uses zoning feature extractor followed by K-Nearest Neighbour (KNN) and Neural Network (NN) classifiers for classifying individual letter. Based on the evaluation of the proposed system, an overall classification accuracy of approximately 70.05% is achieved by using KNN while 72% is achieved by using NN.



### Effective and Efficient Dropout for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03392v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03392v5)
- **Published**: 2019-04-06 09:17:51+00:00
- **Updated**: 2020-07-28 17:30:11+00:00
- **Authors**: Shaofeng Cai, Yao Shu, Gang Chen, Beng Chin Ooi, Wei Wang, Meihui Zhang
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Convolutional Neural networks (CNNs) based applications have become ubiquitous, where proper regularization is greatly needed. To prevent large neural network models from overfitting, dropout has been widely used as an efficient regularization technique in practice. However, many recent works show that the standard dropout is ineffective or even detrimental to the training of CNNs. In this paper, we revisit this issue and examine various dropout variants in an attempt to improve existing dropout-based regularization techniques for CNNs. We attribute the failure of standard dropout to the conflict between the stochasticity of dropout and its following Batch Normalization (BN), and propose to reduce the conflict by placing dropout operations right before the convolutional operation instead of BN, or totally address this issue by replacing BN with Group Normalization (GN). We further introduce a structurally more suited dropout variant Drop-Conv2d, which provides more efficient and effective regularization for deep CNNs. These dropout variants can be readily integrated into the building blocks of CNNs and implemented in existing deep learning platforms. Extensive experiments on benchmark datasets including CIFAR, SVHN and ImageNet are conducted to compare the existing building blocks and the proposed ones with dropout training. Results show that our building blocks improve over state-of-the-art CNNs significantly, which is mainly due to the better regularization and implicit model ensemble effect.



### Deep Surface Normal Estimation with Hierarchical RGB-D Fusion
- **Arxiv ID**: http://arxiv.org/abs/1904.03405v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03405v2)
- **Published**: 2019-04-06 09:54:38+00:00
- **Updated**: 2019-11-20 07:12:19+00:00
- **Authors**: Jin Zeng, Yanfeng Tong, Yunmu Huang, Qiong Yan, Wenxiu Sun, Jing Chen, Yongtian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The growing availability of commodity RGB-D cameras has boosted the applications in the field of scene understanding. However, as a fundamental scene understanding task, surface normal estimation from RGB-D data lacks thorough investigation. In this paper, a hierarchical fusion network with adaptive feature re-weighting is proposed for surface normal estimation from a single RGB-D image. Specifically, the features from color image and depth are successively integrated at multiple scales to ensure global surface smoothness while preserving visually salient details. Meanwhile, the depth features are re-weighted with a confidence map estimated from depth before merging into the color branch to avoid artifacts caused by input depth corruption. Additionally, a hybrid multi-scale loss function is designed to learn accurate normal estimation given noisy ground-truth dataset. Extensive experimental results validate the effectiveness of the fusion strategy and the loss design, outperforming state-of-the-art normal estimation schemes.



### Context-aware Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.03419v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03419v3)
- **Published**: 2019-04-06 11:42:32+00:00
- **Updated**: 2020-03-23 21:19:50+00:00
- **Authors**: Enric Corona, Albert Pumarola, Guillem Alenyà, Francesc Moreno-Noguer
- **Comment**: Accepted at CVPR20
- **Journal**: None
- **Summary**: The problem of predicting human motion given a sequence of past observations is at the core of many applications in robotics and computer vision. Current state-of-the-art formulate this problem as a sequence-to-sequence task, in which a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that predicts future movements, typically in the order of 1 to 2 seconds. However, one aspect that has been obviated so far, is the fact that human motion is inherently driven by interactions with objects and/or other humans in the environment. In this paper, we explore this scenario using a novel context-aware motion prediction architecture. We use a semantic-graph model where the nodes parameterize the human and objects in the scene and the edges their mutual interactions. These interactions are iteratively learned through a graph attention layer, fed with the past observations, which now include both object and human body motions. Once this semantic graph is learned, we inject it to a standard RNN to predict future movements of the human/s and object/s. We consider two variants of our architecture, either freezing the contextual interactions in the future of updating them. A thorough evaluation in the "Whole-Body Human Motion Database" shows that in both cases, our context-aware networks clearly outperform baselines in which the context information is not considered.



### A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.03425v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03425v2)
- **Published**: 2019-04-06 12:12:23+00:00
- **Updated**: 2019-08-11 14:24:30+00:00
- **Authors**: Lei Qi, Lei Wang, Jing Huo, Luping Zhou, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of labelling information in target domain. They are addressed in this paper from the perspective of representation learning. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic of person Re-ID, and develop camera-aware domain adaptation to reduce the discrepancy not only between source and target domains but also across these sub-domains. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved feature representation in training process. Together, the above two methods give rise to a novel unsupervised deep domain adaptation framework for person Re-ID. Experiments and ablation studies on benchmark datasets demonstrate its superiority and interesting properties.



### Unsupervised Embedding Learning via Invariant and Spreading Instance Feature
- **Arxiv ID**: http://arxiv.org/abs/1904.03436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03436v1)
- **Published**: 2019-04-06 12:58:16+00:00
- **Updated**: 2019-04-06 12:58:16+00:00
- **Authors**: Mang Ye, Xu Zhang, Pong C. Yuen, Shih-Fu Chang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: This paper studies the unsupervised embedding learning problem, which requires an effective similarity measurement between samples in low-dimensional embedding space. Motivated by the positive concentrated and negative separated properties observed from category-wise supervised learning, we propose to utilize the instance-wise supervision to approximate these properties, which aims at learning data augmentation invariant and instance spread-out features. To achieve this goal, we propose a novel instance based softmax embedding method, which directly optimizes the `real' instance features on top of the softmax function. It achieves significantly faster learning speed and higher accuracy than all existing methods. The proposed method performs well for both seen and unseen testing categories with cosine similarity. It also achieves competitive performance even without pre-trained network over samples from fine-grained categories.



### Iterative Normalization: Beyond Standardization towards Efficient Whitening
- **Arxiv ID**: http://arxiv.org/abs/1904.03441v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03441v1)
- **Published**: 2019-04-06 13:10:20+00:00
- **Updated**: 2019-04-06 13:10:20+00:00
- **Authors**: Lei Huang, Yi Zhou, Fan Zhu, Li Liu, Ling Shao
- **Comment**: Accepted to CVPR 2019. The Code is available at
  https://github.com/huangleiBuaa/IterNorm
- **Journal**: None
- **Summary**: Batch Normalization (BN) is ubiquitously employed for accelerating neural network training and improving the generalization capability by performing standardization within mini-batches. Decorrelated Batch Normalization (DBN) further boosts the above effectiveness by whitening. However, DBN relies heavily on either a large batch size, or eigen-decomposition that suffers from poor efficiency on GPUs. We propose Iterative Normalization (IterNorm), which employs Newton's iterations for much more efficient whitening, while simultaneously avoiding the eigen-decomposition. Furthermore, we develop a comprehensive study to show IterNorm has better trade-off between optimization and generalization, with theoretical and experimental support. To this end, we exclusively introduce Stochastic Normalization Disturbance (SND), which measures the inherent stochastic uncertainty of samples when applied to normalization operations. With the support of SND, we provide natural explanations to several phenomena from the perspective of optimization, e.g., why group-wise whitening of DBN generally outperforms full-whitening and why the accuracy of BN degenerates with reduced batch sizes. We demonstrate the consistently improved performance of IterNorm with extensive experiments on CIFAR-10 and ImageNet over BN and DBN.



### Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.03451v2
- **DOI**: 10.1109/CVPR.2019.00228
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03451v2)
- **Published**: 2019-04-06 14:02:50+00:00
- **Updated**: 2020-08-19 08:05:50+00:00
- **Authors**: Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados, Yi-Zhe Song
- **Comment**: Oral paper in CVPR 2019
- **Journal**: 2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)
- **Summary**: In this paper, we investigate the problem of zero-shot sketch-based image retrieval (ZS-SBIR), where human sketches are used as queries to conduct retrieval of photos from unseen categories. We importantly advance prior arts by proposing a novel ZS-SBIR scenario that represents a firm step forward in its practical application. The new setting uniquely recognizes two important yet often neglected challenges of practical ZS-SBIR, (i) the large domain gap between amateur sketch and photo, and (ii) the necessity for moving towards large-scale retrieval. We first contribute to the community a novel ZS-SBIR dataset, QuickDraw-Extended, that consists of 330,000 sketches and 204,000 photos spanning across 110 categories. Highly abstract amateur human sketches are purposefully sourced to maximize the domain gap, instead of ones included in existing datasets that can often be semi-photorealistic. We then formulate a ZS-SBIR framework to jointly model sketches and photos into a common embedding space. A novel strategy to mine the mutual information among domains is specifically engineered to alleviate the domain gap. External semantic knowledge is further embedded to aid semantic transfer. We show that, rather surprisingly, retrieval performance significantly outperforms that of state-of-the-art on existing datasets that can already be achieved using a reduced version of our model. We further demonstrate the superior performance of our full model by comparing with a number of alternatives on the newly proposed dataset. The new dataset, plus all training and testing code of our model, will be publicly released to facilitate future research



### Embodied Question Answering in Photorealistic Environments with Point Cloud Perception
- **Arxiv ID**: http://arxiv.org/abs/1904.03461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03461v1)
- **Published**: 2019-04-06 14:50:11+00:00
- **Updated**: 2019-04-06 14:50:11+00:00
- **Authors**: Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: To help bridge the gap between internet vision-style problems and the goal of vision for embodied perception we instantiate a large-scale navigation task -- Embodied Question Answering [1] in photo-realistic environments (Matterport 3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB images, or their combination. Our analysis of these models reveals several key findings. We find that two seemingly naive navigation baselines, forward-only and random, are strong navigators and challenging to outperform, due to the specific choice of the evaluation setting presented by [1]. We find a novel loss-weighting scheme we call Inflection Weighting to be important when training recurrent models for navigation with behavior cloning and are able to out perform the baselines with this technique. We find that point clouds provide a richer signal than RGB images for learning obstacle avoidance, motivating the use (and continued study) of 3D deep learning models for embodied navigation.



### Deep Stacked Hierarchical Multi-patch Network for Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1904.03468v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03468v1)
- **Published**: 2019-04-06 15:15:36+00:00
- **Updated**: 2019-04-06 15:15:36+00:00
- **Authors**: Hongguang Zhang, Yuchao Dai, Hongdong Li, Piotr Koniusz
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2019
- **Journal**: None
- **Summary**: Despite deep end-to-end learning methods have shown their superiority in removing non-uniform motion blur, there still exist major challenges with the current multi-scale and scale-recurrent models: 1) Deconvolution/upsampling operations in the coarse-to-fine scheme result in expensive runtime; 2) Simply increasing the model depth with finer-scale levels cannot improve the quality of deblurring. To tackle the above problems, we present a deep hierarchical multi-patch network inspired by Spatial Pyramid Matching to deal with blurry images via a fine-to-coarse hierarchical representation. To deal with the performance saturation w.r.t. depth, we propose a stacked version of our multi-patch model. Our proposed basic multi-patch model achieves the state-of-the-art performance on the GoPro dataset while enjoying a 40x faster runtime compared to current multi-scale methods. With 30ms to process an image at 1280x720 resolution, it is the first real-time deep motion deblurring model for 720p images at 30fps. For stacked networks, significant improvements (over 1.2dB) are achieved on the GoPro dataset by increasing the network depth. Moreover, by varying the depth of the stacked model, one can adapt the performance and runtime of the same network for different application scenarios.



### Few-Shot Learning via Saliency-guided Hallucination of Samples
- **Arxiv ID**: http://arxiv.org/abs/1904.03472v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03472v1)
- **Published**: 2019-04-06 15:33:39+00:00
- **Updated**: 2019-04-06 15:33:39+00:00
- **Authors**: Hongguang Zhang, Jing Zhang, Piotr Koniusz
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition 2019
- **Journal**: None
- **Summary**: Learning new concepts from a few of samples is a standard challenge in computer vision. The main directions to improve the learning ability of few-shot training models include (i) a robust similarity learning and (ii) generating or hallucinating additional data from the limited existing samples. In this paper, we follow the latter direction and present a novel data hallucination model. Currently, most datapoint generators contain a specialized network (i.e., GAN) tasked with hallucinating new datapoints, thus requiring large numbers of annotated data for their training in the first place. In this paper, we propose a novel less-costly hallucination method for few-shot learning which utilizes saliency maps. To this end, we employ a saliency network to obtain the foregrounds and backgrounds of available image samples and feed the resulting maps into a two-stream network to hallucinate datapoints directly in the feature space from viable foreground-background combinations. To the best of our knowledge, we are the first to leverage saliency maps for such a task and we demonstrate their usefulness in hallucinating additional datapoints for few-shot learning. Our proposed network achieves the state of the art on publicly available datasets.



### SDRSAC: Semidefinite-Based Randomized Approach for Robust Point Cloud Registration without Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1904.03483v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03483v2)
- **Published**: 2019-04-06 16:07:09+00:00
- **Updated**: 2019-04-14 09:50:49+00:00
- **Authors**: Huu Le, Thanh-Toan Do, Tuan Hoang, Ngai-Man Cheung
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: This paper presents a novel randomized algorithm for robust point cloud registration without correspondences. Most existing registration approaches require a set of putative correspondences obtained by extracting invariant descriptors. However, such descriptors could become unreliable in noisy and contaminated settings. In these settings, methods that directly handle input point sets are preferable. Without correspondences, however, conventional randomized techniques require a very large number of samples in order to reach satisfactory solutions. In this paper, we propose a novel approach to address this problem. In particular, our work enables the use of randomized methods for point cloud registration without the need of putative correspondences. By considering point cloud alignment as a special instance of graph matching and employing an efficient semi-definite relaxation, we propose a novel sampling mechanism, in which the size of the sampled subsets can be larger-than-minimal. Our tight relaxation scheme enables fast rejection of the outliers in the sampled sets, resulting in high-quality hypotheses. We conduct extensive experiments to demonstrate that our approach outperforms other state-of-the-art methods. Importantly, our proposed method serves as a generic framework which can be extended to problems with known correspondences.



### When AWGN-based Denoiser Meets Real Noises
- **Arxiv ID**: http://arxiv.org/abs/1904.03485v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03485v2)
- **Published**: 2019-04-06 16:16:49+00:00
- **Updated**: 2019-11-19 03:21:47+00:00
- **Authors**: Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, Thomas Huang
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Discriminative learning-based image denoisers have achieved promising performance on synthetic noises such as Additive White Gaussian Noise (AWGN). The synthetic noises adopted in most previous work are pixel-independent, but real noises are mostly spatially/channel-correlated and spatially/channel-variant. This domain gap yields unsatisfied performance on images with real noises if the model is only trained with AWGN. In this paper, we propose a novel approach to boost the performance of a real image denoiser which is trained only with synthetic pixel-independent noise data dominated by AWGN. First, we train a deep model that consists of a noise estimator and a denoiser with mixed AWGN and Random Value Impulse Noise (RVIN). We then investigate Pixel-shuffle Down-sampling (PD) strategy to adapt the trained model to real noises. Extensive experiments demonstrate the effectiveness and generalization of the proposed approach. Notably, our method achieves state-of-the-art performance on real sRGB images in the DND benchmark among models trained with synthetic noises. Codes are available at https://github.com/yzhouas/PD-Denoising-pytorch.



### Self-supervised speaker embeddings
- **Arxiv ID**: http://arxiv.org/abs/1904.03486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03486v2)
- **Published**: 2019-04-06 16:32:12+00:00
- **Updated**: 2019-04-23 13:27:29+00:00
- **Authors**: Themos Stafylakis, Johan Rohdin, Oldrich Plchot, Petr Mizera, Lukas Burget
- **Comment**: Preprint. Submitted to Interspeech 2019. Updated results compared to
  first version and minor corrections
- **Journal**: None
- **Summary**: Contrary to i-vectors, speaker embeddings such as x-vectors are incapable of leveraging unlabelled utterances, due to the classification loss over training speakers. In this paper, we explore an alternative training strategy to enable the use of unlabelled utterances in training. We propose to train speaker embedding extractors via reconstructing the frames of a target speech segment, given the inferred embedding of another speech segment of the same utterance. We do this by attaching to the standard speaker embedding extractor a decoder network, which we feed not merely with the speaker embedding, but also with the estimated phone sequence of the target frame sequence. The reconstruction loss can be used either as a single objective, or be combined with the standard speaker classification loss. In the latter case, it acts as a regularizer, encouraging generalizability to speakers unseen during training. In all cases, the proposed architectures are trained from scratch and in an end-to-end fashion. We demonstrate the benefits from the proposed approach on VoxCeleb and Speakers in the wild, and we report notable improvements over the baseline.



### VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research
- **Arxiv ID**: http://arxiv.org/abs/1904.03493v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03493v3)
- **Published**: 2019-04-06 16:50:31+00:00
- **Updated**: 2020-06-17 16:47:55+00:00
- **Authors**: Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang
- **Comment**: ICCV 2019 Oral. 17 pages, 14 figures, 6 tables (updated the VATEX
  website link: vatex-challenge.org)
- **Journal**: None
- **Summary**: We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.



### LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03498v1)
- **Published**: 2019-04-06 17:26:15+00:00
- **Updated**: 2019-04-06 17:26:15+00:00
- **Authors**: Sudhakar Kumawat, Shanmuganathan Raman
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: Traditional 3D Convolutional Neural Networks (CNNs) are computationally expensive, memory intensive, prone to overfit, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose Rectified Local Phase Volume (ReLPV) block, an efficient alternative to the standard 3D convolutional layer. The ReLPV block extracts the phase in a 3D local neighborhood (e.g., 3x3x3) of each position of the input map to obtain the feature maps. The phase is extracted by computing 3D Short Term Fourier Transform (STFT) at multiple fixed low frequency points in the 3D local neighborhood of each position. These feature maps at different frequency points are then linearly combined after passing them through an activation function. The ReLPV block provides significant parameter savings of at least, 3^3 to 13^3 times compared to the standard 3D convolutional layer with the filter sizes 3x3x3 to 13x13x13, respectively. We show that the feature learning capabilities of the ReLPV block are significantly better than the standard 3D convolutional layer. Furthermore, it produces consistently better results across different 3D data representations. We achieve state-of-the-art accuracy on the volumetric ModelNet10 and ModelNet40 datasets while utilizing only 11% parameters of the current state-of-the-art. We also improve the state-of-the-art on the UCF-101 split-1 action recognition dataset by 5.68% (when trained from scratch) while using only 15% of the parameters of the state-of-the-art. The project webpage is available at https://sites.google.com/view/lp-3dcnn/home.



### DeepSEED: 3D Squeeze-and-Excitation Encoder-Decoder Convolutional Neural Networks for Pulmonary Nodule Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.03501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03501v2)
- **Published**: 2019-04-06 17:40:47+00:00
- **Updated**: 2020-02-06 19:33:54+00:00
- **Authors**: Yuemeng Li, Yong Fan
- **Comment**: Accepted by 2020 IEEE International Symposium on Biomedical Imaging
  (ISBI)
- **Journal**: None
- **Summary**: Pulmonary nodule detection plays an important role in lung cancer screening with low-dose computed tomography (CT) scans. It remains challenging to build nodule detection deep learning models with good generalization performance due to unbalanced positive and negative samples. In order to overcome this problem and further improve state-of-the-art nodule detection methods, we develop a novel deep 3D convolutional neural network with an Encoder-Decoder structure in conjunction with a region proposal network. Particularly, we utilize a dynamically scaled cross entropy loss to reduce the false positive rate and combat the sample imbalance problem associated with nodule detection. We adopt the squeeze-and-excitation structure to learn effective image features and utilize inter-dependency information of different feature maps. We have validated our method based on publicly available CT scans with manually labelled ground-truth obtained from LIDC/IDRI dataset and its subset LUNA16 with thinner slices. Ablation studies and experimental results have demonstrated that our method could outperform state-of-the-art nodule detection methods by a large margin.



### C2S2: Cost-aware Channel Sparse Selection for Progressive Network Pruning
- **Arxiv ID**: http://arxiv.org/abs/1904.03508v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03508v1)
- **Published**: 2019-04-06 18:40:06+00:00
- **Updated**: 2019-04-06 18:40:06+00:00
- **Authors**: Chih-Yao Chiu, Hwann-Tzong Chen, Tyng-Luh Liu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a channel-selection approach for simplifying deep neural networks. Specifically, we propose a new type of generic network layer, called pruning layer, to seamlessly augment a given pre-trained model for compression. Each pruning layer, comprising $1 \times 1$ depth-wise kernels, is represented with a dual format: one is real-valued and the other is binary. The former enables a two-phase optimization process of network pruning to operate with an end-to-end differentiable network, and the latter yields the mask information for channel selection. Our method progressively performs the pruning task layer-wise, and achieves channel selection according to a sparsity criterion to favor pruning more channels. We also develop a cost-aware mechanism to prevent the compression from sacrificing the expected network performance. Our results for compressing several benchmark deep networks on image classification and semantic segmentation are comparable to those by state-of-the-art.



### Instance-Level Meta Normalization
- **Arxiv ID**: http://arxiv.org/abs/1904.03516v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03516v1)
- **Published**: 2019-04-06 19:37:18+00:00
- **Updated**: 2019-04-06 19:37:18+00:00
- **Authors**: Songhao Jia, Ding-Jie Chen, Hwann-Tzong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a normalization mechanism called Instance-Level Meta Normalization (ILM~Norm) to address a learning-to-normalize problem. ILM~Norm learns to predict the normalization parameters via both the feature feed-forward and the gradient back-propagation paths. ILM~Norm provides a meta normalization mechanism and has several good properties. It can be easily plugged into existing instance-level normalization schemes such as Instance Normalization, Layer Normalization, or Group Normalization. ILM~Norm normalizes each instance individually and therefore maintains high performance even when small mini-batch is used. The experimental results show that ILM~Norm well adapts to different network architectures and tasks, and it consistently improves the performance of the original models. The code is available at url{https://github.com/Gasoonjia/ILM-Norm.



### Dense 3D Face Decoding over 2500FPS: Joint Texture & Shape Convolutional Mesh Decoders
- **Arxiv ID**: http://arxiv.org/abs/1904.03525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03525v1)
- **Published**: 2019-04-06 20:22:53+00:00
- **Updated**: 2019-04-06 20:22:53+00:00
- **Authors**: Yuxiang Zhou, Jiankang Deng, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: 3D Morphable Models (3DMMs) are statistical models that represent facial texture and shape variations using a set of linear bases and more particular Principal Component Analysis (PCA). 3DMMs were used as statistical priors for reconstructing 3D faces from images by solving non-linear least square optimization problems. Recently, 3DMMs were used as generative models for training non-linear mappings (\ie, regressors) from image to the parameters of the models via Deep Convolutional Neural Networks (DCNNs). Nevertheless, all of the above methods use either fully connected layers or 2D convolutions on parametric unwrapped UV spaces leading to large networks with many parameters. In this paper, we present the first, to the best of our knowledge, non-linear 3DMMs by learning joint texture and shape auto-encoders using direct mesh convolutions. We demonstrate how these auto-encoders can be used to train very light-weight models that perform Coloured Mesh Decoding (CMD) in-the-wild at a speed of over 2500 FPS.



### Automatic Target Recognition Using Discrimination Based on Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/1904.03534v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.03534v1)
- **Published**: 2019-04-06 21:43:53+00:00
- **Updated**: 2019-04-06 21:43:53+00:00
- **Authors**: Ali Sadeghian, Deoksu Lim, Johan Karlsson, Jian Li
- **Comment**: None
- **Journal**: 2015 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)
- **Summary**: The use of distances based on optimal transportation has recently shown promise for discrimination of power spectra. In particular, spectral estimation methods based on l1 regularization as well as covariance based methods can be shown to be robust with respect to such distances. These transportation distances provide a geometric framework where geodesics corresponds to smooth transition of spectral mass, and have been useful for tracking. In this paper, we investigate the use of these distances for automatic target recognition. We study the use of the Monge-Kantorovich distance compared to the standard l2 distance for classifying civilian vehicles based on SAR images. We use a version of the Monge-Kantorovich distance that applies also for the case where the spectra may have different total mass, and we formulate the optimization problem as a minimum flow problem that can be computed using efficient algorithms.



### Convex-Concave Backtracking for Inertial Bregman Proximal Gradient Algorithms in Non-Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1904.03537v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, cs.NA, 90C25, 26B25, 49M27, 52A41, 65K05
- **Links**: [PDF](http://arxiv.org/pdf/1904.03537v2)
- **Published**: 2019-04-06 21:59:02+00:00
- **Updated**: 2019-11-05 11:16:22+00:00
- **Authors**: Mahesh Chandra Mukkamala, Peter Ochs, Thomas Pock, Shoham Sabach
- **Comment**: 29 pages
- **Journal**: None
- **Summary**: Backtracking line-search is an old yet powerful strategy for finding a better step sizes to be used in proximal gradient algorithms. The main principle is to locally find a simple convex upper bound of the objective function, which in turn controls the step size that is used. In case of inertial proximal gradient algorithms, the situation becomes much more difficult and usually leads to very restrictive rules on the extrapolation parameter. In this paper, we show that the extrapolation parameter can be controlled by locally finding also a simple concave lower bound of the objective function. This gives rise to a double convex-concave backtracking procedure which allows for an adaptive choice of both the step size and extrapolation parameters. We apply this procedure to the class of inertial Bregman proximal gradient methods, and prove that any sequence generated by these algorithms converges globally to a critical point of the function at hand. Numerical experiments on a number of challenging non-convex problems in image processing and machine learning were conducted and show the power of combining inertial step and double backtracking strategy in achieving improved performances.



