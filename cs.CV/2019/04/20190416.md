# Arxiv Papers in cs.CV on 2019-04-16
### DeepWait: Pedestrian Wait Time Estimation in Mixed Traffic Conditions Using Deep Survival Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.11008v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11008v3)
- **Published**: 2019-04-16 00:04:11+00:00
- **Updated**: 2019-08-08 20:19:10+00:00
- **Authors**: Arash Kalatian, Bilal Farooq
- **Comment**: Accepted for publication in the proceedings of IEEE Intelligent
  Transportation Systems Conference - ITSC 2019
- **Journal**: None
- **Summary**: Pedestrian's road crossing behaviour is one of the important aspects of urban dynamics that will be affected by the introduction of autonomous vehicles. In this study we introduce DeepSurvival, a novel framework for estimating pedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic conditions. We exploit the strengths of deep learning in capturing the nonlinearities in the data and develop a cox proportional hazard model with a deep neural network as the log-risk function. An embedded feature selection algorithm for reducing data dimensionality and enhancing the interpretability of the network is also developed. We test our framework on a dataset collected from 160 participants using an immersive virtual reality environment. Validation results showed that with a C-index of 0.64 our proposed framework outperformed the standard cox proportional hazard-based model with a C-index of 0.58.



### Predicting Fluid Intelligence of Children using T1-weighted MR Images and a StackNet
- **Arxiv ID**: http://arxiv.org/abs/1904.07387v3
- **DOI**: 10.1007/978-3-030-31901-4_2
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07387v3)
- **Published**: 2019-04-16 01:04:20+00:00
- **Updated**: 2020-05-12 02:25:31+00:00
- **Authors**: Po-Yu Kao, Angela Zhang, Michael Goebel, Jefferson W. Chen, B. S. Manjunath
- **Comment**: 8 pages, 2 figures, 3 tables, Accepted by MICCAI ABCD-NP Challenge
  2019; Added NDA
- **Journal**: None
- **Summary**: In this work, we utilize T1-weighted MR images and StackNet to predict fluid intelligence in adolescents. Our framework includes feature extraction, feature normalization, feature denoising, feature selection, training a StackNet, and predicting fluid intelligence. The extracted feature is the distribution of different brain tissues in different brain parcellation regions. The proposed StackNet consists of three layers and 11 models. Each layer uses the predictions from all previous layers including the input layer. The proposed StackNet is tested on a public benchmark Adolescent Brain Cognitive Development Neurocognitive Prediction Challenge 2019 and achieves a mean squared error of 82.42 on the combined training and validation set with 10-fold cross-validation. In addition, the proposed StackNet also achieves a mean squared error of 94.25 on the testing data. The source code is available on GitHub.



### NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.07392v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07392v1)
- **Published**: 2019-04-16 01:32:33+00:00
- **Updated**: 2019-04-16 01:32:33+00:00
- **Authors**: Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, Quoc V. Le
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time.



### Deep learning for image segmentation: veritable or overhyped?
- **Arxiv ID**: http://arxiv.org/abs/1904.08483v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08483v3)
- **Published**: 2019-04-16 01:43:40+00:00
- **Updated**: 2020-07-23 07:33:26+00:00
- **Authors**: Zhenzhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has achieved great success as a powerful classification tool and also made great progress in sematic segmentation. As a result, many researchers also believe that deep learning is the most powerful tool for pixel level image segmentation. Could deep learning achieve the same pixel level accuracy as traditional image segmentation techniques by mapping the features of the object into a non-linear function? This paper gives a short survey of the accuracies achieved by deep learning so far in image classification and image segmentation. Compared to the high accuracies achieved by deep learning in classifying limited categories in international vision challenges, the image segmentation accuracies achieved by deep learning in the same challenges are only about eighty percent. On the contrary, the image segmentation accuracies achieved in international biomedical challenges are close to ninty five percent. Why the difference is so big? Since the accuracies of the competitors methods are only evaluated based on their submitted results instead of reproducing the results by submitting the source codes or the software, are the achieved accuracies verifiable or overhyped? We are going to find it out by analyzing the working principle of deep learning. Finally, we compared the accuracies of state of the art deep learning methods with a threshold selection method quantitatively. Experimental results showed that the threshold selection method could achieve significantly higher accuracy than deep learning methods in image segmentation.



### Combining RGB and Points to Predict Grasping Region for Robotic Bin-Picking
- **Arxiv ID**: http://arxiv.org/abs/1904.07394v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07394v2)
- **Published**: 2019-04-16 01:47:27+00:00
- **Updated**: 2019-04-24 09:06:01+00:00
- **Authors**: Quanquan Shao, Jie Hu
- **Comment**: 5 pages, 6 figures
- **Journal**: None
- **Summary**: This paper focuses on a robotic picking tasks in cluttered scenario. Because of the diversity of objects and clutter by placing, it is much difficult to recognize and estimate their pose before grasping. Here, we use U-net, a special Convolution Neural Networks (CNN), to combine RGB images and depth information to predict picking region without recognition and pose estimation. The efficiency of diverse visual input of the network were compared, including RGB, RGB-D and RGB-Points. And we found the RGB-Points input could get a precision of 95.74%.



### Real Image Denoising with Feature Attention
- **Arxiv ID**: http://arxiv.org/abs/1904.07396v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07396v2)
- **Published**: 2019-04-16 01:55:08+00:00
- **Updated**: 2020-03-23 04:57:08+00:00
- **Authors**: Saeed Anwar, Nick Barnes
- **Comment**: Accepted in ICCV (Oral), 2019
- **Journal**: None
- **Summary**: Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, their performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet.



### Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression
- **Arxiv ID**: http://arxiv.org/abs/1904.07399v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07399v3)
- **Published**: 2019-04-16 02:00:24+00:00
- **Updated**: 2020-05-19 05:30:24+00:00
- **Authors**: Xinyao Wang, Liefeng Bo, Li Fuxin
- **Comment**: [v2] Camera-ready version for ICCV 2019. [v3] Corrected AUC(fr10%) on
  table 2
- **Journal**: None
- **Summary**: Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks. Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.



### Suction Grasp Region Prediction using Self-supervised Learning for Object Picking in Dense Clutter
- **Arxiv ID**: http://arxiv.org/abs/1904.07402v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07402v2)
- **Published**: 2019-04-16 02:03:57+00:00
- **Updated**: 2019-04-24 09:01:33+00:00
- **Authors**: Quanquan Shao, Jie Hu, Weiming Wang, Yi Fang, Wenhai Liu, Jin Qi, Jin Ma
- **Comment**: 6 pages, 7 figures, conference
- **Journal**: None
- **Summary**: This paper focuses on robotic picking tasks in cluttered scenario. Because of the diversity of poses, types of stack and complicated background in bin picking situation, it is much difficult to recognize and estimate their pose before grasping them. Here, this paper combines Resnet with U-net structure, a special framework of Convolution Neural Networks (CNN), to predict picking region without recognition and pose estimation. And it makes robotic picking system learn picking skills from scratch. At the same time, we train the network end to end with online samples. In the end of this paper, several experiments are conducted to demonstrate the performance of our methods.



### What I See Is What You See: Joint Attention Learning for First and Third Person Video Co-analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.07424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07424v1)
- **Published**: 2019-04-16 03:09:50+00:00
- **Updated**: 2019-04-16 03:09:50+00:00
- **Authors**: Huangyue Yu, Minjie Cai, Yunfei Liu, Feng Lu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, more and more videos are captured from the first-person viewpoint by wearable cameras. Such first-person video provides additional information besides the traditional third-person video, and thus has a wide range of applications. However, techniques for analyzing the first-person video can be fundamentally different from those for the third-person video, and it is even more difficult to explore the shared information from both viewpoints. In this paper, we propose a novel method for first- and third-person video co-analysis. At the core of our method is the notion of "joint attention", indicating the learnable representation that corresponds to the shared attention regions in different viewpoints and thus links the two viewpoints. To this end, we develop a multi-branch deep network with a triplet loss to extract the joint attention from the first- and third-person videos via self-supervised learning. We evaluate our method on the public dataset with cross-viewpoint video matching tasks. Our method outperforms the state-of-the-art both qualitatively and quantitatively. We also demonstrate how the learned joint attention can benefit various applications through a set of additional experiments.



### Single Pixel Reconstruction for One-stage Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.07426v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07426v3)
- **Published**: 2019-04-16 03:11:13+00:00
- **Updated**: 2019-05-17 07:39:51+00:00
- **Authors**: Jun Yu, Jinghan Yao, Jian Zhang, Zhou Yu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Object instance segmentation is one of the most fundamental but challenging tasks in computer vision, and it requires the pixel-level image understanding. Most existing approaches address this problem by adding a mask prediction branch to a two-stage object detector with the Region Proposal Network (RPN). Although producing good segmentation results, the efficiency of these two-stage approaches is far from satisfactory, restricting their applicability in practice. In this paper, we propose a one-stage framework, SPRNet, which performs efficient instance segmentation by introducing a single pixel reconstruction (SPR) branch to off-the-shelf one-stage detectors. The added SPR branch reconstructs the pixel-level mask from every single pixel in the convolution feature map directly. Using the same ResNet-50 backbone, SPRNet achieves comparable mask AP to Mask R-CNN at a higher inference speed, and gains all-round improvements on box AP at every scale comparing with RetinaNet.



### Shortest Paths in HSI Space for Color Texture Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.07429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07429v1)
- **Published**: 2019-04-16 03:24:35+00:00
- **Updated**: 2019-04-16 03:24:35+00:00
- **Authors**: Mingxin Jin, Yongsheng Dong, Lintao Zheng, Lingfei Liang, Tianyu Wang, Hongyan zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Color texture representation is an important step in the task of texture classification. Shortest paths was used to extract color texture features from RGB and HSV color spaces. In this paper, we propose to use shortest paths in the HSI space to build a texture representation for classification. In particular, two undirected graphs are used to model the H channel and the S and I channels respectively in order to represent a color texture image. Moreover, the shortest paths is constructed by using four pairs of pixels according to different scales and directions of the texture image. Experimental results on colored Brodatz and USPTex databases reveal that our proposed method is effective, and the highest classification accuracy rate is 96.93% in the Brodatz database.



### Photofeeler-D3: A Neural Network with Voter Modeling for Dating Photo Impression Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.07435v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07435v3)
- **Published**: 2019-04-16 03:44:08+00:00
- **Updated**: 2019-05-10 17:38:08+00:00
- **Authors**: Agastya Kalra, Ben Peterson
- **Comment**: 10 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: In just a few years, online dating has become the dominant way that young people meet to date, making the deceptively error-prone task of picking good dating profile photos vital to a generation's ability to form romantic connections. Until now, artificial intelligence approaches to Dating Photo Impression Prediction (DPIP) have been very inaccurate, unadaptable to real-world application, and have only taken into account a subject's physical attractiveness. To that effect, we propose Photofeeler-D3 - the first convolutional neural network as accurate as 10 human votes for how smart, trustworthy, and attractive the subject appears in highly variable dating photos. Our "attractive" output is also applicable to Facial Beauty Prediction (FBP), making Photofeeler-D3 state-of-the-art for both DPIP and FBP. We achieve this by leveraging Photofeeler's Dating Dataset (PDD) with over 1 million images and tens of millions of votes, our novel technique of voter modeling, and cutting-edge computer vision techniques.



### Decoupling Localization and Classification in Single Shot Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.07442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07442v1)
- **Published**: 2019-04-16 03:50:57+00:00
- **Updated**: 2019-04-16 03:50:57+00:00
- **Authors**: Yupan Huang, Qi Dai, Yutong Lu
- **Comment**: ICME 2019. https://github.com/HYPJUDY/Decouple-SSAD
- **Journal**: None
- **Summary**: Video temporal action detection aims to temporally localize and recognize the action in untrimmed videos. Existing one-stage approaches mostly focus on unifying two subtasks, i.e., localization of action proposals and classification of each proposal through a fully shared backbone. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. In this paper, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling the localization and classification in a one-stage scheme. Particularly, two separate branches are designed in parallel to enable each component to own representations privately for accurate localization or classification. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. Each branch produces a set of feature maps by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the feature representations. We conduct extensive experiments on THUMOS14 dataset and demonstrate superior performance over state-of-the-art methods. Our code is available online.



### Counterfactual Visual Explanations
- **Arxiv ID**: http://arxiv.org/abs/1904.07451v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07451v2)
- **Published**: 2019-04-16 04:16:11+00:00
- **Updated**: 2019-06-11 16:49:55+00:00
- **Authors**: Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a 'distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.



### Point cloud registration: matching a maximal common subset on pointclouds with noise (with 2D implementation)
- **Arxiv ID**: http://arxiv.org/abs/1904.07454v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07454v1)
- **Published**: 2019-04-16 04:23:30+00:00
- **Updated**: 2019-04-16 04:23:30+00:00
- **Authors**: Jorge Arce Garro, David Jiménez López
- **Comment**: 13 pages, 5 figures
- **Journal**: None
- **Summary**: We analyze the problem of determining whether 2 given point clouds in 2D, with any distinct cardinality and any number of outliers, have subsets of the same size that can be matched via a rigid motion. This problem is important, for example, in the application of fingerprint matching with incomplete data. We propose an algorithm that, under assumptions on the noise tolerance, allows to find corresponding subclouds of the maximum possible size. Our procedure optimizes a potential energy function to do so, which was first inspired in the potential energy interaction that occurs between point charges in electrostatics.



### A Bayesian Perspective on the Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1904.07457v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07457v1)
- **Published**: 2019-04-16 04:39:29+00:00
- **Updated**: 2019-04-16 04:39:29+00:00
- **Authors**: Zezhou Cheng, Matheus Gadelha, Subhransu Maji, Daniel Sheldon
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: The deep image prior was recently introduced as a prior for natural images. It represents images as the output of a convolutional network with random inputs. For "inference", gradient descent is performed to adjust network parameters to make the output match observations. This approach yields good performance on a range of image reconstruction tasks. We show that the deep image prior is asymptotically equivalent to a stationary Gaussian process prior in the limit as the number of channels in each layer of the network goes to infinity, and derive the corresponding kernel. This informs a Bayesian approach to inference. We show that by conducting posterior inference using stochastic gradient Langevin we avoid the need for early stopping, which is a drawback of the current approach, and improve results for denoising and impainting tasks. We illustrate these intuitions on a number of 1D and 2D signal reconstruction tasks.



### Fashion-AttGAN: Attribute-Aware Fashion Editing with Multi-Objective GAN
- **Arxiv ID**: http://arxiv.org/abs/1904.07460v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07460v2)
- **Published**: 2019-04-16 04:51:59+00:00
- **Updated**: 2019-04-20 17:05:15+00:00
- **Authors**: Qing Ping, Bing Wu, Wanying Ding, Jiangbo Yuan
- **Comment**: 3 pages, 2 figures, accepted at FFSS-USAD workshop @ CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we introduce attribute-aware fashion-editing, a novel task, to the fashion domain. We re-define the overall objectives in AttGAN and propose the Fashion-AttGAN model for this new task. A dataset is constructed for this task with 14,221 and 22 attributes, which has been made publically available. Experimental results show the effectiveness of our Fashion-AttGAN on fashion editing over the original AttGAN.



### Deep Neural Network Based Hyperspectral Pixel Classification With Factorized Spectral-Spatial Feature Representation
- **Arxiv ID**: http://arxiv.org/abs/1904.07461v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.07461v1)
- **Published**: 2019-04-16 04:52:19+00:00
- **Updated**: 2019-04-16 04:52:19+00:00
- **Authors**: Jingzhou Chen, Siyu Chen, Peilin Zhou, Yuntao Qian
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Deep learning has been widely used for hyperspectral pixel classification due to its ability of generating deep feature representation. However, how to construct an efficient and powerful network suitable for hyperspectral data is still under exploration. In this paper, a novel neural network model is designed for taking full advantage of the spectral-spatial structure of hyperspectral data. Firstly, we extract pixel-based intrinsic features from rich yet redundant spectral bands by a subnetwork with supervised pre-training scheme. Secondly, in order to utilize the local spatial correlation among pixels, we share the previous subnetwork as a spectral feature extractor for each pixel in a patch of image, after which the spectral features of all pixels in a patch are combined and feeded into the subsequent classification subnetwork. Finally, the whole network is further fine-tuned to improve its classification performance. Specially, the spectral-spatial factorization scheme is applied in our model architecture, making the network size and the number of parameters great less than the existing spectral-spatial deep networks for hyperspectral image classification. Experiments on the hyperspectral data sets show that, compared with some state-of-art deep learning methods, our method achieves better classification results while having smaller network size and less parameters.



### Super Resolution Convolutional Neural Network Models for Enhancing Resolution of Rock Micro-CT Images
- **Arxiv ID**: http://arxiv.org/abs/1904.07470v1
- **DOI**: 10.1016/j.petrol.2019.106261
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07470v1)
- **Published**: 2019-04-16 05:24:49+00:00
- **Updated**: 2019-04-16 05:24:49+00:00
- **Authors**: Ying Da Wang, Ryan Armstrong, Peyman Mostaghimi
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Single Image Super Resolution (SISR) techniques based on Super Resolution Convolutional Neural Networks (SRCNN) are applied to micro-computed tomography ({\mu}CT) images of sandstone and carbonate rocks. Digital rock imaging is limited by the capability of the scanning device resulting in trade-offs between resolution and field of view, and super resolution methods tested in this study aim to compensate for these limits. SRCNN models SR-Resnet, Enhanced Deep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital Rock Super Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of 2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone and Estaillades carbonate. The trained models are applied to the validation and test data within the dataset and show a 3-5 dB rise in image quality compared to bicubic interpolation, with all tested models performing within a 0.1 dB range. Difference maps indicate that edge sharpness is completely recovered in images within the scope of the trained model, with only high frequency noise related detail loss. We find that aside from generation of high-resolution images, a beneficial side effect of super resolution methods applied to synthetically downgraded images is the removal of image noise while recovering edgewise sharpness which is beneficial for the segmentation process. The model is also tested against real low-resolution images of Bentheimer rock with image augmentation to account for natural noise and blur. The SRCNN method is shown to act as a preconditioner for image segmentation under these circumstances which naturally leads to further future development and training of models that segment an image directly. Image restoration by SRCNN on the rock images is of significantly higher quality than traditional methods and suggests SRCNN methods are a viable processing step in a digital rock workflow.



### Learning Pyramid-Context Encoder Network for High-Quality Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1904.07475v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07475v4)
- **Published**: 2019-04-16 05:51:37+00:00
- **Updated**: 2019-07-11 03:02:28+00:00
- **Authors**: Yanhong Zeng, Jianlong Fu, Hongyang Chao, Baining Guo
- **Comment**: Accepted as a CVPR 2019 poster paper; update SUPP;update Eq5;
- **Journal**: None
- **Summary**: High-quality image inpainting requires filling missing regions in a damaged image with plausible content. Existing works either fill the regions by copying image patches or generating semantically-coherent patches from region context, while neglect the fact that both visual and semantic plausibility are highly-demanded. In this paper, we propose a Pyramid-context ENcoder Network (PEN-Net) for image inpainting by deep generative models. The PEN-Net is built upon a U-Net structure, which can restore an image by encoding contextual semantics from full resolution input, and decoding the learned semantic features back into images. Specifically, we propose a pyramid-context encoder, which progressively learns region affinity by attention from a high-level semantic feature map and transfers the learned attention to the previous low-level feature map. As the missing content can be filled by attention transfer from deep to shallow in a pyramid fashion, both visual and semantic coherence for image inpainting can be ensured. We further propose a multi-scale decoder with deeply-supervised pyramid losses and an adversarial loss. Such a design not only results in fast convergence in training, but more realistic results in testing. Extensive experiments on various datasets show the superior performance of the proposed network



### GradMask: Reduce Overfitting by Regularizing Saliency
- **Arxiv ID**: http://arxiv.org/abs/1904.07478v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07478v1)
- **Published**: 2019-04-16 05:57:50+00:00
- **Updated**: 2019-04-16 05:57:50+00:00
- **Authors**: Becks Simpson, Francis Dutil, Yoshua Bengio, Joseph Paul Cohen
- **Comment**: None
- **Journal**: None
- **Summary**: With too few samples or too many model parameters, overfitting can inhibit the ability to generalise predictions to new data. Within medical imaging, this can occur when features are incorrectly assigned importance such as distinct hospital specific artifacts, leading to poor performance on a new dataset from a different institution without those features, which is undesirable. Most regularization methods do not explicitly penalize the incorrect association of these features to the target class and hence fail to address this issue. We propose a regularization method, GradMask, which penalizes saliency maps inferred from the classifier gradients when they are not consistent with the lesion segmentation. This prevents non-tumor related features to contribute to the classification of unhealthy samples. We demonstrate that this method can improve test accuracy between 1-3% compared to the baseline without GradMask, showing that it has an impact on reducing overfitting.



### Object-Oriented Dynamics Learning through Multi-Level Abstraction
- **Arxiv ID**: http://arxiv.org/abs/1904.07482v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07482v4)
- **Published**: 2019-04-16 06:01:17+00:00
- **Updated**: 2019-12-05 06:05:28+00:00
- **Authors**: Guangxiang Zhu, Jianhao Wang, Zhizhou Ren, Zichuan Lin, Chongjie Zhang
- **Comment**: Accepted to the Thirthy-Fourth AAAI Conference On Artificial
  Intelligence (AAAI), 2020
- **Journal**: None
- **Summary**: Object-based approaches for learning action-conditioned dynamics has demonstrated promise for generalization and interpretability. However, existing approaches suffer from structural limitations and optimization difficulties for common environments with multiple dynamic objects. In this paper, we present a novel self-supervised learning framework, called Multi-level Abstraction Object-oriented Predictor (MAOP), which employs a three-level learning architecture that enables efficient object-based dynamics learning from raw visual observations. We also design a spatial-temporal relational reasoning mechanism for MAOP to support instance-level dynamics learning and handle partial observability. Our results show that MAOP significantly outperforms previous methods in terms of sample efficiency and generalization over novel environments for learning environment models. We also demonstrate that learned dynamics models enable efficient planning in unseen environments, comparable to true environment models. In addition, MAOP learns semantically and visually interpretable disentangled representations.



### End-to-End Denoising of Dark Burst Images Using Recurrent Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.07483v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07483v1)
- **Published**: 2019-04-16 06:03:49+00:00
- **Updated**: 2019-04-16 06:03:49+00:00
- **Authors**: Di Zhao, Lan Ma, Songnan Li, Dahai Yu
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: When taking photos in dim-light environments, due to the small amount of light entering, the shot images are usually extremely dark, with a great deal of noise, and the color cannot reflect real-world color. Under this condition, the traditional methods used for single image denoising have always failed to be effective. One common idea is to take multiple frames of the same scene to enhance the signal-to-noise ratio. This paper proposes a recurrent fully convolutional network (RFCN) to process burst photos taken under extremely low-light conditions, and to obtain denoised images with improved brightness. Our model maps raw burst images directly to sRGB outputs, either to produce a best image or to generate a multi-frame denoised image sequence. This process has proven to be capable of accomplishing the low-level task of denoising, as well as the high-level task of color correction and enhancement, all of which is end-to-end processing through our network. Our method has achieved better results than state-of-the-art methods. In addition, we have applied the model trained by one type of camera without fine-tuning on photos captured by different cameras and have obtained similar end-to-end enhancements.



### Shared Predictive Cross-Modal Deep Quantization
- **Arxiv ID**: http://arxiv.org/abs/1904.07488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07488v1)
- **Published**: 2019-04-16 06:29:02+00:00
- **Updated**: 2019-04-16 06:29:02+00:00
- **Authors**: Erkun Yang, Cheng Deng, Chao Li, Wei Liu, Jie Li, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: With explosive growth of data volume and ever-increasing diversity of data modalities, cross-modal similarity search, which conducts nearest neighbor search across different modalities, has been attracting increasing interest. This paper presents a deep compact code learning solution for efficient cross-modal similarity search. Many recent studies have proven that quantization-based approaches perform generally better than hashing-based approaches on single-modal similarity search. In this paper, we propose a deep quantization approach, which is among the early attempts of leveraging deep neural networks into quantization-based cross-modal similarity search. Our approach, dubbed shared predictive deep quantization (SPDQ), explicitly formulates a shared subspace across different modalities and two private subspaces for individual modalities, and representations in the shared subspace and the private subspaces are learned simultaneously by embedding them to a reproducing kernel Hilbert space, where the mean embedding of different modality distributions can be explicitly compared. In addition, in the shared subspace, a quantizer is learned to produce the semantics preserving compact codes with the help of label alignment. Thanks to this novel network architecture in cooperation with supervised quantization training, SPDQ can preserve intramodal and intermodal similarities as much as possible and greatly reduce quantization error. Experiments on two popular benchmarks corroborate that our approach outperforms state-of-the-art methods.



### Discriminative Ridge Machine: A Classifier for High-Dimensional Data or Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/1904.07496v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07496v2)
- **Published**: 2019-04-16 07:07:01+00:00
- **Updated**: 2019-12-30 17:54:33+00:00
- **Authors**: Chong Peng, Qiang Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a discriminative regression approach to supervised classification in this paper. It estimates a representation model while accounting for discriminativeness between classes, thereby enabling accurate derivation of categorical information. This new type of regression models extends existing models such as ridge, lasso, and group lasso through explicitly incorporating discriminative information. As a special case we focus on a quadratic model that admits a closed-form analytical solution. The corresponding classifier is called discriminative regression machine (DRM). Three iterative algorithms are further established for the DRM to enhance the efficiency and scalability for real applications. Our approach and the algorithms are applicable to general types of data including images, high-dimensional data, and imbalanced data. We compare the DRM with currently state-of-the-art classifiers. Our extensive experimental results show superior performance of the DRM and confirm the effectiveness of the proposed approach.



### RES-PCA: A Scalable Approach to Recovering Low-rank Matrices
- **Arxiv ID**: http://arxiv.org/abs/1904.07497v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07497v1)
- **Published**: 2019-04-16 07:07:44+00:00
- **Updated**: 2019-04-16 07:07:44+00:00
- **Authors**: Chong Peng, Chenglizhao Chen, Zhao Kang, Jianbo Li, Qiang Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Robust principal component analysis (RPCA) has drawn significant attentions due to its powerful capability in recovering low-rank matrices as well as successful appplications in various real world problems. The current state-of-the-art algorithms usually need to solve singular value decomposition of large matrices, which generally has at least a quadratic or even cubic complexity. This drawback has limited the application of RPCA in solving real world problems. To combat this drawback, in this paper we propose a new type of RPCA method, RES-PCA, which is linearly efficient and scalable in both data size and dimension. For comparison purpose, AltProj, an existing scalable approach to RPCA requires the precise knowlwdge of the true rank; otherwise, it may fail to recover low-rank matrices. By contrast, our method works with or without knowing the true rank; even when both methods work, our method is faster. Extensive experiments have been performed and testified to the effectiveness of proposed method quantitatively and in visual quality, which suggests that our method is suitable to be employed as a light-weight, scalable component for RPCA in any application pipelines.



### A Deep Optimization Approach for Image Deconvolution
- **Arxiv ID**: http://arxiv.org/abs/1904.07516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07516v1)
- **Published**: 2019-04-16 07:52:45+00:00
- **Updated**: 2019-04-16 07:52:45+00:00
- **Authors**: Zhijian Luo, Siyu Chen, Yuntao Qian
- **Comment**: 12 pages, 16 figures
- **Journal**: None
- **Summary**: In blind image deconvolution, priors are often leveraged to constrain the solution space, so as to alleviate the under-determinacy. Priors which are trained separately from the task of deconvolution tend to be instable, or ineffective. We propose the Golf Optimizer, a novel but simple form of network that learns deep priors from data with better propagation behavior. Like playing golf, our method first estimates an aggressive propagation towards optimum using one network, and recurrently applies a residual CNN to learn the gradient of prior for delicate correction on restoration. Experiments show that our network achieves competitive performance on GoPro dataset, and our model is extremely lightweight compared with the state-of-art works.



### Deep Learning Fundus Image Analysis for Diabetic Retinopathy and Macular Edema Grading
- **Arxiv ID**: http://arxiv.org/abs/1904.08764v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08764v1)
- **Published**: 2019-04-16 08:00:40+00:00
- **Updated**: 2019-04-16 08:00:40+00:00
- **Authors**: Jaakko Sahlsten, Joel Jaskari, Jyri Kivinen, Lauri Turunen, Esa Jaanio, Kustaa Hietala, Kimmo Kaski
- **Comment**: None
- **Journal**: None
- **Summary**: Diabetes is a globally prevalent disease that can cause visible microvascular complications such as diabetic retinopathy and macular edema in the human eye retina, the images of which are today used for manual disease screening. This labor-intensive task could greatly benefit from automatic detection using deep learning technique. Here we present a deep learning system that identifies referable diabetic retinopathy comparably or better than presented in the previous studies, although we use only a small fraction of images (<1/4) in training but are aided with higher image resolutions. We also provide novel results for five different screening and clinical grading systems for diabetic retinopathy and macular edema classification, including results for accurately classifying images according to clinical five-grade diabetic retinopathy and four-grade diabetic macular edema scales. These results suggest, that a deep learning system could increase the cost-effectiveness of screening while attaining higher than recommended performance, and that the system could be applied in clinical examinations requiring finer grading.



### A Deep Journey into Super-resolution: A survey
- **Arxiv ID**: http://arxiv.org/abs/1904.07523v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07523v3)
- **Published**: 2019-04-16 08:08:14+00:00
- **Updated**: 2020-03-23 04:37:12+00:00
- **Authors**: Saeed Anwar, Salman Khan, Nick Barnes
- **Comment**: Accepted in ACM Computing Surveys
- **Journal**: None
- **Summary**: Deep convolutional networks based super-resolution is a fast-growing field with numerous practical applications. In this exposition, we extensively compare 30+ state-of-the-art super-resolution Convolutional Neural Networks (CNNs) over three classical and three recently introduced challenging datasets to benchmark single image super-resolution. We introduce a taxonomy for deep-learning based super-resolution networks that groups existing methods into nine categories including linear, residual, multi-branch, recursive, progressive, attention-based and adversarial designs. We also provide comparisons between the models in terms of network complexity, memory footprint, model input and output, learning details, the type of network losses and important architectural differences (e.g., depth, skip-connections, filters). The extensive evaluation performed, shows the consistent and rapid growth in the accuracy in the past few years along with a corresponding boost in model complexity and the availability of large-scale datasets. It is also observed that the pioneering methods identified as the benchmark have been significantly outperformed by the current contenders. Despite the progress in recent years, we identify several shortcomings of existing techniques and provide future research directions towards the solution of these open problems.



### Disentangling Pose from Appearance in Monochrome Hand Images
- **Arxiv ID**: http://arxiv.org/abs/1904.07528v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.07528v1)
- **Published**: 2019-04-16 08:15:26+00:00
- **Updated**: 2019-04-16 08:15:26+00:00
- **Authors**: Yikang Li, Chris Twigg, Yuting Ye, Lingling Tao, Xiaogang Wang
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Hand pose estimation from the monocular 2D image is challenging due to the variation in lighting, appearance, and background. While some success has been achieved using deep neural networks, they typically require collecting a large dataset that adequately samples all the axes of variation of hand images. It would, therefore, be useful to find a representation of hand pose which is independent of the image appearance~(like hand texture, lighting, background), so that we can synthesize unseen images by mixing pose-appearance combinations. In this paper, we present a novel technique that disentangles the representation of pose from a complementary appearance factor in 2D monochrome images. We supervise this disentanglement process using a network that learns to generate images of hand using specified pose+appearance features. Unlike previous work, we do not require image pairs with a matching pose; instead, we use the pose annotations already available and introduce a novel use of cycle consistency to ensure orthogonality between the factors. Experimental results show that our self-disentanglement scheme successfully decomposes the hand image into the pose and its complementary appearance features of comparable quality as the method using paired data. Additionally, training the model with extra synthesized images with unseen hand-appearance combinations by re-mixing pose and appearance factors from different images can improve the 2D pose estimation performance.



### Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.07537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07537v1)
- **Published**: 2019-04-16 08:49:06+00:00
- **Updated**: 2019-04-16 08:49:06+00:00
- **Authors**: Martin Simon, Karl Amende, Andrea Kraus, Jens Honer, Timo Sämann, Hauke Kaulbersch, Stefan Milz, Horst Michael Gross
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20\% and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.



### Long-Term Human Video Generation of Multiple Futures Using Poses
- **Arxiv ID**: http://arxiv.org/abs/1904.07538v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07538v4)
- **Published**: 2019-04-16 08:50:44+00:00
- **Updated**: 2021-06-01 16:00:22+00:00
- **Authors**: Naoya Fushishita, Antonio Tejero-de-Pablos, Yusuke Mukuta, Tatsuya Harada
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting future human behavior from an input human video is a useful task for applications such as autonomous driving and robotics. While most previous works predict a single future, multiple futures with different behavior can potentially occur. Moreover, if the predicted future is too short (e.g., less than one second), it may not be fully usable by a human or other systems. In this paper, we propose a novel method for future human pose prediction capable of predicting multiple long-term futures. This makes the predictions more suitable for real applications. Also, from the input video and the predicted human behavior, we generate future videos. First, from an input human video, we generate sequences of future human poses (i.e., the image coordinates of their body-joints) via adversarial learning. Adversarial learning suffers from mode collapse, which makes it difficult to generate a variety of multiple poses. We solve this problem by utilizing two additional inputs to the generator to make the outputs diverse, namely, a latent code (to reflect various behaviors) and an attraction point (to reflect various trajectories). In addition, we generate long-term future human poses using a novel approach based on unidimensional convolutional neural networks. Last, we generate an output video based on the generated poses for visualization. We evaluate the generated future poses and videos using three criteria (i.e., realism, diversity and accuracy), and show that our proposed method outperforms other state-of-the-art works.



### Patch alignment manifold matting
- **Arxiv ID**: http://arxiv.org/abs/1904.07588v1
- **DOI**: 10.1109/TNNLS.2017.2727140
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07588v1)
- **Published**: 2019-04-16 10:52:24+00:00
- **Updated**: 2019-04-16 10:52:24+00:00
- **Authors**: Xuelong Li, Kang Liu, Yongsheng Dong, Dacheng Tao
- **Comment**: 13 pages, 7 figures
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, July,
  2018
- **Summary**: Image matting is generally modeled as a space transform from the color space to the alpha space. By estimating the alpha factor of the model, the foreground of an image can be extracted. However, there is some dimensional information redundancy in the alpha space. It usually leads to the misjudgments of some pixels near the boundary between the foreground and the background. In this paper, a manifold matting framework named Patch Alignment Manifold Matting is proposed for image matting. In particular, we first propose a part modeling of color space in the local image patch. We then perform whole alignment optimization for approximating the alpha results using subspace reconstructing error. Furthermore, we utilize Nesterov's algorithm to solve the optimization problem. Finally, we apply some manifold learning methods in the framework, and obtain several image matting methods, such as named ISOMAP matting and its derived Cascade ISOMAP matting. The experimental results reveal that the manifold matting framework and its two examples are effective when compared with several representative matting methods.



### Detecting the Unexpected via Image Resynthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.07595v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1904.07595v2)
- **Published**: 2019-04-16 11:08:39+00:00
- **Updated**: 2019-04-17 12:27:02+00:00
- **Authors**: Krzysztof Lis, Krishna Nakka, Pascal Fua, Mathieu Salzmann
- **Comment**: None
- **Journal**: None
- **Summary**: Classical semantic segmentation methods, including the recent deep learning ones, assume that all classes observed at test time have been seen during training. In this paper, we tackle the more realistic scenario where unexpected objects of unknown classes can appear at test time. The main trends in this area either leverage the notion of prediction uncertainty to flag the regions with low confidence as unknown, or rely on autoencoders and highlight poorly-decoded regions. Having observed that, in both cases, the detected regions typically do not correspond to unexpected objects, in this paper, we introduce a drastically different strategy: It relies on the intuition that the network will produce spurious labels in regions depicting unexpected objects. Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image. In other words, we translate the problem of detecting unknown classes to one of identifying poorly-resynthesized image regions. We show that this outperforms both uncertainty- and autoencoder-based methods.



### Relation-Shape Convolutional Neural Network for Point Cloud Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.07601v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.07601v3)
- **Published**: 2019-04-16 11:28:51+00:00
- **Updated**: 2019-05-26 03:55:11+00:00
- **Authors**: Yongcheng Liu, Bin Fan, Shiming Xiang, Chunhong Pan
- **Comment**: Accepted to CVPR 2019 as an oral presentation. Project page at
  https://yochengliu.github.io/Relation-Shape-CNN
- **Journal**: None
- **Summary**: Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.



### Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning
- **Arxiv ID**: http://arxiv.org/abs/1904.07615v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.07615v2)
- **Published**: 2019-04-16 12:11:26+00:00
- **Updated**: 2019-10-17 22:53:52+00:00
- **Authors**: Pedro Hermosilla, Tobias Ritschel, Timo Ropinski
- **Comment**: Proceedings of ICCV 2019
- **Journal**: None
- **Summary**: We show that denoising of 3D point clouds can be learned unsupervised, directly from noisy 3D point cloud data only. This is achieved by extending recent ideas from learning of unsupervised image denoisers to unstructured 3D point clouds. Unsupervised image denoisers operate under the assumption that a noisy pixel observation is a random realization of a distribution around a clean pixel value, which allows appropriate learning on this distribution to eventually converge to the correct value. Regrettably, this assumption is not valid for unstructured points: 3D point clouds are subject to total noise, i. e., deviations in all coordinates, with no reliable pixel grid. Thus, an observation can be the realization of an entire manifold of clean 3D points, which makes a na\"ive extension of unsupervised image denoisers to 3D point clouds impractical. Overcoming this, we introduce a spatial prior term, that steers converges to the unique closest out of the many possible modes on a manifold. Our results demonstrate unsupervised denoising performance similar to that of supervised learning with clean data when given enough training examples - whereby we do not need any pairs of noisy and clean training data.



### SparseMask: Differentiable Connectivity Learning for Dense Image Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.07642v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07642v2)
- **Published**: 2019-04-16 13:13:53+00:00
- **Updated**: 2019-08-04 10:03:53+00:00
- **Authors**: Huikai Wu, Junge Zhang, Kaiqi Huang
- **Comment**: Accepted by ICCV 2019. Code is available at
  https://github.com/wuhuikai/SparseMask
- **Journal**: None
- **Summary**: In this paper, we aim at automatically searching an efficient network architecture for dense image prediction. Particularly, we follow the encoder-decoder style and focus on designing a connectivity structure for the decoder. To achieve that, we design a densely connected network with learnable connections, named Fully Dense Network, which contains a large set of possible final connectivity structures. We then employ gradient descent to search the optimal connectivity from the dense connections. The search process is guided by a novel loss function, which pushes the weight of each connection to be binary and the connections to be sparse. The discovered connectivity achieves competitive results on two segmentation datasets, while runs more than three times faster and requires less than half parameters compared to the state-of-the-art methods. An extensive experiment shows that the discovered connectivity is compatible with various backbones and generalizes well to other dense image prediction tasks.



### LBVCNN: Local Binary Volume Convolutional Neural Network for Facial Expression Recognition from Image Sequences
- **Arxiv ID**: http://arxiv.org/abs/1904.07647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07647v1)
- **Published**: 2019-04-16 13:19:59+00:00
- **Updated**: 2019-04-16 13:19:59+00:00
- **Authors**: Sudhakar Kumawat, Manisha Verma, Shanmuganathan Raman
- **Comment**: Accepted in CVPRW 2019
- **Journal**: None
- **Summary**: Recognizing facial expressions is one of the central problems in computer vision. Temporal image sequences have useful spatio-temporal features for recognizing expressions. In this paper, we propose a new 3D Convolution Neural Network (CNN) that can be trained end-to-end for facial expression recognition on temporal image sequences without using facial landmarks. More specifically, a novel 3D convolutional layer that we call Local Binary Volume (LBV) layer is proposed. The LBV layer, when used with our newly proposed LBVCNN network, achieve comparable results compared to state-of-the-art landmark-based or without landmark-based models on image sequences from CK+, Oulu-CASIA, and UNBC McMaster shoulder pain datasets. Furthermore, our LBV layer reduces the number of trainable parameters by a significant amount when compared to a conventional 3D convolutional layer. As a matter of fact, when compared to a 3x3x3 conventional 3D convolutional layer, the LBV layer uses 27 times less trainable parameters.



### Semantically Aligned Bias Reducing Zero Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.07659v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07659v1)
- **Published**: 2019-04-16 13:37:15+00:00
- **Updated**: 2019-04-16 13:37:15+00:00
- **Authors**: Akanksha Paul, Narayanan C. Krishnan, Prateek Munjal
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2019)
- **Journal**: None
- **Summary**: Zero shot learning (ZSL) aims to recognize unseen classes by exploiting semantic relationships between seen and unseen classes. Two major problems faced by ZSL algorithms are the hubness problem and the bias towards the seen classes. Existing ZSL methods focus on only one of these problems in the conventional and generalized ZSL setting. In this work, we propose a novel approach, Semantically Aligned Bias Reducing (SABR) ZSL, which focuses on solving both the problems. It overcomes the hubness problem by learning a latent space that preserves the semantic relationship between the labels while encoding the discriminating information about the classes. Further, we also propose ways to reduce the bias of the seen classes through a simple cross-validation process in the inductive setting and a novel weak transfer constraint in the transductive setting. Extensive experiments on three benchmark datasets suggest that the proposed model significantly outperforms existing state-of-the-art algorithms by ~1.5-9% in the conventional ZSL setting and by ~2-14% in the generalized ZSL for both the inductive and transductive settings.



### Co-Separating Sounds of Visual Objects
- **Arxiv ID**: http://arxiv.org/abs/1904.07750v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1904.07750v2)
- **Published**: 2019-04-16 15:07:50+00:00
- **Updated**: 2019-08-20 21:18:03+00:00
- **Authors**: Ruohan Gao, Kristen Grauman
- **Comment**: ICCV 2019, Project page:
  http://vision.cs.utexas.edu/projects/coseparation/
- **Journal**: None
- **Summary**: Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of "true" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.



### Persistence Curves: A canonical framework for summarizing persistence diagrams
- **Arxiv ID**: http://arxiv.org/abs/1904.07768v4
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV, math.AT, 55N31, 55-04, 68T10, I.5.2
- **Links**: [PDF](http://arxiv.org/pdf/1904.07768v4)
- **Published**: 2019-04-16 15:38:41+00:00
- **Updated**: 2021-08-09 17:56:55+00:00
- **Authors**: Yu-Min Chung, Austin Lawson
- **Comment**: None
- **Journal**: None
- **Summary**: Persistence diagrams are one of the main tools in the field of Topological Data Analysis (TDA). They contain fruitful information about the shape of data. The use of machine learning algorithms on the space of persistence diagrams proves to be challenging as the space lacks an inner product. For that reason, transforming these diagrams in a way that is compatible with machine learning is an important topic currently researched in TDA. In this paper, our main contribution consists of three components. First, we develop a general and unifying framework of vectorizing diagrams that we call the \textit{Persistence Curves} (PCs), and show that several well-known summaries, such as Persistence Landscapes, fall under the PC framework. Second, we propose several new summaries based on PC framework and provide a theoretical foundation for their stability analysis. Finally, we apply proposed PCs to two applications---texture classification and determining the parameters of a discrete dynamical system; their performances are competitive with other TDA methods.



### Cryo-Electron Microscopy Image Analysis Using Multi-Frequency Vector Diffusion Maps
- **Arxiv ID**: http://arxiv.org/abs/1904.07772v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.BM
- **Links**: [PDF](http://arxiv.org/pdf/1904.07772v1)
- **Published**: 2019-04-16 15:45:28+00:00
- **Updated**: 2019-04-16 15:45:28+00:00
- **Authors**: Yifeng Fan, Zhizhen Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Cryo-electron microscopy (EM) single particle reconstruction is an entirely general technique for 3D structure determination of macromolecular complexes. However, because the images are taken at low electron dose, it is extremely hard to visualize the individual particle with low contrast and high noise level. In this paper, we propose a novel approach called multi-frequency vector diffusion maps (MFVDM) to improve the efficiency and accuracy of cryo-EM 2D image classification and denoising. This framework incorporates different irreducible representations of the estimated alignment between similar images. In addition, we propose a graph filtering scheme to denoise the images using the eigenvalues and eigenvectors of the MFVDM matrices. Through both simulated and publicly available real data, we demonstrate that our proposed method is efficient and robust to noise compared with the state-of-the-art cryo-EM 2D class averaging and image restoration algorithms.



### Weakly Supervised Gaussian Networks for Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.07774v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07774v4)
- **Published**: 2019-04-16 15:48:36+00:00
- **Updated**: 2020-01-06 02:46:05+00:00
- **Authors**: Basura Fernando, Cheston Tan Yin Chet, Hakan Bilen
- **Comment**: Accepted in WACV 2020
- **Journal**: None
- **Summary**: Detecting temporal extents of human actions in videos is a challenging computer vision problem that requires detailed manual supervision including frame-level labels. This expensive annotation process limits deploying action detectors to a limited number of categories. We propose a novel method, called WSGN, that learns to detect actions from \emph{weak supervision}, using only video-level labels. WSGN learns to exploit both video-specific and dataset-wide statistics to predict relevance of each frame to an action category. This strategy leads to significant gains in action detection for two standard benchmarks THUMOS14 and Charades. Our method obtains excellent results compared to state-of-the-art methods that uses similar features and loss functions on THUMOS14 dataset. Similarly, our weakly supervised method is only 0.3% mAP behind a state-of-the-art supervised method on challenging Charades dataset for action localization.



### The ALOS Dataset for Advert Localization in Outdoor Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.07776v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07776v1)
- **Published**: 2019-04-16 15:50:24+00:00
- **Updated**: 2019-04-16 15:50:24+00:00
- **Authors**: Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, François Pitié
- **Comment**: Published in Proc. Eleventh International Conference on Quality of
  Multimedia Experience (QoMEX), 2019
- **Journal**: None
- **Summary**: The rapid increase in the number of online videos provides the marketing and advertising agents ample opportunities to reach out to their audience. One of the most widely used strategies is product placement, or embedded marketing, wherein new advertisements are integrated seamlessly into existing advertisements in videos. Such strategies involve accurately localizing the position of the advert in the image frame, either manually in the video editing phase, or by using machine learning frameworks. However, these machine learning techniques and deep neural networks need a massive amount of data for training. In this paper, we propose and release the first large-scale dataset of advertisement billboards, captured in outdoor scenes. We also benchmark several state-of-the-art semantic segmentation algorithms on our proposed dataset.



### AT-GAN: An Adversarial Generator Model for Non-constrained Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1904.07793v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07793v4)
- **Published**: 2019-04-16 16:26:19+00:00
- **Updated**: 2020-02-07 18:11:58+00:00
- **Authors**: Xiaosen Wang, Kun He, Chuanbiao Song, Liwei Wang, John E. Hopcroft
- **Comment**: 15 pages, 6 figures
- **Journal**: None
- **Summary**: Despite the rapid development of adversarial machine learning, most adversarial attack and defense researches mainly focus on the perturbation-based adversarial examples, which is constrained by the input images. In comparison with existing works, we propose non-constrained adversarial examples, which are generated entirely from scratch without any constraint on the input. Unlike perturbation-based attacks, or the so-called unrestricted adversarial attack which is still constrained by the input noise, we aim to learn the distribution of adversarial examples to generate non-constrained but semantically meaningful adversarial examples. Following this spirit, we propose a novel attack framework called AT-GAN (Adversarial Transfer on Generative Adversarial Net). Specifically, we first develop a normal GAN model to learn the distribution of benign data, and then transfer the pre-trained GAN model to estimate the distribution of adversarial examples for the target model. In this way, AT-GAN can learn the distribution of adversarial examples that is very close to the distribution of real data. To our knowledge, this is the first work of building an adversarial generator model that could produce adversarial examples directly from any input noise. Extensive experiments and visualizations show that the proposed AT-GAN can very efficiently generate diverse adversarial examples that are more realistic to human perception. In addition, AT-GAN yields higher attack success rates against adversarially trained models under white-box attack setting and exhibits moderate transferability against black-box models.



### Visual Relationship Detection with Language prior and Softmax
- **Arxiv ID**: http://arxiv.org/abs/1904.07798v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07798v1)
- **Published**: 2019-04-16 16:29:52+00:00
- **Updated**: 2019-04-16 16:29:52+00:00
- **Authors**: Jaewon Jung, Jongyoul Park
- **Comment**: 6 pages, 4 figures
- **Journal**: Third IEEE International Conference on Image Processing,
  Applications and Systems (IPAS 2018)
- **Summary**: Visual relationship detection is an intermediate image understanding task that detects two objects and classifies a predicate that explains the relationship between two objects in an image. The three components are linguistically and visually correlated (e.g. "wear" is related to "person" and "shirt", while "laptop" is related to "table" and "on") thus, the solution space is huge because there are many possible cases between them. Language and visual modules are exploited and a sophisticated spatial vector is proposed. The models in this work outperformed the state of arts without costly linguistic knowledge distillation from a large text corpus and building complex loss functions. All experiments were only evaluated on Visual Relationship Detection and Visual Genome dataset.



### Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents
- **Arxiv ID**: http://arxiv.org/abs/1904.07826v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07826v2)
- **Published**: 2019-04-16 17:07:20+00:00
- **Updated**: 2019-08-31 20:43:53+00:00
- **Authors**: Jack Hessel, Lillian Lee, David Mimno
- **Comment**: Code and data available at
  http://www.cs.cornell.edu/~jhessel/multiretrieval/multiretrieval.html
- **Journal**: EMNLP 2019
- **Summary**: Images and text co-occur constantly on the web, but explicit links between images and sentences (or other intra-document textual units) are often not present. We present algorithms that discover image-sentence relationships without relying on explicit multimodal annotation in training. We experiment on seven datasets of varying difficulty, ranging from documents consisting of groups of images captioned post hoc by crowdworkers to naturally-occurring user-generated multimodal documents. We find that a structured training objective based on identifying whether collections of images and sentences co-occur in documents can suffice to predict links between specific sentences and specific images within the same document at test time.



### Double Transfer Learning for Breast Cancer Histopathologic Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.07834v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07834v1)
- **Published**: 2019-04-16 17:26:34+00:00
- **Updated**: 2019-04-16 17:26:34+00:00
- **Authors**: Jonathan de Matos, Alceu de S. Britto Jr., Luiz E. S. Oliveira, Alessandro L. Koerich
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes a classification approach for breast cancer histopathologic images (HI) that uses transfer learning to extract features from HI using an Inception-v3 CNN pre-trained with ImageNet dataset. We also use transfer learning on training a support vector machine (SVM) classifier on a tissue labeled colorectal cancer dataset aiming to filter the patches from a breast cancer HI and remove the irrelevant ones. We show that removing irrelevant patches before training a second SVM classifier, improves the accuracy for classifying malign and benign tumors on breast cancer images. We are able to improve the classification accuracy in 3.7% using the feature extraction transfer learning and an additional 0.7% using the irrelevant patch elimination. The proposed approach outperforms the state-of-the-art in three out of the four magnification factors of the breast cancer dataset.



### Temporal Cycle-Consistency Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.07846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07846v1)
- **Published**: 2019-04-16 17:49:50+00:00
- **Updated**: 2019-04-16 17:49:50+00:00
- **Authors**: Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman
- **Comment**: Accepted at CVPR 2019. Project webpage:
  https://sites.google.com/view/temporal-cycle-consistency
- **Journal**: None
- **Summary**: We introduce a self-supervised representation learning method based on the task of temporal alignment between videos. The method trains a network using temporal cycle consistency (TCC), a differentiable cycle-consistency loss that can be used to find correspondences across time in multiple videos. The resulting per-frame embeddings can be used to align videos by simply matching frames using the nearest-neighbors in the learned embedding space.   To evaluate the power of the embeddings, we densely label the Pouring and Penn Action video datasets for action phases. We show that (i) the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and (ii) TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings are also used for a number of applications based on alignment (dense temporal correspondence) between video pairs, including transfer of metadata of synchronized modalities between videos (sounds, temporal semantic labels), synchronized playback of multiple videos, and anomaly detection. Project webpage: https://sites.google.com/view/temporal-cycle-consistency .



### Active Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1904.07848v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07848v2)
- **Published**: 2019-04-16 17:52:54+00:00
- **Updated**: 2020-03-09 19:19:09+00:00
- **Authors**: Jong-Chyi Su, Yi-Hsuan Tsai, Kihyuk Sohn, Buyu Liu, Subhransu Maji, Manmohan Chandraker
- **Comment**: WACV 2020 Camera Ready Version
- **Journal**: None
- **Summary**: We propose an active learning approach for transferring representations across domains. Our approach, active adversarial domain adaptation (AADA), explores a duality between two related problems: adversarial domain alignment and importance sampling for adapting models across domains. The former uses a domain discriminative model to align domains, while the latter utilizes it to weigh samples to account for distribution shifts. Specifically, our importance weight promotes samples with large uncertainty in classification and diversity from labeled examples, thus serves as a sample selection scheme for active learning. We show that these two views can be unified in one framework for domain adaptation and transfer learning when the source domain has many labeled examples while the target domain does not. AADA provides significant improvements over fine-tuning based approaches and other sampling methods when the two domains are closely related. Results on challenging domain adaptation tasks, e.g., object detection, demonstrate that the advantage over baseline approaches is retained even after hundreds of examples being actively annotated.



### Objects as Points
- **Arxiv ID**: http://arxiv.org/abs/1904.07850v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07850v2)
- **Published**: 2019-04-16 17:54:26+00:00
- **Updated**: 2019-04-25 16:20:02+00:00
- **Authors**: Xingyi Zhou, Dequan Wang, Philipp Krähenbühl
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.



### Matrix and tensor decompositions for training binary neural networks
- **Arxiv ID**: http://arxiv.org/abs/1904.07852v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07852v1)
- **Published**: 2019-04-16 17:57:27+00:00
- **Updated**: 2019-04-16 17:57:27+00:00
- **Authors**: Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: This paper is on improving the training of binary neural networks in which both activations and weights are binary. While prior methods for neural network binarization binarize each filter independently, we propose to instead parametrize the weight tensor of each layer using matrix or tensor decomposition. The binarization process is then performed using this latent parametrization, via a quantization function (e.g. sign function) applied to the reconstructed weights. A key feature of our method is that while the reconstruction is binarized, the computation in the latent factorized space is done in the real domain. This has several advantages: (i) the latent factorization enforces a coupling of the filters before binarization, which significantly improves the accuracy of the trained models. (ii) while at training time, the binary weights of each convolutional layer are parametrized using real-valued matrix or tensor decomposition, during inference we simply use the reconstructed (binary) weights. As a result, our method does not sacrifice any advantage of binary networks in terms of model compression and speeding-up inference. As a further contribution, instead of computing the binary weight scaling factors analytically, as in prior work, we propose to learn them discriminatively via back-propagation. Finally, we show that our approach significantly outperforms existing methods when tested on the challenging tasks of (a) human pose estimation (more than 4% improvements) and (b) ImageNet classification (up to 5% performance gains).



### End-to-End Robotic Reinforcement Learning without Reward Engineering
- **Arxiv ID**: http://arxiv.org/abs/1904.07854v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07854v2)
- **Published**: 2019-04-16 17:59:23+00:00
- **Updated**: 2019-05-16 00:00:22+00:00
- **Authors**: Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, Sergey Levine
- **Comment**: Accepted to RSS 2019. 14 pages and 13 figures including references
  and appendix. Website: https://sites.google.com/view/reward-learning-rl/home
- **Journal**: None
- **Summary**: The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.



### Histopathologic Image Processing: A Review
- **Arxiv ID**: http://arxiv.org/abs/1904.07900v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07900v1)
- **Published**: 2019-04-16 18:04:24+00:00
- **Updated**: 2019-04-16 18:04:24+00:00
- **Authors**: Jonathan de Matos, Alceu de Souza Britto Jr., Luiz E. S. Oliveira, Alessandro L. Koerich
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathologic Images (HI) are the gold standard for evaluation of some tumors. However, the analysis of such images is challenging even for experienced pathologists, resulting in problems of inter and intra observer. Besides that, the analysis is time and resource consuming. One of the ways to accelerate such an analysis is by using Computer Aided Diagnosis systems. In this work we present a literature review about the computing techniques to process HI, including shallow and deep methods. We cover the most common tasks for processing HI such as segmentation, feature extraction, unsupervised learning and supervised learning. A dataset section show some datasets found during the literature review. We also bring a study case of breast cancer classification using a mix of deep and shallow machine learning methods. The proposed method obtained an accuracy of 91% in the best case, outperforming the compared baseline of the dataset.



### REPAIR: Removing Representation Bias by Dataset Resampling
- **Arxiv ID**: http://arxiv.org/abs/1904.07911v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07911v1)
- **Published**: 2019-04-16 18:35:40+00:00
- **Updated**: 2019-04-16 18:35:40+00:00
- **Authors**: Yi Li, Nuno Vasconcelos
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Modern machine learning datasets can have biases for certain representations that are leveraged by algorithms to achieve high performance without learning to solve the underlying task. This problem is referred to as "representation bias". The question of how to reduce the representation biases of a dataset is investigated and a new dataset REPresentAtion bIas Removal (REPAIR) procedure is proposed. This formulates bias minimization as an optimization problem, seeking a weight distribution that penalizes examples easy for a classifier built on a given feature representation. Bias reduction is then equated to maximizing the ratio between the classification loss on the reweighted dataset and the uncertainty of the ground-truth class labels. This is a minimax problem that REPAIR solves by alternatingly updating classifier parameters and dataset resampling weights, using stochastic gradient descent. An experimental set-up is also introduced to measure the bias of any dataset for a given representation, and the impact of this bias on the performance of recognition models. Experiments with synthetic and action recognition data show that dataset REPAIR can significantly reduce representation bias, and lead to improved generalization of models trained on REPAIRed datasets. The tools used for characterizing representation bias, and the proposed dataset REPAIR algorithm, are available at https://github.com/JerryYLi/Dataset-REPAIR/.



### IAN: Combining Generative Adversarial Networks for Imaginative Face Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.07916v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.4; I.4; I.2.6; D.4.6; E.3
- **Links**: [PDF](http://arxiv.org/pdf/1904.07916v1)
- **Published**: 2019-04-16 18:45:33+00:00
- **Updated**: 2019-04-16 18:45:33+00:00
- **Authors**: Abdullah Hamdi, Bernard Ghanem
- **Comment**: preprint
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.



### Audio-Visual Model Distillation Using Acoustic Images
- **Arxiv ID**: http://arxiv.org/abs/1904.07933v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1904.07933v2)
- **Published**: 2019-04-16 19:15:00+00:00
- **Updated**: 2020-02-11 11:01:28+00:00
- **Authors**: Andrés F. Pérez, Valentina Sanguineti, Pietro Morerio, Vittorio Murino
- **Comment**: Accepted at WACV 2020; supplementary material at page 11; code
  available at https://github.com/afperezm/acoustic-images-distillation
- **Journal**: None
- **Summary**: In this paper, we investigate how to learn rich and robust feature representations for audio classification from visual data and acoustic images, a novel audio data modality. Former models learn audio representations from raw signals or spectral data acquired by a single microphone, with remarkable results in classification and retrieval. However, such representations are not so robust towards variable environmental sound conditions. We tackle this drawback by exploiting a new multimodal labeled action recognition dataset acquired by a hybrid audio-visual sensor that provides RGB video, raw audio signals, and spatialized acoustic data, also known as acoustic images, where the visual and acoustic images are aligned in space and synchronized in time. Using this richer information, we train audio deep learning models in a teacher-student fashion. In particular, we distill knowledge into audio networks from both visual and acoustic image teachers. Our experiments suggest that the learned representations are more powerful and have better generalization capabilities than the features learned from models trained using just single-microphone audio data.



### Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations
- **Arxiv ID**: http://arxiv.org/abs/1904.07934v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.07934v2)
- **Published**: 2019-04-16 19:16:57+00:00
- **Updated**: 2019-06-09 17:30:52+00:00
- **Authors**: David Acuna, Amlan Kar, Sanja Fidler
- **Comment**: Accepted as a CVPR 2019 oral paper (Project Page:
  https://nv-tlabs.github.io/STEAL/)
- **Journal**: CVPR 2019
- **Summary**: We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.



### Beyond Correlation: A Path-Invariant Measure for Seismogram Similarity
- **Arxiv ID**: http://arxiv.org/abs/1904.07936v4
- **DOI**: 10.1785/0220190090
- **Categories**: **physics.geo-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07936v4)
- **Published**: 2019-04-16 19:22:00+00:00
- **Updated**: 2019-10-08 18:48:26+00:00
- **Authors**: Joshua Dickey, Brett Borghetti, William Junek, Richard Martin
- **Comment**: None
- **Journal**: Seismological Research Letters 2019
- **Summary**: Similarity search is a popular technique for seismic signal processing, with template matching, matched filters and subspace detectors being utilized for a wide variety of tasks, including both signal detection and source discrimination. Traditionally, these techniques rely on the cross-correlation function as the basis for measuring similarity. Unfortunately, seismogram correlation is dominated by path effects, essentially requiring a distinct waveform template along each path of interest. To address this limitation, we propose a novel measure of seismogram similarity that is explicitly invariant to path. Using Earthscope's USArray experiment, a path-rich dataset of 207,291 regional seismograms across 8,452 unique events is constructed, and then employed via the batch-hard triplet loss function, to train a deep convolutional neural network which maps raw seismograms to a low dimensional embedding space, where nearness on the space corresponds to nearness of source function, regardless of path or recording instrumentation. This path-agnostic embedding space forms a new representation for seismograms, characterized by robust, source-specific features, which we show to be useful for performing both pairwise event association as well as template-based source discrimination with a single template.



### A Comprehensive Study of Alzheimer's Disease Classification Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.07950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07950v1)
- **Published**: 2019-04-16 20:00:15+00:00
- **Updated**: 2019-04-16 20:00:15+00:00
- **Authors**: Ziqiang Guan, Ritesh Kumar, Yi Ren Fung, Yeahuay Wu, Madalina Fiterau
- **Comment**: None
- **Journal**: None
- **Summary**: A plethora of deep learning models have been developed for the task of Alzheimer's disease classification from brain MRI scans. Many of these models report high performance, achieving three-class classification accuracy of up to 95%. However, it is common for these studies to draw performance comparisons between models that are trained on different subsets of a dataset or use varying imaging preprocessing techniques, making it difficult to objectively assess model performance. Furthermore, many of these works do not provide details such as hyperparameters, the specific MRI scans used, or their source code, making it difficult to replicate their experiments. To address these concerns, we present a comprehensive study of some of the deep learning methods and architectures on the full set of images available from ADNI. We find that, (1) classification using 3D models gives an improvement of 1% in our setup, at the cost of significantly longer training time and more computation power, (2) with our dataset, pre-training yields minimal ($<0.5\%$) improvement in model performance, (3) most popular convolutional neural network models yield similar performance when compared to each other. Lastly, we briefly compare the effects of two image preprocessing programs: FreeSurfer and Clinica, and find that the spatially normalized and segmented outputs from Clinica increased the accuracy of model prediction from 63% to 89% when compared to FreeSurfer images.



### Are State-of-the-art Visual Place Recognition Techniques any Good for Aerial Robotics?
- **Arxiv ID**: http://arxiv.org/abs/1904.07967v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07967v2)
- **Published**: 2019-04-16 20:34:39+00:00
- **Updated**: 2019-05-22 18:21:04+00:00
- **Authors**: Mubariz Zaffar, Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Kostas Alexis, Klaus McDonald-Maier
- **Comment**: IEEE ICRA 2019 Workshop on Aerial Robotics 8 pages, 7 figures
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) has seen significant advances at the frontiers of matching performance and computational superiority over the past few years. However, these evaluations are performed for ground-based mobile platforms and cannot be generalized to aerial platforms. The degree of viewpoint variation experienced by aerial robots is complex, with their processing power and on-board memory limited by payload size and battery ratings. Therefore, in this paper, we collect $8$ state-of-the-art VPR techniques that have been previously evaluated for ground-based platforms and compare them on $2$ recently proposed aerial place recognition datasets with three prime focuses: a) Matching performance b) Processing power consumption c) Projected memory requirements. This gives a birds-eye view of the applicability of contemporary VPR research to aerial robotics and lays down the the nature of challenges for aerial-VPR.



### DNN Architecture for High Performance Prediction on Natural Videos Loses Submodule's Ability to Learn Discrete-World Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.07969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07969v1)
- **Published**: 2019-04-16 20:35:09+00:00
- **Updated**: 2019-04-16 20:35:09+00:00
- **Authors**: Lana Sinapayen, Atsushi Noda
- **Comment**: None
- **Journal**: None
- **Summary**: Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.



### CloudSegNet: A Deep Network for Nychthemeron Cloud Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.07979v1
- **DOI**: 10.1109/LGRS.2019.2912140
- **Categories**: **physics.ao-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07979v1)
- **Published**: 2019-04-16 20:49:20+00:00
- **Updated**: 2019-04-16 20:49:20+00:00
- **Authors**: Soumyabrata Dev, Atul Nautiyal, Yee Hui Lee, Stefan Winkler
- **Comment**: Published in IEEE Geoscience and Remote Sensing Letters, 2019
- **Journal**: None
- **Summary**: We analyze clouds in the earth's atmosphere using ground-based sky cameras. An accurate segmentation of clouds in the captured sky/cloud image is difficult, owing to the fuzzy boundaries of clouds. Several techniques have been proposed that use color as the discriminatory feature for cloud detection. In the existing literature, however, analysis of daytime and nighttime images is considered separately, mainly because of differences in image characteristics and applications. In this paper, we propose a light-weight deep-learning architecture called CloudSegNet. It is the first that integrates daytime and nighttime (also known as nychthemeron) image segmentation in a single framework, and achieves state-of-the-art results on public databases.



### Fast object detection in compressed JPEG Images
- **Arxiv ID**: http://arxiv.org/abs/1904.08408v1
- **DOI**: 10.1109/ITSC.2019.8916937
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08408v1)
- **Published**: 2019-04-16 22:10:53+00:00
- **Updated**: 2019-04-16 22:10:53+00:00
- **Authors**: Benjamin Deguerre, Clément Chatelain, Gilles Gasso
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in still images has drawn a lot of attention over past few years, and with the advent of Deep Learning impressive performances have been achieved with numerous industrial applications. Most of these deep learning models rely on RGB images to localize and identify objects in the image. However in some application scenarii, images are compressed either for storage savings or fast transmission. Therefore a time consuming image decompression step is compulsory in order to apply the aforementioned deep models. To alleviate this drawback, we propose a fast deep architecture for object detection in JPEG images, one of the most widespread compression format. We train a neural network to detect objects based on the blockwise DCT (discrete cosine transform) coefficients {issued from} the JPEG compression algorithm. We modify the well-known Single Shot multibox Detector (SSD) by replacing its first layers with one convolutional layer dedicated to process the DCT inputs. Experimental evaluations on PASCAL VOC and industrial dataset comprising images of road traffic surveillance show that the model is about $2\times$ faster than regular SSD with promising detection performances. To the best of our knowledge, this paper is the first to address detection in compressed JPEG images.



### Clustered Object Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1904.08008v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08008v3)
- **Published**: 2019-04-16 23:01:53+00:00
- **Updated**: 2019-08-27 01:43:34+00:00
- **Authors**: Fan Yang, Heng Fan, Peng Chu, Erik Blasch, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting objects in aerial images is challenging for at least two reasons: (1) target objects like pedestrians are very small in pixels, making them hardly distinguished from surrounding background; and (2) targets are in general sparsely and non-uniformly distributed, making the detection very inefficient. In this paper, we address both issues inspired by observing that these targets are often clustered. In particular, we propose a Clustered Detection (ClusDet) network that unifies object clustering and detection in an end-to-end framework. The key components in ClusDet include a cluster proposal sub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated detection network (DetecNet). Given an input image, CPNet produces object cluster regions and ScaleNet estimates object scales for these regions. Then, each scale-normalized cluster region is fed into DetecNet for object detection. ClusDet has several advantages over previous solutions: (1) it greatly reduces the number of chips for final object detection and hence achieves high running time efficiency, (2) the cluster-based scale estimation is more accurate than previously used single-object based ones, hence effectively improves the detection for small objects, and (3) the final DetecNet is dedicated for clustered regions and implicitly models the prior context information so as to boost detection accuracy. The proposed method is tested on three popular aerial image datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet achieves promising performance in comparison with state-of-the-art detectors. Code will be available in \url{https://github.com/fyangneil}.



### A-CNN: Annularly Convolutional Neural Networks on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.08017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08017v1)
- **Published**: 2019-04-16 23:34:55+00:00
- **Updated**: 2019-04-16 23:34:55+00:00
- **Authors**: Artem Komarichev, Zichun Zhong, Jing Hua
- **Comment**: 17 pages, 14 figures. To appear, Proceedings of the IEEE Conference
  on Computer Vision and Pattern Recognition (CVPR), June 2019
- **Journal**: None
- **Summary**: Analyzing the geometric and semantic properties of 3D point clouds through the deep networks is still challenging due to the irregularity and sparsity of samplings of their geometric structures. This paper presents a new method to define and compute convolution directly on 3D point clouds by the proposed annular convolution. This new convolution operator can better capture the local neighborhood geometry of each point by specifying the (regular and dilated) ring-shaped structures and directions in the computation. It can adapt to the geometric variability and scalability at the signal processing level. We apply it to the developed hierarchical neural networks for object classification, part segmentation, and semantic segmentation in large-scale scenes. The extensive experiments and comparisons demonstrate that our approach outperforms the state-of-the-art methods on a variety of standard benchmark datasets (e.g., ModelNet10, ModelNet40, ShapeNet-part, S3DIS, and ScanNet).



