# Arxiv Papers in cs.CV on 2019-04-20
### LORAKI: Autocalibrated Recurrent Neural Networks for Autoregressive MRI Reconstruction in k-Space
- **Arxiv ID**: http://arxiv.org/abs/1904.09390v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09390v2)
- **Published**: 2019-04-20 03:02:34+00:00
- **Updated**: 2019-04-24 01:16:06+00:00
- **Authors**: Tae Hyung Kim, Pratyush Garg, Justin P. Haldar
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and evaluate a new MRI reconstruction method named LORAKI that trains an autocalibrated scan-specific recurrent neural network (RNN) to recover missing k-space data. Methods like GRAPPA, SPIRiT, and AC-LORAKS assume that k-space data has shift-invariant autoregressive structure, and that the scan-specific autoregression relationships needed to recover missing samples can be learned from fully-sampled autocalibration (ACS) data. Recently, the structure of the linear GRAPPA method has been translated into a nonlinear deep learning method named RAKI. RAKI uses ACS data to train an artificial neural network to interpolate missing k-space samples, and often outperforms GRAPPA. In this work, we apply a similar principle to translate the linear AC-LORAKS method (simultaneously incorporating support, phase, and parallel imaging constraints) into a nonlinear deep learning method named LORAKI. Since AC-LORAKS is iterative and convolutional, LORAKI takes the form of a convolutional RNN. This new architecture admits a wide range of sampling patterns, and even calibrationless patterns are possible if synthetic ACS data is generated. The performance of LORAKI was evaluated with retrospectively undersampled brain datasets, with comparisons against other related reconstruction methods. Results suggest that LORAKI can provide improved reconstruction compared to other scan-specific autocalibrated reconstruction methods like GRAPPA, RAKI, and AC-LORAKS. LORAKI offers a new deep-learning approach to MRI reconstruction based on RNNs in k-space, and enables improved image quality and enhanced sampling flexibility.



### FACLSTM: ConvLSTM with Focused Attention for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09405v2
- **DOI**: 10.1007/s11432-019-2713-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09405v2)
- **Published**: 2019-04-20 05:44:37+00:00
- **Updated**: 2020-01-05 06:59:13+00:00
- **Authors**: Qingqing Wang, Wenjing Jia, Xiangjian He, Yue Lu, Michael Blumenstein, Ye Huang
- **Comment**: Accepted by Science China Information Science
- **Journal**: None
- **Summary**: Scene text recognition has recently been widely treated as a sequence-to-sequence prediction problem, where traditional fully-connected-LSTM (FC-LSTM) has played a critical role. Due to the limitation of FC-LSTM, existing methods have to convert 2-D feature maps into 1-D sequential feature vectors, resulting in severe damages of the valuable spatial and structural information of text images. In this paper, we argue that scene text recognition is essentially a spatiotemporal prediction problem for its 2-D image inputs, and propose a convolution LSTM (ConvLSTM)-based scene text recognizer, namely, FACLSTM, i.e., Focused Attention ConvLSTM, where the spatial correlation of pixels is fully leveraged when performing sequential prediction with LSTM. Particularly, the attention mechanism is properly incorporated into an efficient ConvLSTM structure via the convolutional operations and additional character center masks are generated to help focus attention on right feature areas. The experimental results on benchmark datasets IIIT5K, SVT and CUTE demonstrate that our proposed FACLSTM performs competitively on the regular, low-resolution and noisy text images, and outperforms the state-of-the-art approaches on the curved text with large margins.



### Funnel Transform for Straight Line Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09409v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09409v1)
- **Published**: 2019-04-20 07:30:11+00:00
- **Updated**: 2019-04-20 07:30:11+00:00
- **Authors**: QianRu Wei, DaZheng Feng, WeiXing Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the classical approaches to straight line detection only deal with a binary edge image and need to use 2D interpolation operation. This paper proposes a new transform method figuratively named as funnel transform which can efficiently and rapidly detect straight lines. The funnel transform consists of three 1D Fourier transforms and one nonlinear variable-metric transform (NVMT). It only needs to exploit 1D interpolation operation for achieving its NVMT, and can directly handle grayscale images by using its high-pass filter property, which significantly improves the performance of the closely-related approaches. Based on the slope-intercept line equation, the funnel transform can more uniformly turn the straight lines formed by ridge-typical and step-typical edges into the local maximum points (peaks). The parameters of each line can be uniquely extracted from its corresponding peak coordinates. Additionally, each peak can be theoretically specified by a 2D delta function, which makes the peaks and lines more easily identified and detected, respectively. Theoretical analysis and experimental results demonstrate that the funnel transform has advantages including smaller computational complexity, lower hardware cost, higher detection probability, greater location precision, better parallelization properties, stronger anti-occlusion and noise robustness.



### LEARNet Dynamic Imaging Network for Micro Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09410v1
- **DOI**: 10.1109/TIP.2019.2912358
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09410v1)
- **Published**: 2019-04-20 07:32:39+00:00
- **Updated**: 2019-04-20 07:32:39+00:00
- **Authors**: Monu Verma, Santosh Kumar Vipparthi, Girdhari Singh, Subrahmanyam Murala
- **Comment**: Dynamic imaging, accretion, lateral, micro expression recognition
- **Journal**: None
- **Summary**: Unlike prevalent facial expressions, micro expressions have subtle, involuntary muscle movements which are short-lived in nature. These minute muscle movements reflect true emotions of a person. Due to the short duration and low intensity, these micro-expressions are very difficult to perceive and interpret correctly. In this paper, we propose the dynamic representation of micro-expressions to preserve facial movement information of a video in a single frame. We also propose a Lateral Accretive Hybrid Network (LEARNet) to capture micro-level features of an expression in the facial region. The LEARNet refines the salient expression features in accretive manner by incorporating accretion layers (AL) in the network. The response of the AL holds the hybrid feature maps generated by prior laterally connected convolution layers. Moreover, LEARNet architecture incorporates the cross decoupled relationship between convolution layers which helps in preserving the tiny but influential facial muscle change information. The visual responses of the proposed LEARNet depict the effectiveness of the system by preserving both high- and micro-level edge features of facial expression. The effectiveness of the proposed LEARNet is evaluated on four benchmark datasets: CASME-I, CASME-II, CAS(ME)^2 and SMIC. The experimental results after investigation show a significant improvement of 4.03%, 1.90%, 1.79% and 2.82% as compared with ResNet on CASME-I, CASME-II, CAS(ME)^2 and SMIC datasets respectively.



### Cubic LSTMs for Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.09412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09412v1)
- **Published**: 2019-04-20 07:45:08+00:00
- **Updated**: 2019-04-20 07:45:08+00:00
- **Authors**: Hehe Fan, Linchao Zhu, Yi Yang
- **Comment**: Accepted to AAAI-2019
- **Journal**: None
- **Summary**: Predicting future frames in videos has become a promising direction of research for both computer vision and robot learning communities. The core of this problem involves moving object capture and future motion prediction. While object capture specifies which objects are moving in videos, motion prediction describes their future dynamics. Motivated by this analysis, we propose a Cubic Long Short-Term Memory (CubicLSTM) unit for video prediction. CubicLSTM consists of three branches, i.e., a spatial branch for capturing moving objects, a temporal branch for processing motions, and an output branch for combining the first two branches to generate predicted frames. Stacking multiple CubicLSTM units along the spatial branch and output branch, and then evolving along the temporal branch can form a cubic recurrent neural network (CubicRNN). Experiment shows that CubicRNN produces more accurate video predictions than prior methods on both synthetic and real-world datasets.



### EV-Action: Electromyography-Vision Multi-Modal Action Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.12602v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12602v2)
- **Published**: 2019-04-20 08:06:40+00:00
- **Updated**: 2020-02-26 02:37:33+00:00
- **Authors**: Lichen Wang, Bin Sun, Joseph Robinson, Taotao Jing, Yun Fu
- **Comment**: IEEE International Conference on Automatic Face & Gesture Recognition
- **Journal**: None
- **Summary**: Multi-modal human action analysis is a critical and attractive research topic. However, the majority of the existing datasets only provide visual modalities (i.e., RGB, depth and skeleton). To make up this, we introduce a new, large-scale EV-Action dataset in this work, which consists of RGB, depth, electromyography (EMG), and two skeleton modalities. Compared with the conventional datasets, EV-Action dataset has two major improvements: (1) we deploy a motion capturing system to obtain high quality skeleton modality, which provides more comprehensive motion information including skeleton, trajectory, acceleration with higher accuracy, sampling frequency, and more skeleton markers. (2) we introduce an EMG modality which is usually used as an effective indicator in the biomechanics area, also it has yet to be well explored in motion related research. To the best of our knowledge, this is the first action dataset with EMG modality. The details of EV-Action dataset are clarified, meanwhile, a simple yet effective framework for EMG-based action recognition is proposed. Moreover, state-of-the-art baselines are applied to evaluate the effectiveness of all the modalities. The obtained result clearly shows the validity of EMG modality in human action analysis tasks. We hope this dataset can make significant contributions to human motion analysis, computer vision, machine learning, biomechanics, and other interdisciplinary fields.



### Multi-modal gated recurrent units for image description
- **Arxiv ID**: http://arxiv.org/abs/1904.09421v1
- **DOI**: 10.1007/s11042-018-5856-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09421v1)
- **Published**: 2019-04-20 08:58:33+00:00
- **Updated**: 2019-04-20 08:58:33+00:00
- **Authors**: Xuelong Li, Aihong Yuan, Xiaoqiang Lu
- **Comment**: 25 pages, 7 figures, 6 tables, magazine
- **Journal**: Multi-modal gated recurrent units for image description.
  Multimedia Tools Appl. 77(22): 29847-29869 (2018)
- **Summary**: Using a natural language sentence to describe the content of an image is a challenging but very important task. It is challenging because a description must not only capture objects contained in the image and the relationships among them, but also be relevant and grammatically correct. In this paper a multi-modal embedding model based on gated recurrent units (GRU) which can generate variable-length description for a given image. In the training step, we apply the convolutional neural network (CNN) to extract the image feature. Then the feature is imported into the multi-modal GRU as well as the corresponding sentence representations. The multi-modal GRU learns the inter-modal relations between image and sentence. And in the testing step, when an image is imported to our multi-modal GRU model, a sentence which describes the image content is generated. The experimental results demonstrate that our multi-modal GRU model obtains the state-of-the-art performance on Flickr8K, Flickr30K and MS COCO datasets.



### Everyone is a Cartoonist: Selfie Cartoonization with Attentive Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.12615v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12615v1)
- **Published**: 2019-04-20 11:23:40+00:00
- **Updated**: 2019-04-20 11:23:40+00:00
- **Authors**: Xinyu Li, Wei Zhang, Tong Shen, Tao Mei
- **Comment**: None
- **Journal**: None
- **Summary**: Selfie and cartoon are two popular artistic forms that are widely presented in our daily life. Despite the great progress in image translation/stylization, few techniques focus specifically on selfie cartoonization, since cartoon images usually contain artistic abstraction (e.g., large smoothing areas) and exaggeration (e.g., large/delicate eyebrows). In this paper, we address this problem by proposing a selfie cartoonization Generative Adversarial Network (scGAN), which mainly uses an attentive adversarial network (AAN) to emphasize specific facial regions and ignore low-level details. More specifically, we first design a cycle-like architecture to enable training with unpaired data. Then we design three losses from different aspects. A total variation loss is used to highlight important edges and contents in cartoon portraits. An attentive cycle loss is added to lay more emphasis on delicate facial areas such as eyes. In addition, a perceptual loss is included to eliminate artifacts and improve robustness of our method. Experimental results show that our method is capable of generating different cartoon styles and outperforms a number of state-of-the-art methods.



### A Differential Approach for Gaze Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.09459v3
- **DOI**: 10.1109/TPAMI.2019.2957373
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09459v3)
- **Published**: 2019-04-20 15:17:45+00:00
- **Updated**: 2020-02-06 11:52:11+00:00
- **Authors**: Gang Liu, Yu Yu, Kenneth A. Funes Mora, Jean-Marc Odobez
- **Comment**: Extension to our paper A differential approach for gaze estimation
  with calibration (BMVC 2018) Submitted to PAMI on Aug. 7th, 2018 Accepted by
  PAMI short on Dec. 2019, in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: Non-invasive gaze estimation methods usually regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as biases which are subject dependent. Therefore, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to his/her actual gaze. In this paper, we introduce a novel image differential method for gaze estimation. We propose to directly train a differential convolutional neural network to predict the gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by allowing the comparison between two eye images, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or when the latter methods are followed by subject specific gaze adaptation.



### Data-Driven Neuron Allocation for Scale Aggregation Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09460v1)
- **Published**: 2019-04-20 15:18:20+00:00
- **Updated**: 2019-04-20 15:18:20+00:00
- **Authors**: Yi Li, Zhanghui Kuang, Yimin Chen, Wayne Zhang
- **Comment**: 11 pages,
- **Journal**: CVPR 2019
- **Summary**: Successful visual recognition networks benefit from aggregating information spanning from a wide range of scales. Previous research has investigated information fusion of connected layers or multiple branches in a block, seeking to strengthen the power of multi-scale representations. Despite their great successes, existing practices often allocate the neurons for each scale manually, and keep the same ratio in all aggregation blocks of an entire network, rendering suboptimal performance. In this paper, we propose to learn the neuron allocation for aggregating multi-scale information in different building blocks of a deep network. The most informative output neurons in each block are preserved while others are discarded, and thus neurons for multiple scales are competitively and adaptively allocated. Our scale aggregation network (ScaleNet) is constructed by repeating a scale aggregation (SA) block that concatenates feature maps at a wide range of scales. Feature maps for each scale are generated by a stack of downsampling, convolution and upsampling operations. The data-driven neuron allocation and SA block achieve strong representational power at the cost of considerably low computational complexity. The proposed ScaleNet, by replacing all 3x3 convolutions in ResNet with our SA blocks, achieves better performance than ResNet and its outstanding variants like ResNeXt and SE-ResNet, in the same computational complexity. On ImageNet classification, ScaleNets absolutely reduce the top-1 error rate of ResNets by 1.12 (101 layers) and 1.82 (50 layers). On COCO object detection, ScaleNets absolutely improve the mmAP with backbone of ResNets by 3.6 (101 layers) and 4.6 (50 layers) on Faster RCNN, respectively. Code and models are released at https://github.com/Eli-YiLi/ScaleNet.



### Facial Feature Embedded CycleGAN for VIS-NIR Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.09464v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09464v2)
- **Published**: 2019-04-20 15:45:29+00:00
- **Updated**: 2019-04-25 11:56:51+00:00
- **Authors**: Huijiao Wang, Li Wang, Xulei Yang, Lei Yu, Haijian Zhang
- **Comment**: reference [9] corrected; the organization of co-author Xulei Yang
  corrected;
- **Journal**: None
- **Summary**: VIS-NIR face recognition remains a challenging task due to the distinction between spectral components of two modalities and insufficient paired training data. Inspired by the CycleGAN, this paper presents a method aiming to translate VIS face images into fake NIR images whose distributions are intended to approximate those of true NIR images, which is achieved by proposing a new facial feature embedded CycleGAN. Firstly, to learn the particular feature of NIR domain while preserving common facial representation between VIS and NIR domains, we employ a general facial feature extractor (FFE) to replace the encoder in the original generator of CycleGAN. For implementing the facial feature extractor, herein the MobileFaceNet is pretrained on a VIS face database, and is able to extract effective features. Secondly, the domain-invariant feature learning is enhanced by considering a new pixel consistency loss. Lastly, we establish a new WHU VIS-NIR database which varies in face rotation and expressions to enrich the training data. Experimental results on the Oulu-CASIA NIR-VIS database and the WHU VIS-NIR database show that the proposed FFE-based CycleGAN (FFE-CycleGAN) outperforms state-of-the-art VIS-NIR face recognition methods and achieves 96.5\% accuracy.



### Saliency-Guided Attention Network for Image-Sentence Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.09471v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09471v4)
- **Published**: 2019-04-20 17:27:55+00:00
- **Updated**: 2021-04-30 10:19:50+00:00
- **Authors**: Zhong Ji, Haoran Wang, Jungong Han, Yanwei Pang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: This paper studies the task of matching image and sentence, where learning appropriate representations across the multi-modal data appears to be the main challenge. Unlike previous approaches that predominantly deploy symmetrical architecture to represent both modalities, we propose Saliency-guided Attention Network (SAN) that asymmetrically employs visual and textual attention modules to learn the fine-grained correlation intertwined between vision and language. The proposed SAN mainly includes three components: saliency detector, Saliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual Attention (STA) module. Concretely, the saliency detector provides the visual saliency information as the guidance for the two attention modules. SVA is designed to leverage the advantage of the saliency information to improve discrimination of visual representations. By fusing the visual information from SVA and textual information as a multi-modal guidance, STA learns discriminative textual representations that are highly sensitive to visual clues. Extensive experiments demonstrate SAN can substantially improve the state-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a large margin.



### ChoiceNet: CNN learning through choice of multiple feature map representations
- **Arxiv ID**: http://arxiv.org/abs/1904.09472v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09472v3)
- **Published**: 2019-04-20 17:37:46+00:00
- **Updated**: 2019-08-24 22:57:17+00:00
- **Authors**: Farshid Rayhan, Aphrodite Galata, Timothy F. Cootes
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new architecture called ChoiceNet where each layer of the network is highly connected with skip connections and channelwise concatenations. This enables the network to alleviate the problem of vanishing gradients, reduces the number of parameters without sacrificing performance, and encourages feature reuse. We evaluate our proposed architecture on three benchmark datasets for object recognition tasks (ImageNet, CIFAR- 10, CIFAR-100, SVHN) and on a semantic segmentation dataset (CamVid).



### Model-free Deep Reinforcement Learning for Urban Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1904.09503v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.09503v2)
- **Published**: 2019-04-20 22:02:45+00:00
- **Updated**: 2019-10-21 21:11:58+00:00
- **Authors**: Jianyu Chen, Bodi Yuan, Masayoshi Tomizuka
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: Urban autonomous driving decision making is challenging due to complex road geometry and multi-agent interactions. Current decision making methods are mostly manually designing the driving policy, which might result in sub-optimal solutions and is expensive to develop, generalize and maintain at scale. On the other hand, with reinforcement learning (RL), a policy can be learned and improved automatically without any manual designs. However, current RL methods generally do not work well on complex urban scenarios. In this paper, we propose a framework to enable model-free deep reinforcement learning in challenging urban autonomous driving scenarios. We design a specific input representation and use visual encoding to capture the low-dimensional latent states. Several state-of-the-art model-free deep RL algorithms are implemented into our framework, with several tricks to improve their performance. We evaluate our method in a challenging roundabout task with dense surrounding vehicles in a high-definition driving simulator. The result shows that our method can solve the task well and is significantly better than the baseline.



### Social Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs
- **Arxiv ID**: http://arxiv.org/abs/1904.09507v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09507v2)
- **Published**: 2019-04-20 22:11:56+00:00
- **Updated**: 2019-04-24 12:25:07+00:00
- **Authors**: Javad Amirian, Jean-Bernard Hayet, Julien Pettre
- **Comment**: Accepted at CVPR Workshops 2019
- **Journal**: None
- **Summary**: This paper proposes a novel approach for predicting the motion of pedestrians interacting with others. It uses a Generative Adversarial Network (GAN) to sample plausible predictions for any agent in the scene. As GANs are very susceptible to mode collapsing and dropping, we show that the recently proposed Info-GAN allows dramatic improvements in multi-modal pedestrian trajectory prediction to avoid these issues. We also left out L2-loss in training the generator, unlike some previous works, because it causes serious mode collapsing though faster convergence.   We show through experiments on real and synthetic data that the proposed method leads to generate more diverse samples and to preserve the modes of the predictive distribution. In particular, to prove this claim, we have designed a toy example dataset of trajectories that can be used to assess the performance of different methods in preserving the predictive distribution modes.



