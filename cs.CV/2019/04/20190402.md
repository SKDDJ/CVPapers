# Arxiv Papers in cs.CV on 2019-04-02
### Thickened 2D Networks for Efficient 3D Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.01150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01150v2)
- **Published**: 2019-04-02 00:08:51+00:00
- **Updated**: 2019-11-23 01:29:28+00:00
- **Authors**: Qihang Yu, Yingda Xia, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille
- **Comment**: None
- **Journal**: None
- **Summary**: There has been a debate in 3D medical image segmentation on whether to use 2D or 3D networks, where both pipelines have advantages and disadvantages. 2D methods enjoy a low inference time and greater transfer-ability while 3D methods are superior in performance for hard targets requiring contextual information. This paper investigates efficient 3D segmentation from another perspective, which uses 2D networks to mimic 3D segmentation. To compensate the lack of contextual information in 2D manner, we propose to thicken the 2D network inputs by feeding multiple slices as multiple channels into 2D networks and thus 3D contextual information is incorporated. We also put forward to use early-stage multiplexing and slice sensitive attention to solve the confusion problem of information loss which occurs when 2D networks face thickened inputs. With this design, we achieve a higher performance while maintaining a lower inference latency on a few abdominal organs from CT scans, in particular when the organ has a peculiar 3D shape and thus strongly requires contextual information, demonstrating our method's effectiveness and ability in capturing 3D information. We also point out that "thickened" 2D inputs pave a new method of 3D segmentation, and look forward to more efforts in this direction. Experiments on segmenting a few abdominal targets in particular blood vessels which require strong 3D contexts demonstrate the advantages of our approach.



### Curls & Whey: Boosting Black-Box Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1904.01160v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01160v1)
- **Published**: 2019-04-02 01:16:01+00:00
- **Updated**: 2019-04-02 01:16:01+00:00
- **Authors**: Yucheng Shi, Siyu Wang, Yahong Han
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: Image classifiers based on deep neural networks suffer from harassment caused by adversarial examples. Two defects exist in black-box iterative attacks that generate adversarial examples by incrementally adjusting the noise-adding direction for each step. On the one hand, existing iterative attacks add noises monotonically along the direction of gradient ascent, resulting in a lack of diversity and adaptability of the generated iterative trajectories. On the other hand, it is trivial to perform adversarial attack by adding excessive noises, but currently there is no refinement mechanism to squeeze redundant noises. In this work, we propose Curls & Whey black-box attack to fix the above two defects. During Curls iteration, by combining gradient ascent and descent, we `curl' up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks. The Whey optimization further squeezes the `whey' of noises by exploiting the robustness of adversarial perturbation. Extensive experiments on Imagenet and Tiny-Imagenet demonstrate that our approach achieves impressive decrease on noise magnitude in l2 norm. Curls & Whey attack also shows promising transferability against ensemble models as well as adversarially trained models. In addition, we extend our attack to the targeted misclassification, effectively reducing the difficulty of targeted attacks under black-box condition.



### X-Ray CT Reconstruction of Additively Manufactured Parts using 2.5D Deep Learning MBIR
- **Arxiv ID**: http://arxiv.org/abs/1904.12585v2
- **DOI**: 10.1017/S1431927619002617
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12585v2)
- **Published**: 2019-04-02 01:16:29+00:00
- **Updated**: 2019-05-06 10:45:34+00:00
- **Authors**: Amirkoushyar Ziabari, Michael Kirka, Vincent Paquit, Philip Bingham, Singanallur Venkatakrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a deep learning algorithm to rapidly obtain high quality CT reconstructions for AM parts. In particular, we propose to use CAD models of the parts that are to be manufactured, introduce typical defects and simulate XCT measurements. These simulated measurements were processed using FBP (computationally simple but result in noisy images) and the MBIR technique. We then train a 2.5D deep convolutional neural network [4], deemed 2.5D Deep Learning MBIR (2.5D DL-MBIR), on these pairs of noisy and high-quality 3D volumes to learn a fast, non-linear mapping function. The 2.5D DL-MBIR reconstructs a 3D volume in a 2.5D scheme where each slice is reconstructed from multiple inputs slices of the FBP input. Given this trained system, we can take a small set of measurements on an actual part, process it using a combination of FBP followed by 2.5D DL-MBIR. Both steps can be rapidly performed using GPUs, resulting in a real-time algorithm that achieves the high-quality of MBIR as fast as standard techniques. Intuitively, since CAD models are typically available for parts to be manufactured, this provides a strong constraint "prior" which can be leveraged to improve the reconstruction.



### Res2Net: A New Multi-scale Backbone Architecture
- **Arxiv ID**: http://arxiv.org/abs/1904.01169v3
- **DOI**: 10.1109/TPAMI.2019.2938758
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01169v3)
- **Published**: 2019-04-02 01:56:34+00:00
- **Updated**: 2021-01-27 09:55:20+00:00
- **Authors**: Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, Philip Torr
- **Comment**: 11 pages, 7 figures
- **Journal**: IEEE TPAMI 2021
- **Summary**: Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.



### DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/1904.01175v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.01175v1)
- **Published**: 2019-04-02 02:15:09+00:00
- **Updated**: 2019-04-02 02:15:09+00:00
- **Authors**: Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, Paul Debevec
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based method to infer plausible high dynamic range (HDR), omnidirectional illumination given an unconstrained, low dynamic range (LDR) image from a mobile phone camera with a limited field of view (FOV). For training data, we collect videos of various reflective spheres placed within the camera's FOV, leaving most of the background unoccluded, leveraging that materials with diverse reflectance functions reveal different lighting cues in a single exposure. We train a deep neural network to regress from the LDR background image to HDR lighting by matching the LDR ground truth sphere images to those rendered with the predicted illumination using image-based relighting, which is differentiable. Our inference runs at interactive frame rates on a mobile device, enabling realistic rendering of virtual objects into real scenes for mobile mixed reality. Training on automatically exposed and white-balanced videos, we improve the realism of rendered objects compared to the state-of-the art methods for both indoor and outdoor scenes.



### Person Identification with Visual Summary for a Safe Access to a Smart Home
- **Arxiv ID**: http://arxiv.org/abs/1904.01178v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01178v2)
- **Published**: 2019-04-02 02:25:31+00:00
- **Updated**: 2019-04-18 23:24:00+00:00
- **Authors**: Shahinur Alam, Mohammed Yeasin
- **Comment**: None
- **Journal**: None
- **Summary**: SafeAccess is an integrated system designed to provide easier and safer access to a smart home for people with or without disabilities. The system is designed to enhance safety and promote the independence of people with disability (i.e., visually impaired). The key functionality of the system includes the detection and identification of human and generating contextual visual summary from the real-time video streams obtained from the cameras placed in strategic locations around the house. In addition, the system classifies human into groups (i.e. friends/families/caregiver versus intruders/burglars/unknown). These features allow the user to grant/deny remote access to the premises or ability to call emergency services. In this paper, we focus on designing a prototype system for the smart home and building a robust recognition engine that meets the system criteria and addresses speed, accuracy, deployment and environmental challenges under a wide variety of practical and real-life situations. To interact with the system, we implemented a dialog enabled interface to create a personalized profile using face images or video of friend/families/caregiver. To improve computational efficiency, we apply change detection to filter out frames and use Faster-RCNN to detect the human presence and extract faces using Multitask Cascaded Convolutional Networks (MTCNN). Subsequently, we apply LBP/FaceNet to identify a person and groups by matching extracted faces with the profile. SafeAccess sends a visual summary to the users with an MMS containing a person's name if any match found or as "Unknown", scene image, facial description, and contextual information. SafeAccess identifies friends/families/caregiver versus intruders/unknown with an average F-score 0.97 and generates a visual summary from 10 classes with an average accuracy of 98.01%.



### Data-Free Learning of Student Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01186v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01186v4)
- **Published**: 2019-04-02 03:00:06+00:00
- **Updated**: 2019-12-31 06:58:35+00:00
- **Authors**: Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, Qi Tian
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DAFL) method achieve 92.22% and 74.47% accuracies using ResNet-18 without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56% accuracy on the CelebA benchmark.



### Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01189v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01189v3)
- **Published**: 2019-04-02 03:08:36+00:00
- **Updated**: 2020-04-17 07:27:14+00:00
- **Authors**: Pengfei Zhang, Cuiling Lan, Wenjun Zeng, Junliang Xing, Jianru Xue, Nanning Zheng
- **Comment**: Accepted by CVPR2020. The source code is available at
  https://github.com/microsoft/SGN
- **Journal**: None
- **Summary**: Skeleton-based human action recognition has attracted great interest thanks to the easy accessibility of the human skeleton data. Recently, there is a trend of using very deep feedforward neural networks to model the 3D coordinates of joints without considering the computational efficiency. In this paper, we propose a simple yet effective semantics-guided neural network (SGN) for skeleton-based action recognition. We explicitly introduce the high level semantics of joints (joint type and frame index) into the network to enhance the feature representation capability. In addition, we exploit the relationship of joints hierarchically through two modules, i.e., a joint-level module for modeling the correlations of joints in the same frame and a framelevel module for modeling the dependencies of frames by taking the joints in the same frame as a whole. A strong baseline is proposed to facilitate the study of this field. With an order of magnitude smaller model size than most previous works, SGN achieves the state-of-the-art performance on the NTU60, NTU120, and SYSU datasets. The source code is available at https://github.com/microsoft/SGN.



### C2AE: Class Conditioned Auto-Encoder for Open-set Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01198v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01198v1)
- **Published**: 2019-04-02 03:47:39+00:00
- **Updated**: 2019-04-02 03:47:39+00:00
- **Authors**: Poojan Oza, Vishal M Patel
- **Comment**: CVPR2019 (Oral)
- **Journal**: None
- **Summary**: Models trained for classification often assume that all testing classes are known while training. As a result, when presented with an unknown class during testing, such closed-set assumption forces the model to classify it as one of the known classes. However, in a real world scenario, classification models are likely to encounter such examples. Hence, identifying those examples as unknown becomes critical to model performance. A potential solution to overcome this problem lies in a class of learning problems known as open-set recognition. It refers to the problem of identifying the unknown classes during testing, while maintaining performance on the known classes. In this paper, we propose an open-set recognition algorithm using class conditioned auto-encoders with novel training and testing methodology. In contrast to previous methods, training procedure is divided in two sub-tasks, 1. closed-set classification and, 2. open-set identification (i.e. identifying a class as known or unknown). Encoder learns the first task following the closed-set classification training pipeline, whereas decoder learns the second task by reconstructing conditioned on class identity. Furthermore, we model reconstruction errors using the Extreme Value Theory of statistical modeling to find the threshold for identifying known/unknown class samples. Experiments performed on multiple image classification datasets show proposed method performs significantly better than state of the art.



### Habitat: A Platform for Embodied AI Research
- **Arxiv ID**: http://arxiv.org/abs/1904.01201v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.01201v2)
- **Published**: 2019-04-02 03:52:27+00:00
- **Updated**: 2019-11-25 01:39:04+00:00
- **Authors**: Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.   These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.



### Peak Alignment of Gas Chromatography-Mass Spectrometry Data with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01205v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01205v3)
- **Published**: 2019-04-02 04:16:45+00:00
- **Updated**: 2019-08-20 01:21:59+00:00
- **Authors**: Mike Li, X. Rosalind Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present ChromAlignNet, a deep learning model for alignment of peaks in Gas Chromatography-Mass Spectrometry (GC-MS) data. In GC-MS data, a compound's retention time (RT) may not stay fixed across multiple chromatograms. To use GC-MS data for biomarker discovery requires alignment of identical analyte's RT from different samples. Current methods of alignment are all based on a set of formal, mathematical rules. We present a solution to GC-MS alignment using deep learning neural networks, which are more adept at complex, fuzzy data sets. We tested our model on several GC-MS data sets of various complexities and analysed the alignment results quantitatively. We show the model has very good performance (AUC $\sim 1$ for simple data sets and AUC $\sim 0.85$ for very complex data sets). Further, our model easily outperforms existing algorithms on complex data sets. Compared with existing methods, ChromAlignNet is very easy to use as it requires no user input of reference chromatograms and parameters. This method can easily be adapted to other similar data such as those from liquid chromatography. The source code is written in Python and available online.



### Progressive LiDAR Adaptation for Road Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.01206v1
- **DOI**: 10.1109/JAS.2019.1911459
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01206v1)
- **Published**: 2019-04-02 04:22:23+00:00
- **Updated**: 2019-04-02 04:22:23+00:00
- **Authors**: Zhe Chen, Jing Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Despite rapid developments in visual image-based road detection, robustly identifying road areas in visual images remains challenging due to issues like illumination changes and blurry images. To this end, LiDAR sensor data can be incorporated to improve the visual image-based road detection, because LiDAR data is less susceptible to visual noises. However, the main difficulty in introducing LiDAR information into visual image-based road detection is that LiDAR data and its extracted features do not share the same space with the visual data and visual features. Such gaps in spaces may limit the benefits of LiDAR information for road detection. To overcome this issue, we introduce a novel Progressive LiDAR Adaptation-aided Road Detection (PLARD) approach to adapt LiDAR information into visual image-based road detection and improve detection performance. In PLARD, progressive LiDAR adaptation consists of two subsequent modules: 1) data space adaptation, which transforms the LiDAR data to the visual data space to align with the perspective view by applying altitude difference-based transformation; and 2) feature space adaptation, which adapts LiDAR features to visual features through a cascaded fusion structure. Comprehensive empirical studies on the well-known KITTI road detection benchmark demonstrate that PLARD takes advantage of both the visual and LiDAR information, achieving much more robust road detection even in challenging urban scenes. In particular, PLARD outperforms other state-of-the-art road detection models and is currently top of the publicly accessible benchmark leader-board.



### DSAL-GAN: Denoising based Saliency Prediction with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01215v1)
- **Published**: 2019-04-02 04:56:57+00:00
- **Updated**: 2019-04-02 04:56:57+00:00
- **Authors**: Prerana Mukherjee, Manoj Sharma, Megh Makwana, Ajay Pratap Singh, Avinash Upadhyay, Akkshita Trivedi, Brejesh Lall, Santanu Chaudhury
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesizing high quality saliency maps from noisy images is a challenging problem in computer vision and has many practical applications. Samples generated by existing techniques for saliency detection cannot handle the noise perturbations smoothly and fail to delineate the salient objects present in the given scene. In this paper, we present a novel end-to-end coupled Denoising based Saliency Prediction with Generative Adversarial Network (DSAL-GAN) framework to address the problem of salient object detection in noisy images. DSAL-GAN consists of two generative adversarial-networks (GAN) trained end-to-end to perform denoising and saliency prediction altogether in a holistic manner. The first GAN consists of a generator which denoises the noisy input image, and in the discriminator counterpart we check whether the output is a denoised image or ground truth original image. The second GAN predicts the saliency maps from raw pixels of the input denoised image using a data-driven metric based on saliency prediction method with adversarial loss. Cycle consistency loss is also incorporated to further improve salient region prediction. We demonstrate with comprehensive evaluation that the proposed framework outperforms several baseline saliency models on various performance benchmarks.



### Deep Learning for Face Recognition: Pride or Prejudiced?
- **Arxiv ID**: http://arxiv.org/abs/1904.01219v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01219v2)
- **Published**: 2019-04-02 05:14:58+00:00
- **Updated**: 2019-06-19 18:34:16+00:00
- **Authors**: Shruti Nagpal, Maneet Singh, Richa Singh, Mayank Vatsa
- **Comment**: None
- **Journal**: None
- **Summary**: Do very high accuracies of deep networks suggest pride of effective AI or are deep networks prejudiced? Do they suffer from in-group biases (own-race-bias and own-age-bias), and mimic the human behavior? Is in-group specific information being encoded sub-consciously by the deep networks?   This research attempts to answer these questions and presents an in-depth analysis of `bias' in deep learning based face recognition systems. This is the first work which decodes if and where bias is encoded for face recognition. Taking cues from cognitive studies, we inspect if deep networks are also affected by social in- and out-group effect. Networks are analyzed for own-race and own-age bias, both of which have been well established in human beings. The sub-conscious behavior of face recognition models is examined to understand if they encode race or age specific features for face recognition. Analysis is performed based on 36 experiments conducted on multiple datasets. Four deep learning networks either trained from scratch or pre-trained on over 10M images are used. Variations across class activation maps and feature visualizations provide novel insights into the functioning of deep learning systems, suggesting behavior similar to humans. It is our belief that a better understanding of state-of-the-art deep learning networks would enable researchers to address the given challenge of bias in AI, and develop fairer systems.



### Adversarial Attacks against Deep Saliency Models
- **Arxiv ID**: http://arxiv.org/abs/1904.01231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01231v1)
- **Published**: 2019-04-02 06:32:52+00:00
- **Updated**: 2019-04-02 06:32:52+00:00
- **Authors**: Zhaohui Che, Ali Borji, Guangtao Zhai, Suiyi Ling, Guodong Guo, Patrick Le Callet
- **Comment**: None
- **Journal**: None
- **Summary**: Currently, a plethora of saliency models based on deep neural networks have led great breakthroughs in many complex high-level vision tasks (e.g. scene description, object detection). The robustness of these models, however, has not yet been studied. In this paper, we propose a sparse feature-space adversarial attack method against deep saliency models for the first time. The proposed attack only requires a part of the model information, and is able to generate a sparser and more insidious adversarial perturbation, compared to traditional image-space attacks. These adversarial perturbations are so subtle that a human observer cannot notice their presences, but the model outputs will be revolutionized. This phenomenon raises security threats to deep saliency models in practical applications. We also explore some intriguing properties of the feature-space attack, e.g. 1) the hidden layers with bigger receptive fields generate sparser perturbations, 2) the deeper hidden layers achieve higher attack success rates, and 3) different loss functions and different attacked layers will result in diverse perturbations. Experiments indicate that the proposed method is able to successfully attack different model architectures across various image scenes.



### Centerline Depth World Reinforcement Learning-based Left Atrial Appendage Orifice Localization
- **Arxiv ID**: http://arxiv.org/abs/1904.01241v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 14J60
- **Links**: [PDF](http://arxiv.org/pdf/1904.01241v2)
- **Published**: 2019-04-02 06:56:11+00:00
- **Updated**: 2020-12-18 01:28:42+00:00
- **Authors**: Walid Abdullah Al, Il Dong Yun, Eun Ju Chun
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Left atrial appendage (LAA) closure (LAAC) is a minimally invasive implant-based method to prevent cardiovascular stroke in patients with non-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT angiography plays a crucial role in choosing an appropriate LAAC implant size and a proper C-arm angulation. However, accurate orifice localization is hard because of the high anatomic variation of LAA, and unclear position and orientation of the orifice in available CT views. Deep localization models also yield high error in localizing the orifice in CT image because of the tiny structure of orifice compared to the vastness of CT image. In this paper, we propose a centerline depth-based reinforcement learning (RL) world for effective orifice localization in a small search space. In our scheme, an RL agent observes the centerline-to-surface distance and navigates through the LAA centerline to localize the orifice. Thus, the search space is significantly reduced facilitating improved localization. The proposed formulation could result in high localization accuracy comparing to the expert-annotations in 98 CT images. Moreover, the localization process takes about 8 seconds which is 18 times more efficient than the existing method. Therefore, this can be a useful aid to physicians during the preprocedural planning of LAAC.



### Metric-Learning based Deep Hashing Network for Content Based Retrieval of Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1904.01258v3
- **DOI**: 10.1109/LGRS.2020.2974629
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01258v3)
- **Published**: 2019-04-02 07:43:01+00:00
- **Updated**: 2021-01-06 09:43:36+00:00
- **Authors**: Subhankar Roy, Enver Sangineto, Begüm Demir, Nicu Sebe
- **Comment**: Accepted to IEEE Geoscience and Remote Sensing Letters. For code
  visit: https://github.com/MLEnthusiast/MHCLN
- **Journal**: None
- **Summary**: Hashing methods have been recently found very effective in retrieval of remote sensing (RS) images due to their computational efficiency and fast search speed. The traditional hashing methods in RS usually exploit hand-crafted features to learn hash functions to obtain binary codes, which can be insufficient to optimally represent the information content of RS images. To overcome this problem, in this paper we introduce a metric-learning based hashing network, which learns: 1) a semantic-based metric space for effective feature representation; and 2) compact binary hash codes for fast archive search. Our network considers an interplay of multiple loss functions that allows to jointly learn a metric based semantic space facilitating similar images to be clustered together in that target space and at the same time producing compact final activations that lose negligible information when binarized. Experiments carried out on two benchmark RS archives point out that the proposed network significantly improves the retrieval performance under the same retrieval time when compared to the state-of-the-art hashing methods in RS.



### Hierarchical method for cataract grading based on retinal images using improved Haar wavelet
- **Arxiv ID**: http://arxiv.org/abs/1904.01261v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01261v1)
- **Published**: 2019-04-02 07:47:53+00:00
- **Updated**: 2019-04-02 07:47:53+00:00
- **Authors**: Lvchen Cao, Huiqi Li, Yanjun Zhang, Liang Xu, Li Zhang
- **Comment**: Under Review by Information Fusion (Elsevier)
- **Journal**: None
- **Summary**: Cataracts, which are lenticular opacities that may occur at different lens locations, are the leading cause of visual impairment worldwide. Accurate and timely diagnosis can improve the quality of life of cataract patients. In this paper, a feature extraction-based method for grading cataract severity using retinal images is proposed. To obtain more appropriate features for the automatic grading, the Haar wavelet is improved according to the characteristics of retinal images. Retinal images of non-cataract, as well as mild, moderate, and severe cataracts, are automatically recognized using the improved Haar wavelet. A hierarchical strategy is used to transform the four-class classification problem into three adjacent two-class classification problems. Three sets of two-class classifiers based on a neural network are trained individually and integrated together to establish a complete classification system. The accuracies of the two-class classification (cataract and non-cataract) and four-class classification are 94.83% and 85.98%, respectively. The performance analysis demonstrates that the improved Haar wavelet feature achieves higher accuracy than the original Haar wavelet feature, and the fusion of three sets of two-class classifiers is superior to a simple four-class classifier. The discussion indicates that the retinal image-based method offers significant potential for cataract detection.



### A PCA-like Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1904.01277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01277v1)
- **Published**: 2019-04-02 08:27:52+00:00
- **Updated**: 2019-04-02 08:27:52+00:00
- **Authors**: Saïd Ladjal, Alasdair Newson, Chi-Hieu Pham
- **Comment**: None
- **Journal**: None
- **Summary**: An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.



### FKIMNet: A Finger Dorsal Image Matching Network Comparing Component (Major, Minor and Nail) Matching with Holistic (Finger Dorsal) Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.01289v1
- **DOI**: 10.1109/IJCNN.2019.8852390
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01289v1)
- **Published**: 2019-04-02 08:47:16+00:00
- **Updated**: 2019-04-02 08:47:16+00:00
- **Authors**: Daksh Thapar, Gaurav Jaswal, Aditya Nigam
- **Comment**: Accepted in IJCNN 2019
- **Journal**: None
- **Summary**: Current finger knuckle image recognition systems, often require users to place fingers' major or minor joints flatly towards the capturing sensor. To extend these systems for user non-intrusive application scenarios, such as consumer electronics, forensic, defence etc, we suggest matching the full dorsal fingers, rather than the major/ minor region of interest (ROI) alone. In particular, this paper makes a comprehensive study on the comparisons between full finger and fusion of finger ROI's for finger knuckle image recognition. These experiments suggest that using full-finger, provides a more elegant solution. Addressing the finger matching problem, we propose a CNN (convolutional neural network) which creates a $128$-D feature embedding of an image. It is trained via. triplet loss function, which enforces the L2 distance between the embeddings of the same subject to be approaching zero, whereas the distance between any 2 embeddings of different subjects to be at least a margin. For precise training of the network, we use dynamic adaptive margin, data augmentation, and hard negative mining. In distinguished experiments, the individual performance of finger, as well as weighted sum score level fusion of major knuckle, minor knuckle, and nail modalities have been computed, justifying our assumption to consider full finger as biometrics instead of its counterparts. The proposed method is evaluated using two publicly available finger knuckle image datasets i.e., PolyU FKP dataset and PolyU Contactless FKI Datasets.



### Event-Based Motion Segmentation by Motion Compensation
- **Arxiv ID**: http://arxiv.org/abs/1904.01293v4
- **DOI**: 10.1109/ICCV.2019.00734
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01293v4)
- **Published**: 2019-04-02 08:51:01+00:00
- **Updated**: 2019-08-22 23:15:45+00:00
- **Authors**: Timo Stoffregen, Guillermo Gallego, Tom Drummond, Lindsay Kleeman, Davide Scaramuzza
- **Comment**: When viewed in Acrobat Reader, several of the figures animate. Video:
  https://youtu.be/0q6ap_OSBAk
- **Journal**: IEEE International Conference on Computer Vision 2019
- **Summary**: In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called "events"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement.



### CANU-ReID: A Conditional Adversarial Network for Unsupervised person Re-IDentification
- **Arxiv ID**: http://arxiv.org/abs/1904.01308v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01308v2)
- **Published**: 2019-04-02 09:35:15+00:00
- **Updated**: 2020-04-28 09:40:19+00:00
- **Authors**: Guillaume Delorme, Yihong Xu, Stephane Lathuilière, Radu Horaud, Xavier Alameda-Pineda
- **Comment**: None
- **Journal**: None
- **Summary**: Unsupervised person re-ID is the task of identifying people on a target data set for which the ID labels are unavailable during training. In this paper, we propose to unify two trends in unsupervised person re-ID: clustering & fine-tuning and adversarial learning. On one side, clustering groups training images into pseudo-ID labels, and uses them to fine-tune the feature extractor. On the other side, adversarial learning is used, inspired by domain adaptation, to match distributions from different domains. Since target data is distributed across different camera viewpoints, we propose to model each camera as an independent domain, and aim to learn domain-independent features. Straightforward adversarial learning yields negative transfer, we thus introduce a conditioning vector to mitigate this undesirable effect. In our framework, the centroid of the cluster to which the visual sample belongs is used as conditioning vector of our conditional adversarial network, where the vector is permutation invariant (clusters ordering does not matter) and its size is independent of the number of clusters. To our knowledge, we are the first to propose the use of conditional adversarial networks for unsupervised person re-ID. We evaluate the proposed architecture on top of two state-of-the-art clustering-based unsupervised person re-identification (re-ID) methods on four different experimental settings with three different data sets and set the new state-of-the-art performance on all four of them. Our code and model will be made publicly available at https://team.inria.fr/perception/canu-reid/.



### DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.01310v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01310v1)
- **Published**: 2019-04-02 09:43:23+00:00
- **Updated**: 2019-04-02 09:43:23+00:00
- **Authors**: Minfeng Zhu, Pingbo Pan, Wei Chen, Yi Yang
- **Comment**: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)
- **Journal**: None
- **Summary**: In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.



### Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents
- **Arxiv ID**: http://arxiv.org/abs/1904.01318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01318v1)
- **Published**: 2019-04-02 10:21:23+00:00
- **Updated**: 2019-04-02 10:21:23+00:00
- **Authors**: Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal
- **Comment**: None
- **Journal**: None
- **Summary**: As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.



### Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking
- **Arxiv ID**: http://arxiv.org/abs/1904.01324v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01324v2)
- **Published**: 2019-04-02 10:35:14+00:00
- **Updated**: 2019-08-21 08:19:13+00:00
- **Authors**: Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal, Abhishek Sharma, Arjun Jain
- **Comment**: In Proceedings of the 2019 IEEE International Conference on Computer
  Vision (ICCV)
- **Journal**: None
- **Summary**: Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ssfootball04/generative_pose.



### HoloGAN: Unsupervised learning of 3D representations from natural images
- **Arxiv ID**: http://arxiv.org/abs/1904.01326v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01326v2)
- **Published**: 2019-04-02 10:36:01+00:00
- **Updated**: 2019-10-01 10:41:28+00:00
- **Authors**: Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, Yong-Liang Yang
- **Comment**: International Conference on Computer Vision ICCV 2019. For project
  page, see
  https://www.monkeyoverflow.com/#/hologan-unsupervised-learning-of-3d-representations-from-natural-images/
- **Journal**: None
- **Summary**: We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.



### Point in, Box out: Beyond Counting Persons in Crowds
- **Arxiv ID**: http://arxiv.org/abs/1904.01333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01333v2)
- **Published**: 2019-04-02 11:03:32+00:00
- **Updated**: 2019-04-03 12:23:37+00:00
- **Authors**: Yuting Liu, Miaojing Shi, Qijun Zhao, Xiaofang Wang
- **Comment**: Accepted by CVPR2019(Oral)
- **Journal**: None
- **Summary**: Modern crowd counting methods usually employ deep neural networks (DNN) to estimate crowd counts via density regression. Despite their significant improvements, the regression-based methods are incapable of providing the detection of individuals in crowds. The detection-based methods, on the other hand, have not been largely explored in recent trends of crowd counting due to the needs for expensive bounding box annotations. In this work, we instead propose a new deep detection network with only point supervision required. It can simultaneously detect the size and location of human heads and count them in crowds. We first mine useful person size information from point-level annotations and initialize the pseudo ground truth bounding boxes. An online updating scheme is introduced to refine the pseudo ground truth during training; while a locally-constrained regression loss is designed to provide additional constraints on the size of the predicted boxes in a local neighborhood. In the end, we propose a curriculum learning strategy to train the network from images of relatively accurate and easy pseudo ground truth first. Extensive experiments are conducted in both detection and counting tasks on several standard benchmarks, e.g. ShanghaiTech, UCF_CC_50, WiderFace, and TRANCOS datasets, and the results show the superiority of our method over the state-of-the-art.



### Correlated Parameters to Accurately Measure Uncertainty in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01334v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01334v1)
- **Published**: 2019-04-02 11:06:50+00:00
- **Updated**: 2019-04-02 11:06:50+00:00
- **Authors**: Konstantin Posch, Jürgen Pilz
- **Comment**: None
- **Journal**: None
- **Summary**: In this article a novel approach for training deep neural networks using Bayesian techniques is presented. The Bayesian methodology allows for an easy evaluation of model uncertainty and additionally is robust to overfitting. These are commonly the two main problems classical, i.e. non-Bayesian, architectures have to struggle with. The proposed approach applies variational inference in order to approximate the intractable posterior distribution. In particular, the variational distribution is defined as product of multiple multivariate normal distributions with tridiagonal covariance matrices. Each single normal distribution belongs either to the weights, or to the biases corresponding to one network layer. The layer-wise a posteriori variances are defined based on the corresponding expectation values and further the correlations are assumed to be identical. Therefore, only a few additional parameters need to be optimized compared to non-Bayesian settings. The novel approach is successfully evaluated on basis of the popular benchmark datasets MNIST and CIFAR-10.



### Looking back at Labels: A Class based Domain Adaptation Technique
- **Arxiv ID**: http://arxiv.org/abs/1904.01341v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01341v1)
- **Published**: 2019-04-02 11:28:19+00:00
- **Updated**: 2019-04-02 11:28:19+00:00
- **Authors**: Vinod Kumar Kurmi, Vinay P. Namboodiri
- **Comment**: IJCNN 2019 Accepted
- **Journal**: None
- **Summary**: In this paper, we solve the problem of adapting classifiers across domains. We consider the problem of domain adaptation for multi-class classification where we are provided a labeled set of examples in a source dataset and we are provided a target dataset with no supervision. In this setting, we propose an adversarial discriminator based approach. While the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. Our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structure adapted space. Using this formulation, we obtain state-of-the-art results for the standard evaluation on benchmark datasets. We further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation.



### FCOS: Fully Convolutional One-Stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.01355v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01355v5)
- **Published**: 2019-04-02 11:56:36+00:00
- **Updated**: 2019-08-20 11:26:21+00:00
- **Authors**: Zhi Tian, Chunhua Shen, Hao Chen, Tong He
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision 2019. 13 pages. Code is
  available at: https://github.com/tianzhi0549/FCOS/
- **Journal**: None
- **Summary**: We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1



### Aiding Intra-Text Representations with Visual Context for Multimodal Named Entity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01356v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1904.01356v1)
- **Published**: 2019-04-02 11:57:40+00:00
- **Updated**: 2019-04-02 11:57:40+00:00
- **Authors**: Omer Arshad, Ignazio Gallo, Shah Nawaz, Alessandro Calefati
- **Comment**: None
- **Journal**: None
- **Summary**: With massive explosion of social media such as Twitter and Instagram, people daily share billions of multimedia posts, containing images and text. Typically, text in these posts is short, informal and noisy, leading to ambiguities which can be resolved using images. In this paper we explore text-centric Named Entity Recognition task on these multimedia posts. We propose an end to end model which learns a joint representation of a text and an image. Our model extends multi-dimensional self attention technique, where now image helps to enhance relationship between words. Experiments show that our model is capable of capturing both textual and visual contexts with greater accuracy, achieving state-of-the-art results on Twitter multimodal Named Entity Recognition dataset.



### Fast Bayesian Restoration of Poisson Corrupted Images with INLA
- **Arxiv ID**: http://arxiv.org/abs/1904.01357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01357v1)
- **Published**: 2019-04-02 12:05:40+00:00
- **Updated**: 2019-04-02 12:05:40+00:00
- **Authors**: Takahiro Kawashima, Hayaru Shouno
- **Comment**: 6 pages, 6 figures
- **Journal**: None
- **Summary**: Photon-limited images are often seen in fields such as medical imaging. Although the number of collected photons on an image sensor statistically follows Poisson distribution, this type of noise is intractable, unlike Gaussian noise. In this study, we propose a Bayesian restoration method of Poisson corrupted image using Integrated Nested Laplace Approximation (INLA), which is a computational method to evaluate marginalized posterior distributions of latent Gaussian models (LGMs). When the original image can be regarded as ICAR (intrinsic conditional auto-regressive) model reasonably, our method performs very faster than well-known ones such as loopy belief propagation-based method and Markov chain Monte Carlo (MCMC) without decreasing the accuracy.



### A Benchmark for Edge-Preserving Image Smoothing
- **Arxiv ID**: http://arxiv.org/abs/1904.01579v1
- **DOI**: 10.1109/TIP.2019.2908778
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01579v1)
- **Published**: 2019-04-02 12:19:57+00:00
- **Updated**: 2019-04-02 12:19:57+00:00
- **Authors**: Feida Zhu, Zhetong Liang, Xixi Jia, Lei Zhang, Yizhou Yu
- **Comment**: Accepted by IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Edge-preserving image smoothing is an important step for many low-level vision problems. Though many algorithms have been proposed, there are several difficulties hindering its further development. First, most existing algorithms cannot perform well on a wide range of image contents using a single parameter setting. Second, the performance evaluation of edge-preserving image smoothing remains subjective, and there lacks a widely accepted datasets to objectively compare the different algorithms. To address these issues and further advance the state of the art, in this work we propose a benchmark for edge-preserving image smoothing. This benchmark includes an image dataset with groundtruth image smoothing results as well as baseline algorithms that can generate competitive edge-preserving smoothing results for a wide range of image contents. The established dataset contains 500 training and testing images with a number of representative visual object categories, while the baseline methods in our benchmark are built upon representative deep convolutional network architectures, on top of which we design novel loss functions well suited for edge-preserving image smoothing. The trained deep networks run faster than most state-of-the-art smoothing algorithms with leading smoothing results both qualitatively and quantitatively. The benchmark is publicly accessible via https://github.com/zhufeida/Benchmark_EPS.



### A Holistic Representation Guided Attention Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01375v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01375v5)
- **Published**: 2019-04-02 12:43:29+00:00
- **Updated**: 2021-03-30 07:16:50+00:00
- **Authors**: Lu Yang, Fan Dang, Peng Wang, Hui Li, Zhen Li, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Reading irregular scene text of arbitrary shape in natural images is still a challenging problem, despite the progress made recently. Many existing approaches incorporate sophisticated network structures to handle various shapes, use extra annotations for stronger supervision, or employ hard-to-train recurrent neural networks for sequence modeling. In this work, we propose a simple yet strong approach for scene text recognition. With no need to convert input images to sequence representations, we directly connect two-dimensional CNN features to an attention-based sequence decoder which guided by holistic representation. The holistic representation can guide the attention-based decoder focus on more accurate area. As no recurrent module is adopted, our model can be trained in parallel. It achieves 1.5x to 9.4x acceleration to backward pass and 1.3x to 7.9x acceleration to forward pass, compared with the RNN counterparts. The proposed model is trained with only word-level annotations. With this simple design, our method achieves state-of-the-art or competitive recognition performance on the evaluated regular and irregular scene text benchmark datasets.



### Easy Transfer Learning By Exploiting Intra-domain Structures
- **Arxiv ID**: http://arxiv.org/abs/1904.01376v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01376v2)
- **Published**: 2019-04-02 12:43:53+00:00
- **Updated**: 2019-04-10 02:44:33+00:00
- **Authors**: Jindong Wang, Yiqiang Chen, Han Yu, Meiyu Huang, Qiang Yang
- **Comment**: Camera-ready version of IEEE International Conference on Multimedia
  and Expo (ICME) 2019; code available at
  http://transferlearning.xyz/code/traditional/EasyTL
- **Journal**: None
- **Summary**: Transfer learning aims at transferring knowledge from a well-labeled domain to a similar but different domain with limited or no labels. Unfortunately, existing learning-based methods often involve intensive model selection and hyperparameter tuning to obtain good results. Moreover, cross-validation is not possible for tuning hyperparameters since there are often no labels in the target domain. This would restrict wide applicability of transfer learning especially in computationally-constraint devices such as wearables. In this paper, we propose a practically Easy Transfer Learning (EasyTL) approach which requires no model selection and hyperparameter tuning, while achieving competitive performance. By exploiting intra-domain structures, EasyTL is able to learn both non-parametric transfer features and classifiers. Extensive experiments demonstrate that, compared to state-of-the-art traditional and deep methods, EasyTL satisfies the Occam's Razor principle: it is extremely easy to implement and use while achieving comparable or better performance in classification accuracy and much better computational efficiency. Additionally, it is shown that EasyTL can increase the performance of existing transfer feature learning methods.



### Effective Aesthetics Prediction with Multi-level Spatially Pooled Features
- **Arxiv ID**: http://arxiv.org/abs/1904.01382v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01382v1)
- **Published**: 2019-04-02 12:58:12+00:00
- **Updated**: 2019-04-02 12:58:12+00:00
- **Authors**: Vlad Hosu, Bastian Goldlucke, Dietmar Saupe
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: We propose an effective deep learning approach to aesthetics quality assessment that relies on a new type of pre-trained features, and apply it to the AVA data set, the currently largest aesthetics database. While previous approaches miss some of the information in the original images, due to taking small crops, down-scaling or warping the originals during training, we propose the first method that efficiently supports full resolution images as an input, and can be trained on variable input sizes. This allows us to significantly improve upon the state of the art, increasing the Spearman rank-order correlation coefficient (SRCC) of ground-truth mean opinion scores (MOS) from the existing best reported of 0.612 to 0.756. To achieve this performance, we extract multi-level spatially pooled (MLSP) features from all convolutional blocks of a pre-trained InceptionResNet-v2 network, and train a custom shallow Convolutional Neural Network (CNN) architecture on these new features.



### Meta-learning Convolutional Neural Architectures for Multi-target Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.08486v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08486v1)
- **Published**: 2019-04-02 13:08:33+00:00
- **Updated**: 2019-04-02 13:08:33+00:00
- **Authors**: Martin Mundt, Sagnik Majumder, Sreenivas Murali, Panagiotis Panetsos, Visvanathan Ramesh
- **Comment**: Accepted for publication at CVPR 2019. Version includes supplementary
  material
- **Journal**: None
- **Summary**: Recognition of defects in concrete infrastructure, especially in bridges, is a costly and time consuming crucial first step in the assessment of the structural integrity. Large variation in appearance of the concrete material, changing illumination and weather conditions, a variety of possible surface markings as well as the possibility for different types of defects to overlap, make it a challenging real-world task. In this work we introduce the novel COncrete DEfect BRidge IMage dataset (CODEBRIM) for multi-target classification of five commonly appearing concrete defects. We investigate and compare two reinforcement learning based meta-learning approaches, MetaQNN and efficient neural architecture search, to find suitable convolutional neural network architectures for this challenging multi-class multi-target task. We show that learned architectures have fewer overall parameters in addition to yielding better multi-target accuracy in comparison to popular neural architectures from the literature evaluated in the context of our application.



### Vehicle Re-identification in Aerial Imagery: Dataset and Approach
- **Arxiv ID**: http://arxiv.org/abs/1904.01400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01400v1)
- **Published**: 2019-04-02 13:24:16+00:00
- **Updated**: 2019-04-02 13:24:16+00:00
- **Authors**: Peng Wang, Bingliang Jiao, Lu Yang, Yifei Yang, Shizhou Zhang, Wei Wei, Yanning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we construct a large-scale dataset for vehicle re-identification (ReID), which contains 137k images of 13k vehicle instances captured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based vehicle ReID dataset. To increase intra-class variation, each vehicle is captured by at least two UAVs at different locations, with diverse view-angles and flight-altitudes. We manually label a variety of vehicle attributes, including vehicle type, color, skylight, bumper, spare tire and luggage rack. Furthermore, for each vehicle image, the annotator is also required to mark the discriminative parts that helps them to distinguish this particular vehicle from others. Besides the dataset, we also design a specific vehicle ReID algorithm to make full use of the rich annotation information. It is capable of explicitly detecting discriminative parts for each specific vehicle and significantly outperforms the evaluated baselines and state-of-the-art vehicle ReID approaches.



### Context and Attribute Grounded Dense Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.01410v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01410v1)
- **Published**: 2019-04-02 13:45:57+00:00
- **Updated**: 2019-04-02 13:45:57+00:00
- **Authors**: Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao
- **Comment**: 12 pages, 9 figures, accepted as a POSTER at CVPR2019
- **Journal**: None
- **Summary**: Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to state-of-the-art methods.



### SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences
- **Arxiv ID**: http://arxiv.org/abs/1904.01416v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.01416v3)
- **Published**: 2019-04-02 13:53:16+00:00
- **Updated**: 2019-08-16 09:30:52+00:00
- **Authors**: Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, Juergen Gall
- **Comment**: ICCV2019. See teaser video at http://bit.ly/SemanticKITTI-teaser
- **Journal**: None
- **Summary**: Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR.   In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete $360^{o}$ field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.



### Cooperative Embeddings for Instance, Attribute and Category Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.01421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01421v1)
- **Published**: 2019-04-02 13:55:47+00:00
- **Updated**: 2019-04-02 13:55:47+00:00
- **Authors**: William Thong, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of this paper is to retrieve an image based on instance, attribute and category similarity notions. Different from existing works, which usually address only one of these entities in isolation, we introduce a cooperative embedding to integrate them while preserving their specific level of semantic representation. An algebraic structure defines a superspace filled with instances. Attributes are axis-aligned to form subspaces, while categories influence the arrangement of similar instances. These relationships enable them to cooperate for their mutual benefits for image retrieval. We derive a proxy-based softmax embedding loss to learn simultaneously all similarity measures in both superspace and subspaces. We evaluate our model on datasets from two different domains. Experiments on image retrieval tasks show the benefits of the cooperative embeddings for modeling multiple image similarities, and for discovering style evolution of instances between- and within-categories.



### Non-Rigid Point Set Registration Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01428v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.01428v1)
- **Published**: 2019-04-02 14:01:59+00:00
- **Updated**: 2019-04-02 14:01:59+00:00
- **Authors**: Lingjing Wang, Jianchun Chen, Xiang Li, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: Point set registration is defined as a process to determine the spatial transformation from the source point set to the target one. Existing methods often iteratively search for the optimal geometric transformation to register a given pair of point sets, driven by minimizing a predefined alignment loss function. In contrast, the proposed point registration neural network (PR-Net) actively learns the registration pattern as a parametric function from a training dataset, consequently predict the desired geometric transformation to align a pair of point sets. PR-Net can transfer the learned knowledge (i.e. registration pattern) from registering training pairs to testing ones without additional iterative optimization. Specifically, in this paper, we develop novel techniques to learn shape descriptors from point sets that help formulate a clear correlation between source and target point sets. With the defined correlation, PR-Net tends to predict the transformation so that the source and target point sets can be statistically aligned, which in turn leads to an optimal spatial geometric registration. PR-Net achieves robust and superior performance for non-rigid registration of point sets, even in presence of Gaussian noise, outliers, and missing points, but requires much less time for registering large number of pairs. More importantly, for a new pair of point sets, PR-Net is able to directly predict the desired transformation using the learned model without repetitive iterative optimization routine. Our code is available at https://github.com/Lingjing324/PR-Net.



### Good News, Everyone! Context driven entity-aware captioning for news images
- **Arxiv ID**: http://arxiv.org/abs/1904.01475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01475v1)
- **Published**: 2019-04-02 14:55:46+00:00
- **Updated**: 2019-04-02 14:55:46+00:00
- **Authors**: Ali Furkan Biten, Lluis Gomez, Marçal Rusiñol, Dimosthenis Karatzas
- **Comment**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR
  2019)
- **Journal**: None
- **Summary**: Current image captioning systems perform at a merely descriptive level, essentially enumerating the objects in the scene and their relations. Humans, on the contrary, interpret images by integrating several sources of prior knowledge of the world. In this work, we aim to take a step closer to producing captions that offer a plausible interpretation of the scene, by integrating such contextual information into the captioning pipeline. For this we focus on the captioning of images used to illustrate news articles. We propose a novel captioning method that is able to leverage contextual information provided by the text of news articles associated with an image. Our model is able to selectively draw information from the article guided by visual cues, and to dynamically extend the output dictionary to out-of-vocabulary named entities that appear in the context source. Furthermore we introduce `GoodNews', the largest news image captioning dataset in the literature and demonstrate state-of-the-art results.



### Semantics Disentangling for Text-to-Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.01480v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01480v1)
- **Published**: 2019-04-02 15:08:51+00:00
- **Updated**: 2019-04-02 15:08:51+00:00
- **Authors**: Guojun Yin, Bin Liu, Lu Sheng, Nenghai Yu, Xiaogang Wang, Jing Shao
- **Comment**: 14 pages, 11 figures, accepted as an ORAL at CVPR2019
- **Journal**: None
- **Summary**: Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.



### Guided Super-Resolution as Pixel-to-Pixel Transformation
- **Arxiv ID**: http://arxiv.org/abs/1904.01501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01501v2)
- **Published**: 2019-04-02 15:41:44+00:00
- **Updated**: 2019-08-15 14:48:19+00:00
- **Authors**: Riccardo de Lutio, Stefano D'Aronco, Jan Dirk Wegner, Konrad Schindler
- **Comment**: Extended version, ICCV 2019
- **Journal**: None
- **Summary**: Guided super-resolution is a unifying framework for several computer vision tasks where the inputs are a low-resolution source image of some target quantity (e.g., perspective depth acquired with a time-of-flight camera) and a high-resolution guide image from a different domain (e.g., a grey-scale image from a conventional camera); and the target output is a high-resolution version of the source (in our example, a high-res depth map). The standard way of looking at this problem is to formulate it as a super-resolution task, i.e., the source image is upsampled to the target resolution, while transferring the missing high-frequency details from the guide. Here, we propose to turn that interpretation on its head and instead see it as a pixel-to-pixel mapping of the guide image to the domain of the source image. The pixel-wise mapping is parametrised as a multi-layer perceptron, whose weights are learned by minimising the discrepancies between the source image and the downsampled target image. Importantly, our formulation makes it possible to regularise only the mapping function, while avoiding regularisation of the outputs; thus producing crisp, natural-looking images. The proposed method is unsupervised, using only the specific source and guide images to fit the mapping. We evaluate our method on two different tasks, super-resolution of depth maps and of tree height maps. In both cases, we clearly outperform recent baselines in quantitative comparisons, while delivering visually much sharper outputs.



### FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation
- **Arxiv ID**: http://arxiv.org/abs/1904.01509v1
- **DOI**: 10.1109/ICMEW.2019.0-104
- **Categories**: **cs.LG**, cs.CV, cs.GR, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01509v1)
- **Published**: 2019-04-02 15:50:11+00:00
- **Updated**: 2019-04-02 15:50:11+00:00
- **Authors**: Yanfu Yan, Ke Lu, Jian Xue, Pengcheng Gao, Jiayi Lyu
- **Comment**: 9 pages, 7 figures
- **Journal**: 2019 IEEE International Conference on Multimedia & Expo Workshops
  (ICMEW)
- **Summary**: Facial expression analysis based on machine learning requires large number of well-annotated data to reflect different changes in facial motion. Publicly available datasets truly help to accelerate research in this area by providing a benchmark resource, but all of these datasets, to the best of our knowledge, are limited to rough annotations for action units, including only their absence, presence, or a five-level intensity according to the Facial Action Coding System. To meet the need for videos labeled in great detail, we present a well-annotated dataset named FEAFA for Facial Expression Analysis and 3D Facial Animation. One hundred and twenty-two participants, including children, young adults and elderly people, were recorded in real-world conditions. In addition, 99,356 frames were manually labeled using Expression Quantitative Tool developed by us to quantify 9 symmetrical FACS action units, 10 asymmetrical (unilateral) FACS action units, 2 symmetrical FACS action descriptors and 2 asymmetrical FACS action descriptors, and each action unit or action descriptor is well-annotated with a floating point number between 0 and 1. To provide a baseline for use in future research, a benchmark for the regression of action unit values based on Convolutional Neural Networks are presented. We also demonstrate the potential of our FEAFA dataset for 3D facial animation. Almost all state-of-the-art algorithms for facial animation are achieved based on 3D face reconstruction. We hence propose a novel method that drives virtual characters only based on action unit value regression of the 2D video frames of source actors.



### End-to-End Visual Speech Recognition for Small-Scale Datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.01954v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01954v4)
- **Published**: 2019-04-02 15:57:51+00:00
- **Updated**: 2019-07-09 17:42:11+00:00
- **Authors**: Stavros Petridis, Yujiang Wang, Pingchuan Ma, Zuwei Li, Maja Pantic
- **Comment**: Submitted to Pattern Recognition Letters
- **Journal**: None
- **Summary**: Visual speech recognition models traditionally consist of two stages, feature extraction and classification. Several deep learning approaches have been recently presented aiming to replace the feature extraction stage by automatically extracting features from mouth images. However, research on joint learning of features and classification remains limited. In addition, most of the existing methods require large amounts of data in order to achieve state-of-the-art performance, otherwise they under-perform. In this work, we present an end-to-end visual speech recognition system based on fully-connected layers and Long-Short Memory (LSTM) networks which is suitable for small-scale datasets. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by a Bidirectional LSTM (BLSTM) and the fusion of the two streams takes place via another BLSTM. An absolute improvement of 0.6%, 3.4%, 3.9%, 11.4% over the state-of-the-art is reported on the OuluVS2, CUAVE, AVLetters and AVLetters2 databases, respectively.



### Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.01538v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01538v2)
- **Published**: 2019-04-02 16:52:29+00:00
- **Updated**: 2019-08-18 08:31:04+00:00
- **Authors**: Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, Rynson Lau
- **Comment**: Accepted by CVPR'19. Project page:
  https://stevewongv.github.io/derain-project.html
- **Journal**: None
- **Summary**: Removing rain streaks from a single image has been drawing considerable attention as rain streaks can severely degrade the image quality and affect the performance of existing outdoor vision tasks. While recent CNN-based derainers have reported promising performances, deraining remains an open problem for two reasons. First, existing synthesized rain datasets have only limited realism, in terms of modeling real rain characteristics such as rain shape, direction and intensity. Second, there are no public benchmarks for quantitative comparisons on real rain images, which makes the current evaluation less objective. The core challenge is that real world rain/clean image pairs cannot be captured at the same time. In this paper, we address the single image rain removal problem in two ways. First, we propose a semi-automatic method that incorporates temporal priors and human supervision to generate a high-quality clean image from each input sequence of real rain images. Using this method, we construct a large-scale dataset of $\sim$$29.5K$ rain/rain-free image pairs that covers a wide range of natural rain scenes. Second, to better cover the stochastic distribution of real rain streaks, we propose a novel SPatial Attentive Network (SPANet) to remove rain streaks in a local-to-global manner. Extensive experiments demonstrate that our network performs favorably against the state-of-the-art deraining methods.



### Exploring Randomly Wired Neural Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01569v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01569v2)
- **Published**: 2019-04-02 17:57:16+00:00
- **Updated**: 2019-04-08 17:50:26+00:00
- **Authors**: Saining Xie, Alexander Kirillov, Ross Girshick, Kaiming He
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.



### Towards Human Body-Part Learning for Model-Free Gait Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01620v1)
- **Published**: 2019-04-02 18:49:14+00:00
- **Updated**: 2019-04-02 18:49:14+00:00
- **Authors**: Imad Rida
- **Comment**: None
- **Journal**: None
- **Summary**: Gait based biometric aims to discriminate among people by the way or manner they walk. It represents a biometric at distance which has many advantages over other biometric modalities. State-of-the-art methods require a limited cooperation from the individuals. Consequently, contrary to other modalities, gait is a non-invasive approach. As a behavioral analysis, gait is difficult to circumvent. Moreover, gait can be performed without the subject being aware of it. Consequently, it is more difficult to try to tamper one own biometric signature. In this paper we review different features and approaches used in gait recognition. A novel method able to learn the discriminative human body-parts to improve the recognition accuracy will be introduced. Extensive experiments will be performed on CASIA gait benchmark database and results will be compared to state-of-the-art methods.



### Towards annotation-efficient segmentation via image-to-image translation
- **Arxiv ID**: http://arxiv.org/abs/1904.01636v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01636v4)
- **Published**: 2019-04-02 19:35:27+00:00
- **Updated**: 2021-06-12 01:10:35+00:00
- **Authors**: Eugene Vorontsov, Pavlo Molchanov, Christopher Beckham, Jan Kautz, Samuel Kadoury
- **Comment**: None
- **Journal**: None
- **Summary**: Often in medical imaging, it is prohibitively challenging to produce enough boundary annotations to train deep neural networks for accurate tumor segmentation. We propose the use of weak labels about whether an image presents tumor or whether it is absent to extend training over images that lack these annotations. Specifically, we propose a semi-supervised framework that employs unpaired image-to-image translation between two domains, presence vs. absence of cancer, as the unsupervised objective. We conjecture that translation helps segmentation -- both require the target to be separated from the background. We encode images into two codes: one that is common to both domains and one that is unique to the presence domain. Decoding from the common code yields healthy images; decoding with the addition of the unique code produces a residual change to this image that adds cancer. Translation proceeds from presence to absence and vice versa. In the first case, the tumor is re-added to the image and we successfully exploit the residual decoder to also perform segmentation. In the second case, unique codes are sampled, producing a distribution of possible tumors. To validate the method, we created challenging synthetic tasks and tumor segmentation datasets from public BRATS (brain, MRI) and LitS (liver, CT) datasets. We show a clear improvement (0.83 Dice on brain, 0.74 on liver) over baseline semi-supervised training with autoencoding (0.73, 0.66) and a mean teacher approach (0.75, 0.69), demonstrating the ability to generalize from smaller distributions of annotated samples.



### A Strong Baseline for Domain Adaptation and Generalization in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.01638v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01638v1)
- **Published**: 2019-04-02 19:38:34+00:00
- **Updated**: 2019-04-02 19:38:34+00:00
- **Authors**: Li Yao, Jordan Prosky, Ben Covington, Kevin Lyman
- **Comment**: Extended abstract of a journal submission
- **Journal**: None
- **Summary**: This work provides a strong baseline for the problem of multi-source multi-target domain adaptation and generalization in medical imaging. Using a diverse collection of ten chest X-ray datasets, we empirically demonstrate the benefits of training medical imaging deep learning models on varied patient populations for generalization to out-of-sample domains.



### Performance Evalution of 3D Keypoint Detectors and Descriptors for Plants Health Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.08493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08493v1)
- **Published**: 2019-04-02 19:58:23+00:00
- **Updated**: 2019-04-02 19:58:23+00:00
- **Authors**: Shiva Azimi, Brejesh lall, Tapan K. Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Plant Phenomics based on imaging based techniques can be used to monitor the health and the diseases of plants and crops. The use of 3D data for plant phenomics is a recent phenomenon. However, since 3D point cloud contains more information than plant images, in this paper, we compare the performance of different keypoint detectors and local feature descriptors combinations for the plant growth stage and it's growth condition classification based on 3D point clouds of the plants. We have also implemented a modified form of 3D SIFT descriptor, that is invariant to rotation and is computationally less intense than most of the 3D SIFT descriptors reported in the existing literature. The performance is evaluated in terms of the classification accuracy and the results are presented in terms of accuracy tables. We find the ISS-SHOT and the SIFT-SIFT combinations consistently perform better and Fisher Vector (FV) is a better encoder than Vector of Linearly Aggregated (VLAD) for such applications. It can serve as a better modality.



### Sparse Bounded Degree Sum of Squares Optimization for Certifiably Globally Optimal Rotation Averaging
- **Arxiv ID**: http://arxiv.org/abs/1904.01645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01645v2)
- **Published**: 2019-04-02 20:02:14+00:00
- **Updated**: 2019-06-13 19:25:09+00:00
- **Authors**: Matthew Giamou, Filip Maric, Valentin Peretroukhin, Jonathan Kelly
- **Comment**: Prior version contained an erroneous proof which has been removed
- **Journal**: None
- **Summary**: Estimating unknown rotations from noisy measurements is an important step in SfM and other 3D vision tasks. Typically, local optimization methods susceptible to returning suboptimal local minima are used to solve the rotation averaging problem. A new wave of approaches that leverage convex relaxations have provided the first formal guarantees of global optimality for state estimation techniques involving SO(3). However, most of these guarantees are only applicable when the measurement error introduced by noise is within a certain bound that depends on the problem instance's structure. In this paper, we cast rotation averaging as a polynomial optimization problem over unit quaternions to produce the first rotation averaging method that is formally guaranteed to provide a certifiably globally optimal solution for \textit{any} problem instance. This is achieved by formulating and solving a sparse convex sum of squares (SOS) relaxation of the problem. We provide an open source implementation of our algorithm and experiments, demonstrating the benefits of our globally optimal approach.



### Sequential Adaptive Design for Jump Regression Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.01648v4
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01648v4)
- **Published**: 2019-04-02 20:14:47+00:00
- **Updated**: 2021-02-10 20:22:36+00:00
- **Authors**: Chiwoo Park, Peihua Qiu, Jennifer Carpena-Núñez, Rahul Rao, Michael Susner, Benji Maruyama
- **Comment**: None
- **Journal**: None
- **Summary**: Selecting input variables or design points for statistical models has been of great interest in adaptive design and active learning. Motivated by two scientific examples, this paper presents a strategy of selecting the design points for a regression model when the underlying regression function is discontinuous. The first example we undertook was for the purpose of accelerating imaging speed in a high resolution material imaging; the second was use of sequential design for the purpose of mapping a chemical phase diagram. In both examples, the underlying regression functions have discontinuities, so many of the existing design optimization approaches cannot be applied because they mostly assume a continuous regression function. Although some existing adaptive design strategies developed from treed regression models can handle the discontinuities, the Bayesian approaches come with computationally expensive Markov Chain Monte Carlo techniques for posterior inferences and subsequent design point selections, which is not appropriate for the first motivating example that requires computation at least faster than the original imaging speed. In addition, the treed models are based on the domain partitioning that are inefficient when the discontinuities occurs over complex sub-domain boundaries. We propose a simple and effective adaptive design strategy for a regression analysis with discontinuities: some statistical properties with a fixed design will be presented first, and then these properties will be used to propose a new criterion of selecting the design points for the regression analysis. Sequential design with the new criterion will be presented with comprehensive simulated examples, and its application to the two motivating examples will be presented.



### MVX-Net: Multimodal VoxelNet for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.01649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01649v1)
- **Published**: 2019-04-02 20:15:07+00:00
- **Updated**: 2019-04-02 20:15:07+00:00
- **Authors**: Vishwanath A. Sindagi, Yin Zhou, Oncel Tuzel
- **Comment**: 7 pages
- **Journal**: International Conference on Robotics and Automation (ICRA), 2019
- **Summary**: Many recent works on 3D object detection have focused on designing neural network architectures that can consume point cloud data. While these approaches demonstrate encouraging performance, they are typically based on a single modality and are unable to leverage information from other modalities, such as a camera. Although a few approaches fuse data from different modalities, these methods either use a complicated pipeline to process the modalities sequentially, or perform late-fusion and are unable to learn interaction between different modalities at early stages. In this work, we present PointFusion and VoxelFusion: two simple yet effective early-fusion approaches to combine the RGB and point cloud modalities, by leveraging the recently introduced VoxelNet architecture. Evaluation on the KITTI dataset demonstrates significant improvements in performance over approaches which only use point cloud data. Furthermore, the proposed method provides results competitive with the state-of-the-art multimodal algorithms, achieving top-2 ranking in five of the six bird's eye view and 3D detection categories on the KITTI benchmark, by using a simple single stage network.



### Identifying disease-free chest X-ray images with deep transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01654v1
- **DOI**: 10.1117/12.2513164
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01654v1)
- **Published**: 2019-04-02 20:26:53+00:00
- **Updated**: 2019-04-02 20:26:53+00:00
- **Authors**: Ken C. L. Wong, Mehdi Moradi, Joy Wu, Tanveer Syeda-Mahmood
- **Comment**: SPIE Medical Imaging, 2019 (oral presentation)
- **Journal**: None
- **Summary**: Chest X-rays (CXRs) are among the most commonly used medical image modalities. They are mostly used for screening, and an indication of disease typically results in subsequent tests. As this is mostly a screening test used to rule out chest abnormalities, the requesting clinicians are often interested in whether a CXR is normal or not. A machine learning algorithm that can accurately screen out even a small proportion of the "real normal" exams out of all requested CXRs would be highly beneficial in reducing the workload for radiologists. In this work, we report a deep neural network trained for classifying CXRs with the goal of identifying a large number of normal (disease-free) images without risking the discharge of sick patients. We use an ImageNet-pretrained Inception-ResNet-v2 model to provide the image features, which are further used to train a model on CXRs labelled by expert radiologists. The probability threshold for classification is optimized for 100% precision for the normal class, ensuring no sick patients are released. At this threshold we report an average recall of 50%. This means that the proposed solution has the potential to cut in half the number of disease-free CXRs examined by radiologists, without risking the discharge of sick patients.



### Activity Driven Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.01665v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01665v1)
- **Published**: 2019-04-02 20:52:39+00:00
- **Updated**: 2019-04-02 20:52:39+00:00
- **Authors**: Zhenheng Yang, Dhruv Mahajan, Deepti Ghadiyaram, Ram Nevatia, Vignesh Ramanathan
- **Comment**: CVPR'19 camera ready
- **Journal**: None
- **Summary**: Weakly supervised object detection aims at reducing the amount of supervision required to train detection models. Such models are traditionally learned from images/videos labelled only with the object class and not the object bounding box. In our work, we try to leverage not only the object class labels but also the action labels associated with the data. We show that the action depicted in the image/video can provide strong cues about the location of the associated object. We learn a spatial prior for the object dependent on the action (e.g. "ball" is closer to "leg of the person" in "kicking ball"), and incorporate this prior to simultaneously train a joint object detection and action classification model. We conducted experiments on both video datasets and image datasets to evaluate the performance of our weakly supervised object detection model. Our approach outperformed the current state-of-the-art (SOTA) method by more than 6% in mAP on the Charades video dataset.



### Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.01690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01690v1)
- **Published**: 2019-04-02 22:25:29+00:00
- **Updated**: 2019-04-02 22:25:29+00:00
- **Authors**: Jason Ku, Alex D. Pon, Steven L. Waslander
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: We present MonoPSR, a monocular 3D object detection method that leverages proposals and shape reconstruction. First, using the fundamental relations of a pinhole camera model, detections from a mature 2D object detector are used to generate a 3D proposal per object in a scene. The 3D location of these proposals prove to be quite accurate, which greatly reduces the difficulty of regressing the final 3D bounding box detection. Simultaneously, a point cloud is predicted in an object centered coordinate system to learn local scale and shape information. However, the key challenge is how to exploit shape information to guide 3D localization. As such, we devise aggregate losses, including a novel projection alignment loss, to jointly optimize these tasks in the neural network to improve 3D localization accuracy. We validate our method on the KITTI benchmark where we set new state-of-the-art results among published monocular methods, including the harder pedestrian and cyclist classes, while maintaining efficient run-time.



### Multigrid Predictive Filter Flow for Unsupervised Learning on Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.01693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01693v1)
- **Published**: 2019-04-02 22:41:48+00:00
- **Updated**: 2019-04-02 22:41:48+00:00
- **Authors**: Shu Kong, Charless Fowlkes
- **Comment**: webpage (https://www.ics.uci.edu/~skong2/mgpff.html)
- **Journal**: None
- **Summary**: We introduce multigrid Predictive Filter Flow (mgPFF), a framework for unsupervised learning on videos. The mgPFF takes as input a pair of frames and outputs per-pixel filters to warp one frame to the other. Compared to optical flow used for warping frames, mgPFF is more powerful in modeling sub-pixel movement and dealing with corruption (e.g., motion blur). We develop a multigrid coarse-to-fine modeling strategy that avoids the requirement of learning large filters to capture large displacement. This allows us to train an extremely compact model (4.6MB) which operates in a progressive way over multiple resolutions with shared weights. We train mgPFF on unsupervised, free-form videos and show that mgPFF is able to not only estimate long-range flow for frame reconstruction and detect video shot transitions, but also readily amendable for video object segmentation and pose tracking, where it substantially outperforms the published state-of-the-art without bells and whistles. Moreover, owing to mgPFF's nature of per-pixel filter prediction, we have the unique opportunity to visualize how each pixel is evolving during solving these tasks, thus gaining better interpretability.



### 3DRegNet: A Deep Neural Network for 3D Point Registration
- **Arxiv ID**: http://arxiv.org/abs/1904.01701v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01701v2)
- **Published**: 2019-04-02 22:59:46+00:00
- **Updated**: 2020-04-07 11:04:56+00:00
- **Authors**: G. Dias Pais, Srikumar Ramalingam, Venu Madhav Govindu, Jacinto C. Nascimento, Rama Chellappa, Pedro Miraldo
- **Comment**: 15 pages, 8 figures, 6 tables
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2020
- **Summary**: We present 3DRegNet, a novel deep learning architecture for the registration of 3D scans. Given a set of 3D point correspondences, we build a deep neural network to address the following two challenges: (i) classification of the point correspondences into inliers/outliers, and (ii) regression of the motion parameters that align the scans into a common reference frame. With regard to regression, we present two alternative approaches: (i) a Deep Neural Network (DNN) registration and (ii) a Procrustes approach using SVD to estimate the transformation. Our correspondence-based approach achieves a higher speedup compared to competing baselines. We further propose the use of a refinement network, which consists of a smaller 3DRegNet as a refinement to improve the accuracy of the registration. Extensive experiments on two challenging datasets demonstrate that we outperform other methods and achieve state-of-the-art results. The code is available.



