# Arxiv Papers in cs.CV on 2019-04-05
### A Regularization Approach for Instance-Based Superset Label Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.02832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02832v1)
- **Published**: 2019-04-05 00:22:26+00:00
- **Updated**: 2019-04-05 00:22:26+00:00
- **Authors**: Chen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Different from the traditional supervised learning in which each training example has only one explicit label, superset label learning (SLL) refers to the problem that a training example can be associated with a set of candidate labels, and only one of them is correct. Existing SLL methods are either regularization-based or instance-based, and the latter of which has achieved state-of-the-art performance. This is because the latest instance-based methods contain an explicit disambiguation operation that accurately picks up the groundtruth label of each training example from its ambiguous candidate labels. However, such disambiguation operation does not fully consider the mutually exclusive relationship among different candidate labels, so the disambiguated labels are usually generated in a nondiscriminative way, which is unfavorable for the instance-based methods to obtain satisfactory performance. To address this defect, we develop a novel regularization approach for instance-based superset label (RegISL) learning so that our instance-based method also inherits the good discriminative ability possessed by the regularization scheme. Specifically, we employ a graph to represent the training set, and require the examples that are adjacent on the graph to obtain similar labels. More importantly, a discrimination term is proposed to enlarge the gap of values between possible labels and unlikely labels for every training example. As a result, the intrinsic constraints among different candidate labels are deployed, and the disambiguated labels generated by RegISL are more discriminative and accurate than those output by existing instance-based algorithms. The experimental results on various tasks convincingly demonstrate the superiority of our RegISL to other typical SLL methods in terms of both training accuracy and test accuracy.



### FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate Inference
- **Arxiv ID**: http://arxiv.org/abs/1904.02835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02835v1)
- **Published**: 2019-04-05 00:27:16+00:00
- **Updated**: 2019-04-05 00:27:16+00:00
- **Authors**: Ruizhou Ding, Zeye Liu, Ting-Wu Chin, Diana Marculescu, R. D., Blanton
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the throughput and energy efficiency of Deep Neural Networks (DNNs) on customized hardware, lightweight neural networks constrain the weights of DNNs to be a limited combination (denoted as $k\in\{1,2\}$) of powers of 2. In such networks, the multiply-accumulate operation can be replaced with a single shift operation, or two shifts and an add operation. To provide even more design flexibility, the $k$ for each convolutional filter can be optimally chosen instead of being fixed for every filter. In this paper, we formulate the selection of $k$ to be differentiable, and describe model training for determining $k$-based weights on a per-filter basis. Over 46 FPGA-design experiments involving eight configurations and four data sets reveal that lightweight neural networks with a flexible $k$ value (dubbed FLightNNs) fully utilize the hardware resources on Field Programmable Gate Arrays (FPGAs), our experimental results show that FLightNNs can achieve 2$\times$ speedup when compared to lightweight NNs with $k=2$, with only 0.1\% accuracy degradation. Compared to a 4-bit fixed-point quantization, FLightNNs achieve higher accuracy and up to 2$\times$ inference speedup, due to their lightweight shift operations. In addition, our experiments also demonstrate that FLightNNs can achieve higher computational energy efficiency for ASIC implementation.



### Deep Learning-based Universal Beamformer for Ultrasound Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.02843v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02843v2)
- **Published**: 2019-04-05 01:40:52+00:00
- **Updated**: 2019-07-15 14:07:27+00:00
- **Authors**: Shujaat Khan, Jaeyoung Huh, Jong Chul Ye
- **Comment**: Accepted for MICCAI 2019. arXiv admin note: substantial text overlap
  with arXiv:1901.01706
- **Journal**: None
- **Summary**: In ultrasound (US) imaging, individual channel RF measurements are back-propagated and accumulated to form an image after applying specific delays. While this time reversal is usually implemented using a hardware- or software-based delay-and-sum (DAS) beamformer, the performance of DAS decreases rapidly in situations where data acquisition is not ideal. Herein, for the first time, we demonstrate that a single data-driven adaptive beamformer designed as a deep neural network can generate high quality images robustly for various detector channel configurations and subsampling rates. The proposed deep beamformer is evaluated for two distinct acquisition schemes: focused ultrasound imaging and planewave imaging. Experimental results showed that the proposed deep beamformer exhibit significant performance gain for both focused and planar imaging schemes, in terms of contrast-to-noise ratio and structural similarity.



### Deep Tree Learning for Zero-shot Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/1904.02860v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02860v2)
- **Published**: 2019-04-05 03:23:16+00:00
- **Updated**: 2019-04-09 16:27:47+00:00
- **Authors**: Yaojie Liu, Joel Stehouwer, Amin Jourabloo, Xiaoming Liu
- **Comment**: To appear at CVPR 2019 as an oral presentation
- **Journal**: None
- **Summary**: Face anti-spoofing is designed to keep face recognition systems from recognizing fake faces as the genuine users. While advanced face anti-spoofing methods are developed, new types of spoof attacks are also being created and becoming a threat to all existing systems. We define the detection of unknown spoof attacks as Zero-Shot Face Anti-spoofing (ZSFA). Previous works of ZSFA only study 1-2 types of spoof attacks, such as print/replay attacks, which limits the insight of this problem. In this work, we expand the ZSFA problem to a wide range of 13 types of spoof attacks, including print attack, replay attack, 3D mask attacks, and so on. A novel Deep Tree Network (DTN) is proposed to tackle the ZSFA. The tree is learned to partition the spoof samples into semantic sub-groups in an unsupervised fashion. When a data sample arrives, being know or unknown attacks, DTN routes it to the most similar spoof cluster, and make the binary decision. In addition, to enable the study of ZSFA, we introduce the first face anti-spoofing database that contains diverse types of spoof attacks. Experiments show that our proposed method achieves the state of the art on multiple testing protocols of ZSFA.



### Actively Seeking and Learning from Live Data
- **Arxiv ID**: http://arxiv.org/abs/1904.02865v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02865v1)
- **Published**: 2019-04-05 04:23:02+00:00
- **Updated**: 2019-04-05 04:23:02+00:00
- **Authors**: Damien Teney, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: One of the key limitations of traditional machine learning methods is their requirement for training data that exemplifies all the information to be learned. This is a particular problem for visual question answering methods, which may be asked questions about virtually anything. The approach we propose is a step toward overcoming this limitation by searching for the information required at test time. The resulting method dynamically utilizes data from an external source, such as a large set of questions/answers or images/captions. Concretely, we learn a set of base weights for a simple VQA model, that are specifically adapted to a given question with the information specifically retrieved for this question. The adaptation process leverages recent advances in gradient-based meta learning and contributions for efficient retrieval and cross-domain adaptation. We surpass the state-of-the-art on the VQA-CP v2 benchmark and demonstrate our approach to be intrinsically more robust to out-of-distribution test data. We demonstrate the use of external non-VQA data using the MS COCO captioning dataset to support the answering process. This approach opens a new avenue for open-domain VQA systems that interface with diverse sources of data.



### Fast Spatio-Temporal Residual Network for Video Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.02870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02870v1)
- **Published**: 2019-04-05 05:08:00+00:00
- **Updated**: 2019-04-05 05:08:00+00:00
- **Authors**: Sheng Li, Fengxiang He, Bo Du, Lefei Zhang, Yonghao Xu, Dacheng Tao
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Recently, deep learning based video super-resolution (SR) methods have achieved promising performance. To simultaneously exploit the spatial and temporal information of videos, employing 3-dimensional (3D) convolutions is a natural approach. However, straight utilizing 3D convolutions may lead to an excessively high computational complexity which restricts the depth of video SR models and thus undermine the performance. In this paper, we present a novel fast spatio-temporal residual network (FSTRN) to adopt 3D convolutions for the video SR task in order to enhance the performance while maintaining a low computational load. Specifically, we propose a fast spatio-temporal residual block (FRB) that divide each 3D filter to the product of two 3D filters, which have considerably lower dimensions. Furthermore, we design a cross-space residual learning that directly links the low-resolution space and the high-resolution space, which can greatly relieve the computational burden on the feature fusion and up-scaling parts. Extensive evaluations and comparisons on benchmark datasets validate the strengths of the proposed approach and demonstrate that the proposed network significantly outperforms the current state-of-the-art methods.



### Mumford-Shah Loss Functional for Image Segmentation with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.02872v2
- **DOI**: 10.1109/TIP.2019.2941265
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02872v2)
- **Published**: 2019-04-05 05:17:18+00:00
- **Updated**: 2019-09-09 09:14:39+00:00
- **Authors**: Boah Kim, Jong Chul Ye
- **Comment**: Accepted for IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Recent state-of-the-art image segmentation algorithms are mostly based on deep neural networks, thanks to their high performance and fast computation time. However, these methods are usually trained in a supervised manner, which requires large number of high quality ground-truth segmentation masks. On the other hand, classical image segmentation approaches such as level-set methods are formulated in a self-supervised manner by minimizing energy functions such as Mumford-Shah functional, so they are still useful to help generation of segmentation masks without labels. Unfortunately, these algorithms are usually computationally expensive and often have limitation in semantic segmentation. In this paper, we propose a novel loss function based on Mumford-Shah functional that can be used in deep-learning based image segmentation without or with small labeled data. This loss function is based on the observation that the softmax layer of deep neural networks has striking similarity to the characteristic function in the Mumford-Shah functional. We show that the new loss function enables semi-supervised and unsupervised segmentation. In addition, our loss function can be also used as a regularized function to enhance supervised semantic segmentation algorithms. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method.



### Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours
- **Arxiv ID**: http://arxiv.org/abs/1904.02877v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02877v1)
- **Published**: 2019-04-05 05:49:41+00:00
- **Updated**: 2019-04-05 05:49:41+00:00
- **Authors**: Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, Diana Marculescu
- **Comment**: None
- **Journal**: None
- **Summary**: Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the runtime constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar constraints (<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000x faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.



### Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks
- **Arxiv ID**: http://arxiv.org/abs/1904.02884v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02884v1)
- **Published**: 2019-04-05 06:15:51+00:00
- **Updated**: 2019-04-05 06:15:51+00:00
- **Authors**: Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.



### Snap and Find: Deep Discrete Cross-domain Garment Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.02887v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02887v1)
- **Published**: 2019-04-05 06:30:22+00:00
- **Updated**: 2019-04-05 06:30:22+00:00
- **Authors**: Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Huimin Lu
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing number of online stores, there is a pressing need for intelligent search systems to understand the item photos snapped by customers and search against large-scale product databases to find their desired items. However, it is challenging for conventional retrieval systems to match up the item photos captured by customers and the ones officially released by stores, especially for garment images. To bridge the customer- and store- provided garment photos, existing studies have been widely exploiting the clothing attributes (\textit{e.g.,} black) and landmarks (\textit{e.g.,} collar) to learn a common embedding space for garment representations. Unfortunately they omit the sequential correlation of attributes and consume large quantity of human labors to label the landmarks. In this paper, we propose a deep multi-task cross-domain hashing termed \textit{DMCH}, in which cross-domain embedding and sequential attribute learning are modeled simultaneously. Sequential attribute learning not only provides the semantic guidance for embedding, but also generates rich attention on discriminative local details (\textit{e.g.,} black buttons) of clothing items without requiring extra landmark labels. This leads to promising performance and 306$\times$ boost on efficiency when compared with the state-of-the-art models, which is demonstrated through rigorous experiments on two public fashion datasets.



### Dense Haze: A benchmark for image dehazing with dense-haze and haze-free images
- **Arxiv ID**: http://arxiv.org/abs/1904.02904v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02904v1)
- **Published**: 2019-04-05 07:27:32+00:00
- **Updated**: 2019-04-05 07:27:32+00:00
- **Authors**: Codruta O. Ancuti, Cosmin Ancuti, Mateu Sbert, Radu Timofte
- **Comment**: 5 pages, 2 figures
- **Journal**: None
- **Summary**: Single image dehazing is an ill-posed problem that has recently drawn important attention. Despite the significant increase in interest shown for dehazing over the past few years, the validation of the dehazing methods remains largely unsatisfactory, due to the lack of pairs of real hazy and corresponding haze-free reference images. To address this limitation, we introduce Dense-Haze - a novel dehazing dataset. Characterized by dense and homogeneous hazy scenes, Dense-Haze contains 33 pairs of real hazy and corresponding haze-free images of various outdoor scenes. The hazy scenes have been recorded by introducing real haze, generated by professional haze machines. The hazy and haze-free corresponding scenes contain the same visual content captured under the same illumination parameters. Dense-Haze dataset aims to push significantly the state-of-the-art in single-image dehazing by promoting robust methods for real and various hazy scenes. We also provide a comprehensive qualitative and quantitative evaluation of state-of-the-art single image dehazing techniques based on the Dense-Haze dataset. Not surprisingly, our study reveals that the existing dehazing techniques perform poorly for dense homogeneous hazy scenes and that there is still much room for improvement.



### Deep Predictive Video Compression with Bi-directional Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.02909v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02909v1)
- **Published**: 2019-04-05 07:43:02+00:00
- **Updated**: 2019-04-05 07:43:02+00:00
- **Authors**: Woonsung Park, Munchurl Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, deep image compression has shown a big progress in terms of coding efficiency and image quality improvement. However, relatively less attention has been put on video compression using deep learning networks. In the paper, we first propose a deep learning based bi-predictive coding network, called BP-DVC Net, for video compression. Learned from the lesson of the conventional video coding, a B-frame coding structure is incorporated in our BP-DVC Net. While the bi-predictive coding in the conventional video codecs requires to transmit to decoder sides the motion vectors for block motion and the residues from prediction, our BP-DVC Net incorporates optical flow estimation networks in both encoder and decoder sides so as not to transmit the motion information to the decoder sides for coding efficiency improvement. Also, a bi-prediction network in the BP-DVC Net is proposed and used to precisely predict the current frame and to yield the resulting residues as small as possible. Furthermore, our BP-DVC Net allows for the compressive feature maps to be entropy-coded using the temporal context among the feature maps of adjacent frames. The BP-DVC Net has an end-to-end video compression architecture with newly designed flow and prediction losses. Experimental results show that the compression performance of our proposed method is comparable to those of H.264, HEVC in terms of PSNR and MS-SSIM.



### Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit PSF Layer
- **Arxiv ID**: http://arxiv.org/abs/1904.02910v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02910v1)
- **Published**: 2019-04-05 07:43:34+00:00
- **Updated**: 2019-04-05 07:43:34+00:00
- **Authors**: Sungjun Lim, Sang-Eun Lee, Sunghoe Chang, Jong Chul Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Deconvolution microscopy has been extensively used to improve the resolution of the widefield fluorescent microscopy. Conventional approaches, which usually require the point spread function (PSF) measurement or blind estimation, are however computationally expensive. Recently, CNN based approaches have been explored as a fast and high performance alternative. In this paper, we present a novel unsupervised deep neural network for blind deconvolution based on cycle consistency and PSF modeling layers. In contrast to the recent CNN approaches for similar problem, the explicit PSF modeling layers improve the robustness of the algorithm. Experimental results confirm the efficacy of the algorithm.



### Point-to-Point Video Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.02912v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02912v2)
- **Published**: 2019-04-05 07:43:55+00:00
- **Updated**: 2019-08-07 09:14:55+00:00
- **Authors**: Tsun-Hsuan Wang, Yen-Chi Cheng, Chieh Hubert Lin, Hwann-Tzong Chen, Min Sun
- **Comment**: To appear in ICCV 2019. The first two authors contributed equally to
  this work. 16 pages, 21 figures
- **Journal**: None
- **Summary**: While image manipulation achieves tremendous breakthroughs (e.g., generating realistic faces) in recent years, video generation is much less explored and harder to control, which limits its applications in the real world. For instance, video editing requires temporal coherence across multiple clips and thus poses both start and end constraints within a video sequence. We introduce point-to-point video generation that controls the generation process with two control points: the targeted start- and end-frames. The task is challenging since the model not only generates a smooth transition of frames, but also plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of various length. We propose to maximize the modified variational lower bound of conditional data likelihood under a skip-frame training strategy. Our model can generate sequences such that their end-frame is consistent with the targeted end-frame without loss of quality and diversity. Extensive experiments are conducted on Stochastic Moving MNIST, Weizmann Human Action, and Human3.6M to evaluate the effectiveness of the proposed method. We demonstrate our method under a series of scenarios (e.g., dynamic length generation) and the qualitative results showcase the potential and merits of point-to-point generation. For project page, see https://zswang666.github.io/P2PVG-Project-Page/



### 3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization
- **Arxiv ID**: http://arxiv.org/abs/1904.02917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02917v1)
- **Published**: 2019-04-05 07:53:42+00:00
- **Updated**: 2019-04-05 07:53:42+00:00
- **Authors**: Tsun-Hsuan Wang, Hou-Ning Hu, Chieh Hubert Lin, Yi-Hsuan Tsai, Wei-Chen Chiu, Min Sun
- **Comment**: ver.1
- **Journal**: None
- **Summary**: The complementary characteristics of active and passive depth sensing techniques motivate the fusion of the Li-DAR sensor and stereo camera for improved depth perception. Instead of directly fusing estimated depths across LiDAR and stereo modalities, we take advantages of the stereo matching network with two enhanced techniques: Input Fusion and Conditional Cost Volume Normalization (CCVNorm) on the LiDAR information. The proposed framework is generic and closely integrated with the cost volume component that is commonly utilized in stereo matching neural networks. We experimentally verify the efficacy and robustness of our method on the KITTI Stereo and Depth Completion datasets, obtaining favorable performance against various fusion strategies. Moreover, we demonstrate that, with a hierarchical extension of CCVNorm, the proposed method brings only slight overhead to the stereo matching network in terms of computation time and model size. For project page, see https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/



### Branched Multi-Task Networks: Deciding What Layers To Share
- **Arxiv ID**: http://arxiv.org/abs/1904.02920v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02920v5)
- **Published**: 2019-04-05 08:00:32+00:00
- **Updated**: 2020-08-13 06:44:45+00:00
- **Authors**: Simon Vandenhende, Stamatios Georgoulis, Bert De Brabandere, Luc Van Gool
- **Comment**: Accepted at BMVC 2020
- **Journal**: None
- **Summary**: In the context of multi-task learning, neural networks with branched architectures have often been employed to jointly tackle the tasks at hand. Such ramified networks typically start with a number of shared layers, after which different tasks branch out into their own sequence of layers. Understandably, as the number of possible network configurations is combinatorially large, deciding what layers to share and where to branch out becomes cumbersome. Prior works have either relied on ad hoc methods to determine the level of layer sharing, which is suboptimal, or utilized neural architecture search techniques to establish the network design, which is considerably expensive. In this paper, we go beyond these limitations and propose an approach to automatically construct branched multi-task networks, by leveraging the employed tasks' affinities. Given a specific budget, i.e. number of learnable parameters, the proposed approach generates architectures, in which shallow layers are task-agnostic, whereas deeper ones gradually grow more task-specific. Extensive experimental analysis across numerous, diverse multi-tasking datasets shows that, for a given budget, our method consistently yields networks with the highest performance, while for a certain performance threshold it requires the least amount of learnable parameters.



### Center and Scale Prediction: Anchor-free Approach for Pedestrian and Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.02948v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02948v4)
- **Published**: 2019-04-05 09:14:57+00:00
- **Updated**: 2021-11-07 08:58:00+00:00
- **Authors**: Wei Liu, Irtiza Hasan, Shengcai Liao
- **Comment**: An extension of the paper accepted by CVPR2019, the title is changed
  to 'Center and Scale Prediction: A Box-free Approach for Pedestrian and Face
  Detection'
- **Journal**: None
- **Summary**: Object detection generally requires sliding-window classifiers in tradition or anchor box based predictions in modern deep learning approaches. However, either of these approaches requires tedious configurations in boxes. In this paper, we provide a new perspective where detecting objects is motivated as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Besides, like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys a box-free setting. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method. Code and models can be accessed at (https://github.com/liuwei16/CSP and https://github.com/hasanirtiza/Pedestron).



### Learning to Adapt for Stereo
- **Arxiv ID**: http://arxiv.org/abs/1904.02957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02957v1)
- **Published**: 2019-04-05 09:37:27+00:00
- **Updated**: 2019-04-05 09:37:27+00:00
- **Authors**: Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, Philip H. S. Torr
- **Comment**: Accepted at CVPR2019. Code available at
  https://github.com/CVLAB-Unibo/Learning2AdaptForStereo
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  2019, pp. 9661-9670
- **Summary**: Real world applications of stereo depth estimation require models that are robust to dynamic variations in the environment. Even though deep learning based stereo methods are successful, they often fail to generalize to unseen variations in the environment, making them less suitable for practical applications such as autonomous driving. In this work, we introduce a "learning-to-adapt" framework that enables deep stereo methods to continuously adapt to new target domains in an unsupervised manner. Specifically, our approach incorporates the adaptation procedure into the learning objective to obtain a base set of parameters that are better suited for unsupervised online adaptation. To further improve the quality of the adaptation, we learn a confidence measure that effectively masks the errors introduced during the unsupervised adaptation. We evaluate our method on synthetic and real-world stereo datasets and our experiments evidence that learning-to-adapt is, indeed beneficial for online adaptation on vastly different domains.



### Semantic Attribute Matching Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.02969v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02969v1)
- **Published**: 2019-04-05 10:03:43+00:00
- **Updated**: 2019-04-05 10:03:43+00:00
- **Authors**: Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, Kwanghoon Sohn
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present semantic attribute matching networks (SAM-Net) for jointly establishing correspondences and transferring attributes across semantically similar images, which intelligently weaves the advantages of the two tasks while overcoming their limitations. SAM-Net accomplishes this through an iterative process of establishing reliable correspondences by reducing the attribute discrepancy between the images and synthesizing attribute transferred images using the learned correspondences. To learn the networks using weak supervisions in the form of image pairs, we present a semantic attribute matching loss based on the matching similarity between an attribute transferred source feature and a warped target feature. With SAM-Net, the state-of-the-art performance is attained on several benchmarks for semantic matching and attribute transfer.



### Relation-Aware Global Attention for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.02998v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02998v2)
- **Published**: 2019-04-05 11:31:37+00:00
- **Updated**: 2020-03-26 20:19:14+00:00
- **Authors**: Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, Zhibo Chen
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: For person re-identification (re-id), attention mechanisms have become attractive as they aim at strengthening discriminative features and suppressing irrelevant ones, which matches well the key of re-id, i.e., discriminative feature learning. Previous approaches typically learn attention using local convolutions, ignoring the mining of knowledge from global structure patterns. Intuitively, the affinities among spatial positions/nodes in the feature map provide clustering-like information and are helpful for inferring semantics and thus attention, especially for person images where the feasible human poses are constrained. In this work, we propose an effective Relation-Aware Global Attention (RGA) module which captures the global structural information for better attention learning. Specifically, for each feature position, in order to compactly grasp the structural information of global scope and local appearance information, we propose to stack the relations, i.e., its pairwise correlations/affinities with all the feature positions (e.g., in raster scan order), and the feature itself together to learn the attention with a shallow convolutional model. Extensive ablation studies demonstrate that our RGA can significantly enhance the feature representation power and help achieve the state-of-the-art performance on several popular benchmarks. The source code is available at https://github.com/microsoft/Relation-Aware-Global-Attention-Networks.



### What Object Should I Use? - Task Driven Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.03000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.03000v1)
- **Published**: 2019-04-05 11:36:07+00:00
- **Updated**: 2019-04-05 11:36:07+00:00
- **Authors**: Johann Sawatzky, Yaser Souri, Christian Grund, Juergen Gall
- **Comment**: CVPR 2019. The first two authors contributed equally, ordered
  alphabetically
- **Journal**: None
- **Summary**: When humans have to solve everyday tasks, they simply pick the objects that are most suitable. While the question which object should one use for a specific task sounds trivial for humans, it is very difficult to answer for robots or other autonomous systems. This issue, however, is not addressed by current benchmarks for object detection that focus on detecting object categories. We therefore introduce the COCO-Tasks dataset which comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated. We furthermore propose an approach that detects the most suitable objects for a given task. The approach builds on a Gated Graph Neural Network to exploit the appearance of each object as well as the global context of all present objects in the scene. In our experiments, we show that the proposed approach outperforms other approaches that are evaluated on the dataset like classification or ranking approaches.



### Learning Task Relatedness in Multi-Task Learning for Images in Context
- **Arxiv ID**: http://arxiv.org/abs/1904.03011v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03011v1)
- **Published**: 2019-04-05 12:08:16+00:00
- **Updated**: 2019-04-05 12:08:16+00:00
- **Authors**: Gjorgji Strezoski, Nanne van Noord, Marcel Worring
- **Comment**: To appear in ICMR 2019 (Oral + Lightning Talk + Poster)
- **Journal**: None
- **Summary**: Multimedia applications often require concurrent solutions to multiple tasks. These tasks hold clues to each-others solutions, however as these relations can be complex this remains a rarely utilized property. When task relations are explicitly defined based on domain knowledge multi-task learning (MTL) offers such concurrent solutions, while exploiting relatedness between multiple tasks performed over the same dataset. In most cases however, this relatedness is not explicitly defined and the domain expert knowledge that defines it is not available. To address this issue, we introduce Selective Sharing, a method that learns the inter-task relatedness from secondary latent features while the model trains. Using this insight, we can automatically group tasks and allow them to share knowledge in a mutually beneficial way. We support our method with experiments on 5 datasets in classification, regression, and ranking tasks and compare to strong baselines and state-of-the-art approaches showing a consistent improvement in terms of accuracy and parameter counts. In addition, we perform an activation region analysis showing how Selective Sharing affects the learned representation.



### Automatic detection of lesion load change in Multiple Sclerosis using convolutional neural networks with segmentation confidence
- **Arxiv ID**: http://arxiv.org/abs/1904.03041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03041v1)
- **Published**: 2019-04-05 12:59:58+00:00
- **Updated**: 2019-04-05 12:59:58+00:00
- **Authors**: Richard McKinley, Lorenz Grunder, Rik Wepfer, Fabian Aschwanden, Tim Fischer, Christoph Friedli, Raphaela Muri, Christian Rummel, Rajeev Verma, Christian Weisstanner, Mauricio Reyes, Anke Salmen, Andrew Chan, Roland Wiest, Franca Wagner
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of new or enlarged white-matter lesions in multiple sclerosis is a vital task in the monitoring of patients undergoing disease-modifying treatment for multiple sclerosis. However, the definition of 'new or enlarged' is not fixed, and it is known that lesion-counting is highly subjective, with high degree of inter- and intra-rater variability. Automated methods for lesion quantification hold the potential to make the detection of new and enlarged lesions consistent and repeatable. However, the majority of lesion segmentation algorithms are not evaluated for their ability to separate progressive from stable patients, despite this being a pressing clinical use-case. In this paper we show that change in volumetric measurements of lesion load alone is not a good method for performing this separation, even for highly performing segmentation methods. Instead, we propose a method for identifying lesion changes of high certainty, and establish on a dataset of longitudinal multiple sclerosis cases that this method is able to separate progressive from stable timepoints with a very high level of discrimination (AUC = 0.99), while changes in lesion volume are much less able to perform this separation (AUC = 0.71). Validation of the method on a second external dataset confirms that the method is able to generalize beyond the setting in which it was trained, achieving an accuracy of 83% in separating stable and progressive timepoints. Both lesion volume and count have previously been shown to be strong predictors of disease course across a population. However, we demonstrate that for individual patients, changes in these measures are not an adequate means of establishing no evidence of disease activity. Meanwhile, directly detecting tissue which changes, with high confidence, from non-lesion to lesion is a feasible methodology for identifying radiologically active patients.



### Comparative Analysis of Automatic Skin Lesion Segmentation with Two Different Implementations
- **Arxiv ID**: http://arxiv.org/abs/1904.03075v1
- **DOI**: None
- **Categories**: **cs.CV**, 68U10, I.4.6; I.5.3
- **Links**: [PDF](http://arxiv.org/pdf/1904.03075v1)
- **Published**: 2019-04-05 14:05:24+00:00
- **Updated**: 2019-04-05 14:05:24+00:00
- **Authors**: Md. Kamrul Hasan, Basel Alyafi, Fakrul Islam Tushar
- **Comment**: 4 pages, 4 figures, 4 tables, 4 sections
- **Journal**: None
- **Summary**: Lesion segmentation from the surrounding skin is the first task for developing automatic Computer-Aided Diagnosis of skin cancer. Variant features of lesion like uneven distribution of color, irregular shape, border and texture make this task challenging. The contribution of this paper is to present and compare two different approaches to skin lesion segmentation. The first approach uses watershed, while the second approach uses mean-shift. Pre-processing steps were performed in both approaches for removing hair and dark borders of microscopic images. The Evaluation of the proposed approaches was performed using Jaccard Index (Intersection over Union or IoU). An additional contribution of this paper is to present pipelines for performing pre-processing and segmentation applying existing segmentation and morphological algorithms which led to promising results. On average, the first approach showed better performance than the second one with average Jaccard Index over 200 ISIC-2017 challenge images are 89.16% and 76.94% respectively.



### SDC - Stacked Dilated Convolution: A Unified Descriptor Network for Dense Matching Tasks
- **Arxiv ID**: http://arxiv.org/abs/1904.03076v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03076v1)
- **Published**: 2019-04-05 14:07:02+00:00
- **Updated**: 2019-04-05 14:07:02+00:00
- **Authors**: René Schuster, Oliver Wasenmüller, Christian Unger, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Dense pixel matching is important for many computer vision tasks such as disparity and flow estimation. We present a robust, unified descriptor network that considers a large context region with high spatial variance. Our network has a very large receptive field and avoids striding layers to maintain spatial resolution. These properties are achieved by creating a novel neural network layer that consists of multiple, parallel, stacked dilated convolutions (SDC). Several of these layers are combined to form our SDC descriptor network. In our experiments, we show that our SDC features outperform state-of-the-art feature descriptors in terms of accuracy and robustness. In addition, we demonstrate the superior performance of SDC in state-of-the-art stereo matching, optical flow and scene flow algorithms on several famous public benchmarks.



### Controlling Neural Networks via Energy Dissipation
- **Arxiv ID**: http://arxiv.org/abs/1904.03081v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03081v2)
- **Published**: 2019-04-05 14:13:55+00:00
- **Updated**: 2019-08-20 11:54:46+00:00
- **Authors**: Michael Moeller, Thomas Möllenhoff, Daniel Cremers
- **Comment**: Published as a conference paper at ICCV 2019, Seoul
- **Journal**: None
- **Summary**: The last decade has shown a tremendous success in solving various computer vision problems with the help of deep learning techniques. Lately, many works have demonstrated that learning-based approaches with suitable network architectures even exhibit superior performance for the solution of (ill-posed) image reconstruction problems such as deblurring, super-resolution, or medical image reconstruction. The drawback of purely learning-based methods, however, is that they cannot provide provable guarantees for the trained network to follow a given data formation process during inference. In this work we propose energy dissipating networks that iteratively compute a descent direction with respect to a given cost function or energy at the currently estimated reconstruction. Therefore, an adaptive step size rule such as a line-search, along with a suitable number of iterations can guarantee the reconstruction to follow a given data formation model encoded in the energy to arbitrary precision, and hence control the model's behavior even during test time. We prove that under standard assumptions, descent using the direction predicted by the network converges (linearly) to the global minimum of the energy. We illustrate the effectiveness of the proposed approach in experiments on single image super resolution and computed tomography (CT) reconstruction, and further illustrate extensions to convex feasibility problems.



### Radiotherapy Target Contouring with Convolutional Gated Graph Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.03086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03086v1)
- **Published**: 2019-04-05 14:28:55+00:00
- **Updated**: 2019-04-05 14:28:55+00:00
- **Authors**: Chun-Hung Chao, Yen-Chi Cheng, Hsien-Tzu Cheng, Chi-Wen Huang, Tsung-Ying Ho, Chen-Kan Tseng, Le Lu, Min Sun
- **Comment**: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018. Version
  2
- **Journal**: None
- **Summary**: Tomography medical imaging is essential in the clinical workflow of modern cancer radiotherapy. Radiation oncologists identify cancerous tissues, applying delineation on treatment regions throughout all image slices. This kind of task is often formulated as a volumetric segmentation task by means of 3D convolutional networks with considerable computational cost. Instead, inspired by the treating methodology of considering meaningful information across slices, we used Gated Graph Neural Network to frame this problem more efficiently. More specifically, we propose convolutional recurrent Gated Graph Propagator (GGP) to propagate high-level information through image slices, with learnable adjacency weighted matrix. Furthermore, as physicians often investigate a few specific slices to refine their decision, we model this slice-wise interaction procedure to further improve our segmentation result. This can be set by editing any slice effortlessly as updating predictions of other slices using GGP. To evaluate our method, we collect an Esophageal Cancer Radiotherapy Target Treatment Contouring dataset of 81 patients which includes tomography images with radiotherapy target. On this dataset, our convolutional graph network produces state-of-the-art results and outperforms the baselines. With the addition of interactive setting, performance is improved even further. Our method has the potential to be easily applied to diverse kinds of medical tasks with volumetric images. Incorporating both the ability to make a feasible prediction and to consider the human interactive input, the proposed method is suitable for clinical scenarios.



### PerfVis: Pervasive Visualization in Immersive AugmentedReality for Performance Awareness
- **Arxiv ID**: http://arxiv.org/abs/1904.06399v1
- **DOI**: 10.1145/3302541.3313104
- **Categories**: **cs.HC**, cs.CV, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1904.06399v1)
- **Published**: 2019-04-05 14:59:01+00:00
- **Updated**: 2019-04-05 14:59:01+00:00
- **Authors**: Leonel Merino, Mario Hess, Alexandre Bergel, Oscar Nierstrasz, Daniel Weiskopf
- **Comment**: ICPE'19 vision, 4 pages, 2 figure, conference
- **Journal**: None
- **Summary**: Developers are usually unaware of the impact of code changes to the performance of software systems. Although developers can analyze the performance of a system by executing, for instance, a performance test to compare the performance of two consecutive versions of the system, changing from a programming task to a testing task would disrupt the development flow. In this paper, we propose the use of a city visualization that dynamically provides developers with a pervasive view of the continuous performance of a system. We use an immersive augmented reality device (Microsoft HoloLens) to display our visualization and extend the integrated development environment on a computer screen to use the physical space. We report on technical details of the design and implementation of our visualization tool, and discuss early feedback that we collected of its usability. Our investigation explores a new visual metaphor to support the exploration and analysis of possibly very large and multidimensional performance data. Our initial result indicates that the city metaphor can be adequate to analyze dynamic performance data on a large and non-trivial software system.



### 3DQ: Compact Quantized Neural Networks for Volumetric Whole Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.03110v3
- **DOI**: 10.1007/978-3-030-32248-9_49
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03110v3)
- **Published**: 2019-04-05 15:09:07+00:00
- **Updated**: 2019-07-01 11:07:17+00:00
- **Authors**: Magdalini Paschali, Stefano Gasperini, Abhijit Guha Roy, Michael Y. -S. Fang, Nassir Navab
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Model architectures have been dramatically increasing in size, improving performance at the cost of resource requirements. In this paper we propose 3DQ, a ternary quantization method, applied for the first time to 3D Fully Convolutional Neural Networks (F-CNNs), enabling 16x model compression while maintaining performance on par with full precision models. We extensively evaluate 3DQ on two datasets for the challenging task of whole brain segmentation. Additionally, we showcase our method's ability to generalize on two common 3D architectures, namely 3D U-Net and V-Net. Outperforming a variety of baselines, the proposed method is capable of compressing large 3D models to a few MBytes, alleviating the storage needs in space critical applications.



### Fast Weakly Supervised Action Segmentation Using Mutual Consistency
- **Arxiv ID**: http://arxiv.org/abs/1904.03116v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03116v4)
- **Published**: 2019-04-05 15:19:35+00:00
- **Updated**: 2021-06-10 21:31:13+00:00
- **Authors**: Yaser Souri, Mohsen Fayyaz, Luca Minciullo, Gianpiero Francesca, Juergen Gall
- **Comment**: Accepted for publication at TPAMI (IEEE Transactions on Pattern
  Analysis and Machine Intelligence) in 2021. First two authors contributed
  equally
- **Journal**: None
- **Summary**: Action segmentation is the task of predicting the actions for each frame of a video. As obtaining the full annotation of videos for action segmentation is expensive, weakly supervised approaches that can learn only from transcripts are appealing. In this paper, we propose a novel end-to-end approach for weakly supervised action segmentation based on a two-branch neural network. The two branches of our network predict two redundant but different representations for action segmentation and we propose a novel mutual consistency (MuCon) loss that enforces the consistency of the two redundant representations. Using the MuCon loss together with a loss for transcript prediction, our proposed approach achieves the accuracy of state-of-the-art approaches while being $14$ times faster to train and $20$ times faster during inference. The MuCon loss proves beneficial even in the fully supervised setting.



### Leaf segmentation through the classification of edges
- **Arxiv ID**: http://arxiv.org/abs/1904.03124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03124v1)
- **Published**: 2019-04-05 15:39:00+00:00
- **Updated**: 2019-04-05 15:39:00+00:00
- **Authors**: Jonathan Bell, Hannah M. Dee
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to leaf level segmentation of images of Arabidopsis thaliana plants based upon detected edges. We introduce a novel approach to edge classification, which forms an important part of a method to both count the leaves and establish the leaf area of a growing plant from images obtained in a high-throughput phenotyping system. Our technique uses a relatively shallow convolutional neural network to classify image edges as background, plant edge, leaf-on-leaf edge or internal leaf noise. The edges themselves were found using the Canny edge detector and the classified edges can be used with simple image processing techniques to generate a region-based segmentation in which the leaves are distinct. This approach is strong at distinguishing occluding pairs of leaves where one leaf is largely hidden, a situation which has proved troublesome for plant image analysis systems in the past. In addition, we introduce the publicly available plant image dataset that was used for this work.



### Deep Learning Under the Microscope: Improving the Interpretability of Medical Imaging Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03127v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03127v2)
- **Published**: 2019-04-05 15:41:12+00:00
- **Updated**: 2019-04-16 08:01:09+00:00
- **Authors**: Magdalini Paschali, Muhammad Ferjad Naeem, Walter Simson, Katja Steiger, Martin Mollenhauer, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel interpretation method tailored to histological Whole Slide Image (WSI) processing. A Deep Neural Network (DNN), inspired by Bag-of-Features models is equipped with a Multiple Instance Learning (MIL) branch and trained with weak supervision for WSI classification. MIL avoids label ambiguity and enhances our model's expressive power without guiding its attention. We utilize a fine-grained logit heatmap of the models activations to interpret its decision-making process. The proposed method is quantitatively and qualitatively evaluated on two challenging histology datasets, outperforming a variety of baselines. In addition, two expert pathologists were consulted regarding the interpretability provided by our method and acknowledged its potential for integration into several clinical applications.



### Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.03137v4
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03137v4)
- **Published**: 2019-04-05 16:02:15+00:00
- **Updated**: 2019-12-02 14:46:07+00:00
- **Authors**: Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jähnichen, Moin Nabi
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Models trained in the context of continual learning (CL) should be able to learn from a stream of data over an undefined period of time. The main challenges herein are: 1) maintaining old knowledge while simultaneously benefiting from it when learning new tasks, and 2) guaranteeing model scalability with a growing amount of data to learn from. In order to tackle these challenges, we introduce Dynamic Generative Memory (DGM) - a synaptic plasticity driven framework for continual learning. DGM relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking. Specifically, we evaluate two variants of neural masking: applied to (i) layer activations and (ii) to connection weights directly. Furthermore, we propose a dynamic network expansion mechanism that ensures sufficient model capacity to accommodate for continually incoming tasks. The amount of added capacity is determined dynamically from the learned binary mask. We evaluate DGM in the continual class-incremental setup on visual classification tasks.



### Spatial Shortcut Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.03141v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1904.03141v1)
- **Published**: 2019-04-05 16:06:58+00:00
- **Updated**: 2019-04-05 16:06:58+00:00
- **Authors**: Te Qi, Bayram Bayramli, Usman Ali, Qinchuan Zhang, Hongtao Lu
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Like many computer vision problems, human pose estimation is a challenging problem in that recognizing a body part requires not only information from local area but also from areas with large spatial distance. In order to spatially pass information, large convolutional kernels and deep layers have been normally used, introducing high computation cost and large parameter space. Luckily for pose estimation, human body is geometrically structured in images, enabling modeling of spatial dependency. In this paper, we propose a spatial shortcut network for pose estimation task, where information is easier to flow spatially. We evaluate our model with detailed analyses and present its outstanding performance with smaller structure.



### Unsupervised Image Matching and Object Discovery as Optimization
- **Arxiv ID**: http://arxiv.org/abs/1904.03148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03148v1)
- **Published**: 2019-04-05 16:29:44+00:00
- **Updated**: 2019-04-05 16:29:44+00:00
- **Authors**: Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick Perez, Jean Ponce
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Learning with complete or partial supervision is powerful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsupervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object categories among images in a collection, following the work of Cho et al. 2015. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach.



### HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/1904.03167v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.03167v2)
- **Published**: 2019-04-05 17:16:09+00:00
- **Updated**: 2019-09-30 18:49:11+00:00
- **Authors**: Roman Kaskman, Sergey Zakharov, Ivan Shugurov, Slobodan Ilic
- **Comment**: ICCVW 2019
- **Journal**: None
- **Summary**: Among the most important prerequisites for creating and evaluating 6D object pose detectors are datasets with labeled 6D poses. With the advent of deep learning, demand for such datasets is growing continuously. Despite the fact that some of exist, they are scarce and typically have restricted setups, such as a single object per sequence, or they focus on specific object types, such as textureless industrial parts. Besides, two significant components are often ignored: training using only available 3D models instead of real data and scalability, i.e. training one method to detect all objects rather than training one detector per object. Other challenges, such as occlusions, changing light conditions and changes in object appearance, as well precisely defined benchmarks are either not present or are scattered among different datasets. In this paper we present a dataset for 6D pose estimation that covers the above-mentioned challenges, mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. We also present a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter. We also set a baseline for the presented benchmarks using a state-of-the-art DPOD detector. Considering the difficulty of making such datasets, we plan to release the code allowing other researchers to extend this dataset or make their own datasets in the future.



### Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?
- **Arxiv ID**: http://arxiv.org/abs/1904.03189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03189v2)
- **Published**: 2019-04-05 17:31:56+00:00
- **Updated**: 2019-09-03 15:37:25+00:00
- **Authors**: Rameen Abdal, Yipeng Qin, Peter Wonka
- **Comment**: Accepted for oral presentation at ICCV 2019, "For videos visit
  https://youtu.be/RnTXLXw9o_I , https://youtu.be/zJoYY2eHAF0 and
  https://youtu.be/bA893L-PjbI"
- **Journal**: None
- **Summary**: We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHQ dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.



### Moving Object Detection under Discontinuous Change in Illumination Using Tensor Low-Rank and Invariant Sparse Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1904.03175v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03175v2)
- **Published**: 2019-04-05 17:49:37+00:00
- **Updated**: 2019-04-08 01:10:09+00:00
- **Authors**: Moein Shakeri, Hong Zhang
- **Comment**: 14 pages, 14 figures, including supplementary materials
- **Journal**: None
- **Summary**: Although low-rank and sparse decomposition based methods have been successfully applied to the problem of moving object detection using structured sparsity-inducing norms, they are still vulnerable to significant illumination changes that arise in certain applications. We are interested in moving object detection in applications involving time-lapse image sequences for which current methods mistakenly group moving objects and illumination changes into foreground. Our method relies on the multilinear (tensor) data low-rank and sparse decomposition framework to address the weaknesses of existing methods. The key to our proposed method is to create first a set of prior maps that can characterize the changes in the image sequence due to illumination. We show that they can be detected by a k-support norm. To deal with concurrent, two types of changes, we employ two regularization terms, one for detecting moving objects and the other for accounting for illumination changes, in the tensor low-rank and sparse decomposition formulation. Through comprehensive experiments using challenging datasets, we show that our method demonstrates a remarkable ability to detect moving objects under discontinuous change in illumination, and outperforms the state-of-the-art solutions to this challenging problem.



### Detecting Human-Object Interactions via Functional Generalization
- **Arxiv ID**: http://arxiv.org/abs/1904.03181v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03181v3)
- **Published**: 2019-04-05 17:58:54+00:00
- **Updated**: 2020-09-02 06:28:11+00:00
- **Authors**: Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa
- **Comment**: AAAI 2020
- **Journal**: None
- **Summary**: We present an approach for detecting human-object interactions (HOIs) in images, based on the idea that humans interact with functionally similar objects in a similar manner. The proposed model is simple and efficiently uses the data, visual features of the human, relative spatial orientation of the human and the object, and the knowledge that functionally similar objects take part in similar interactions with humans. We provide extensive experimental validation for our approach and demonstrate state-of-the-art results for HOI detection. On the HICO-Det dataset our method achieves a gain of over 2.5% absolute points in mean average precision (mAP) over state-of-the-art. We also show that our approach leads to significant performance gains for zero-shot HOI detection in the seen object setting. We further demonstrate that using a generic object detector, our model can generalize to interactions involving previously unseen objects.



### Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.03208v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03208v3)
- **Published**: 2019-04-05 18:04:40+00:00
- **Updated**: 2019-09-27 18:14:45+00:00
- **Authors**: Qing Liu, Lingxi Xie, Huiyu Wang, Alan Yuille
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Sketch-based image retrieval (SBIR) is widely recognized as an important vision problem which implies a wide range of real-world applications. Recently, research interests arise in solving this problem under the more realistic and challenging setting of zero-shot learning. In this paper, we investigate this problem from the viewpoint of domain adaptation which we show is critical in improving feature embedding in the zero-shot scenario. Based on a framework which starts with a pre-trained model on ImageNet and fine-tunes it on the training set of SBIR benchmark, we advocate the importance of preserving previously acquired knowledge, e.g., the rich discriminative features learned from ImageNet, to improve the model's transfer ability. For this purpose, we design an approach named Semantic-Aware Knowledge prEservation (SAKE), which fine-tunes the pre-trained model in an economical way and leverages semantic information, e.g., inter-class relationship, to achieve the goal of knowledge preservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin and Sketchy, verify the superior performance of our approach. Extensive diagnostic experiments validate that knowledge preserved benefits SBIR in zero-shot settings, as a large fraction of the performance gain is from the more properly structured feature embedding for photo images. Code is available at: https://github.com/qliu24/SAKE.



### The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.03215v4
- **DOI**: 10.1007/s11263-021-01511-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03215v4)
- **Published**: 2019-04-05 18:17:25+00:00
- **Updated**: 2021-09-16 10:50:36+00:00
- **Authors**: Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, Cesar Cadena
- **Comment**: bechmark website: https://fishyscapes.com/
- **Journal**: None
- **Summary**: Deep learning has enabled impressive progress in the accuracy of semantic segmentation. Yet, the ability to estimate uncertainty and detect failure is key for safety-critical applications like autonomous driving. Existing uncertainty estimates have mostly been evaluated on simple tasks, and it is unclear whether these methods generalize to more complex scenarios. We present Fishyscapes, the first public benchmark for uncertainty estimation in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects in front of the vehicle. We~adapt state-of-the-art methods to recent semantic segmentation models and compare approaches based on softmax confidence, Bayesian learning, and embedding density. Our results show that anomaly detection is far from solved even for ordinary situations, while our benchmark allows measuring advancements beyond the state-of-the-art.



### ShapeMask: Learning to Segment Novel Objects by Refining Shape Priors
- **Arxiv ID**: http://arxiv.org/abs/1904.03239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03239v1)
- **Published**: 2019-04-05 19:03:26+00:00
- **Updated**: 2019-04-05 19:03:26+00:00
- **Authors**: Weicheng Kuo, Anelia Angelova, Jitendra Malik, Tsung-Yi Lin
- **Comment**: None
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019,
  pp. 9207-9216
- **Summary**: Instance segmentation aims to detect and segment individual objects in a scene. Most existing methods rely on precise mask annotations of every category. However, it is difficult and costly to segment objects in novel categories because a large number of mask annotations is required. We introduce ShapeMask, which learns the intermediate concept of object shape to address the problem of generalization in instance segmentation to novel categories. ShapeMask starts with a bounding box detection and gradually refines it by first estimating the shape of the detected object through a collection of shape priors. Next, ShapeMask refines the coarse shape into an instance level mask by learning instance embeddings. The shape priors provide a strong cue for object-like prediction, and the instance embeddings model the instance specific appearance information. ShapeMask significantly outperforms the state-of-the-art by 6.4 and 3.8 AP when learning across categories, and obtains competitive performance in the fully supervised setting. It is also robust to inaccurate detections, decreased model capacity, and small training data. Moreover, it runs efficiently with 150ms inference time and trains within 11 hours on TPUs. With a larger backbone model, ShapeMask increases the gap with state-of-the-art to 9.4 and 6.2 AP across categories. Code will be released.



### Attention Distillation for Learning Video Representations
- **Arxiv ID**: http://arxiv.org/abs/1904.03249v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03249v2)
- **Published**: 2019-04-05 19:43:08+00:00
- **Updated**: 2020-08-14 19:42:37+00:00
- **Authors**: Miao Liu, Xin Chen, Yun Zhang, Yin Li, James M. Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: We address the challenging problem of learning motion representations using deep models for video recognition. To this end, we make use of attention modules that learn to highlight regions in the video and aggregate features for recognition. Specifically, we propose to leverage output attention maps as a vehicle to transfer the learned representation from a motion (flow) network to an RGB network. We systematically study the design of attention modules, and develop a novel method for attention distillation. Our method is evaluated on major action benchmarks, and consistently improves the performance of the baseline RGB network by a significant margin. Moreover, we demonstrate that our attention maps can leverage motion cues in learning to identify the location of actions in video frames. We believe our method provides a step towards learning motion-aware representations in deep models. Our project page is available at https://aptx4869lm.github.io/AttentionDistillation/



### Is 'Unsupervised Learning' a Misconceived Term?
- **Arxiv ID**: http://arxiv.org/abs/1904.03259v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03259v1)
- **Published**: 2019-04-05 20:05:46+00:00
- **Updated**: 2019-04-05 20:05:46+00:00
- **Authors**: Stephen G. Odaibo
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Is all of machine learning supervised to some degree? The field of machine learning has traditionally been categorized pedagogically into $supervised~vs~unsupervised~learning$; where supervised learning has typically referred to learning from labeled data, while unsupervised learning has typically referred to learning from unlabeled data. In this paper, we assert that all machine learning is in fact supervised to some degree, and that the scope of supervision is necessarily commensurate to the scope of learning potential. In particular, we argue that clustering algorithms such as k-means, and dimensionality reduction algorithms such as principal component analysis, variational autoencoders, and deep belief networks are each internally supervised by the data itself to learn their respective representations of its features. Furthermore, these algorithms are not capable of external inference until their respective outputs (clusters, principal components, or representation codes) have been identified and externally labeled in effect. As such, they do not suffice as examples of unsupervised learning. We propose that the categorization `supervised vs unsupervised learning' be dispensed with, and instead, learning algorithms be categorized as either $internally~or~externally~supervised$ (or both). We believe this change in perspective will yield new fundamental insights into the structure and character of data and of learning algorithms.



### A Variational Auto-Encoder Model for Stochastic Point Processes
- **Arxiv ID**: http://arxiv.org/abs/1904.03273v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03273v1)
- **Published**: 2019-04-05 20:49:16+00:00
- **Updated**: 2019-04-05 20:49:16+00:00
- **Authors**: Nazanin Mehrasa, Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori
- **Comment**: CVPR 19
- **Journal**: None
- **Summary**: We propose a novel probabilistic generative model for action sequences. The model is termed the Action Point Process VAE (APP-VAE), a variational auto-encoder that can capture the distribution over the times and categories of action sequences. Modeling the variety of possible action sequences is a challenge, which we show can be addressed via the APP-VAE's use of latent representations and non-linear functions to parameterize distributions over which event is likely to occur next in a sequence and at what time. We empirically validate the efficacy of APP-VAE for modeling action sequences on the MultiTHUMOS and Breakfast datasets.



### AMASS: Archive of Motion Capture as Surface Shapes
- **Arxiv ID**: http://arxiv.org/abs/1904.03278v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.03278v1)
- **Published**: 2019-04-05 21:00:25+00:00
- **Updated**: 2019-04-05 21:00:25+00:00
- **Authors**: Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, Michael J. Black
- **Comment**: 12 pages, 9 figures, 1 table
- **Journal**: None
- **Summary**: Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model; here we use SMPL [doi:10.1145/2816795.2818013], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11,000 motions, and will be publicly available to the research community.



### Prediction-Tracking-Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.03280v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03280v1)
- **Published**: 2019-04-05 21:05:46+00:00
- **Updated**: 2019-04-05 21:05:46+00:00
- **Authors**: Jianren Wang, Yihui He, Xiaobo Wang, Xinjia Yu, Xia Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a prediction driven method for visual tracking and segmentation in videos. Instead of solely relying on matching with appearance cues for tracking, we build a predictive model which guides finding more accurate tracking regions efficiently. With the proposed prediction mechanism, we improve the model robustness against distractions and occlusions during tracking. We demonstrate significant improvements over state-of-the-art methods not only on visual tracking tasks (VOT 2016 and VOT 2018) but also on video segmentation datasets (DAVIS 2016 and DAVIS 2017).



### Weakly Supervised Video Moment Retrieval From Text Queries
- **Arxiv ID**: http://arxiv.org/abs/1904.03282v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.03282v2)
- **Published**: 2019-04-05 21:11:25+00:00
- **Updated**: 2019-09-04 23:03:18+00:00
- **Authors**: Niluthpol Chowdhury Mithun, Sujoy Paul, Amit K. Roy-Chowdhury
- **Comment**: Revised Table 1 in Page 6, A small bug related to rounding resulted
  in a slightly improved score in the previous version. Our conclusion remains
  the same after the update
- **Journal**: None
- **Summary**: There have been a few recent methods proposed in text to video moment retrieval using natural language queries, but requiring full supervision during training. However, acquiring a large number of training videos with temporal boundary annotations for each text description is extremely time-consuming and often not scalable. In order to cope with this issue, in this work, we introduce the problem of learning from weak labels for the task of text to video moment retrieval. The weak nature of the supervision is because, during training, we only have access to the video-text pairs rather than the temporal extent of the video to which different text descriptions relate. We propose a joint visual-semantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. Specifically, our main idea is to utilize latent alignment between video frames and sentence descriptions using Text-Guided Attention (TGA). TGA is then used during the test phase to retrieve relevant moments. Experiments on two benchmark datasets demonstrate that our method achieves comparable performance to state-of-the-art fully supervised approaches.



### Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.03285v4
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1904.03285v4)
- **Published**: 2019-04-05 21:26:39+00:00
- **Updated**: 2019-09-21 17:13:50+00:00
- **Authors**: Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, Giedrius Burachas
- **Comment**: 2019 AAAI Conference on Human Computation and Crowdsourcing
- **Journal**: 2019 AAAI Conference on Human Computation and Crowdsourcing
- **Summary**: While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as "helpful" significantly improve human performance, "incorrect" and "unhelpful" explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanations on a human-AI collaborative task.



### In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate 3D Representations
- **Arxiv ID**: http://arxiv.org/abs/1904.03289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03289v1)
- **Published**: 2019-04-05 21:37:55+00:00
- **Updated**: 2019-04-05 21:37:55+00:00
- **Authors**: Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Gerard Pons-Moll, Christian Theobalt
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Convolutional Neural Network based approaches for monocular 3D human pose estimation usually require a large amount of training images with 3D pose annotations. While it is feasible to provide 2D joint annotations for large corpora of in-the-wild images with humans, providing accurate 3D annotations to such in-the-wild corpora is hardly feasible in practice. Most existing 3D labelled data sets are either synthetically created or feature in-studio images. 3D pose estimation algorithms trained on such data often have limited ability to generalize to real world scene diversity. We therefore propose a new deep learning based method for monocular 3D human pose estimation that shows high accuracy and generalizes better to in-the-wild scenes. It has a network architecture that comprises a new disentangled hidden space encoding of explicit 2D and 3D features, and uses supervision by a new learned projection model from predicted 3D pose. Our algorithm can be jointly trained on image data with 3D labels and image data with only 2D labels. It achieves state-of-the-art accuracy on challenging in-the-wild data.



### Revealing Scenes by Inverting Structure from Motion Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1904.03303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03303v1)
- **Published**: 2019-04-05 22:03:42+00:00
- **Updated**: 2019-04-05 22:03:42+00:00
- **Authors**: Francesco Pittaluga, Sanjeev J. Koppal, Sing Bing Kang, Sudipta N. Sinha
- **Comment**: 10 pages, 8 figures, to be published in IEEE Conference on Computer
  Vision and Pattern Recognition 2019
- **Journal**: None
- **Summary**: Many 3D vision systems localize cameras within a scene using 3D point clouds. Such point clouds are often obtained using structure from motion (SfM), after which the images are discarded to preserve privacy. In this paper, we show, for the first time, that such point clouds retain enough information to reveal scene appearance and compromise privacy. We present a privacy attack that reconstructs color images of the scene from the point cloud. Our method is based on a cascaded U-Net that takes as input, a 2D multichannel image of the points rendered from a specific viewpoint containing point depth and optionally color and SIFT descriptors and outputs a color image of the scene from that viewpoint. Unlike previous feature inversion methods, we deal with highly sparse and irregular 2D point distributions and inputs where many point attributes are missing, namely keypoint orientation and scale, the descriptor image source and the 3D point visibility. We evaluate our attack algorithm on public datasets and analyze the significance of the point cloud attributes. Finally, we show that novel views can also be generated thereby enabling compelling virtual tours of the underlying scene.



### Convolutional Relational Machine for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.03308v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03308v1)
- **Published**: 2019-04-05 22:26:47+00:00
- **Updated**: 2019-04-05 22:26:47+00:00
- **Authors**: Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad Nickabadi, Alexandre Alahi
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end deep Convolutional Neural Network called Convolutional Relational Machine (CRM) for recognizing group activities that utilizes the information in spatial relations between individual persons in image or video. It learns to produce an intermediate spatial representation (activity map) based on individual and group activities. A multi-stage refinement component is responsible for decreasing the incorrect predictions in the activity map. Finally, an aggregation component uses the refined information to recognize group activities. Experimental results demonstrate the constructive contribution of the information extracted and represented in the form of the activity map. CRM shows advantages over state-of-the-art models on Volleyball and Collective Activity datasets.



