# Arxiv Papers in cs.CV on 2019-04-14
### LiveSketch: Query Perturbations for Guided Sketch-based Visual Search
- **Arxiv ID**: http://arxiv.org/abs/1904.06611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1904.06611v1)
- **Published**: 2019-04-14 00:33:15+00:00
- **Updated**: 2019-04-14 00:33:15+00:00
- **Authors**: John Collomosse, Tu Bui, Hailin Jin
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: LiveSketch is a novel algorithm for searching large image collections using hand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch search by creating visual suggestions that augment the query as it is drawn, making query specification an iterative rather than one-shot process that helps disambiguate users' search intent. Our technical contributions are: a triplet convnet architecture that incorporates an RNN based variational autoencoder to search for images using vector (stroke-based) queries; real-time clustering to identify likely search intents (and so, targets within the search embedding); and the use of backpropagation from those targets to perturb the input stroke sequence, so suggesting alterations to the query in order to guide the search. We show improvements in accuracy and time-to-task over contemporary baselines using a 67M image corpus.



### Biphasic Learning of GANs for High-Resolution Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.06624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06624v1)
- **Published**: 2019-04-14 04:16:04+00:00
- **Updated**: 2019-04-14 04:16:04+00:00
- **Authors**: Jie Cao, Huaibo Huang, Yi Li, Jingtuo Liu, Ran He, Zhenan Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Despite that the performance of image-to-image translation has been significantly improved by recent progress in generative models, current methods still suffer from severe degradation in training stability and sample quality when applied to the high-resolution situation. In this work, we present a novel training framework for GANs, namely biphasic learning, to achieve image-to-image translation in multiple visual domains at $1024^2$ resolution. Our core idea is to design an adjustable objective function that varies across training phases. Within the biphasic learning framework, we propose a novel inherited adversarial loss to achieve the enhancement of model capacity and stabilize the training phase transition. Furthermore, we introduce a perceptual-level consistency loss through mutual information estimation and maximization. To verify the superiority of the proposed method, we apply it to a wide range of face-related synthesis tasks and conduct experiments on multiple large-scale datasets. Through comprehensive quantitative analyses, we demonstrate that our method significantly outperforms existing methods.



### Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.06627v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06627v3)
- **Published**: 2019-04-14 04:46:25+00:00
- **Updated**: 2020-03-23 03:54:56+00:00
- **Authors**: Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, Matthew R. Scott
- **Comment**: Accepted CVPR 2019, rewrite main method to be more clear
- **Journal**: None
- **Summary**: A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE\cite{Kim_2018_ECCV} and HTL by a large margin: 60.6% to 65.7% on CUB200, and 80.9% to 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github.com/MalongTech/research-ms-loss.



### Unsupervised Synthesis of Anomalies in Videos: Transforming the Normal
- **Arxiv ID**: http://arxiv.org/abs/1904.06633v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06633v1)
- **Published**: 2019-04-14 05:49:43+00:00
- **Updated**: 2019-04-14 05:49:43+00:00
- **Authors**: Abhishek Joshi, Vinay P. Namboodiri
- **Comment**: Accepted in IJCNN 2019
- **Journal**: None
- **Summary**: Abnormal activity recognition requires detection of occurrence of anomalous events that suffer from a severe imbalance in data. In a video, normal is used to describe activities that conform to usual events while the irregular events which do not conform to the normal are referred to as abnormal. It is far more common to observe normal data than to obtain abnormal data in visual surveillance. In this paper, we propose an approach where we can obtain abnormal data by transforming normal data. This is a challenging task that is solved through a multi-stage pipeline approach. We utilize a number of techniques from unsupervised segmentation in order to synthesize new samples of data that are transformed from an existing set of normal examples. Further, this synthesis approach has useful applications as a data augmentation technique. An incrementally trained Bayesian convolutional neural network (CNN) is used to carefully select the set of abnormal samples that can be added. Finally through this synthesis approach we obtain a comparable set of abnormal samples that can be used for training the CNN for the classification of normal vs abnormal samples. We show that this method generalizes to multiple settings by evaluating it on two real world datasets and achieves improved performance over other probabilistic techniques that have been used in the past for this task.



### Localizing Discriminative Visual Landmarks for Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.06635v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06635v1)
- **Published**: 2019-04-14 06:05:54+00:00
- **Updated**: 2019-04-14 06:05:54+00:00
- **Authors**: Zhe Xin, Yinghao Cai, Tao Lu, Xiaoxia Xing, Shaojun Cai, Jixiang Zhang, Yiping Yang, Yanqing Wang
- **Comment**: 7 pages, 8 figures, ICRA 2019
- **Journal**: None
- **Summary**: We address the problem of visual place recognition with perceptual changes. The fundamental problem of visual place recognition is generating robust image representations which are not only insensitive to environmental changes but also distinguishable to different places. Taking advantage of the feature extraction ability of Convolutional Neural Networks (CNNs), we further investigate how to localize discriminative visual landmarks that positively contribute to the similarity measurement, such as buildings and vegetations. In particular, a Landmark Localization Network (LLN) is designed to indicate which regions of an image are used for discrimination. Detailed experiments are conducted on open source datasets with varied appearance and viewpoint changes. The proposed approach achieves superior performance against state-of-the-art methods.



### A Hybrid Traffic Speed Forecasting Approach Integrating Wavelet Transform and Motif-based Graph Convolutional Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.06656v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1904.06656v1)
- **Published**: 2019-04-14 08:40:58+00:00
- **Updated**: 2019-04-14 08:40:58+00:00
- **Authors**: Na Zhang, Xuefeng Guan, Jun Cao, Xinglei Wang, Huayi Wu
- **Comment**: 7 pages, IJCAI19
- **Journal**: None
- **Summary**: Traffic forecasting is crucial for urban traffic management and guidance. However, existing methods rarely exploit the time-frequency properties of traffic speed observations, and often neglect the propagation of traffic flows from upstream to downstream road segments. In this paper, we propose a hybrid approach that learns the spatio-temporal dependency in traffic flows and predicts short-term traffic speeds on a road network. Specifically, we employ wavelet transform to decompose raw traffic data into several components with different frequency sub-bands. A Motif-based Graph Convolutional Recurrent Neural Network (Motif-GCRNN) and Auto-Regressive Moving Average (ARMA) are used to train and predict low-frequency components and high-frequency components, respectively. In the Motif-GCRNN framework, we integrate Graph Convolutional Networks (GCNs) with local sub-graph structures - Motifs - to capture the spatial correlations among road segments, and apply Long Short-Term Memory (LSTM) to extract the short-term and periodic patterns in traffic speeds. Experiments on a traffic dataset collected in Chengdu, China, demonstrate that the proposed hybrid method outperforms six state-of-art prediction methods.



### EXPERTNet Exigent Features Preservative Network for Facial Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.06658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06658v1)
- **Published**: 2019-04-14 08:45:42+00:00
- **Updated**: 2019-04-14 08:45:42+00:00
- **Authors**: Monu Verma, Jaspreet Kaur Bhui, Santosh Vipparthi, Girdhari Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expressions have essential cues to infer the humans state of mind, that conveys adequate information to understand individuals actual feelings. Thus, automatic facial expression recognition is an interesting and crucial task to interpret the humans cognitive state through the machine. In this paper, we proposed an Exigent Features Preservative Network (EXPERTNet), to describe the features of the facial expressions. The EXPERTNet extracts only pertinent features and neglect others by using exigent feature (ExFeat) block, mainly comprises of elective layer. Specifically, elective layer selects the desired edge variation features from the previous layer outcomes, which are generated by applying different sized filters as 1 x 1, 3 x 3, 5 x 5 and 7 x 7. Different sized filters aid to elicits both micro and high-level features that enhance the learnability of neurons. ExFeat block preserves the spatial structural information of the facial expression, which allows to discriminate between different classes of facial expressions. Visual representation of the proposed method over different facial expressions shows the learning capability of the neurons of different layers. Experimental and comparative analysis results over four comprehensive datasets CK+, MMI DISFA and GEMEP-FERA, ensures the better performance of the proposed network as compared to existing networks.



### Automatic Target Detection for Sparse Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1904.09030v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09030v3)
- **Published**: 2019-04-14 12:00:54+00:00
- **Updated**: 2020-03-05 15:21:11+00:00
- **Authors**: Ahmad W. Bitar, Jean-Philippe Ovarlez, Loong-Fah Cheong, Ali Chehab
- **Comment**: Accepted for publication in the book "Hyperspectral Image Analysis -
  Advances in Signal Processing and Machine Learning". arXiv admin note: text
  overlap with arXiv:1711.08970, arXiv:1808.06490
- **Journal**: None
- **Summary**: In this work, a novel target detector for hyperspectral imagery is developed. The detector is independent on the unknown covariance matrix, behaves well in large dimensions, distributional free, invariant to atmospheric effects, and does not require a background dictionary to be constructed. Based on a modification of the robust principal component analysis (RPCA), a given hyperspectral image (HSI) is regarded as being made up of the sum of a low-rank background HSI and a sparse target HSI that contains the targets based on a pre-learned target dictionary specified by the user. The sparse component is directly used for the detection, that is, the targets are simply detected at the non-zero entries of the sparse target HSI. Hence, a novel target detector is developed, which is simply a sparse HSI generated automatically from the original HSI, but containing only the targets with the background is suppressed. The detector is evaluated on real experiments, and the results of which demonstrate its effectiveness for hyperspectral target detection especially when the targets are well matched to the surroundings.



### Lunar surface image restoration using U-net based deep neural networks
- **Arxiv ID**: http://arxiv.org/abs/1904.06683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06683v1)
- **Published**: 2019-04-14 12:10:43+00:00
- **Updated**: 2019-04-14 12:10:43+00:00
- **Authors**: Hiya Roy, Subhajit Chaudhury, Toshihiko Yamasaki, Danielle DeLatte, Makiko Ohtake, Tatsuaki Hashimoto
- **Comment**: None
- **Journal**: None
- **Summary**: Image restoration is a technique that reconstructs a feasible estimate of the original image from the noisy observation. In this paper, we present a U-Net based deep neural network model to restore the missing pixels on the lunar surface image in a context-aware fashion, which is often known as image inpainting problem. We use the grayscale image of the lunar surface captured by Multiband Imager (MI) onboard Kaguya satellite for our experiments and the results show that our method can reconstruct the lunar surface image with good visual quality and improved PSNR values.



### Robust and Discriminative Labeling for Multi-label Active Learning Based on Maximum Correntropy Criterion
- **Arxiv ID**: http://arxiv.org/abs/1904.06689v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.06689v1)
- **Published**: 2019-04-14 12:53:57+00:00
- **Updated**: 2019-04-14 12:53:57+00:00
- **Authors**: Bo Du, Zengmao Wang, Lefei Zhang, Liangpei Zhang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-label learning draws great interests in many real world applications. It is a highly costly task to assign many labels by the oracle for one instance. Meanwhile, it is also hard to build a good model without diagnosing discriminative labels. Can we reduce the label costs and improve the ability to train a good model for multi-label learning simultaneously?   Active learning addresses the less training samples problem by querying the most valuable samples to achieve a better performance with little costs. In multi-label active learning, some researches have been done for querying the relevant labels with less training samples or querying all labels without diagnosing the discriminative information. They all cannot effectively handle the outlier labels for the measurement of uncertainty. Since Maximum Correntropy Criterion (MCC) provides a robust analysis for outliers in many machine learning and data mining algorithms, in this paper, we derive a robust multi-label active learning algorithm based on MCC by merging uncertainty and representativeness, and propose an efficient alternating optimization method to solve it. With MCC, our method can eliminate the influence of outlier labels that are not discriminative to measure the uncertainty. To make further improvement on the ability of information measurement, we merge uncertainty and representativeness with the prediction labels of unknown data. It can not only enhance the uncertainty but also improve the similarity measurement of multi-label data with labels information. Experiments on benchmark multi-label data sets have shown a superior performance than the state-of-the-art methods.



### Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.06699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06699v2)
- **Published**: 2019-04-14 14:16:32+00:00
- **Updated**: 2019-04-21 10:48:52+00:00
- **Authors**: Yi Wei, Shaohui Liu, Wang Zhao, Jiwen Lu, Jie Zhou
- **Comment**: 14 pages, 13 figures, to appear in CVPR 2019. Code and data:
  https://github.com/weiyithu/OptimizeMVS
- **Journal**: None
- **Summary**: In this paper, we present a new perspective towards image-based shape generation. Most existing deep learning based shape reconstruction methods employ a single-view deterministic model which is sometimes insufficient to determine a single groundtruth shape because the back part is occluded. In this work, we first introduce a conditional generative network to model the uncertainty for single-view reconstruction. Then, we formulate the task of multi-view reconstruction as taking the intersection of the predicted shape spaces on each single image. We design new differentiable guidance including the front constraint, the diversity constraint, and the consistency loss to enable effective single-view conditional generation and multi-view synthesis. Experimental results and ablation studies show that our proposed approach outperforms state-of-the-art methods on 3D reconstruction test error and demonstrate its generalization ability on real world data.



### VORNet: Spatio-temporally Consistent Video Inpainting for Object Removal
- **Arxiv ID**: http://arxiv.org/abs/1904.06726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06726v1)
- **Published**: 2019-04-14 17:12:53+00:00
- **Updated**: 2019-04-14 17:12:53+00:00
- **Authors**: Ya-Liang Chang, Zhe Yu Liu, Winston Hsu
- **Comment**: Accepted to CVPRW 2019
- **Journal**: None
- **Summary**: Video object removal is a challenging task in video processing that often requires massive human efforts. Given the mask of the foreground object in each frame, the goal is to complete (inpaint) the object region and generate a video without the target object. While recently deep learning based methods have achieved great success on the image inpainting task, they often lead to inconsistent results between frames when applied to videos. In this work, we propose a novel learning-based Video Object Removal Network (VORNet) to solve the video object removal task in a spatio-temporally consistent manner, by combining the optical flow warping and image-based inpainting model. Experiments are done on our Synthesized Video Object Removal (SVOR) dataset based on the YouTube-VOS video segmentation dataset, and both the objective and subjective evaluation demonstrate that our VORNet generates more spatially and temporally consistent videos compared with existing methods.



### Gyroscope-aided Relative Pose Estimation for Rolling Shutter Cameras
- **Arxiv ID**: http://arxiv.org/abs/1904.06770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06770v1)
- **Published**: 2019-04-14 22:07:10+00:00
- **Updated**: 2019-04-14 22:07:10+00:00
- **Authors**: Chang-Ryeol Lee, Ju Hong Yoon, Min-Gyu Park, Kuk-Jin Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: The rolling shutter camera has received great attention due to its low cost imaging capability, however, the estimation of relative pose between rolling shutter cameras still remains a difficult problem owing to its line-by-line image capturing characteristics. To alleviate this problem, we exploit gyroscope measurements, angular velocity, along with image measurement to compute the relative pose between rolling shutter cameras. The gyroscope measurements provide the information about instantaneous motion that causes the rolling shutter distortion. Having gyroscope measurements in one hand, we simplify the relative pose estimation problem and find a minimal solution for the problem based on the Grobner basis polynomial solver. The proposed method requires only five points to compute relative pose between rolling shutter cameras, whereas previous methods require 20 or 44 corresponding points for linear and uniform rolling shutter geometry models, respectively. Experimental results on synthetic and real data verify the superiority of the proposed method over existing relative pose estimation methods.



### See the World through Network Cameras
- **Arxiv ID**: http://arxiv.org/abs/1904.06775v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1904.06775v1)
- **Published**: 2019-04-14 22:37:51+00:00
- **Updated**: 2019-04-14 22:37:51+00:00
- **Authors**: Yung-Hsiang Lu, George K. Thiruvathukal, Ahmed S. Kaseb, Kent Gauen, Damini Rijhwani, Ryan Dailey, Deeptanshu Malik, Yutong Huang, Sarah Aghajanzadeh, Minghao Guo
- **Comment**: This paper is accepted by IEEE Computer for publication
- **Journal**: None
- **Summary**: Millions of network cameras have been deployed worldwide. Real-time data from many network cameras can offer instant views of multiple locations with applications in public safety, transportation management, urban planning, agriculture, forestry, social sciences, atmospheric information, and more. This paper describes the real-time data available from worldwide network cameras and potential applications. Second, this paper outlines the CAM2 System available to users at https://www.cam2project.net/. This information includes strategies to discover network cameras and create the camera database, user interface, and computing platforms. Third, this paper describes many opportunities provided by data from network cameras and challenges to be addressed.



