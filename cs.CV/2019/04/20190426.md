# Arxiv Papers in cs.CV on 2019-04-26
### Weighted second-order cone programming twin support vector machine for imbalanced data classification
- **Arxiv ID**: http://arxiv.org/abs/1904.11634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11634v2)
- **Published**: 2019-04-26 01:04:11+00:00
- **Updated**: 2019-07-07 21:53:41+00:00
- **Authors**: Saeideh Roshanfekr, Shahriar Esmaeili, Hassan Ataeian, Ali Amiri
- **Comment**: This manuscript is under revision at Pattern Recognition Letters
- **Journal**: None
- **Summary**: We propose a method of using a Weighted second-order cone programming twin support vector machine (WSOCP-TWSVM) for imbalanced data classification. This method constructs a graph based under-sampling method which is utilized to remove outliers and reduce the dispensable majority samples. Then, appropriate weights are set in order to decrease the impact of samples of the majority class and increase the effect of the minority class in the optimization formula of the classifier. These weights are embedded in the optimization problem of the Second Order Cone Programming (SOCP) Twin Support Vector Machine formulations. This method is tested, and its performance is compared to previous methods on standard datasets. Results of experiments confirm the feasibility and efficiency of the proposed method.



### Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.12659v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.12659v1)
- **Published**: 2019-04-26 02:20:07+00:00
- **Updated**: 2019-04-26 02:20:07+00:00
- **Authors**: Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higher-order dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, we further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to learn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art methods. As a side product, AS-GCN also shows promising results for future pose prediction.



### Weakly Supervised Instance Learning for Thyroid Malignancy Prediction from Whole Slide Cytopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1904.12739v2
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12739v2)
- **Published**: 2019-04-26 03:03:20+00:00
- **Updated**: 2020-07-30 20:31:50+00:00
- **Authors**: David Dov, Shahar Ziv Kovalsky, Serge Assaad, Avani A. Pendse Jonathan Cohen, Danielle Elliott Range, Ricardo Henao, Lawrence Carin
- **Comment**: None
- **Journal**: None
- **Summary**: We consider machine-learning-based thyroid-malignancy prediction from cytopathology whole-slide images (WSI). Multiple instance learning (MIL) approaches, typically used for the analysis of WSIs, divide the image (bag) into patches (instances), which are used to predict a single bag-level label. These approaches perform poorly in cytopathology slides due to a unique bag structure: sparsely located informative instances with varying characteristics of abnormality. We address these challenges by considering multiple types of labels: bag-level malignancy and ordered diagnostic scores, as well as instance-level informativeness and abnormality labels. We study their contribution beyond the MIL setting by proposing a maximum likelihood estimation (MLE) framework, from which we derive a two-stage deep-learning-based algorithm. The algorithm identifies informative instances and assigns them local malignancy scores that are incorporated into a global malignancy prediction. We derive a lower bound of the MLE, leading to an improved training strategy based on weak supervision, that we motivate through statistical analysis. The lower bound further allows us to extend the proposed algorithm to simultaneously predict multiple bag and instance-level labels from a single output of a neural network. Experimental results demonstrate that the proposed algorithm provides competitive performance compared to several competing methods, achieves (expert) human-level performance, and allows augmentation of human decisions.



### Self Training Autonomous Driving Agent
- **Arxiv ID**: http://arxiv.org/abs/1904.12738v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.12738v1)
- **Published**: 2019-04-26 05:22:29+00:00
- **Updated**: 2019-04-26 05:22:29+00:00
- **Authors**: Shashank Kotyan, Danilo Vasconcellos Vargas, Venkanna U
- **Comment**: None
- **Journal**: None
- **Summary**: Intrinsically, driving is a Markov Decision Process which suits well the reinforcement learning paradigm. In this paper, we propose a novel agent which learns to drive a vehicle without any human assistance. We use the concept of reinforcement learning and evolutionary strategies to train our agent in a 2D simulation environment. Our model's architecture goes beyond the World Model's by introducing difference images in the auto encoder. This novel involvement of difference images in the auto-encoder gives better representation of the latent space with respect to the motion of vehicle and helps an autonomous agent to learn more efficiently how to drive a vehicle. Results show that our method requires fewer (96% less) total agents, (87.5% less) agents per generations, (70% less) generations and (90% less) rollouts than the original architecture while achieving the same accuracy of the original.



### A Survey on Face Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.11685v1
- **DOI**: 10.1007/s00521-020-04748-3
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.4.7; I.4.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1904.11685v1)
- **Published**: 2019-04-26 06:23:35+00:00
- **Updated**: 2019-04-26 06:23:35+00:00
- **Authors**: Xiang Wang, Kai Wang, Shiguo Lian
- **Comment**: 26 pages, 22 figures. Neural Comput & Applic (2020)
- **Journal**: None
- **Summary**: The quality and size of training set have great impact on the results of deep learning-based face related tasks. However, collecting and labeling adequate samples with high quality and balanced distributions still remains a laborious and expensive work, and various data augmentation techniques have thus been widely used to enrich the training dataset. In this paper, we systematically review the existing works of face data augmentation from the perspectives of the transformation types and methods, with the state-of-the-art approaches involved. Among all these approaches, we put the emphasis on the deep learning-based works, especially the generative adversarial networks which have been recognized as more powerful and effective tools in recent years. We present their principles, discuss the results and show their applications as well as limitations. Different evaluation metrics for evaluating these approaches are also introduced. We point out the challenges and opportunities in the field of face data augmentation, and provide brief yet insightful discussions.



### Box-driven Class-wise Region Masking and Filling Rate Guided Loss for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.11693v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11693v1)
- **Published**: 2019-04-26 06:47:58+00:00
- **Updated**: 2019-04-26 06:47:58+00:00
- **Authors**: Chunfeng Song, Yan Huang, Wanli Ouyang, Liang Wang
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Semantic segmentation has achieved huge progress via adopting deep Fully Convolutional Networks (FCN). However, the performance of FCN based models severely rely on the amounts of pixel-level annotations which are expensive and time-consuming. To address this problem, it is a good choice to learn to segment with weak supervision from bounding boxes. How to make full use of the class-level and region-level supervisions from bounding boxes is the critical challenge for the weakly supervised learning task. In this paper, we first introduce a box-driven class-wise masking model (BCM) to remove irrelevant regions of each class. Moreover, based on the pixel-level segment proposal generated from the bounding box supervision, we could calculate the mean filling rates of each class to serve as an important prior cue, then we propose a filling rate guided adaptive loss (FR-Loss) to help the model ignore the wrongly labeled pixels in proposals. Unlike previous methods directly training models with the fixed individual segment proposals, our method can adjust the model learning with global statistical information. Thus it can help reduce the negative impacts from wrongly labeled proposals. We evaluate the proposed method on the challenging PASCAL VOC 2012 benchmark and compare with other methods. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results.



### The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision
- **Arxiv ID**: http://arxiv.org/abs/1904.12584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.12584v1)
- **Published**: 2019-04-26 06:50:54+00:00
- **Updated**: 2019-04-26 06:50:54+00:00
- **Authors**: Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu
- **Comment**: ICLR 2019 (Oral). Project page: http://nscl.csail.mit.edu/
- **Journal**: None
- **Summary**: We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.



### Interactive user interface based on Convolutional Auto-encoders for annotating CT-scans
- **Arxiv ID**: http://arxiv.org/abs/1904.11701v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11701v1)
- **Published**: 2019-04-26 07:45:48+00:00
- **Updated**: 2019-04-26 07:45:48+00:00
- **Authors**: Martin Längkvist, Jonas Widell, Per Thunberg, Amy Loutfi, Mats Lidén
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: High resolution computed tomography (HRCT) is the most important imaging modality for interstitial lung diseases, where the radiologists are interested in identifying certain patterns, and their volumetric and regional distribution. The use of machine learning can assist the radiologists with both these tasks by performing semantic segmentation. In this paper, we propose an interactive annotation-tool for semantic segmentation that assists the radiologist in labeling CT scans. The annotation tool is evaluated by six radiologists and radiology residents classifying healthy lung and reticular pattern i HRCT images. The usability of the system is evaluated with a System Usability Score (SUS) and interaction information from the readers that used the tool for annotating the CT volumes. It was discovered that the experienced usability and how the users interactied with the system differed between the users. A higher SUS-score was given by users that prioritized learning speed over model accuracy and spent less time with manual labeling and instead utilized the suggestions provided by the GUI. An analysis of the annotation variations between the readers show substantial agreement (Cohen's kappa=0.69) for classification of healthy and affected lung parenchyma in pulmonary fibrosis. The inter-reader variation is a challenge for the definition of ground truth.



### Digitally Capturing Physical Prototypes During Early-Stage Engineering Design Projects for Initial Analysis of Project Output and Progression
- **Arxiv ID**: http://arxiv.org/abs/1905.01950v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1905.01950v2)
- **Published**: 2019-04-26 09:04:37+00:00
- **Updated**: 2020-03-19 09:08:16+00:00
- **Authors**: Jorgen F. Erichsen, Heikki Sjöman, Martin Steinert, Torgeir Welo
- **Comment**: 27 pages, 2 tables, 9 figures
- **Journal**: None
- **Summary**: Aiming to help researchers capture output from the early stages of engineering design projects, this article presents a new research tool for digitally capturing physical prototypes. The motivation for this work is to collect observations that can aid in understanding prototyping in the early stages of engineering design projects, and this article investigates if and how digital capture of physical prototypes can be used for this purpose. Early-stage prototypes are usually rough and of low-fidelity and are thus often discarded or substantially modified through the projects. Hence, retrospective access to prototypes is a challenge when trying to gather accurate empirical data. To capture the prototypes developed through the early stages of a project, a new research tool has been developed for capturing prototypes through multi-view images, along with metadata describing by whom, why, when and where the prototypes were captured. Over the course of 17 months, this research tool has been used to capture more than 800 physical prototypes from 76 individual users across many projects. In this article, one project is shown in detail to demonstrate how this capturing system can gather empirical data for enriching engineering design project cases that focus on prototyping for concept generation. The authors also analyse the metadata provided by the system to give understanding into prototyping patterns in the projects. Lastly, through enabling digital capture of large quantities of data, the research tool presents the foundations for training artificial intelligence-based predictors and classifiers that can be used for analysis in engineering design research.



### Deep Fitting Degree Scoring Network for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.12681v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12681v2)
- **Published**: 2019-04-26 09:40:22+00:00
- **Updated**: 2019-06-08 09:49:06+00:00
- **Authors**: Lijie Liu, Jiwen Lu, Chunjing Xu, Qi Tian, Jie Zhou
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose to learn a deep fitting degree scoring network for monocular 3D object detection, which aims to score fitting degree between proposals and object conclusively. Different from most existing monocular frameworks which use tight constraint to get 3D location, our approach achieves high-precision localization through measuring the visual fitting degree between the projected 3D proposals and the object. We first regress the dimension and orientation of the object using an anchor-based method so that a suitable 3D proposal can be constructed. We propose FQNet, which can infer the 3D IoU between the 3D proposals and the object solely based on 2D cues. Therefore, during the detection process, we sample a large number of candidates in the 3D space and project these 3D bounding boxes on 2D image individually. The best candidate can be picked out by simply exploring the spatial overlap between proposals and the object, in the form of the output 3D IoU score of FQNet. Experiments on the KITTI dataset demonstrate the effectiveness of our framework.



### Representation Similarity Analysis for Efficient Task taxonomy & Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.11740v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11740v1)
- **Published**: 2019-04-26 09:43:11+00:00
- **Updated**: 2019-04-26 09:43:11+00:00
- **Authors**: Kshitij Dwivedi, Gemma Roig
- **Comment**: Accepted at CVPR 2019. Code available at
  https://github.com/kshitijd20/RSA-CVPR19-release
- **Journal**: None
- **Summary**: Transfer learning is widely used in deep neural network models when there are few labeled examples available. The common approach is to take a pre-trained network in a similar task and finetune the model parameters. This is usually done blindly without a pre-selection from a set of pre-trained models, or by finetuning a set of models trained on different tasks and selecting the best performing one by cross-validation. We address this problem by proposing an approach to assess the relationship between visual tasks and their task-specific models. Our method uses Representation Similarity Analysis (RSA), which is commonly used to find a correlation between neuronal responses from brain data and models. With RSA we obtain a similarity score among tasks by computing correlations between models trained on different tasks. Our method is efficient as it requires only pre-trained models, and a few images with no further training. We demonstrate the effectiveness and efficiency of our method for generating task taxonomy on Taskonomy dataset. We next evaluate the relationship of RSA with the transfer learning performance on Taskonomy tasks and a new task: Pascal VOC semantic segmentation. Our results reveal that models trained on tasks with higher similarity score show higher transfer learning performance. Surprisingly, the best transfer learning result for Pascal VOC semantic segmentation is not obtained from the pre-trained model on semantic segmentation, probably due to the domain differences, and our method successfully selects the high performing models.



### Current Trends in the Use of Eye Tracking in Mathematics Education Research: A PME Survey
- **Arxiv ID**: http://arxiv.org/abs/1904.12581v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12581v2)
- **Published**: 2019-04-26 11:41:17+00:00
- **Updated**: 2019-06-04 10:50:43+00:00
- **Authors**: Achim J. Lilienthal, Maike Schindler
- **Comment**: It is planned to update this review yearly after the respective PME
  conference
- **Journal**: None
- **Summary**: Eye tracking (ET) is a research method that receives growing interest in mathematics education research (MER). This paper aims to give a literature overview, specifically focusing on the evolution of interest in this technology, ET equipment, and analysis methods used in mathematics education. To capture the current state, we focus on papers published in the proceedings of PME, one of the primary conferences dedicated to MER, of the last ten years. We identify trends in interest, methodology, and methods of analysis that are used in the community, and discuss possible future developments.



### EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association
- **Arxiv ID**: http://arxiv.org/abs/1904.11781v2
- **DOI**: 10.1109/ICCV.2019.00596
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.11781v2)
- **Published**: 2019-04-26 11:54:50+00:00
- **Updated**: 2020-11-24 15:15:03+00:00
- **Authors**: Michael Strecke, Jörg Stückler
- **Comment**: IEEE/CVF International Conference on Computer Vision (ICCV) 2019,
  Project page: https://emfusion.is.tue.mpg.de/, Source code:
  https://github.com/EmbodiedVision/emfusion
- **Journal**: IEEE/CVF International Conference on Computer Vision (ICCV) 2019
- **Summary**: The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy.



### A Large Scale Urban Surveillance Video Dataset for Multiple-Object Tracking and Behavior Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.11784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11784v2)
- **Published**: 2019-04-26 11:58:36+00:00
- **Updated**: 2020-07-28 06:46:58+00:00
- **Authors**: Guojun Yin, Bin Liu, Huihui Zhu, Tao Gong, Nenghai Yu
- **Comment**: 6 pages. This dataset are not available due to the data license
- **Journal**: None
- **Summary**: Multiple-object tracking and behavior analysis have been the essential parts of surveillance video analysis for public security and urban management. With billions of surveillance video captured all over the world, multiple-object tracking and behavior analysis by manual labor are cumbersome and cost expensive. Due to the rapid development of deep learning algorithms in recent years, automatic object tracking and behavior analysis put forward an urgent demand on a large scale well-annotated surveillance video dataset that can reflect the diverse, congested, and complicated scenarios in real applications. This paper introduces an urban surveillance video dataset (USVD) which is by far the largest and most comprehensive. The dataset consists of 16 scenes captured in 7 typical outdoor scenarios: street, crossroads, hospital entrance, school gate, park, pedestrian mall, and public square. Over 200k video frames are annotated carefully, resulting in more than 3:7 million object bounding boxes and about 7:1 thousand trajectories. We further use this dataset to evaluate the performance of typical algorithms for multiple-object tracking and anomaly behavior analysis and explore the robustness of these methods in urban congested scenarios.



### Survey of Computer Vision and Machine Learning in Gastrointestinal Endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1904.13307v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.13307v1)
- **Published**: 2019-04-26 12:46:03+00:00
- **Updated**: 2019-04-26 12:46:03+00:00
- **Authors**: Anant S. Vemuri
- **Comment**: None
- **Journal**: None
- **Summary**: This paper attempts to provide the reader a place to begin studying the application of computer vision and machine learning to gastrointestinal (GI) endoscopy. They have been classified into 18 categories. It should be be noted by the reader that this is a review from pre-deep learning era. A lot of deep learning based applications have not been covered in this thesis.



### Producing Corpora of Medieval and Premodern Occitan
- **Arxiv ID**: http://arxiv.org/abs/1904.11815v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11815v1)
- **Published**: 2019-04-26 12:55:03+00:00
- **Updated**: 2019-04-26 12:55:03+00:00
- **Authors**: Jean-Baptiste Camps, Gilles Guilhem Couffignal
- **Comment**: in French. Actes du XIIe Congr{\`e}s de l'Association internationale
  d'{\'e}tudes occitanes Albi, 2017, Association internationale d'{\'e}tudes
  occitanes (AIEO), Jul 2017, Albi, France
- **Journal**: None
- **Summary**: At a time when the quantity of - more or less freely - available data is increasing significantly, thanks to digital corpora, editions or libraries, the development of data mining tools or deep learning methods allows researchers to build a corpus of study tailored for their research, to enrich their data and to exploit them.Open optical character recognition (OCR) tools can be adapted to old prints, incunabula or even manuscripts, with usable results, allowing the rapid creation of textual corpora. The alternation of training and correction phases makes it possible to improve the quality of the results by rapidly accumulating raw text data. These can then be structured, for example in XML/TEI, and enriched.The enrichment of the texts with graphic or linguistic annotations can also be automated. These processes, known to linguists and functional for modern languages, present difficulties for languages such as Medieval Occitan, due in part to the absence of big enough lemmatized corpora. Suggestions for the creation of tools adapted to the considerable spelling variation of ancient languages will be presented, as well as experiments for the lemmatization of Medieval and Premodern Occitan.These techniques open the way for many exploitations. The much desired increase in the amount of available quality texts and data makes it possible to improve digital philology methods, if everyone takes the trouble to make their data freely available online and reusable.By exposing different technical solutions and some micro-analyses as examples, this paper aims to show part of what digital philology can offer to researchers in the Occitan domain, while recalling the ethical issues on which such practices are based.



### DeepFreak: Learning Crystallography Diffraction Patterns with Automated Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.11834v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.11834v2)
- **Published**: 2019-04-26 13:12:40+00:00
- **Updated**: 2019-05-03 15:11:32+00:00
- **Authors**: Artur Souza, Leonardo B. Oliveira, Sabine Hollatz, Matt Feldman, Kunle Olukotun, James M. Holton, Aina E. Cohen, Luigi Nardi
- **Comment**: None
- **Journal**: None
- **Summary**: Serial crystallography is the field of science that studies the structure and properties of crystals via diffraction patterns. In this paper, we introduce a new serial crystallography dataset comprised of real and synthetic images; the synthetic images are generated through the use of a simulator that is both scalable and accurate. The resulting dataset is called DiffraNet, and it is composed of 25,457 512x512 grayscale labeled images. We explore several computer vision approaches for classification on DiffraNet such as standard feature extraction algorithms associated with Random Forests and Support Vector Machines but also an end-to-end CNN topology dubbed DeepFreak tailored to work on this new dataset. All implementations are publicly available and have been fine-tuned using off-the-shelf AutoML optimization tools for a fair comparison. Our best model achieves 98.5% accuracy on synthetic images and 94.51% accuracy on real images. We believe that the DiffraNet dataset and its classification methods will have in the long term a positive impact in accelerating discoveries in many disciplines, including chemistry, geology, biology, materials science, metallurgy, and physics.



### Unifying Part Detection and Association for Recurrent Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.11864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11864v1)
- **Published**: 2019-04-26 14:22:07+00:00
- **Updated**: 2019-04-26 14:22:07+00:00
- **Authors**: Rania Briq, Andreas Doering, Juergen Gall
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a joint model of human joint detection and association for 2D multi-person pose estimation (MPPE). The approach unifies training of joint detection and association without a need for further processing or sophisticated heuristics in order to associate the joints with people individually. The approach consists of two stages, where in the first stage joint detection heatmaps and association features are extracted, and in the second stage, whose input are the extracted features of the first stage, we introduce a recurrent neural network (RNN) which predicts the heatmaps of a single person's joints in each iteration. In addition, the network learns a stopping criterion in order to halt once it has identified all individuals in the image. This approach allowed us to eliminate several heuristic assumptions and parameters needed for association which do not necessarily hold true. Additionally, such an end-to-end approach allows the final objective to be known and directly optimized over during training. We evaluated our model on the challenging MSCOCO dataset and obtained an improvement over the baseline, particularly in challenging scenes with occlusions.



### Robust Graph Data Learning via Latent Graph Convolutional Representation
- **Arxiv ID**: http://arxiv.org/abs/1904.11883v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11883v2)
- **Published**: 2019-04-26 15:05:45+00:00
- **Updated**: 2021-10-13 04:26:10+00:00
- **Authors**: Bo Jiang, Ziyan Zhang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Representation (GCR) has achieved impressive performance for graph data representation. However, existing GCR is generally defined on the input fixed graph which may restrict the representation capacity and also be vulnerable to the structural attacks and noises. To address this issue, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is derived based on reformulating graph convolutional representation from the aspect of graph neighborhood reconstruction. Given an input graph $\textbf{A}$, LatGCR aims to generate a flexible latent graph $\widetilde{\textbf{A}}$ for graph convolutional representation which obviously enhances the representation capacity and also performs robustly w.r.t graph structural attacks and noises. Moreover, LatGCR is implemented in a self-supervised manner and thus provides a basic block for both supervised and unsupervised graph learning tasks. Experiments on several datasets demonstrate the effectiveness and robustness of LatGCR.



### Perceptual Attention-based Predictive Control
- **Arxiv ID**: http://arxiv.org/abs/1904.11898v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1904.11898v2)
- **Published**: 2019-04-26 15:38:37+00:00
- **Updated**: 2019-10-16 01:05:22+00:00
- **Authors**: Keuntaek Lee, Gabriel Nakajima An, Viacheslav Zakharov, Evangelos A. Theodorou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a novel information processing architecture for safe deep learning-based visual navigation of autonomous systems. The proposed information processing architecture is used to support a perceptual attention-based predictive control algorithm that leverages model predictive control (MPC), convolutional neural networks (CNNs), and uncertainty quantification methods. The novelty of our approach lies in using MPC to learn how to place attention on relevant areas of the visual input, which ultimately allows the system to more rapidly detect unsafe conditions. We accomplish this by using MPC to learn to select regions of interest in the input image, which are used to output control actions as well as estimates of epistemic and aleatoric uncertainty in the attention-aware visual input. We use these uncertainty estimates to quantify the safety of our network controller under the current navigation condition. The proposed architecture and algorithm is tested on a 1:5 scale terrestrial vehicle. Experimental results show that the proposed algorithm outperforms previous approaches on early detection of unsafe conditions, such as when novel obstacles are present in the navigation environment. The proposed architecture is the first step towards using deep learning-based perceptual control policies in safety-critical domains.



### Capturing human categorization of natural images at scale by combining deep networks and cognitive models
- **Arxiv ID**: http://arxiv.org/abs/1904.12690v1
- **DOI**: 10.1038/s41467-020-18946-z
- **Categories**: **cs.CV**, cs.AI, cs.LG, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12690v1)
- **Published**: 2019-04-26 15:47:59+00:00
- **Updated**: 2019-04-26 15:47:59+00:00
- **Authors**: Ruairidh M. Battleday, Joshua C. Peterson, Thomas L. Griffiths
- **Comment**: 29 pages; 4 figures. arXiv admin note: text overlap with
  arXiv:1711.04855
- **Journal**: None
- **Summary**: Human categorization is one of the most important and successful targets of cognitive modeling in psychology, yet decades of development and assessment of competing models have been contingent on small sets of simple, artificial experimental stimuli. Here we extend this modeling paradigm to the domain of natural images, revealing the crucial role that stimulus representation plays in categorization and its implications for conclusions about how people form categories. Applying psychological models of categorization to natural images required two significant advances. First, we conducted the first large-scale experimental study of human categorization, involving over 500,000 human categorization judgments of 10,000 natural images from ten non-overlapping object categories. Second, we addressed the traditional bottleneck of representing high-dimensional images in cognitive models by exploring the best of current supervised and unsupervised deep and shallow machine learning methods. We find that selecting sufficiently expressive, data-driven representations is crucial to capturing human categorization, and using these representations allows simple models that represent categories with abstract prototypes to outperform the more complex memory-based exemplar accounts of categorization that have dominated in studies using less naturalistic stimuli.



### Accurate and Robust Alignment of Variable-stained Histologic Images Using a General-purpose Greedy Diffeomorphic Registration Tool
- **Arxiv ID**: http://arxiv.org/abs/1904.11929v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11929v1)
- **Published**: 2019-04-26 16:57:11+00:00
- **Updated**: 2019-04-26 16:57:11+00:00
- **Authors**: Ludovic Venet, Sarthak Pati, Paul Yushkevich, Spyridon Bakas
- **Comment**: None
- **Journal**: None
- **Summary**: Variously stained histology slices are routinely used by pathologists to assess extracted tissue samples from various anatomical sites and determine the presence or extent of a disease. Evaluation of sequential slides is expected to enable a better understanding of the spatial arrangement and growth patterns of cells and vessels. In this paper we present a practical two-step approach based on diffeomorphic registration to align digitized sequential histopathology stained slides to each other, starting with an initial affine step followed by the estimation of a detailed deformation field.



### GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization
- **Arxiv ID**: http://arxiv.org/abs/1904.11932v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11932v3)
- **Published**: 2019-04-26 16:58:34+00:00
- **Updated**: 2019-11-27 11:10:15+00:00
- **Authors**: Lukas von Stumberg, Patrick Wenzel, Qadeer Khan, Daniel Cremers
- **Comment**: None
- **Journal**: None
- **Summary**: Direct SLAM methods have shown exceptional performance on odometry tasks. However, they are susceptible to dynamic lighting and weather changes while also suffering from a bad initialization on large baselines. To overcome this, we propose GN-Net: a network optimized with the novel Gauss-Newton loss for training weather invariant deep features, tailored for direct image alignment. Our network can be trained with pixel correspondences between images taken from different sequences. Experiments on both simulated and real-world datasets demonstrate that our approach is more robust against bad initialization, variations in day-time, and weather changes thereby outperforming state-of-the-art direct and indirect methods. Furthermore, we release an evaluation benchmark for relocalization tracking against different types of weather. Our benchmark is available at https://vision.in.tum.de/gn-net.



### Single Image Reflection Removal with Physically-Based Training Images
- **Arxiv ID**: http://arxiv.org/abs/1904.11934v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11934v2)
- **Published**: 2019-04-26 17:09:38+00:00
- **Updated**: 2020-09-30 09:14:01+00:00
- **Authors**: Soomin Kim, Yuchi Huo, Sung-Eui Yoon
- **Comment**: Accepted to CVPR 2020, project page:
  https://sgvr.kaist.ac.kr/~smkim/Reflection_removal_rendering
- **Journal**: None
- **Summary**: Recently, deep learning-based single image reflection separation methods have been exploited widely. To benefit the learning approach, a large number of training image pairs (i.e., with and without reflections) were synthesized in various ways, yet they are away from a physically-based direction. In this paper, physically based rendering is used for faithfully synthesizing the required training images, and a corresponding network structure and loss term are proposed. We utilize existing RGBD/RGB images to estimate meshes, then physically simulate the light transportation between meshes, glass, and lens with path tracing to synthesize training data, which successfully reproduce the spatially variant anisotropic visual effect of glass reflection. For guiding the separation better, we additionally consider a module, backtrack network (BT-net) for backtracking the reflections, which removes complicated ghosting, attenuation, blurred and defocused effect of glass/lens. This enables obtaining a priori information before having the distortion. The proposed method considering additional a priori information with physically simulated training data is validated with various real reflection images and shows visually pleasant and numerical advantages compared with state-of-the-art techniques.



### On Exact Computation with an Infinitely Wide Neural Net
- **Arxiv ID**: http://arxiv.org/abs/1904.11955v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.11955v2)
- **Published**: 2019-04-26 17:29:37+00:00
- **Updated**: 2019-11-04 15:10:47+00:00
- **Authors**: Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang
- **Comment**: In NeurIPS 2019. Code available: https://github.com/ruosongwang/cntk
- **Journal**: None
- **Summary**: How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its width --- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers --- is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width.   The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for the performance of a pure kernel-based method on CIFAR-10, being $10\%$ higher than the methods reported in [Novak et al., 2019], and only $6\%$ lower than the performance of the corresponding finite deep net architecture (once batch normalization, etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.



### Lifting AutoEncoders: Unsupervised Learning of a Fully-Disentangled 3D Morphable Model using Deep Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1904.11960v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11960v1)
- **Published**: 2019-04-26 17:43:51+00:00
- **Updated**: 2019-04-26 17:43:51+00:00
- **Authors**: Mihir Sahasrabudhe, Zhixin Shu, Edward Bartrum, Riza Alp Guler, Dimitris Samaras, Iasonas Kokkinos
- **Comment**: 19 pages; 12 figures; code will be released; Project page:
  https://msahasrabudhe.github.io/projects/lae/
- **Journal**: None
- **Summary**: In this work we introduce Lifting Autoencoders, a generative 3D surface-based model of object categories. We bring together ideas from non-rigid structure from motion, image formation, and morphable models to learn a controllable, geometric model of 3D categories in an entirely unsupervised manner from an unstructured set of images. We exploit the 3D geometric nature of our model and use normal information to disentangle appearance into illumination, shading and albedo. We further use weak supervision to disentangle the non-rigid shape variability of human faces into identity and expression. We combine the 3D representation with a differentiable renderer to generate RGB images and append an adversarially trained refinement network to obtain sharp, photorealistic image reconstruction results. The learned generative model can be controlled in terms of interpretable geometry and appearance factors, allowing us to perform photorealistic image manipulation of identity, expression, 3D pose, and illumination properties.



### Relevant features for Gender Classification in NIR Periocular Images
- **Arxiv ID**: http://arxiv.org/abs/1904.12007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12007v1)
- **Published**: 2019-04-26 18:23:58+00:00
- **Updated**: 2019-04-26 18:23:58+00:00
- **Authors**: Ignacio Viedma, Juan Tapia, Andres Iturriaga, Christoph Busch
- **Comment**: 12 pages, Paper accepted by IET Biometrics
- **Journal**: None
- **Summary**: Most gender classifications methods from NIR images have used iris information. Recent work has explored the use of the whole periocular iris region which has surprisingly achieve better results. This suggests the most relevant information for gender classification is not located in the iris as expected. In this work, we analyze and demonstrate the location of the most relevant features that describe gender in periocular NIR images and evaluate its influence its classification. Experiments show that the periocular region contains more gender information than the iris region. We extracted several features (intensity, texture, and shape) and classified them according to its relevance using the XgBoost algorithm. Support Vector Machine and nine ensemble classifiers were used for testing gender accuracy when using the most relevant features. The best classification results were obtained when 4,000 features located on the periocular region were used (89.22\%). Additional experiments with the full periocular iris images versus the iris-Occluded images were performed. The gender classification rates obtained were 84.35\% and 85.75\% respectively. We also contribute to the state of the art with a new database (UNAB-Gender). From results, we suggest focussing only on the surrounding area of the iris. This allows us to realize a faster classification of gender from NIR periocular images.



### RevealNet: Seeing Behind Objects in RGB-D Scans
- **Arxiv ID**: http://arxiv.org/abs/1904.12012v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12012v3)
- **Published**: 2019-04-26 18:33:34+00:00
- **Updated**: 2020-03-11 19:16:11+00:00
- **Authors**: Ji Hou, Angela Dai, Matthias Nießner
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: During 3D reconstruction, it is often the case that people cannot scan each individual object from all views, resulting in missing geometry in the captured scan. This missing geometry can be fundamentally limiting for many applications, e.g., a robot needs to know the unseen geometry to perform a precise grasp on an object. Thus, we introduce the task of semantic instance completion: from an incomplete RGB-D scan of a scene, we aim to detect the individual object instances and infer their complete object geometry. This will open up new possibilities for interactions with objects in a scene, for instance for virtual or robotic agents. We tackle this problem by introducing RevealNet, a new data-driven approach that jointly detects object instances and predicts their complete geometry. This enables a semantically meaningful decomposition of a scanned scene into individual, complete 3D objects, including hidden and unobserved object parts. RevealNet is an end-to-end 3D neural network architecture that leverages joint color and geometry feature learning. The fully-convolutional nature of our 3D network enables efficient inference of semantic instance completion for 3D scans at scale of large indoor environments in a single forward pass. We show that predicting complete object geometry improves both 3D detection and instance segmentation performance. We evaluate on both real and synthetic scan benchmark data for the new task, where we outperform state-of-the-art approaches by over 15 in mAP@0.5 on ScanNet, and over 18 in mAP@0.5 on SUNCG.



### Recurrent Embedding Aggregation Network for Video Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.12019v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12019v2)
- **Published**: 2019-04-26 19:22:41+00:00
- **Updated**: 2019-06-25 21:56:15+00:00
- **Authors**: Sixue Gong, Yichun Shi, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Recurrent networks have been successful in analyzing temporal data and have been widely used for video analysis. However, for video face recognition, where the base CNNs trained on large-scale data already provide discriminative features, using Long Short-Term Memory (LSTM), a popular recurrent network, for feature learning could lead to overfitting and degrade the performance instead. We propose a Recurrent Embedding Aggregation Network (REAN) for set to set face recognition. Compared with LSTM, REAN is robust against overfitting because it only learns how to aggregate the pre-trained embeddings rather than learning representations from scratch. Compared with quality-aware aggregation methods, REAN can take advantage of the context information to circumvent the noise introduced by redundant video frames. Empirical results on three public domain video face recognition datasets, IJB-S, YTF, and PaSC show that the proposed REAN significantly outperforms naive CNN-LSTM structure and quality-aware aggregation methods.



### Discovering Common Change-Point Patterns in Functional Connectivity Across Subjects
- **Arxiv ID**: http://arxiv.org/abs/1904.12023v1
- **DOI**: 10.1016/j.media.2019.101532
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12023v1)
- **Published**: 2019-04-26 19:34:52+00:00
- **Updated**: 2019-04-26 19:34:52+00:00
- **Authors**: Mengyu Dai, Zhengwu Zhang, Anuj Srivastava
- **Comment**: None
- **Journal**: Published in Medical Image Analysis, 2019
- **Summary**: This paper studies change-points in human brain functional connectivity (FC) and seeks patterns that are common across multiple subjects under identical external stimulus. FC relates to the similarity of fMRI responses across different brain regions when the brain is simply resting or performing a task. While the dynamic nature of FC is well accepted, this paper develops a formal statistical test for finding {\it change-points} in times series associated with FC. It represents short-term connectivity by a symmetric positive-definite matrix, and uses a Riemannian metric on this space to develop a graphical method for detecting change-points in a time series of such matrices. It also provides a graphical representation of estimated FC for stationary subintervals in between the detected change-points. Furthermore, it uses a temporal alignment of the test statistic, viewed as a real-valued function over time, to remove inter-subject variability and to discover common change-point patterns across subjects. This method is illustrated using data from Human Connectome Project (HCP) database for multiple subjects and tasks.



### Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources
- **Arxiv ID**: http://arxiv.org/abs/1904.12043v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12043v2)
- **Published**: 2019-04-26 20:45:28+00:00
- **Updated**: 2019-05-02 06:48:24+00:00
- **Authors**: Haibin Lin, Hang Zhang, Yifei Ma, Tong He, Zhi Zhang, Sheng Zha, Mu Li
- **Comment**: None
- **Journal**: None
- **Summary**: With an increasing demand for training powers for deep learning algorithms and the rapid growth of computation resources in data centers, it is desirable to dynamically schedule different distributed deep learning tasks to maximize resource utilization and reduce cost. In this process, different tasks may receive varying numbers of machines at different time, a setting we call elastic distributed training. Despite the recent successes in large mini-batch distributed training, these methods are rarely tested in elastic distributed training environments and suffer degraded performance in our experiments, when we adjust the learning rate linearly immediately with respect to the batch size. One difficulty we observe is that the noise in the stochastic momentum estimation is accumulated over time and will have delayed effects when the batch size changes. We therefore propose to smoothly adjust the learning rate over time to alleviate the influence of the noisy momentum estimation. Our experiments on image classification, object detection and semantic segmentation have demonstrated that our proposed Dynamic SGD method achieves stabilized performance when varying the number of GPUs from 8 to 128. We also provide theoretical understanding on the optimality of linear learning rate scheduling and the effects of stochastic momentum.



### ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes on the Blockchain
- **Arxiv ID**: http://arxiv.org/abs/1904.12059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1904.12059v1)
- **Published**: 2019-04-26 21:59:02+00:00
- **Updated**: 2019-04-26 21:59:02+00:00
- **Authors**: Tu Bui, Daniel Cooper, John Collomosse, Mark Bell, Alex Green, John Sheridan, Jez Higgins, Arindra Das, Jared Keller, Olivier Thereaux, Alan Brown
- **Comment**: Accepted to CVPR Blockchain Workshop 2019
- **Journal**: None
- **Summary**: We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives. First, we describe a novel deep network architecture for computing compact temporal content hashes (TCHs) from audio-visual streams with durations of minutes or hours. Our TCHs are sensitive to accidental or malicious content modification (tampering) but invariant to the codec used to encode the video. This is necessary due to the curatorial requirement for archives to format shift video over time to ensure future accessibility. Second, we describe how the TCHs (and the models used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, Estonia and Norway participated.



