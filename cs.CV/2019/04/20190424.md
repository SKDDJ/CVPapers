# Arxiv Papers in cs.CV on 2019-04-24
### Neural Collaborative Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1904.10596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10596v1)
- **Published**: 2019-04-24 01:29:17+00:00
- **Updated**: 2019-04-24 01:29:17+00:00
- **Authors**: Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, Hongdong Li
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: We introduce the Neural Collaborative Subspace Clustering, a neural model that discovers clusters of data points drawn from a union of low-dimensional subspaces. In contrast to previous attempts, our model runs without the aid of spectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model benefits from a classifier which determines whether a pair of points lies on the same subspace or not. Essential to our model is the construction of two affinity matrices, one from the classifier and the other from a notion of subspace self-expressiveness, to supervise training in a collaborative scheme. We thoroughly assess and contrast the performance of our model against various state-of-the-art clustering algorithms including deep subspace-based ones.



### Asynchronous "Events" are Better For Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.11578v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11578v1)
- **Published**: 2019-04-24 02:10:10+00:00
- **Updated**: 2019-04-24 02:10:10+00:00
- **Authors**: Yuhu Guo, Han Xiao, Yidong Chen, Xiaodong Shi
- **Comment**: Submitted at IJCAI 2019
- **Journal**: None
- **Summary**: Event-based camera is a bio-inspired vision sensor that records intensity changes (called event) asynchronously in each pixel. As an instance of event-based camera, Dynamic and Active-pixel Vision Sensor (DAVIS) combines a standard camera and an event-based camera. However, traditional models could not deal with the event stream asynchronously. To analyze the event stream asynchronously, most existing approaches accumulate events within a certain time interval and treat the accumulated events as a synchronous frame, which wastes the intensity change information and weakens the advantages of DAVIS. Therefore, in this paper, we present the first neural asynchronous approach to process event stream for event-based camera. Our method asynchronously extracts dynamic information from events by leveraging previous motion and critical features of gray-scale frames. To our best knowledge, this is the first neural asynchronous method to analyze event stream through a novel deep neural network. Extensive experiments demonstrate that our proposed model achieves remarkable improvements against the state-of-the-art baselines.



### Defocused images removal of axial overlapping scattering particles by using three-dimensional nonlinear diffusion based on digital holography
- **Arxiv ID**: http://arxiv.org/abs/1904.10613v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10613v4)
- **Published**: 2019-04-24 02:43:03+00:00
- **Updated**: 2019-08-14 08:53:41+00:00
- **Authors**: Wei-Na Li, Zhengyun Zhang, Jianshe Ma, Xiaohao Wang, Ping Su
- **Comment**: no
- **Journal**: None
- **Summary**: We propose a three-dimensional nonlinear diffusion method to implement the similar autofocusing function of multiple micro-objects and simultaneously remove the defocused images, which can distinguish the locations of certain sized scattering particles that are overlapping along z-axis. It is applied to all of the reconstruction slices that are generated from the captured hologram after each back propagation. For certain small sized particles, the maxima of maximum gradient magnitude of each reconstruction slice appears at the ground truth z position after applying the proposed scheme when the reconstruction range along z-axis is sufficiently long and the reconstruction depth spacing is sufficiently fine. Therefore, the reconstructed image at ground truth z position is remained, while the defocused images are diffused out. The results demonstrated that the proposed scheme can diffuse out the defocused images which are 20 um away from the ground truth z position in spite of that several scattering particles with different diameters are completely overlapping along z-axis with a distance of 800 um when the hologram pixel pitch is 2 um. It also demonstrated that the sparsity distribution of the ground truth z slice cannot be affected by the sparsity distribution of corresponding defocused images when the diameter of the particle is not more than 35um and the reconstruction depth spacing is not less than 20 um.



### Understanding Art through Multi-Modal Retrieval in Paintings
- **Arxiv ID**: http://arxiv.org/abs/1904.10615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10615v1)
- **Published**: 2019-04-24 02:45:18+00:00
- **Updated**: 2019-04-24 02:45:18+00:00
- **Authors**: Noa Garcia, Benjamin Renoust, Yuta Nakashima
- **Comment**: None
- **Journal**: None
- **Summary**: In computer vision, visual arts are often studied from a purely aesthetics perspective, mostly by analysing the visual appearance of an artistic reproduction to infer its style, its author, or its representative features. In this work, however, we explore art from both a visual and a language perspective. Our aim is to bridge the gap between the visual appearance of an artwork and its underlying meaning, by jointly analysing its aesthetics and its semantics. We introduce the use of multi-modal techniques in the field of automatic art analysis by 1) collecting a multi-modal dataset with fine-art paintings and comments, and 2) exploring robust visual and textual representations in artistic images.



### Reinterpreting CTC training as iterative fitting
- **Arxiv ID**: http://arxiv.org/abs/1904.10619v2
- **DOI**: 10.1016/j.patcog.2020.107392
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.10619v2)
- **Published**: 2019-04-24 02:50:29+00:00
- **Updated**: 2020-07-07 09:18:11+00:00
- **Authors**: Hongzhu Li, Weiqiang Wang
- **Comment**: to be published in Pattern Recognition
- **Journal**: None
- **Summary**: The connectionist temporal classification (CTC) enables end-to-end sequence learning by maximizing the probability of correctly recognizing sequences during training. The outputs of a CTC-trained model tend to form a series of spikes separated by strongly predicted blanks, know as the spiky problem. To figure out the reason for it, we reinterpret the CTC training process as an iterative fitting task that is based on frame-wise cross-entropy loss. It offers us an intuitive way to compare target probabilities with model outputs for each iteration, and explain how the model outputs gradually turns spiky. Inspired by it, we put forward two ways to modify the CTC training. The experiments demonstrate that our method can well solve the spiky problem and moreover, lead to faster convergence over various training settings. Beside this, the reinterpretation of CTC, as a brand new perspective, may be potentially useful in other situations. The code is publicly available at https://github.com/hzli-ucas/caffe/tree/ctc.



### Bidirectional Learning for Domain Adaptation of Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.10620v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10620v1)
- **Published**: 2019-04-24 03:00:22+00:00
- **Updated**: 2019-04-24 03:00:22+00:00
- **Authors**: Yunsheng Li, Lu Yuan, Nuno Vasconcelos
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.



### Computer-aided diagnosis in histopathological images of the endometrium using a convolutional neural network and attention mechanisms
- **Arxiv ID**: http://arxiv.org/abs/1904.10626v1
- **DOI**: 10.1109/JBHI.2019.2944977
- **Categories**: **cs.CV**, 92C55
- **Links**: [PDF](http://arxiv.org/pdf/1904.10626v1)
- **Published**: 2019-04-24 03:29:12+00:00
- **Updated**: 2019-04-24 03:29:12+00:00
- **Authors**: Hao Sun, Xianxu Zeng, Tao Xu, Gang Peng, Yutao Ma
- **Comment**: 22 pages, 8 figures, and 4 tables
- **Journal**: IEEE Journal of Biomedical and Health Informatics, 2020, 26(6):
  1664-1676
- **Summary**: Uterine cancer, also known as endometrial cancer, can seriously affect the female reproductive organs, and histopathological image analysis is the gold standard for diagnosing endometrial cancer. However, due to the limited capability of modeling the complicated relationships between histopathological images and their interpretations, these computer-aided diagnosis (CADx) approaches based on traditional machine learning algorithms often failed to achieve satisfying results. In this study, we developed a CADx approach using a convolutional neural network (CNN) and attention mechanisms, called HIENet. Because HIENet used the attention mechanisms and feature map visualization techniques, it can provide pathologists better interpretability of diagnoses by highlighting the histopathological correlations of local (pixel-level) image features to morphological characteristics of endometrial tissue. In the ten-fold cross-validation process, the CADx approach, HIENet, achieved a 76.91 $\pm$ 1.17% (mean $\pm$ s. d.) classification accuracy for four classes of endometrial tissue, namely normal endometrium, endometrial polyp, endometrial hyperplasia, and endometrial adenocarcinoma. Also, HIENet achieved an area-under-the-curve (AUC) of 0.9579 $\pm$ 0.0103 with an 81.04 $\pm$ 3.87% sensitivity and 94.78 $\pm$ 0.87% specificity in a binary classification task that detected endometrioid adenocarcinoma (Malignant). Besides, in the external validation process, HIENet achieved an 84.50% accuracy in the four-class classification task, and it achieved an AUC of 0.9829 with a 77.97% (95% CI, 65.27%-87.71%) sensitivity and 100% (95% CI, 97.42%-100.00%) specificity. In summary, the proposed CADx approach, HIENet, outperformed three human experts and four end-to-end CNN-based classifiers on this small-scale dataset composed of 3,500 hematoxylin and eosin (H&E) images regarding overall classification performance.



### LFFD: A Light and Fast Face Detector for Edge Devices
- **Arxiv ID**: http://arxiv.org/abs/1904.10633v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10633v3)
- **Published**: 2019-04-24 03:47:24+00:00
- **Updated**: 2019-08-12 08:09:32+00:00
- **Authors**: Yonghao He, Dezhong Xu, Lifang Wu, Meng Jian, Shiming Xiang, Chunhong Pan
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: Face detection, as a fundamental technology for various applications, is always deployed on edge devices which have limited memory storage and low computing power. This paper introduces a Light and Fast Face Detector (LFFD) for edge devices. The proposed method is anchor-free and belongs to the one-stage category. Specifically, we rethink the importance of receptive field (RF) and effective receptive field (ERF) in the background of face detection. Essentially, the RFs of neurons in a certain layer are distributed regularly in the input image and theses RFs are natural "anchors". Combining RF "anchors" and appropriate RF strides, the proposed method can detect a large range of continuous face scales with 100% coverage in theory. The insightful understanding of relations between ERF and face scales motivates an efficient backbone for one-stage detection. The backbone is characterized by eight detection branches and common layers, resulting in efficient computation. Comprehensive and extensive experiments on popular benchmarks: WIDER FACE and FDDB are conducted. A new evaluation schema is proposed for application-oriented scenarios. Under the new schema, the proposed method can achieve superior accuracy (WIDER FACE Val/Test -- Easy: 0.910/0.896, Medium: 0.881/0.865, Hard: 0.780/0.770; FDDB -- discontinuous: 0.973, continuous: 0.724). Multiple hardware platforms are introduced to evaluate the running efficiency. The proposed method can obtain fast inference speed (NVIDIA TITAN Xp: 131.45 FPS at 640x480; NVIDIA TX2: 136.99 PFS at 160x120; Raspberry Pi 3 Model B+: 8.44 FPS at 160x120) with model size of 9 MB.



### Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.10638v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10638v1)
- **Published**: 2019-04-24 04:38:30+00:00
- **Updated**: 2019-04-24 04:38:30+00:00
- **Authors**: Yu Yu, Gang Liu, Jean-Marc Odobez
- **Comment**: Work started in June 2018, Submitted to CVPR on November 15th 2018,
  accepted at CVPR 2019
- **Journal**: None
- **Summary**: As an indicator of human attention gaze is a subtle behavioral cue which can be exploited in many applications. However, inferring 3D gaze direction is challenging even for deep neural networks given the lack of large amount of data (groundtruthing gaze is expensive and existing datasets use different setups) and the inherent presence of gaze biases due to person-specific difference. In this work, we address the problem of person-specific gaze model adaptation from only a few reference training samples. The main and novel idea is to improve gaze adaptation by generating additional training samples through the synthesis of gaze-redirected eye images from existing reference samples. In doing so, our contributions are threefold: (i) we design our gaze redirection framework from synthetic data, allowing us to benefit from aligned training sample pairs to predict accurate inverse mapping fields; (ii) we proposed a self-supervised approach for domain adaptation; (iii) we exploit the gaze redirection to improve the performance of person-specific gaze estimation. Extensive experiments on two public datasets demonstrate the validity of our gaze retargeting and gaze estimation framework.



### Beauty Learning and Counterfactual Inference
- **Arxiv ID**: http://arxiv.org/abs/1904.12629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12629v1)
- **Published**: 2019-04-24 05:21:53+00:00
- **Updated**: 2019-04-24 05:21:53+00:00
- **Authors**: Tao Li
- **Comment**: In CVPR-19 Workshop on Explainable AI
- **Journal**: None
- **Summary**: This work showcases a new approach for causal discovery by leveraging user experiments and recent advances in photo-realistic image editing, demonstrating a potential of identifying causal factors and understanding complex systems counterfactually. We introduce the beauty learning problem as an example, which has been discussed metaphysically for centuries and been proved exists, is quantifiable, and can be learned by deep models in our recent paper, where we utilize a natural image generator coupled with user studies to infer causal effects from facial semantics to beauty outcomes, the results of which also align with existing empirical studies. We expect the proposed framework for a broader application in causal inference.



### Super-Resolved Image Perceptual Quality Improvement via Multi-Feature Discriminators
- **Arxiv ID**: http://arxiv.org/abs/1904.10654v2
- **DOI**: 10.1117/1.JEI.29.1.013017
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10654v2)
- **Published**: 2019-04-24 06:28:57+00:00
- **Updated**: 2019-08-27 07:53:07+00:00
- **Authors**: Xuan Zhu, Yue Cheng, Jinye Peng, Rongzhi Wang, Mingnan Le, Xin Liu
- **Comment**: 18 pages, 10 figures, 6 tables
- **Journal**: None
- **Summary**: Generative adversarial network (GAN) for image super-resolution (SR) has attracted enormous interests in recent years. However, the GAN-based SR methods only use image discriminator to distinguish SR images and high-resolution (HR) images. Image discriminator fails to discriminate images accurately since image features cannot be fully expressed. In this paper, we design a new GAN-based SR framework GAN-IMC which includes generator, image discriminator, morphological component discriminator and color discriminator. The combination of multiple feature discriminators improves the accuracy of image discrimination. Adversarial training between the generator and multi-feature discriminators forces SR images to converge with HR images in terms of data and features distribution. Moreover, in some cases, feature enhancement of salient regions is also worth considering. GAN-IMC is further optimized by weighted content loss (GAN-IMCW), which effectively restores and enhances salient regions in SR images. The effectiveness and robustness of our method are confirmed by extensive experiments on public datasets. Compared with state-of-the-art methods, the proposed method not only achieves competitive Perceptual Index (PI) and Natural Image Quality Evaluator (NIQE) values but also obtains pleasant visual perception in image edge, texture, color and salient regions.



### Segmenting the Future
- **Arxiv ID**: http://arxiv.org/abs/1904.10666v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10666v2)
- **Published**: 2019-04-24 07:30:34+00:00
- **Updated**: 2019-12-12 18:10:20+00:00
- **Authors**: Hsu-kuang Chiu, Ehsan Adeli, Juan Carlos Niebles
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the future is an important aspect for decision-making in robotics or autonomous driving systems, which heavily rely upon visual scene understanding. While prior work attempts to predict future video pixels, anticipate activities or forecast future scene semantic segments from segmentation of the preceding frames, methods that predict future semantic segmentation solely from the previous frame RGB data in a single end-to-end trainable model do not exist. In this paper, we propose a temporal encoder-decoder network architecture that encodes RGB frames from the past and decodes the future semantic segmentation. The network is coupled with a new knowledge distillation training framework specific for the forecasting task. Our method, only seeing preceding video frames, implicitly models the scene segments while simultaneously accounting for the object dynamics to infer the future scene semantic segments. Our results on Cityscapes and Apolloscape outperform the baseline and current state-of-the-art methods. Code is available at https://github.com/eddyhkchiu/segmenting_the_future/.



### A General Framework for Edited Video and Raw Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1904.10669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10669v1)
- **Published**: 2019-04-24 07:42:23+00:00
- **Updated**: 2019-04-24 07:42:23+00:00
- **Authors**: Xuelong Li, Bin Zhao, Xiaoqiang Lu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, 2017
- **Summary**: In this paper, we build a general summarization framework for both of edited video and raw video summarization. Overall, our work can be divided into three folds: 1) Four models are designed to capture the properties of video summaries, i.e., containing important people and objects (importance), representative to the video content (representativeness), no similar key-shots (diversity) and smoothness of the storyline (storyness). Specifically, these models are applicable to both edited videos and raw videos. 2) A comprehensive score function is built with the weighted combination of the aforementioned four models. Note that the weights of the four models in the score function, denoted as property-weight, are learned in a supervised manner. Besides, the property-weights are learned for edited videos and raw videos, respectively. 3) The training set is constructed with both edited videos and raw videos in order to make up the lack of training data. Particularly, each training video is equipped with a pair of mixing-coefficients which can reduce the structure mess in the training set caused by the rough mixture. We test our framework on three datasets, including edited videos, short raw videos and long raw videos. Experimental results have verified the effectiveness of the proposed framework.



### Deep Learning for Classification of Hyperspectral Data: A Comparative Review
- **Arxiv ID**: http://arxiv.org/abs/1904.10674v1
- **DOI**: 10.1109/MGRS.2019.2912563
- **Categories**: **cs.LG**, cs.CV, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10674v1)
- **Published**: 2019-04-24 07:56:37+00:00
- **Updated**: 2019-04-24 07:56:37+00:00
- **Authors**: Nicolas Audebert, Bertrand Saux, Sébastien Lefèvre
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning techniques revolutionized the way remote sensing data are processed. Classification of hyperspectral data is no exception to the rule, but has intrinsic specificities which make application of deep learning less straightforward than with other optical data. This article presents a state of the art of previous machine learning approaches, reviews the various deep learning approaches currently proposed for hyperspectral classification, and identifies the problems and difficulties which arise to implement deep neural networks for this task. In particular, the issues of spatial and spectral resolution, data volume, and transfer of models from multimedia images to hyperspectral data are addressed. Additionally, a comparative study of various families of network architectures is provided and a software toolbox is publicly released to allow experimenting with these methods. 1 This article is intended for both data scientists with interest in hyperspectral data and remote sensing experts eager to apply deep learning techniques to their own dataset.



### A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.10681v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10681v1)
- **Published**: 2019-04-24 08:09:35+00:00
- **Updated**: 2019-04-24 08:09:35+00:00
- **Authors**: Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, Wei-Shi Zheng
- **Comment**: Origianl version has been published by ACMMM 2018
- **Journal**: None
- **Summary**: Current researches of action recognition mainly focus on single-view and multi-view recognition, which can hardly satisfies the requirements of human-robot interaction (HRI) applications to recognize actions from arbitrary views. The lack of datasets also sets up barriers. To provide data for arbitrary-view action recognition, we newly collect a large-scale RGB-D action dataset for arbitrary-view action analysis, including RGB videos, depth and skeleton sequences. The dataset includes action samples captured in 8 fixed viewpoints and varying-view sequences which covers the entire 360 degree view angles. In total, 118 persons are invited to act 40 action categories, and 25,600 video samples are collected. Our dataset involves more participants, more viewpoints and a large number of samples. More importantly, it is the first dataset containing the entire 360 degree varying-view sequences. The dataset provides sufficient data for multi-view, cross-view and arbitrary-view action analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to tackle the problem of arbitrary-view action recognition. Experiment results show that the VS-CNN achieves superior performance.



### Multi-scale deep neural networks for real image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.10698v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10698v1)
- **Published**: 2019-04-24 08:43:18+00:00
- **Updated**: 2019-04-24 08:43:18+00:00
- **Authors**: Shangqi Gao, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Single image super-resolution (SR) is extremely difficult if the upscaling factors of image pairs are unknown and different from each other, which is common in real image SR. To tackle the difficulty, we develop two multi-scale deep neural networks (MsDNN) in this work. Firstly, due to the high computation complexity in high-resolution spaces, we process an input image mainly in two different downscaling spaces, which could greatly lower the usage of GPU memory. Then, to reconstruct the details of an image, we design a multi-scale residual network (MsRN) in the downscaling spaces based on the residual blocks. Besides, we propose a multi-scale dense network based on the dense blocks to compare with MsRN. Finally, our empirical experiments show the robustness of MsDNN for image SR when the upscaling factor is unknown. According to the preliminary results of NTIRE 2019 image SR challenge, our team (ZXHresearch@fudan) ranks 21-st among all participants. The implementation of MsDNN is released https://github.com/shangqigao/gsq-image-SR



### Context-Aware Zero-Shot Learning for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.12638v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12638v2)
- **Published**: 2019-04-24 08:50:05+00:00
- **Updated**: 2019-04-30 11:39:52+00:00
- **Authors**: Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari
- **Comment**: Accepted at ICML 2019
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic representations. A limitation of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive principle that objects tend to be found in certain contexts but not others, we propose a new and challenging approach, context-aware ZSL, that leverages semantic representations in a new way to model the conditional likelihood of an object to appear in a given context. Finally, through extensive experiments conducted on Visual Genome, we show that contextual information can substantially improve the standard ZSL approach and is robust to unbalanced classes.



### The VIA Annotation Software for Images, Audio and Video
- **Arxiv ID**: http://arxiv.org/abs/1904.10699v3
- **DOI**: 10.1145/3343031.3350535
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10699v3)
- **Published**: 2019-04-24 08:55:27+00:00
- **Updated**: 2019-08-09 10:29:56+00:00
- **Authors**: Abhishek Dutta, Andrew Zisserman
- **Comment**: to appear in Proceedings of the 27th ACM International Conference on
  Multimedia (MM '19), October 21-25, 2019, Nice, France. ACM, New York, NY,
  USA, 4 pages
- **Journal**: None
- **Summary**: In this paper, we introduce a simple and standalone manual annotation tool for images, audio and video: the VGG Image Annotator (VIA). This is a light weight, standalone and offline software package that does not require any installation or setup and runs solely in a web browser. The VIA software allows human annotators to define and describe spatial regions in images or video frames, and temporal segments in audio or video. These manual annotations can be exported to plain text data formats such as JSON and CSV and therefore are amenable to further processing by other software tools. VIA also supports collaborative annotation of a large dataset by a group of human annotators. The BSD open source license of this software allows it to be used in any academic project or commercial application.



### PCA-RECT: An Energy-efficient Object Detection Approach for Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1904.12665v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.12665v1)
- **Published**: 2019-04-24 09:11:22+00:00
- **Updated**: 2019-04-24 09:11:22+00:00
- **Authors**: Bharath Ramesh, Andres Ussa, Luca Della Vedova, Hong Yang, Garrick Orchard
- **Comment**: Accepted in ACCV 2018 Workshops, to appear
- **Journal**: None
- **Summary**: We present the first purely event-based, energy-efficient approach for object detection and categorization using an event camera. Compared to traditional frame-based cameras, choosing event cameras results in high temporal resolution (order of microseconds), low power consumption (few hundred mW) and wide dynamic range (120 dB) as attractive properties. However, event-based object recognition systems are far behind their frame-based counterparts in terms of accuracy. To this end, this paper presents an event-based feature extraction method devised by accumulating local activity across the image frame and then applying principal component analysis (PCA) to the normalized neighborhood region. Subsequently, we propose a backtracking-free k-d tree mechanism for efficient feature matching by taking advantage of the low-dimensionality of the feature representation. Additionally, the proposed k-d tree mechanism allows for feature selection to obtain a lower-dimensional dictionary representation when hardware resources are limited to implement dimensionality reduction. Consequently, the proposed system can be realized on a field-programmable gate array (FPGA) device leading to high performance over resource ratio. The proposed system is tested on real-world event-based datasets for object categorization, showing superior classification performance and relevance to state-of-the-art algorithms. Additionally, we verified the object detection method and real-time FPGA performance in lab settings under non-controlled illumination conditions with limited training data and ground truth annotations.



### A CNN-RNN Architecture for Multi-Label Weather Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.10709v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.10709v1)
- **Published**: 2019-04-24 09:27:29+00:00
- **Updated**: 2019-04-24 09:27:29+00:00
- **Authors**: Bin Zhao, Xuelong Li, Xiaoqiang Lu, Zhigang Wang
- **Comment**: One weather recognition dataset is constructed
- **Journal**: None
- **Summary**: Weather Recognition plays an important role in our daily lives and many computer vision applications. However, recognizing the weather conditions from a single image remains challenging and has not been studied thoroughly. Generally, most previous works treat weather recognition as a single-label classification task, namely, determining whether an image belongs to a specific weather class or not. This treatment is not always appropriate, since more than one weather conditions may appear simultaneously in a single image. To address this problem, we make the first attempt to view weather recognition as a multi-label classification task, i.e., assigning an image more than one labels according to the displayed weather conditions. Specifically, a CNN-RNN based multi-label classification approach is proposed in this paper. The convolutional neural network (CNN) is extended with a channel-wise attention model to extract the most correlated visual features. The Recurrent Neural Network (RNN) further processes the features and excavates the dependencies among weather classes. Finally, the weather labels are predicted step by step. Besides, we construct two datasets for the weather recognition task and explore the relationships among different weather conditions. Experimental results demonstrate the superiority and effectiveness of the proposed approach. The new constructed datasets will be available at https://github.com/wzgwzg/Multi-Label-Weather-Recognition.



### OperatorNet: Recovering 3D Shapes From Difference Operators
- **Arxiv ID**: http://arxiv.org/abs/1904.10754v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10754v2)
- **Published**: 2019-04-24 11:47:08+00:00
- **Updated**: 2019-08-28 16:27:34+00:00
- **Authors**: Ruqi Huang, Marie-Julie Rakotosaona, Panos Achlioptas, Leonidas Guibas, Maks Ovsjanikov
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications.



### CED: Color Event Camera Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.10772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10772v1)
- **Published**: 2019-04-24 12:42:12+00:00
- **Updated**: 2019-04-24 12:42:12+00:00
- **Authors**: Cedric Scheerlinck, Henri Rebecq, Timo Stoffregen, Nick Barnes, Robert Mahony, Davide Scaramuzza
- **Comment**: Conference on Computer Vision and Pattern Recognition Workshops
- **Journal**: None
- **Summary**: Event cameras are novel, bio-inspired visual sensors, whose pixels output asynchronous and independent timestamped spikes at local intensity changes, called 'events'. Event cameras offer advantages over conventional frame-based cameras in terms of latency, high dynamic range (HDR) and temporal resolution. Until recently, event cameras have been limited to outputting events in the intensity channel, however, recent advances have resulted in the development of color event cameras, such as the Color-DAVIS346. In this work, we present and release the first Color Event Camera Dataset (CED), containing 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes, which we hope will help drive forward event-based vision research. We also present an extension of the event camera simulator ESIM that enables simulation of color events. Finally, we present an evaluation of three state-of-the-art image reconstruction methods that can be used to convert the Color-DAVIS346 into a continuous-time, HDR, color video camera to visualise the event stream, and for use in downstream vision applications.



### Informative sample generation using class aware generative adversarial networks for classification of chest Xrays
- **Arxiv ID**: http://arxiv.org/abs/1904.10781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10781v2)
- **Published**: 2019-04-24 13:05:38+00:00
- **Updated**: 2019-04-30 04:16:32+00:00
- **Authors**: Behzad Bozorgtabar, Dwarikanath Mahapatra, Hendrik von Teng, Alexander Pollinger, Lukas Ebner, Jean-Phillipe Thiran, Mauricio Reyes
- **Comment**: None
- **Journal**: None
- **Summary**: Training robust deep learning (DL) systems for disease detection from medical images is challenging due to limited images covering different disease types and severity. The problem is especially acute, where there is a severe class imbalance. We propose an active learning (AL) framework to select most informative samples for training our model using a Bayesian neural network. Informative samples are then used within a novel class aware generative adversarial network (CAGAN) to generate realistic chest xray images for data augmentation by transferring characteristics from one class label to another. Experiments show our proposed AL framework is able to achieve state-of-the-art performance by using about $35\%$ of the full dataset, thus saving significant time and effort over conventional methods.



### Simultaneous regression and feature learning for facial landmarking
- **Arxiv ID**: http://arxiv.org/abs/1904.10787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10787v1)
- **Published**: 2019-04-24 13:15:30+00:00
- **Updated**: 2019-04-24 13:15:30+00:00
- **Authors**: Janez Križaj, Peter Peer, Vitomir Štruc, Simon Dobrišek
- **Comment**: None
- **Journal**: None
- **Summary**: Face alignment (or facial landmarking) is an important task in many face-related applications, ranging from registration, tracking and animation to higher-level classification problems such as face, expression or attribute recognition. While several solutions have been presented in the literature for this task so far, reliably locating salient facial features across a wide range of posses still remains challenging. To address this issue, we propose in this paper a novel method for automatic facial landmark localization in 3D face data designed specifically to address appearance variability caused by significant pose variations. Our method builds on recent cascaded-regression-based methods to facial landmarking and uses a gating mechanism to incorporate multiple linear cascaded regression models each trained for a limited range of poses into a single powerful landmarking model capable of processing arbitrary posed input data. We develop two distinct approaches around the proposed gating mechanism: i) the first uses a gated multiple ridge descent (GRID) mechanism in conjunction with established (hand-crafted) HOG features for face alignment and achieves state-of-the-art landmarking performance across a wide range of facial poses, ii) the second simultaneously learns multiple-descent directions as well as binary features (SMUF) that are optimal for the alignment tasks and in addition to competitive landmarking results also ensures extremely rapid processing. We evaluate both approaches in rigorous experiments on several popular datasets of 3D face images, i.e., the FRGCv2 and Bosphorus 3D Face datasets and image collections F and G from the University of Notre Dame. The results of our evaluation show that both approaches are competitive in comparison to the state-of-the-art, while exhibiting considerable robustness to pose variations.



### Automatic cephalometric landmarks detection on frontal faces: an approach based on supervised learning techniques
- **Arxiv ID**: http://arxiv.org/abs/1904.10816v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1904.10816v1)
- **Published**: 2019-04-24 13:44:28+00:00
- **Updated**: 2019-04-24 13:44:28+00:00
- **Authors**: Lucas Faria Porto, Laise Nascimento Correia Lima, Marta Flores, Andrea Valsecchi, Oscar Ibanez, Carlos Eduardo Machado Palhares, Flavio de Barros Vidal
- **Comment**: 24 pages, 6 figures
- **Journal**: None
- **Summary**: Facial landmarks are employed in many research areas such as facial recognition, craniofacial identification, age and sex estimation among the most important. In the forensic field, the focus is on the analysis of a particular set of facial landmarks, defined as cephalometric landmarks. Previous works demonstrated that the descriptive adequacy of these anatomical references for an indirect application (photo-anthropometric description) increased the marking precision of these points, contributing to a greater reliability of these analyzes. However, most of them are performed manually and all of them are subjectivity inherent to the expert examiners. In this sense, the purpose of this work is the development and validation of automatic techniques to detect cephalometric landmarks from digital images of frontal faces in forensic field. The presented approach uses a combination of computer vision and image processing techniques within a supervised learning procedures. The proposed methodology obtains similar precision to a group of human manual cephalometric reference markers and result to be more accurate against others state-of-the-art facial landmark detection frameworks. It achieves a normalized mean distance (in pixel) error of 0.014, similar to the mean inter-expert dispersion (0.009) and clearly better than other automatic approaches also analyzed along of this work (0.026 and 0.101).



### Optical machine learning with incoherent light and a single-pixel detector
- **Arxiv ID**: http://arxiv.org/abs/1904.10851v3
- **DOI**: 10.1364/OL.44.005186
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10851v3)
- **Published**: 2019-04-24 14:49:36+00:00
- **Updated**: 2019-11-24 20:01:45+00:00
- **Authors**: Shuming Jiao, Jun Feng, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan
- **Comment**: None
- **Journal**: Optics Letters 44(21) 2019
- **Summary**: An optical diffractive neural network (DNN) can be implemented with a cascaded phase mask architecture. Like an optical computer, the system can perform machine learning tasks such as number digit recognition in an all-optical manner. However, the system can only work under coherent light illumination and the precision requirement in practical experiments is quite high. This paper proposes an optical machine learning framework based on single-pixel imaging (MLSPI). The MLSPI system can perform the same linear pattern recognition task as DNN. Furthermore, it can work under incoherent lighting conditions, has lower experimental complexity and can be easily programmable.



### Unsupervised Assignment Flow: Label Learning on Feature Manifolds by Spatially Regularized Geometric Assignment
- **Arxiv ID**: http://arxiv.org/abs/1904.10863v3
- **DOI**: 10.1007/s10851-019-00935-7
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1904.10863v3)
- **Published**: 2019-04-24 15:08:01+00:00
- **Updated**: 2019-12-16 15:44:49+00:00
- **Authors**: Artjom Zern, Matthias Zisler, Stefania Petra, Christoph Schnörr
- **Comment**: 34 pages, 13 figures, published in Journal of Mathematical Imaging
  and Vision (JMIV)
- **Journal**: None
- **Summary**: This paper introduces the unsupervised assignment flow that couples the assignment flow for supervised image labeling with Riemannian gradient flows for label evolution on feature manifolds. The latter component of the approach encompasses extensions of state-of-the-art clustering approaches to manifold-valued data. Coupling label evolution with the spatially regularized assignment flow induces a sparsifying effect that enables to learn compact label dictionaries in an unsupervised manner. Our approach alleviates the requirement for supervised labeling to have proper labels at hand, because an initial set of labels can evolve and adapt to better values while being assigned to given data. The separation between feature and assignment manifolds enables the flexible application which is demonstrated for three scenarios with manifold-valued features. Experiments demonstrate a beneficial effect in both directions: adaptivity of labels improves image labeling, and steering label evolution by spatially regularized assignments leads to proper labels, because the assignment flow for supervised labeling is exactly used without any approximation for label learning.



### $S^{2}$-LBI: Stochastic Split Linearized Bregman Iterations for Parsimonious Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.10873v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.10873v1)
- **Published**: 2019-04-24 15:31:55+00:00
- **Updated**: 2019-04-24 15:31:55+00:00
- **Authors**: Yanwei Fu, Donghao Li, Xinwei Sun, Shun Zhang, Yizhou Wang, Yuan Yao
- **Comment**: technical report
- **Journal**: None
- **Summary**: This paper proposes a novel Stochastic Split Linearized Bregman Iteration ($S^{2}$-LBI) algorithm to efficiently train the deep network. The $S^{2}$-LBI introduces an iterative regularization path with structural sparsity. Our $S^{2}$-LBI combines the computational efficiency of the LBI, and model selection consistency in learning the structural sparsity. The computed solution path intrinsically enables us to enlarge or simplify a network, which theoretically, is benefited from the dynamics property of our $S^{2}$-LBI algorithm. The experimental results validate our $S^{2}$-LBI on MNIST and CIFAR-10 dataset. For example, in MNIST, we can either boost a network with only 1.5K parameters (1 convolutional layer of 5 filters, and 1 FC layer), achieves 98.40\% recognition accuracy; or we simplify $82.5\%$ of parameters in LeNet-5 network, and still achieves the 98.47\% recognition accuracy. In addition, we also have the learning results on ImageNet, which will be added in the next version of our report.



### ViDeNN: Deep Blind Video Denoising
- **Arxiv ID**: http://arxiv.org/abs/1904.10898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10898v1)
- **Published**: 2019-04-24 16:08:27+00:00
- **Updated**: 2019-04-24 16:08:27+00:00
- **Authors**: Michele Claus, Jan van Gemert
- **Comment**: Submission of NTIRE: New Trends in Image Restoration and Enhancement
  workshop and challenges at CVPR 2019
- **Journal**: None
- **Summary**: We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the noise distribution (blind denoising). The CNN architecture uses a combination of spatial and temporal filtering, learning to spatially denoise the frames first and at the same time how to combine their temporal information, handling objects motion, brightness changes, low-light conditions and temporal inconsistencies. We demonstrate the importance of the data used for CNNs training, creating for this purpose a specific dataset for low-light conditions. We test ViDeNN on common benchmarks and on self-collected data, achieving good results comparable with the state-of-the-art.



### The iterative convolution-thresholding method (ICTM) for image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.10917v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10917v1)
- **Published**: 2019-04-24 16:50:12+00:00
- **Updated**: 2019-04-24 16:50:12+00:00
- **Authors**: Dong Wang, Xiao-Ping Wang
- **Comment**: 13 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel iterative convolution-thresholding method (ICTM) that is applicable to a range of variational models for image segmentation. A variational model usually minimizes an energy functional consisting of a fidelity term and a regularization term. In the ICTM, the interface between two different segment domains is implicitly represented by their characteristic functions. The fidelity term is then usually written as a linear functional of the characteristic functions and the regularized term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolution-thresholding method to minimize the approximate energy. The method is simple, efficient and enjoys the energy-decaying property. Numerical experiments show that the method is easy to implement, robust and applicable to various image segmentation models.



### Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation
- **Arxiv ID**: http://arxiv.org/abs/1904.10506v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10506v2)
- **Published**: 2019-04-24 17:48:17+00:00
- **Updated**: 2019-05-08 23:11:30+00:00
- **Authors**: Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, Ruigang Yang
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: This paper presents a novel framework to recover detailed human body shapes from a single image. It is a challenging task due to factors such as variations in human shapes, body poses, and viewpoints. Prior methods typically attempt to recover the human body shape using a parametric based template that lacks the surface details. As such the resulting body shape appears to be without clothing. In this paper, we propose a novel learning-based framework that combines the robustness of parametric model with the flexibility of free-form 3D deformation. We use the deep neural networks to refine the 3D shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the constraints from body joints, silhouettes, and per-pixel shading information. We are able to restore detailed human body shapes beyond skinned models. Experiments demonstrate that our method has outperformed previous state-of-the-art approaches, achieving better accuracy in terms of both 2D IoU number and 3D metric distance. The code is available in https://github.com/zhuhao-nju/hmd.git



### Analytical Moment Regularizer for Gaussian Robust Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.11005v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11005v1)
- **Published**: 2019-04-24 18:37:36+00:00
- **Updated**: 2019-04-24 18:37:36+00:00
- **Authors**: Modar Alfadly, Adel Bibi, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive performance of deep neural networks (DNNs) on numerous vision tasks, they still exhibit yet-to-understand uncouth behaviours. One puzzling behaviour is the subtle sensitive reaction of DNNs to various noise attacks. Such a nuisance has strengthened the line of research around developing and training noise-robust networks. In this work, we propose a new training regularizer that aims to minimize the probabilistic expected training loss of a DNN subject to a generic Gaussian input. We provide an efficient and simple approach to approximate such a regularizer for arbitrary deep networks. This is done by leveraging the analytic expression of the output mean of a shallow neural network; avoiding the need for the memory and computationally expensive data augmentation. We conduct extensive experiments on LeNet and AlexNet on various datasets including MNIST, CIFAR10, and CIFAR100 demonstrating the effectiveness of our proposed regularizer. In particular, we show that networks that are trained with the proposed regularizer benefit from a boost in robustness equivalent to performing 3-21 folds of data augmentation.



### Ultrasound segmentation using U-Net: learning from simulated data and testing on real data
- **Arxiv ID**: http://arxiv.org/abs/1904.11031v1
- **DOI**: 10.1109/EMBC.2019.8857218
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11031v1)
- **Published**: 2019-04-24 19:24:01+00:00
- **Updated**: 2019-04-24 19:24:01+00:00
- **Authors**: Bahareh Behboodi, Hassan Rivaz
- **Comment**: Accepted in EMBC 2019
- **Journal**: None
- **Summary**: Segmentation of ultrasound images is an essential task in both diagnosis and image-guided interventions given the ease-of-use and low cost of this imaging modality. As manual segmentation is tedious and time consuming, a growing body of research has focused on the development of automatic segmentation algorithms. Deep learning algorithms have shown remarkable achievements in this regard; however, they need large training datasets. Unfortunately, preparing large labeled datasets in ultrasound images is prohibitively difficult. Therefore, in this study, we propose the use of simulated ultrasound (US) images for training the U-Net deep learning segmentation architecture and test on tissue-mimicking phantom data collected by an ultrasound machine. We demonstrate that the trained architecture on the simulated data is transferrable to real data, and therefore, simulated data can be considered as an alternative training dataset when real datasets are not available. The second contribution of this paper is that we train our U- Net network on envelope and B-mode images of the simulated dataset, and test the trained network on real envelope and B- mode images of phantom, respectively. We show that test results are superior for the envelope data compared to B-mode image.



### Multi-Scale Body-Part Mask Guided Attention for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.11041v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11041v1)
- **Published**: 2019-04-24 19:56:28+00:00
- **Updated**: 2019-04-24 19:56:28+00:00
- **Authors**: Honglong Cai, Zhiguan Wang, Jinxing Cheng
- **Comment**: None
- **Journal**: CVPRW 2019
- **Summary**: Person re-identification becomes a more and more important task due to its wide applications. In practice, person re-identification still remains challenging due to the variation of person pose, different lighting, occlusion, misalignment, background clutter, etc. In this paper, we propose a multi-scale body-part mask guided attention network (MMGA), which jointly learns whole-body and part body attention to help extract global and local features simultaneously. In MMGA, body-part masks are used to guide the training of corresponding attention. Experiments show that our proposed method can reduce the negative influence of variation of person pose, misalignment and background clutter. Our method achieves rank-1/mAP of 95.0%/87.2% on the Market1501 dataset, 89.5%/78.1% on the DukeMTMC-reID dataset, outperforming current state-of-the-art methods.



### Physical Adversarial Textures that Fool Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.11042v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11042v2)
- **Published**: 2019-04-24 19:56:57+00:00
- **Updated**: 2019-09-15 20:12:35+00:00
- **Authors**: Rey Reza Wiyatno, Anqi Xu
- **Comment**: Accepted to the International Conference on Computer Vision (ICCV)
  2019
- **Journal**: None
- **Summary**: We present a system for generating inconspicuous-looking textures that, when displayed in the physical world as digital or printed posters, cause visual object tracking systems to become confused. For instance, as a target being tracked by a robot's camera moves in front of such a poster, our generated texture makes the tracker lock onto it and allows the target to evade. This work aims to fool seldom-targeted regression tasks, and in particular compares diverse optimization strategies: non-targeted, targeted, and a new family of guided adversarial losses. While we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different conditioning variables, including viewpoint, lighting, and appearances, to find practical attack setups with high resulting adversarial strength and convergence speed. We further showcase textures optimized solely using simulated scenes can confuse real-world tracking systems.



### Bridging the Domain Gap for Ground-to-Aerial Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.11045v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11045v2)
- **Published**: 2019-04-24 20:02:44+00:00
- **Updated**: 2019-08-09 09:05:02+00:00
- **Authors**: Krishna Regmi, Mubarak Shah
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: The visual entities in cross-view images exhibit drastic domain changes due to the difference in viewpoints each set of images is captured from. Existing state-of-the-art methods address the problem by learning view-invariant descriptors for the images. We propose a novel method for solving this task by exploiting the generative powers of conditional GANs to synthesize an aerial representation of a ground level panorama and use it to minimize the domain gap between the two views. The synthesized image being from the same view as the target image helps the network to preserve important cues in aerial images following our Joint Feature Learning approach. Our Feature Fusion method combines the complementary features from a synthesized aerial image with the corresponding ground features to obtain a robust query representation. In addition, multi-scale feature aggregation preserves image representations at different feature scales useful for solving this complex task. Experimental results show that our proposed approach performs significantly better than the state-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and top-1% retrieval accuracies. Furthermore, to evaluate the generalization of our method on urban landscapes, we collected a new cross-view localization dataset with geo-reference information.



### How much do you perceive this? An analysis on perceptions of geometric features, personalities and emotions in virtual humans (Extended Version)
- **Arxiv ID**: http://arxiv.org/abs/1904.11084v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11084v1)
- **Published**: 2019-04-24 21:50:34+00:00
- **Updated**: 2019-04-24 21:50:34+00:00
- **Authors**: Victor Araujo, Rodolfo Migon Favaretto, Paulo Knob, Soraia Raupp Musse, Felipe Vilanova, Angelo Brandelli Costa
- **Comment**: Extended Version of a paper published at IVA 2019
- **Journal**: None
- **Summary**: This work aims to evaluate people's perception regarding geometric features, personalities and emotions characteristics in virtual humans. For this, we use as a basis, a dataset containing the tracking files of pedestrians captured from spontaneous videos and visualized them as identical virtual humans. The goal is to focus on their behavior and not being distracted by other features. In addition to tracking files containing their positions, the dataset also contains pedestrian emotions and personalities detected using Computer Vision and Pattern Recognition techniques. We proceed with our analysis in order to answer the question if subjects can perceive geometric features as distances/speeds as well as emotions and personalities in video sequences when pedestrians are represented by virtual humans. Regarding the participants, an amount of 73 people volunteered for the experiment. The analysis was divided in two parts: i) evaluation on perception of geometric characteristics, such as density, angular variation, distances and speeds, and ii) evaluation on personality and emotion perceptions. Results indicate that, even without explaining to the participants the concepts of each personality or emotion and how they were calculated (considering geometric characteristics), in most of the cases, participants perceived the personality and emotion expressed by the virtual agents, in accordance with the available ground truth.



### Simultaneous Feature Aggregating and Hashing for Compact Binary Code Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.11820v1
- **DOI**: 10.1109/TIP.2019.2913509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11820v1)
- **Published**: 2019-04-24 22:05:07+00:00
- **Updated**: 2019-04-24 22:05:07+00:00
- **Authors**: Thanh-Toan Do, Khoa Le, Tuan Hoang, Huu Le, Tam V. Nguyen, Ngai-Man Cheung
- **Comment**: Accepted to IEEE Trans. on Image Processing (TIP), 2019. arXiv admin
  note: substantial text overlap with arXiv:1704.00860
- **Journal**: None
- **Summary**: Representing images by compact hash codes is an attractive approach for large-scale content-based image retrieval. In most state-of-the-art hashing-based image retrieval systems, for each image, local descriptors are first aggregated as a global representation vector. This global vector is then subjected to a hashing function to generate a binary hash code. In previous works, the aggregating and the hashing processes are designed independently. Hence these frameworks may generate suboptimal hash codes. In this paper, we first propose a novel unsupervised hashing framework in which feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization generates aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, the proposed method is flexible. It can be extended for supervised hashing. When the data label is available, the framework can be adapted to learn binary codes which minimize the reconstruction loss w.r.t. label vectors. Furthermore, we also propose a fast version of the state-of-the-art hashing method Binary Autoencoder to be used in our proposed frameworks. Extensive experiments on benchmark datasets under various settings show that the proposed methods outperform state-of-the-art unsupervised and supervised hashing methods.



### Deep Sparse Representation-based Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.11093v1
- **DOI**: 10.1109/LSP.2019.2913022
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, 62H30, I.5.3; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1904.11093v1)
- **Published**: 2019-04-24 22:52:18+00:00
- **Updated**: 2019-04-24 22:52:18+00:00
- **Authors**: Mahdi Abavisani, Vishal M. Patel
- **Comment**: None
- **Journal**: IEEE Signal Processing Letters, 2019
- **Summary**: We present a transductive deep learning-based formulation for the sparse representation-based classification (SRC) method. The proposed network consists of a convolutional autoencoder along with a fully-connected layer. The role of the autoencoder network is to learn robust deep features for classification. On the other hand, the fully-connected layer, which is placed in between the encoder and the decoder networks, is responsible for finding the sparse representation. The estimated sparse codes are then used for classification. Various experiments on three different datasets show that the proposed network leads to sparse representations that give better classification results than state-of-the-art SRC methods. The source code is available at: github.com/mahdiabavisani/DSRC.



