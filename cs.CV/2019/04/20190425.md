# Arxiv Papers in cs.CV on 2019-04-25
### Learning the Depths of Moving People by Watching Frozen People
- **Arxiv ID**: http://arxiv.org/abs/1904.11111v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11111v1)
- **Published**: 2019-04-25 01:10:24+00:00
- **Updated**: 2019-04-25 01:10:24+00:00
- **Authors**: Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, William T. Freeman
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: We present a method for predicting dense depth in scenarios where both a monocular camera and people in the scene are freely moving. Existing methods for recovering depth for dynamic, non-rigid objects from monocular video impose strong assumptions on the objects' motion and may only recover sparse depth. In this paper, we take a data-driven approach and learn human depth priors from a new source of data: thousands of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a hand-held camera tours the scene. Because people are stationary, training data can be generated using multi-view stereo reconstruction. At inference time, our method uses motion parallax cues from the static areas of the scenes to guide the depth prediction. We demonstrate our method on real-world sequences of complex human actions captured by a moving hand-held camera, show improvement over state-of-the-art monocular depth prediction methods, and show various 3D effects produced using our predicted depth.



### Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.11112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11112v1)
- **Published**: 2019-04-25 01:13:16+00:00
- **Updated**: 2019-04-25 01:13:16+00:00
- **Authors**: Chaoyang Wang, Simon Lucey, Federico Perazzi, Oliver Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a fully data-driven method to compute depth from diverse monocular video sequences that contain large amounts of non-rigid objects, e.g., people. In order to learn reconstruction cues for non-rigid scenes, we introduce a new dataset consisting of stereo videos scraped in-the-wild. This dataset has a wide variety of scene types, and features large amounts of nonrigid objects, especially people. From this, we compute disparity maps to be used as supervision to train our approach. We propose a loss function that allows us to generate a depth prediction even with unknown camera intrinsics and stereo baselines in the dataset. We validate the use of large amounts of Internet video by evaluating our method on existing video datasets with depth supervision, including SINTEL, and KITTI, and show that our approach generalizes better to natural scenes.



### Skin Cancer Segmentation and Classification with NABLA-N and Inception Recurrent Residual Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.11126v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11126v1)
- **Published**: 2019-04-25 02:24:55+00:00
- **Updated**: 2019-04-25 02:24:55+00:00
- **Authors**: Md Zahangir Alom, Theus Aspiras, Tarek M. Taha, Vijayan K. Asari
- **Comment**: 7 pages, 7 figures, 2 Tables
- **Journal**: None
- **Summary**: In the last few years, Deep Learning (DL) has been showing superior performance in different modalities of biomedical image analysis. Several DL architectures have been proposed for classification, segmentation, and detection tasks in medical imaging and computational pathology. In this paper, we propose a new DL architecture, the NABLA-N network, with better feature fusion techniques in decoding units for dermoscopic image segmentation tasks. The NABLA-N network has several advances for segmentation tasks. First, this model ensures better feature representation for semantic segmentation with a combination of low to high-level feature maps. Second, this network shows better quantitative and qualitative results with the same or fewer network parameters compared to other methods. In addition, the Inception Recurrent Residual Convolutional Neural Network (IRRCNN) model is used for skin cancer classification. The proposed NABLA-N network and IRRCNN models are evaluated for skin cancer segmentation and classification on the benchmark datasets from the International Skin Imaging Collaboration 2018 (ISIC-2018). The experimental results show superior performance on segmentation tasks compared to the Recurrent Residual U-Net (R2U-Net). The classification model shows around 87% testing accuracy for dermoscopic skin cancer classification on ISIC2018 dataset.



### CBHE: Corner-based Building Height Estimation for Complex Street Scene Images
- **Arxiv ID**: http://arxiv.org/abs/1904.11128v2
- **DOI**: 10.1145/3308558.3313394
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.11128v2)
- **Published**: 2019-04-25 02:30:50+00:00
- **Updated**: 2021-12-16 00:52:30+00:00
- **Authors**: Yunxiang Zhao, Jianzhong Qi, Rui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Building height estimation is important in many applications such as 3D city reconstruction, urban planning, and navigation. Recently, a new building height estimation method using street scene images and 2D maps was proposed. This method is more scalable than traditional methods that use high-resolution optical data, LiDAR data, or RADAR data which are expensive to obtain. The method needs to detect building rooflines and then compute building height via the pinhole camera model. We observe that this method has limitations in handling complex street scene images in which buildings overlap with each other and the rooflines are difficult to locate. We propose CBHE, a building height estimation algorithm considering both building corners and rooflines. CBHE first obtains building corner and roofline candidates in street scene images based on building footprints from 2D maps and the camera parameters. Then, we use a deep neural network named BuildingNet to classify and filter corner and roofline candidates. Based on the valid corners and rooflines from BuildingNet, CBHE computes building height via the pinhole camera model. Experimental results show that the proposed BuildingNet yields a higher accuracy on building corner and roofline candidate filtering compared with the state-of-the-art open set classifiers. Meanwhile, CBHE outperforms the baseline algorithm by over 10% in building height estimation accuracy.



### Learning Discriminative Features Via Weights-biased Softmax Loss
- **Arxiv ID**: http://arxiv.org/abs/1904.11138v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11138v1)
- **Published**: 2019-04-25 03:24:53+00:00
- **Updated**: 2019-04-25 03:24:53+00:00
- **Authors**: XiaoBin Li, WeiQiang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Loss functions play a key role in training superior deep neural networks. In convolutional neural networks (CNNs), the popular cross entropy loss together with softmax does not explicitly guarantee minimization of intra-class variance or maximization of inter-class variance. In the early studies, there is no theoretical analysis and experiments explicitly indicating how to choose the number of units in fully connected layer. To help CNNs learn features more fast and discriminative, there are two contributions in this paper. First, we determine the minimum number of units in FC layer by rigorous theoretical analysis and extensive experiment, which reduces CNNs' parameter memory and training time. Second, we propose a negative-focused weights-biased softmax (W-Softmax) loss to help CNNs learn more discriminative features. The proposed W-Softmax loss not only theoretically formulates the intraclass compactness and inter-class separability, but also can avoid overfitting by enlarging decision margins. Moreover, the size of decision margins can be flexibly controlled by adjusting a hyperparameter $\alpha$. Extensive experimental results on several benchmark datasets show the superiority of W-Softmax in image classification tasks.



### HAR-Net: Joint Learning of Hybrid Attention for Single-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.11141v1
- **DOI**: 10.1109/TIP.2019.2957850
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11141v1)
- **Published**: 2019-04-25 03:37:19+00:00
- **Updated**: 2019-04-25 03:37:19+00:00
- **Authors**: Ya-Li Li, Shengjin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection has been a challenging task in computer vision. Although significant progress has been made in object detection with deep neural networks, the attention mechanism is far from development. In this paper, we propose the hybrid attention mechanism for single-stage object detection. First, we present the modules of spatial attention, channel attention and aligned attention for single-stage object detection. In particular, stacked dilated convolution layers with symmetrically fixed rates are constructed to learn spatial attention. The channel attention is proposed with the cross-level group normalization and squeeze-and-excitation module. Aligned attention is constructed with organized deformable filters. Second, the three kinds of attention are unified to construct the hybrid attention mechanism. We then embed the hybrid attention into Retina-Net and propose the efficient single-stage HAR-Net for object detection. The attention modules and the proposed HAR-Net are evaluated on the COCO detection dataset. Experiments demonstrate that hybrid attention can significantly improve the detection accuracy and the HAR-Net can achieve the state-of-the-art 45.8\% mAP, outperform existing single-stage object detectors.



### A Deeper Look at Facial Expression Dataset Bias
- **Arxiv ID**: http://arxiv.org/abs/1904.11150v1
- **DOI**: 10.1109/TAFFC.2020.2973158
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11150v1)
- **Published**: 2019-04-25 04:07:13+00:00
- **Updated**: 2019-04-25 04:07:13+00:00
- **Authors**: Shan Li, Weihong Deng
- **Comment**: None
- **Journal**: IEEE Transactions on Affective Computing 2020
- **Summary**: Datasets play an important role in the progress of facial expression recognition algorithms, but they may suffer from obvious biases caused by different cultures and collection conditions. To look deeper into this bias, we first conduct comprehensive experiments on dataset recognition and crossdataset generalization tasks, and for the first time explore the intrinsic causes of the dataset discrepancy. The results quantitatively verify that current datasets have a strong buildin bias and corresponding analyses indicate that the conditional probability distributions between source and target datasets are different. However, previous researches are mainly based on shallow features with limited discriminative ability under the assumption that the conditional distribution remains unchanged across domains. To address these issues, we further propose a novel deep Emotion-Conditional Adaption Network (ECAN) to learn domain-invariant and discriminative feature representations, which can match both the marginal and the conditional distributions across domains simultaneously. In addition, the largely ignored expression class distribution bias is also addressed by a learnable re-weighting parameter, so that the training and testing domains can share similar class distribution. Extensive cross-database experiments on both lab-controlled datasets (CK+, JAFFE, MMI and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB 2.0 and SFEW 2.0) demonstrate that our ECAN can yield competitive performances across various facial expression transfer tasks and outperform the state-of-theart methods.



### Deep Multi-View Learning using Neuron-Wise Correlation-Maximizing Regularizers
- **Arxiv ID**: http://arxiv.org/abs/1904.11151v1
- **DOI**: 10.1109/TIP.2019.2912356
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11151v1)
- **Published**: 2019-04-25 04:10:10+00:00
- **Updated**: 2019-04-25 04:10:10+00:00
- **Authors**: Kui Jia, Jiehong Lin, Mingkui Tan, Dacheng Tao
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing 2019
- **Summary**: Many machine learning problems concern with discovering or associating common patterns in data of multiple views or modalities. Multi-view learning is of the methods to achieve such goals. Recent methods propose deep multi-view networks via adaptation of generic Deep Neural Networks (DNNs), which concatenate features of individual views at intermediate network layers (i.e., fusion layers). In this work, we study the problem of multi-view learning in such end-to-end networks. We take a regularization approach via multi-view learning criteria, and propose a novel, effective, and efficient neuron-wise correlation-maximizing regularizer. We implement our proposed regularizers collectively as a correlation-regularized network layer (CorrReg). CorrReg can be applied to either fully-connected or convolutional fusion layers, simply by replacing them with their CorrReg counterparts. By partitioning neurons of a hidden layer in generic DNNs into multiple subsets, we also consider a multi-view feature learning perspective of generic DNNs. Such a perspective enables us to study deep multi-view learning in the context of regularized network training, for which we present control experiments of benchmark image classification to show the efficacy of our proposed CorrReg. To investigate how CorrReg is useful for practical multi-view learning problems, we conduct experiments of RGB-D object/scene recognition and multi-view based 3D object recognition, using networks with fusion layers that concatenate intermediate features of individual modalities or views for subsequent classification. Applying CorrReg to fusion layers of these networks consistently improves classification performance. In particular, we achieve the new state of the art on the benchmark RGB-D object and RGB-D scene datasets. We make the implementation of CorrReg publicly available.



### Out of the Box: A combined approach for handling occlusion in Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.11157v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11157v1)
- **Published**: 2019-04-25 05:10:18+00:00
- **Updated**: 2019-04-25 05:10:18+00:00
- **Authors**: Rohit Jena
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: Human Pose estimation is a challenging problem, especially in the case of 3D pose estimation from 2D images due to many different factors like occlusion, depth ambiguities, intertwining of people, and in general crowds. 2D multi-person human pose estimation in the wild also suffers from the same problems - occlusion, ambiguities, and disentanglement of people's body parts. Being a fundamental problem with loads of applications, including but not limited to surveillance, economical motion capture for video games and movies, and physiotherapy, this is an interesting problem to be solved both from a practical perspective and from an intellectual perspective as well. Although there are cases where no pose estimation can ever predict with 100% accuracy (cases where even humans would fail), there are several algorithms that have brought new state-of-the-art performance in human pose estimation in the wild. We look at a few algorithms with different approaches and also formulate our own approach to tackle a consistently bugging problem, i.e. occlusions.



### A Conditional Adversarial Network for Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.11163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11163v1)
- **Published**: 2019-04-25 06:03:06+00:00
- **Updated**: 2019-04-25 06:03:06+00:00
- **Authors**: Ravi Kumar Thakur, Snehasis Mukherjee
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of Scene flow estimation in depth videos has been attracting attention of researchers of robot vision, due to its potential application in various areas of robotics. The conventional scene flow methods are difficult to use in reallife applications due to their long computational overhead. We propose a conditional adversarial network SceneFlowGAN for scene flow estimation. The proposed SceneFlowGAN uses loss function at two ends: both generator and descriptor ends. The proposed network is the first attempt to estimate scene flow using generative adversarial networks, and is able to estimate both the optical flow and disparity from the input stereo images simultaneously. The proposed method is experimented on a large RGB-D benchmark sceneflow dataset.



### Indoor dense depth map at drone hovering
- **Arxiv ID**: http://arxiv.org/abs/1904.11175v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11175v1)
- **Published**: 2019-04-25 06:48:08+00:00
- **Updated**: 2019-04-25 06:48:08+00:00
- **Authors**: Arindam Saha, Soumyadip Maity, Brojeshwar Bhowmick
- **Comment**: Published on ICIP 2018
- **Journal**: None
- **Summary**: Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recent years. Autonomous flight in indoor requires a dense depth map for navigable space detection which is the fundamental component for autonomous navigation. In this paper, we address the problem of reconstructing dense depth while a drone is hovering (small camera motion) in indoor scenes using already estimated cameras and sparse point cloud obtained from a vSLAM. We start by segmenting the scene based on sudden depth variation using sparse 3D points and introduce a patch-based local plane fitting via energy minimization which combines photometric consistency and co-planarity with neighbouring patches. The method also combines a plane sweep technique for image segments having almost no sparse point for initialization. Experiments show, the proposed method produces better depth for indoor in artificial lighting condition, low-textured environment compared to earlier literature in small motion.



### Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping for 4K UHD HDR Applications
- **Arxiv ID**: http://arxiv.org/abs/1904.11176v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11176v3)
- **Published**: 2019-04-25 06:51:11+00:00
- **Updated**: 2019-08-31 09:44:39+00:00
- **Authors**: Soo Ye Kim, Jihyong Oh, Munchurl Kim
- **Comment**: Accepted at ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Recent modern displays are now able to render high dynamic range (HDR), high resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently, UHD HDR broadcasting and streaming have emerged as high quality premium services. However, due to the lack of original UHD HDR video content, appropriate conversion technologies are urgently needed to transform the legacy low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions. In this paper, we propose a joint super-resolution (SR) and inverse tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an intricate task, where high frequency details must be restored for SR, jointly with the local contrast, for ITM. Our network is able to restore fine details by decomposing the input image and focusing on the separate base (low frequency) and detail (high frequency) layers. Moreover, the proposed modulation blocks apply location-variant operations to enhance local contrast. The Deep SR-ITM shows good subjective quality with increased contrast and details, outperforming the previous joint SR-ITM method.



### Optimal Approach for Image Recognition using Deep Convolutional Architecture
- **Arxiv ID**: http://arxiv.org/abs/1904.11187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.11187v1)
- **Published**: 2019-04-25 07:40:30+00:00
- **Updated**: 2019-04-25 07:40:30+00:00
- **Authors**: Parth Shah, Vishvajit Bakrola, Supriya Pati
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent time deep learning has achieved huge popularity due to its performance in various machine learning algorithms. Deep learning as hierarchical or structured learning attempts to model high level abstractions in data by using a group of processing layers. The foundation of deep learning architectures is inspired by the understanding of information processing and neural responses in human brain. The architectures are created by stacking multiple linear or non-linear operations. The article mainly focuses on the state-of-art deep learning models and various real world applications specific training methods. Selecting optimal architecture for specific problem is a challenging task, at a closing stage of the article we proposed optimal approach to deep convolutional architecture for the application of image recognition.



### Transferrable Prototypical Networks for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1904.11227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11227v1)
- **Published**: 2019-04-25 09:21:16+00:00
- **Updated**: 2019-04-25 09:21:16+00:00
- **Authors**: Yingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-Wah Ngo, Tao Mei
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: In this paper, we introduce a new idea for unsupervised domain adaptation via a remold of Prototypical Networks, which learn an embedding space and perform classification via a remold of the distances to the prototype of each class. Specifically, we present Transferrable Prototypical Networks (TPN) for adaptation such that the prototypes for each class in source and target domains are close in the embedding space and the score distributions predicted by prototypes separately on source and target data are similar. Technically, TPN initially matches each target example to the nearest prototype in the source domain and assigns an example a "pseudo" label. The prototype of each class could then be computed on source-only, target-only and source-target data, respectively. The optimization of TPN is end-to-end trained by jointly minimizing the distance across the prototypes on three types of data and KL-divergence of score distributions output by each pair of the prototypes. Extensive experiments are conducted on the transfers across MNIST, USPS and SVHN datasets, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain an accuracy of 80.4% of single model on VisDA 2017 dataset.



### Unsupervised Label Noise Modeling and Loss Correction
- **Arxiv ID**: http://arxiv.org/abs/1904.11238v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11238v2)
- **Published**: 2019-04-25 09:53:48+00:00
- **Updated**: 2019-06-05 14:23:24+00:00
- **Authors**: Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, Kevin McGuinness
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at https://git.io/fjsvE



### Exploring Object Relation in Mean Teacher for Cross-Domain Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.11245v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11245v2)
- **Published**: 2019-04-25 10:03:44+00:00
- **Updated**: 2019-12-25 05:20:30+00:00
- **Authors**: Qi Cai, Yingwei Pan, Chong-Wah Ngo, Xinmei Tian, Lingyu Duan, Ting Yao
- **Comment**: CVPR 2019; The codes and model of our MTOR are publicly available at:
  https://github.com/caiqi/mean-teacher-cross-domain-detection
- **Journal**: None
- **Summary**: Rendering synthetic data (e.g., 3D CAD-rendered images) to generate annotations for learning deep models in vision tasks has attracted increasing attention in recent years. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. To address this issue, recent progress in cross-domain recognition has featured the Mean Teacher, which directly simulates unsupervised domain adaptation as semi-supervised learning. The domain gap is thus naturally bridged with consistency regularization in a teacher-student scheme. In this work, we advance this Mean Teacher paradigm to be applicable for cross-domain detection. Specifically, we present Mean Teacher with Object Relations (MTOR) that novelly remolds Mean Teacher under the backbone of Faster R-CNN by integrating the object relations into the measure of consistency cost between teacher and student modules. Technically, MTOR firstly learns relational graphs that capture similarities between pairs of regions for teacher and student respectively. The whole architecture is then optimized with three consistency regularizations: 1) region-level consistency to align the region-level predictions between teacher and student, 2) inter-graph consistency for matching the graph structures between teacher and student, and 3) intra-graph consistency to enhance the similarity between regions of same class within the graph of student. Extensive experiments are conducted on the transfers across Cityscapes, Foggy Cityscapes, and SIM10k, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain a new record of single model: 22.8% of mAP on Syn2Real detection dataset.



### MSDC-Net: Multi-Scale Dense and Contextual Networks for Automated Disparity Map for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.12658v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12658v2)
- **Published**: 2019-04-25 10:12:56+00:00
- **Updated**: 2019-04-30 02:50:46+00:00
- **Authors**: Zhibo Rao, Mingyi He, Yuchao Dai, Zhidong Zhu, Bo Li, Renjie He
- **Comment**: Accepted at ICIGP2019
- **Journal**: None
- **Summary**: Disparity prediction from stereo images is essential to computer vision applications including autonomous driving, 3D model reconstruction, and object detection. To predict accurate disparity map, we propose a novel deep learning architecture for detectingthe disparity map from a rectified pair of stereo images, called MSDC-Net. Our MSDC-Net contains two modules: multi-scale fusion 2D convolution and multi-scale residual 3D convolution modules. The multi-scale fusion 2D convolution module exploits the potential multi-scale features, which extracts and fuses the different scale features by Dense-Net. The multi-scale residual 3D convolution module learns the different scale geometry context from the cost volume which aggregated by the multi-scale fusion 2D convolution module. Experimental results on Scene Flow and KITTI datasets demonstrate that our MSDC-Net significantly outperforms other approaches in the non-occluded region.



### Pointing Novel Objects in Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.11251v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11251v1)
- **Published**: 2019-04-25 10:22:35+00:00
- **Updated**: 2019-04-25 10:22:35+00:00
- **Authors**: Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, Tao Mei
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Image captioning has received significant attention with remarkable improvements in recent advances. Nevertheless, images in the wild encapsulate rich knowledge and cannot be sufficiently described with models built on image-caption pairs containing only in-domain objects. In this paper, we propose to address the problem by augmenting standard deep captioning architectures with object learners. Specifically, we present Long Short-Term Memory with Pointing (LSTM-P) --- a new architecture that facilitates vocabulary expansion and produces novel objects via pointing mechanism. Technically, object learners are initially pre-trained on available object recognition data. Pointing in LSTM-P then balances the probability between generating a word through LSTM and copying a word from the recognized objects at each time step in decoder stage. Furthermore, our captioning encourages global coverage of objects in the sentence. Extensive experiments are conducted on both held-out COCO image captioning and ImageNet datasets for describing novel objects, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain an average of 60.9% in F1 score on held-out COCO~dataset.



### On guiding video object segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.11256v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11256v1)
- **Published**: 2019-04-25 11:03:16+00:00
- **Updated**: 2019-04-25 11:03:16+00:00
- **Authors**: Diego Ortego, Kevin McGuinness, Juan C. SanMiguel, Eric Arazo, José M. Martínez, Noel E. O'Connor
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel approach for segmenting moving objects in unconstrained environments using guided convolutional neural networks. This guiding process relies on foreground masks from independent algorithms (i.e. state-of-the-art algorithms) to implement an attention mechanism that incorporates the spatial location of foreground and background to compute their separated representations. Our approach initially extracts two kinds of features for each frame using colour and optical flow information. Such features are combined following a multiplicative scheme to benefit from their complementarity. These unified colour and motion features are later processed to obtain the separated foreground and background representations. Then, both independent representations are concatenated and decoded to perform foreground segmentation. Experiments conducted on the challenging DAVIS 2016 dataset demonstrate that our guided representations not only outperform non-guided, but also recent and top-performing video object segmentation algorithms.



### LADN: Local Adversarial Disentangling Network for Facial Makeup and De-Makeup
- **Arxiv ID**: http://arxiv.org/abs/1904.11272v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11272v2)
- **Published**: 2019-04-25 11:52:06+00:00
- **Updated**: 2019-08-09 07:31:36+00:00
- **Authors**: Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Qiao and Guanzhi have equal contribution. Accepted to ICCV 2019.
  Project website: https://georgegu1997.github.io/LADN-project-page/
- **Journal**: None
- **Summary**: We propose a local adversarial disentangling network (LADN) for facial makeup and de-makeup. Central to our method are multiple and overlapping local adversarial discriminators in a content-style disentangling network for achieving local detail transfer between facial images, with the use of asymmetric loss functions for dramatic makeup styles with high-frequency details. Existing techniques do not demonstrate or fail to transfer high-frequency details in a global adversarial setting, or train a single local discriminator only to ensure image structure consistency and thus work only for relatively simple styles. Unlike others, our proposed local adversarial discriminators can distinguish whether the generated local image details are consistent with the corresponding regions in the given reference image in cross-image style transfer in an unsupervised setting. Incorporating these technical contributions, we achieve not only state-of-the-art results on conventional styles but also novel results involving complex and dramatic styles with high-frequency details covering large areas across multiple facial features. A carefully designed dataset of unpaired before and after makeup images is released.



### Reducing Anomaly Detection in Images to Detection in Noise
- **Arxiv ID**: http://arxiv.org/abs/1904.11276v1
- **DOI**: 10.1109/ICIP.2018.8451059
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11276v1)
- **Published**: 2019-04-25 11:59:04+00:00
- **Updated**: 2019-04-25 11:59:04+00:00
- **Authors**: Axel Davy, Thibaud Ehret, Jean-Michel Morel, Mauricio Delbracio
- **Comment**: ICIP 2018
- **Journal**: 25th IEEE International Conference on Image Processing (2018)
  1058-1062
- **Summary**: Anomaly detectors address the difficult problem of detecting automatically exceptions in an arbitrary background image. Detection methods have been proposed by the thousands because each problem requires a different background model. By analyzing the existing approaches, we show that the problem can be reduced to detecting anomalies in residual images (extracted from the target image) in which noise and anomalies prevail. Hence, the general and impossible background modeling problem is replaced by simpler noise modeling, and allows the calculation of rigorous thresholds based on the a contrario detection theory. Our approach is therefore unsupervised and works on arbitrary images.



### Multi-scale Cross-form Pyramid Network for Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.11309v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11309v3)
- **Published**: 2019-04-25 13:02:01+00:00
- **Updated**: 2019-06-04 05:44:34+00:00
- **Authors**: Zhidong Zhu, Mingyi He, Yuchao Dai, Zhibo Rao, Bo Li
- **Comment**: Accepted by ICIEA2019
- **Journal**: None
- **Summary**: Stereo matching plays an indispensable part in autonomous driving, robotics and 3D scene reconstruction. We propose a novel deep learning architecture, which called CFP-Net, a Cross-Form Pyramid stereo matching network for regressing disparity from a rectified pair of stereo images. The network consists of three modules: Multi-Scale 2D local feature extraction module, Cross-form spatial pyramid module and Multi-Scale 3D Feature Matching and Fusion module. The Multi-Scale 2D local feature extraction module can extract enough multi-scale features. The Cross-form spatial pyramid module aggregates the context information in different scales and locations to form a cost volume. Moreover, it is proved to be more effective than SPP and ASPP in ill-posed regions. The Multi-Scale 3D feature matching and fusion module is proved to regularize the cost volume using two parallel 3D deconvolution structure with two different receptive fields. Our proposed method has been evaluated on the Scene Flow and KITTI datasets. It achieves state-of-the-art performance on the KITTI 2012 and 2015 benchmarks.



### JPEG XT Image Compression with Hue Compensation for Two-Layer HDR Coding
- **Arxiv ID**: http://arxiv.org/abs/1904.11315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11315v1)
- **Published**: 2019-04-25 13:07:31+00:00
- **Updated**: 2019-04-25 13:07:31+00:00
- **Authors**: Hiroyuki Kobayashi, Hitoshi Kiya
- **Comment**: To appear in The 4th IEEE International Conference on Consumer
  Electronics (ICCE) Asia, Bangkok Thailand
- **Journal**: None
- **Summary**: We propose a novel JPEG XT image compression with hue compensation for two-layer HDR coding. LDR images produced from JPEG XT bitstreams have some distortion in hue due to tone mapping operations. In order to suppress the color distortion, we apply a novel hue compensation method based on the maximally saturated colors. Moreover, the bitstreams generated by using the proposed method are fully compatible with the JPEG XT standard. In an experiment, the proposed method is demonstrated not only to produce images with small hue degradation but also to maintain well-mapped luminance, in terms of three kinds of criterion: TMQI, hue value in CIEDE2000, and the maximally saturated color on the constant-hue plane.



### Unsupervised Deep Learning for Bayesian Brain MRI Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.11319v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11319v2)
- **Published**: 2019-04-25 13:13:51+00:00
- **Updated**: 2019-07-23 17:26:09+00:00
- **Authors**: Adrian V. Dalca, Evan Yu, Polina Golland, Bruce Fischl, Mert R. Sabuncu, Juan Eugenio Iglesias
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Probabilistic atlas priors have been commonly used to derive adaptive and robust brain MRI segmentation algorithms. Widely-used neuroimage analysis pipelines rely heavily on these techniques, which are often computationally expensive. In contrast, there has been a recent surge of approaches that leverage deep learning to implement segmentation tools that are computationally efficient at test time. However, most of these strategies rely on learning from manually annotated images. These supervised deep learning methods are therefore sensitive to the intensity profiles in the training dataset. To develop a deep learning-based segmentation model for a new image dataset (e.g., of different contrast), one usually needs to create a new labeled training dataset, which can be prohibitively expensive, or rely on suboptimal ad hoc adaptation or augmentation approaches. In this paper, we propose an alternative strategy that combines a conventional probabilistic atlas-based segmentation with deep learning, enabling one to train a segmentation model for new MRI scans without the need for any manually segmented images. Our experiments include thousands of brain MRI scans and demonstrate that the proposed method achieves good accuracy for a brain MRI segmentation task for different MRI contrasts, requiring only approximately 15 seconds at test time on a GPU. The code is freely available at http://voxelmorph.mit.edu.



### Breast Cancer Classification with Ultrasound Images Based on SLIC
- **Arxiv ID**: http://arxiv.org/abs/1904.11322v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11322v2)
- **Published**: 2019-04-25 13:23:29+00:00
- **Updated**: 2019-11-24 10:39:11+00:00
- **Authors**: Zhihao Fang, Wanyi Zhang, He Ma
- **Comment**: This is a pre-print of a contribution published in Frontier Computing
  - Theory, Technologies and Applications (FC 2019) published by Springer
- **Journal**: None
- **Summary**: Ultrasound image diagnosis of breast tumors has been widely used in recent years. However, there are some problems of it, for instance, poor quality, intense noise and uneven echo distribution, which has created a huge obstacle to diagnosis. To overcome these problems, we propose a novel method, a breast cancer classification with ultrasound images based on SLIC (BCCUI). We first utilize the Region of Interest (ROI) extraction based on Simple Linear Iterative Clustering (SLIC) algorithm and region growing algorithm to extract the ROI at the super-pixel level. Next, the features of ROI are extracted. Furthermore, the Support Vector Machine (SVM) classifier is applied. The calculation states that the accuracy of this segment algorithm is up to 88.00% and the sensitivity of the algorithm is up to 92.05%, which proves that the classifier presents in this paper has certain research meaning and applied worthiness.



### Deep Constrained Dominant Sets for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.11397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11397v2)
- **Published**: 2019-04-25 15:07:13+00:00
- **Updated**: 2019-06-18 21:27:08+00:00
- **Authors**: Leulseged Tesfaye Alemu, Marcello Pelillo, Mubarak Shah
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (re-id) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a {\em constrained clustering optimization} problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale Resnet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods.



### DynamoNet: Dynamic Action and Motion Network
- **Arxiv ID**: http://arxiv.org/abs/1904.11407v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11407v1)
- **Published**: 2019-04-25 15:27:12+00:00
- **Updated**: 2019-04-25 15:27:12+00:00
- **Authors**: Ali Diba, Vivek Sharma, Luc Van Gool, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we are interested in self-supervised learning the motion cues in videos using dynamic motion filters for a better motion representation to finally boost human action recognition in particular. Thus far, the vision community has focused on spatio-temporal approaches using standard filters, rather we here propose dynamic filters that adaptively learn the video-specific internal motion representation by predicting the short-term future frames. We name this new motion representation, as dynamic motion representation (DMR) and is embedded inside of 3D convolutional network as a new layer, which captures the visual appearance and motion dynamics throughout entire video clip via end-to-end network learning. Simultaneously, we utilize these motion representation to enrich video classification. We have designed the frame prediction task as an auxiliary task to empower the classification problem. With these overall objectives, to this end, we introduce a novel unified spatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the video classification and learning motion representation by predicting future frames as a multi-task learning problem. We conduct experiments on challenging human action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the proposed DynamoNet show promising results on all the datasets.



### Large Scale Holistic Video Understanding
- **Arxiv ID**: http://arxiv.org/abs/1904.11451v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11451v3)
- **Published**: 2019-04-25 16:49:13+00:00
- **Updated**: 2020-12-15 06:53:39+00:00
- **Authors**: Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri, Jurgen Gall, Rainer Stiefelhagen, Luc Van Gool
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale "Holistic Video Understanding Dataset"~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.   We demonstrate the generalization capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called "Holistic Appearance and Temporal Network"~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications.



### Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.11466v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.11466v1)
- **Published**: 2019-04-25 17:20:31+00:00
- **Updated**: 2019-04-25 17:20:31+00:00
- **Authors**: Gregory P. Meyer, Jake Charland, Darshan Hegde, Ankit Laddha, Carlos Vallespi-Gonzalez
- **Comment**: Accepted for publication at CVPR Workshop on Autonomous Driving 2019
- **Journal**: None
- **Summary**: In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime.



### The Mutex Watershed and its Objective: Efficient, Parameter-Free Graph Partitioning
- **Arxiv ID**: http://arxiv.org/abs/1904.12654v2
- **DOI**: 10.1109/TPAMI.2020.2980827
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12654v2)
- **Published**: 2019-04-25 17:29:45+00:00
- **Updated**: 2021-04-19 13:06:08+00:00
- **Authors**: Steffen Wolf, Alberto Bailoni, Constantin Pape, Nasim Rahaman, Anna Kreshuk, Ullrich Köthe, Fred A. Hamprecht
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (2020) 1-1
- **Summary**: Image partitioning, or segmentation without semantics, is the task of decomposing an image into distinct segments, or equivalently to detect closed contours. Most prior work either requires seeds, one per segment; or a threshold; or formulates the task as multicut / correlation clustering, an NP-hard problem. Here, we propose an efficient algorithm for graph partitioning, the "Mutex Watershed''. Unlike seeded watershed, the algorithm can accommodate not only attractive but also repulsive cues, allowing it to find a previously unspecified number of segments without the need for explicit seeds or a tunable threshold. We also prove that this simple algorithm solves to global optimality an objective function that is intimately related to the multicut / correlation clustering integer linear programming formulation. The algorithm is deterministic, very simple to implement, and has empirically linearithmic complexity. When presented with short-range attractive and long-range repulsive cues from a deep neural network, the Mutex Watershed gives the best results currently known for the competitive ISBI 2012 EM segmentation benchmark.



### Radar-only ego-motion estimation in difficult settings via graph matching
- **Arxiv ID**: http://arxiv.org/abs/1904.11476v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11476v1)
- **Published**: 2019-04-25 17:40:02+00:00
- **Updated**: 2019-04-25 17:40:02+00:00
- **Authors**: Sarah H. Cen, Paul Newman
- **Comment**: 6 content pages, 1 page of references, 5 figures, 4 tables, 2019 IEEE
  International Conference on Robotics and Automation (ICRA)
- **Journal**: None
- **Summary**: Radar detects stable, long-range objects under variable weather and lighting conditions, making it a reliable and versatile sensor well suited for ego-motion estimation. In this work, we propose a radar-only odometry pipeline that is highly robust to radar artifacts (e.g., speckle noise and false positives) and requires only one input parameter. We demonstrate its ability to adapt across diverse settings, from urban UK to off-road Iceland, achieving a scan matching accuracy of approximately 5.20 cm and 0.0929 deg when using GPS as ground truth (compared to visual odometry's 5.77 cm and 0.1032 deg). We present algorithms for keypoint extraction and data association, framing the latter as a graph matching optimization problem, and provide an in-depth system analysis.



### Making Convolutional Networks Shift-Invariant Again
- **Arxiv ID**: http://arxiv.org/abs/1904.11486v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11486v2)
- **Published**: 2019-04-25 17:56:21+00:00
- **Updated**: 2019-06-09 00:27:38+00:00
- **Authors**: Richard Zhang
- **Comment**: Accepted to ICML 2019
- **Journal**: None
- **Summary**: Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe \textit{increased accuracy} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe \textit{better generalization}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ .



### Blurring the Line Between Structure and Learning to Optimize and Adapt Receptive Fields
- **Arxiv ID**: http://arxiv.org/abs/1904.11487v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11487v1)
- **Published**: 2019-04-25 17:57:10+00:00
- **Updated**: 2019-04-25 17:57:10+00:00
- **Authors**: Evan Shelhamer, Dequan Wang, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: The visual world is vast and varied, but its variations divide into structured and unstructured factors. We compose free-form filters and structured Gaussian filters, optimized end-to-end, to factorize deep representations and learn both local features and their degree of locality. Our semi-structured composition is strictly more expressive than free-form filtering, and changes in its structured parameters would require changes in free-form architecture. In effect this optimizes over receptive field size and shape, tuning locality to the data and task. Dynamic inference, in which the Gaussian structure varies with the input, adapts receptive field size to compensate for local scale variation. Optimizing receptive field size improves semantic segmentation accuracy on Cityscapes by 1-2 points for strong dilated and skip architectures and by up to 10 points for suboptimal designs. Adapting receptive fields by dynamic Gaussian structure further improves results, equaling the accuracy of free-form deformation while improving efficiency.



### Spatial-Temporal Relation Networks for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.11489v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11489v1)
- **Published**: 2019-04-25 17:59:17+00:00
- **Updated**: 2019-04-25 17:59:17+00:00
- **Authors**: Jiarui Xu, Yue Cao, Zheng Zhang, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is key to the success of trackers. A good similarity score is expected to reflect multiple cues, e.g. appearance, location, and topology, over a long period of time. However, these cues are heterogeneous, making them hard to be combined in a unified network. As a result, existing methods usually encode them in separate networks or require a complex training approach. In this paper, we present a unified framework for similarity measurement which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains. We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers. The resulting approach is named spatial-temporal relation networks (STRN). It runs in a feed-forward way and can be trained in an end-to-end manner. The state-of-the-art accuracy was achieved on all of the MOT15-17 benchmarks using public detection and online settings.



### RepPoints: Point Set Representation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.11490v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11490v2)
- **Published**: 2019-04-25 17:59:28+00:00
- **Updated**: 2019-08-19 14:12:18+00:00
- **Authors**: Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, Stephen Lin
- **Comment**: International Conference on Computer Vision (ICCV), 2019
- **Journal**: None
- **Summary**: Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present \textbf{RepPoints} (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 $AP_{50}$ on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at https://github.com/microsoft/RepPoints.



### Local Relation Networks for Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.11491v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11491v1)
- **Published**: 2019-04-25 17:59:35+00:00
- **Updated**: 2019-04-25 17:59:35+00:00
- **Authors**: Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin
- **Comment**: None
- **Journal**: None
- **Summary**: The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.



### GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1904.11492v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11492v1)
- **Published**: 2019-04-25 17:59:42+00:00
- **Updated**: 2019-04-25 17:59:42+00:00
- **Authors**: Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks. The code and configurations are released at https://github.com/xvjiarui/GCNet.



### Face Video Generation from a Single Image and Landmarks
- **Arxiv ID**: http://arxiv.org/abs/1904.11521v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11521v1)
- **Published**: 2019-04-25 18:10:27+00:00
- **Updated**: 2019-04-25 18:10:27+00:00
- **Authors**: Kritaphat Songsri-in, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we are concerned with the challenging problem of producing a full image sequence of a deformable face given only an image and generic facial motions encoded by a set of sparse landmarks. To this end we build upon recent breakthroughs in image-to-image translation such as pix2pix, CycleGAN and StarGAN which learn Deep Convolutional Neural Networks (DCNNs) that learn to map aligned pairs or images between different domains (i.e., having different labels) and propose a new architecture which is not driven any more by labels but by spatial maps, facial landmarks. In particular, we propose the MotionGAN which transforms an input face image into a new one according to a heatmap of target landmarks. We show that it is possible to create very realistic face videos using a single image and a set of target landmarks. Furthermore, our method can be used to edit a facial image with arbitrary motions according to landmarks (e.g., expression, speech, etc.). This provides much more flexibility to face editing, expression transfer, facial video creation, etc. than models based on discrete expressions, audios or action units.



### Unsupervised Deep Learning by Neighbourhood Discovery
- **Arxiv ID**: http://arxiv.org/abs/1904.11567v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11567v3)
- **Published**: 2019-04-25 20:09:00+00:00
- **Updated**: 2019-05-30 15:26:41+00:00
- **Authors**: Jiabo Huang, Qi Dong, Shaogang Gong, Xiatian Zhu
- **Comment**: 36th International Conference on Machine Learning (ICML'19). Code is
  available at https://github.com/Raymond-sci/AND
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) have demonstrated remarkable success in computer vision by supervisedly learning strong visual feature representations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deployment and scalability in many application scenarios. In this work, we introduce a generic unsupervised deep learning approach to training deep models without the need for any manual label supervision. Specifically, we progressively discover sample anchored/centred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is specially formulated so that all the member samples can share the same unseen class labels at high probability for facilitating the extraction of class discriminative feature representations during training. Experiments on image classification show the performance advantages of the proposed method over the state-of-the-art unsupervised learning models on six benchmarks including both coarse-grained and fine-grained object image categorisation.



### TVQA+: Spatio-Temporal Grounding for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1904.11574v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1904.11574v2)
- **Published**: 2019-04-25 20:37:26+00:00
- **Updated**: 2020-05-11 19:43:42+00:00
- **Authors**: Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal
- **Comment**: ACL 2020 camera-ready (15 pages)
- **Journal**: None
- **Summary**: We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. Dataset and code are publicly available at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus



### Multiple Linear Regression Haze-removal Model Based on Dark Channel Prior
- **Arxiv ID**: http://arxiv.org/abs/1904.11587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11587v1)
- **Published**: 2019-04-25 21:00:33+00:00
- **Updated**: 2019-04-25 21:00:33+00:00
- **Authors**: Binghan Li, Wenrui Zhang, Mi Lu
- **Comment**: IEEE CPS (CSCI 2018 Int'l Conference)
- **Journal**: None
- **Summary**: Dark Channel Prior (DCP) is a widely recognized traditional dehazing algorithm. However, it may fail in bright region and the brightness of the restored image is darker than hazy image. In this paper, we propose an effective method to optimize DCP. We build a multiple linear regression haze-removal model based on DCP atmospheric scattering model and train this model with RESIDE dataset, which aims to reduce the unexpected errors caused by the rough estimations of transmission map t(x) and atmospheric light A. The RESIDE dataset provides enough synthetic hazy images and their corresponding groundtruth images to train and test. We compare the performances of different dehazing algorithms in terms of two important full-reference metrics, the peak-signal-to-noise ratio (PSNR) as well as the structural similarity index measure (SSIM). The experiment results show that our model gets highest SSIM value and its PSNR value is also higher than most of state-of-the-art dehazing algorithms. Our results also overcome the weakness of DCP on real-world hazy images



### Optical Flow Techniques for Facial Expression Analysis -- a Practical Evaluation Study
- **Arxiv ID**: http://arxiv.org/abs/1904.11592v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11592v3)
- **Published**: 2019-04-25 21:14:13+00:00
- **Updated**: 2022-01-31 16:35:31+00:00
- **Authors**: Benjamin Allaert, Isaac Ronald Ward, Ioan Marius Bilasco, Chaabane Djeraba, Mohammed Bennamoun
- **Comment**: None
- **Journal**: None
- **Summary**: Optical flow techniques are becoming increasingly performant and robust when estimating motion in a scene, but their performance has yet to be proven in the area of facial expression recognition. In this work, a variety of optical flow approaches are evaluated across multiple facial expression datasets, so as to provide a consistent performance evaluation. The aim of this work is not to propose a new expression recognition technique, but to understand better the adequacy of existing state-of-the art optical flow for encoding facial motion in the context of facial expression recognition. Our evaluations highlight the fact that motion approximation methods used to overcome motion discontinuities have a significant impact when optical flows are used to characterize facial expressions.



### DeepPerimeter: Indoor Boundary Estimation from Posed Monocular Sequences
- **Arxiv ID**: http://arxiv.org/abs/1904.11595v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11595v2)
- **Published**: 2019-04-25 21:20:41+00:00
- **Updated**: 2019-07-01 21:52:11+00:00
- **Authors**: Ameya Phalak, Zhao Chen, Darvin Yi, Khushi Gupta, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: We present DeepPerimeter, a deep learning based pipeline for inferring a full indoor perimeter (i.e. exterior boundary map) from a sequence of posed RGB images. Our method relies on robust deep methods for depth estimation and wall segmentation to generate an exterior boundary point cloud, and then uses deep unsupervised clustering to fit wall planes to obtain a final boundary map of the room. We demonstrate that DeepPerimeter results in excellent visual and quantitative performance on the popular ScanNet and FloorNet datasets and works for room shapes of various complexities as well as in multiroom scenarios. We also establish important baselines for future work on indoor perimeter estimation, topics which will become increasingly prevalent as application areas like augmented reality and robotics become more significant.



### High-Resolution Network for Photorealistic Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1904.11617v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11617v1)
- **Published**: 2019-04-25 22:59:37+00:00
- **Updated**: 2019-04-25 22:59:37+00:00
- **Authors**: Ming Li, Chunyang Ye, Wei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Photorealistic style transfer aims to transfer the style of one image to another, but preserves the original structure and detail outline of the content image, which makes the content image still look like a real shot after the style transfer. Although some realistic image styling methods have been proposed, these methods are vulnerable to lose the details of the content image and produce some irregular distortion structures. In this paper, we use a high-resolution network as the image generation network. Compared to other methods, which reduce the resolution and then restore the high resolution, our generation network maintains high resolution throughout the process. By connecting high-resolution subnets to low-resolution subnets in parallel and repeatedly multi-scale fusion, high-resolution subnets can continuously receive information from low-resolution subnets. This allows our network to discard less information contained in the image, so the generated images may have a more elaborate structure and less distortion, which is crucial to the visual quality. We conducted extensive experiments and compared the results with existing methods. The experimental results show that our model is effective and produces better results than existing methods for photorealistic image stylization. Our source code with PyTorch framework will be publicly available at https://github.com/limingcv/Photorealistic-Style-Transfer



### Small Target Detection for Search and Rescue Operations using Distributed Deep Learning and Synthetic Data Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.11619v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.11619v1)
- **Published**: 2019-04-25 23:10:54+00:00
- **Updated**: 2019-04-25 23:10:54+00:00
- **Authors**: Kyongsik Yun, Luan Nguyen, Tuan Nguyen, Doyoung Kim, Sarah Eldin, Alexander Huyen, Thomas Lu, Edward Chow
- **Comment**: 6 pages, 4 figures, SPIE
- **Journal**: None
- **Summary**: It is important to find the target as soon as possible for search and rescue operations. Surveillance camera systems and unmanned aerial vehicles (UAVs) are used to support search and rescue. Automatic object detection is important because a person cannot monitor multiple surveillance screens simultaneously for 24 hours. Also, the object is often too small to be recognized by the human eye on the surveillance screen. This study used UAVs around the Port of Houston and fixed surveillance cameras to build an automatic target detection system that supports the US Coast Guard (USCG) to help find targets (e.g., person overboard). We combined image segmentation, enhancement, and convolution neural networks to reduce detection time to detect small targets. We compared the performance between the auto-detection system and the human eye. Our system detected the target within 8 seconds, but the human eye detected the target within 25 seconds. Our systems also used synthetic data generation and data augmentation techniques to improve target detection accuracy. This solution may help the search and rescue operations of the first responders in a timely manner.



### Improved visible to IR image transformation using synthetic data augmentation with cycle-consistent adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1904.11620v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.11620v1)
- **Published**: 2019-04-25 23:12:52+00:00
- **Updated**: 2019-04-25 23:12:52+00:00
- **Authors**: Kyongsik Yun, Kevin Yu, Joseph Osborne, Sarah Eldin, Luan Nguyen, Alexander Huyen, Thomas Lu
- **Comment**: 8 pages, 6 figures, SPIE
- **Journal**: None
- **Summary**: Infrared (IR) images are essential to improve the visibility of dark or camouflaged objects. Object recognition and segmentation based on a neural network using IR images provide more accuracy and insight than color visible images. But the bottleneck is the amount of relevant IR images for training. It is difficult to collect real-world IR images for special purposes, including space exploration, military and fire-fighting applications. To solve this problem, we created color visible and IR images using a Unity-based 3D game editor. These synthetically generated color visible and IR images were used to train cycle consistent adversarial networks (CycleGAN) to convert visible images to IR images. CycleGAN has the advantage that it does not require precisely matching visible and IR pairs for transformation training. In this study, we discovered that additional synthetic data can help improve CycleGAN performance. Neural network training using real data (N = 20) performed more accurate transformations than training using real (N = 10) and synthetic (N = 10) data combinations. The result indicates that the synthetic data cannot exceed the quality of the real data. Neural network training using real (N = 10) and synthetic (N = 100) data combinations showed almost the same performance as training using real data (N = 20). At least 10 times more synthetic data than real data is required to achieve the same performance. In summary, CycleGAN is used with synthetic data to improve the IR image conversion performance of visible images.



### Meta-Sim: Learning to Generate Synthetic Datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.11621v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.11621v1)
- **Published**: 2019-04-25 23:18:36+00:00
- **Updated**: 2019-04-25 23:18:36+00:00
- **Authors**: Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler
- **Comment**: Webpage: https://nv-tlabs.github.io/meta-sim/
- **Journal**: None
- **Summary**: Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.



### Scene Graph Prediction with Limited Labels
- **Arxiv ID**: http://arxiv.org/abs/1904.11622v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.11622v3)
- **Published**: 2019-04-25 23:26:25+00:00
- **Updated**: 2019-11-30 21:52:18+00:00
- **Authors**: Vincent S. Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein, Christopher Re, Li Fei-Fei
- **Comment**: ICCV 2019, 10 pages, 9 figures
- **Journal**: International Conference on Computer Vision, 2019
- **Summary**: Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by 5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.



