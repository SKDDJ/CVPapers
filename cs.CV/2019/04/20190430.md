# Arxiv Papers in cs.CV on 2019-04-30
### Curvature: A signature for Action Recognition in Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1904.13003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13003v2)
- **Published**: 2019-04-30 00:27:13+00:00
- **Updated**: 2019-06-16 05:41:32+00:00
- **Authors**: He Chen, Gregory S. Chirikjian
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel signature of human action recognition, namely the curvature of a video sequence, is introduced. In this way, the distribution of sequential data is modeled, which enables few-shot learning. Instead of depending on recognizing features within images, our algorithm views actions as sequences on the universal time scale across a whole sequence of images. The video sequence, viewed as a curve in pixel space, is aligned by reparameterization using the arclength of the curve in pixel space. Once such curvatures are obtained, statistical indexes are extracted and fed into a learning-based classifier. Overall, our method is simple but powerful. Preliminary experimental results show that our method is effective and achieves state-of-the-art performance in video-based human action recognition. Moreover, we see latent capacity in transferring this idea into other sequence-based recognition applications such as speech recognition, machine translation, and text generation.



### Virtual-Blind-Road Following Based Wearable Navigation Device for Blind People
- **Arxiv ID**: http://arxiv.org/abs/1904.13028v1
- **DOI**: 10.1109/TCE.2018.2812498
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.13028v1)
- **Published**: 2019-04-30 03:07:01+00:00
- **Updated**: 2019-04-30 03:07:01+00:00
- **Authors**: Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, Dijun Liu
- **Comment**: 8 pages, 9 figures, TCE accepted
- **Journal**: None
- **Summary**: To help the blind people walk to the destination efficiently and safely in indoor environment, a novel wearable navigation device is presented in this paper. The locating, way-finding, route following and obstacle avoiding modules are the essential components in a navigation system, while it remains a challenging task to consider obstacle avoiding during route following, as the indoor environment is complex, changeable and possibly with dynamic objects. To address this issue, we propose a novel scheme which utilizes a dynamic sub-goal selecting strategy to guide the users to the destination and help them bypass obstacles at the same time. This scheme serves as the key component of a complete navigation system deployed on a pair of wearable optical see-through glasses for the ease of use of blind people's daily walks. The proposed navigation device has been tested on a collection of individuals and proved to be effective on indoor navigation tasks. The sensors embedded are of low cost, small volume and easy integration, making it possible for the glasses to be widely used as a wearable consumer device.



### SeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1904.13030v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.13030v2)
- **Published**: 2019-04-30 03:09:01+00:00
- **Updated**: 2019-08-19 08:31:40+00:00
- **Authors**: Zhe Liu, Chuanzhe Suo, Shunbo Zhou, Huanshu Wei, Yingtian Liu, Hesheng Wang, Yun-Hui Liu
- **Comment**: This paper has been accepted by IROS-2019
- **Journal**: None
- **Summary**: Place recognition and loop-closure detection are main challenges in the localization, mapping and navigation tasks of self-driving vehicles. In this paper, we solve the loop-closure detection problem by incorporating the deep-learning based point cloud description method and the coarse-to-fine sequence matching strategy. More specifically, we propose a deep neural network to extract a global descriptor from the original large-scale 3D point cloud, then based on which, a typical place analysis approach is presented to investigate the feature space distribution of the global descriptors and select several super keyframes. Finally, a coarse-to-fine strategy, which includes a super keyframe based coarse matching stage and a local sequence matching stage, is presented to ensure the loop-closure detection accuracy and real-time performance simultaneously. Thanks to the sequence matching operation, the proposed approach obtains an improvement against the existing deep-learning based methods. Experiment results on a self-driving vehicle validate the effectiveness of the proposed loop-closure detection algorithm.



### Deep Learning Based Robot for Automatically Picking up Garbage on the Grass
- **Arxiv ID**: http://arxiv.org/abs/1904.13034v1
- **DOI**: 10.1109/TCE.2018.2859629
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.13034v1)
- **Published**: 2019-04-30 03:21:37+00:00
- **Updated**: 2019-04-30 03:21:37+00:00
- **Authors**: Jinqiang Bai, Shiguo Lian, Zhaoxiang Liu, Kai Wang, Dijun Liu
- **Comment**: 8 pages, 13 figures,TCE accepted
- **Journal**: None
- **Summary**: This paper presents a novel garbage pickup robot which operates on the grass. The robot is able to detect the garbage accurately and autonomously by using a deep neural network for garbage recognition. In addition, with the ground segmentation using a deep neural network, a novel navigation strategy is proposed to guide the robot to move around. With the garbage recognition and automatic navigation functions, the robot can clean garbage on the ground in places like parks or schools efficiently and autonomously. Experimental results show that the garbage recognition accuracy can reach as high as 95%, and even without path planning, the navigation strategy can reach almost the same cleaning efficiency with traditional methods. Thus, the proposed robot can serve as a good assistance to relieve dustman's physical labor on garbage cleaning tasks.



### Wearable Travel Aid for Environment Perception and Navigation of Visually Impaired People
- **Arxiv ID**: http://arxiv.org/abs/1904.13037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1904.13037v1)
- **Published**: 2019-04-30 03:33:45+00:00
- **Updated**: 2019-04-30 03:33:45+00:00
- **Authors**: Jinqiang Bai, Zhaoxiang Liu, Yimin Lin, Ye Li, Shiguo Lian, Dijun Liu
- **Comment**: 7 pages, 12 figures
- **Journal**: 2019 Electronics
- **Summary**: This paper presents a wearable assistive device with the shape of a pair of eyeglasses that allows visually impaired people to navigate safely and quickly in unfamiliar environment, as well as perceive the complicated environment to automatically make decisions on the direction to move. The device uses a consumer Red, Green, Blue and Depth (RGB-D) camera and an Inertial Measurement Unit (IMU) to detect obstacles. As the device leverages the ground height continuity among adjacent image frames, it is able to segment the ground from obstacles accurately and rapidly. Based on the detected ground, the optimal walkable direction is computed and the user is then informed via converted beep sound. Moreover, by utilizing deep learning techniques, the device can semantically categorize the detected obstacles to improve the users' perception of surroundings. It combines a Convolutional Neural Network (CNN) deployed on a smartphone with a depth-image-based object detection to decide what the object type is and where the object is located, and then notifies the user of such information via speech. We evaluated the device's performance with different experiments in which 20 visually impaired people were asked to wear the device and move in an office, and found that they were able to avoid obstacle collisions and find the way in complicated scenarios.



### Cross Domain Knowledge Learning with Dual-branch Adversarial Network for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1905.00006v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00006v1)
- **Published**: 2019-04-30 05:17:57+00:00
- **Updated**: 2019-04-30 05:17:57+00:00
- **Authors**: Jinjia Peng, Huibing Wang, Xianping Fu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.07868
- **Journal**: None
- **Summary**: The widespread popularization of vehicles has facilitated all people's life during the last decades. However, the emergence of a large number of vehicles poses the critical but challenging problem of vehicle re-identification (reID). Till now, for most vehicle reID algorithms, both the training and testing processes are conducted on the same annotated datasets under supervision. However, even a well-trained model will still cause fateful performance drop due to the severe domain bias between the trained dataset and the real-world scenes.   To address this problem, this paper proposes a domain adaptation framework for vehicle reID (DAVR), which narrows the cross-domain bias by fully exploiting the labeled data from the source domain to adapt the target domain. DAVR develops an image-to-image translation network named Dual-branch Adversarial Network (DAN), which could promote the images from the source domain (well-labeled) to learn the style of target domain (unlabeled) without any annotation and preserve identity information from source domain. Then the generated images are employed to train the vehicle reID model by a proposed attention-based feature learning model with more reasonable styles. Through the proposed framework, the well-trained reID model has better domain adaptation ability for various scenes in real-world situations. Comprehensive experimental results have demonstrated that our proposed DAVR can achieve excellent performances on both VehicleID dataset and VeRi-776 dataset.



### Cross-Modal Message Passing for Two-stream Fusion
- **Arxiv ID**: http://arxiv.org/abs/1904.13072v1
- **DOI**: 10.1109/ICASSP.2018.8461792
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13072v1)
- **Published**: 2019-04-30 06:57:04+00:00
- **Updated**: 2019-04-30 06:57:04+00:00
- **Authors**: Dong Wang, Yuan Yuan, Qi Wang
- **Comment**: 2018 IEEE International Conference on Acoustics, Speech and Signal
  Processing
- **Journal**: None
- **Summary**: Processing and fusing information among multi-modal is a very useful technique for achieving high performance in many computer vision problems. In order to tackle multi-modal information more effectively, we introduce a novel framework for multi-modal fusion: Cross-modal Message Passing (CMMP). Specifically, we propose a cross-modal message passing mechanism to fuse two-stream network for action recognition, which composes of an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of individual networks in this framework are two-fold: a standard classification objective and a competing objective. The classification object ensures that each modal network predicts the true action category while the competing objective encourages each modal network to outperform the other one. We quantitatively show that the proposed CMMP fuses the traditional two-stream network more effectively, and outperforms all existing two-stream fusion method on UCF-101 and HMDB-51 datasets.



### SurfelWarp: Efficient Non-Volumetric Single View Dynamic Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.13073v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13073v1)
- **Published**: 2019-04-30 06:57:53+00:00
- **Updated**: 2019-04-30 06:57:53+00:00
- **Authors**: Wei Gao, Russ Tedrake
- **Comment**: RSS 2018. The video and source code are available on
  https://sites.google.com/view/surfelwarp/home
- **Journal**: None
- **Summary**: We contribute a dense SLAM system that takes a live stream of depth images as input and reconstructs non-rigid deforming scenes in real time, without templates or prior models. In contrast to existing approaches, we do not maintain any volumetric data structures, such as truncated signed distance function (TSDF) fields or deformation fields, which are performance and memory intensive. Our system works with a flat point (surfel) based representation of geometry, which can be directly acquired from commodity depth sensors. Standard graphics pipelines and general purpose GPU (GPGPU) computing are leveraged for all central operations: i.e., nearest neighbor maintenance, non-rigid deformation field estimation and fusion of depth measurements. Our pipeline inherently avoids expensive volumetric operations such as marching cubes, volumetric fusion and dense deformation field update, leading to significantly improved performance. Furthermore, the explicit and flexible surfel based geometry representation enables efficient tackling of topology changes and tracking failures, which makes our reconstructions consistent with updated depth observations. Our system allows robots to maintain a scene description with non-rigidly deformed objects that potentially enables interactions with dynamic working environments.



### Interpretation of Feature Space using Multi-Channel Attentional Sub-Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.13078v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.13078v1)
- **Published**: 2019-04-30 07:13:52+00:00
- **Updated**: 2019-04-30 07:13:52+00:00
- **Authors**: Masanari Kimura, Masayuki Tanaka
- **Comment**: CVPR2019 Workshop on Explainable AI
- **Journal**: None
- **Summary**: Convolutional Neural Networks have achieved impressive results in various tasks, but interpreting the internal mechanism is a challenging problem. To tackle this problem, we exploit a multi-channel attention mechanism in feature space. Our network architecture allows us to obtain an attention mask for each feature while existing CNN visualization methods provide only a common attention mask for all features. We apply the proposed multi-channel attention mechanism to multi-attribute recognition task. We can obtain different attention mask for each feature and for each attribute. Those analyses give us deeper insight into the feature space of CNNs. The experimental results for the benchmark dataset show that the proposed method gives high interpretability to humans while accurately grasping the attributes of the data.



### Anomaly Detection in Traffic Scenes via Spatial-aware Motion Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.13079v1
- **DOI**: 10.1109/TITS.2016.2601655
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13079v1)
- **Published**: 2019-04-30 07:14:03+00:00
- **Updated**: 2019-04-30 07:14:03+00:00
- **Authors**: Yuan Yuan, Dong Wang, Qi Wang
- **Comment**: IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Anomaly detection from a driver's perspective when driving is important to autonomous vehicles. As a part of Advanced Driver Assistance Systems (ADAS), it can remind the driver about dangers timely. Compared with traditional studied scenes such as the university campus and market surveillance videos, it is difficult to detect abnormal event from a driver's perspective due to camera waggle, abidingly moving background, drastic change of vehicle velocity, etc. To tackle these specific problems, this paper proposes a spatial localization constrained sparse coding approach for anomaly detection in traffic scenes, which firstly measures the abnormality of motion orientation and magnitude respectively and then fuses these two aspects to obtain a robust detection result. The main contributions are threefold: 1) This work describes the motion orientation and magnitude of the object respectively in a new way, which is demonstrated to be better than the traditional motion descriptors. 2) The spatial localization of object is taken into account of the sparse reconstruction framework, which utilizes the scene's structural information and outperforms the conventional sparse coding methods. 3) Results of motion orientation and magnitude are adaptively weighted and fused by a Bayesian model, which makes the proposed method more robust and handle more kinds of abnormal events. The efficiency and effectiveness of the proposed method are validated by testing on nine difficult video sequences captured by ourselves. Observed from the experimental results, the proposed method is more effective and efficient than the popular competitors, and yields a higher performance.



### Memory-Augmented Temporal Dynamic Learning for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.13080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13080v1)
- **Published**: 2019-04-30 07:19:50+00:00
- **Updated**: 2019-04-30 07:19:50+00:00
- **Authors**: Yuan Yuan, Dong Wang, Qi Wang
- **Comment**: The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: Human actions captured in video sequences contain two crucial factors for action recognition, i.e., visual appearance and motion dynamics. To model these two aspects, Convolutional and Recurrent Neural Networks (CNNs and RNNs) are adopted in most existing successful methods for recognizing actions. However, CNN based methods are limited in modeling long-term motion dynamics. RNNs are able to learn temporal motion dynamics but lack effective ways to tackle unsteady dynamics in long-duration motion. In this work, we propose a memory-augmented temporal dynamic learning network, which learns to write the most evident information into an external memory module and ignore irrelevant ones. In particular, we present a differential memory controller to make a discrete decision on whether the external memory module should be updated with current feature. The discrete memory controller takes in the memory history, context embedding and current feature as inputs and controls information flow into the external memory module. Additionally, we train this discrete memory controller using straight-through estimator. We evaluate this end-to-end system on benchmark datasets (UCF101 and HMDB51) of human action recognition. The experimental results show consistent improvements on both datasets over prior works and our baselines.



### Appearance and Pose-Conditioned Human Image Generation using Deformable GANs
- **Arxiv ID**: http://arxiv.org/abs/1905.00007v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00007v2)
- **Published**: 2019-04-30 07:35:15+00:00
- **Updated**: 2019-10-14 15:06:52+00:00
- **Authors**: Aliaksandr Siarohin, Stéphane Lathuilière, Enver Sangineto, Nicu Sebe
- **Comment**: To appear on IEEE TPAMI. arXiv admin note: substantial text overlap
  with arXiv:1801.00055
- **Journal**: None
- **Summary**: In this paper, we address the problem of generating person images conditioned on both pose and appearance information. Specifically, given an image xa of a person and a target pose P(xb), extracted from a different image xb, we synthesize a new image of that person in pose P(xb), while preserving the visual details in xa. In order to deal with pixel-to-pixel misalignments caused by the pose differences between P(xa) and P(xb), we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. Quantitative and qualitative results, using common datasets and protocols recently proposed for this task, show that our approach is competitive with respect to the state of the art. Moreover, we conduct an extensive evaluation using off-the-shell person re-identification (Re-ID) systems trained with person-generation based augmented data, which is one of the main important applications for this task. Our experiments show that our Deformable GANs can significantly boost the Re-ID accuracy and are even better than data-augmentation methods specifically trained using Re-ID losses.



### Early Action Prediction with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.13085v1
- **DOI**: 10.1109/ACCESS.2019.2904857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13085v1)
- **Published**: 2019-04-30 07:36:19+00:00
- **Updated**: 2019-04-30 07:36:19+00:00
- **Authors**: Dong Wang, Yuan Yuan, Qi Wang
- **Comment**: IEEE Access
- **Journal**: None
- **Summary**: Action Prediction is aimed to determine what action is occurring in a video as early as possible, which is crucial to many online applications, such as predicting a traffic accident before it happens and detecting malicious actions in the monitoring system. In this work, we address this problem by developing an end-to-end architecture that improves the discriminability of features of partially observed videos by assimilating them to features from complete videos. For this purpose, the generative adversarial network is introduced for tackling action prediction problem, which improves the recognition accuracy of partially observed videos though narrowing the feature difference of partially observed videos from complete ones. Specifically, its generator comprises of two networks: a CNN for feature extraction and an LSTM for estimating residual error between features of the partially observed videos and complete ones, and then the features from CNN adds the residual error from LSTM, which is regarded as the enhanced feature to fool a competing discriminator. Meanwhile, the generator is trained with an additional perceptual objective, which forces the enhanced features of partially observed videos are discriminative enough for action prediction. Extensive experimental results on UCF101, BIT and UT-Interaction datasets demonstrate that our approach outperforms the state-of-the-art methods, especially for videos that less than 50% portion of frames is observed.



### Facial Pose Estimation by Deep Learning from Label Distributions
- **Arxiv ID**: http://arxiv.org/abs/1904.13102v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.13102v4)
- **Published**: 2019-04-30 08:38:20+00:00
- **Updated**: 2020-10-12 02:48:44+00:00
- **Authors**: Zhaoxiang Liu, Zezhou Chen, Jinqiang Bai, Shaohua Li, Shiguo Lian
- **Comment**: 9 pages,5 figures, Accepted by ICCV 2019 workshop
- **Journal**: None
- **Summary**: Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods.



### Deep Spectral Clustering using Dual Autoencoder Network
- **Arxiv ID**: http://arxiv.org/abs/1904.13113v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.13113v1)
- **Published**: 2019-04-30 09:12:22+00:00
- **Updated**: 2019-04-30 09:12:22+00:00
- **Authors**: Xu Yang, Cheng Deng, Feng Zheng, Junchi Yan, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The clustering methods have recently absorbed even-increasing attention in learning and vision. Deep clustering combines embedding and clustering together to obtain optimal embedding subspace for clustering, which can be more effective compared with conventional clustering methods. In this paper, we propose a joint learning framework for discriminative embedding and spectral clustering. We first devise a dual autoencoder network, which enforces the reconstruction constraint for the latent representations and their noisy versions, to embed the inputs into a latent space for clustering. As such the learned latent representations can be more robust to noise. Then the mutual information estimation is utilized to provide more discriminative information from the inputs. Furthermore, a deep spectral clustering method is applied to embed the latent representations into the eigenspace and subsequently clusters them, which can fully exploit the relationship between inputs to achieve optimal clustering results. Experimental results on benchmark datasets show that our method can significantly outperform state-of-the-art clustering approaches.



### A critical analysis of self-supervision, or what we can learn from a single image
- **Arxiv ID**: http://arxiv.org/abs/1904.13132v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13132v3)
- **Published**: 2019-04-30 10:10:38+00:00
- **Updated**: 2020-02-19 17:56:41+00:00
- **Authors**: Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi
- **Comment**: Accepted paper at the International Conference on Learning
  Representations (ICLR) 2020
- **Journal**: None
- **Summary**: We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.



### PR Product: A Substitute for Inner Product in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.13148v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.13148v2)
- **Published**: 2019-04-30 10:43:38+00:00
- **Updated**: 2019-08-16 11:37:06+00:00
- **Authors**: Zhennan Wang, Wenbin Zou, Chen Xu
- **Comment**: ICCV2019 oral
- **Journal**: Proceedings of the IEEE International Conference on Computer
  Vision. 2019: 6013-6022
- **Summary**: In this paper, we analyze the inner product of weight vector w and data vector x in neural networks from the perspective of vector orthogonal decomposition and prove that the direction gradient of w decreases with the angle between them close to 0 or {\pi}. We propose the Projection and Rejection Product (PR Product) to make the direction gradient of w independent of the angle and consistently larger than the one in standard inner product while keeping the forward propagation identical. As a reliable substitute for standard inner product, the PR Product can be applied into many existing deep learning modules, so we develop the PR Product version of fully connected layer, convolutional layer and LSTM layer. In static image classification, the experiments on CIFAR10 and CIFAR100 datasets demonstrate that the PR Product can robustly enhance the ability of various state-of-the-art classification networks. On the task of image captioning, even without any bells and whistles, our PR Product version of captioning model can compete or outperform the state-of-the-art models on MS COCO dataset. Code has been made available at:https://github.com/wzn0828/PR_Product.



### Facial Expressions Analysis Under Occlusions Based on Specificities of Facial Motion Propagation
- **Arxiv ID**: http://arxiv.org/abs/1904.13154v1
- **DOI**: 10.1007/s11042-020-08993-5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13154v1)
- **Published**: 2019-04-30 10:57:37+00:00
- **Updated**: 2019-04-30 10:57:37+00:00
- **Authors**: Delphine Poux, Benjamin Allaert, Jose Mennesson, Nacim Ihaddadene, Ioan Marius Bilasco, Chaabane Djeraba
- **Comment**: None
- **Journal**: None
- **Summary**: Although much progress has been made in the facial expression analysis field, facial occlusions are still challenging. The main innovation brought by this contribution consists in exploiting the specificities of facial movement propagation for recognizing expressions in presence of important occlusions. The movement induced by an expression extends beyond the movement epicenter. Thus, the movement occurring in an occluded region propagates towards neighboring visible regions. In presence of occlusions, per expression, we compute the importance of each unoccluded facial region and we construct adapted facial frameworks that boost the performance of per expression binary classifier. The output of each expression-dependant binary classifier is then aggregated and fed into a fusion process that aims constructing, per occlusion, a unique model that recognizes all the facial expressions considered. The evaluations highlight the robustness of this approach in presence of significant facial occlusions.



### Using cameras for precise measurement of two-dimensional plant features: CASS
- **Arxiv ID**: http://arxiv.org/abs/1904.13187v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13187v2)
- **Published**: 2019-04-30 12:23:48+00:00
- **Updated**: 2020-02-28 20:59:22+00:00
- **Authors**: Amy Tabb, Germán A Holguín, Rachel Naegele
- **Comment**: 5 pages, protocol. Code and data:
  http://doi.org/10.5281/zenodo.3677473
- **Journal**: None
- **Summary**: Images are used frequently in plant phenotyping to capture measurements. This chapter offers a repeatable method for capturing two-dimensional measurements of plant parts in field or laboratory settings using a variety of camera styles (cellular phone, DSLR), with the addition of a printed calibration pattern. The method is based on calibrating the camera using information available from the EXIF tags from the image, as well as visual information from the pattern. Code is provided to implement the method, as well as a dataset for testing. We include steps to verify protocol correctness by imaging an artifact. The use of this protocol for two-dimensional plant phenotyping will allow data capture from different cameras and environments, with comparison on the same physical scale. We abbreviate this method as CASS, for CAmera aS Scanner. Code and data is available at http://doi.org/10.5281/zenodo.3677473.



### Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.13196v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/1904.13196v1)
- **Published**: 2019-04-30 12:44:22+00:00
- **Updated**: 2019-04-30 12:44:22+00:00
- **Authors**: Marjan Alirezaie, Martin Längkvist, Michael Sioutis, Amy Loutfi
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding why machine learning algorithms may fail is usually the task of the human expert that uses domain knowledge and contextual information to discover systematic shortcomings in either the data or the algorithm. In this paper, we propose a semantic referee, which is able to extract qualitative features of the errors emerging from deep machine learning frameworks and suggest corrections. The semantic referee relies on ontological reasoning about spatial knowledge in order to characterize errors in terms of their spatial relations with the environment. Using semantics, the reasoner interacts with the learning algorithm as a supervisor. In this paper, the proposed method of the interaction between a neural network classifier and a semantic referee shows how to improve the performance of semantic segmentation for satellite imagery data.



### GaborNet: Gabor filters with learnable parameters in deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1904.13204v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.13204v1)
- **Published**: 2019-04-30 13:12:36+00:00
- **Updated**: 2019-04-30 13:12:36+00:00
- **Authors**: Andrey Alekseev, Anatoly Bobe
- **Comment**: 10 pages, 6 figures, 3 tables, preprint
- **Journal**: None
- **Summary**: The article describes a system for image recognition using deep convolutional neural networks. Modified network architecture is proposed that focuses on improving convergence and reducing training complexity. The filters in the first layer of the network are constrained to fit the Gabor function. The parameters of Gabor functions are learnable and are updated by standard backpropagation techniques. The system was implemented on Python, tested on several datasets and outperformed the common convolutional networks.



### Alignment-Free Cross-Sensor Fingerprint Matching based on the Co-Occurrence of Ridge Orientations and Gabor-HoG Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1905.03699v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03699v1)
- **Published**: 2019-04-30 13:26:04+00:00
- **Updated**: 2019-04-30 13:26:04+00:00
- **Authors**: Helala AlShehri, Muhammad Hussain, Hatim AboAlSamh, Qazi Emad-ul-Haq, Aqil M. Azmi
- **Comment**: None
- **Journal**: None
- **Summary**: The existing automatic fingerprint verification methods are designed to work under the assumption that the same sensor is installed for enrollment and authentication (regular matching). There is a remarkable decrease in efficiency when one type of contact-based sensor is employed for enrolment and another type of contact-based sensor is used for authentication (cross-matching or fingerprint sensor interoperability problem,). The ridge orientation patterns in a fingerprint are invariant to sensor type. Based on this observation, we propose a robust fingerprint descriptor called the co-occurrence of ridge orientations (Co-Ror), which encodes the spatial distribution of ridge orientations. Employing this descriptor, we introduce an efficient automatic fingerprint verification method for cross-matching problem. Further, to enhance the robustness of the method, we incorporate scale based ridge orientation information through Gabor-HoG descriptor. The two descriptors are fused with canonical correlation analysis (CCA), and the matching score between two fingerprints is calculated using city-block distance. The proposed method is alignment-free and can handle the matching process without the need for a registration step. The intensive experiments on two benchmark databases (FingerPass and MOLF) show the effectiveness of the method and reveal its significant enhancement over the state-of-the-art methods such as VeriFinger (a commercial SDK), minutia cylinder-code (MCC), MCC with scale, and the thin-plate spline (TPS) model. The proposed research will help security agencies, service providers and law-enforcement departments to overcome the interoperability problem of contact sensors of different technology and interaction types.



### Handwritten Chinese Font Generation with Collaborative Stroke Refinement
- **Arxiv ID**: http://arxiv.org/abs/1904.13268v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13268v3)
- **Published**: 2019-04-30 14:12:58+00:00
- **Updated**: 2019-05-06 11:50:09+00:00
- **Authors**: Chuan Wen, Jie Chang, Ya Zhang, Siheng Chen, Yanfeng Wang, Mei Han, Qi Tian
- **Comment**: 8 pages(exclude reference)
- **Journal**: None
- **Summary**: Automatic character generation is an appealing solution for new typeface design, especially for Chinese typefaces including over 3700 most commonly-used characters. This task has two main pain points: (i) handwritten characters are usually associated with thin strokes of few information and complex structure which are error prone during deformation; (ii) thousands of characters with various shapes are needed to synthesize based on a few manually designed characters. To solve those issues, we propose a novel convolutional-neural-network-based model with three main techniques: collaborative stroke refinement, using collaborative training strategy to recover the missing or broken strokes; online zoom-augmentation, taking the advantage of the content-reuse phenomenon to reduce the size of training set; and adaptive pre-deformation, standardizing and aligning the characters. The proposed model needs only 750 paired training samples; no pre-trained network, extra dataset resource or labels is needed. Experimental results show that the proposed method significantly outperforms the state-of-the-art methods under the practical restriction on handwritten font synthesis.



### Country-wide high-resolution vegetation height mapping with Sentinel-2
- **Arxiv ID**: http://arxiv.org/abs/1904.13270v2
- **DOI**: 10.1016/j.rse.2019.111347
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.13270v2)
- **Published**: 2019-04-30 14:13:13+00:00
- **Updated**: 2019-08-14 08:28:21+00:00
- **Authors**: Nico Lang, Konrad Schindler, Jan Dirk Wegner
- **Comment**: None
- **Journal**: Remote Sensing of Environment 233 (2019) 111347
- **Summary**: Sentinel-2 multi-spectral images collected over periods of several months were used to estimate vegetation height for Gabon and Switzerland. A deep convolutional neural network (CNN) was trained to extract suitable spectral and textural features from reflectance images and to regress per-pixel vegetation height. In Gabon, reference heights for training and validation were derived from airborne LiDAR measurements. In Switzerland, reference heights were taken from an existing canopy height model derived via photogrammetric surface reconstruction. The resulting maps have a mean absolute error (MAE) of 1.7 m in Switzerland and 4.3 m in Gabon (a root mean square error (RMSE) of 3.4 m and 5.6 m, respectively), and correctly estimate vegetation heights up to >50 m. They also show good qualitative agreement with existing vegetation height maps. Our work demonstrates that, given a moderate amount of reference data (i.e., 2000 km$^2$ in Gabon and $\approx$5800 km$^2$ in Switzerland), high-resolution vegetation height maps with 10 m ground sampling distance (GSD) can be derived at country scale from Sentinel-2 imagery.



### Non-Rigid Structure-From-Motion by Rank-One Basis Shapes
- **Arxiv ID**: http://arxiv.org/abs/1904.13271v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13271v1)
- **Published**: 2019-04-30 14:23:03+00:00
- **Updated**: 2019-04-30 14:23:03+00:00
- **Authors**: Sami S. Brandt, Hanno Ackermann
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we show that the affine, non-rigid structure-from-motion problem can be solved by rank-one, thus degenerate, basis shapes. It is a natural reformulation of the classic low-rank method by Bregler et al., where it was assumed that the deformable 3D structure is generated by a linear combination of rigid basis shapes. The non-rigid shape will be decomposed into the mean shape and the degenerate shapes, constructed from the right singular vectors of the low-rank decomposition. The right singular vectors are affinely back-projected into the 3D space, and the affine back-projections will also be solved as part of the factorisation. By construction, a direct interpretation for the right singular vectors of the low-rank decomposition will also follow: they can be seen as principal components, hence, the first variant of our method is referred to as Rank-1-PCA. The second variant, referred to as Rank-1-ICA, additionally estimates the orthogonal transform which maps the deformation modes into as statistically independent modes as possible. It has the advantage of pinpointing statistically dependent subspaces related to, for instance, lip movements on human faces. Moreover, in contrast to prior works, no predefined dimensionality for the subspaces is imposed. The experiments on several datasets show that the method achieves better results than the state-of-the-art, it can be computed faster, and it provides an intuitive interpretation for the deformation modes.



### Detecting Reflections by Combining Semantic and Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.13273v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1904.13273v1)
- **Published**: 2019-04-30 14:25:43+00:00
- **Updated**: 2019-04-30 14:25:43+00:00
- **Authors**: David Owen, Ping-Lin Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Reflections in natural images commonly cause false positives in automated detection systems. These false positives can lead to significant impairment of accuracy in the tasks of detection, counting and segmentation. Here, inspired by the recent panoptic approach to segmentation, we show how fusing instance and semantic segmentation can automatically identify reflection false positives, without explicitly needing to have the reflective regions labelled. We explore in detail how state of the art two-stage detectors suffer a loss of broader contextual features, and hence are unable to learn to ignore these reflections. We then present an approach to fuse instance and semantic segmentations for this application, and subsequently show how this reduces false positive detections in a real world surveillance data with a large number of reflective surfaces. This demonstrates how panoptic segmentation and related work, despite being in its infancy, can already be useful in real world computer vision problems.



### CT-To-MR Conditional Generative Adversarial Networks for Ischemic Stroke Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.13281v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.13281v1)
- **Published**: 2019-04-30 14:42:53+00:00
- **Updated**: 2019-04-30 14:42:53+00:00
- **Authors**: Jonathan Rubin, S. Mazdak Abulnaga
- **Comment**: Seventh IEEE International Conference on Healthcare Informatics (ICHI
  2019)
- **Journal**: None
- **Summary**: Infarcted brain tissue resulting from acute stroke readily shows up as hyperintense regions within diffusion-weighted magnetic resonance imaging (DWI). It has also been proposed that computed tomography perfusion (CTP) could alternatively be used to triage stroke patients, given improvements in speed and availability, as well as reduced cost. However, CTP has a lower signal to noise ratio compared to MR. In this work, we investigate whether a conditional mapping can be learned by a generative adversarial network to map CTP inputs to generated MR DWI that more clearly delineates hyperintense regions due to ischemic stroke. We detail the architectures of the generator and discriminator and describe the training process used to perform image-to-image translation from multi-modal CT perfusion maps to diffusion weighted MR outputs. We evaluate the results both qualitatively by visual comparison of generated MR to ground truth, as well as quantitatively by training fully convolutional neural networks that make use of generated MR data inputs to perform ischemic stroke lesion segmentation. Segmentation networks trained using generated CT-to-MR inputs result in at least some improvement on all metrics used for evaluation, compared with networks that only use CT perfusion input.



### Segmentation is All You Need
- **Arxiv ID**: http://arxiv.org/abs/1904.13300v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13300v3)
- **Published**: 2019-04-30 15:13:01+00:00
- **Updated**: 2019-05-26 02:28:04+00:00
- **Authors**: Zehua Cheng, Yuxiang Wu, Zhenghua Xu, Thomas Lukasiewicz, Weiyang Wang
- **Comment**: 10 Pages
- **Journal**: None
- **Summary**: Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low. This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchor-free and NMS-free object detection model called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors.



### Unsupervised automatic classification of Scanning Electron Microscopy (SEM) images of CD4+ cells with varying extent of HIV virion infection
- **Arxiv ID**: http://arxiv.org/abs/1905.03700v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1905.03700v1)
- **Published**: 2019-04-30 15:15:21+00:00
- **Updated**: 2019-04-30 15:15:21+00:00
- **Authors**: John M. Wandeto, Birgitta Dresp-Langley
- **Comment**: None
- **Journal**: None
- **Summary**: Archiving large sets of medical or cell images in digital libraries may require ordering randomly scattered sets of image data according to specific criteria, such as the spatial extent of a specific local color or contrast content that reveals different meaningful states of a physiological structure, tissue, or cell in a certain order, indicating progression or recession of a pathology, or the progressive response of a cell structure to treatment. Here we used a Self Organized Map (SOM)-based, fully automatic and unsupervised, classification procedure described in our earlier work and applied it to sets of minimally processed grayscale and/or color processed Scanning Electron Microscopy (SEM) images of CD4+ T-lymphocytes (so-called helper cells) with varying extent of HIV virion infection. It is shown that the quantization error in the SOM output after training permits to scale the spatial magnitude and the direction of change (+ or -) in local pixel contrast or color across images of a series with a reliability that exceeds that of any human expert. The procedure is easily implemented and fast, and represents a promising step towards low-cost automatic digital image archiving with minimal intervention of a human operator.



### PYRO-NN: Python Reconstruction Operators in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.13342v1
- **DOI**: 10.1002/mp.13753
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13342v1)
- **Published**: 2019-04-30 16:12:55+00:00
- **Updated**: 2019-04-30 16:12:55+00:00
- **Authors**: Christopher Syben, Markus Michen, Bernhard Stimpel, Stephan Seitz, Stefan Ploner, Andreas K. Maier
- **Comment**: V1: Submitted to Medical Physics, 11 pages, 7 figures
- **Journal**: None
- **Summary**: Purpose: Recently, several attempts were conducted to transfer deep learning to medical image reconstruction. An increasingly number of publications follow the concept of embedding the CT reconstruction as a known operator into a neural network. However, most of the approaches presented lack an efficient CT reconstruction framework fully integrated into deep learning environments. As a result, many approaches are forced to use workarounds for mathematically unambiguously solvable problems. Methods: PYRO-NN is a generalized framework to embed known operators into the prevalent deep learning framework Tensorflow. The current status includes state-of-the-art parallel-, fan- and cone-beam projectors and back-projectors accelerated with CUDA provided as Tensorflow layers. On top, the framework provides a high level Python API to conduct FBP and iterative reconstruction experiments with data from real CT systems. Results: The framework provides all necessary algorithms and tools to design end-to-end neural network pipelines with integrated CT reconstruction algorithms. The high level Python API allows a simple use of the layers as known from Tensorflow. To demonstrate the capabilities of the layers, the framework comes with three baseline experiments showing a cone-beam short scan FDK reconstruction, a CT reconstruction filter learning setup, and a TV regularized iterative reconstruction. All algorithms and tools are referenced to a scientific publication and are compared to existing non deep learning reconstruction frameworks. The framework is available as open-source software at \url{https://github.com/csyben/PYRO-NN}. Conclusions: PYRO-NN comes with the prevalent deep learning framework Tensorflow and allows to setup end-to-end trainable neural networks in the medical image reconstruction context. We believe that the framework will be a step towards reproducible research



### Object Contour and Edge Detection with RefineContourNet
- **Arxiv ID**: http://arxiv.org/abs/1904.13353v2
- **DOI**: 10.1007/978-3-030-29888-3_20
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.13353v2)
- **Published**: 2019-04-30 16:34:27+00:00
- **Updated**: 2019-05-02 15:19:10+00:00
- **Authors**: Andre Peter Kelm, Vijesh Soorya Rao, Udo Zoelzer
- **Comment**: Keywords: Object Contour Detection, Edge Detection, Multi-Path
  Refinement CNN
- **Journal**: None
- **Summary**: A ResNet-based multi-path refinement CNN is used for object contour detection. For this task, we prioritise the effective utilization of the high-level abstraction capability of a ResNet, which leads to state-of-the-art results for edge detection. Keeping our focus in mind, we fuse the high, mid and low-level features in that specific order, which differs from many other approaches. It uses the tensor with the highest-levelled features as the starting point to combine it layer-by-layer with features of a lower abstraction level until it reaches the lowest level. We train this network on a modified PASCAL VOC 2012 dataset for object contour detection and evaluate on a refined PASCAL-val dataset reaching an excellent performance and an Optimal Dataset Scale (ODS) of 0.752. Furthermore, by fine-training on the BSDS500 dataset we reach state-of-the-art results for edge-detection with an ODS of 0.824.



### Structured Prediction using cGANs with Fusion Discriminator
- **Arxiv ID**: http://arxiv.org/abs/1904.13358v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.13358v1)
- **Published**: 2019-04-30 16:42:55+00:00
- **Updated**: 2019-04-30 16:42:55+00:00
- **Authors**: Faisal Mahmood, Wenhao Xu, Nicholas J. Durr, Jeremiah W. Johnson, Alan Yuille
- **Comment**: 13 pages, 5 figures, 3 tables
- **Journal**: Workshop on Deep Generative Models for Structured Prediction at
  ICLR 2019
- **Summary**: We propose the fusion discriminator, a single unified framework for incorporating conditional information into a generative adversarial network (GAN) for a variety of distinct structured prediction tasks, including image synthesis, semantic segmentation, and depth estimation. Much like commonly used convolutional neural network -- conditional Markov random field (CNN-CRF) models, the proposed method is able to enforce higher-order consistency in the model, but without being limited to a very specific class of potentials. The method is conceptually simple and flexible, and our experimental results demonstrate improvement on several diverse structured prediction tasks.



### The Level Weighted Structural Similarity Loss: A Step Away from the MSE
- **Arxiv ID**: http://arxiv.org/abs/1904.13362v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.13362v1)
- **Published**: 2019-04-30 16:45:35+00:00
- **Updated**: 2019-04-30 16:45:35+00:00
- **Authors**: Yingjing Lu
- **Comment**: None
- **Journal**: None
- **Summary**: The Mean Square Error (MSE) has shown its strength when applied in deep generative models such as Auto-Encoders to model reconstruction loss. However, in image domain especially, the limitation of MSE is obvious: it assumes pixel independence and ignores spatial relationships of samples. This contradicts most architectures of Auto-Encoders which use convolutional layers to extract spatial dependent features. We base on the structural similarity metric (SSIM) and propose a novel level weighted structural similarity (LWSSIM) loss for convolutional Auto-Encoders. Experiments on common datasets on various Auto-Encoder variants show that our loss is able to outperform the MSE loss and the Vanilla SSIM loss. We also provide reasons why our model is able to succeed in cases where the standard SSIM loss fails.



### Comparative evaluation of 2D feature correspondence selection algorithms
- **Arxiv ID**: http://arxiv.org/abs/1904.13383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.13383v1)
- **Published**: 2019-04-30 17:31:07+00:00
- **Updated**: 2019-04-30 17:31:07+00:00
- **Authors**: Chen Zhao, Jiaqi Yang, Yang Xiao, Zhiguo Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Correspondence selection aiming at seeking correct feature correspondences from raw feature matches is pivotal for a number of feature-matching-based tasks. Various 2D (image) correspondence selection algorithms have been presented with decades of progress. Unfortunately, the lack of an in-depth evaluation makes it difficult for developers to choose a proper algorithm given a specific application. This paper fills this gap by evaluating eight 2D correspondence selection algorithms ranging from classical methods to the most recent ones on four standard datasets. The diversity of experimental datasets brings various nuisances including zoom, rotation, blur, viewpoint change, JPEG compression, light change, different rendering styles and multi-structures for comprehensive test. To further create different distributions of initial matches, a set of combinations of detector and descriptor is also taken into consideration. We measure the quality of a correspondence selection algorithm from four perspectives, i.e., precision, recall, F-measure and efficiency. According to evaluation results, the current advantages and limitations of all considered algorithms are aggregately summarized which could be treated as a "user guide" for the following developers.



### OpenEDS: Open Eye Dataset
- **Arxiv ID**: http://arxiv.org/abs/1905.03702v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.03702v2)
- **Published**: 2019-04-30 17:47:53+00:00
- **Updated**: 2019-05-17 16:18:02+00:00
- **Authors**: Stephan J. Garbin, Yiru Shen, Immo Schuetz, Robert Cavin, Gregory Hughes, Sachin S. Talathi
- **Comment**: 11 pages; 12 figures
- **Journal**: None
- **Summary**: We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on OpenEDS for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications. The dataset is available for download upon request at https://research.fb.com/programs/openeds-challenge



### Attentive Spatio-Temporal Representation Learning for Diving Classification
- **Arxiv ID**: http://arxiv.org/abs/1905.00050v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00050v1)
- **Published**: 2019-04-30 18:22:09+00:00
- **Updated**: 2019-04-30 18:22:09+00:00
- **Authors**: Gagan Kanojia, Sudhakar Kumawat, Shanmuganathan Raman
- **Comment**: Accepted in CVPRW 2019
- **Journal**: None
- **Summary**: Competitive diving is a well recognized aquatic sport in which a person dives from a platform or a springboard into the water. Based on the acrobatics performed during the dive, diving is classified into a finite set of action classes which are standardized by FINA. In this work, we propose an attention guided LSTM-based neural network architecture for the task of diving classification. The network takes the frames of a diving video as input and determines its class. We evaluate the performance of the proposed model on a recently introduced competitive diving dataset, Diving48. It contains over 18000 video clips which covers 48 classes of diving. The proposed model outperforms the classification accuracy of the state-of-the-art models in both 2D and 3D frameworks by 11.54% and 4.24%, respectively. We show that the network is able to localize the diver in the video frames during the dive without being trained with such a supervision.



### Predicting How to Distribute Work Between Algorithms and Humans to Segment an Image Batch
- **Arxiv ID**: http://arxiv.org/abs/1905.00060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.00060v1)
- **Published**: 2019-04-30 18:56:24+00:00
- **Updated**: 2019-04-30 18:56:24+00:00
- **Authors**: Danna Gurari, Yinan Zhao, Suyog Dutt Jain, Margrit Betke, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: Foreground object segmentation is a critical step for many image analysis tasks. While automated methods can produce high-quality results, their failures disappoint users in need of practical solutions. We propose a resource allocation framework for predicting how best to allocate a fixed budget of human annotation effort in order to collect higher quality segmentations for a given batch of images and automated methods. The framework is based on a prediction module that estimates the quality of given algorithm-drawn segmentations. We demonstrate the value of the framework for two novel tasks related to predicting how to distribute annotation efforts between algorithms and humans. Specifically, we develop two systems that automatically decide, for a batch of images, when to recruit humans versus computers to create 1) coarse segmentations required to initialize segmentation tools and 2) final, fine-grained segmentations. Experiments demonstrate the advantage of relying on a mix of human and computer efforts over relying on either resource alone for segmenting objects in images coming from three diverse modalities (visible, phase contrast microscopy, and fluorescence microscopy).



### To believe or not to believe: Validating explanation fidelity for dynamic malware analysis
- **Arxiv ID**: http://arxiv.org/abs/1905.00122v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00122v1)
- **Published**: 2019-04-30 22:45:30+00:00
- **Updated**: 2019-04-30 22:45:30+00:00
- **Authors**: Li Chen, Carter Yagemann, Evan Downing
- **Comment**: Accepted at the IEEE Computer Vision Pattern Recognition 2019
  Explainable AI Workshop
- **Journal**: None
- **Summary**: Converting malware into images followed by vision-based deep learning algorithms has shown superior threat detection efficacy compared with classical machine learning algorithms. When malware are visualized as images, visual-based interpretation schemes can also be applied to extract insights of why individual samples are classified as malicious. In this work, via two case studies of dynamic malware classification, we extend the local interpretable model-agnostic explanation algorithm to explain image-based dynamic malware classification and examine its interpretation fidelity. For both case studies, we first train deep learning models via transfer learning on malware images, demonstrate high classification effectiveness, apply an explanation method on the images, and correlate the results back to the samples to validate whether the algorithmic insights are consistent with security domain expertise. In our first case study, the interpretation framework identifies indirect calls that uniquely characterize the underlying exploit behavior of a malware family. In our second case study, the interpretation framework extracts insightful information such as cryptography-related APIs when applied on images created from API existence, but generate ambiguous interpretation on images created from API sequences and frequencies. Our findings indicate that current image-based interpretation techniques are promising for explaining vision-based malware classification. We continue to develop image-based interpretation schemes specifically for security applications.



### Harmonic Networks with Limited Training Samples
- **Arxiv ID**: http://arxiv.org/abs/1905.00135v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1905.00135v1)
- **Published**: 2019-04-30 23:35:30+00:00
- **Updated**: 2019-04-30 23:35:30+00:00
- **Authors**: Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot
- **Comment**: None
- **Journal**: European Signal Processing Conference (EUSIPCO) 2019
- **Summary**: Convolutional neural networks (CNNs) are very popular nowadays for image processing. CNNs allow one to learn optimal filters in a (mostly) supervised machine learning context. However this typically requires abundant labelled training data to estimate the filter parameters. Alternative strategies have been deployed for reducing the number of parameters and / or filters to be learned and thus decrease overfitting. In the context of reverting to preset filters, we propose here a computationally efficient harmonic block that uses Discrete Cosine Transform (DCT) filters in CNNs. In this work we examine the performance of harmonic networks in limited training data scenario. We validate experimentally that its performance compares well against scattering networks that use wavelets as preset filters.



### ResNet Can Be Pruned 60x: Introducing Network Purification and Unused Path Removal (P-RM) after Weight Pruning
- **Arxiv ID**: http://arxiv.org/abs/1905.00136v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1905.00136v1)
- **Published**: 2019-04-30 23:40:51+00:00
- **Updated**: 2019-04-30 23:40:51+00:00
- **Authors**: Xiaolong Ma, Geng Yuan, Sheng Lin, Zhengang Li, Hao Sun, Yanzhi Wang
- **Comment**: Submitted to ICML workshop
- **Journal**: None
- **Summary**: The state-of-art DNN structures involve high computation and great demand for memory storage which pose intensive challenge on DNN framework resources. To mitigate the challenges, weight pruning techniques has been studied. However, high accuracy solution for extreme structured pruning that combines different types of structured sparsity still waiting for unraveling due to the extremely reduced weights in DNN networks. In this paper, we propose a DNN framework which combines two different types of structured weight pruning (filter and column prune) by incorporating alternating direction method of multipliers (ADMM) algorithm for better prune performance. We are the first to find non-optimality of ADMM process and unused weights in a structured pruned model, and further design an optimization framework which contains the first proposed Network Purification and Unused Path Removal algorithms which are dedicated to post-processing an structured pruned model after ADMM steps. Some high lights shows we achieve 232x compression on LeNet-5, 60x compression on ResNet-18 CIFAR-10 and over 5x compression on AlexNet. We share our models at anonymous link http://bit.ly/2VJ5ktv.



