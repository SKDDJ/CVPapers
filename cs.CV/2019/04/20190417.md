# Arxiv Papers in cs.CV on 2019-04-17
### People infer recursive visual concepts from just a few examples
- **Arxiv ID**: http://arxiv.org/abs/1904.08034v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08034v2)
- **Published**: 2019-04-17 00:45:05+00:00
- **Updated**: 2019-07-29 15:26:40+00:00
- **Authors**: Brenden M. Lake, Steven T. Piantadosi
- **Comment**: In press at "Computational Brain & Behavior"
- **Journal**: None
- **Summary**: Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal "programs" -- latent generating processes with nontrivial algorithmic properties -- from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pre-trained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.



### DENet: A Universal Network for Counting Crowd with Varying Densities and Scales
- **Arxiv ID**: http://arxiv.org/abs/1904.08056v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08056v1)
- **Published**: 2019-04-17 02:34:04+00:00
- **Updated**: 2019-04-17 02:34:04+00:00
- **Authors**: Lei Liu, Jie Jiang, Wenjing Jia, Saeed Amirgholipour, Michelle Zeibots, Xiangjian He
- **Comment**: None
- **Journal**: None
- **Summary**: Counting people or objects with significantly varying scales and densities has attracted much interest from the research community and yet it remains an open problem. In this paper, we propose a simple but an efficient and effective network, named DENet, which is composed of two components, i.e., a detection network (DNet) and an encoder-decoder estimation network (ENet). We first run DNet on an input image to detect and count individuals who can be segmented clearly. Then, ENet is utilized to estimate the density maps of the remaining areas, where the numbers of individuals cannot be detected. We propose a modified Xception as an encoder for feature extraction and a combination of dilated convolution and transposed convolution as a decoder. In the ShanghaiTech Part A, UCF and WorldExpo'10 datasets, our DENet achieves lower Mean Absolute Error (MAE) than those of the state-of-the-art methods.



### Deep Fusion Network for Image Completion
- **Arxiv ID**: http://arxiv.org/abs/1904.08060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08060v1)
- **Published**: 2019-04-17 02:48:02+00:00
- **Updated**: 2019-04-17 02:48:02+00:00
- **Authors**: Xin Hong, Pengfei Xiong, Renhe Ji, Haoqiang Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep image completion usually fails to harmonically blend the restored image into existing content, especially in the boundary area. This paper handles with this problem from a new perspective of creating a smooth transition and proposes a concise Deep Fusion Network (DFNet). Firstly, a fusion block is introduced to generate a flexible alpha composition map for combining known and unknown regions. The fusion block not only provides a smooth fusion between restored and existing content, but also provides an attention map to make network focus more on the unknown pixels. In this way, it builds a bridge for structural and texture information, so that information can be naturally propagated from known region into completion. Furthermore, fusion blocks are embedded into several decoder layers of the network. Accompanied by the adjustable loss constraints on each layer, more accurate structure information are achieved. We qualitatively and quantitatively compare our method with other state-of-the-art methods on Places2 and CelebA datasets. The results show the superior performance of DFNet, especially in the aspects of harmonious texture transition, texture detail and semantic structural consistency. Our source code will be avaiable at: \url{https://github.com/hughplay/DFNet}



### Neural Painters: A learned differentiable constraint for generating brushstroke paintings
- **Arxiv ID**: http://arxiv.org/abs/1904.08410v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08410v2)
- **Published**: 2019-04-17 03:03:49+00:00
- **Updated**: 2019-04-22 17:14:53+00:00
- **Authors**: Reiichiro Nakano
- **Comment**: Added more references and acknowledgments
- **Journal**: None
- **Summary**: We explore neural painters, a generative model for brushstrokes learned from a real non-differentiable and non-deterministic painting program. We show that when training an agent to "paint" images using brushstrokes, using a differentiable neural painter leads to much faster convergence. We propose a method for encouraging this agent to follow human-like strokes when reconstructing digits. We also explore the use of a neural painter as a differentiable image parameterization. By directly optimizing brushstrokes to activate neurons in a pre-trained convolutional network, we can directly visualize ImageNet categories and generate "ideal" paintings of each class. Finally, we present a new concept called intrinsic style transfer. By minimizing only the content loss from neural style transfer, we allow the artistic medium, in this case, brushstrokes, to naturally dictate the resulting style.



### Forecasting with time series imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.08064v3
- **DOI**: 10.1016/j.eswa.2020.113680
- **Categories**: **stat.ML**, cs.CV, cs.LG, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08064v3)
- **Published**: 2019-04-17 03:18:45+00:00
- **Updated**: 2020-06-05 03:13:12+00:00
- **Authors**: Xixi Li, Yanfei Kang, Feng Li
- **Comment**: None
- **Journal**: Expert Systems with Applications (2020)
- **Summary**: Feature-based time series representations have attracted substantial attention in a wide range of time series analysis methods. Recently, the use of time series features for forecast model averaging has been an emerging research focus in the forecasting community. Nonetheless, most of the existing approaches depend on the manual choice of an appropriate set of features. Exploiting machine learning methods to extract features from time series automatically becomes crucial in state-of-the-art time series analysis. In this paper, we introduce an automated approach to extract time series features based on time series imaging. We first transform time series into recurrence plots, from which local features can be extracted using computer vision algorithms. The extracted features are used for forecast model averaging. Our experiments show that forecasting based on automatically extracted features, with less human intervention and a more comprehensive view of the raw time series data, yields highly comparable performances with the best methods in the largest forecasting competition dataset (M4) and outperforms the top methods in the Tourism forecasting competition dataset.



### Collaboration Analysis Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.08066v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1904.08066v1)
- **Published**: 2019-04-17 03:23:17+00:00
- **Updated**: 2019-04-17 03:23:17+00:00
- **Authors**: Zhang Guo, Kevin Yu, Rebecca Pearlman, Nassir Navab, Roghayeh Barmaki
- **Comment**: 6 pages, 4 Figures and a Table
- **Journal**: None
- **Summary**: The analysis of the collaborative learning process is one of the growing fields of education research, which has many different analytic solutions. In this paper, we provided a new solution to improve automated collaborative learning analyses using deep neural networks. Instead of using self-reported questionnaires, which are subject to bias and noise, we automatically extract group-working information by object recognition results using Mask R-CNN method. This process is based on detecting the people and other objects from pictures and video clips of the collaborative learning process, then evaluate the mobile learning performance using the collaborative indicators. We tested our approach to automatically evaluate the group-work collaboration in a controlled study of thirty-three dyads while performing an anatomy body painting intervention. The results indicate that our approach recognizes the differences of collaborations among teams of treatment and control groups in the case study. This work introduces new methods for automated quality prediction of collaborations among human-human interactions using computer vision techniques.



### Converting a Common Document Scanner to a Multispectral Scanner
- **Arxiv ID**: http://arxiv.org/abs/1904.12603v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12603v1)
- **Published**: 2019-04-17 04:41:21+00:00
- **Updated**: 2019-04-17 04:41:21+00:00
- **Authors**: Zohaib Khan, Faisal Shafait, Ajmal Mian
- **Comment**: None
- **Journal**: None
- **Summary**: We propose the construction of a prototype scanner designed to capture multispectral images of documents. A standard sheet-feed scanner is modified by disconnecting its internal light source and connecting an external multispectral light source comprising of narrow band light emitting diodes (LED). A document is scanned by illuminating the scanner light guide successively with different LEDs and capturing a scan of the document. The system is portable and can be used for potential applications in verification of questioned documents, cheques, receipts and bank notes.



### SCE: A manifold regularized set-covering method for data partitioning
- **Arxiv ID**: http://arxiv.org/abs/1904.08412v1
- **DOI**: 10.1109/TNNLS.2017.2682179
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.08412v1)
- **Published**: 2019-04-17 04:43:17+00:00
- **Updated**: 2019-04-17 04:43:17+00:00
- **Authors**: Xuelong Li, Quanmao Lu, Yongsheng Dong, Dacheng Tao
- **Comment**: 14 pages, 10 figures
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems, May,
  2018
- **Summary**: Cluster analysis plays a very important role in data analysis. In these years, cluster ensemble, as a cluster analysis tool, has drawn much attention for its robustness, stability, and accuracy. Many efforts have been done to combine different initial clustering results into a single clustering solution with better performance. However, they neglect the structure information of the raw data in performing the cluster ensemble. In this paper, we propose a Structural Cluster Ensemble (SCE) algorithm for data partitioning formulated as a set-covering problem. In particular, we construct a Laplacian regularized objective function to capture the structure information among clusters. Moreover, considering the importance of the discriminative information underlying in the initial clustering results, we add a discriminative constraint into our proposed objective function. Finally, we verify the performance of the SCE algorithm on both synthetic and real data sets. The experimental results show the effectiveness of our proposed method SCE algorithm.



### Bottleneck potentials in Markov Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1904.08080v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08080v2)
- **Published**: 2019-04-17 04:43:38+00:00
- **Updated**: 2019-08-15 19:45:59+00:00
- **Authors**: Ahmed Abbas, Paul Swoboda
- **Comment**: Published in ICCV 2019 as Oral
- **Journal**: None
- **Summary**: We consider general discrete Markov Random Fields(MRFs) with additional bottleneck potentials which penalize the maximum (instead of the sum) over local potential value taken by the MRF-assignment. Bottleneck potentials or analogous constructions have been considered in (i) combinatorial optimization (e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree problem, bottleneck function minimization in greedoids), (ii) inverse problems with $L_{\infty}$-norm regularization, and (iii) valued constraint satisfaction on the $(\min,\max)$-pre-semirings. Bottleneck potentials for general discrete MRFs are a natural generalization of the above direction of modeling work to Maximum-A-Posteriori (MAP) inference in MRFs. To this end, we propose MRFs whose objective consists of two parts: terms that factorize according to (i) $(\min,+)$, i.e. potentials as in plain MRFs, and (ii) $(\min,\max)$, i.e. bottleneck potentials. To solve the ensuing inference problem, we propose high-quality relaxations and efficient algorithms for solving them. We empirically show efficacy of our approach on large scale seismic horizon tracking problems.



### General Purpose (GenP) Bioimage Ensemble of Handcrafted and Learned Features with Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08084v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08084v4)
- **Published**: 2019-04-17 05:11:55+00:00
- **Updated**: 2021-07-06 05:31:12+00:00
- **Authors**: L. Nanni, S. Brahnam, S. Ghidoni, G. Maguolo
- **Comment**: 27 pages, 1 figure, 5 tables, manuscript
- **Journal**: None
- **Summary**: Bioimage classification plays a crucial role in many biological problems. In this work, we present a new General Purpose (GenP) ensemble that boosts performance by combining local features, dense sampling features, and deep learning approaches. First, we introduce three new methods for data augmentation based on PCA/DCT; second, we show that different data augmentation approaches can boost the performance of an ensemble of CNNs; and, finally, we propose a set of handcrafted/learned descriptors that are highly generalizable. Each handcrafted descriptor is used to train a different Support Vector Machine (SVM), and the different SVMs are combined with the ensemble of CNNs. Our method is evaluated on a diverse set of bioimage classification problems. Results demonstrate that the proposed GenP bioimage ensemble obtains state-of-the-art performance without any ad-hoc dataset tuning of parameters (thus avoiding the risk of overfitting/overtraining).



### Adversarial Defense Through Network Profiling Based Path Extraction
- **Arxiv ID**: http://arxiv.org/abs/1904.08089v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08089v2)
- **Published**: 2019-04-17 05:51:24+00:00
- **Updated**: 2019-05-09 04:40:40+00:00
- **Authors**: Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, Yuhao Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, researchers have started decomposing deep neural network models according to their semantics or functions. Recent work has shown the effectiveness of decomposed functional blocks for defending adversarial attacks, which add small input perturbation to the input image to fool the DNN models. This work proposes a profiling-based method to decompose the DNN models to different functional blocks, which lead to the effective path as a new approach to exploring DNNs' internal organization. Specifically, the per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. We propose an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability than the state-of-the-art technique.



### TextCaps : Handwritten Character Recognition with Very Small Datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.08095v1
- **DOI**: 10.1109/WACV.2019.00033
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08095v1)
- **Published**: 2019-04-17 06:09:59+00:00
- **Updated**: 2019-04-17 06:09:59+00:00
- **Authors**: Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Jathushan Rajasegaran, Suranga Seneviratne, Ranga Rodrigo
- **Comment**: None
- **Journal**: In 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV) (pp. 254-262). IEEE 2019
- **Summary**: Many localized languages struggle to reap the benefits of recent advancements in character recognition systems due to the lack of substantial amount of labeled training data. This is due to the difficulty in generating large amounts of labeled data for such languages and inability of deep learning techniques to properly learn from small number of training samples. We solve this problem by introducing a technique of generating new training samples from the existing samples, with realistic augmentations which reflect actual variations that are present in human hand writing, by adding random controlled noise to their corresponding instantiation parameters. Our results with a mere 200 training samples per class surpass existing character recognition results in the EMNIST-letter dataset while achieving the existing results in the three datasets: EMNIST-balanced, EMNIST-digits, and MNIST. We also develop a strategy to effectively use a combination of loss functions to improve reconstructions. Our system is useful in character recognition for localized languages that lack much labeled training data and even in other related more general contexts such as object recognition.



### Correlated Logistic Model With Elastic Net Regularization for Multilabel Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.08098v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08098v1)
- **Published**: 2019-04-17 06:16:59+00:00
- **Updated**: 2019-04-17 06:16:59+00:00
- **Authors**: Qiang Li, Bo Xie, Jane You, Wei Bian, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present correlated logistic (CorrLog) model for multilabel image classification. CorrLog extends conventional logistic regression model into multilabel cases, via explicitly modeling the pairwise correlation between labels. In addition, we propose to learn the model parameters of CorrLog with elastic net regularization, which helps exploit the sparsity in feature selection and label correlations and thus further boost the performance of multilabel classification. CorrLog can be efficiently learned, though approximately, by regularized maximum pseudo likelihood estimation, and it enjoys a satisfying generalization bound that is independent of the number of labels. CorrLog performs competitively for multilabel image classification on benchmark data sets MULAN scene, MIT outdoor scene, PASCAL VOC 2007, and PASCAL VOC 2012, compared with the state-of-the-art multilabel classification algorithms.



### Multi-Scale Geometric Consistency Guided Multi-View Stereo
- **Arxiv ID**: http://arxiv.org/abs/1904.08103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08103v1)
- **Published**: 2019-04-17 06:36:44+00:00
- **Updated**: 2019-04-17 06:36:44+00:00
- **Authors**: Qingshan Xu, Wenbing Tao
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: In this paper, we propose an efficient multi-scale geometric consistency guided multi-view stereo method for accurate and complete depth map estimation. We first present our basic multi-view stereo method with Adaptive Checkerboard sampling and Multi-Hypothesis joint view selection (ACMH). It leverages structured region information to sample better candidate hypotheses for propagation and infer the aggregation view subset at each pixel. For the depth estimation of low-textured areas, we further propose to combine ACMH with multi-scale geometric consistency guidance (ACMM) to obtain the reliable depth estimates for low-textured areas at coarser scales and guarantee that they can be propagated to finer scales. To correct the erroneous estimates propagated from the coarser scales, we present a novel detail restorer. Experiments on extensive datasets show our method achieves state-of-the-art performance, recovering the depth estimation not only in low-textured areas but also in details.



### DistanceNet: Estimating Traveled Distance from Monocular Images using a Recurrent Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.08105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08105v1)
- **Published**: 2019-04-17 06:38:00+00:00
- **Updated**: 2019-04-17 06:38:00+00:00
- **Authors**: Robin Kreuzig, Matthias Ochs, Rudolf Mester
- **Comment**: Paper is accepted at CVPR 2019 - Workshop on Autonomous Driving
- **Journal**: None
- **Summary**: Classical monocular vSLAM/VO methods suffer from the scale ambiguity problem. Hybrid approaches solve this problem by adding deep learning methods, for example by using depth maps which are predicted by a CNN. We suggest that it is better to base scale estimation on estimating the traveled distance for a set of subsequent images. In this paper, we propose a novel end-to-end many-to-one traveled distance estimator. By using a deep recurrent convolutional neural network (RCNN), the traveled distance between the first and last image of a set of consecutive frames is estimated by our DistanceNet. Geometric features are learned in the CNN part of our model, which are subsequently used by the RNN to learn dynamics and temporal information. Moreover, we exploit the natural order of distances by using ordinal regression to predict the distance. The evaluation on the KITTI dataset shows that our approach outperforms current state-of-the-art deep learning pose estimators and classical mono vSLAM/VO methods in terms of distance prediction. Thus, our DistanceNet can be used as a component to solve the scale problem and help improve current and future classical mono vSLAM/VO methods.



### Class specific or shared? A cascaded dictionary learning framework for image classification
- **Arxiv ID**: http://arxiv.org/abs/1904.08928v2
- **DOI**: 10.1016/j.sigpro.2020.107697
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08928v2)
- **Published**: 2019-04-17 08:03:36+00:00
- **Updated**: 2020-10-23 13:59:55+00:00
- **Authors**: Yan-Jiang Wang, Shuai Shao, Rui Xu, Werifeng Liu, Bao-Di Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Dictionary learning methods can be split into: i) class specific dictionary learning ii) class shared dictionary learning. The difference between the two categories is how to use discriminative information. With the first category, samples of different classes are mapped into different subspaces, which leads to some redundancy with the class specific base vectors. While for the second category, the samples in each specific class can not be described accurately. In this paper, we first propose a novel class shared dictionary learning method named label embedded dictionary learning (LEDL). It is the improvement based on LCKSVD, which is easier to find out the optimal solution. Then we propose a novel framework named cascaded dictionary learning framework (CDLF) to combine the specific dictionary learning with shared dictionary learning to describe the feature to boost the performance of classification sufficiently. Extensive experimental results on six benchmark datasets illustrate that our methods are capable of achieving superior performance compared to several state-of-art classification algorithms.



### Modulating Image Restoration with Continual Levels via Adaptive Feature Modification Layers
- **Arxiv ID**: http://arxiv.org/abs/1904.08118v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08118v3)
- **Published**: 2019-04-17 08:10:22+00:00
- **Updated**: 2019-06-03 02:55:31+00:00
- **Authors**: Jingwen He, Chao Dong, Yu Qiao
- **Comment**: Accepted by CVPR 2019 (oral); code is available:
  https://github.com/hejingwenhejingwen/AdaFM
- **Journal**: None
- **Summary**: In image restoration tasks, like denoising and super resolution, continual modulation of restoration levels is of great importance for real-world applications, but has failed most of existing deep learning based image restoration methods. Learning from discrete and fixed restoration levels, deep models cannot be easily generalized to data of continuous and unseen levels. This topic is rarely touched in literature, due to the difficulty of modulating well-trained models with certain hyper-parameters. We make a step forward by proposing a unified CNN framework that consists of few additional parameters than a single-level model yet could handle arbitrary restoration levels between a start and an end level. The additional module, namely AdaFM layer, performs channel-wise feature modification, and can adapt a model to another restoration level with high accuracy. By simply tweaking an interpolation coefficient, the intermediate model - AdaFM-Net could generate smooth and continuous restoration effects without artifacts. Extensive experiments on three image restoration tasks demonstrate the effectiveness of both model training and modulation testing. Besides, we carefully investigate the properties of AdaFM layers, providing a detailed guidance on the usage of the proposed method.



### Automated Design of Deep Learning Methods for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08128v2
- **DOI**: 10.1038/s41592-020-01008-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08128v2)
- **Published**: 2019-04-17 08:30:17+00:00
- **Updated**: 2020-04-02 13:32:30+00:00
- **Authors**: Fabian Isensee, Paul F. Jäger, Simon A. A. Kohl, Jens Petersen, Klaus H. Maier-Hein
- **Comment**: * Fabian Isensee and Paul F. J\"ager share the first authorship
- **Journal**: Nature Methods (2020)
- **Summary**: Biomedical imaging is a driver of scientific discovery and core component of medical care, currently stimulated by the field of deep learning. While semantic segmentation algorithms enable 3D image analysis and quantification in many applications, the design of respective specialised solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We propose nnU-Net, a deep learning framework that condenses the current domain knowledge and autonomously takes the key decisions required to transfer a basic architecture to different datasets and segmentation tasks. Without manual tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public international competitions and sets a new state of the art in the majority of the 49 tasks. The results demonstrate a vast hidden potential in the systematic adaptation of deep learning methods to different datasets. We make nnU-Net publicly available as an open-source tool that can effectively be used out-of-the-box, rendering state of the art segmentation accessible to non-experts and catalyzing scientific progress as a framework for automated method design.



### MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08141v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08141v1)
- **Published**: 2019-04-17 08:52:03+00:00
- **Updated**: 2019-04-17 08:52:03+00:00
- **Authors**: Shuangjie Xu, Daizong Liu, Linchao Bao, Wei Liu, Pan Zhou
- **Comment**: accepted to CVPR 2019 as oral presentation
- **Journal**: None
- **Summary**: We address the problem of semi-supervised video object segmentation (VOS), where the masks of objects of interests are given in the first frame of an input video. To deal with challenging cases where objects are occluded or missing, previous work relies on greedy data association strategies that make decisions for each frame individually. In this paper, we propose a novel approach to defer the decision making for a target object in each frame, until a global view can be established with the entire video being taken into consideration. Our approach is in the same spirit as Multiple Hypotheses Tracking (MHT) methods, making several critical adaptations for the VOS problem. We employ the bounding box (bbox) hypothesis for tracking tree formation, and the multiple hypotheses are spawned by propagating the preceding bbox into the detected bbox proposals within a gated region starting from the initial object mask in the first frame. The gated region is determined by a gating scheme which takes into account a more comprehensive motion model rather than the simple Kalman filtering model in traditional MHT. To further design more customized algorithms tailored for VOS, we develop a novel mask propagation score instead of the appearance similarity score that could be brittle due to large deformations. The mask propagation score, together with the motion score, determines the affinity between the hypotheses during tree pruning. Finally, a novel mask merging strategy is employed to handle mask conflicts between objects. Extensive experiments on challenging datasets demonstrate the effectiveness of the proposed method, especially in the case of object missing.



### 2D Car Detection in Radar Data with PointNets
- **Arxiv ID**: http://arxiv.org/abs/1904.08414v3
- **DOI**: 10.1109/ITSC.2019.8917000
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08414v3)
- **Published**: 2019-04-17 09:27:56+00:00
- **Updated**: 2019-12-02 10:54:34+00:00
- **Authors**: Andreas Danzer, Thomas Griebel, Martin Bach, Klaus Dietmayer
- **Comment**: None
- **Journal**: IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand, 2019, pp. 61-66
- **Summary**: For many automated driving functions, a highly accurate perception of the vehicle environment is a crucial prerequisite. Modern high-resolution radar sensors generate multiple radar targets per object, which makes these sensors particularly suitable for the 2D object detection task. This work presents an approach to detect 2D objects solely depending on sparse radar data using PointNets. In literature, only methods are presented so far which perform either object classification or bounding box estimation for objects. In contrast, this method facilitates a classification together with a bounding box estimation of objects using a single radar sensor. To this end, PointNets are adjusted for radar data performing 2D object classification with segmentation, and 2D bounding box regression in order to estimate an amodal 2D bounding box. The algorithm is evaluated using an automatically created dataset which consist of various realistic driving maneuvers. The results show the great potential of object detection in high-resolution radar data using PointNets.



### Deep learning investigation for chess player attention prediction using eye-tracking and game data
- **Arxiv ID**: http://arxiv.org/abs/1904.08155v1
- **DOI**: 10.1145/3314111.3319827
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08155v1)
- **Published**: 2019-04-17 09:31:37+00:00
- **Updated**: 2019-04-17 09:31:37+00:00
- **Authors**: Justin Le Louedec, Thomas Guntz, James Crowley, Dominique Vaufreydaz
- **Comment**: None
- **Journal**: ACM Symposium On Eye Tracking Research & Applications (ETRA 2019),
  Jun 2019, Denver, United States.
- **Summary**: This article reports on an investigation of the use of convolutional neural networks to predict the visual attention of chess players. The visual attention model described in this article has been created to generate saliency maps that capture hierarchical and spatial features of chessboard, in order to predict the probability fixation for individual pixels Using a skip-layer architecture of an autoencoder, with a unified decoder, we are able to use multiscale features to predict saliency of part of the board at different scales, showing multiple relations between pieces. We have used scan path and fixation data from players engaged in solving chess problems, to compute 6600 saliency maps associated to the corresponding chess piece configurations. This corpus is completed with synthetically generated data from actual games gathered from an online chess platform. Experiments realized using both scan-paths from chess players and the CAT2000 saliency dataset of natural images, highlights several results. Deep features, pretrained on natural images, were found to be helpful in training visual attention prediction for chess. The proposed neural network architecture is able to generate meaningful saliency maps on unseen chess configurations with good scores on standard metrics. This work provides a baseline for future work on visual attention prediction in similar contexts.



### 3D Object Recognition with Ensemble Learning --- A Study of Point Cloud-Based Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1904.08159v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08159v2)
- **Published**: 2019-04-17 09:51:12+00:00
- **Updated**: 2019-05-22 22:46:23+00:00
- **Authors**: Daniel Koguciuk, Łukasz Chechliński, Tarek El-Gaaly
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, we present an analysis of model-based ensemble learning for 3D point-cloud object classification and detection. An ensemble of multiple model instances is known to outperform a single model instance, but there is little study of the topic of ensemble learning for 3D point clouds. First, an ensemble of multiple model instances trained on the same part of the $\textit{ModelNet40}$ dataset was tested for seven deep learning, point cloud-based classification algorithms: $\textit{PointNet}$, $\textit{PointNet++}$, $\textit{SO-Net}$, $\textit{KCNet}$, $\textit{DeepSets}$, $\textit{DGCNN}$, and $\textit{PointCNN}$. Second, the ensemble of different architectures was tested. Results of our experiments show that the tested ensemble learning methods improve over state-of-the-art on the $\textit{ModelNet40}$ dataset, from $92.65\%$ to $93.64\%$ for the ensemble of single architecture instances, $94.03\%$ for two different architectures, and $94.15\%$ for five different architectures. We show that the ensemble of two models with different architectures can be as effective as the ensemble of 10 models with the same architecture. Third, a study on classic bagging i.e. with different subsets used for training multiple model instances) was tested and sources of ensemble accuracy growth were investigated for best-performing architecture, i.e. $\textit{SO-Net}$. We also investigate the ensemble learning of $\textit{Frustum PointNet}$ approach in the task of 3D object detection, increasing the average precision of 3D box detection on the $\textit{KITTI}$ dataset from $63.1\%$ to $66.5\%$ using only three model instances. We measure the inference time of all 3D classification architectures on a $\textit{Nvidia Jetson TX2}$, a common embedded computer for mobile robots, to allude to the use of these models in real-life applications.



### CaseNet: Content-Adaptive Scale Interaction Networks for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1904.08170v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08170v3)
- **Published**: 2019-04-17 10:33:03+00:00
- **Updated**: 2020-10-09 08:13:39+00:00
- **Authors**: Xin Jin, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, Zhibo Chen
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: Objects at different spatial positions in an image exhibit different scales. Adaptive receptive fields are expected to capture suitable ranges of context for accurate pixel level semantic prediction. Recently, atrous convolution with different dilation rates has been used to generate features of multi-scales through several branches which are then fused for prediction. However, there is a lack of explicit interaction among the branches of different scales to adaptively make full use of the contexts. In this paper, we propose a Content-Adaptive Scale Interaction Network (CASINet) to exploit the multi-scale features for scene parsing. We build CASINet based on the classic Atrous Spatial Pyramid Pooling (ASPP) module, followed by a proposed contextual scale interaction (CSI) module, and a scale adaptation (SA) module. Specifically, in the CSI module, for each spatial position of some scale, instead of being limited by a fixed set of convolutional filters that are shared across different spatial positions for feature learning, we promote the adaptivity of the convolutional filters to spatial positions. We achieve this by the context interaction among the features of different scales. The SA module explicitly and softly selects the suitable scale for each spatial position and each channel. Ablation studies demonstrate the effectiveness of the proposed modules. We achieve state-of-the-art performance on three scene parsing benchmarks Cityscapes, ADE20K and LIP.



### Downhole Track Detection via Multiscale Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1904.08177v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08177v1)
- **Published**: 2019-04-17 10:50:37+00:00
- **Updated**: 2019-04-17 10:50:37+00:00
- **Authors**: Jia Li, Xing Wei, Guoqiang Yang, Xiao Sun, Changliang Li
- **Comment**: arXiv admin note: text overlap with arXiv:1711.11585 by other authors
- **Journal**: None
- **Summary**: Frequent mine disasters cause a large number of casualties and property losses. Autonomous driving is a fundamental measure for solving this problem, and track detection is one of the key technologies for computer vision to achieve downhole automatic driving. The track detection result based on the traditional convolutional neural network (CNN) algorithm lacks the detailed and unique description of the object and relies too much on visual postprocessing technology. Therefore, this paper proposes a track detection algorithm based on a multiscale conditional generative adversarial network (CGAN). The generator is decomposed into global and local parts using a multigranularity structure in the generator network. A multiscale shared convolution structure is adopted in the discriminator network to further supervise training the generator. Finally, the Monte Carlo search technique is introduced to search the intermediate state of the generator, and the result is sent to the discriminator for comparison. Compared with the existing work, our model achieved 82.43\% pixel accuracy and an average intersection-over-union (IOU) of 0.6218, and the detection of the track reached 95.01\% accuracy in the downhole roadway scene test set.



### CenterNet: Keypoint Triplets for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08189v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08189v3)
- **Published**: 2019-04-17 11:20:01+00:00
- **Updated**: 2019-04-19 02:23:52+00:00
- **Authors**: Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, Qi Tian
- **Comment**: 10 pages (including 2 pages of References), 7 figures, 5 tables
- **Journal**: None
- **Summary**: In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/Duankaiwen/CenterNet.



### Guided Anisotropic Diffusion and Iterative Learning for Weakly Supervised Change Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08208v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08208v1)
- **Published**: 2019-04-17 11:53:52+00:00
- **Updated**: 2019-04-17 11:53:52+00:00
- **Authors**: Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau
- **Comment**: Accepted at CVPR 2019 Workshops
- **Journal**: None
- **Summary**: Large scale datasets created from user labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose an iterative learning method that extracts the useful information from a large scale change detection dataset generated from open vector data to train a fully convolutional network which surpasses the performance obtained by naive supervised learning. We also propose the guided anisotropic diffusion algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering, and is used in conjunction with the iterative training method to improve results.



### Self-Supervised Flow Estimation using Geometric Regularization with Applications to Camera Image and Grid Map Sequences
- **Arxiv ID**: http://arxiv.org/abs/1904.12599v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.12599v1)
- **Published**: 2019-04-17 12:22:03+00:00
- **Updated**: 2019-04-17 12:22:03+00:00
- **Authors**: Sascha Wirges, Johannes Gräter, Qiuhao Zhang, Christoph Stiller
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: We present a self-supervised approach to estimate flow in camera image and top-view grid map sequences using fully convolutional neural networks in the domain of automated driving. We extend existing approaches for self-supervised optical flow estimation by adding a regularizer expressing motion consistency assuming a static environment. However, as this assumption is violated for other moving traffic participants we also estimate a mask to scale this regularization. Adding a regularization towards motion consistency improves convergence and flow estimation accuracy. Furthermore, we scale the errors due to spatial flow inconsistency by a mask that we derive from the motion mask. This improves accuracy in regions where the flow drastically changes due to a better separation between static and dynamic environment. We apply our approach to optical flow estimation from camera image sequences, validate on odometry estimation and suggest a method to iteratively increase optical flow estimation accuracy using the generated motion masks. Finally, we provide quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences. We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization.



### Registration of retinal images from Public Health by minimising an error between vessels using an affine model with radial distortions
- **Arxiv ID**: http://arxiv.org/abs/1904.12733v1
- **DOI**: 10.1109/ISBI.2019.8759415
- **Categories**: **physics.med-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12733v1)
- **Published**: 2019-04-17 12:30:38+00:00
- **Updated**: 2019-04-17 12:30:38+00:00
- **Authors**: Guillaume Noyel, R Thomas, S Iles, G Bhakta, A Crowder, D. Owens, P. Boyle
- **Comment**: None
- **Journal**: IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019), IEEE, Apr 2019, Venice, Italy. pp.561-564
- **Summary**: In order to estimate a registration model of eye fundus images made of an affinity and two radial distortions, we introduce an estimation criterion based on an error between the vessels. In [1], we estimated this model by minimising the error between characteristics points. In this paper, the detected vessels are selected using the circle and ellipse equations of the overlap area boundaries deduced from our model. Our method successfully registers 96 % of the 271 pairs in a Public Health dataset acquired mostly with different cameras. This is better than our previous method [1] and better than three other state-of-the-art methods. On a publicly available dataset, ours still better register the images than the reference method.



### Region homogeneity in the Logarithmic Image Processing framework: application to region growing algorithms
- **Arxiv ID**: http://arxiv.org/abs/1904.12597v1
- **DOI**: 10.5566/ias.2038
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12597v1)
- **Published**: 2019-04-17 12:34:28+00:00
- **Updated**: 2019-04-17 12:34:28+00:00
- **Authors**: Guillaume Noyel, Michel Jourlin
- **Comment**: The original publication is available at www.ias-iss.org
- **Journal**: Image Analysis and Stereology, International Society for
  Stereology, 2019, 38 (1), pp.43-52
- **Summary**: In order to create an image segmentation method robust to lighting changes, two novel homogeneity criteria of an image region were studied. Both were defined using the Logarithmic Image Processing (LIP) framework whose laws model lighting changes. The first criterion estimates the LIP-additive homogeneity and is based on the LIP-additive law. It is theoretically insensitive to lighting changes caused by variations of the camera exposure-time or source intensity. The second, the LIP-multiplicative homogeneity criterion, is based on the LIP-multiplicative law and is insensitive to changes due to variations of the object thickness or opacity. Each criterion is then applied in Revol and Jourlin's (1997) region growing method which is based on the homogeneity of an image region. The region growing method becomes therefore robust to the lighting changes specific to each criterion. Experiments on simulated and on real images presenting lighting variations prove the robustness of the criteria to those variations. Compared to a state-of the art method based on the image component-tree, ours is more robust. These results open the way to numerous applications where the lighting is uncontrolled or partially controlled.



### Deep Anomaly Detection for Generalized Face Anti-Spoofing
- **Arxiv ID**: http://arxiv.org/abs/1904.08241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08241v1)
- **Published**: 2019-04-17 12:52:21+00:00
- **Updated**: 2019-04-17 12:52:21+00:00
- **Authors**: Daniel Pérez-Cabo, David Jiménez-Cabello, Artur Costa-Pazo, Roberto J. López-Sastre
- **Comment**: To appear at CVPR19 (workshop)
- **Journal**: None
- **Summary**: Face recognition has achieved unprecedented results, surpassing human capabilities in certain scenarios. However, these automatic solutions are not ready for production because they can be easily fooled by simple identity impersonation attacks. And although much effort has been devoted to develop face anti-spoofing models, their generalization capacity still remains a challenge in real scenarios. In this paper, we introduce a novel approach that reformulates the Generalized Presentation Attack Detection (GPAD) problem from an anomaly detection perspective. Technically, a deep metric learning model is proposed, where a triplet focal loss is used as a regularization for a novel loss coined "metric-softmax", which is in charge of guiding the learning process towards more discriminative feature representations in an embedding space. Finally, we demonstrate the benefits of our deep anomaly detection architecture, by introducing a few-shot a posteriori probability estimation that does not need any classifier to be trained on the learned features. We conduct extensive experiments using the GRAD-GPAD framework that provides the largest aggregated dataset for face GPAD. Results confirm that our approach is able to outperform all the state-of-the-art methods by a considerable margin.



### LO-Net: Deep Real-time Lidar Odometry
- **Arxiv ID**: http://arxiv.org/abs/1904.08242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08242v2)
- **Published**: 2019-04-17 12:53:30+00:00
- **Updated**: 2020-01-17 08:35:14+00:00
- **Authors**: Qing Li, Shaoyang Chen, Cheng Wang, Xin Li, Chenglu Wen, Ming Cheng, Jonathan Li
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel deep convolutional network pipeline, LO-Net, for real-time lidar odometry estimation. Unlike most existing lidar odometry (LO) estimations that go through individually designed feature selection, feature matching, and pose estimation pipeline, LO-Net can be trained in an end-to-end manner. With a new mask-weighted geometric constraint loss, LO-Net can effectively learn feature representation for LO estimation, and can implicitly exploit the sequential dependencies and dynamics in the data. We also design a scan-to-map module, which uses the geometric and semantic information learned in LO-Net, to improve the estimation accuracy. Experiments on benchmark datasets demonstrate that LO-Net outperforms existing learning based approaches and has similar accuracy with the state-of-the-art geometry-based approach, LOAM.



### End-to-End Learning of Representations for Asynchronous Event-Based Data
- **Arxiv ID**: http://arxiv.org/abs/1904.08245v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08245v4)
- **Published**: 2019-04-17 12:54:37+00:00
- **Updated**: 2019-08-20 08:01:19+00:00
- **Authors**: Daniel Gehrig, Antonio Loquercio, Konstantinos G. Derpanis, Davide Scaramuzza
- **Comment**: To appear at ICCV 2019
- **Journal**: None
- **Summary**: Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as "events". They have appealing advantages over frame-based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatiotemporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations through a sequence of differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.



### USE-Net: incorporating Squeeze-and-Excitation blocks into U-Net for prostate zonal segmentation of multi-institutional MRI datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.08254v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08254v2)
- **Published**: 2019-04-17 13:02:30+00:00
- **Updated**: 2019-07-17 04:10:46+00:00
- **Authors**: Leonardo Rundo, Changhee Han, Yudai Nagano, Jin Zhang, Ryuichiro Hataya, Carmelo Militello, Andrea Tangherloni, Marco S. Nobile, Claudio Ferretti, Daniela Besozzi, Maria Carla Gilardi, Salvatore Vitabile, Giancarlo Mauri, Hideki Nakayama, Paolo Cazzaniga
- **Comment**: 44 pages, 6 figures, Accepted to Neurocomputing, Co-first authors:
  Leonardo Rundo and Changhee Han
- **Journal**: None
- **Summary**: Prostate cancer is the most common malignant tumors in men but prostate Magnetic Resonance Imaging (MRI) analysis remains challenging. Besides whole prostate gland segmentation, the capability to differentiate between the blurry boundary of the Central Gland (CG) and Peripheral Zone (PZ) can lead to differential diagnosis, since tumor's frequency and severity differ in these regions. To tackle the prostate zonal segmentation task, we propose a novel Convolutional Neural Network (CNN), called USE-Net, which incorporates Squeeze-and-Excitation (SE) blocks into U-Net. Especially, the SE blocks are added after every Encoder (Enc USE-Net) or Encoder-Decoder block (Enc-Dec USE-Net). This study evaluates the generalization ability of CNN-based architectures on three T2-weighted MRI datasets, each one consisting of a different number of patients and heterogeneous image characteristics, collected by different institutions. The following mixed scheme is used for training/testing: (i) training on either each individual dataset or multiple prostate MRI datasets and (ii) testing on all three datasets with all possible training/testing combinations. USE-Net is compared against three state-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-Scale Dense Network), along with a semi-automatic continuous max-flow model. The results show that training on the union of the datasets generally outperforms training on each dataset separately, allowing for both intra-/cross-dataset generalization. Enc USE-Net shows good overall generalization under any training condition, while Enc-Dec USE-Net remarkably outperforms the other methods when trained on all datasets. These findings reveal that the SE blocks' adaptive feature recalibration provides excellent cross-dataset generalization when testing is performed on samples of the datasets used during training.



### Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for Unsupervised Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1904.08265v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08265v1)
- **Published**: 2019-04-17 13:30:49+00:00
- **Updated**: 2019-04-17 13:30:49+00:00
- **Authors**: Li Yuan, Francis EH Tay, Ping Li, Li Zhou, Jiashi Feng
- **Comment**: Accepted at AAAI 2019
- **Journal**: None
- **Summary**: In this paper, we present a novel unsupervised video summarization model that requires no manual annotation. The proposed model termed Cycle-SUM adopts a new cycle-consistent adversarial LSTM architecture that can effectively maximize the information preserving and compactness of the summary video. It consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-direction LSTM network that learns video representations that embed the long-range relationships among video frames. The evaluator defines a learnable information preserving metric between original video and summary video and "supervises" the selector to identify the most informative frames to form the summary video. In particular, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN is learned to reconstruct original video from summary video while the backward GAN learns to invert the processing. The consistency between the output of such cycle learning is adopted as the information preserving metric for video summarization. We demonstrate the close relation between mutual information maximization and such cycle learning procedure. Experiments on two video summarization benchmark datasets validate the state-of-the-art performance and superiority of the Cycle-SUM model over previous baselines.



### BS-Nets: An End-to-End Framework For Band Selection of Hyperspectral Image
- **Arxiv ID**: http://arxiv.org/abs/1904.08269v1
- **DOI**: 10.1109/TGRS.2019.2951433
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08269v1)
- **Published**: 2019-04-17 13:41:38+00:00
- **Updated**: 2019-04-17 13:41:38+00:00
- **Authors**: Yaoming Cai, Xiaobo Liu, Zhihua Cai
- **Comment**: The paper has been submitted to IEEE TGRS
- **Journal**: None
- **Summary**: Hyperspectral image (HSI) consists of hundreds of continuous narrow bands with high spectral correlation, which would lead to the so-called Hughes phenomenon and the high computational cost in processing. Band selection has been proven effective in avoiding such problems by removing the redundant bands. However, many of existing band selection methods separately estimate the significance for every single band and cannot fully consider the nonlinear and global interaction between spectral bands. In this paper, by assuming that a complete HSI can be reconstructed from its few informative bands, we propose a general band selection framework, Band Selection Network (termed as BS-Net). The framework consists of a band attention module (BAM), which aims to explicitly model the nonlinear inter-dependencies between spectral bands, and a reconstruction network (RecNet), which is used to restore the original HSI cube from the learned informative bands, resulting in a flexible architecture. The resulting framework is end-to-end trainable, making it easier to train from scratch and to combine with existing networks. We implement two BS-Nets respectively using fully connected networks (BS-Net-FC) and convolutional neural networks (BS-Net-Conv), and compare the results with many existing band selection approaches for three real hyperspectral images, demonstrating that the proposed BS-Nets can accurately select informative band subset with less redundancy and achieve significantly better classification performance with an acceptable time cost.



### Interpreting Adversarial Examples with Attributes
- **Arxiv ID**: http://arxiv.org/abs/1904.08279v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08279v1)
- **Published**: 2019-04-17 14:04:17+00:00
- **Updated**: 2019-04-17 14:04:17+00:00
- **Authors**: Sadaf Gulshad, Jan Hendrik Metzen, Arnold Smeulders, Zeynep Akata
- **Comment**: None
- **Journal**: None
- **Summary**: Deep computer vision systems being vulnerable to imperceptible and carefully crafted noise have raised questions regarding the robustness of their decisions. We take a step back and approach this problem from an orthogonal direction. We propose to enable black-box neural networks to justify their reasoning both for clean and for adversarial examples by leveraging attributes, i.e. visually discriminative properties of objects. We rank attributes based on their class relevance, i.e. how the classification decision changes when the input is visually slightly perturbed, as well as image relevance, i.e. how well the attributes can be localized on both clean and perturbed images. We present comprehensive experiments for attribute prediction, adversarial example generation, adversarially robust learning, and their qualitative and quantitative analysis using predicted attributes on three benchmark datasets.



### Events-to-Video: Bringing Modern Computer Vision to Event Cameras
- **Arxiv ID**: http://arxiv.org/abs/1904.08298v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08298v1)
- **Published**: 2019-04-17 14:54:49+00:00
- **Updated**: 2019-04-17 14:54:49+00:00
- **Authors**: Henri Rebecq, René Ranftl, Vladlen Koltun, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Long Beach, 2019
- **Summary**: Event cameras are novel sensors that report brightness changes in the form of asynchronous "events" instead of intensity frames. They have significant advantages over conventional cameras: high temporal resolution, high dynamic range, and no motion blur. Since the output of event cameras is fundamentally different from conventional cameras, it is commonly accepted that they require the development of specialized algorithms to accommodate the particular nature of events. In this work, we take a different view and propose to apply existing, mature computer vision techniques to videos reconstructed from event data. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. Our experiments show that our approach surpasses state-of-the-art reconstruction methods by a large margin (> 20%) in terms of image quality. We further apply off-the-shelf computer vision algorithms to videos reconstructed from event data on tasks such as object classification and visual-inertial odometry, and show that this strategy consistently outperforms algorithms that were specifically designed for event data. We believe that our approach opens the door to bringing the outstanding properties of event cameras to an entirely new range of tasks. A video of the experiments is available at https://youtu.be/IdYrC4cUO0I



### Question Guided Modular Routing Networks for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1904.08324v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08324v3)
- **Published**: 2019-04-17 15:45:13+00:00
- **Updated**: 2020-09-04 17:21:28+00:00
- **Authors**: Yanze Wu, Qiang Sun, Jianqi Ma, Bin Li, Yanwei Fu, Yao Peng, Xiangyang Xue
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the task of Visual Question Answering (VQA), which is topical in Multimedia community recently. Particularly, we explore two critical research problems existed in VQA: (1) efficiently fusing the visual and textual modalities; (2) enabling the visual reasoning ability of VQA models in answering complex questions. To address these challenging problems, a novel Question Guided Modular Routing Networks (QGMRN) has been proposed in this paper. Particularly, The QGMRN is composed of visual, textual and routing network. The visual and textual network serve as the backbones for the generic feature extractors of visual and textual modalities. QGMRN can fuse the visual and textual modalities at multiple semantic levels. Typically, the visual reasoning is facilitated by the routing network in a discrete and stochastic way by using Gumbel-Softmax trick for module selection. When the input reaches a certain modular layer, routing network newly proposed in this paper, dynamically selects a portion of modules from that layer to process the input depending on the question features generated by the textual network. It can also learn to reason by routing between the generic modules without additional supervision information or expert knowledge. Benefiting from the dynamic routing mechanism, QGMRN can outperform the previous classical VQA methods by a large margin and achieve the competitive results against the state-of-the-art methods. Furthermore, attention mechanism is integrated into our QGMRN model and thus can further boost the model performance. Empirically, extensive experiments on the CLEVR and CLEVR-Humans datasets validate the effectiveness of our proposed model, and the state-of-the-art performance has been achieved.



### A large-scale field test on word-image classification in large historical document collections using a traditional and two deep-learning methods
- **Arxiv ID**: http://arxiv.org/abs/1904.08421v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08421v1)
- **Published**: 2019-04-17 16:03:14+00:00
- **Updated**: 2019-04-17 16:03:14+00:00
- **Authors**: Lambert Schomaker
- **Comment**: Field test of a large operational image search system, comparing BOVW
  and end-to-end CNN
- **Journal**: None
- **Summary**: This technical report describes a practical field test on word-image classification in a very large collection of more than 300 diverse handwritten historical manuscripts, with 1.6 million unique labeled images and more than 11 million images used in testing. Results indicate that several deep-learning tests completely failed (mean accuracy 83%). In the tests with more than 1000 output units (lexical words) in one-hot encoding for classification, performance steeply drops to almost zero percent accuracy, even with a modest size of the pre-final (i.e., penultimate) layer (150 units). A traditional feature method (BOVW) displays a consistent performance over numbers of classes and numbers of training examples (mean accuracy 87%). Additional tests using nearest mean on the output of the pre-final layer of an Inception V3 network, for each book, only yielded mediocre results (mean accuracy 49\%), but was not sensitive to high numbers of classes. Notably, this experiment was only possible on the basis of labels that were harvested on the basis of a traditional method which already works starting from a single labeled image per class. It is expected that the performance of the failed deep learning tests can be repaired, but only on the basis of human handcrafting (sic) of network architecture and hyperparameters. When the failed problematic books are not considered, end-to-end CNN training yields about 95% accuracy. This average is dominated by a large subset of Chinese characters, performances for other script styles being lower.



### Aggregation Cross-Entropy for Sequence Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.08364v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08364v2)
- **Published**: 2019-04-17 17:03:48+00:00
- **Updated**: 2019-04-18 03:32:38+00:00
- **Authors**: Zecheng Xie, Yaoxiong Huang, Yuanzhi Zhu, Lianwen Jin, Yuliang Liu, Lele Xie
- **Comment**: 10 pages, 6 figures, Accepted by CVPR2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel method, aggregation cross-entropy (ACE), for sequence recognition from a brand new perspective. The ACE loss function exhibits competitive performance to CTC and the attention mechanism, with much quicker implementation (as it involves only four fundamental formulas), faster inference\back-propagation (approximately O(1) in parallel), less storage requirement (no parameter and negligible runtime memory), and convenient employment (by replacing CTC with ACE). Furthermore, the proposed ACE loss function exhibits two noteworthy properties: (1) it can be directly applied for 2D prediction by flattening the 2D prediction into 1D prediction as the input and (2) it requires only characters and their numbers in the sequence annotation for supervision, which allows it to advance beyond sequence recognition, e.g., counting problem. The code is publicly available at https://github.com/summerlvsong/Aggregation-Cross-Entropy.



### Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion
- **Arxiv ID**: http://arxiv.org/abs/1904.08366v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08366v4)
- **Published**: 2019-04-17 17:07:47+00:00
- **Updated**: 2019-09-21 22:25:37+00:00
- **Authors**: Tao Hu, Zhizhong Han, Abhinav Shrivastava, Matthias Zwicker
- **Comment**: ICCV 2019 workshop on Geometry meets Deep Learning
- **Journal**: None
- **Summary**: We propose a novel approach for 3D shape completion by synthesizing multi-view depth maps. While previous work for shape completion relies on volumetric representations, meshes, or point clouds, we propose to use multi-view depth maps from a set of fixed viewing angles as our shape representation. This allows us to be free of the limitations of memory for volumetric representations and point clouds by casting shape completion into an image-to-image translation problem. Specifically, we render depth maps of the incomplete shape from a fixed set of viewpoints, and perform depth map completion in each view. Different from image-to-image translation network that completes each view separately, our novel network, multi-view completion net (MVCN), leverages information from all views of a 3D shape to help the completion of each single view. This enables MVCN to leverage more information from different depth views to achieve high accuracy in single depth view completion and keep the consistency among the completed depth images in different views. Benefited by the multi-view representation and the novel network structure, MVCN significantly improves the accuracy of 3D shape completion in large-scale benchmarks compared to the state of the art.



### Gaze Training by Modulated Dropout Improves Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.08377v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08377v2)
- **Published**: 2019-04-17 17:25:37+00:00
- **Updated**: 2019-08-16 11:36:33+00:00
- **Authors**: Yuying Chen, Congcong Liu, Lei Tai, Ming Liu, Bertram E. Shi
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: Imitation learning by behavioral cloning is a prevalent method that has achieved some success in vision-based autonomous driving. The basic idea behind behavioral cloning is to have the neural network learn from observing a human expert's behavior. Typically, a convolutional neural network learns to predict the steering commands from raw driver-view images by mimicking the behaviors of human drivers. However, there are other cues, such as gaze behavior, available from human drivers that have yet to be exploited. Previous researches have shown that novice human learners can benefit from observing experts' gaze patterns. We present here that deep neural networks can also profit from this. We propose a method, gaze-modulated dropout, for integrating this gaze information into a deep driving network implicitly rather than as an additional input. Our experimental results demonstrate that gaze-modulated dropout enhances the generalization capability of the network to unseen scenes. Prediction error in steering commands is reduced by 23.5% compared to uniform dropout. Running closed loop in the simulator, the gaze-modulated dropout net increased the average distance travelled between infractions by 58.5%. Consistent with these results, the gaze-modulated dropout net shows lower model uncertainty.



### Vid2Game: Controllable Characters Extracted from Real-World Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.08379v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.GR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08379v1)
- **Published**: 2019-04-17 17:26:14+00:00
- **Updated**: 2019-04-17 17:26:14+00:00
- **Authors**: Oran Gafni, Lior Wolf, Yaniv Taigman
- **Comment**: None
- **Journal**: None
- **Summary**: We are given a video of a person performing a certain activity, from which we extract a controllable model. The model generates novel image sequences of that person, according to arbitrary user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person.   The method is based on two networks. The first network maps a current pose, and a single-instance control signal to the next pose. The second network maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.



### Process of image super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.08396v8
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08396v8)
- **Published**: 2019-04-17 17:52:52+00:00
- **Updated**: 2021-08-16 06:07:46+00:00
- **Authors**: Sebastien Lablanche, Gerard Lablanche
- **Comment**: This article has been removed by arXiv administrators as part of
  arXiv's Code of Conduct Enforcement
- **Journal**: None
- **Summary**: In this paper we explain a process of super-resolution reconstruction allowing to increase the resolution of an image.The need for high-resolution digital images exists in diverse domains, for example the medical and spatial domains. The obtaining of high-resolution digital images can be made at the time of the shooting, but it is often synonymic of important costs because of the necessary material to avoid such costs, it is known how to use methods of super-resolution reconstruction, consisting from one or several low resolution images to obtain a high-resolution image. The american patent US 9208537 describes such an algorithm. A zone of one low-resolution image is isolated and categorized according to the information contained in pixels forming the borders of the zone. The category of it zone determines the type of interpolation used to add pixels in aforementioned zone, to increase the neatness of the images. It is also known how to reconstruct a low-resolution image there high-resolution image by using a model of super-resolution reconstruction whose learning is based on networks of neurons and on image or a picture library. The demand of chinese patent CN 107563965 and the scientist publication "Pixel Recursive Super Resolution", R. Dahl, M. Norouzi, J. Shlens propose such methods. The aim of this paper is to demonstrate that it is possible to reconstruct coherent human faces from very degraded pixelated images with a very fast algorithm, more faster than compressed sensing (CS), easier to compute and without deep learning, so without important technology resources, i.e. a large database of thousands training images (see arXiv:2003.13063).   This technological breakthrough has been patented in 2018 with the demand of French patent FR 1855485 (https://patents.google.com/patent/FR3082980A1, see the HAL reference https://hal.archives-ouvertes.fr/hal-01875898v1).



### Event-based Vision: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1904.08405v3
- **DOI**: 10.1109/TPAMI.2020.3008413
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08405v3)
- **Published**: 2019-04-17 17:59:34+00:00
- **Updated**: 2020-08-08 10:55:56+00:00
- **Authors**: Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Joerg Conradt, Kostas Daniilidis, Davide Scaramuzza
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  2020
- **Summary**: Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.



### Defensive Quantization: When Efficiency Meets Robustness
- **Arxiv ID**: http://arxiv.org/abs/1904.08444v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08444v1)
- **Published**: 2019-04-17 18:23:24+00:00
- **Updated**: 2019-04-17 18:23:24+00:00
- **Authors**: Ji Lin, Chuang Gan, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.



### Online Adaptation through Meta-Learning for Stereo Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.08462v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08462v1)
- **Published**: 2019-04-17 19:24:15+00:00
- **Updated**: 2019-04-17 19:24:15+00:00
- **Authors**: Zhenyu Zhang, Stéphane Lathuilière, Andrea Pilzer, Nicu Sebe, Elisa Ricci, Jian Yang
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: In this work, we tackle the problem of online adaptation for stereo depth estimation, that consists in continuously adapting a deep network to a target video recordedin an environment different from that of the source training set. To address this problem, we propose a novel Online Meta-Learning model with Adaption (OMLA). Our proposal is based on two main contributions. First, to reducethe domain-shift between source and target feature distributions we introduce an online feature alignment procedurederived from Batch Normalization. Second, we devise a meta-learning approach that exploits feature alignment forfaster convergence in an online learning setting. Additionally, we propose a meta-pre-training algorithm in order toobtain initial network weights on the source dataset whichfacilitate adaptation on future data streams. Experimentally, we show that both OMLA and meta-pre-training helpthe model to adapt faster to a new environment. Our proposal is evaluated on the wellestablished KITTI dataset,where we show that our online method is competitive withstate of the art algorithms trained in a batch setting.



### DeepAtlas: Joint Semi-Supervised Learning of Image Registration and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08465v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08465v2)
- **Published**: 2019-04-17 19:33:37+00:00
- **Updated**: 2019-07-26 03:25:03+00:00
- **Authors**: Zhenlin Xu, Marc Niethammer
- **Comment**: To appear in MICCAI 2019
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are state-of-the-art for semantic image segmentation, but typically require many labeled training samples. Obtaining 3D segmentations of medical images for supervised training is difficult and labor intensive. Motivated by classical approaches for joint segmentation and registration we therefore propose a deep learning framework that jointly learns networks for image registration and image segmentation. In contrast to previous work on deep unsupervised image registration, which showed the benefit of weak supervision via image segmentations, our approach can use existing segmentations when available and computes them via the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training benefits from the registration, which essentially provides a realistic form of data augmentation. Experiments on knee and brain 3D magnetic resonance (MR) images show that our approach achieves large simultaneous improvements of segmentation and registration accuracy (over independently trained networks) and allows training high-quality models with very limited training data. Specifically, in a one-shot-scenario (with only one manually labeled image) our approach increases Dice scores (%) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images respectively.



### Image Resizing by Reconstruction from Deep Features
- **Arxiv ID**: http://arxiv.org/abs/1904.08475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08475v2)
- **Published**: 2019-04-17 19:56:23+00:00
- **Updated**: 2021-06-22 04:30:28+00:00
- **Authors**: Moab Arar, Dov Danon, Daniel Cohen-Or, Ariel Shamir
- **Comment**: 13 pages, 21 figures
- **Journal**: None
- **Summary**: Traditional image resizing methods usually work in pixel space and use various saliency measures. The challenge is to adjust the image shape while trying to preserve important content. In this paper we perform image resizing in feature space where the deep layers of a neural network contain rich important semantic information. We directly adjust the image feature maps, extracted from a pre-trained classification network, and reconstruct the resized image using a neural-network based optimization. This novel approach leverages the hierarchical encoding of the network, and in particular, the high-level discriminative power of its deeper layers, that recognizes semantic objects and regions and allows maintaining their aspect ratio. Our use of reconstruction from deep features diminishes the artifacts introduced by image-space resizing operators. We evaluate our method on benchmarks, compare to alternative approaches, and demonstrate its strength on challenging images.



### An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.08479v6
- **DOI**: 10.1007/978-3-030-58517-4_24
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08479v6)
- **Published**: 2019-04-17 20:02:24+00:00
- **Updated**: 2020-07-17 09:31:15+00:00
- **Authors**: Yaoyao Liu, Bernt Schiele, Qianru Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning aims to train efficient predictive models with a few examples. The lack of training data leads to poor models that perform high-variance or low-confidence predictions. In this paper, we propose to meta-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieve robust predictions. "Epoch-wise" means that each training epoch has a Bayes model whose parameters are specifically learned and deployed. "Empirical" means that the hyperparameters, e.g., used for learning and ensembling the epoch-wise models, are generated by hyperprior learners conditional on task-specific data. We introduce four kinds of hyperprior learners by considering inductive vs. transductive, and epoch-dependent vs. epoch-independent, in the paradigm of meta-learning. We conduct extensive experiments for five-class few-shot tasks on three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, and achieve top performance using the epoch-dependent transductive hyperprior learner, which captures the richest information. Our ablation study shows that both "epoch-wise ensemble" and "empirical" encourage high efficiency and robustness in the model performance.



### Variational Prototyping-Encoder: One-Shot Learning with Prototypical Images
- **Arxiv ID**: http://arxiv.org/abs/1904.08482v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08482v1)
- **Published**: 2019-04-17 20:26:09+00:00
- **Updated**: 2019-04-17 20:26:09+00:00
- **Authors**: Junsik Kim, Tae-Hyun Oh, Seokju Lee, Fei Pan, In So Kweon
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: In daily life, graphic symbols, such as traffic signs and brand logos, are ubiquitously utilized around us due to its intuitive expression beyond language boundary. We tackle an open-set graphic symbol recognition problem by one-shot classification with prototypical images as a single training example for each novel class. We take an approach to learn a generalizable embedding space for novel tasks. We propose a new approach called variational prototyping-encoder (VPE) that learns the image translation task from real-world input images to their corresponding prototypical images as a meta-task. As a result, VPE learns image similarity as well as prototypical concepts which differs from widely used metric learning based approaches. Our experiments with diverse datasets demonstrate that the proposed VPE performs favorably against competing metric learning based one-shot methods. Also, our qualitative analyses show that our meta-task induces an effective embedding space suitable for unseen data representation.



### Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers
- **Arxiv ID**: http://arxiv.org/abs/1904.08489v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08489v2)
- **Published**: 2019-04-17 20:39:17+00:00
- **Updated**: 2019-08-15 19:56:17+00:00
- **Authors**: Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde
- **Comment**: Accepted to International Conference on Computer Vision, (ICCV) 2019
- **Journal**: None
- **Summary**: Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such `semantic' adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.



### ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.08516v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08516v1)
- **Published**: 2019-04-17 21:52:20+00:00
- **Updated**: 2019-04-17 21:52:20+00:00
- **Authors**: Guanxiong Liu, Issa Khalil, Abdallah Khreishah
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Network classifiers have been used successfully in a wide range of applications. However, their underlying assumption of attack free environment has been defied by adversarial examples. Researchers tried to develop defenses; however, existing approaches are still far from providing effective solutions to this evolving problem. In this paper, we design a generative adversarial net (GAN) based zero knowledge adversarial training defense, dubbed ZK-GanDef, which does not consume adversarial examples during training. Therefore, ZK-GanDef is not only efficient in training but also adaptive to new adversarial examples. This advantage comes at the cost of small degradation in test accuracy compared to full knowledge approaches. Our experiments show that ZK-GanDef enhances test accuracy on adversarial examples by up-to 49.17% compared to zero knowledge approaches. More importantly, its test accuracy is close to that of the state-of-the-art full knowledge approaches (maximum degradation of 8.46%), while taking much less training time.



### Graph based Dynamic Segmentation of Generic Objects in 3D
- **Arxiv ID**: http://arxiv.org/abs/1904.08518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08518v1)
- **Published**: 2019-04-17 22:01:59+00:00
- **Updated**: 2019-04-17 22:01:59+00:00
- **Authors**: Xiao Lin, Josep R. Casas, Montse Pardàs
- **Comment**: CVPR 2016 workshops - SUNw: Scene Understanding Workshop 2016 See
  workshop listing in http://cvpr2016.thecvf.com/program/workshops and paper
  listing in http://sunw.csail.mit.edu/2016/posters.html)
- **Journal**: None
- **Summary**: We propose a novel 3D segmentation method for RBGD stream data to deal with 3D object segmentation task in a generic scenario with frequent object interactions. It mainly contributes in two aspects, while being generic and not requiring initialization: firstly, a novel tree structure representation for the point cloud of the scene is proposed. Then, a dynamic manangement mechanism for connected component splits and merges exploits the tree structure representation.



### Do Lateral Views Help Automated Chest X-ray Predictions?
- **Arxiv ID**: http://arxiv.org/abs/1904.08534v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08534v2)
- **Published**: 2019-04-17 23:26:21+00:00
- **Updated**: 2019-07-25 12:56:18+00:00
- **Authors**: Hadrien Bertrand, Mohammad Hashir, Joseph Paul Cohen
- **Comment**: 3 pages and 1 figure. Under review as extended abstract at MIDL 2019
  [arXiv:1907.08612]
- **Journal**: None
- **Summary**: Most convolutional neural networks in chest radiology use only the frontal posteroanterior (PA) view to make a prediction. However the lateral view is known to help the diagnosis of certain diseases and conditions. The recently released PadChest dataset contains paired PA and lateral views, allowing us to study for which diseases and conditions the performance of a neural network improves when provided a lateral x-ray view as opposed to a frontal posteroanterior (PA) view. Using a simple DenseNet model, we find that using the lateral view increases the AUC of 8 of the 56 labels in our data and achieves the same performance as the PA view for 21 of the labels. We find that using the PA and lateral views jointly doesn't trivially lead to an increase in performance but suggest further investigation.



### Material Segmentation of Multi-View Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1904.08537v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08537v1)
- **Published**: 2019-04-17 23:52:20+00:00
- **Updated**: 2019-04-17 23:52:20+00:00
- **Authors**: Matthew Purri, Jia Xue, Kristin Dana, Matthew Leotta, Dan Lipsa, Zhixin Li, Bo Xu, Jie Shan
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Material recognition methods use image context and local cues for pixel-wise classification. In many cases only a single image is available to make a material prediction. Image sequences, routinely acquired in applications such as mutliview stereo, can provide a sampling of the underlying reflectance functions that reveal pixel-level material attributes. We investigate multi-view material segmentation using two datasets generated for building material segmentation and scene material segmentation from the SpaceNet Challenge satellite image dataset. In this paper, we explore the impact of multi-angle reflectance information by introducing the \textit{reflectance residual encoding}, which captures both the multi-angle and multispectral information present in our datasets. The residuals are computed by differencing the sparse-sampled reflectance function with a dictionary of pre-defined dense-sampled reflectance functions. Our proposed reflectance residual features improves material segmentation performance when integrated into pixel-wise and semantic segmentation architectures. At test time, predictions from individual segmentations are combined through softmax fusion and refined by building segment voting. We demonstrate robust and accurate pixelwise segmentation results using the proposed material segmentation pipeline.



