# Arxiv Papers in cs.CV on 2019-04-04
### Gated-GAN: Adversarial Gated Networks for Multi-Collection Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1904.02296v1
- **DOI**: 10.1109/TIP.2018.2869695
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02296v1)
- **Published**: 2019-04-04 01:20:52+00:00
- **Updated**: 2019-04-04 01:20:52+00:00
- **Authors**: Xinyuan Chen, Chang Xu, Xiaokang Yang, Li Song, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer describes the rendering of an image semantic content as different artistic styles. Recently, generative adversarial networks (GANs) have emerged as an effective approach in style transfer by adversarially training the generator to synthesize convincing counterfeits. However, traditional GAN suffers from the mode collapse issue, resulting in unstable training and making style transfer quality difficult to guarantee. In addition, the GAN generator is only compatible with one style, so a series of GANs must be trained to provide users with choices to transfer more than one kind of style. In this paper, we focus on tackling these challenges and limitations to improve style transfer. We propose adversarial gated networks (Gated GAN) to transfer multiple styles in a single model. The generative networks have three modules: an encoder, a gated transformer, and a decoder. Different styles can be achieved by passing input images through different branches of the gated transformer. To stabilize training, the encoder and decoder are combined as an autoencoder to reconstruct the input images. The discriminative networks are used to distinguish whether the input image is a stylized or genuine image. An auxiliary classifier is used to recognize the style categories of transferred images, thereby helping the generative networks generate images in multiple styles. In addition, Gated GAN makes it possible to explore a new style by investigating styles learned from artists or genres. Our extensive experiments demonstrate the stability and effectiveness of the proposed model for multistyle transfer.



### Cost-Sensitive Feature Selection by Optimizing F-Measures
- **Arxiv ID**: http://arxiv.org/abs/1904.02301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02301v1)
- **Published**: 2019-04-04 01:36:04+00:00
- **Updated**: 2019-04-04 01:36:04+00:00
- **Authors**: Meng Liu, Chang Xu, Yong Luo, Chao Xu, Yonggang Wen, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Feature selection is beneficial for improving the performance of general machine learning tasks by extracting an informative subset from the high-dimensional features. Conventional feature selection methods usually ignore the class imbalance problem, thus the selected features will be biased towards the majority class. Considering that F-measure is a more reasonable performance measure than accuracy for imbalanced data, this paper presents an effective feature selection algorithm that explores the class imbalance issue by optimizing F-measures. Since F-measure optimization can be decomposed into a series of cost-sensitive classification problems, we investigate the cost-sensitive feature selection by generating and assigning different costs to each class with rigorous theory guidance. After solving a series of cost-sensitive feature selection problems, features corresponding to the best F-measure will be selected. In this way, the selected features will fully represent the properties of all classes. Experimental results on popular benchmarks and challenging real-world data sets demonstrate the significance of cost-sensitive feature selection for the imbalanced data setting and validate the effectiveness of the proposed method.



### A Training-free, One-shot Detection Framework For Geospatial Objects In Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1904.02302v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02302v1)
- **Published**: 2019-04-04 01:36:51+00:00
- **Updated**: 2019-04-04 01:36:51+00:00
- **Authors**: Tengfei Zhang, Yue Zhang, Xian Sun, Menglong Yan, Yaoling Wang, Kun Fu
- **Comment**: 5 pages 4 figures
- **Journal**: None
- **Summary**: Deep learning based object detection has achieved great success. However, these supervised learning methods are data-hungry and time-consuming. This restriction makes them unsuitable for limited data and urgent tasks, especially in the applications of remote sensing. Inspired by the ability of humans to quickly learn new visual concepts from very few examples, we propose a training-free, one-shot geospatial object detection framework for remote sensing images. It consists of (1) a feature extractor with remote sensing domain knowledge, (2) a multi-level feature fusion method, (3) a novel similarity metric method, and (4) a 2-stage object detection pipeline. Experiments on sewage treatment plant and airport detections show that proposed method has achieved a certain effect. Our method can serve as a baseline for training-free, one-shot geospatial object detection.



### Improved Inference via Deep Input Transfer
- **Arxiv ID**: http://arxiv.org/abs/1904.02307v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02307v4)
- **Published**: 2019-04-04 02:04:13+00:00
- **Updated**: 2019-07-10 05:40:15+00:00
- **Authors**: Saied Asgari Taghanaki, Kumar Abhishek, Ghassan Hamarneh
- **Comment**: Accepted to MICCAI 2019
- **Journal**: MICCAI 2019
- **Summary**: Although numerous improvements have been made in the field of image segmentation using convolutional neural networks, the majority of these improvements rely on training with larger datasets, model architecture modifications, novel loss functions, and better optimizers. In this paper, we propose a new segmentation performance boosting paradigm that relies on optimally modifying the network's input instead of the network itself. In particular, we leverage the gradients of a trained segmentation network with respect to the input to transfer it to a space where the segmentation accuracy improves. We test the proposed method on three publicly available medical image segmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, the Shenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our method achieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores, respectively.



### Comparison Network for One-Shot Conditional Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.02317v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02317v2)
- **Published**: 2019-04-04 02:31:55+00:00
- **Updated**: 2020-01-20 07:10:17+00:00
- **Authors**: Tengfei Zhang, Yue Zhang, Xian Sun, Hao Sun, Menglong Yan, Xue Yang, Kun Fu
- **Comment**: The paper is under revision now. Some problem are not well described.
  However, this paper has spread out. I think the impact of an imperfect first
  draft is not good, so we want to withdraw and revise
- **Journal**: None
- **Summary**: The current advances in object detection depend on large-scale datasets to get good performance. However, there may not always be sufficient samples in many scenarios, which leads to the research on few-shot detection as well as its extreme variation one-shot detection. In this paper, the one-shot detection has been formulated as a conditional probability problem. With this insight, a novel one-shot conditional object detection (OSCD) framework, referred as Comparison Network (ComparisonNet), has been proposed. Specifically, query and target image features are extracted through a Siamese network as mapped metrics of marginal probabilities. A two-stage detector for OSCD is introduced to compare the extracted query and target features with the learnable metric to approach the optimized non-linear conditional probability. Once trained, ComparisonNet can detect objects of both seen and unseen classes without further training, which also has the advantages including class-agnostic, training-free for unseen classes, and without catastrophic forgetting. Experiments show that the proposed approach achieves state-of-the-art performance on the proposed datasets of Fashion-MNIST and PASCAL VOC.



### Towards a Robust Aerial Cinematography Platform: Localizing and Tracking Moving Targets in Unstructured Environments
- **Arxiv ID**: http://arxiv.org/abs/1904.02319v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02319v2)
- **Published**: 2019-04-04 02:37:05+00:00
- **Updated**: 2019-07-29 03:29:55+00:00
- **Authors**: Rogerio Bonatti, Cherie Ho, Wenshan Wang, Sanjiban Choudhury, Sebastian Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: The use of drones for aerial cinematography has revolutionized several applications and industries that require live and dynamic camera viewpoints such as entertainment, sports, and security. However, safely controlling a drone while filming a moving target usually requires multiple expert human operators; hence the need for an autonomous cinematographer. Current approaches have severe real-life limitations such as requiring fully scripted scenes, high-precision motion-capture systems or GPS tags to localize targets, and prior maps of the environment to avoid obstacles and plan for occlusion.   In this work, we overcome such limitations and propose a complete system for aerial cinematography that combines: (1) a vision-based algorithm for target localization; (2) a real-time incremental 3D signed-distance map algorithm for occlusion and safety computation; and (3) a real-time camera motion planner that optimizes smoothness, collisions, occlusions and artistic guidelines. We evaluate robustness and real-time performance in series of field experiments and simulations by tracking dynamic targets moving through unknown, unstructured environments. Finally, we verify that despite removing previous limitations, our system achieves state-of-the-art performance. Videos of the system in action can be seen at https://youtu.be/ZE9MnCVmumc



### Modified Distribution Alignment for Domain Adaptation with Pre-trained Inception ResNet
- **Arxiv ID**: http://arxiv.org/abs/1904.02322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02322v2)
- **Published**: 2019-04-04 03:00:24+00:00
- **Updated**: 2019-04-18 15:04:36+00:00
- **Authors**: Youshan Zhang, Brian D. Davison
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have been widely used in computer vision. There are several well trained deep neural networks for the ImageNet classification challenge, which has played a significant role in image recognition. However, little work has explored pre-trained neural networks for image recognition in domain adaption. In this paper, we are the first to extract better-represented features from a pre-trained Inception ResNet model for domain adaptation. We then present a modified distribution alignment method for classification using the extracted features. We test our model using three benchmark datasets (Office+Caltech-10, Office-31, and Office-Home). Extensive experiments demonstrate significant improvements (4.8%, 5.5%, and 10%) in classification accuracy over the state-of-the-art.



### Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations
- **Arxiv ID**: http://arxiv.org/abs/1904.02323v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02323v3)
- **Published**: 2019-04-04 03:00:40+00:00
- **Updated**: 2019-09-02 19:42:14+00:00
- **Authors**: Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau
- **Comment**: Published in IEEE Transactions on Visualization and Computer Graphics
  2020, and presented at IEEE VAST 2019
- **Journal**: None
- **Summary**: Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.



### Feature Pyramid Hashing
- **Arxiv ID**: http://arxiv.org/abs/1904.02325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02325v1)
- **Published**: 2019-04-04 03:05:39+00:00
- **Updated**: 2019-04-04 03:05:39+00:00
- **Authors**: Yifan Yang, Libing Geng, Hanjiang Lai, Yan Pan, Jian Yin
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep-networks-based hashing has become a leading approach for large-scale image retrieval. Most deep hashing approaches use the high layer to extract the powerful semantic representations. However, these methods have limited ability for fine-grained image retrieval because the semantic features extracted from the high layer are difficult in capturing the subtle differences. To this end, we propose a novel two-pyramid hashing architecture to learn both the semantic information and the subtle appearance details for fine-grained image search. Inspired by the feature pyramids of convolutional neural network, a vertical pyramid is proposed to capture the high-layer features and a horizontal pyramid combines multiple low-layer features with structural information to capture the subtle differences. To fuse the low-level features, a novel combination strategy, called consensus fusion, is proposed to capture all subtle information from several low-layers for finer retrieval. Extensive evaluation on two fine-grained datasets CUB-200-2011 and Stanford Dogs demonstrate that the proposed method achieves significant performance compared with the state-of-art baselines.



### Multi-View Intact Space Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.02340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02340v1)
- **Published**: 2019-04-04 04:04:57+00:00
- **Updated**: 2019-04-04 04:04:57+00:00
- **Authors**: Chang Xu, Dacheng Tao, Chao Xu
- **Comment**: None
- **Journal**: None
- **Summary**: It is practical to assume that an individual view is unlikely to be sufficient for effective multi-view learning. Therefore, integration of multi-view information is both valuable and necessary. In this paper, we propose the Multi-view Intact Space Learning (MISL) algorithm, which integrates the encoded complementary information in multiple views to discover a latent intact representation of the data. Even though each view on its own is insufficient, we show theoretically that by combing multiple views we can obtain abundant information for latent intact space learning. Employing the Cauchy loss (a technique used in statistical learning) as the error measurement strengthens robustness to outliers. We propose a new definition of multi-view stability and then derive the generalization error bound based on multi-view stability and Rademacher complexity, and show that the complementarity between multiple views is beneficial for the stability and generalization. MISL is efficiently optimized using a novel Iteratively Reweight Residuals (IRR) technique, whose convergence is theoretically analyzed. Experiments on synthetic data and real-world datasets demonstrate that MISL is an effective and promising algorithm for practical applications.



### MMED: A Multi-domain and Multi-modality Event Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.02354v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1904.02354v2)
- **Published**: 2019-04-04 05:27:10+00:00
- **Updated**: 2019-04-09 12:05:49+00:00
- **Authors**: Zhenguo Yang, Zehang Lin, Min Cheng, Qing Li, Wenyin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we construct and release a multi-domain and multi-modality event dataset (MMED), containing 25,165 textual news articles collected from hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and 76,516 image posts shared on Flickr social media, which are annotated according to 412 real-world events. The dataset is collected to explore the problem of organizing heterogeneous data contributed by professionals and amateurs in different data domains, and the problem of transferring event knowledge obtained from one data domain to heterogeneous data domain, thus summarizing the data with different contributors. We hope that the release of the MMED dataset can stimulate innovate research on related challenging problems, such as event discovery, cross-modal (event) retrieval, and visual question answering, etc.



### Deep Back-Projection Networks for Single Image Super-resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.05677v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05677v2)
- **Published**: 2019-04-04 05:32:53+00:00
- **Updated**: 2020-06-13 03:56:55+00:00
- **Authors**: Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita
- **Comment**: To appear in TPAMI 2020. The code is available at
  https://github.com/alterzero/DBPN-Pytorch arXiv admin note: substantial text
  overlap with arXiv:1803.02735
- **Journal**: None
- **Summary**: Previous feed-forward architectures of recently proposed deep super-resolution networks learn the features of low-resolution inputs and the non-linear mapping from those to a high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), the winner of two image super-resolution challenges (NTIRE2018 and PIRM2018), that exploit iterative up- and down-sampling layers. These layers are formed as a unit providing an error feedback mechanism for projection errors. We construct mutually-connected up- and down-sampling units each of which represents different types of low- and high-resolution components. We also show that extending this idea to demonstrate a new insight towards more efficient network design substantially, such as parameter sharing on the projection module and transition layer on projection step. The experimental results yield superior results and in particular establishing new state-of-the-art results across multiple data sets, especially for large scaling factors such as 8x.



### Lightweight Image Super-Resolution with Adaptive Weighted Learning Network
- **Arxiv ID**: http://arxiv.org/abs/1904.02358v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1904.02358v1)
- **Published**: 2019-04-04 05:44:32+00:00
- **Updated**: 2019-04-04 05:44:32+00:00
- **Authors**: Chaofeng Wang, Zheng Li, Jun Shi
- **Comment**: 9 pages, 6 figures
- **Journal**: None
- **Summary**: Deep learning has been successfully applied to the single-image super-resolution (SISR) task with great performance in recent years. However, most convolutional neural network based SR models require heavy computation, which limit their real-world applications. In this work, a lightweight SR network, named Adaptive Weighted Super-Resolution Network (AWSRN), is proposed for SISR to address this issue. A novel local fusion block (LFB) is designed in AWSRN for efficient residual learning, which consists of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Moreover, an adaptive weighted multi-scale (AWMS) module is proposed to make full use of features in reconstruction layer. AWMS consists of several different scale convolutions, and the redundancy scale branch can be removed according to the contribution of adaptive weights in AWMS for lightweight network. The experimental results on the commonly used datasets show that the proposed lightweight AWSRN achieves superior performance on x2, x3, x4, and x8 scale factors to state-of-the-art methods with similar parameters and computational overhead. Code is avaliable at: https://github.com/ChaofWang/AWSRN



### A Robust Learning Approach to Domain Adaptive Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.02361v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02361v3)
- **Published**: 2019-04-04 05:50:10+00:00
- **Updated**: 2019-11-18 05:43:00+00:00
- **Authors**: Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, William G. Macready
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.



### Spatiotemporal CNN for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.02363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02363v1)
- **Published**: 2019-04-04 05:53:15+00:00
- **Updated**: 2019-04-04 05:53:15+00:00
- **Authors**: Kai Xu, Longyin Wen, Guorong Li, Liefeng Bo, Qingming Huang
- **Comment**: 10 pages, 3 figures, 6 tables, CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present a unified, end-to-end trainable spatiotemporal CNN model for VOS, which consists of two branches, i.e., the temporal coherence branch and the spatial segmentation branch. Specifically, the temporal coherence branch pretrained in an adversarial fashion from unlabeled video data, is designed to capture the dynamic appearance and motion cues of video sequences to guide object segmentation. The spatial segmentation branch focuses on segmenting objects accurately based on the learned appearance and motion cues. To obtain accurate segmentation results, we design a coarse-to-fine process to sequentially apply a designed attention module on multi-scale feature maps, and concatenate them to produce the final prediction. In this way, the spatial segmentation branch is enforced to gradually concentrate on object regions. These two branches are jointly fine-tuned on video segmentation sequences in an end-to-end manner. Several experiments are carried out on three challenging datasets (i.e., DAVIS-2016, DAVIS-2017 and Youtube-Object) to show that our method achieves favorable performance against the state-of-the-arts. Code is available at https://github.com/longyin880815/STCNN.



### Template-Based Automatic Search of Compact Semantic Segmentation Architectures
- **Arxiv ID**: http://arxiv.org/abs/1904.02365v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02365v2)
- **Published**: 2019-04-04 06:06:32+00:00
- **Updated**: 2020-05-22 03:25:48+00:00
- **Authors**: Vladimir Nekrasov, Chunhua Shen, Ian Reid
- **Comment**: Updated runtime numbers on CityScapes. WACV 2020
- **Journal**: None
- **Summary**: Automatic search of neural architectures for various vision and natural language tasks is becoming a prominent tool as it allows to discover high-performing structures on any dataset of interest. Nevertheless, on more difficult domains, such as dense per-pixel classification, current automatic approaches are limited in their scope - due to their strong reliance on existing image classifiers they tend to search only for a handful of additional layers with discovered architectures still containing a large number of parameters. In contrast, in this work we propose a novel solution able to find light-weight and accurate segmentation architectures starting from only few blocks of a pre-trained classification network. To this end, we progressively build up a methodology that relies on templates of sets of operations, predicts which template and how many times should be applied at each step, while also generating the connectivity structure and downsampling factors. All these decisions are being made by a recurrent neural network that is rewarded based on the score of the emitted architecture on the holdout set and trained using reinforcement learning. One discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models and the search code are available at https://github.com/DrSleep/nas-segm-pytorch.



### Architecture Search of Dynamic Cells for Semantic Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.02371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02371v1)
- **Published**: 2019-04-04 06:32:30+00:00
- **Updated**: 2019-04-04 06:32:30+00:00
- **Authors**: Vladimir Nekrasov, Hao Chen, Chunhua Shen, Ian Reid
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: In semantic video segmentation the goal is to acquire consistent dense semantic labelling across image frames. To this end, recent approaches have been reliant on manually arranged operations applied on top of static semantic segmentation networks - with the most prominent building block being the optical flow able to provide information about scene dynamics. Related to that is the line of research concerned with speeding up static networks by approximating expensive parts of them with cheaper alternatives, while propagating information from previous frames. In this work we attempt to come up with generalisation of those methods, and instead of manually designing contextual blocks that connect per-frame outputs, we propose a neural architecture search solution, where the choice of operations together with their sequential arrangement are being predicted by a separate neural network. We showcase that such generalisation leads to stable and accurate results across common benchmarks, such as CityScapes and CamVid datasets. Importantly, the proposed methodology takes only 2 GPU-days, finds high-performing cells and does not rely on the expensive optical flow computation.



### ConvPoint: Continuous Convolutions for Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/1904.02375v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02375v5)
- **Published**: 2019-04-04 06:51:56+00:00
- **Updated**: 2020-02-19 10:49:28+00:00
- **Authors**: Alexandre Boulch
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: Point clouds are unstructured and unordered data, as opposed to images. Thus, most machine learning approach developed for image cannot be directly transferred to point clouds. In this paper, we propose a generalization of discrete convolutional neural networks (CNNs) in order to deal with point clouds by replacing discrete kernels by continuous ones. This formulation is simple, allows arbitrary point cloud sizes and can easily be used for designing neural networks similarly to 2D CNNs. We present experimental results with various architectures, highlighting the flexibility of the proposed approach. We obtain competitive results compared to the state-of-the-art on shape classification, part segmentation and semantic segmentation for large-scale point clouds.



### Inferring Dynamic Representations of Facial Actions from a Still Image
- **Arxiv ID**: http://arxiv.org/abs/1904.02382v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1904.02382v1)
- **Published**: 2019-04-04 07:15:53+00:00
- **Updated**: 2019-04-04 07:15:53+00:00
- **Authors**: Siyang Song, Enrique Sánchez-Lozano, Linlin Shen, Alan Johnston, Michel Valstar
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Facial actions are spatio-temporal signals by nature, and therefore their modeling is crucially dependent on the availability of temporal information. In this paper, we focus on inferring such temporal dynamics of facial actions when no explicit temporal information is available, i.e. from still images. We present a novel approach to capture multiple scales of such temporal dynamics, with an application to facial Action Unit (AU) intensity estimation and dimensional affect estimation. In particular, 1) we propose a framework that infers a dynamic representation (DR) from a still image, which captures the bi-directional flow of time within a short time-window centered at the input image; 2) we show that we can train our method without the need of explicitly generating target representations, allowing the network to represent dynamics more broadly; and 3) we propose to apply a multiple temporal scale approach that infers DRs for different window lengths (MDR) from a still image. We empirically validate the value of our approach on the task of frame ranking, and show how our proposed MDR attains state of the art results on BP4D for AU intensity estimation and on SEMAINE for dimensional affect estimation, using only still images at test time.



### A Hybrid Approach with Optimization and Metric-based Meta-Learner for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.03014v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03014v2)
- **Published**: 2019-04-04 07:31:34+00:00
- **Updated**: 2019-08-27 01:35:22+00:00
- **Authors**: Duo Wang, Yu Cheng, Mo Yu, Xiaoxiao Guo, Tao Zhang
- **Comment**: Accepted to Neurocomputing journal, code will be released soon. arXiv
  admin note: text overlap with arXiv:1901.09890
- **Journal**: None
- **Summary**: Few-shot learning aims to learn classifiers for new classes with only a few training examples per class. Most existing few-shot learning approaches belong to either metric-based meta-learning or optimization-based meta-learning category, both of which have achieved successes in the simplified "$k$-shot $N$-way" image classification settings. Specifically, the optimization-based approaches train a meta-learner to predict the parameters of the task-specific classifiers. The task-specific classifiers are required to be homogeneous-structured to ease the parameter prediction, so the meta-learning approaches could only handle few-shot learning problems where the tasks share a uniform number of classes. The metric-based approaches learn one task-invariant metric for all the tasks. Even though the metric-learning approaches allow different numbers of classes, they require the tasks all coming from a similar domain such that there exists a uniform metric that could work across tasks. In this work, we propose a hybrid meta-learning model called Meta-Metric-Learner which combines the merits of both optimization- and metric-based approaches. Our meta-metric-learning approach consists of two components, a task-specific metric-based learner as a base model, and a meta-learner that learns and specifies the base model. Thus our model is able to handle flexible numbers of classes as well as generate more generalized metrics for classification across tasks. We test our approach in the standard "$k$-shot $N$-way" few-shot learning setting following previous works and a new realistic few-shot setting with flexible class numbers in both single-source form and multi-source forms. Experiments show that our approach can obtain superior performance in all settings.



### Accurate and Fast reconstruction of Porous Media from Extremely Limited Information Using Conditional Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1905.02135v1
- **DOI**: 10.1103/PhysRevE.100.033308
- **Categories**: **eess.IV**, cs.CE, cs.CV, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1905.02135v1)
- **Published**: 2019-04-04 09:08:28+00:00
- **Updated**: 2019-04-04 09:08:28+00:00
- **Authors**: Junxi Feng, Xiaohai He, Qizhi Teng, Chao Ren, Honggang Chen, Yang Li
- **Comment**: None
- **Journal**: Phys. Rev. E 100, 033308 (2019)
- **Summary**: Porous media are ubiquitous in both nature and engineering applications, thus their modelling and understanding is of vital importance. In contrast to direct acquisition of three-dimensional (3D) images of such medium, obtaining its sub-region (s) like two-dimensional (2D) images or several small areas could be much feasible. Therefore, reconstructing whole images from the limited information is a primary technique in such cases. Specially, in practice the given data cannot generally be determined by users and may be incomplete or partially informed, thus making existing reconstruction methods inaccurate or even ineffective. To overcome this shortcoming, in this study we proposed a deep learning-based framework for reconstructing full image from its much smaller sub-area(s). Particularly, conditional generative adversarial network (CGAN) is utilized to learn the mapping between input (partial image) and output (full image). To preserve the reconstruction accuracy, two simple but effective objective functions are proposed and then coupled with the other two functions to jointly constrain the training procedure. Due to the inherent essence of this ill-posed problem, a Gaussian noise is introduced for producing reconstruction diversity, thus allowing for providing multiple candidate outputs. Extensively tested on a variety of porous materials and demonstrated by both visual inspection and quantitative comparison, the method is shown to be accurate, stable yet fast ($\sim0.08s$ for a $128 \times 128$ image reconstruction). We highlight that the proposed approach can be readily extended, such as incorporating any user-define conditional data and an arbitrary number of object functions into reconstruction, and being coupled with other reconstruction methods.



### Transfer Learning with Sparse Associative Memories
- **Arxiv ID**: http://arxiv.org/abs/1904.02420v3
- **DOI**: 10.1007/978-3-030-30487-4_39
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02420v3)
- **Published**: 2019-04-04 09:16:30+00:00
- **Updated**: 2019-09-19 12:30:09+00:00
- **Authors**: Quentin Jodelet, Vincent Gripon, Masafumi Hagiwara
- **Comment**: Presented at the 28th International Conference on Artificial Neural
  Networks (ICANN 2019)
- **Journal**: Artificial Neural Networks and Machine Learning - ICANN 2019:
  Theoretical Neural Computation. ICANN 2019. Lecture Notes in Computer
  Science, vol 11727. Springer, Cham
- **Summary**: In this paper, we introduce a novel layer designed to be used as the output of pre-trained neural networks in the context of classification. Based on Associative Memories, this layer can help design Deep Neural Networks which support incremental learning and that can be (partially) trained in real time on embedded devices. Experiments on the ImageNet dataset and other different domain specific datasets show that it is possible to design more flexible and faster-to-train Neural Networks at the cost of a slight decrease in accuracy.



### Resource Efficient 3D Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.02422v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02422v5)
- **Published**: 2019-04-04 09:19:19+00:00
- **Updated**: 2021-10-18 09:43:53+00:00
- **Authors**: Okan Köpüklü, Neslihan Kose, Ahmet Gunduz, Gerhard Rigoll
- **Comment**: Accepted to ICCV 2019 workshop - Neural Architects
- **Journal**: None
- **Summary**: Recently, convolutional neural networks with 3D kernels (3D CNNs) have been very popular in computer vision community as a result of their superior ability of extracting spatio-temporal features within video frames compared to 2D CNNs. Although there has been great advances recently to build resource efficient 2D CNN architectures considering memory and power budget, there is hardly any similar resource efficient architectures for 3D CNNs. In this paper, we have converted various well-known resource efficient 2D CNNs to 3D CNNs and evaluated their performance on three major benchmarks in terms of classification accuracy for different complexity levels. We have experimented on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to inspect the applicability of transfer learning. We have evaluated the run-time performance of each model on a single Titan XP GPU and a Jetson TX2 embedded system. The results of this study show that these models can be utilized for different types of real-world applications since they provide real-time performance with considerable accuracies and memory usage. Our analysis on different complexity levels shows that the resource efficient 3D CNNs should not be designed too shallow or narrow in order to save complexity. The codes and pretrained models used in this work are publicly available.



### Active Transfer Learning Network: A Unified Deep Joint Spectral-Spatial Feature Learning Model For Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.02454v1
- **DOI**: 10.1109/TGRS.2018.2868851
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02454v1)
- **Published**: 2019-04-04 10:18:06+00:00
- **Updated**: 2019-04-04 10:18:06+00:00
- **Authors**: Cheng Deng, Yumeng Xue, Xianglong Liu, Chao Li, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has recently attracted significant attention in the field of hyperspectral images (HSIs) classification. However, the construction of an efficient deep neural network (DNN) mostly relies on a large number of labeled samples being available. To address this problem, this paper proposes a unified deep network, combined with active transfer learning that can be well-trained for HSIs classification using only minimally labeled training data. More specifically, deep joint spectral-spatial feature is first extracted through hierarchical stacked sparse autoencoder (SSAE) networks. Active transfer learning is then exploited to transfer the pre-trained SSAE network and the limited training samples from the source domain to the target domain, where the SSAE network is subsequently fine-tuned using the limited labeled samples selected from both source and target domain by corresponding active learning strategies. The advantages of our proposed method are threefold: 1) the network can be effectively trained using only limited labeled samples with the help of novel active learning strategies; 2) the network is flexible and scalable enough to function across various transfer situations, including cross-dataset and intra-image; 3) the learned deep joint spectral-spatial feature representation is more generic and robust than many joint spectral-spatial feature representation. Extensive comparative evaluations demonstrate that our proposed method significantly outperforms many state-of-the-art approaches, including both traditional and deep network-based methods, on three popular datasets.



### Unsupervised Learning of Eye Gaze Representation from the Web
- **Arxiv ID**: http://arxiv.org/abs/1904.02459v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02459v1)
- **Published**: 2019-04-04 10:25:13+00:00
- **Updated**: 2019-04-04 10:25:13+00:00
- **Authors**: Neeru Dubey, Shreya Ghosh, Abhinav Dhall
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network "Ize-Net" in self-supervised manner, we collect a large `in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently fine-tuned for any eye gaze estimation dataset.



### A new algorithm for shape matching and pattern recognition using dynamic programming
- **Arxiv ID**: http://arxiv.org/abs/1904.13219v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1904.13219v1)
- **Published**: 2019-04-04 11:27:30+00:00
- **Updated**: 2019-04-04 11:27:30+00:00
- **Authors**: Noreddine Gherabi, Bahaj Mohamed
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.08501
- **Journal**: None
- **Summary**: We propose a new method for shape recognition and retrieval based on dynamic programming. Our approach uses the dynamic programming algorithm to compute the optimal score and to find the optimal alignment between two strings. First, each contour of shape is represented by a set of points. After alignment and matching between two shapes, the contours are transformed into a string of symbols and numbers. Finally we find the best alignment of two complete strings and compute the optimal cost of similarity. In general, dynamic programming has two phases: the forward phase and the backward phase. In the forward phase, we compute the optimal cost for each subproblem. In the backward phase, we reconstruct the solution that gives the optimal cost. Our algorithm is tested in a database that contains various shapes such as MPEG-7.



### A new approach for measuring semantic similarity of ontology concepts using dynamic programming
- **Arxiv ID**: http://arxiv.org/abs/1904.08501v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08501v2)
- **Published**: 2019-04-04 11:37:53+00:00
- **Updated**: 2021-10-02 09:18:18+00:00
- **Authors**: Noreddine Gherabi, Abdelhadi Daoui, Abderrahim Marzouk
- **Comment**: None
- **Journal**: None
- **Summary**: Today, with the emergence of semantic web technologies and increasing of information quantity, searching for information based on the semantic web has become a fertile area of research. For this reason, a large number of studies are performed based on the measure of semantic similarity. Therefore, in this paper, we propose a new method of semantic similarity measuring which uses the dynamic programming to compute the semantic distance between any two concepts defined in the same hierarchy of ontology. Then, we base on this result to compute the semantic similarity. Finally, we present an experimental comparison between our method and other methods of similarity measuring. Where we will show the limits of these methods and how we avoid them with our method. This one bases on a function of weight allocation, which allows finding different rate of semantic similarity between a given concept and two other sibling concepts which is impossible using the other methods.



### Deep Multi-scale Discriminative Networks for Double JPEG Compression Forensics
- **Arxiv ID**: http://arxiv.org/abs/1904.02520v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02520v1)
- **Published**: 2019-04-04 12:44:57+00:00
- **Updated**: 2019-04-04 12:44:57+00:00
- **Authors**: Cheng Deng, Zhao Li, Xinbo Gao, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: As JPEG is the most widely used image format, the importance of tampering detection for JPEG images in blind forensics is self-evident. In this area, extracting effective statistical characteristics from a JPEG image for classification remains a challenge. Effective features are designed manually in traditional methods, suggesting that extensive labor-consuming research and derivation is required. In this paper, we propose a novel image tampering detection method based on deep multi-scale discriminative networks (MSD-Nets). The multi-scale module is designed to automatically extract multiple features from the discrete cosine transform (DCT) coefficient histograms of the JPEG image. This module can capture the characteristic information in different scale spaces. In addition, a discriminative module is also utilized to improve the detection effect of the networks in those difficult situations when the first compression quality (QF1) is higher than the second one (QF2). A special network in this module is designed to distinguish the small statistical difference between authentic and tampered regions in these cases. Finally, a probability map can be obtained and the specific tampering area is located using the last classification results. Extensive experiments demonstrate the superiority of our proposed method in both quantitative and qualitative metrics when compared with state-of-the-art approaches.



### DeCaFA: Deep Convolutional Cascade for Face Alignment In The Wild
- **Arxiv ID**: http://arxiv.org/abs/1904.02549v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02549v1)
- **Published**: 2019-04-04 13:36:11+00:00
- **Updated**: 2019-04-04 13:36:11+00:00
- **Authors**: Arnaud Dapogny, Kévin Bailly, Matthieu Cord
- **Comment**: None
- **Journal**: None
- **Summary**: Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce DeCaFA, an end-to-end deep convolutional cascade architecture for face alignment. DeCaFA uses fully-convolutional stages to keep full spatial resolution throughout the cascade. Between each cascade stage, DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark-wise attention maps for each of several landmark alignment tasks. Weighted intermediate supervision, as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end-to-end manner. We show experimentally that DeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW databases. In addition, we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data.



### Generic Multiview Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.02553v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02553v1)
- **Published**: 2019-04-04 13:42:12+00:00
- **Updated**: 2019-04-04 13:42:12+00:00
- **Authors**: Minye Wu, Haibin Ling, Ning Bi, Shenghua Gao, Hao Sheng, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progresses in visual tracking have greatly improved the tracking performance. However, challenges such as occlusion and view change remain obstacles in real world deployment. A natural solution to these challenges is to use multiple cameras with multiview inputs, though existing systems are mostly limited to specific targets (e.g. human), static cameras, and/or camera calibration. To break through these limitations, we propose a generic multiview tracking (GMT) framework that allows camera movement, while requiring neither specific object model nor camera calibration. A key innovation in our framework is a cross-camera trajectory prediction network (TPN), which implicitly and dynamically encodes camera geometric relations, and hence addresses missing target issues such as occlusion. Moreover, during tracking, we assemble information across different cameras to dynamically update a novel collaborative correlation filter (CCF), which is shared among cameras to achieve robustness against view change. The two components are integrated into a correlation filter tracking framework, where the features are trained offline using existing single view tracking datasets. For evaluation, we first contribute a new generic multiview tracking dataset (GMTD) with careful annotations, and then run experiments on GMTD and the PETS2009 datasets. On both datasets, the proposed GMT algorithm shows clear advantages over state-of-the-art ones.



### Noise-Level Estimation from Single Color Image Using Correlations Between Textures in RGB Channels
- **Arxiv ID**: http://arxiv.org/abs/1904.02566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02566v1)
- **Published**: 2019-04-04 14:12:10+00:00
- **Updated**: 2019-04-04 14:12:10+00:00
- **Authors**: Akihiro Nakamura, Michihiro Kobayashi
- **Comment**: 9 pages, 11 figures
- **Journal**: None
- **Summary**: We propose a simple method for estimating noise level from a single color image. In most image-denoising algorithms, an accurate noise-level estimate results in good denoising performance; however, it is difficult to estimate noise level from a single image because it is an ill-posed problem. We tackle this problem by using prior knowledge that textures are highly correlated between RGB channels and noise is uncorrelated to other signals. We also extended our method for RAW images because they are available in almost all digital cameras and often used in practical situations. Experiments show the high noise-estimation performance of our method in synthetic noisy images. We also applied our method to natural images including RAW images and achieved better noise-estimation performance than conventional methods.



### Segmentation of the Prostatic Gland and the Intraprostatic Lesions on Multiparametic MRI Using Mask-RCNN
- **Arxiv ID**: http://arxiv.org/abs/1904.02575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02575v1)
- **Published**: 2019-04-04 14:25:14+00:00
- **Updated**: 2019-04-04 14:25:14+00:00
- **Authors**: Zhenzhen Dai, Eric Carver, Chang Liu, Joon Lee, Aharon Feldman, Weiwei Zong, Milan Pantelic, Mohamed Elshaikh, Ning Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Prostate cancer (PCa) is the most common cancer in men in the United States. Multiparametic magnetic resonance imaging (mp-MRI) has been explored by many researchers to targeted prostate biopsies and radiation therapy. However, assessment on mp-MRI can be subjective, development of computer-aided diagnosis systems to automatically delineate the prostate gland and the intraprostratic lesions (ILs) becomes important to facilitate with radiologists in clinical practice. In this paper, we first study the implementation of the Mask-RCNN model to segment the prostate and ILs. We trained and evaluated models on 120 patients from two different cohorts of patients. We also used 2D U-Net and 3D U-Net as benchmarks to segment the prostate and compared the model's performance. The contour variability of ILs using the algorithm was also benchmarked against the interobserver variability between two different radiation oncologists on 19 patients. Our results indicate that the Mask-RCNN model is able to reach state-of-art performance in the prostate segmentation and outperforms several competitive baselines in ILs segmentation.



### Geometry of the Hough transforms with applications to synthetic data
- **Arxiv ID**: http://arxiv.org/abs/1904.02587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02587v1)
- **Published**: 2019-04-04 14:44:23+00:00
- **Updated**: 2019-04-04 14:44:23+00:00
- **Authors**: Mauro C. Beltrametti, Cristina Campi, Anna Maria Massone, Maria-Laura Torrente
- **Comment**: None
- **Journal**: None
- **Summary**: In the framework of the Hough transform technique to detect curves in images, we provide a bound for the number of Hough transforms to be considered for a successful optimization of the accumulator function in the recognition algorithm. Such a bound is consequence of geometrical arguments. We also show the robustness of the results when applied to synthetic datasets strongly perturbed by noise. An algebraic approach, discussed in the appendix, leads to a better bound of theoretical interest in the exact case.



### TightCap: 3D Human Shape Capture with Clothing Tightness Field
- **Arxiv ID**: http://arxiv.org/abs/1904.02601v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02601v4)
- **Published**: 2019-04-04 15:21:39+00:00
- **Updated**: 2021-08-06 07:38:11+00:00
- **Authors**: Xin Chen, Anqi Pang, Yang Wei, Lan Xui, Jingyi Yu
- **Comment**: 18 pages, 18 figures
- **Journal**: None
- **Summary**: In this paper, we present TightCap, a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan, which enables numerous applications such as virtual try-on, biometrics and body evaluation. To break the severe variations of the human poses and garments, we propose to model the clothing tightness - the displacements from the garments to the human shape implicitly in the global UV texturing domain. To this end, we utilize an enhanced statistical human template and an effective multi-stage alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on this 2D representation, we propose a novel framework to predicted clothing tightness via a novel tightness formulation, as well as an effective optimization scheme to further reconstruct multi-layer human shape and garments under various clothing categories and human postures. We further propose a new clothing tightness dataset (CTD) of human scans with a large variety of clothing styles, poses and corresponding ground-truth human shapes to stimulate further research. Extensive experiments demonstrate the effectiveness of our TightCap to achieve high-quality human shape and dressed garments reconstruction, as well as the further applications for clothing segmentation, retargeting and animation.



### 3D Face Reconstruction Using Color Photometric Stereo with Uncalibrated Near Point Lights
- **Arxiv ID**: http://arxiv.org/abs/1904.02605v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02605v2)
- **Published**: 2019-04-04 15:24:32+00:00
- **Updated**: 2019-12-22 10:18:32+00:00
- **Authors**: Zhang Chen, Yu Ji, Mingyuan Zhou, Sing Bing Kang, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new color photometric stereo (CPS) method that recovers high quality, detailed 3D face geometry in a single shot. Our system uses three uncalibrated near point lights of different colors and a single camera. For robust self-calibration of the light sources, we use 3D morphable model (3DMM) and semantic segmentation of facial parts. We address the spectral ambiguity problem by incorporating albedo consensus, albedo similarity, and proxy prior into a unified framework. We avoid the need for spatial constancy of albedo; instead, we use a new measure for albedo similarity that is based on the albedo norm profile. Experiments show that our new approach produces state-of-the-art results from single image with high-fidelity geometry that includes details such as wrinkles.



### Sampling Limits for Electron Tomography with Sparsity-exploiting Reconstructions
- **Arxiv ID**: http://arxiv.org/abs/1904.02614v1
- **DOI**: 10.1016/j.ultramic.2017.12.010
- **Categories**: **cs.CV**, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/1904.02614v1)
- **Published**: 2019-04-04 15:40:56+00:00
- **Updated**: 2019-04-04 15:40:56+00:00
- **Authors**: Yi Jiang, Elliot Padgett, Robert Hovden, David A. Muller
- **Comment**: None
- **Journal**: Ultramicroscopy 186, 94-103 (2018)
- **Summary**: Electron tomography (ET) has become a standard technique for 3D characterization of materials at the nano-scale. Traditional reconstruction algorithms such as weighted back projection suffer from disruptive artifacts with insufficient projections. Popularized by compressed sensing, sparsity-exploiting algorithms have been applied to experimental ET data and show promise for improving reconstruction quality or reducing the total beam dose applied to a specimen. Nevertheless, theoretical bounds for these methods have been less explored in the context of ET applications. Here, we perform numerical simulations to investigate performance of l_1-norm and total-variation (TV) minimization under various imaging conditions. From 36,100 different simulated structures, our results show specimens with more complex structures generally require more projections for exact reconstruction. However, once sufficient data is acquired, dividing the beam dose over more projections provides no improvements - analogous to the traditional dose-fraction theorem. Moreover, a limited tilt range of +-75 or less can result in distorting artifacts in sparsity-exploiting reconstructions. The influence of optimization parameters on reconstructions is also discussed.



### Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.02616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02616v1)
- **Published**: 2019-04-04 15:42:58+00:00
- **Updated**: 2019-04-04 15:42:58+00:00
- **Authors**: Tongtong Yuan, Weihong Deng, Jian Tang, Yinan Tang, Binghui Chen
- **Comment**: cvpr2019
- **Journal**: None
- **Summary**: Deep metric learning, which learns discriminative features to process image clustering and retrieval tasks, has attracted extensive attention in recent years. A number of deep metric learning methods, which ensure that similar examples are mapped close to each other and dissimilar examples are mapped farther apart, have been proposed to construct effective structures for loss functions and have shown promising results. In this paper, different from the approaches on learning the loss structures, we propose a robust SNR distance metric based on Signal-to-Noise Ratio (SNR) for measuring the similarity of image pairs for deep metric learning. By exploring the properties of our SNR distance metric from the view of geometry space and statistical theory, we analyze the properties of our metric and show that it can preserve the semantic similarity between image pairs, which well justify its suitability for deep metric learning. Compared with Euclidean distance metric, our SNR distance metric can further jointly reduce the intra-class distances and enlarge the inter-class distances for learned features. Leveraging our SNR distance metric, we propose Deep SNR-based Metric Learning (DSML) to generate discriminative feature embeddings. By extensive experiments on three widely adopted benchmarks, including CARS196, CUB200-2011 and CIFAR10, our DSML has shown its superiority over other state-of-the-art methods. Additionally, we extend our SNR distance metric to deep hashing learning, and conduct experiments on two benchmarks, including CIFAR10 and NUS-WIDE, to demonstrate the effectiveness and generality of our SNR distance metric.



### End-to-End Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.02628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.02628v2)
- **Published**: 2019-04-04 15:57:23+00:00
- **Updated**: 2019-11-08 10:28:48+00:00
- **Authors**: Silvio Olivastri, Gurkirt Singh, Fabio Cuzzolin
- **Comment**: Accepted at Large Scale Holistic Video Understanding, ICCVW 2019
- **Journal**: None
- **Summary**: Building correspondences across different modalities, such as video and language, has recently become critical in many visual recognition applications, such as video captioning. Inspired by machine translation, recent models tackle this task using an encoder-decoder strategy. The (video) encoder is traditionally a Convolutional Neural Network (CNN), while the decoding (for language generation) is done using a Recurrent Neural Network (RNN). Current state-of-the-art methods, however, train encoder and decoder separately. CNNs are pretrained on object and/or action recognition tasks and used to encode video-level features. The decoder is then optimised on such static features to generate the video's description. This disjoint setup is arguably sub-optimal for input (video) to output (description) mapping. In this work, we propose to optimise both encoder and decoder simultaneously in an end-to-end fashion. In a two-stage training setting, we first initialise our architecture using pre-trained encoders and decoders -- then, the entire network is trained end-to-end in a fine-tuning stage to learn the most relevant features for video caption generation. In our experiments, we use GoogLeNet and Inception-ResNet-v2 as encoders and an original Soft-Attention (SA-) LSTM as a decoder. Analogously to gains observed in other computer vision problems, we show that end-to-end training significantly improves over the traditional, disjoint training process. We evaluate our End-to-End (EtENet) Networks on the Microsoft Research Video Description (MSVD) and the MSR Video to Text (MSR-VTT) benchmark datasets, showing how EtENet achieves state-of-the-art performance across the board.



### A Learned Representation for Scalable Vector Graphics
- **Arxiv ID**: http://arxiv.org/abs/1904.02632v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02632v1)
- **Published**: 2019-04-04 16:04:03+00:00
- **Updated**: 2019-04-04 16:04:03+00:00
- **Authors**: Raphael Gontijo Lopes, David Ha, Douglas Eck, Jonathon Shlens
- **Comment**: None
- **Journal**: None
- **Summary**: Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.



### Clinically Accurate Chest X-Ray Report Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.02633v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1904.02633v2)
- **Published**: 2019-04-04 16:04:30+00:00
- **Updated**: 2019-07-29 04:15:47+00:00
- **Authors**: Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, Marzyeh Ghassemi
- **Comment**: None
- **Journal**: None
- **Summary**: The automatic generation of radiology reports given medical radiographs has significant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which first predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is fine-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that our model offers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.



### Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.02639v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02639v2)
- **Published**: 2019-04-04 16:16:50+00:00
- **Updated**: 2019-08-06 08:20:46+00:00
- **Authors**: Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, Anton van den Hengel
- **Comment**: Accepted to appear at ICCV 2019
- **Journal**: None
- **Summary**: Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder "generalizes" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.



### Siamese Encoding and Alignment by Multiscale Learning with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1904.02643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02643v1)
- **Published**: 2019-04-04 16:31:01+00:00
- **Updated**: 2019-04-04 16:31:01+00:00
- **Authors**: Eric Mitchell, Stefan Keselj, Sergiy Popovych, Davit Buniatyan, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method of aligning a source image to a target image, where the transform is specified by a dense vector field. The two images are encoded as feature hierarchies by siamese convolutional nets. Then a hierarchy of aligner modules computes the transform in a coarse-to-fine recursion. Each module receives as input the transform that was computed by the module at the level above, aligns the source and target encodings at the same level of the hierarchy, and then computes an improved approximation to the transform using a convolutional net. The entire architecture of encoder and aligner nets is trained in a self-supervised manner to minimize the squared error between source and target remaining after alignment. We show that siamese encoding enables more accurate alignment than the image pyramids of SPyNet, a previous deep learning approach to coarse-to-fine alignment. Furthermore, self-supervision applies even without target values for the transform, unlike the strongly supervised SPyNet. We also show that our approach outperforms one-shot approaches to alignment, because the fine pathways in the latter approach may fail to contribute to alignment accuracy when displacements are large. As shown by previous one-shot approaches, good results from self-supervised learning require that the loss function additionally penalize non-smooth transforms. We demonstrate that "masking out" the penalty function near discontinuities leads to correct recovery of non-smooth transforms. Our claims are supported by empirical comparisons using images from serial section electron microscopy of brain tissue.



### On Direct Distribution Matching for Adapting Segmentation Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.02657v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02657v2)
- **Published**: 2019-04-04 16:49:18+00:00
- **Updated**: 2021-11-24 23:56:22+00:00
- **Authors**: Georg Pichler, Jose Dolz, Ismail Ben Ayed, Pablo Piantanida
- **Comment**: includes appendix; published at MIDL2020:
  https://2020.midl.io/papers/pichler20.html
- **Journal**: None
- **Summary**: Minimization of distribution matching losses is a principled approach to domain adaptation in the context of image classification. However, it is largely overlooked in adapting segmentation networks, which is currently dominated by adversarial models. We propose a class of loss functions, which encourage direct kernel density matching in the network-output space, up to some geometric transformations computed from unlabeled inputs. Rather than using an intermediate domain discriminator, our direct approach unifies distribution matching and segmentation in a single loss. Therefore, it simplifies segmentation adaptation by avoiding extra adversarial steps, while improving both the quality, stability and efficiency of training. We juxtapose our approach to state-of-the-art segmentation adaptation via adversarial training in the network-output space. In the challenging task of adapting brain segmentation across different magnetic resonance images (MRI) modalities, our approach achieves significantly better results both in terms of accuracy and stability.



### Algebraic Characterization of Essential Matrices and Their Averaging in Multiview Settings
- **Arxiv ID**: http://arxiv.org/abs/1904.02663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02663v2)
- **Published**: 2019-04-04 17:00:00+00:00
- **Updated**: 2020-02-25 19:14:06+00:00
- **Authors**: Yoni Kasten, Amnon Geifman, Meirav Galun, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Essential matrix averaging, i.e., the task of recovering camera locations and orientations in calibrated, multiview settings, is a first step in global approaches to Euclidean structure from motion. A common approach to essential matrix averaging is to separately solve for camera orientations and subsequently for camera positions. This paper presents a novel approach that solves simultaneously for both camera orientations and positions. We offer a complete characterization of the algebraic conditions that enable a unique Euclidean reconstruction of $n$ cameras from a collection of $(^n_2)$ essential matrices. We next use these conditions to formulate essential matrix averaging as a constrained optimization problem, allowing us to recover a consistent set of essential matrices given a (possibly partial) set of measured essential matrices computed independently for pairs of images. We finally use the recovered essential matrices to determine the global positions and orientations of the $n$ cameras. We test our method on common SfM datasets, demonstrating high accuracy while maintaining efficiency and robustness, compared to existing methods.



### Deep Multi-class Adversarial Specularity Removal
- **Arxiv ID**: http://arxiv.org/abs/1904.02672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02672v1)
- **Published**: 2019-04-04 17:14:14+00:00
- **Updated**: 2019-04-04 17:14:14+00:00
- **Authors**: John Lin, Mohamed El Amine Seddik, Mohamed Tamaazousti, Youssef Tamaazousti, Adrien Bartoli
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel learning approach, in the form of a fully-convolutional neural network (CNN), which automatically and consistently removes specular highlights from a single image by generating its diffuse component. To train the generative network, we define an adversarial loss on a discriminative network as in the GAN framework and combined it with a content loss. In contrast to existing GAN approaches, we implemented the discriminator to be a multi-class classifier instead of a binary one, to find more constraining features. This helps the network pinpoint the diffuse manifold by providing two more gradient terms. We also rendered a synthetic dataset designed to help the network generalize well. We show that our model performs well across various synthetic and real images and outperforms the state-of-the-art in consistency.



### UU-Nets Connecting Discriminator and Generator for Image to Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.02675v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.02675v1)
- **Published**: 2019-04-04 17:25:21+00:00
- **Updated**: 2019-04-04 17:25:21+00:00
- **Authors**: Wu Jionghao
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial generative model have successfully manifest itself in image synthesis. However, the performance deteriorate and unstable, because discriminator is far stable than generator, and it is hard to control the game between the two modules. Various methods have been introduced to tackle the problem such as WGAN, Relativistic GAN and their successors by adding or restricting the loss function, which certainly help balance the min-max game, but they all focused on the loss function ignoring the intrinsic structure limitation. We present a UU-Net architecture inspired by U-net bridging the encoder and the decoder, UU-Net composed by two U-Net liked modules respectively served as generator and discriminator. Because the modules in U-net are symmetrical, therefore it shares weights easily between all four components. Thanks to UU-net's modules identical and symmetric property, we could not only carried the features from inner generator's encoder to its decoder, but also to the discriminator's encoder and decoder. By this design, it give us more control and condition flexibility to intervene the process between the generator and the discriminator.



### Estimating 3D Motion and Forces of Person-Object Interactions from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1904.02683v2
- **DOI**: 10.1109/CVPR.2019.00884
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02683v2)
- **Published**: 2019-04-04 17:43:35+00:00
- **Updated**: 2019-06-17 11:45:10+00:00
- **Authors**: Zongmian Li, Jiri Sedlar, Justin Carpentier, Ivan Laptev, Nicolas Mansard, Josef Sivic
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a method to automatically reconstruct the 3D motion of a person interacting with an object from a single RGB video. Our method estimates the 3D poses of the person and the object, contact positions, and forces and torques actuated by the human limbs. The main contributions of this work are three-fold. First, we introduce an approach to jointly estimate the motion and the actuation forces of the person on the manipulated object by modeling contacts and the dynamics of their interactions. This is cast as a large-scale trajectory optimization problem. Second, we develop a method to automatically recognize from the input video the position and timing of contacts between the person and the object or the ground, thereby significantly simplifying the complexity of the optimization. Third, we validate our approach on a recent MoCap dataset with ground truth contact forces and demonstrate its performance on a new dataset of Internet videos showing people manipulating a variety of tools in unconstrained environments.



### YOLACT: Real-time Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.02689v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02689v2)
- **Published**: 2019-04-04 17:46:12+00:00
- **Updated**: 2019-10-24 18:09:40+00:00
- **Authors**: Daniel Bolya, Chong Zhou, Fanyi Xiao, Yong Jae Lee
- **Comment**: Updated for ICCV 2019 and added appendix
- **Journal**: None
- **Summary**: We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.



### T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor
- **Arxiv ID**: http://arxiv.org/abs/1904.02698v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02698v1)
- **Published**: 2019-04-04 17:55:37+00:00
- **Updated**: 2019-04-04 17:55:37+00:00
- **Authors**: Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Recent findings indicate that over-parametrization, while crucial for successfully training deep neural networks, also introduces large amounts of redundancy. Tensor methods have the potential to efficiently parametrize over-complete representations by leveraging this redundancy. In this paper, we propose to fully parametrize Convolutional Neural Networks (CNNs) with a single high-order, low-rank tensor. Previous works on network tensorization have focused on parametrizing individual layers (convolutional or fully connected) only, and perform the tensorization layer-by-layer separately. In contrast, we propose to jointly capture the full structure of a neural network by parametrizing it with a single high-order tensor, the modes of which represent each of the architectural design parameters of the network (e.g. number of convolutional blocks, depth, number of stacks, input features, etc). This parametrization allows to regularize the whole network and drastically reduce the number of parameters. Our model is end-to-end trainable and the low-rank structure imposed on the weight tensor acts as an implicit regularization. We study the case of networks with rich structure, namely Fully Convolutional Networks (FCNs), which we propose to parametrize with a single 8th-order tensor. We show that our approach can achieve superior performance with small compression rates, and attain high compression rates with negligible drop in accuracy for the challenging task of human pose estimation.



### Libra R-CNN: Towards Balanced Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.02701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02701v1)
- **Published**: 2019-04-04 17:58:22+00:00
- **Updated**: 2019-04-04 17:58:22+00:00
- **Authors**: Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, Dahua Lin
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection. It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN Faster R-CNN and RetinaNet respectively on MSCOCO.



### Neural Models of the Psychosemantics of `Most'
- **Arxiv ID**: http://arxiv.org/abs/1904.02734v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.02734v1)
- **Published**: 2019-04-04 18:14:23+00:00
- **Updated**: 2019-04-04 18:14:23+00:00
- **Authors**: Lewis O'Sullivan, Shane Steinert-Threlkeld
- **Comment**: to appear at 9th Workshop on Cognitive Modeling and Computational
  Linguistics (CMCL2019)
- **Journal**: None
- **Summary**: How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network -- a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) -- on the "most" verification task from \citet{Pietroski2009}, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.



### Learning to Cluster Faces on an Affinity Graph
- **Arxiv ID**: http://arxiv.org/abs/1904.02749v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02749v2)
- **Published**: 2019-04-04 19:01:35+00:00
- **Updated**: 2019-05-05 08:41:15+00:00
- **Authors**: Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy, Dahua Lin
- **Comment**: 8 pages, 8 figures, CVPR 2019
- **Journal**: None
- **Summary**: Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (i.e. million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.



### DeceptionNet: Network-Driven Domain Randomization
- **Arxiv ID**: http://arxiv.org/abs/1904.02750v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02750v2)
- **Published**: 2019-04-04 19:01:42+00:00
- **Updated**: 2019-08-20 16:38:36+00:00
- **Authors**: Sergey Zakharov, Wadim Kehl, Slobodan Ilic
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing "blind" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities.



### Blind Visual Motif Removal from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1904.02756v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02756v1)
- **Published**: 2019-04-04 19:17:05+00:00
- **Updated**: 2019-04-04 19:17:05+00:00
- **Authors**: Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Many images shared over the web include overlaid objects, or visual motifs, such as text, symbols or drawings, which add a description or decoration to the image. For example, decorative text that specifies where the image was taken, repeatedly appears across a variety of different images. Often, the reoccurring visual motif, is semantically similar, yet, differs in location, style and content (e.g. text placement, font and letters). This work proposes a deep learning based technique for blind removal of such objects. In the blind setting, the location and exact geometry of the motif are unknown. Our approach simultaneously estimates which pixels contain the visual motif, and synthesizes the underlying latent image. It is applied to a single input image, without any user assistance in specifying the location of the motif, achieving state-of-the-art results for blind removal of both opaque and semi-transparent visual motifs.



### Learning Implicit Generative Models by Matching Perceptual Features
- **Arxiv ID**: http://arxiv.org/abs/1904.02762v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02762v1)
- **Published**: 2019-04-04 19:34:23+00:00
- **Updated**: 2019-04-04 19:34:23+00:00
- **Authors**: Cicero Nogueira dos Santos, Youssef Mroueh, Inkit Padhi, Pierre Dognin
- **Comment**: 16 pages
- **Journal**: ICCV 2019
- **Summary**: Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks.



### Biometric Fish Classification of Temperate Species Using Convolutional Neural Network with Squeeze-and-Excitation
- **Arxiv ID**: http://arxiv.org/abs/1904.02768v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02768v1)
- **Published**: 2019-04-04 19:50:49+00:00
- **Updated**: 2019-04-04 19:50:49+00:00
- **Authors**: Erlend Olsvik, Christian M. D. Trinh, Kristian Muri Knausgård, Arne Wiklund, Tonje Knutsen Sørdalen, Alf Ring Kleiven, Lei Jiao, Morten Goodwin
- **Comment**: None
- **Journal**: None
- **Summary**: Our understanding and ability to effectively monitor and manage coastal ecosystems are severely limited by observation methods. Automatic recognition of species in natural environment is a promising tool which would revolutionize video and image analysis for a wide range of applications in marine ecology. However, classifying fish from images captured by underwater cameras is in general very challenging due to noise and illumination variations in water. Previous classification methods in the literature relies on filtering the images to separate the fish from the background or sharpening the images by removing background noise. This pre-filtering process may negatively impact the classification accuracy. In this work, we propose a Convolutional Neural Network (CNN) using the Squeeze-and-Excitation (SE) architecture for classifying images of fish without pre-filtering. Different from conventional schemes, this scheme is divided into two steps. The first step is to train the fish classifier via a public data set, i.e., Fish4Knowledge, without using image augmentation, named as pre-training. The second step is to train the classifier based on a new data set consisting of species that we are interested in for classification, named as post-training. The weights obtained from pre-training are applied to post-training as a priori. This is also known as transfer learning. Our solution achieves the state-of-the-art accuracy of 99.27% accuracy on the pre-training. The accuracy on the post-training is 83.68%. Experiments on the post-training with image augmentation yields an accuracy of 87.74%, indicating that the solution is viable with a larger data set.



### Crowd Transformer Network
- **Arxiv ID**: http://arxiv.org/abs/1904.02774v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02774v1)
- **Published**: 2019-04-04 20:04:39+00:00
- **Updated**: 2019-04-04 20:04:39+00:00
- **Authors**: Viresh Ranjan, Mubarak Shah, Minh Hoai Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of Crowd Counting, and present a crowd density estimation based approach for obtaining the crowd count. Most of the existing crowd counting approaches rely on local features for estimating the crowd density map. In this work, we investigate the usefulness of combining local with non-local features for crowd counting. We use convolution layers for extracting local features, and a type of self-attention mechanism for extracting non-local features. We combine the local and the non-local features, and use it for estimating crowd density map. We conduct experiments on three publicly available Crowd Counting datasets, and achieve significant improvement over the previous approaches.



### VQD: Visual Query Detection in Natural Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.02794v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02794v2)
- **Published**: 2019-04-04 21:12:37+00:00
- **Updated**: 2019-04-11 19:59:37+00:00
- **Authors**: Manoj Acharya, Karan Jariwala, Christopher Kanan
- **Comment**: To appear in NAACL 2019 ( To download the dataset please go to
  http://www.manojacharya.com/ )
- **Journal**: None
- **Summary**: We propose Visual Query Detection (VQD), a new visual grounding task. In VQD, a system is guided by natural language to localize a variable number of objects in an image. VQD is related to visual referring expression recognition, where the task is to localize only one object. We describe the first dataset for VQD and we propose baseline algorithms that demonstrate the difficulty of the task compared to referring expression recognition.



### Assessment of Faster R-CNN in Man-Machine collaborative search
- **Arxiv ID**: http://arxiv.org/abs/1904.02805v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.02805v1)
- **Published**: 2019-04-04 22:03:53+00:00
- **Updated**: 2019-04-04 22:03:53+00:00
- **Authors**: Arturo Deza, Amit Surana, Miguel P. Eckstein
- **Comment**: To be presented at CVPR 2019 in Long Beach, CA
- **Journal**: None
- **Summary**: With the advent of modern expert systems driven by deep learning that supplement human experts (e.g. radiologists, dermatologists, surveillance scanners), we analyze how and when do such expert systems enhance human performance in a fine-grained small target visual search task. We set up a 2 session factorial experimental design in which humans visually search for a target with and without a Deep Learning (DL) expert system. We evaluate human changes of target detection performance and eye-movements in the presence of the DL system. We find that performance improvements with the DL system (computed via a Faster R-CNN with a VGG16) interacts with observer's perceptual abilities (e.g., sensitivity). The main results include: 1) The DL system reduces the False Alarm rate per Image on average across observer groups of both high/low sensitivity; 2) Only human observers with high sensitivity perform better than the DL system, while the low sensitivity group does not surpass individual DL system performance, even when aided with the DL system itself; 3) Increases in number of trials and decrease in viewing time were mainly driven by the DL system only for the low sensitivity group. 4) The DL system aids the human observer to fixate at a target by the 3rd fixation. These results provide insights of the benefits and limitations of deep learning systems that are collaborative or competitive with humans.



### Video Classification with Channel-Separated Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.02811v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.02811v4)
- **Published**: 2019-04-04 22:28:24+00:00
- **Updated**: 2019-11-18 22:30:49+00:00
- **Authors**: Du Tran, Heng Wang, Lorenzo Torresani, Matt Feiszli
- **Comment**: None
- **Journal**: None
- **Summary**: Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks.   This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -- Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M, Kinetics, and Something-Something, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient.



### Regularizing Activation Distribution for Training Binarized Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.02823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02823v1)
- **Published**: 2019-04-04 23:20:09+00:00
- **Updated**: 2019-04-04 23:20:09+00:00
- **Authors**: Ruizhou Ding, Ting-Wu Chin, Zeye Liu, Diana Marculescu
- **Comment**: None
- **Journal**: None
- **Summary**: Binarized Neural Networks (BNNs) can significantly reduce the inference latency and energy consumption in resource-constrained devices due to their pure-logical computation and fewer memory accesses. However, training BNNs is difficult since the activation flow encounters degeneration, saturation, and gradient mismatch problems. Prior work alleviates these issues by increasing activation bits and adding floating-point scaling factors, thereby sacrificing BNN's energy efficiency. In this paper, we propose to use distribution loss to explicitly regularize the activation flow, and develop a framework to systematically formulate the loss. Our experiments show that the distribution loss can consistently improve the accuracy of BNNs without losing their energy benefits. Moreover, equipped with the proposed regularization, BNN training is shown to be robust to the selection of hyper-parameters including optimizer and learning rate.



