# Arxiv Papers in cs.CV on 2019-04-22
### STGAN: A Unified Selective Transfer Network for Arbitrary Image Attribute Editing
- **Arxiv ID**: http://arxiv.org/abs/1904.09709v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09709v1)
- **Published**: 2019-04-22 03:24:33+00:00
- **Updated**: 2019-04-22 03:24:33+00:00
- **Authors**: Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding, Wangmeng Zuo, Shilei Wen
- **Comment**: None
- **Journal**: CVPR 2019; code is available at https://github.com/csmliu/STGAN
- **Summary**: Arbitrary attribute editing generally can be tackled by incorporating encoder-decoder and generative adversarial networks. However, the bottleneck layer in encoder-decoder usually gives rise to blurry and low quality editing result. And adding skip connections improves image quality at the cost of weakened attribute manipulation ability. Moreover, existing methods exploit target attribute vector to guide the flexible translation to desired target domain. In this work, we suggest to address these issues from selective transfer perspective. Considering that specific editing task is certainly only related to the changed attributes instead of all target attributes, our model selectively takes the difference between target and source attribute vectors as input. Furthermore, selective transfer units are incorporated with encoder-decoder to adaptively select and modify encoder feature for enhanced attribute editing. Experiments show that our method (i.e., STGAN) simultaneously improves attribute manipulation accuracy as well as perception quality, and performs favorably against state-of-the-arts in arbitrary facial attribute editing and season translation.



### FeatherNets: Convolutional Neural Networks as Light as Feather for Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1904.09290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09290v1)
- **Published**: 2019-04-22 05:04:36+00:00
- **Updated**: 2019-04-22 05:04:36+00:00
- **Authors**: Peng Zhang, Fuhao Zou, Zhiwen Wu, Nengli Dai, Skarpness Mark, Michael Fu, Juan Zhao, Kai Li
- **Comment**: 10 pages;6 figures
- **Journal**: None
- **Summary**: Face Anti-spoofing gains increased attentions recently in both academic and industrial fields. With the emergence of various CNN based solutions, the multi-modal(RGB, depth and IR) methods based CNN showed better performance than single modal classifiers. However, there is a need for improving the performance and reducing the complexity. Therefore, an extreme light network architecture(FeatherNet A/B) is proposed with a streaming module which fixes the weakness of Global Average Pooling and uses less parameters. Our single FeatherNet trained by depth image only, provides a higher baseline with 0.00168 ACER, 0.35M parameters and 83M FLOPS. Furthermore, a novel fusion procedure with ``ensemble + cascade'' structure is presented to satisfy the performance preferred use cases. Meanwhile, the MMFD dataset is collected to provide more attacks and diversity to gain better generalization. We use the fusion method in the Face Anti-spoofing Attack Detection Challenge@CVPR2019 and got the result of 0.0013(ACER), 0.999(TPR@FPR=10e-2), 0.998(TPR@FPR=10e-3) and 0.9814(TPR@FPR=10e-4).



### FishNet: A Camera Localizer using Deep Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09722v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09722v1)
- **Published**: 2019-04-22 05:11:14+00:00
- **Updated**: 2019-04-22 05:11:14+00:00
- **Authors**: Hsin-I Chen, Sebastian Agethen, Chiamin Wu, Winston Hsu, Bing-Yu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a robust localization system that employs deep learning for better scene representation, and enhances the accuracy of 6-DOF camera pose estimation. Inspired by the fact that global scene structure can be revealed by wide field-of-view, we leverage the large overlap of a fisheye camera between adjacent frames, and the powerful high-level feature representations of deep learning. Our main contribution is the novel network architecture that extracts both temporal and spatial information using a Recurrent Neural Network. Specifically, we propose a novel pose regularization term combined with LSTM. This leads to smoother pose estimation, especially for large outdoor scenery. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.



### An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09730v1)
- **Published**: 2019-04-22 05:45:57+00:00
- **Updated**: 2019-04-22 05:45:57+00:00
- **Authors**: Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok Bae, Jongyoul Park
- **Comment**: CVPR2019 CEFRL Workshop
- **Journal**: None
- **Summary**: As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet.



### Facial Expression Recognition Research Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.09737v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09737v3)
- **Published**: 2019-04-22 06:13:47+00:00
- **Updated**: 2019-10-21 11:30:32+00:00
- **Authors**: Yongpei Zhu, Hongwei Fan, Kehong Yuan
- **Comment**: 12 pages,13 figures.The paper is under consideration at Pattern
  Recognition Letters
- **Journal**: None
- **Summary**: With the development of deep learning, the structure of convolution neural network is becoming more and more complex and the performance of object recognition is getting better. However, the classification mechanism of convolution neural networks is still an unsolved core problem. The main problem is that convolution neural networks have too many parameters, which makes it difficult to analyze them. In this paper, we design and train a convolution neural network based on the expression recognition, and explore the classification mechanism of the network. By using the Deconvolution visualization method, the extremum point of the convolution neural network is projected back to the pixel space of the original image, and we qualitatively verify that the trained expression recognition convolution neural network forms a detector for the specific facial action unit. At the same time, we design the distance function to measure the distance between the presence of facial feature unit and the maximal value of the response on the feature map of convolution neural network. The greater the distance, the more sensitive the feature map is to the facial feature unit. By comparing the maximum distance of all facial feature elements in the feature graph, the mapping relationship between facial feature element and convolution neural network feature map is determined. Therefore, we have verified that the convolution neural network has formed a detector for the facial Action unit in the training process to realize the expression recognition.



### Switchable Whitening for Deep Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.09739v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09739v4)
- **Published**: 2019-04-22 06:22:55+00:00
- **Updated**: 2019-12-12 11:18:05+00:00
- **Authors**: Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, Ping Luo
- **Comment**: Accepted to ICCV2019
- **Journal**: None
- **Summary**: Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset. Code is available at https://github.com/XingangPan/Switchable-Whitening.



### NLP Driven Ensemble Based Automatic Subtitle Generation and Semantic Video Summarization Technique
- **Arxiv ID**: http://arxiv.org/abs/1904.09740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09740v1)
- **Published**: 2019-04-22 06:32:15+00:00
- **Updated**: 2019-04-22 06:32:15+00:00
- **Authors**: VB Aswin, Mohammed Javed, Parag Parihar, K Aswanth, CR Druval, Anpam Dagar, CV Aravinda
- **Comment**: Accepted in AIDE2019
- **Journal**: None
- **Summary**: This paper proposes an automatic subtitle generation and semantic video summarization technique. The importance of automatic video summarization is vast in the present era of big data. Video summarization helps in efficient storage and also quick surfing of large collection of videos without losing the important ones. The summarization of the videos is done with the help of subtitles which is obtained using several text summarization algorithms. The proposed technique generates the subtitle for videos with/without subtitles using speech recognition and then applies NLP based Text summarization algorithms on the subtitles. The performance of subtitle generation and video summarization is boosted through Ensemble method with two approaches such as Intersection method and Weight based learning method Experimental results reported show the satisfactory performance of the proposed method



### 2D3D-MatchNet: Learning to Match Keypoints Across 2D Image and 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1904.09742v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09742v1)
- **Published**: 2019-04-22 06:49:46+00:00
- **Updated**: 2019-04-22 06:49:46+00:00
- **Authors**: Mengdan Feng, Sixing Hu, Marcelo Ang, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale point cloud generated from 3D sensors is more accurate than its image-based counterpart. However, it is seldom used in visual pose estimation due to the difficulty in obtaining 2D-3D image to point cloud correspondences. In this paper, we propose the 2D3D-MatchNet - an end-to-end deep network architecture to jointly learn the descriptors for 2D and 3D keypoint from image and point cloud, respectively. As a result, we are able to directly match and establish 2D-3D correspondences from the query image and 3D point cloud reference map for visual pose estimation. We create our Oxford 2D-3D Patches dataset from the Oxford Robotcar dataset with the ground truth camera poses and 2D-3D image to point cloud correspondences for training and testing the deep network. Experimental results verify the feasibility of our approach.



### Local Deep-Feature Alignment for Unsupervised Dimension Reduction
- **Arxiv ID**: http://arxiv.org/abs/1904.09747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09747v1)
- **Published**: 2019-04-22 07:04:02+00:00
- **Updated**: 2019-04-22 07:04:02+00:00
- **Authors**: Jian Zhang, Jun Yu, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an unsupervised deep-learning framework named Local Deep-Feature Alignment (LDFA) for dimension reduction. We construct neighbourhood for each data sample and learn a local Stacked Contractive Auto-encoder (SCAE) from the neighbourhood to extract the local deep features. Next, we exploit an affine transformation to align the local deep features of each neighbourhood with the global features. Moreover, we derive an approach from LDFA to map explicitly a new data sample into the learned low-dimensional subspace. The advantage of the LDFA method is that it learns both local and global characteristics of the data sample set: the local SCAEs capture local characteristics contained in the data set, while the global alignment procedures encode the interdependencies between neighbourhoods into the final low-dimensional feature representations. Experimental results on data visualization, clustering and classification show that the LDFA method is competitive with several well-known dimension reduction techniques, and exploiting locality in deep learning is a research topic worth further exploring.



### Tracking as A Whole: Multi-Target Tracking by Modeling Group Behavior with Sequential Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.12641v1
- **DOI**: 10.1109/TITS.2017.2686871
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.12641v1)
- **Published**: 2019-04-22 07:39:00+00:00
- **Updated**: 2019-04-22 07:39:00+00:00
- **Authors**: Yuan Yuan, Yuwei Lu, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based vehicle detection and tracking is one of the most important components for Intelligent Transportation Systems (ITS). When it comes to road junctions, the problem becomes even more difficult due to the occlusions and complex interactions among vehicles. In order to get a precise detection and tracking result, in this work we propose a novel tracking-by-detection framework. In the detection stage, we present a sequential detection model to deal with serious occlusions. In the tracking stage, we model group behavior to treat complex interactions with overlaps and ambiguities. The main contributions of this paper are twofold: 1) Shape prior is exploited in the sequential detection model to tackle occlusions in crowded scene. 2) Traffic force is defined in the traffic scene to model group behavior, and it can assist to handle complex interactions among vehicles. We evaluate the proposed approach on real surveillance videos at road junctions and the performance has demonstrated the effectiveness of our method.



### Forward Vehicle Collision Warning Based on Quick Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/1904.12642v1
- **DOI**: 10.1109/ICASSP.2018.8461620
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.12642v1)
- **Published**: 2019-04-22 07:39:32+00:00
- **Updated**: 2019-04-22 07:39:32+00:00
- **Authors**: Yuwei Lu, Yuan Yuan, Qi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Forward Vehicle Collision Warning (FCW) is one of the most important functions for autonomous vehicles. In this procedure, vehicle detection and distance measurement are core components, requiring accurate localization and estimation. In this paper, we propose a simple but efficient forward vehicle collision warning framework by aggregating monocular distance measurement and precise vehicle detection. In order to obtain forward vehicle distance, a quick camera calibration method which only needs three physical points to calibrate related camera parameters is utilized. As for the forward vehicle detection, a multi-scale detection algorithm that regards the result of calibration as distance priori is proposed to improve the precision. Intensive experiments are conducted in our established real scene dataset and the results have demonstrated the effectiveness of the proposed framework.



### Non-local Attention Optimized Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1904.09757v1
- **DOI**: 10.1109/TIP.2021.3058615
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09757v1)
- **Published**: 2019-04-22 07:51:17+00:00
- **Updated**: 2019-04-22 07:51:17+00:00
- **Authors**: Haojie Liu, Tong Chen, Peiyao Guo, Qiu Shen, Xun Cao, Yao Wang, Zhan Ma
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 30, pp. 3179-3191,
  2021
- **Summary**: This paper proposes a novel Non-Local Attention Optimized Deep Image Compression (NLAIC) framework, which is built on top of the popular variational auto-encoder (VAE) structure. Our NLAIC framework embeds non-local operations in the encoders and decoders for both image and latent feature probability information (known as hyperprior) to capture both local and global correlations, and apply attention mechanism to generate masks that are used to weigh the features for the image and hyperprior, which implicitly adapt bit allocation for different features based on their importance. Furthermore, both hyperpriors and spatial-channel neighbors of the latent features are used to improve entropy coding. The proposed model outperforms the existing methods on Kodak dataset, including learned (e.g., Balle2019, Balle2018) and conventional (e.g., BPG, JPEG2000, JPEG) image compression methods, for both PSNR and MS-SSIM distortion metrics.



### FoxNet: A Multi-face Alignment Method
- **Arxiv ID**: http://arxiv.org/abs/1904.09758v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09758v2)
- **Published**: 2019-04-22 07:52:04+00:00
- **Updated**: 2019-05-15 09:50:11+00:00
- **Authors**: Yuxiang Wu, Zehua Cheng, Bin Huang, Yiming Chen, Xinghui Zhu, Weiyang Wang
- **Comment**: Accepted by the 26th IEEE International Conference on Image
  Processing(ICIP2019)
- **Journal**: None
- **Summary**: Multi-face alignment aims to identify geometry structures of multiple faces in an image, and its performance is essential for the many practical tasks, such as face recognition, face tracking, and face animation. In this work, we present a fast bottom-up multi-face alignment approach, which can simultaneously localize multi-person facial landmarks with high precision.In more detail, our bottom-up architecture maps the landmarks to the high-dimensional space with which landmarks of all faces are represented. By clustering the features belonging to the same face, our approach can align the multi-person facial landmarks synchronously.Extensive experiments show that our method can achieve high performance in the multi-face landmark alignment task while our model is extremely fast. Moreover, we propose a new multi-face dataset to compare the speed and precision of bottom-up face alignment method with top-down methods. Our dataset is publicly available at https://github.com/AISAResearch/FoxNet



### Water-Filling: An Efficient Algorithm for Digitized Document Shadow Removal
- **Arxiv ID**: http://arxiv.org/abs/1904.09763v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09763v2)
- **Published**: 2019-04-22 08:01:27+00:00
- **Updated**: 2019-05-02 18:44:59+00:00
- **Authors**: Seungjun Jung, Muhammad Abul Hasan, Changick Kim
- **Comment**: Accepted at Asian Conference on Computer Vision (2018)
- **Journal**: None
- **Summary**: In this paper, we propose a novel algorithm to rectify illumination of the digitized documents by eliminating shading artifacts. Firstly, a topographic surface of an input digitized document is created using luminance value of each pixel. Then the shading artifact on the document is estimated by simulating an immersion process. The simulation of the immersion process is modeled using a novel diffusion equation with an iterative update rule. After estimating the shading artifacts, the digitized document is reconstructed using the Lambertian surface model. In order to evaluate the performance of the proposed algorithm, we conduct rigorous experiments on a set of digitized documents which is generated using smartphones under challenging lighting conditions. According to the experimental results, it is found that the proposed method produces promising illumination correction results and outperforms the results of the state-of-the-art methods.



### Deep Anchored Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09764v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09764v1)
- **Published**: 2019-04-22 08:07:00+00:00
- **Updated**: 2019-04-22 08:07:00+00:00
- **Authors**: Jiahui Huang, Kshitij Dwivedi, Gemma Roig
- **Comment**: This paper is accepted to 2019 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops (CVPRW)
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been proven to be extremely successful at solving computer vision tasks. State-of-the-art methods favor such deep network architectures for its accuracy performance, with the cost of having massive number of parameters and high weights redundancy. Previous works have studied how to prune such CNNs weights. In this paper, we go to another extreme and analyze the performance of a network stacked with a single convolution kernel across layers, as well as other weights sharing techniques. We name it Deep Anchored Convolutional Neural Network (DACNN). Sharing the same kernel weights across layers allows to reduce the model size tremendously, more precisely, the network is compressed in memory by a factor of L, where L is the desired depth of the network, disregarding the fully connected layer for prediction. The number of parameters in DACNN barely increases as the network grows deeper, which allows us to build deep DACNNs without any concern about memory costs. We also introduce a partial shared weights network (DACNN-mix) as well as an easy-plug-in module, coined regulators, to boost the performance of our architecture. We validated our idea on 3 datasets: CIFAR-10, CIFAR-100 and SVHN. Our results show that we can save massive amounts of memory with our model, while maintaining a high accuracy performance.



### Detecting retail products in situ using CNN without human effort labeling
- **Arxiv ID**: http://arxiv.org/abs/1904.09781v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09781v1)
- **Published**: 2019-04-22 09:20:30+00:00
- **Updated**: 2019-04-22 09:20:30+00:00
- **Authors**: Wei Yi, Yaoran Sun, Tao Ding, Sailing He
- **Comment**: None
- **Journal**: None
- **Summary**: CNN is a powerful tool for many computer vision tasks, achieving much better result than traditional methods. Since CNN has a very large capacity, training such a neural network often requires many data, but it is often expensive to obtain labeled images in real practice, especially for object detection, where collecting bounding box of every object in training set requires many human efforts. This is the case in detection of retail products where there can be many different categories. In this paper, we focus on applying CNN to detect 324-categories products in situ, while requiring no extra effort of labeling bounding box for any image. Our approach is based on an algorithm that extracts bounding box from in-vitro dataset and an algorithm to simulate occlusion. We have successfully shown the effectiveness and usefulness of our methods to build up a Faster RCNN detection model. Similar idea is also applicable in other scenarios.



### Fast User-Guided Video Object Segmentation by Interaction-and-Propagation Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09791v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09791v2)
- **Published**: 2019-04-22 10:17:46+00:00
- **Updated**: 2019-05-02 09:17:22+00:00
- **Authors**: Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a deep learning method for the interactive video object segmentation. Our method is built upon two core operations, interaction and propagation, and each operation is conducted by Convolutional Neural Networks. The two networks are connected both internally and externally so that the networks are trained jointly and interact with each other to solve the complex video object segmentation problem. We propose a new multi-round training scheme for the interactive video object segmentation so that the networks can learn how to understand the user's intention and update incorrect estimations during the training. At the testing time, our method produces high-quality results and also runs fast enough to work with users interactively. We evaluated the proposed method quantitatively on the interactive track benchmark at the DAVIS Challenge 2018. We outperformed other competing methods by a significant margin in both the speed and the accuracy. We also demonstrated that our method works well with real user interactions.



### PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.09793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.09793v1)
- **Published**: 2019-04-22 10:28:20+00:00
- **Updated**: 2019-04-22 10:28:20+00:00
- **Authors**: Wenxiao Zhang, Chunxia Xiao
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Point cloud based retrieval for place recognition is an emerging problem in vision field. The main challenge is how to find an efficient way to encode the local features into a discriminative global descriptor. In this paper, we propose a Point Contextual Attention Network (PCAN), which can predict the significance of each local point feature based on point context. Our network makes it possible to pay more attention to the task-relevent features when aggregating local features. Experiments on various benchmark datasets show that the proposed network can provide outperformance than current state-of-the-art approaches.



### blessing in disguise: Designing Robust Turing Test by Employing Algorithm Unrobustness
- **Arxiv ID**: http://arxiv.org/abs/1904.09804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09804v1)
- **Published**: 2019-04-22 11:41:30+00:00
- **Updated**: 2019-04-22 11:41:30+00:00
- **Authors**: Jiaming Zhang, Jitao Sang, Kaiyuan Xu, Shangxi Wu, Yongli Hu, Yanfeng Sun, Jian Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Turing test was originally proposed to examine whether machine's behavior is indistinguishable from a human. The most popular and practical Turing test is CAPTCHA, which is to discriminate algorithm from human by offering recognition-alike questions. The recent development of deep learning has significantly advanced the capability of algorithm in solving CAPTCHA questions, forcing CAPTCHA designers to increase question complexity. Instead of designing questions difficult for both algorithm and human, this study attempts to employ the limitations of algorithm to design robust CAPTCHA questions easily solvable to human. Specifically, our data analysis observes that human and algorithm demonstrates different vulnerability to visual distortions: adversarial perturbation is significantly annoying to algorithm yet friendly to human. We are motivated to employ adversarially perturbed images for robust CAPTCHA design in the context of character-based questions. Three modules of multi-target attack, ensemble adversarial training, and image preprocessing differentiable approximation are proposed to address the characteristics of character-based CAPTCHA cracking. Qualitative and quantitative experimental results demonstrate the effectiveness of the proposed solution. We hope this study can lead to the discussions around adversarial attack/defense in CAPTCHA design and also inspire the future attempts in employing algorithm limitation for practical usage.



### Machine Learning Based Analysis of Finnish World War II Photographers
- **Arxiv ID**: http://arxiv.org/abs/1904.09811v4
- **DOI**: 10.1109/ACCESS.2020.3014458
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09811v4)
- **Published**: 2019-04-22 12:07:03+00:00
- **Updated**: 2020-12-08 17:13:48+00:00
- **Authors**: Kateryna Chumachenko, Anssi Männistö, Alexandros Iosifidis, Jenni Raitoharju
- **Comment**: None
- **Journal**: IEEE Access (2020), Vol. 8, pp. 144184-144196
- **Summary**: In this paper, we demonstrate the benefits of using state-of-the-art machine learning methods in the analysis of historical photo archives. Specifically, we analyze prominent Finnish World War II photographers, who have captured high numbers of photographs in the publicly available Finnish Wartime Photograph Archive, which contains 160,000 photographs from Finnish Winter, Continuation, and Lapland Wars captures in 1939-1945. We were able to find some special characteristics for different photographers in terms of their typical photo content and framing (e.g., close-ups vs. overall shots, number of people). Furthermore, we managed to train a neural network that can successfully recognize the photographer from some of the photos, which shows that such photos are indeed characteristic for certain photographers. We further analyzed the similarities and differences between the photographers using the features extracted from the photographer classifier network. We make our annotations and analysis pipeline publicly available, in an effort to introduce this new research problem to the machine learning and computer vision communities and facilitate future research in historical and societal studies over the photo archives.



### Ship Instance Segmentation From Remote Sensing Images Using Sequence Local Context Module
- **Arxiv ID**: http://arxiv.org/abs/1904.09823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09823v1)
- **Published**: 2019-04-22 12:33:06+00:00
- **Updated**: 2019-04-22 12:33:06+00:00
- **Authors**: Yingchao Feng, Wenhui Diao, Zhonghan Chang, Menglong Yan, Xian Sun, Xin Gao
- **Comment**: 4 pages, 5 figures, IEEE Geoscience and Remote Sensing Society 2019
- **Journal**: None
- **Summary**: The performance of object instance segmentation in remote sensing images has been greatly improved through the introduction of many landmark frameworks based on convolutional neural network. However, the object densely issue still affects the accuracy of such segmentation frameworks. Objects of the same class are easily confused, which is most likely due to the close docking between objects. We think context information is critical to address this issue. So, we propose a novel framework called SLCMASK-Net, in which a sequence local context module (SLC) is introduced to avoid confusion between objects of the same class. The SLC module applies a sequence of dilation convolution blocks to progressively learn multi-scale context information in the mask branch. Besides, we try to add SLC module to different locations in our framework and experiment with the effect of different parameter settings. Comparative experiments are conducted on remote sensing images acquired by QuickBird with a resolution of $0.5m-1m$ and the results show that the proposed method achieves state-of-the-art performance.



### Stochastic Region Pooling: Make Attention More Expressive
- **Arxiv ID**: http://arxiv.org/abs/1904.09853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09853v1)
- **Published**: 2019-04-22 13:17:19+00:00
- **Updated**: 2019-04-22 13:17:19+00:00
- **Authors**: Mingnan Luo, Guihua Wen, Yang Hu, Dan Dai, Yingxue Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Global Average Pooling (GAP) is used by default on the channel-wise attention mechanism to extract channel descriptors. However, the simple global aggregation method of GAP is easy to make the channel descriptors have homogeneity, which weakens the detail distinction between feature maps, thus affecting the performance of the attention mechanism. In this work, we propose a novel method for channel-wise attention network, called Stochastic Region Pooling (SRP), which makes the channel descriptors more representative and diversity by encouraging the feature map to have more or wider important feature responses. Also, SRP is the general method for the attention mechanisms without any additional parameters or computation. It can be widely applied to attention networks without modifying the network structure. Experimental results on image recognition datasets including CIAFR-10/100, ImageNet and three Fine-grained datasets (CUB-200-2011, Stanford Cars and Stanford Dogs) show that SRP brings the significant improvements of the performance over efficient CNNs and achieves the state-of-the-art results.



### TextCohesion: Detecting Text for Arbitrary Shapes
- **Arxiv ID**: http://arxiv.org/abs/1904.12640v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12640v2)
- **Published**: 2019-04-22 13:21:38+00:00
- **Updated**: 2019-04-30 07:37:56+00:00
- **Authors**: Weijia Wu, Jici Xing, Hong Zhou
- **Comment**: Scene Text Detection Instance Segmentation
- **Journal**: None
- **Summary**: In this paper, we propose a pixel-wise method named TextCohesion for scene text detection, which splits a text instance into five key components: a Text Skeleton and four Directional Pixel Regions. These components are easier to handle than the entire text instance. A confidence scoring mechanism is designed to filter characters that are similar to text. Our method can integrate text contexts intensively when backgrounds are complex. Experiments on two curved challenging benchmarks demonstrate that TextCohesion outperforms state-of-the-art methods, achieving the F-measure of 84.6% on Total-Text and bfseries86.3% on SCUT-CTW1500.



### Learning to Calibrate Straight Lines for Fisheye Image Rectification
- **Arxiv ID**: http://arxiv.org/abs/1904.09856v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09856v2)
- **Published**: 2019-04-22 13:25:36+00:00
- **Updated**: 2019-05-01 13:11:51+00:00
- **Authors**: Zhucun Xue, Nan Xue, Gui-Song Xia, Weiming Shen
- **Comment**: None
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2019
- **Summary**: This paper presents a new deep-learning based method to simultaneously calibrate the intrinsic parameters of fisheye lens and rectify the distorted images. Assuming that the distorted lines generated by fisheye projection should be straight after rectification, we propose a novel deep neural network to impose explicit geometry constraints onto processes of the fisheye lens calibration and the distorted image rectification. In addition, considering the nonlinearity of distortion distribution in fisheye images, the proposed network fully exploits multi-scale perception to equalize the rectification effects on the whole image. To train and evaluate the proposed model, we also create a new largescale dataset labeled with corresponding distortion parameters and well-annotated distorted lines. Compared with the state-of-the-art methods, our model achieves the best published rectification quality and the most accurate estimation of distortion parameters on a large set of synthetic and real fisheye images.



### Real-time Intent Prediction of Pedestrians for Autonomous Ground Vehicles via Spatio-Temporal DenseNet
- **Arxiv ID**: http://arxiv.org/abs/1904.09862v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.09862v1)
- **Published**: 2019-04-22 13:30:34+00:00
- **Updated**: 2019-04-22 13:30:34+00:00
- **Authors**: Khaled Saleh, Mohammed Hossny, Saeid Nahavandi
- **Comment**: Accepted to ICRA 2019
- **Journal**: None
- **Summary**: Understanding the behaviors and intentions of humans are one of the main challenges autonomous ground vehicles still faced with. More specifically, when it comes to complex environments such as urban traffic scenes, inferring the intentions and actions of vulnerable road users such as pedestrians become even harder. In this paper, we address the problem of intent action prediction of pedestrians in urban traffic environments using only image sequences from a monocular RGB camera. We propose a real-time framework that can accurately detect, track and predict the intended actions of pedestrians based on a tracking-by-detection technique in conjunction with a novel spatio-temporal DenseNet model. We trained and evaluated our framework based on real data collected from urban traffic environments. Our framework has shown resilient and competitive results in comparison to other baseline approaches. Overall, we achieved an average precision score of 84.76% with a real-time performance at 20 FPS.



### Towards Learning of Filter-Level Heterogeneous Compression of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09872v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.09872v4)
- **Published**: 2019-04-22 13:43:34+00:00
- **Updated**: 2019-09-26 14:45:46+00:00
- **Authors**: Yochai Zur, Chaim Baskin, Evgenii Zheltonozhskii, Brian Chmiel, Itay Evron, Alex M. Bronstein, Avi Mendelson
- **Comment**: Accepted to ICML Workshop on AutoML 2019
- **Journal**: None
- **Summary**: Recently, deep learning has become a de facto standard in machine learning with convolutional neural networks (CNNs) demonstrating spectacular success on a wide variety of tasks. However, CNNs are typically very demanding computationally at inference time. One of the ways to alleviate this burden on certain hardware platforms is quantization relying on the use of low-precision arithmetic representation for the weights and the activations. Another popular method is the pruning of the number of filters in each layer. While mainstream deep learning methods train the neural networks weights while keeping the network architecture fixed, the emerging neural architecture search (NAS) techniques make the latter also amenable to training. In this paper, we formulate optimal arithmetic bit length allocation and neural network pruning as a NAS problem, searching for the configurations satisfying a computational complexity budget while maximizing the accuracy. We use a differentiable search method based on the continuous relaxation of the search space proposed by Liu et al. (arXiv:1806.09055). We show, by grid search, that heterogeneous quantized networks suffer from a high variance which renders the benefit of the search questionable. For pruning, improvement over homogeneous cases is possible, but it is still challenging to find those configurations with the proposed method. The code is publicly available at https://github.com/yochaiz/Slimmable and https://github.com/yochaiz/darts-UNIQ



### You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions
- **Arxiv ID**: http://arxiv.org/abs/1904.09882v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09882v2)
- **Published**: 2019-04-22 13:58:49+00:00
- **Updated**: 2020-03-28 03:56:28+00:00
- **Authors**: Evonne Ng, Donglai Xiang, Hanbyul Joo, Kristen Grauman
- **Comment**: None
- **Journal**: None
- **Summary**: The body pose of a person wearing a camera is of great interest for applications in augmented reality, healthcare, and robotics, yet much of the person's body is out of view for a typical wearable camera. We propose a learning-based approach to estimate the camera wearer's 3D body pose from egocentric video sequences. Our key insight is to leverage interactions with another person---whose body pose we can directly observe---as a signal inherently linked to the body pose of the first-person subject. We show that since interactions between individuals often induce a well-ordered series of back-and-forth responses, it is possible to learn a temporal model of the interlinked poses even though one party is largely out of view. We demonstrate our idea on a variety of domains with dyadic interaction and show the substantial impact on egocentric body pose estimation, which improves the state of the art. Video results are available at http://vision.cs.utexas.edu/projects/you2me/



### City-scale Road Extraction from Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1904.09901v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09901v2)
- **Published**: 2019-04-22 14:36:57+00:00
- **Updated**: 2019-07-22 16:00:47+00:00
- **Authors**: Adam Van Etten
- **Comment**: 6 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: Automated road network extraction from remote sensing imagery remains a significant challenge despite its importance in a broad array of applications. To this end, we leverage recent open source advances and the high quality SpaceNet dataset to explore road network extraction at scale, an approach we call City-scale Road Extraction from Satellite Imagery (CRESI). Specifically, we create an algorithm to extract road networks directly from imagery over city-scale regions, which can subsequently be used for routing purposes. We quantify the performance of our algorithm with the APLS and TOPO graph-theoretic metrics over a diverse 608 square kilometer test area covering four cities. We find an aggregate score of APLS = 0.73, and a TOPO score of 0.58 (a significant improvement over existing methods). Inference speed is 160 square kilometers per hour on modest hardware. Finally, we demonstrate that one can use the extracted road network for any number of applications, such as optimized routing.



### Attention Augmented Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09925v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09925v5)
- **Published**: 2019-04-22 15:31:15+00:00
- **Updated**: 2020-09-09 18:52:40+00:00
- **Authors**: Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a $1.3\%$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.



### Late or Earlier Information Fusion from Depth and Spectral Data? Large-Scale Digital Surface Model Refinement by Hybrid-cGAN
- **Arxiv ID**: http://arxiv.org/abs/1904.09935v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09935v1)
- **Published**: 2019-04-22 15:51:18+00:00
- **Updated**: 2019-04-22 15:51:18+00:00
- **Authors**: Ksenia Bittner, Marco Körner, Peter Reinartz
- **Comment**: 8 pages, This work was accepted to be presented at the IEEE/ISPRS
  Workshop on Large Scale Computer Vision for Remote Sensing Imagery
  (EarthVision) to be held at the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR) 2019
- **Journal**: None
- **Summary**: We present the workflow of a DSM refinement methodology using a Hybrid-cGAN where the generative part consists of two encoders and a common decoder which blends the spectral and height information within one network. The inputs to the Hybrid-cGAN are single-channel photogrammetric DSMs with continuous values and single-channel pan-chromatic (PAN) half-meter resolution satellite images. Experimental results demonstrate that the earlier information fusion from data with different physical meanings helps to propagate fine details and complete an inaccurate or missing 3D information about building forms. Moreover, it improves the building boundaries making them more rectilinear.



### Tripping through time: Efficient Localization of Activities in Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.09936v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09936v5)
- **Published**: 2019-04-22 15:53:13+00:00
- **Updated**: 2020-08-18 16:56:23+00:00
- **Authors**: Meera Hahn, Asim Kadav, James M. Rehg, Hans Peter Graf
- **Comment**: Presented at BMVC, 2020
- **Journal**: None
- **Summary**: Localizing moments in untrimmed videos via language queries is a new and interesting task that requires the ability to accurately ground language into video. Previous works have approached this task by processing the entire video, often more than once, to localize relevant activities. In the real world applications of this approach, such as video surveillance, efficiency is a key system requirement. In this paper, we present TripNet, an end-to-end system that uses a gated attention architecture to model fine-grained textual and visual representations in order to align text and video content. Furthermore, TripNet uses reinforcement learning to efficiently localize relevant activity clips in long videos, by learning how to intelligently skip around the video. It extracts visual features for few frames to perform activity classification. In our evaluation over Charades-STA, ActivityNet Captions and the TACoS dataset, we find that TripNet achieves high accuracy and saves processing time by only looking at 32-41% of the entire video.



### Semantic Relationships Guided Representation Learning for Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09939v1)
- **Published**: 2019-04-22 16:26:20+00:00
- **Updated**: 2019-04-22 16:26:20+00:00
- **Authors**: Guanbin Li, Xin Zhu, Yirui Zeng, Qing Wang, Liang Lin
- **Comment**: Accepted by AAAI2019 as oral presentation
- **Journal**: None
- **Summary**: Facial action unit (AU) recognition is a crucial task for facial expressions analysis and has attracted extensive attention in the field of artificial intelligence and computer vision. Existing works have either focused on designing or learning complex regional feature representations, or delved into various types of AU relationship modeling. Albeit with varying degrees of progress, it is still arduous for existing methods to handle complex situations. In this paper, we investigate how to integrate the semantic relationship propagation between AUs in a deep neural network framework to enhance the feature representation of facial regions, and propose an AU semantic relationship embedded representation learning (SRERL) framework. Specifically, by analyzing the symbiosis and mutual exclusion of AUs in various facial expressions, we organize the facial AUs in the form of structured knowledge-graph and integrate a Gated Graph Neural Network (GGNN) in a multi-scale CNN framework to propagate node information through the graph for generating enhanced AU representation. As the learned feature involves both the appearance characteristics and the AU relationship reasoning, the proposed model is more robust and can cope with more challenging cases, e.g., illumination change and partial occlusion. Extensive experiments on the two public benchmarks demonstrate that our method outperforms the previous work and achieves state of the art performance.



### Inner-Imaging Networks: Put Lenses into Convolutional Structure
- **Arxiv ID**: http://arxiv.org/abs/1904.12639v3
- **DOI**: 10.1109/TCYB.2020.3034605
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12639v3)
- **Published**: 2019-04-22 16:44:10+00:00
- **Updated**: 2021-08-27 21:19:16+00:00
- **Authors**: Yang Hu, Guihua Wen, Mingnan Luo, Dan Dai, Wenming Cao, Zhiwen Yu, Wendy Hall
- **Comment**: 14 pages, 10 figures, formal edition on IEEE Transactions on
  Cybernetics, 2021
- **Journal**: None
- **Summary**: Despite the tremendous success in computer vision, deep convolutional networks suffer from serious computation costs and redundancies. Although previous works address this issue by enhancing diversities of filters, they have not considered the complementarity and the completeness of the internal structure of the convolutional network. To deal with these problems, a novel Inner-Imaging architecture is proposed in this paper, which allows relationships between channels to meet the above requirement. Specifically, we organize the channel signal points in groups using convolutional kernels to model both the intra-group and inter-group relationships simultaneously. The convolutional filter is a powerful tool for modeling spatial relations and organizing grouped signals, so the proposed methods map the channel signals onto a pseudo-image, like putting a lens into convolution internal structure. Consequently, not only the diversity of channels is increased, but also the complementarity and completeness can be explicitly enhanced. The proposed architecture is lightweight and easy to be implemented. It provides an efficient self-organization strategy for convolutional networks so as to improve their efficiency and performance. Extensive experiments are conducted on multiple benchmark image recognition data sets including CIFAR, SVHN and ImageNet. Experimental results verify the effectiveness of the Inner-Imaging mechanism with the most popular convolutional networks as the backbones.



### Synaptic Partner Assignment Using Attentional Voxel Association Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09947v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09947v2)
- **Published**: 2019-04-22 16:49:41+00:00
- **Updated**: 2019-11-22 01:39:03+00:00
- **Authors**: Nicholas Turner, Kisuk Lee, Ran Lu, Jingpeng Wu, Dodam Ih, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: Connectomics aims to recover a complete set of synaptic connections within a dataset imaged by volume electron microscopy. Many systems have been proposed for locating synapses, and recent research has included a way to identify the synaptic partners that communicate at a synaptic cleft. We re-frame the problem of identifying synaptic partners as directly generating the mask of the synaptic partners from a given cleft. We train a convolutional network to perform this task. The network takes the local image context and a binary mask representing a single cleft as input. It is trained to produce two binary output masks: one which labels the voxels of the presynaptic partner within the input image, and another similar labeling for the postsynaptic partner. The cleft mask acts as an attentional gating signal for the network. We find that an implementation of this approach performs well on a dataset of mouse somatosensory cortex, and evaluate it as part of a combined system to predict both clefts and connections.



### Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids
- **Arxiv ID**: http://arxiv.org/abs/1904.09970v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09970v1)
- **Published**: 2019-04-22 17:54:06+00:00
- **Updated**: 2019-04-22 17:54:06+00:00
- **Authors**: Despoina Paschalidou, Ali Osman Ulusoy, Andreas Geiger
- **Comment**: CVPR 2019 Camera Ready. Project
  Page:https://github.com/paschalidoud/superquadric_parsing
- **Journal**: None
- **Summary**: Abstracting complex 3D shapes with parsimonious part-based representations has been a long standing goal in computer vision. This paper presents a learning-based solution to this problem which goes beyond the traditional 3D cuboid representation by exploiting superquadrics as atomic elements. We demonstrate that superquadrics lead to more expressive 3D scene parses while being easier to learn than 3D cuboid representations. Moreover, we provide an analytical solution to the Chamfer loss which avoids the need for computational expensive reinforcement learning or iterative prediction. Our model learns to parse 3D objects into consistent superquadric representations without supervision. Results on various ShapeNet categories as well as the SURREAL human body dataset demonstrate the flexibility of our model in capturing fine details and complex poses that could not have been modelled using cuboids.



### Linked Dynamic Graph CNN: Learning on Point Cloud via Linking Hierarchical Features
- **Arxiv ID**: http://arxiv.org/abs/1904.10014v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10014v2)
- **Published**: 2019-04-22 18:16:34+00:00
- **Updated**: 2019-08-06 03:56:20+00:00
- **Authors**: Kuangen Zhang, Ming Hao, Jing Wang, Clarence W. de Silva, Chenglong Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning on point cloud is eagerly in demand because the point cloud is a common type of geometric data and can aid robots to understand environments robustly. However, the point cloud is sparse, unstructured, and unordered, which cannot be recognized accurately by a traditional convolutional neural network (CNN) nor a recurrent neural network (RNN). Fortunately, a graph convolutional neural network (Graph CNN) can process sparse and unordered data. Hence, we propose a linked dynamic graph CNN (LDGCNN) to classify and segment point cloud directly in this paper. We remove the transformation network, link hierarchical features from dynamic graphs, freeze feature extractor, and retrain the classifier to increase the performance of LDGCNN. We explain our network using theoretical analysis and visualization. Through experiments, we show that the proposed LDGCNN achieves state-of-art performance on two standard datasets: ModelNet40 and ShapeNet.



### The Profiling Potential of Computer Vision and the Challenge of Computational Empiricism
- **Arxiv ID**: http://arxiv.org/abs/1904.10016v1
- **DOI**: 10.1145/3287560.3287568
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10016v1)
- **Published**: 2019-04-22 18:20:38+00:00
- **Updated**: 2019-04-22 18:20:38+00:00
- **Authors**: Jake Goldenfein
- **Comment**: None
- **Journal**: Proceedings of the 2019 Conference on Fairness, Accountability,
  and Transparency
- **Summary**: Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.



### Leveraging Orientation for Weakly Supervised Object Detection with Application to Firearm Localization
- **Arxiv ID**: http://arxiv.org/abs/1904.10032v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10032v2)
- **Published**: 2019-04-22 18:56:43+00:00
- **Updated**: 2021-01-29 19:40:10+00:00
- **Authors**: Javed Iqbal, Muhammad Akhtar Munir, Arif Mahmood, Afsheen Rafaqat Ali, Mohsen Ali
- **Comment**: Accepted for Publication in Neurocomputing
- **Journal**: None
- **Summary**: Automatic detection of firearms is important for enhancing the security and safety of people, however, it is a challenging task owing to the wide variations in shape, size, and appearance of firearms. Also, most of the generic object detectors process axis-aligned rectangular areas though, a thin and long rifle may actually cover only a small percentage of that area and the rest may contain irrelevant details suppressing the required object signatures. To handle these challenges, we propose a weakly supervised Orientation Aware Object Detection (OAOD) algorithm which learns to detect oriented object bounding boxes (OBB) while using AxisAligned Bounding Boxes (AABB) for training. The proposed OAOD is different from the existing oriented object detectors which strictly require OBB during training which may not always be present. The goal of training on AABB and detection of OBB is achieved by employing a multistage scheme, with Stage-1 predicting the AABB and Stage-2 predicting OBB. In-between the two stages, the oriented proposal generation module along with the object aligned RoI pooling is designed to extract features based on the predicted orientation and to make these features orientation invariant. A diverse and challenging dataset consisting of eleven thousand images is also proposed for firearm detection which is manually annotated for firearm classification and localization. The proposed ITU Firearm dataset (ITUF) contains a wide range of guns and rifles. The OAOD algorithm is evaluated on the ITUF dataset and compared with current state-of-the-art object detectors, including fully supervised oriented object detectors. OAOD has outperformed both types of object detectors with a significant margin. The experimental results (mAP: 88.3 on AABB & mAP: 77.5 on OBB) demonstrate the effectiveness of the proposed algorithm for firearm detection.



### LBS Autoencoder: Self-supervised Fitting of Articulated Meshes to Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.10037v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.10037v1)
- **Published**: 2019-04-22 19:12:18+00:00
- **Updated**: 2019-04-22 19:12:18+00:00
- **Authors**: Chun-Liang Li, Tomas Simon, Jason Saragih, Barnabás Póczos, Yaser Sheikh
- **Comment**: In the Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2019)
- **Journal**: None
- **Summary**: We present LBS-AE; a self-supervised autoencoding algorithm for fitting articulated mesh models to point clouds. As input, we take a sequence of point clouds to be registered as well as an artist-rigged mesh, i.e. a template mesh equipped with a linear-blend skinning (LBS) deformation space parameterized by a skeleton hierarchy. As output, we learn an LBS-based autoencoder that produces registered meshes from the input point clouds. To bridge the gap between the artist-defined geometry and the captured point clouds, our autoencoder models pose-dependent deviations from the template geometry. During training, instead of using explicit correspondences, such as key points or pose supervision, our method leverages LBS deformations to bootstrap the learning process. To avoid poor local minima from erroneous point-to-point correspondences, we utilize a structured Chamfer distance based on part-segmentations, which are learned concurrently using self-supervision. We demonstrate qualitative results on real captured hands, and report quantitative evaluations on the FAUST benchmark for body registration. Our method achieves performance that is superior to other unsupervised approaches and comparable to methods using supervised examples.



### UDFNet: Unsupervised Disparity Fusion with Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.10044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10044v1)
- **Published**: 2019-04-22 19:40:04+00:00
- **Updated**: 2019-04-22 19:40:04+00:00
- **Authors**: Can Pu, Robert B. Fisher
- **Comment**: 13 pages. arXiv admin note: text overlap with arXiv:1803.06657
- **Journal**: None
- **Summary**: Existing disparity fusion methods based on deep learning achieve state-of-the-art performance, but they require ground truth disparity data to train. As far as I know, this is the first time an unsupervised disparity fusion not using ground truth disparity data has been proposed. In this paper, a mathematical model for disparity fusion is proposed to guide an adversarial network to train effectively without ground truth disparity data. The initial disparity maps are inputted from the left view along with auxiliary information (gradient, left & right intensity image) into the refiner and the refiner is trained to output the refined disparity map registered on the left view. The refined left disparity map and left intensity image are used to reconstruct a fake right intensity image. Finally, the fake and real right intensity images (from the right stereo vision camera) are fed into the discriminator. In the model, the refiner is trained to output a refined disparity value close to the weighted sum of the disparity inputs for global initialisation. Then, three refinement principles are adopted to refine the results further. (1) The reconstructed intensity error between the fake and real right intensity image is minimised. (2) The similarities between the fake and real right image in different receptive fields are maximised. (3) The refined disparity map is smoothed based on the corresponding intensity image. The adversarial networks' architectures are effective for the fusion task. The fusion time using the proposed network is small. The network can achieve 90 fps using Nvidia Geforce GTX 1080Ti on the Kitti2015 dataset when the input resolution is 1242 * 375 (Width * Height) without downsampling and cropping. The accuracy of this work is equal to (or better than) the state-of-the-art supervised methods.



### Learning Feature-to-Feature Translator by Alternating Back-Propagation for Generative Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.10056v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10056v3)
- **Published**: 2019-04-22 20:30:21+00:00
- **Updated**: 2019-11-11 04:49:15+00:00
- **Authors**: Yizhe Zhu, Jianwen Xie, Bingchen Liu, Ahmed Elgammal
- **Comment**: accepted to ICCV'19
- **Journal**: None
- **Summary**: We investigate learning feature-to-feature translator networks by alternating back-propagation as a general-purpose solution to zero-shot learning (ZSL) problems. It is a generative model-based ZSL framework. In contrast to models based on generative adversarial networks (GAN) or variational autoencoders (VAE) that require auxiliary networks to assist the training, our model consists of a single conditional generator that maps class-level semantic features and Gaussian white noise vector accounting for instance-level latent factors to visual features, and is trained by maximum likelihood estimation. The training process is a simple yet effective alternating back-propagation process that iterates the following two steps: (i) the inferential back-propagation to infer the latent factors of each observed example, and (ii) the learning back-propagation to update the model parameters. We show that, with slight modifications, our model is capable of learning from incomplete visual features for ZSL. We conduct extensive comparisons with existing generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only ZSL performance but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art methods by a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings, respectively.



### Bold Hearts Team Description for RoboCup 2019 (Humanoid Kid Size League)
- **Arxiv ID**: http://arxiv.org/abs/1904.10066v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1904.10066v1)
- **Published**: 2019-04-22 21:10:25+00:00
- **Updated**: 2019-04-22 21:10:25+00:00
- **Authors**: Marcus M. Scheunemann, Sander G. van Dijk, Rebecca Miko, Daniel Barry, George M. Evans, Alessandra Rossi, Daniel Polani
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We participated in the RoboCup 2018 competition in Montreal with our newly developed BoldBot based on the Darwin-OP and mostly self-printed custom parts. This paper is about the lessons learnt from that competition and further developments for the RoboCup 2019 competition. Firstly, we briefly introduce the team along with an overview of past achievements. We then present a simple, standalone 2D simulator we use for simplifying the entry for new members with making basic RoboCup concepts quickly accessible. We describe our approach for semantic-segmentation for our vision used in the 2018 competition, which replaced the lookup-table (LUT) implementation we had before. We also discuss the extra structural support we plan to add to the printed parts of the BoldBot and our transition to ROS 2 as our new middleware. Lastly, we will present a collection of open-source contributions of our team.



### Using Videos to Evaluate Image Model Robustness
- **Arxiv ID**: http://arxiv.org/abs/1904.10076v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.10076v3)
- **Published**: 2019-04-22 22:13:22+00:00
- **Updated**: 2019-08-29 23:18:47+00:00
- **Authors**: Keren Gu, Brandon Yang, Jiquan Ngiam, Quoc Le, Jonathon Shlens
- **Comment**: Video Robustness Dataset included in directory
- **Journal**: None
- **Summary**: Human visual systems are robust to a wide range of image transformations that are challenging for artificial networks. We present the first study of image model robustness to the minute transformations found across video frames, which we term "natural robustness". Compared to previous studies on adversarial examples and synthetic distortions, natural robustness captures a more diverse set of common image transformations that occur in the natural environment. Our study across a dozen model architectures shows that more accurate models are more robust to natural transformations, and that robustness to synthetic color distortions is a good proxy for natural robustness. In examining brittleness in videos, we find that majority of the brittleness found in videos lies outside the typical definition of adversarial examples (99.9\%). Finally, we investigate training techniques to reduce brittleness and find that no single technique systematically improves natural robustness across twelve tested architectures.



### Adaptive Transform Domain Image Super-resolution Via Orthogonally Regularized Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.10082v1
- **DOI**: 10.1109/TIP.2019.2913500
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10082v1)
- **Published**: 2019-04-22 22:29:52+00:00
- **Updated**: 2019-04-22 22:29:52+00:00
- **Authors**: Tiantong Guo, Hojjat S. Mousavi, Vishal Monga
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods, in particular, trained Convolutional Neural Networks (CNN) have recently been shown to produce compelling results for single image Super-Resolution (SR). Invariably, a CNN is learned to map the Low Resolution (LR) image to its corresponding High Resolution (HR) version in the spatial domain. We propose a novel network structure for learning the SR mapping function in an image transform domain, specifically the Discrete Cosine Transform (DCT). As the first contribution, we show that DCT can be integrated into the network structure as a Convolutional DCT (CDCT) layer. With the CDCT layer, we construct the DCT Deep SR (DCT-DSR) network. We further extend the DCT-DSR to allow the CDCT layer to become trainable (i.e., optimizable). Because this layer represents an image transform, we enforce pairwise orthogonality constraints and newly formulated complexity order constraints on the individual basis functions/filters. This Orthogonally Regularized Deep SR network (ORDSR) simplifies the SR task by taking advantage of image transform domain while adapting the design of transform basis to the training image set. Experimental results show ORDSR achieves state-of-the-art SR image quality with fewer parameters than most of the deep CNN methods. A particular success of ORDSR is in overcoming the artifacts introduced by bicubic interpolation. A key burden of deep SR has been identified as the requirement of generous training LR and HR image pairs; ORSDR exhibits a much more graceful degradation as training size is reduced with significant benefits in the regime of limited training. Analysis of memory and computation requirements confirms that ORDSR can allow for a more efficient network with faster inference.



### Tertiary Eye Movement Classification by a Hybrid Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1904.10085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10085v1)
- **Published**: 2019-04-22 22:51:55+00:00
- **Updated**: 2019-04-22 22:51:55+00:00
- **Authors**: Samuel-Hunter Berndt, Douglas Kirkpatrick, Timothy Taviano, Oleg Komogortsev
- **Comment**: 10 pages, 18 figures, 3 tables
- **Journal**: None
- **Summary**: The proper classification of major eye movements, saccades, fixations, and smooth pursuits, remains essential to utilizing eye-tracking data. There is difficulty in separating out smooth pursuits from the other behavior types, particularly from fixations. To this end, we propose a new offline algorithm, I-VDT-HMM, for tertiary classification of eye movements. The algorithm combines the simplicity of two foundational algorithms, I-VT and I-DT, as has been implemented in I-VDT, with the statistical predictive power of the Viterbi algorithm. We evaluate the fitness across a dataset of eight eye movement records at eight sampling rates gathered from previous research, with a comparison to the current state-of-the-art using the proposed quantitative and qualitative behavioral scores. The proposed algorithm achieves promising results in clean high sampling frequency data and with slight modifications could show similar results with lower quality data. Though, the statistical aspect of the algorithm comes at a cost of classification time.



### DirectShape: Direct Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.10097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10097v2)
- **Published**: 2019-04-22 23:58:25+00:00
- **Updated**: 2020-03-09 20:50:08+00:00
- **Authors**: Rui Wang, Nan Yang, Joerg Stueckler, Daniel Cremers
- **Comment**: Accepted by IEEE International Conference on Robotics and Automation
  (ICRA) 2020
- **Journal**: None
- **Summary**: Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.



