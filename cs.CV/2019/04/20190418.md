# Arxiv Papers in cs.CV on 2019-04-18
### Generative Model for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.08542v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08542v1)
- **Published**: 2019-04-18 00:11:04+00:00
- **Updated**: 2019-04-18 00:11:04+00:00
- **Authors**: Vinay Kumar Verma, Aakansha Mishra, Ashish Mishra, Piyush Rai
- **Comment**: Accepted at CVPR-Workshop 2019
- **Journal**: None
- **Summary**: We present a probabilistic model for Sketch-Based Image Retrieval (SBIR) where, at retrieval time, we are given sketches from novel classes, that were not present at training time. Existing SBIR methods, most of which rely on learning class-wise correspondences between sketches and images, typically work well only for previously seen sketch classes, and result in poor retrieval performance on novel classes. To address this, we propose a generative model that learns to generate images, conditioned on a given novel class sketch. This enables us to reduce the SBIR problem to a standard image-to-image search problem. Our model is based on an inverse auto-regressive flow based variational autoencoder, with a feedback mechanism to ensure robust image generation. We evaluate our model on two very challenging datasets, Sketchy, and TU Berlin, with novel train-test split. The proposed approach significantly outperforms various baselines on both the datasets.



### Fast Single Image Dehazing via Multilevel Wavelet Transform based Optimization
- **Arxiv ID**: http://arxiv.org/abs/1904.08573v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, G.1.6
- **Links**: [PDF](http://arxiv.org/pdf/1904.08573v1)
- **Published**: 2019-04-18 02:38:07+00:00
- **Updated**: 2019-04-18 02:38:07+00:00
- **Authors**: Jiaxi He, Frank Z. Xing, Ran Yang, Cishen Zhang
- **Comment**: 23 pages, 13 figures
- **Journal**: None
- **Summary**: The quality of images captured in outdoor environments can be affected by poor weather conditions such as fog, dust, and atmospheric scattering of other particles. This problem can bring extra challenges to high-level computer vision tasks like image segmentation and object detection. However, previous studies on image dehazing suffer from a huge computational workload and corruption of the original image, such as over-saturation and halos. In this paper, we present a novel image dehazing approach based on the optical model for haze images and regularized optimization. Specifically, we convert the non-convex, bilinear problem concerning the unknown haze-free image and light transmission distribution to a convex, linear optimization problem by estimating the atmosphere light constant. Our method is further accelerated by introducing a multilevel Haar wavelet transform. The optimization, instead, is applied to the low frequency sub-band decomposition of the original image. This dimension reduction significantly improves the processing speed of our method and exhibits the potential for real-time applications. Experimental results show that our approach outperforms state-of-the-art dehazing algorithms in terms of both image reconstruction quality and computational efficiency. For implementation details, source code can be publicly accessed via http://github.com/JiaxiHe/Image-and-Video-Dehazing.



### Deep AutoEncoder-based Lossy Geometry Compression for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1905.03691v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03691v1)
- **Published**: 2019-04-18 02:44:50+00:00
- **Updated**: 2019-04-18 02:44:50+00:00
- **Authors**: Wei Yan, Yiting shao, Shan Liu, Thomas H Li, Zhu Li, Ge Li
- **Comment**: None
- **Journal**: None
- **Summary**: Point cloud is a fundamental 3D representation which is widely used in real world applications such as autonomous driving. As a newly-developed media format which is characterized by complexity and irregularity, point cloud creates a need for compression algorithms which are more flexible than existing codecs. Recently, autoencoders(AEs) have shown their effectiveness in many visual analysis tasks as well as image compression, which inspires us to employ it in point cloud compression. In this paper, we propose a general autoencoder-based architecture for lossy geometry point cloud compression. To the best of our knowledge, it is the first autoencoder-based geometry compression codec that directly takes point clouds as input rather than voxel grids or collections of images. Compared with handcrafted codecs, this approach adapts much more quickly to previously unseen media contents and media formats, meanwhile achieving competitive performance. Our architecture consists of a pointnet-based encoder, a uniform quantizer, an entropy estimation block and a nonlinear synthesis transformation module. In lossy geometry compression of point cloud, results show that the proposed method outperforms the test model for categories 1 and 3 (TMC13) published by MPEG-3DG group on the 125th meeting, and on average a 73.15\% BD-rate gain is achieved.



### Road Crack Detection Using Deep Convolutional Neural Network and Adaptive Thresholding
- **Arxiv ID**: http://arxiv.org/abs/1904.08582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08582v1)
- **Published**: 2019-04-18 03:38:52+00:00
- **Updated**: 2019-04-18 03:38:52+00:00
- **Authors**: Rui Fan, Mohammud Junaid Bocus, Yilong Zhu, Jianhao Jiao, Li Wang, Fulong Ma, Shanshan Cheng, Ming Liu
- **Comment**: 6 pages, 8 figures, 2019 IEEE Intelligent Vehicles Symposium
- **Journal**: None
- **Summary**: Crack is one of the most common road distresses which may pose road safety hazards. Generally, crack detection is performed by either certified inspectors or structural engineers. This task is, however, time-consuming, subjective and labor-intensive. In this paper, we propose a novel road crack detection algorithm based on deep learning and adaptive image segmentation. Firstly, a deep convolutional neural network is trained to determine whether an image contains cracks or not. The images containing cracks are then smoothed using bilateral filtering, which greatly minimizes the number of noisy pixels. Finally, we utilize an adaptive thresholding method to extract the cracks from road surface. The experimental results illustrate that our network can classify images with an accuracy of 99.92%, and the cracks can be successfully extracted from the images using our proposed thresholding algorithm.



### Catch Me If You Can
- **Arxiv ID**: http://arxiv.org/abs/1904.12627v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12627v1)
- **Published**: 2019-04-18 04:36:54+00:00
- **Updated**: 2019-04-18 04:36:54+00:00
- **Authors**: Antoine Viscardi, Casey Juanxi Li, Thomas Hollis
- **Comment**: None
- **Journal**: None
- **Summary**: As advances in signature recognition have reached a new plateau of performance at around 2% error rate, it is interesting to investigate alternative approaches. The approach detailed in this paper looks at using Variational Auto-Encoders (VAEs) to learn a latent space representation of genuine signatures. This is then used to pass unlabelled signatures such that only the genuine ones will successfully be reconstructed by the VAE. This latent space representation and the reconstruction loss is subsequently used by random forest and kNN classifiers for prediction. Subsequently, VAE disentanglement and the possibility of posterior collapse are ascertained and analysed. The final results suggest that while this method performs less well than existing alternatives, further work may allow this to be used as part of an ensemble for future models.



### Computational Attention System for Children, Adults and Elderly
- **Arxiv ID**: http://arxiv.org/abs/1904.12628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.12628v1)
- **Published**: 2019-04-18 06:22:39+00:00
- **Updated**: 2019-04-18 06:22:39+00:00
- **Authors**: Onkar Krishna, Kiyoharu Aizawa, Go Irie
- **Comment**: None
- **Journal**: None
- **Summary**: The existing computational visual attention systems have focused on the objective to basically simulate and understand the concept of visual attention system in adults. Consequently, the impact of observer's age in scene viewing behavior has rarely been considered. This study quantitatively analyzed the age-related differences in gaze landings during scene viewing for three different class of images: naturals, man-made, and fractals. Observer's of different age-group have shown different scene viewing tendencies independent to the class of the image viewed. Several interesting observations are drawn from the results. First, gaze landings for man-made dataset showed that whereas child observers focus more on the scene foreground, i.e., locations that are near, elderly observers tend to explore the scene background, i.e., locations farther in the scene. Considering this result a framework is proposed in this paper to quantitatively measure the depth bias tendency across age groups. Second, the quantitative analysis results showed that children exhibit the lowest exploratory behavior level but the highest central bias tendency among the age groups and across the different scene categories. Third, inter-individual similarity metrics reveal that an adult had significantly lower gaze consistency with children and elderly compared to other adults for all the scene categories. Finally, these analysis results were consequently leveraged to develop a more accurate age-adapted saliency model independent to the image type. The prediction accuracy suggests that our model fits better to the collected eye-gaze data of the observers belonging to different age groups than the existing models do.



### Deep Optics for Monocular Depth Estimation and 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08601v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08601v1)
- **Published**: 2019-04-18 06:25:30+00:00
- **Updated**: 2019-04-18 06:25:30+00:00
- **Authors**: Julie Chang, Gordon Wetzstein
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.



### Progressive Attention Memory Network for Movie Story Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1904.08607v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08607v1)
- **Published**: 2019-04-18 06:52:17+00:00
- **Updated**: 2019-04-18 06:52:17+00:00
- **Authors**: Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, Chang D. Yoo
- **Comment**: CVPR 2019, Accepted
- **Journal**: None
- **Summary**: This paper proposes the progressive attention memory network (PAMN) for movie story question answering (QA). Movie story QA is challenging compared to VQA in two aspects: (1) pinpointing the temporal parts relevant to answer the question is difficult as the movies are typically longer than an hour, (2) it has both video and subtitle where different questions require different modality to infer the answer. To overcome these challenges, PAMN involves three main features: (1) progressive attention mechanism that utilizes cues from both question and answer to progressively prune out irrelevant temporal parts in memory, (2) dynamic modality fusion that adaptively determines the contribution of each modality for answering the current question, and (3) belief correction answering scheme that successively corrects the prediction score on each candidate answer. Experiments on publicly available benchmark datasets, MovieQA and TVQA, demonstrate that each feature contributes to our movie story QA architecture, PAMN, and improves performance to achieve the state-of-the-art result. Qualitative analysis by visualizing the inference mechanism of PAMN is also provided.



### Learning to Collocate Neural Modules for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.08608v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08608v1)
- **Published**: 2019-04-18 07:03:19+00:00
- **Updated**: 2019-04-18 07:03:19+00:00
- **Authors**: Xu Yang, Hanwang Zhang, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: We do not speak word by word from scratch; our brain quickly structures a pattern like \textsc{sth do sth at someplace} and then fill in the detailed descriptions. To render existing encoder-decoder image captioners such human-like reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the `inner pattern' connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q\&A, where the language (ie, question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design --- one for function words and three for visual content words (eg, noun, adjective, and verb), 2) soft module fusion and multi-step module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (eg, adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, eg, by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.



### Client/Server Based Online Environment for Manual Segmentation of Medical Images
- **Arxiv ID**: http://arxiv.org/abs/1904.08610v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08610v1)
- **Published**: 2019-04-18 07:17:05+00:00
- **Updated**: 2019-04-18 07:17:05+00:00
- **Authors**: Daniel Wild, Maximilian Weber, Jan Egger
- **Comment**: 8 pages
- **Journal**: Proceedings of CESCG 2019: The 23rd Central European Seminar on
  Computer Graphics
- **Summary**: Segmentation is a key step in analyzing and processing medical images. Due to the low fault tolerance in medical imaging, manual segmentation remains the de facto standard in this domain. Besides, efforts to automate the segmentation process often rely on large amounts of manually labeled data. While existing software supporting manual segmentation is rich in features and delivers accurate results, the necessary time to set it up and get comfortable using it can pose a hurdle for the collection of large datasets. This work introduces a client/server based online environment, referred to as Studierfenster (studierfenster.at), that can be used to perform manual segmentations directly in a web browser. The aim of providing this functionality in the form of a web application is to ease the collection of ground truth segmentation datasets. Providing a tool that is quickly accessible and usable on a broad range of devices, offers the potential to accelerate this process. The manual segmentation workflow of Studierfenster consists of dragging and dropping the input file into the browser window and slice-by-slice outlining the object under consideration. The final segmentation can then be exported as a file storing its contours and as a binary segmentation mask. In order to evaluate the usability of Studierfenster, a user study was performed. The user study resulted in a mean of 6.3 out of 7.0 possible points given by users, when asked about their overall impression of the tool. The evaluation also provides insights into the results achievable with the tool in practice, by presenting two ground truth segmentations performed by physicians.



### Disentangled Representation Learning with Information Maximizing Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1904.08613v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08613v1)
- **Published**: 2019-04-18 07:25:19+00:00
- **Updated**: 2019-04-18 07:25:19+00:00
- **Authors**: Kazi Nazmul Haque, Siddique Latif, Rajib Rana
- **Comment**: None
- **Journal**: None
- **Summary**: Learning disentangled representation from any unlabelled data is a non-trivial problem. In this paper we propose Information Maximising Autoencoder (InfoAE) where the encoder learns powerful disentangled representation through maximizing the mutual information between the representation and given information in an unsupervised fashion. We have evaluated our model on MNIST dataset and achieved 98.9 ($\pm .1$) $\%$ test accuracy while using complete unsupervised training.



### Discriminative Online Learning for Fast Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08630v1)
- **Published**: 2019-04-18 08:11:07+00:00
- **Updated**: 2019-04-18 08:11:07+00:00
- **Authors**: Andreas Robinson, Felix Järemo Lawin, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg
- **Comment**: None
- **Journal**: None
- **Summary**: We address the highly challenging problem of video object segmentation. Given only the initial mask, the task is to segment the target in the subsequent frames. In order to effectively handle appearance changes and similar background objects, a robust representation of the target is required. Previous approaches either rely on fine-tuning a segmentation network on the first frame, or employ generative appearance models. Although partially successful, these methods often suffer from impractically low frame rates or unsatisfactory robustness.   We propose a novel approach, based on a dedicated target appearance model that is exclusively learned online to discriminate between the target and background image regions. Importantly, we design a specialized loss and customized optimization techniques to enable highly efficient online training. Our light-weight target model is integrated into a carefully designed segmentation network, trained offline to enhance the predictions generated by the target model. Extensive experiments are performed on three datasets. Our approach achieves an overall score of over 70 on YouTube-VOS, while operating at 25 frames per second.



### Unsupervised Open Domain Recognition by Semantic Discrepancy Minimization
- **Arxiv ID**: http://arxiv.org/abs/1904.08631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08631v1)
- **Published**: 2019-04-18 08:13:54+00:00
- **Updated**: 2019-04-18 08:13:54+00:00
- **Authors**: Junbao Zhuo, Shuhui Wang, Shuhao Cui, Qingming Huang
- **Comment**: Accepted to CVPR 2019, 10 pages, 4 figures
- **Journal**: None
- **Summary**: We address the unsupervised open domain recognition (UODR) problem, where categories in labeled source domain S is only a subset of those in unlabeled target domain T. The task is to correctly classify all samples in T including known and unknown categories. UODR is challenging due to the domain discrepancy, which becomes even harder to bridge when a large number of unknown categories exist in T. Moreover, the classification rules propagated by graph CNN (GCN) may be distracted by unknown categories and lack generalization capability. To measure the domain discrepancy for asymmetric label space between S and T, we propose Semantic-Guided Matching Discrepancy (SGMD), which first employs instance matching between S and T, and then the discrepancy is measured by a weighted feature distance between matched instances. We further design a limited balance constraint to achieve a more balanced classification output on known and unknown categories. We develop Unsupervised Open Domain Transfer Network (UODTN), which learns both the backbone classification network and GCN jointly by reducing the SGMD, enforcing the limited balance constraint and minimizing the classification loss on S. UODTN better preserves the semantic structure and enforces the consistency between the learned domain invariant visual features and the semantic embeddings. Experimental results show superiority of our method on recognizing images of both known and unknown categories.



### Learning a No-Reference Quality Assessment Model of Enhanced Images With Big Data
- **Arxiv ID**: http://arxiv.org/abs/1904.08632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08632v1)
- **Published**: 2019-04-18 08:14:24+00:00
- **Updated**: 2019-04-18 08:14:24+00:00
- **Authors**: Ke Gu, Dacheng Tao, Junfei Qiao, Weisi Lin
- **Comment**: 12 pages, 45 figures
- **Journal**: None
- **Summary**: In this paper we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g. object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g. visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images which are generally thought to be of the best quality. In this work, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measre of visual quality using a regression module, which is learned with big-data training samples that are much bigger than the size of relevant image datasets. Results of experiments on nine datasets validate the superiority and efficiency of our blind metric compared with typical state-of-the-art full-, reduced- and no-reference IQA methods. The second contribution is that a robust image enhancement framework is established based on quality optimization. For an input image, by the guidance of the proposed NR-IQA measure, we conduct histogram modification to successively rectify image brightness and contrast to a proper level. Thorough tests demonstrate that our framework can well enhance natural images, low-contrast images, low-light images and dehazed images. The source code will be released at https://sites.google.com/site/guke198701/publications.



### Crowd Management in Open Spaces
- **Arxiv ID**: http://arxiv.org/abs/1904.12625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12625v1)
- **Published**: 2019-04-18 08:16:39+00:00
- **Updated**: 2019-04-18 08:16:39+00:00
- **Authors**: Tauseef Ali, Ahmed B. Altamimi
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd analysis and management is a challenging problem to ensure public safety and security. For this purpose, many techniques have been proposed to cope with various problems. However, the generalization capabilities of these techniques is limited due to ignoring the fact that the density of crowd changes from low to extreme high depending on the scene under observation. We propose robust feature based approach to deal with the problem of crowd management for people safety and security. We have evaluated our method using a benchmark dataset and have presented details analysis.



### DDLSTM: Dual-Domain LSTM for Cross-Dataset Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.08634v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08634v1)
- **Published**: 2019-04-18 08:17:35+00:00
- **Updated**: 2019-04-18 08:17:35+00:00
- **Authors**: Toby Perrett, Dima Damen
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Domain alignment in convolutional networks aims to learn the degree of layer-specific feature alignment beneficial to the joint learning of source and target datasets. While increasingly popular in convolutional networks, there have been no previous attempts to achieve domain alignment in recurrent networks. Similar to spatial features, both source and target domains are likely to exhibit temporal dependencies that can be jointly learnt and aligned.   In this paper we introduce Dual-Domain LSTM (DDLSTM), an architecture that is able to learn temporal dependencies from two domains concurrently. It performs cross-contaminated batch normalisation on both input-to-hidden and hidden-to-hidden weights, and learns the parameters for cross-contamination, for both single-layer and multi-layer LSTM architectures. We evaluate DDLSTM on frame-level action recognition using three datasets, taking a pair at a time, and report an average increase in accuracy of 3.5%. The proposed DDLSTM architecture outperforms standard, fine-tuned, and batch-normalised LSTMs.



### Real-Time Style Transfer With Strength Control
- **Arxiv ID**: http://arxiv.org/abs/1904.08643v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/1904.08643v1)
- **Published**: 2019-04-18 08:58:49+00:00
- **Updated**: 2019-04-18 08:58:49+00:00
- **Authors**: Victor Kitov
- **Comment**: None
- **Journal**: None
- **Summary**: Style transfer is a problem of rendering a content image in the style of another style image. A natural and common practical task in applications of style transfer is to adjust the strength of stylization. Algorithm of Gatys et al. (2016) provides this ability by changing the weighting factors of content and style losses but is computationally inefficient. Real-time style transfer introduced by Johnson et al. (2016) enables fast stylization of any image by passing it through a pre-trained transformer network. Although fast, this architecture is not able to continuously adjust style strength. We propose an extension to real-time style transfer that allows direct control of style strength at inference, still requiring only a single transformer network. We conduct qualitative and quantitative experiments that demonstrate that the proposed method is capable of smooth stylization strength control and removes certain stylization artifacts appearing in the original real-time style transfer method. Comparisons with alternative real-time style transfer algorithms, capable of adjusting stylization strength, show that our method reproduces style with more details.



### Tex2Shape: Detailed Full Human Body Geometry From a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1904.08645v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08645v2)
- **Published**: 2019-04-18 09:10:20+00:00
- **Updated**: 2019-09-15 21:17:04+00:00
- **Authors**: Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, Marcus Magnor
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple yet effective method to infer detailed full human body shape from only a single photograph. Our model can infer full-body shape including face, hair, and clothing including wrinkles at interactive frame-rates. Results feature details even on parts that are occluded in the input image. Our main idea is to turn shape regression into an aligned image-to-image translation problem. The input to our method is a partial texture map of the visible region obtained from off-the-shelf methods. From a partial texture, we estimate detailed normal and vector displacement maps, which can be applied to a low-resolution smooth body model to add detail and clothing. Despite being trained purely with synthetic data, our model generalizes well to real-world photographs. Numerous results demonstrate the versatility and robustness of our method.



### Fooling automated surveillance cameras: adversarial patches to attack person detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08653v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08653v1)
- **Published**: 2019-04-18 09:46:03+00:00
- **Updated**: 2019-04-18 09:46:03+00:00
- **Authors**: Simen Thys, Wiebe Van Ranst, Toon Goedemé
- **Comment**: Accepted for CVPR Workshop: CV-COPS 2019
- **Journal**: None
- **Summary**: Adversarial attacks on machine learning models have seen increasing interest in the past years. By making only subtle changes to the input of a convolutional neural network, the output of the network can be swayed to output a completely different result. The first attacks did this by changing pixel values of an input image slightly to fool a classifier to output the wrong class. Other approaches have tried to learn "patches" that can be applied to an object to fool detectors and classifiers. Some of these approaches have also shown that these attacks are feasible in the real-world, i.e. by modifying an object and filming it with a video camera. However, all of these approaches target classes that contain almost no intra-class variety (e.g. stop signs). The known structure of the object is then used to generate an adversarial patch on top of it.   In this paper, we present an approach to generate adversarial patches to targets with lots of intra-class variety, namely persons. The goal is to generate a patch that is able successfully hide a person from a person detector. An attack that could for instance be used maliciously to circumvent surveillance systems, intruders can sneak around undetected by holding a small cardboard plate in front of their body aimed towards the surveillance camera.   From our results we can see that our system is able significantly lower the accuracy of a person detector. Our approach also functions well in real-life scenarios where the patch is filmed by a camera. To the best of our knowledge we are the first to attempt this kind of attack on targets with a high level of intra-class variety like persons.



### Fully Automatic Segmentation of 3D Brain Ultrasound: Learning from Coarse Annotations
- **Arxiv ID**: http://arxiv.org/abs/1904.08655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08655v1)
- **Published**: 2019-04-18 09:51:06+00:00
- **Updated**: 2019-04-18 09:51:06+00:00
- **Authors**: Julia Rackerseder, Rüdiger Göbl, Nassir Navab, Christoph Hennersperger
- **Comment**: * Julia Rackerseder abd R\"udiger G\"obl contributed equally to this
  work
- **Journal**: None
- **Summary**: Intra-operative ultrasound is an increasingly important imaging modality in neurosurgery. However, manual interaction with imaging data during the procedures, for example to select landmarks or perform segmentation, is difficult and can be time consuming. Yet, as registration to other imaging modalities is required in most cases, some annotation is necessary. We propose a segmentation method based on DeepVNet and specifically evaluate the integration of pre-training with simulated ultrasound sweeps to improve automatic segmentation and enable a fully automatic initialization of registration. In this view, we show that despite training on coarse and incomplete semi-automatic annotations, our approach is able to capture the desired superficial structures such as \textit{sulci}, the \textit{cerebellar tentorium}, and the \textit{falx cerebri}. We perform a five-fold cross-validation on the publicly available RESECT dataset. Trained on the dataset alone, we report a Dice and Jaccard coefficient of $0.45 \pm 0.09$ and $0.30 \pm 0.07$ respectively, as well as an average distance of $0.78 \pm 0.36~mm$. With the suggested pre-training, we computed a Dice and Jaccard coefficient of $0.47 \pm 0.10$ and $0.31 \pm 0.08$, and an average distance of $0.71 \pm 0.38~mm$. The qualitative evaluation suggest that with pre-training the network can learn to generalize better and provide refined and more complete segmentations in comparison to incomplete annotations provided as input.



### An Efficient Approximate kNN Graph Method for Diffusion on Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.08668v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08668v1)
- **Published**: 2019-04-18 10:15:41+00:00
- **Updated**: 2019-04-18 10:15:41+00:00
- **Authors**: Federico Magliani, Kevin McGuinness, Eva Mohedano, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: The application of the diffusion in many computer vision and artificial intelligence projects has been shown to give excellent improvements in performance. One of the main bottlenecks of this technique is the quadratic growth of the kNN graph size due to the high-quantity of new connections between nodes in the graph, resulting in long computation times. Several strategies have been proposed to address this, but none are effective and efficient. Our novel technique, based on LSH projections, obtains the same performance as the exact kNN graph after diffusion, but in less time (approximately 18 times faster on a dataset of a hundred thousand images). The proposed method was validated and compared with other state-of-the-art on several public image datasets, including Oxford5k, Paris6k, and Oxford105k.



### Coupled Learning for Facial Deblur
- **Arxiv ID**: http://arxiv.org/abs/1904.08671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08671v1)
- **Published**: 2019-04-18 10:24:15+00:00
- **Updated**: 2019-04-18 10:24:15+00:00
- **Authors**: Dayong Tian, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Blur in facial images significantly impedes the efficiency of recognition approaches. However, most existing blind deconvolution methods cannot generate satisfactory results due to their dependence on strong edges, which are sufficient in natural images but not in facial images. In this paper, we represent point spread functions (PSFs) by the linear combination of a set of pre-defined orthogonal PSFs, and similarly, an estimated intrinsic (EI) sharp face image is represented by the linear combination of a set of pre-defined orthogonal face images. In doing so, PSF and EI estimation is simplified to discovering two sets of linear combination coefficients, which are simultaneously found by our proposed coupled learning algorithm. To make our method robust to different types of blurry face images, we generate several candidate PSFs and EIs for a test image, and then, a non-blind deconvolution method is adopted to generate more EIs by those candidate PSFs. Finally, we deploy a blind image quality assessment metric to automatically select the optimal EI. Thorough experiments on the facial recognition technology database, extended Yale face database B, CMU pose, illumination, and expression (PIE) database, and face recognition grand challenge database version 2.0 demonstrate that the proposed approach effectively restores intrinsic sharp face images and, consequently, improves the performance of face recognition.



### Global Hashing System for Fast Image Search
- **Arxiv ID**: http://arxiv.org/abs/1904.08685v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08685v1)
- **Published**: 2019-04-18 11:02:52+00:00
- **Updated**: 2019-04-18 11:02:52+00:00
- **Authors**: Dayong Tian, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing methods have been widely investigated for fast approximate nearest neighbor searching in large data sets. Most existing methods use binary vectors in lower dimensional spaces to represent data points that are usually real vectors of higher dimensionality. We divide the hashing process into two steps. Data points are first embedded in a low-dimensional space, and the global positioning system method is subsequently introduced but modified for binary embedding. We devise dataindependent and data-dependent methods to distribute the satellites at appropriate locations. Our methods are based on finding the tradeoff between the information losses in these two steps. Experiments show that our data-dependent method outperforms other methods in different-sized data sets from 100k to 10M. By incorporating the orthogonality of the code matrix, both our data-independent and data-dependent methods are particularly impressive in experiments on longer bits.



### Examining the Capability of GANs to Replace Real Biomedical Images in Classification Models Training
- **Arxiv ID**: http://arxiv.org/abs/1904.08688v1
- **DOI**: None
- **Categories**: **cs.CV**, I.3.3; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1904.08688v1)
- **Published**: 2019-04-18 11:05:51+00:00
- **Updated**: 2019-04-18 11:05:51+00:00
- **Authors**: Vassili Kovalev, Siarhei Kazlouski
- **Comment**: 10 pages, 2 figures, 3 tables
- **Journal**: None
- **Summary**: In this paper, we explore the possibility of generating artificial biomedical images that can be used as a substitute for real image datasets in applied machine learning tasks. We are focusing on generation of realistic chest X-ray images as well as on the lymph node histology images using the two recent GAN architectures including DCGAN and PGGAN. The possibility of the use of artificial images instead of real ones for training machine learning models was examined by benchmark classification tasks being solved using conventional and deep learning methods. In particular, a comparison was made by replacing real images with synthetic ones at the model training stage and comparing the prediction results with the ones obtained while training on the real image data. It was found that the drop of classification accuracy caused by such training data substitution ranged between 2.2% and 3.5% for deep learning models and between 5.5% and 13.25% for conventional methods such as LBP + Random Forests.



### Out-of-Distribution Detection for Generalized Zero-Shot Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.08703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08703v2)
- **Published**: 2019-04-18 11:37:23+00:00
- **Updated**: 2019-05-06 11:36:37+00:00
- **Authors**: Devraj Mandal, Sanath Narayan, Saikumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, Ling Shao
- **Comment**: 10 pages, 3 figures, 6 Tables. To appear in the proceedings of CVPR
  2019
- **Journal**: None
- **Summary**: Generalized zero-shot action recognition is a challenging problem, where the task is to recognize new action categories that are unavailable during the training stage, in addition to the seen action categories. Existing approaches suffer from the inherent bias of the learned classifier towards the seen action categories. As a consequence, unseen category samples are incorrectly classified as belonging to one of the seen action categories. In this paper, we set out to tackle this issue by arguing for a separate treatment of seen and unseen action categories in generalized zero-shot action recognition. We introduce an out-of-distribution detector that determines whether the video features belong to a seen or unseen action category. To train our out-of-distribution detector, video features for unseen action categories are synthesized using generative adversarial networks trained on seen action category features. To the best of our knowledge, we are the first to propose an out-of-distribution detector based GZSL framework for action recognition in videos. Experiments are performed on three action recognition datasets: Olympic Sports, HMDB51 and UCF101. For generalized zero-shot action recognition, our proposed approach outperforms the baseline (f-CLSWGAN) with absolute gains (in classification accuracy) of 7.0%, 3.4%, and 4.9%, respectively, on these datasets.



### Knowledge-rich Image Gist Understanding Beyond Literal Meaning
- **Arxiv ID**: http://arxiv.org/abs/1904.08709v1
- **DOI**: 10.1016/j.datak.2018.07.006
- **Categories**: **cs.IR**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08709v1)
- **Published**: 2019-04-18 11:50:20+00:00
- **Updated**: 2019-04-18 11:50:20+00:00
- **Authors**: Lydia Weiland, Ioana Hulpus, Simone Paolo Ponzetto, Wolfgang Effelsberg, Laura Dietz
- **Comment**: None
- **Journal**: Data & Knowledge Engineering, Volume 117, September 2018, Pages
  114-132
- **Summary**: We investigate the problem of understanding the message (gist) conveyed by images and their captions as found, for instance, on websites or news articles. To this end, we propose a methodology to capture the meaning of image-caption pairs on the basis of large amounts of machine-readable knowledge that has previously been shown to be highly effective for text understanding. Our method identifies the connotation of objects beyond their denotation: where most approaches to image understanding focus on the denotation of objects, i.e., their literal meaning, our work addresses the identification of connotations, i.e., iconic meanings of objects, to understand the message of images. We view image understanding as the task of representing an image-caption pair on the basis of a wide-coverage vocabulary of concepts such as the one provided by Wikipedia, and cast gist detection as a concept-ranking problem with image-caption pairs as queries. To enable a thorough investigation of the problem of gist understanding, we produce a gold standard of over 300 image-caption pairs and over 8,000 gist annotations covering a wide variety of topics at different levels of abstraction. We use this dataset to experimentally benchmark the contribution of signals from heterogeneous sources, namely image and text. The best result with a Mean Average Precision (MAP) of 0.69 indicate that by combining both dimensions we are able to better understand the meaning of our image-caption pairs than when using language or vision information alone. We test the robustness of our gist detection approach when receiving automatically generated input, i.e., using automatically generated image tags or generated captions, and prove the feasibility of an end-to-end automated process.



### A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.08720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08720v1)
- **Published**: 2019-04-18 12:14:50+00:00
- **Updated**: 2019-04-18 12:14:50+00:00
- **Authors**: Thanh-Toan Do, Toan Tran, Ian Reid, Vijay Kumar, Tuan Hoang, Gustavo Carneiro
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: We propose a method that substantially improves the efficiency of deep distance metric learning based on the optimization of the triplet loss function. One epoch of such training process based on a naive optimization of the triplet loss function has a run-time complexity O(N^3), where N is the number of training samples. Such optimization scales poorly, and the most common approach proposed to address this high complexity issue is based on sub-sampling the set of triplets needed for the training process. Another approach explored in the field relies on an ad-hoc linearization (in terms of N) of the triplet loss that introduces class centroids, which must be optimized using the whole training set for each mini-batch - this means that a naive implementation of this approach has run-time complexity O(N^2). This complexity issue is usually mitigated with poor, but computationally cheap, approximate centroid optimization methods. In this paper, we first propose a solid theory on the linearization of the triplet loss with the use of class centroids, where the main conclusion is that our new linear loss represents a tight upper-bound to the triplet loss. Furthermore, based on the theory above, we propose a training algorithm that no longer requires the centroid optimization step, which means that our approach is the first in the field with a guaranteed linear run-time complexity. We show that the training of deep distance metric learning methods using the proposed upper-bound is substantially faster than triplet-based methods, while producing competitive retrieval accuracy results on benchmark datasets (CUB-200-2011 and CAR196).



### Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss
- **Arxiv ID**: http://arxiv.org/abs/1904.12732v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1904.12732v2)
- **Published**: 2019-04-18 12:20:53+00:00
- **Updated**: 2019-08-14 09:38:52+00:00
- **Authors**: Mhd Hasan Sarhan, Shadi Albarqouni, Mehmet Yigitsoy, Nassir Navab, Abouzar Eslami
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning techniques are recently being used in fundus image analysis and diabetic retinopathy detection. Microaneurysms are an important indicator of diabetic retinopathy progression. We introduce a two-stage deep learning approach for microaneurysms segmentation using multiple scales of the input with selective sampling and embedding triplet loss. The model first segments on two scales and then the segmentations are refined with a classification model. To enhance the discriminative power of the classification model, we incorporate triplet embedding loss with a selective sampling routine. The model is evaluated quantitatively to assess the segmentation performance and qualitatively to analyze the model predictions. This approach introduces a 30.29% relative improvement over the fully convolutional neural network.



### Cascaded Partial Decoder for Fast and Accurate Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08739v1)
- **Published**: 2019-04-18 12:53:30+00:00
- **Updated**: 2019-04-18 12:53:30+00:00
- **Authors**: Zhe Wu, Li Su, Qingming Huang
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Existing state-of-the-art salient object detection networks rely on aggregating multi-level features of pre-trained convolutional neural networks (CNNs). Compared to high-level features, low-level features contribute less to performance but cost more computations because of their larger spatial resolutions. In this paper, we propose a novel Cascaded Partial Decoder (CPD) framework for fast and accurate salient object detection. On the one hand, the framework constructs partial decoder which discards larger resolution features of shallower layers for acceleration. On the other hand, we observe that integrating features of deeper layers obtain relatively precise saliency map. Therefore we directly utilize generated saliency map to refine the features of backbone network. This strategy efficiently suppresses distractors in the features and significantly improves their representation ability. Experiments conducted on five benchmark datasets exhibit that the proposed model not only achieves state-of-the-art performance but also runs much faster than existing models. Besides, the proposed framework is further applied to improve existing multi-level feature aggregation models and significantly improve their efficiency and accuracy.



### Targetless Rotational Auto-Calibration of Radar and Camera for Intelligent Transportation Systems
- **Arxiv ID**: http://arxiv.org/abs/1904.08743v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08743v2)
- **Published**: 2019-04-18 13:02:34+00:00
- **Updated**: 2019-07-28 16:47:25+00:00
- **Authors**: Christoph Schöller, Maximilian Schnettler, Annkathrin Krämmer, Gereon Hinz, Maida Bakovic, Müge Güzet, Alois Knoll
- **Comment**: Accepted at the IEEE Intelligent Transportation Systems Conference
  (ITSC) 2019
- **Journal**: None
- **Summary**: Most intelligent transportation systems use a combination of radar sensors and cameras for robust vehicle perception. The calibration of these heterogeneous sensor types in an automatic fashion during system operation is challenging due to differing physical measurement principles and the high sparsity of traffic radars. We propose - to the best of our knowledge - the first data-driven method for automatic rotational radar-camera calibration without dedicated calibration targets. Our approach is based on a coarse and a fine convolutional neural network. We employ a boosting-inspired training algorithm, where we train the fine network on the residual error of the coarse network. Due to the unavailability of public datasets combining radar and camera measurements, we recorded our own real-world data. We demonstrate that our method is able to reach precise and robust sensor registration and show its generalization capabilities to different sensor alignments and perspectives.



### 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.08755v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08755v4)
- **Published**: 2019-04-18 13:19:50+00:00
- **Updated**: 2019-06-13 23:00:57+00:00
- **Authors**: Christopher Choy, JunYoung Gwak, Silvio Savarese
- **Comment**: CVPR'19
- **Journal**: None
- **Summary**: In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, those 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors and propose the generalized sparse convolution that encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for high-dimensional convolutional neural networks. We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases.



### Uncovering convolutional neural network decisions for diagnosing multiple sclerosis on conventional MRI using layer-wise relevance propagation
- **Arxiv ID**: http://arxiv.org/abs/1904.08771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08771v1)
- **Published**: 2019-04-18 13:37:07+00:00
- **Updated**: 2019-04-18 13:37:07+00:00
- **Authors**: Fabian Eitel, Emily Soehler, Judith Bellmann-Strobl, Alexander U. Brandt, Klemens Ruprecht, René M. Giess, Joseph Kuchling, Susanna Asseyer, Martin Weygandt, John-Dylan Haynes, Michael Scheel, Friedemann Paul, Kerstin Ritter
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning-based imaging diagnostics has recently reached or even superseded the level of clinical experts in several clinical domains. However, classification decisions of a trained machine learning system are typically non-transparent, a major hindrance for clinical integration, error tracking or knowledge discovery. In this study, we present a transparent deep learning framework relying on convolutional neural networks (CNNs) and layer-wise relevance propagation (LRP) for diagnosing multiple sclerosis (MS). MS is commonly diagnosed utilizing a combination of clinical presentation and conventional magnetic resonance imaging (MRI), specifically the occurrence and presentation of white matter lesions in T2-weighted images. We hypothesized that using LRP in a naive predictive model would enable us to uncover relevant image features that a trained CNN uses for decision-making. Since imaging markers in MS are well-established this would enable us to validate the respective CNN model. First, we pre-trained a CNN on MRI data from the Alzheimer's Disease Neuroimaging Initiative (n = 921), afterwards specializing the CNN to discriminate between MS patients and healthy controls (n = 147). Using LRP, we then produced a heatmap for each subject in the holdout set depicting the voxel-wise relevance for a particular classification decision. The resulting CNN model resulted in a balanced accuracy of 87.04% and an area under the curve of 96.08% in a receiver operating characteristic curve. The subsequent LRP visualization revealed that the CNN model focuses indeed on individual lesions, but also incorporates additional information such as lesion location, non-lesional white matter or gray matter areas such as the thalamus, which are established conventional and advanced MRI markers in MS. We conclude that LRP and the proposed framework have the capability to make diagnostic decisions of...



### DDNet: Cartesian-polar Dual-domain Network for the Joint Optic Disc and Cup Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08773v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08773v1)
- **Published**: 2019-04-18 13:38:55+00:00
- **Updated**: 2019-04-18 13:38:55+00:00
- **Authors**: Qing Liu, Xiaopeng Hong, Wei Ke, Zailiang Chen, Beiji Zou
- **Comment**: None
- **Journal**: None
- **Summary**: Existing joint optic disc and cup segmentation approaches are developed either in Cartesian or polar coordinate system. However, due to the subtle optic cup, the contextual information exploited from the single domain even by the prevailing CNNs is still insufficient. In this paper, we propose a novel segmentation approach, named Cartesian-polar dual-domain network (DDNet), which for the first time considers the complementary of the Cartesian domain and the polar domain. We propose a two-branch of domain feature encoder and learn translation equivariant representations on rectilinear grid from Cartesian domain and rotation equivariant representations on polar grid from polar domain parallelly. To fuse the features on two different grids, we propose a dual-domain fusion module. This module builds the correspondence between two grids by the differentiable polar transform layer and learns the feature importance across two domains in element-wise to enhance the expressive capability. Finally, the decoder aggregates the fused features from low-level to high-level and makes dense predictions. We validate the state-of-the-art segmentation performances of our DDNet on the public dataset ORIGA. According to the segmentation masks, we estimate the commonly used clinical measure for glaucoma, i.e., the vertical cup-to-disc ratio. The low cup-to-disc ratio estimation error demonstrates the potential application in glaucoma screening.



### Learning a Controller Fusion Network by Online Trajectory Filtering for Vision-based UAV Racing
- **Arxiv ID**: http://arxiv.org/abs/1904.08801v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08801v1)
- **Published**: 2019-04-18 14:23:33+00:00
- **Updated**: 2019-04-18 14:23:33+00:00
- **Authors**: Matthias Müller, Guohao Li, Vincent Casser, Neil Smith, Dominik L. Michels, Bernard Ghanem
- **Comment**: Accepted at CVPRW'19: UAVision 2019. First two authors contributed
  equally. Based on the initial work of arXiv:1803.01129 which was eventually
  split into two separate projects
- **Journal**: None
- **Summary**: Autonomous UAV racing has recently emerged as an interesting research problem. The dream is to beat humans in this new fast-paced sport. A common approach is to learn an end-to-end policy that directly predicts controls from raw images by imitating an expert. However, such a policy is limited by the expert it imitates and scaling to other environments and vehicle dynamics is difficult. One approach to overcome the drawbacks of an end-to-end policy is to train a network only on the perception task and handle control with a PID or MPC controller. However, a single controller must be extensively tuned and cannot usually cover the whole state space. In this paper, we propose learning an optimized controller using a DNN that fuses multiple controllers. The network learns a robust controller with online trajectory filtering, which suppresses noisy trajectories and imperfections of individual controllers. The result is a network that is able to learn a good fusion of filtered trajectories from different controllers leading to significant improvements in overall performance. We compare our trained network to controllers it has learned from, end-to-end baselines and human pilots in a realistic simulation; our network beats all baselines in extensive experiments and approaches the performance of a professional human pilot. A video summarizing this work is available at https://youtu.be/hGKlE5X9Z5U



### On The Classification-Distortion-Perception Tradeoff
- **Arxiv ID**: http://arxiv.org/abs/1904.08816v1
- **DOI**: None
- **Categories**: **cs.IT**, cs.CV, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1904.08816v1)
- **Published**: 2019-04-18 14:43:29+00:00
- **Updated**: 2019-04-18 14:43:29+00:00
- **Authors**: Dong Liu, Haochen Zhang, Zhiwei Xiong
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 32, 2019,
  https://papers.nips.cc/paper/8404-on-the-classification-distortion-perception-tradeoff
- **Summary**: Signal degradation is ubiquitous and computational restoration of degraded signal has been investigated for many years. Recently, it is reported that the capability of signal restoration is fundamentally limited by the perception-distortion tradeoff, i.e. the distortion and the perceptual difference between the restored signal and the ideal `original' signal cannot be made both minimal simultaneously. Distortion corresponds to signal fidelity and perceptual difference corresponds to perceptual naturalness, both of which are important metrics in practice. Besides, there is another dimension worthy of consideration, namely the semantic quality or the utility for recognition purpose, of the restored signal. In this paper, we extend the previous perception-distortion tradeoff to the case of classification-distortion-perception (CDP) tradeoff, where we introduced the classification error rate of the restored signal in addition to distortion and perceptual difference. Two versions of the CDP tradeoff are considered, one using a predefined classifier and the other dealing with the optimal classifier for the restored signal. For both versions, we can rigorously prove the existence of the CDP tradeoff, i.e. the distortion, perceptual difference, and classification error rate cannot be made all minimal simultaneously. Our findings can be useful especially for computer vision researches where some low-level vision tasks (signal restoration) serve for high-level vision tasks (visual understanding).



### (De)Constructing Bias on Skin Lesion Datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.08818v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08818v1)
- **Published**: 2019-04-18 14:49:23+00:00
- **Updated**: 2019-04-18 14:49:23+00:00
- **Authors**: Alceu Bissoto, Michel Fornaciali, Eduardo Valle, Sandra Avila
- **Comment**: 9 pages, 6 figures. Paper accepted at 2019 ISIC Skin Image Anaylsis
  Workshop @ IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)
- **Journal**: None
- **Summary**: Melanoma is the deadliest form of skin cancer. Automated skin lesion analysis plays an important role for early detection. Nowadays, the ISIC Archive and the Atlas of Dermoscopy dataset are the most employed skin lesion sources to benchmark deep-learning based tools. However, all datasets contain biases, often unintentional, due to how they were acquired and annotated. Those biases distort the performance of machine-learning models, creating spurious correlations that the models can unfairly exploit, or, contrarily destroying cogent correlations that the models could learn. In this paper, we propose a set of experiments that reveal both types of biases, positive and negative, in existing skin lesion datasets. Our results show that models can correctly classify skin lesion images without clinically-meaningful information: disturbingly, the machine-learning model learned over images where no information about the lesion remains, presents an accuracy above the AI benchmark curated with dermatologists' performances. That strongly suggests spurious correlations guiding the models. We fed models with additional clinically meaningful information, which failed to improve the results even slightly, suggesting the destruction of cogent correlations. Our main findings raise awareness of the limitations of models trained and evaluated in small datasets such as the ones we evaluated, and may suggest future guidelines for models intended for real-world deployment.



### Generating Training Data for Denoising Real RGB Images via Camera Pipeline Simulation
- **Arxiv ID**: http://arxiv.org/abs/1904.08825v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08825v1)
- **Published**: 2019-04-18 15:04:48+00:00
- **Updated**: 2019-04-18 15:04:48+00:00
- **Authors**: Ronnachai Jaroensri, Camille Biscarrat, Miika Aittala, Frédo Durand
- **Comment**: None
- **Journal**: None
- **Summary**: Image reconstruction techniques such as denoising often need to be applied to the RGB output of cameras and cellphones. Unfortunately, the commonly used additive white noise (AWGN) models do not accurately reproduce the noise and the degradation encountered on these inputs. This is particularly important for learning-based techniques, because the mismatch between training and real world data will hurt their generalization. This paper aims to accurately simulate the degradation and noise transformation performed by camera pipelines. This allows us to generate realistic degradation in RGB images that can be used to train machine learning models. We use our simulation to study the importance of noise modeling for learning-based denoising. Our study shows that a realistic noise model is required for learning to denoise real JPEG images. A neural network trained on realistic noise outperforms the one trained with AWGN by 3 dB. An ablation study of our pipeline shows that simulating denoising and demosaicking is important to this improvement and that realistic demosaicking algorithms, which have been rarely considered, is needed. We believe this simulation will also be useful for other image reconstruction tasks, and we will distribute our code publicly.



### Understanding Neural Networks via Feature Visualization: A survey
- **Arxiv ID**: http://arxiv.org/abs/1904.08939v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08939v1)
- **Published**: 2019-04-18 15:46:26+00:00
- **Updated**: 2019-04-18 15:46:26+00:00
- **Authors**: Anh Nguyen, Jason Yosinski, Jeff Clune
- **Comment**: A book chapter in an Interpretable ML book
  (http://www.interpretable-ml.org/book/)
- **Journal**: None
- **Summary**: A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.



### Enhanced Center Coding for Cell Detection with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.08864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08864v1)
- **Published**: 2019-04-18 16:18:01+00:00
- **Updated**: 2019-04-18 16:18:01+00:00
- **Authors**: Haoyi Liang, Aijaz Naik, Cedric L. Williams, Jaideep Kapur, Daniel S. Weller
- **Comment**: None
- **Journal**: None
- **Summary**: Cell imaging and analysis are fundamental to biomedical research because cells are the basic functional units of life. Among different cell-related analysis, cell counting and detection are widely used. In this paper, we focus on one common step of learning-based cell counting approaches: coding the raw dot labels into more suitable maps for learning. Two criteria of coding raw dot labels are discussed, and a new coding scheme is proposed in this paper. The two criteria measure how easy it is to train the model with a coding scheme, and how robust the recovered raw dot labels are when predicting. The most compelling advantage of the proposed coding scheme is the ability to distinguish neighboring cells in crowded regions. Cell counting and detection experiments are conducted for five coding schemes on four types of cells and two network architectures. The proposed coding scheme improves the counting accuracy versus the widely-used Gaussian and rectangle kernels up to 12%, and also improves the detection accuracy versus the common proximity coding up to 14%.



### Salient Object Detection: A Distinctive Feature Integration Model
- **Arxiv ID**: http://arxiv.org/abs/1904.08868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08868v1)
- **Published**: 2019-04-18 16:23:45+00:00
- **Updated**: 2019-04-18 16:23:45+00:00
- **Authors**: Abdullah J. Alzahrani, Hina Afridi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel method for salient object detection in different images. Our method integrates spatial features for efficient and robust representation to capture meaningful information about the salient objects. We then train a conditional random field (CRF) using the integrated features. The trained CRF model is then used to detect salient objects during the online testing stage. We perform experiments on two standard datasets and compare the performance of our method with different reference methods. Our experiments show that our method outperforms the compared methods in terms of precision, recall, and F-Measure.



### No-Reference Quality Assessment of Contrast-Distorted Images using Contrast Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1904.08879v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08879v1)
- **Published**: 2019-04-18 16:36:43+00:00
- **Updated**: 2019-04-18 16:36:43+00:00
- **Authors**: Jia Yan, Jie Li, Xin Fu
- **Comment**: Draft version
- **Journal**: None
- **Summary**: No-reference image quality assessment (NR-IQA) aims to measure the image quality without reference image. However, contrast distortion has been overlooked in the current research of NR-IQA. In this paper, we propose a very simple but effective metric for predicting quality of contrast-altered images based on the fact that a high-contrast image is often more similar to its contrast enhanced image. Specifically, we first generate an enhanced image through histogram equalization. We then calculate the similarity of the original image and the enhanced one by using structural-similarity index (SSIM) as the first feature. Further, we calculate the histogram based entropy and cross entropy between the original image and the enhanced one respectively, to gain a sum of 4 features. Finally, we learn a regression module to fuse the aforementioned 5 features for inferring the quality score. Experiments on four publicly available databases validate the superiority and efficiency of the proposed technique.



### KPConv: Flexible and Deformable Convolution for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.08889v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08889v2)
- **Published**: 2019-04-18 16:55:44+00:00
- **Updated**: 2019-08-19 14:45:58+00:00
- **Authors**: Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François Goulette, Leonidas J. Guibas
- **Comment**: Camera-ready, accepted to ICCV 2019; project website:
  https://github.com/HuguesTHOMAS/KPConv
- **Journal**: None
- **Summary**: We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.



### Signal2Image Modules in Deep Neural Networks for EEG Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.13216v8
- **DOI**: 10.1109/EMBC.2019.8856620
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.13216v8)
- **Published**: 2019-04-18 17:05:22+00:00
- **Updated**: 2022-05-16 18:20:27+00:00
- **Authors**: Paschalis Bizopoulos, George I Lambrou, Dimitrios Koutsouris
- **Comment**: 4 pages, 2 figures, 1 table, EMBC 2019
- **Journal**: 2019 41st Annual International Conference of the IEEE Engineering
  in Medicine and Biology Society (EMBC)
- **Summary**: Deep learning has revolutionized computer vision utilizing the increased availability of big data and the power of parallel computational units such as graphical processing units. The vast majority of deep learning research is conducted using images as training data, however the biomedical domain is rich in physiological signals that are used for diagnosis and prediction problems. It is still an open research question how to best utilize signals to train deep neural networks.   In this paper we define the term Signal2Image (S2Is) as trainable or non-trainable prefix modules that convert signals, such as Electroencephalography (EEG), to image-like representations making them suitable for training image-based deep neural networks defined as `base models'. We compare the accuracy and time performance of four S2Is (`signal as image', spectrogram, one and two layer Convolutional Neural Networks (CNNs)) combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet) along with the depth-wise and 1D variations of the latter. We also provide empirical evidence that the one layer CNN S2I performs better in eleven out of fifteen tested models than non-trainable S2Is for classifying EEG signals and we present visual comparisons of the outputs of the S2Is.



### CornerNet-Lite: Efficient Keypoint Based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.08900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08900v2)
- **Published**: 2019-04-18 17:21:15+00:00
- **Updated**: 2020-09-16 23:22:32+00:00
- **Authors**: Hei Law, Yun Teng, Olga Russakovsky, Jia Deng
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: Keypoint-based methods are a relatively new paradigm in object detection, eliminating the need for anchor boxes and offering a simplified detection framework. Keypoint-based CornerNet achieves state of the art accuracy among single-stage detectors. However, this accuracy comes at high processing cost. In this work, we tackle the problem of efficient keypoint-based object detection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two efficient variants of CornerNet: CornerNet-Saccade, which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image, and CornerNet-Squeeze, which introduces a new compact backbone architecture. Together these two variants address the two critical use cases in efficient object detection: improving efficiency without sacrificing accuracy, and improving accuracy at real-time efficiency. CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 30ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO). Together these contributions for the first time reveal the potential of keypoint-based detection to be useful for applications requiring processing efficiency.



### Combating the Elsagate phenomenon: Deep learning architectures for disturbing cartoons
- **Arxiv ID**: http://arxiv.org/abs/1904.08910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08910v1)
- **Published**: 2019-04-18 17:43:42+00:00
- **Updated**: 2019-04-18 17:43:42+00:00
- **Authors**: Akari Ishikawa, Edson Bollis, Sandra Avila
- **Comment**: 6 pages, 5 figures, 2 tables. Paper accepted at 7th IAPR/IEEE
  International Workshop on Biometrics and Forensics (IWBF)
- **Journal**: None
- **Summary**: Watching cartoons can be useful for children's intellectual, social and emotional development. However, the most popular video sharing platform today provides many videos with Elsagate content. Elsagate is a phenomenon that depicts childhood characters in disturbing circumstances (e.g., gore, toilet humor, drinking urine, stealing). Even with this threat easily available for children, there is no work in the literature addressing the problem. As the first to explore disturbing content in cartoons, we proceed from the most recent pornography detection literature applying deep convolutional neural networks combined with static and motion information of the video. Our solution is compatible with mobile platforms and achieved 92.6% of accuracy. Our goal is not only to introduce the first solution but also to bring up the discussion around Elsagate.



### Deep Rigid Instance Scene Flow
- **Arxiv ID**: http://arxiv.org/abs/1904.08913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.08913v1)
- **Published**: 2019-04-18 17:48:26+00:00
- **Updated**: 2019-04-18 17:48:26+00:00
- **Authors**: Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, Raquel Urtasun
- **Comment**: CVPR 2019. Rank 1st on KITTI scene flow benchmark. 800 times faster
  than prior art
- **Journal**: None
- **Summary**: In this paper we tackle the problem of scene flow estimation in the context of self-driving. We leverage deep learning techniques as well as strong priors as in our application domain the motion of the scene can be composed by the motion of the robot and the 3D motion of the actors in the scene. We formulate the problem as energy minimization in a deep structured model, which can be solved efficiently in the GPU by unrolling a Gaussian-Newton solver. Our experiments in the challenging KITTI scene flow dataset show that we outperform the state-of-the-art by a very large margin, while being 800 times faster.



### Early Detection of Injuries in MLB Pitchers from Video
- **Arxiv ID**: http://arxiv.org/abs/1904.08916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08916v1)
- **Published**: 2019-04-18 17:52:01+00:00
- **Updated**: 2019-04-18 17:52:01+00:00
- **Authors**: AJ Piergiovanni, Michael S. Ryoo
- **Comment**: CVPR Workshop on Computer Vision in Sports 2019
- **Journal**: None
- **Summary**: Injuries are a major cost in sports. Teams spend millions of dollars every year on players who are hurt and unable to play, resulting in lost games, decreased fan interest and additional wages for replacement players. Modern convolutional neural networks have been successfully applied to many video recognition tasks. In this paper, we introduce the problem of injury detection/prediction in MLB pitchers and experimentally evaluate the ability of such convolutional models to detect and predict injuries in pitches only from video data. We conduct experiments on a large dataset of TV broadcast MLB videos of 20 different pitchers who were injured during the 2017 season. We experimentally evaluate the model's performance on each individual pitcher, how well it generalizes to new pitchers, how it performs for various injuries, and how early it can predict or detect an injury.



### Attentive Single-Tasking of Multiple Tasks
- **Arxiv ID**: http://arxiv.org/abs/1904.08918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08918v1)
- **Published**: 2019-04-18 17:54:30+00:00
- **Updated**: 2019-04-18 17:54:30+00:00
- **Authors**: Kevis-Kokitsi Maninis, Ilija Radosavovic, Iasonas Kokkinos
- **Comment**: CVPR 2019 Camera Ready
- **Journal**: None
- **Summary**: In this work we address task interference in universal networks by considering that a network is trained on multiple tasks, but performs one task at a time, an approach we refer to as "single-tasking multiple tasks". The network thus modifies its behaviour through task-dependent feature adaptation, or task attention. This gives the network the ability to accentuate the features that are adapted to a task, while shunning irrelevant ones. We further reduce task interference by forcing the task gradients to be statistically indistinguishable through adversarial training, ensuring that the common backbone architecture serving all tasks is not dominated by any of the task-specific gradients. Results in three multi-task dense labelling problems consistently show: (i) a large reduction in the number of parameters while preserving, or even improving performance and (ii) a smooth trade-off between computation and multi-task accuracy. We provide our system's code and pre-trained models at http://vision.ee.ethz.ch/~kmaninis/astmt/.



### Towards VQA Models That Can Read
- **Arxiv ID**: http://arxiv.org/abs/1904.08920v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.08920v2)
- **Published**: 2019-04-18 17:55:37+00:00
- **Updated**: 2019-05-13 23:28:48+00:00
- **Authors**: Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new "TextVQA" dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.



### Deep Parametric Shape Predictions using Distance Fields
- **Arxiv ID**: http://arxiv.org/abs/1904.08921v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08921v2)
- **Published**: 2019-04-18 17:55:57+00:00
- **Updated**: 2020-03-19 15:07:10+00:00
- **Authors**: Dmitriy Smirnov, Matthew Fisher, Vladimir G. Kim, Richard Zhang, Justin Solomon
- **Comment**: Accepted to CVPR 2020
- **Journal**: Proc. of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition. (2020) 561-570
- **Summary**: Many tasks in graphics and vision demand machinery for converting shapes into consistent representations with sparse sets of parameters; these representations facilitate rendering, editing, and storage. When the source data is noisy or ambiguous, however, artists and engineers often manually construct such representations, a tedious and potentially time-consuming process. While advances in deep learning have been successfully applied to noisy geometric data, the task of generating parametric shapes has so far been difficult for these methods. Hence, we propose a new framework for predicting parametric shape primitives using deep learning. We use distance fields to transition between shape parameters like control points and input data on a pixel grid. We demonstrate efficacy on 2D and 3D tasks, including font vectorization and surface abstraction.



### RepGN:Object Detection with Relational Proposal Graph Network
- **Arxiv ID**: http://arxiv.org/abs/1904.08959v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08959v1)
- **Published**: 2019-04-18 18:07:20+00:00
- **Updated**: 2019-04-18 18:07:20+00:00
- **Authors**: Xingjian Du, Xuan Shi, Risheng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Region based object detectors achieve the state-of-the-art performance, but few consider to model the relation of proposals. In this paper, we explore the idea of modeling the relationships among the proposals for object detection from the graph learning perspective. Specifically, we present relational proposal graph network (RepGN) which is defined on object proposals and the semantic and spatial relation modeled as the edge. By integrating our RepGN module into object detectors, the relation and context constraints will be introduced to the feature extraction of regions and bounding boxes regression and classification. Besides, we propose a novel graph-cut based pooling layer for hierarchical coarsening of the graph, which empowers the RepGN module to exploit the inter-regional correlation and scene description in a hierarchical manner. We perform extensive experiments on COCO object detection dataset and show promising results.



### VoteNet: A Deep Learning Label Fusion Method for Multi-Atlas Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08963v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08963v2)
- **Published**: 2019-04-18 18:16:18+00:00
- **Updated**: 2019-05-31 02:46:44+00:00
- **Authors**: Zhipeng Ding, Xu Han, Marc Niethammer
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning (DL) approaches are state-of-the-art for many medical image segmentation tasks. They offer a number of advantages: they can be trained for specific tasks, computations are fast at test time, and segmentation quality is typically high. In contrast, previously popular multi-atlas segmentation (MAS) methods are relatively slow (as they rely on costly registrations) and even though sophisticated label fusion strategies have been proposed, DL approaches generally outperform MAS. In this work, we propose a DL-based label fusion strategy (VoteNet) which locally selects a set of reliable atlases whose labels are then fused via plurality voting. Experiments on 3D brain MRI data show that by selecting a good initial atlas set MAS with VoteNet significantly outperforms a number of other label fusion strategies as well as a direct DL segmentation approach. We also provide an experimental analysis of the upper performance bound achievable by our method. While unlikely achievable in practice, this bound suggests room for further performance improvements. Lastly, to address the runtime disadvantage of standard MAS, all our results make use of a fast DL registration approach.



### Exploring the Limitations of Behavior Cloning for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1904.08980v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08980v1)
- **Published**: 2019-04-18 19:29:56+00:00
- **Updated**: 2019-04-18 19:29:56+00:00
- **Authors**: Felipe Codevilla, Eder Santana, Antonio M. López, Adrien Gaidon
- **Comment**: None
- **Journal**: None
- **Summary**: Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-of-the-art results, including in unseen environments, executing complex lateral and longitudinal maneuvers without these reactions being explicitly programmed. However, we confirm well-known limitations (due to dataset bias and overfitting), new generalization issues (due to dynamic objects and the lack of a causal model), and training instability requiring further research before behavior cloning can graduate to real-world driving. The code of the studied behavior cloning approaches can be found at https://github.com/felipecode/coiltraine .



### DeepLocalization: Landmark-based Self-Localization with Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09007v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.09007v2)
- **Published**: 2019-04-18 20:41:10+00:00
- **Updated**: 2019-07-19 09:24:45+00:00
- **Authors**: Nico Engel, Stefan Hoermann, Markus Horn, Vasileios Belagiannis, Klaus Dietmayer
- **Comment**: Accepted for publication by the IEEE Intelligent Transportation
  Systems Conference (ITSC 2019), Auckland, New Zealand
- **Journal**: None
- **Summary**: We address the problem of vehicle self-localization from multi-modal sensor information and a reference map. The map is generated off-line by extracting landmarks from the vehicle's field of view, while the measurements are collected similarly on the fly. Our goal is to determine the autonomous vehicle's pose from the landmark measurements and map landmarks. To learn this mapping, we propose DeepLocalization, a deep neural network that regresses the vehicle's translation and rotation parameters from unordered and dynamic input landmarks. The proposed network architecture is robust to changes of the dynamic environment and can cope with a small number of extracted landmarks. During the training process we rely on synthetically generated ground-truth. In our experiments, we evaluate two inference approaches in real-world scenarios. We show that DeepLocalization can be combined with regular GPS signals and filtering algorithms such as the extended Kalman filter. Our approach achieves state-of-the-art accuracy and is about ten times faster than the related work.



### Self-Supervised Audio-Visual Co-Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.09013v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09013v1)
- **Published**: 2019-04-18 21:11:03+00:00
- **Updated**: 2019-04-18 21:11:03+00:00
- **Authors**: Andrew Rouditchenko, Hang Zhao, Chuang Gan, Josh McDermott, Antonio Torralba
- **Comment**: Accepted to ICASSP 2019
- **Journal**: None
- **Summary**: Segmenting objects in images and separating sound sources in audio are challenging tasks, in part because traditional approaches require large amounts of labeled data. In this paper we develop a neural network model for visual object segmentation and sound source separation that learns from natural videos through self-supervision. The model is an extension of recently proposed work that maps image pixels to sounds. Here, we introduce a learning approach to disentangle concepts in the neural networks, and assign semantic categories to network feature channels to enable independent image segmentation and sound source separation after audio-visual training on videos. Our evaluations show that the disentangled model outperforms several baselines in semantic segmentation and sound source separation.



### A deep learning based solution for construction equipment detection: from development to deployment
- **Arxiv ID**: http://arxiv.org/abs/1904.09021v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09021v1)
- **Published**: 2019-04-18 21:37:04+00:00
- **Updated**: 2019-04-18 21:37:04+00:00
- **Authors**: Saeed Arabi, Arya Haghighat, Anuj Sharma
- **Comment**: 17 pages, 16 figures, 6 tables
- **Journal**: None
- **Summary**: This paper aims at providing researchers and engineering professionals with a practical and comprehensive deep learning based solution to detect construction equipment from the very first step of its development to the last one which is deployment. This paper focuses on the last step of deployment. The first phase of solution development, involved data preparation, model selection, model training, and model evaluation. The second phase of the study comprises of model optimization, application specific embedded system selection, and economic analysis. Several embedded systems were proposed and compared. The review of the results confirms superior real-time performance of the solutions with a consistent above 90% rate of accuracy. The current study validates the practicality of deep learning based object detection solutions for construction scenarios. Moreover, the detailed knowledge, presented in this study, can be employed for several purposes such as, safety monitoring, productivity assessments, and managerial decisions.



### Talk Proposal: Towards the Realistic Evaluation of Evasion Attacks using CARLA
- **Arxiv ID**: http://arxiv.org/abs/1904.12622v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12622v1)
- **Published**: 2019-04-18 22:01:53+00:00
- **Updated**: 2019-04-18 22:01:53+00:00
- **Authors**: Cory Cornelius, Shang-Tse Chen, Jason Martin, Duen Horng Chau
- **Comment**: Submitted as talk proposal to Dependable and Secure Machine Learning
  (DSML '19)
- **Journal**: None
- **Summary**: In this talk we describe our content-preserving attack on object detectors, ShapeShifter, and demonstrate how to evaluate this threat in realistic scenarios. We describe how we use CARLA, a realistic urban driving simulator, to create these scenarios, and how we use ShapeShifter to generate content-preserving attacks against those scenarios.



### A Novel BiLevel Paradigm for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.09028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09028v1)
- **Published**: 2019-04-18 22:32:51+00:00
- **Updated**: 2019-04-18 22:32:51+00:00
- **Authors**: Liqian Ma, Qianru Sun, Bernt Schiele, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: Image-to-image (I2I) translation is a pixel-level mapping that requires a large number of paired training data and often suffers from the problems of high diversity and strong category bias in image scenes. In order to tackle these problems, we propose a novel BiLevel (BiL) learning paradigm that alternates the learning of two models, respectively at an instance-specific (IS) and a general-purpose (GP) level. In each scene, the IS model learns to maintain the specific scene attributes. It is initialized by the GP model that learns from all the scenes to obtain the generalizable translation knowledge. This GP initialization gives the IS model an efficient starting point, thus enabling its fast adaptation to the new scene with scarce training data. We conduct extensive I2I translation experiments on human face and street view datasets. Quantitative results validate that our approach can significantly boost the performance of classical I2I translation models, such as PG2 and Pix2Pix. Our visualization results show both higher image quality and more appropriate instance-specific details, e.g., the translated image of a person looks more like that person in terms of identity.



### ProductNet: a Collection of High-Quality Datasets for Product Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.09037v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.09037v1)
- **Published**: 2019-04-18 23:17:07+00:00
- **Updated**: 2019-04-18 23:17:07+00:00
- **Authors**: Chu Wang, Lei Tang, Yang Lu, Shujun Bian, Hirohisa Fujita, Da Zhang, Zuohua Zhang, Yongning Wu
- **Comment**: None
- **Journal**: None
- **Summary**: ProductNet is a collection of high-quality product datasets for better product understanding. Motivated by ImageNet, ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product image and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process. For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.



### Human Motion Prediction via Pattern Completion in Latent Representation Space
- **Arxiv ID**: http://arxiv.org/abs/1904.09039v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.09039v1)
- **Published**: 2019-04-18 23:36:19+00:00
- **Updated**: 2019-04-18 23:36:19+00:00
- **Authors**: Yi Tian Xu, Yaqiao Li, David Meger
- **Comment**: Accepted in the 16th Conference on Computer and Robot Vision (CRV
  2019)
- **Journal**: None
- **Summary**: Inspired by ideas in cognitive science, we propose a novel and general approach to solve human motion understanding via pattern completion on a learned latent representation space. Our model outperforms current state-of-the-art methods in human motion prediction across a number of tasks, with no customization. To construct a latent representation for time-series of various lengths, we propose a new and generic autoencoder based on sequence-to-sequence learning. While traditional inference strategies find a correlation between an input and an output, we use pattern completion, which views the input as a partial pattern and to predict the best corresponding complete pattern. Our results demonstrate that this approach has advantages when combined with our autoencoder in solving human motion prediction, motion generation and action classification.



