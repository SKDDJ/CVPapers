# Arxiv Papers in cs.CV on 2019-04-09
### Towards Universal Object Detection by Domain Attention
- **Arxiv ID**: http://arxiv.org/abs/1904.04402v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04402v4)
- **Published**: 2019-04-09 00:11:12+00:00
- **Updated**: 2019-07-06 02:43:56+00:00
- **Authors**: Xudong Wang, Zhaowei Cai, Dashan Gao, Nuno Vasconcelos
- **Comment**: 11 pages, 6 figures. Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Despite increasing efforts on universal representations for visual recognition, few have addressed object detection. In this paper, we develop an effective and efficient universal object detection system that is capable of working on various image domains, from human faces and traffic signs to medical CT images. Unlike multi-domain models, this universal model does not require prior knowledge of the domain of interest. This is achieved by the introduction of a new family of adaptation layers, based on the principles of squeeze and excitation, and a new domain-attention mechanism. In the proposed universal detector, all parameters and computations are shared across domains, and a single network processes all domains all the time. Experiments, on a newly established universal object detection benchmark of 11 diverse datasets, show that the proposed detector outperforms a bank of individual detectors, a multi-domain detector, and a baseline universal detector, with a 1.3x parameter increase over a single-domain baseline detector. The code and benchmark will be released at http://www.svcl.ucsd.edu/projects/universal-detection/.



### Embodied Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.04404v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.04404v1)
- **Published**: 2019-04-09 00:33:17+00:00
- **Updated**: 2019-04-09 00:33:17+00:00
- **Authors**: Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David Crandall, Devi Parikh, Dhruv Batra
- **Comment**: 14 pages, 13 figures, technical report
- **Journal**: None
- **Summary**: Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment, and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Visual Recognition (EVR): An agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this, we develop a new model called Embodied Mask R-CNN, for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using the House3D environment. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones; 2) in order to improve visual recognition abilities, agents can learn strategical moving paths that are different from shortest paths.



### Context-Aware Query Selection for Active Learning in Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.04406v1
- **DOI**: 10.1109/TPAMI.2018.2878696
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04406v1)
- **Published**: 2019-04-09 00:58:23+00:00
- **Updated**: 2019-04-09 00:58:23+00:00
- **Authors**: Mahmudul Hasan, Sujoy Paul, Anastasios I. Mourikis, Amit K. Roy-Chowdhury
- **Comment**: To appear in Transactions of Pattern Pattern Analysis and Machine
  Intelligence (T-PAMI)
- **Journal**: None
- **Summary**: Activity recognition is a challenging problem with many practical applications. In addition to the visual features, recent approaches have benefited from the use of context, e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled, entirely available beforehand, and not designed to be updated continuously, which make them unsuitable for surveillance applications. In contrast, we propose a continuous-learning framework for context-aware activity recognition from unlabeled video, which has two distinct advantages over existing methods. First, it employs a novel active-learning technique that not only exploits the informativeness of the individual activities but also utilizes their contextual information during query selection; this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field model that encodes the context and devise an information-theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative queries, which are labeled by a human. These labels are combined with graphical inference techniques for incremental updates. We provide a theoretical formulation of the active learning framework with an analytic solution. Experiments on six challenging datasets demonstrate that our framework achieves superior performance with significantly less manual labeling.



### 3D Quantum Cuts for Automatic Segmentation of Porous Media in Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1904.04412v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04412v2)
- **Published**: 2019-04-09 01:43:24+00:00
- **Updated**: 2019-04-10 06:38:04+00:00
- **Authors**: Junaid Malik, Serkan Kiranyaz, Riyadh Al-Raoush, Olivier Monga, Patricia Garnier, Sebti Foufou, Abdelaziz Bouras, Alexandros Iosifidis, Moncef Gabbouj, Philippe C. Baveye
- **Comment**: None
- **Journal**: None
- **Summary**: Binary segmentation of volumetric images of porous media is a crucial step towards gaining a deeper understanding of the factors governing biogeochemical processes at minute scales. Contemporary work primarily revolves around primitive techniques based on global or local adaptive thresholding that have known common drawbacks in image segmentation. Moreover, absence of a unified benchmark prohibits quantitative evaluation, which further clouds the impact of existing methodologies. In this study, we tackle the issue on both fronts. Firstly, by drawing parallels with natural image segmentation, we propose a novel, and automatic segmentation technique, 3D Quantum Cuts (QCuts-3D) grounded on a state-of-the-art spectral clustering technique. Secondly, we curate and present a publicly available dataset of 68 multiphase volumetric images of porous media with diverse solid geometries, along with voxel-wise ground truth annotations for each constituting phase. We provide comparative evaluations between QCuts-3D and the current state-of-the-art over this dataset across a variety of evaluation metrics. The proposed systematic approach achieves a 26% increase in AUROC while achieving a substantial reduction of the computational complexity of the state-of-the-art competitors. Moreover, statistical analysis reveals that the proposed method exhibits significant robustness against the compositional variations of porous media.



### Embryo staging with weakly-supervised region selection and dynamically-decoded predictions
- **Arxiv ID**: http://arxiv.org/abs/1904.04419v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04419v1)
- **Published**: 2019-04-09 02:03:08+00:00
- **Updated**: 2019-04-09 02:03:08+00:00
- **Authors**: Tingfung Lau, Nathan Ng, Julian Gingold, Nina Desai, Julian McAuley, Zachary C. Lipton
- **Comment**: None
- **Journal**: None
- **Summary**: To optimize clinical outcomes, fertility clinics must strategically select which embryos to transfer. Common selection heuristics are formulas expressed in terms of the durations required to reach various developmental milestones, quantities historically annotated manually by experienced embryologists based on time-lapse EmbryoScope videos. We propose a new method for automatic embryo staging that exploits several sources of structure in this time-lapse data. First, noting that in each image the embryo occupies a small subregion, we jointly train a region proposal network with the downstream classifier to isolate the embryo. Notably, because we lack ground-truth bounding boxes, our we weakly supervise the region proposal network optimizing its parameters via reinforcement learning to improve the downstream classifier's loss. Moreover, noting that embryos reaching the blastocyst stage progress monotonically through earlier stages, we develop a dynamic-programming-based decoder that post-processes our predictions to select the most likely monotonic sequence of developmental stages. Our methods outperform vanilla residual networks and rival the best numbers in contemporary papers, as measured by both per-frame accuracy and transition prediction error, despite operating on smaller data than many.



### FPGA/DNN Co-Design: An Efficient Design Methodology for IoT Intelligence on the Edge
- **Arxiv ID**: http://arxiv.org/abs/1904.04421v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04421v1)
- **Published**: 2019-04-09 02:06:16+00:00
- **Updated**: 2019-04-09 02:06:16+00:00
- **Authors**: Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow, Wen-mei Hwu, Deming Chen
- **Comment**: Accepted by Design Automation Conference (DAC'2019)
- **Journal**: None
- **Summary**: While embedded FPGAs are attractive platforms for DNN acceleration on edge-devices due to their low latency and high energy efficiency, the scarcity of resources of edge-scale FPGA devices also makes it challenging for DNN deployment. In this paper, we propose a simultaneous FPGA/DNN co-design methodology with both bottom-up and top-down approaches: a bottom-up hardware-oriented DNN model search for high accuracy, and a top-down FPGA accelerator design considering DNN-specific characteristics. We also build an automatic co-design flow, including an Auto-DNN engine to perform hardware-oriented DNN model search, as well as an Auto-HLS engine to generate synthesizable C code of the FPGA accelerator for explored DNNs. We demonstrate our co-design approach on an object detection task using PYNQ-Z1 FPGA. Results show that our proposed DNN model and accelerator outperform the state-of-the-art FPGA designs in all aspects including Intersection-over-Union (IoU) (6.2% higher), frames per second (FPS) (2.48X higher), power consumption (40% lower), and energy efficiency (2.5X higher). Compared to GPU-based solutions, our designs deliver similar accuracy but consume far less energy.



### 3D Point Cloud Denoising via Deep Neural Network based Local Surface Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.04427v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1904.04427v1)
- **Published**: 2019-04-09 02:29:39+00:00
- **Updated**: 2019-04-09 02:29:39+00:00
- **Authors**: Chaojing Duan, Siheng Chen, Jelena Kovacevic
- **Comment**: None
- **Journal**: None
- **Summary**: We present a neural-network-based architecture for 3D point cloud denoising called neural projection denoising (NPD). In our previous work, we proposed a two-stage denoising algorithm, which first estimates reference planes and follows by projecting noisy points to estimated reference planes. Since the estimated reference planes are inevitably noisy, multi-projection is applied to stabilize the denoising performance. NPD algorithm uses a neural network to estimate reference planes for points in noisy point clouds. With more accurate estimations of reference planes, we are able to achieve better denoising performances with only one-time projection. To the best of our knowledge, NPD is the first work to denoise 3D point clouds with deep learning techniques. To conduct the experiments, we sample 40000 point clouds from the 3D data in ShapeNet to train a network and sample 350 point clouds from the 3D data in ModelNet10 to test. Experimental results show that our algorithm can estimate normal vectors of points in noisy point clouds. Comparing to five competitive methods, the proposed algorithm achieves better denoising performance and produces much smaller variances.



### Label Super Resolution with Inter-Instance Loss
- **Arxiv ID**: http://arxiv.org/abs/1904.04429v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04429v2)
- **Published**: 2019-04-09 02:35:03+00:00
- **Updated**: 2020-01-07 21:50:57+00:00
- **Authors**: Maozheng Zhao, Le Hou, Han Le, Dimitris Samaras, Nebojsa Jojic, Danielle Fassler, Tahsin Kurc, Rajarsi Gupta, Kolya Malkin, Shroyer Kenneth, Joel Saltz
- **Comment**: None
- **Journal**: None
- **Summary**: For the task of semantic segmentation, high-resolution (pixel-level) ground truth is very expensive to collect, especially for high resolution images such as gigapixel pathology images. On the other hand, collecting low resolution labels (labels for a block of pixels) for these high resolution images is much more cost efficient. Conventional methods trained on these low-resolution labels are only capable of giving low-resolution predictions. The existing state-of-the-art label super resolution (LSR) method is capable of predicting high resolution labels, using only low-resolution supervision, given the joint distribution between low resolution and high resolution labels. However, it does not consider the inter-instance variance which is crucial in the ideal mathematical formulation. In this work, we propose a novel loss function modeling the inter-instance variance. We test our method on a real world application: infiltrating breast cancer region segmentation in histopathology slides. Experimental results show the effectiveness of our method.



### Efficient Decision-based Black-box Adversarial Attacks on Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.04433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04433v1)
- **Published**: 2019-04-09 02:45:35+00:00
- **Updated**: 2019-04-09 02:45:35+00:00
- **Authors**: Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.



### Reliable and Efficient Image Cropping: A Grid Anchor based Approach
- **Arxiv ID**: http://arxiv.org/abs/1904.04441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04441v1)
- **Published**: 2019-04-09 03:18:20+00:00
- **Updated**: 2019-04-09 03:18:20+00:00
- **Authors**: Hui Zeng, Lida Li, Zisheng Cao, Lei Zhang
- **Comment**: To appear in CVPR 2019. Code and dataset are released
- **Journal**: None
- **Summary**: Image cropping aims to improve the composition as well as aesthetic quality of an image by removing extraneous content from it. Existing image cropping databases provide only one or several human-annotated bounding boxes as the groundtruth, which cannot reflect the non-uniqueness and flexibility of image cropping in practice. The employed evaluation metrics such as intersection-over-union cannot reliably reflect the real performance of cropping models, either. This work revisits the problem of image cropping, and presents a grid anchor based formulation by considering the special properties and requirements (e.g., local redundancy, content preservation, aspect ratio) of image cropping. Our formulation reduces the searching space of candidate crops from millions to less than one hundred. Consequently, a grid anchor based cropping benchmark is constructed, where all crops of each image are annotated and more reliable evaluation metrics are defined. We also design an effective and lightweight network module, which simultaneously considers the region of interest and region of discard for more accurate image cropping. Our model can stably output visually pleasing crops for images of different scenes and run at a speed of 125 FPS. Code and dataset are available at: https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping.



### Multimodal Style Transfer via Graph Cuts
- **Arxiv ID**: http://arxiv.org/abs/1904.04443v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04443v6)
- **Published**: 2019-04-09 03:23:20+00:00
- **Updated**: 2020-01-07 15:03:47+00:00
- **Authors**: Yulun Zhang, Chen Fang, Yilin Wang, Zhaowen Wang, Zhe Lin, Yun Fu, Jimei Yang
- **Comment**: Accepted to ICCV 2019. Typos in Eqs. (11) and (12) have been fixed in
  arXiv V2 and this version (V6). Code: https://github.com/yulunzhang/MST
- **Journal**: None
- **Summary**: An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST.



### Semi-Supervised Segmentation of Salt Bodies in Seismic Images using an Ensemble of Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04445v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04445v3)
- **Published**: 2019-04-09 03:25:21+00:00
- **Updated**: 2019-08-05 16:44:47+00:00
- **Authors**: Yauhen Babakhin, Artsiom Sanakoyeu, Hirotoshi Kitamura
- **Comment**: Accepted at GCPR 2019, Source code:
  https://github.com/ybabakhin/kaggle_salt_bes_phalanx
- **Journal**: None
- **Summary**: Seismic image analysis plays a crucial role in a wide range of industrial applications and has been receiving significant attention. One of the essential challenges of seismic imaging is detecting subsurface salt structure which is indispensable for identification of hydrocarbon reservoirs and drill path planning. Unfortunately, exact identification of large salt deposits is notoriously difficult and professional seismic imaging often requires expert human interpretation of salt bodies. Convolutional neural networks (CNNs) have been successfully applied in many fields, and several attempts have been made in the field of seismic imaging. But the high cost of manual annotations by geophysics experts and scarce publicly available labeled datasets hinder the performance of the existing CNN-based methods. In this work, we propose a semi-supervised method for segmentation (delineation) of salt bodies in seismic images which utilizes unlabeled data for multi-round self-training. To reduce error amplification during self-training we propose a scheme which uses an ensemble of CNNs. We show that our approach outperforms state-of-the-art on the TGS Salt Identification Challenge dataset and is ranked the first among the 3234 competing methods.



### Ultrafast Video Attention Prediction with Coupled Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.04449v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04449v2)
- **Published**: 2019-04-09 03:32:08+00:00
- **Updated**: 2020-01-02 07:35:44+00:00
- **Authors**: Kui Fu, Peipei Shi, Yafei Song, Shiming Ge, Xiangju Lu, Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: Large convolutional neural network models have recently demonstrated impressive performance on video attention prediction. Conventionally, these models are with intensive computation and large memory. To address these issues, we design an extremely light-weight network with ultrafast speed, named UVA-Net. The network is constructed based on depth-wise convolutions and takes low-resolution images as input. However, this straight-forward acceleration method will decrease performance dramatically. To this end, we propose a coupled knowledge distillation strategy to augment and train the network effectively. With this strategy, the model can further automatically discover and emphasize implicit useful cues contained in the data. Both spatial and temporal knowledge learned by the high-resolution complex teacher networks also can be distilled and transferred into the proposed low-resolution light-weight spatiotemporal network. Experimental results show that the performance of our model is comparable to 11 state-of-the-art models in video attention prediction, while it costs only 0.68 MB memory footprint, runs about 10,106 FPS on GPU and 404 FPS on CPU, which is 206 times faster than previous models.



### SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.04452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04452v1)
- **Published**: 2019-04-09 03:36:47+00:00
- **Updated**: 2019-04-09 03:36:47+00:00
- **Authors**: Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng
- **Comment**: to appear in CVPR'19
- **Journal**: None
- **Summary**: The greatest challenge facing visual object tracking is the simultaneous requirements on robustness and discrimination power. In this paper, we propose a SiamFC-based tracker, named SPM-Tracker, to tackle this challenge. The basic idea is to address the two requirements in two separate matching stages. Robustness is strengthened in the coarse matching (CM) stage through generalized training while discrimination power is enhanced in the fine matching (FM) stage through a distance learning network. The two stages are connected in series as the input proposals of the FM stage are generated by the CM stage. They are also connected in parallel as the matching scores and box location refinements are fused to generate the final results. This innovative series-parallel structure takes advantage of both stages and results in superior performance. The proposed SPM-Tracker, running at 120fps on GPU, achieves an AUC of 0.687 on OTB-100 and an EAO of 0.434 on VOT-16, exceeding other real-time trackers by a notable margin.



### Intra-Ensemble in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04466v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04466v2)
- **Published**: 2019-04-09 04:53:17+00:00
- **Updated**: 2020-05-10 02:09:23+00:00
- **Authors**: Yuan Gao, Zixiang Cai, Lei Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Improving model performance is always the key problem in machine learning including deep learning. However, stand-alone neural networks always suffer from marginal effect when stacking more layers. At the same time, ensemble is an useful technique to further enhance model performance. Nevertheless, training several independent deep neural networks for ensemble costs multiple resources. If so, is it possible to utilize ensemble in only one neural network? In this work, we propose Intra-Ensemble, an end-to-end ensemble strategy with stochastic channel recombination operations to train several sub-networks simultaneously within one neural network. Additional parameter size is marginal since the majority of parameters are mutually shared. Meanwhile, stochastic channel recombination significantly increases the diversity of sub-networks, which finally enhances ensemble performance. Extensive experiments and ablation studies prove the applicability of intra-ensemble on various kinds of datasets and network architectures.



### MVF-Net: Multi-View 3D Face Morphable Model Regression
- **Arxiv ID**: http://arxiv.org/abs/1904.04473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04473v1)
- **Published**: 2019-04-09 05:42:11+00:00
- **Updated**: 2019-04-09 05:42:11+00:00
- **Authors**: Fanzi Wu, Linchao Bao, Yajing Chen, Yonggen Ling, Yibing Song, Songnan Li, King Ngi Ngan, Wei Liu
- **Comment**: 2019 Conference on Computer Vision and Pattern Recognition
- **Journal**: None
- **Summary**: We address the problem of recovering the 3D geometry of a human face from a set of facial images in multiple views. While recent studies have shown impressive progress in 3D Morphable Model (3DMM) based facial reconstruction, the settings are mostly restricted to a single view. There is an inherent drawback in the single-view setting: the lack of reliable 3D constraints can cause unresolvable ambiguities. We in this paper explore 3DMM-based shape recovery in a different setting, where a set of multi-view facial images are given as input. A novel approach is proposed to regress 3DMM parameters from multi-view inputs with an end-to-end trainable Convolutional Neural Network (CNN). Multiview geometric constraints are incorporated into the network by establishing dense correspondences between different views leveraging a novel self-supervised view alignment loss. The main ingredient of the view alignment loss is a differentiable dense optical flow estimator that can backpropagate the alignment errors between an input view and a synthetic rendering from another input view, which is projected to the target view through the 3D shape to be inferred. Through minimizing the view alignment loss, better 3D shapes can be recovered such that the synthetic projections from one view to another can better align with the observed image. Extensive experiments demonstrate the superiority of the proposed method over other 3DMM methods.



### UG$^{2+}$ Track 2: A Collective Benchmark Effort for Evaluating and Advancing Image Understanding in Poor Visibility Environments
- **Arxiv ID**: http://arxiv.org/abs/1904.04474v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04474v4)
- **Published**: 2019-04-09 05:48:15+00:00
- **Updated**: 2020-03-31 10:13:20+00:00
- **Authors**: Ye Yuan, Wenhan Yang, Wenqi Ren, Jiaying Liu, Walter J. Scheirer, Zhangyang Wang
- **Comment**: A summary paper on datasets, fact sheets, baseline results, challenge
  results, and winning methods in UG$^{2+}$ Challenge (Track 2). More materials
  are provided in http://www.ug2challenge.org/index.html
- **Journal**: None
- **Summary**: The UG$^{2+}$ challenge in IEEE CVPR 2019 aims to evoke a comprehensive discussion and exploration about how low-level vision techniques can benefit the high-level automatic visual recognition in various scenarios. In its second track, we focus on object or face detection in poor visibility enhancements caused by bad weathers (haze, rain) and low light conditions. While existing enhancement methods are empirically expected to help the high-level end task, that is observed to not always be the case in practice. To provide a more thorough examination and fair comparison, we introduce three benchmark sets collected in real-world hazy, rainy, and low-light conditions, respectively, with annotate objects/faces annotated. To our best knowledge, this is the first and currently largest effort of its kind. Baseline results by cascading existing enhancement and detection models are reported, indicating the highly challenging nature of our new data as well as the large room for further technical innovations. We expect a large participation from the broad research community to address these challenges together.



### Convolutional Temporal Attention Model for Video-based Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.04492v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04492v2)
- **Published**: 2019-04-09 07:03:53+00:00
- **Updated**: 2019-04-10 07:19:33+00:00
- **Authors**: Tanzila Rahman, Mrigank Rochan, Yang Wang
- **Comment**: 6 pages, 4 figures, ICME 2019
- **Journal**: ICME 2019
- **Summary**: The goal of video-based person re-identification is to match two input videos, so that the distance of the two videos is small if two videos contain the same person. A common approach for person re-identification is to first extract image features for all frames in the video, then aggregate all the features to form a video-level feature. The video-level features of two videos can then be used to calculate the distance of the two videos. In this paper, we propose a temporal attention approach for aggregating frame-level features into a video-level feature vector for re-identification. Our method is motivated by the fact that not all frames in a video are equally informative. We propose a fully convolutional temporal attention model for generating the attention scores. Fully convolutional network (FCN) has been widely used in semantic segmentation for generating 2D output maps. In this paper, we formulate video based person re-identification as a sequence labeling problem like semantic segmentation. We establish a connection between them and modify FCN to generate attention scores to represent the importance of each frame. Extensive experiments on three different benchmark datasets (i.e. iLIDS-VID, PRID-2011 and SDU-VID) show that our proposed method outperforms other state-of-the-art approaches.



### High-Resolution Representations for Labeling Pixels and Regions
- **Arxiv ID**: http://arxiv.org/abs/1904.04514v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04514v1)
- **Published**: 2019-04-09 08:08:16+00:00
- **Updated**: 2019-04-09 08:08:16+00:00
- **Authors**: Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu, Yadong Mu, Xinggang Wang, Wenyu Liu, Jingdong Wang
- **Comment**: None
- **Journal**: None
- **Summary**: High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet)~\cite{SunXLW19}, recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in \emph{parallel} and produces strong high-resolution representations by repeatedly conducting fusions across parallel convolutions.   In this paper, we conduct a further study on high-resolution representations by introducing a simple yet effective modification and apply it to a wide range of vision tasks. We augment the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions rather than only the representation from the high-resolution convolution as done in~\cite{SunXLW19}. This simple modification leads to stronger representations, evidenced by superior results. We show top results in semantic segmentation on Cityscapes, LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, $300$W, and WFLW. In addition, we build a multi-level representation from the high-resolution representation and apply it to the Faster R-CNN object detection framework and the extended frameworks. The proposed approach achieves superior results to existing single-model networks on COCO object detection. The code and models have been publicly available at \url{https://github.com/HRNet}.



### Uncertainty Measures and Prediction Quality Rating for the Semantic Segmentation of Nested Multi Resolution Street Scene Images
- **Arxiv ID**: http://arxiv.org/abs/1904.04516v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, 62-07
- **Links**: [PDF](http://arxiv.org/pdf/1904.04516v1)
- **Published**: 2019-04-09 08:14:09+00:00
- **Updated**: 2019-04-09 08:14:09+00:00
- **Authors**: Matthias Rottmann, Marius Schubert
- **Comment**: None
- **Journal**: None
- **Summary**: In the semantic segmentation of street scenes the reliability of the prediction and therefore uncertainty measures are of highest interest. We present a method that generates for each input image a hierarchy of nested crops around the image center and presents these, all re-scaled to the same size, to a neural network for semantic segmentation. The resulting softmax outputs are then post processed such that we can investigate mean and variance over all image crops as well as mean and variance of uncertainty heat maps obtained from pixel-wise uncertainty measures, like the entropy, applied to each crop's softmax output. In our tests, we use the publicly available DeepLabv3+ MobilenetV2 network (trained on the Cityscapes dataset) and demonstrate that the incorporation of crops improves the quality of the prediction and that we obtain more reliable uncertainty measures. These are then aggregated over predicted segments for either classifying between IoU=0 and IoU>0 (meta classification) or predicting the IoU via linear regression (meta regression). The latter yields reliable performance estimates for segmentation networks, in particular useful in the absence of ground truth. For the task of meta classification we obtain a classification accuracy of $81.93\%$ and an AUROC of $89.89\%$. For meta regression we obtain an $R^2$ value of $84.77\%$. These results yield significant improvements compared to other approaches.



### Regression Concept Vectors for Bidirectional Explanations in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/1904.04520v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04520v1)
- **Published**: 2019-04-09 08:26:02+00:00
- **Updated**: 2019-04-09 08:26:02+00:00
- **Authors**: Mara Graziani, Vincent Andrearczyk, Henning Müller
- **Comment**: 9 pages, 3 figures, 3 tables
- **Journal**: Understanding and Interpreting Machine Learning in Medical Image
  Computing Applications: First International Workshops, Proceedings. Vol.
  11038. Springer, 2018
- **Summary**: Explanations for deep neural network predictions in terms of domain-related concepts can be valuable in medical applications, where justifications are important for confidence in the decision-making. In this work, we propose a methodology to exploit continuous concept measures as Regression Concept Vectors (RCVs) in the activation space of a layer. The directional derivative of the decision function along the RCVs represents the network sensitivity to increasing values of a given concept measure. When applied to breast cancer grading, nuclei texture emerges as a relevant concept in the detection of tumor tissue in breast lymph node samples. We evaluate score robustness and consistency by statistical analysis.



### QANet -- Quality Assurance Network for Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.08503v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08503v5)
- **Published**: 2019-04-09 08:38:57+00:00
- **Updated**: 2019-11-05 19:09:56+00:00
- **Authors**: Assaf Arbelle, Eliav Elul, Tammy Riklin Raviv
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel Deep Learning framework, which quantitatively estimates image segmentation quality without the need for human inspection or labeling. We refer to this method as a Quality Assurance Network -- QANet. Specifically, given an image and a `proposed' corresponding segmentation, obtained by any method including manual annotation, the QANet solves a regression problem in order to estimate a predefined quality measure with respect to the unknown ground truth. The QANet is by no means yet another segmentation method. Instead, it performs a multi-level, multi-feature comparison of an image-segmentation pair based on a unique network architecture, called the RibCage.   To demonstrate the strength of the QANet, we addressed the evaluation of instance segmentation using two different datasets from different domains, namely, high throughput live cell microscopy images from the Cell Segmentation Benchmark and natural images of plants from the Leaf Segmentation Challenge. While synthesized segmentations were used to train the QANet, it was tested on segmentations obtained by publicly available methods that participated in the different challenges. We show that the QANet accurately estimates the scores of the evaluated segmentations with respect to the hidden ground truth, as published by the challenges' organizers.   The code is available at: TBD.



### Graphonomy: Universal Human Parsing via Graph Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04536v1)
- **Published**: 2019-04-09 08:49:18+00:00
- **Updated**: 2019-04-09 08:49:18+00:00
- **Authors**: Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, Liang Lin
- **Comment**: Accepted to CVPR 2019. The Code is available at
  https://github.com/Gaoyiminggithub/Graphonomy
- **Journal**: None
- **Summary**: Prior highly-tuned human parsing models tend to fit towards each dataset in a specific domain or with discrepant label granularity, and can hardly be adapted to other human parsing tasks without extensive re-training. In this paper, we aim to learn a single universal human parsing model that can tackle all kinds of human parsing needs by unifying label annotations from different domains or at various levels of granularity. This poses many fundamental learning challenges, e.g. discovering underlying semantic structures among different label granularity, performing proper transfer learning across different image domains, and identifying and utilizing label redundancies across related tasks.   To address these challenges, we propose a new universal human parsing agent, named "Graphonomy", which incorporates hierarchical graph transfer learning upon the conventional parsing network to encode the underlying label semantic structures and propagate relevant semantic information. In particular, Graphonomy first learns and propagates compact high-level graph representation among the labels within one dataset via Intra-Graph Reasoning, and then transfers semantic information across multiple datasets via Inter-Graph Transfer. Various graph transfer dependencies (\eg, similarity, linguistic knowledge) between different datasets are analyzed and encoded to enhance graph transfer capability. By distilling universal semantic graph representation to each specific task, Graphonomy is able to predict all levels of parsing labels in one system without piling up the complexity. Experimental results show Graphonomy effectively achieves the state-of-the-art results on three human parsing benchmarks as well as advantageous universal human parsing performance.



### PUNCH: Positive UNlabelled Classification based information retrieval in Hyperspectral images
- **Arxiv ID**: http://arxiv.org/abs/1904.04547v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04547v1)
- **Published**: 2019-04-09 09:00:28+00:00
- **Updated**: 2019-04-09 09:00:28+00:00
- **Authors**: Anirban Santara, Jayeeta Datta, Sourav Sarkar, Ankur Garg, Kirti Padia, Pabitra Mitra
- **Comment**: 9 pages, under review at ACMMM-2019
- **Journal**: None
- **Summary**: Hyperspectral images of land-cover captured by airborne or satellite-mounted sensors provide a rich source of information about the chemical composition of the materials present in a given place. This makes hyperspectral imaging an important tool for earth sciences, land-cover studies, and military and strategic applications. However, the scarcity of labeled training examples and spatial variability of spectral signature are two of the biggest challenges faced by hyperspectral image classification. In order to address these issues, we aim to develop a framework for material-agnostic information retrieval in hyperspectral images based on Positive-Unlabelled (PU) classification. Given a hyperspectral scene, the user labels some positive samples of a material he/she is looking for and our goal is to retrieve all the remaining instances of the query material in the scene. Additionally, we require the system to work equally well for any material in any scene without the user having to disclose the identity of the query material. This material-agnostic nature of the framework provides it with superior generalization abilities. We explore two alternative approaches to solve the hyperspectral image classification problem within this framework. The first approach is an adaptation of non-negative risk estimation based PU learning for hyperspectral data. The second approach is based on one-versus-all positive-negative classification where the negative class is approximately sampled using a novel spectral-spatial retrieval model. We propose two annotator models - uniform and blob - that represent the labelling patterns of a human annotator. We compare the performances of the proposed algorithms for each annotator model on three benchmark hyperspectral image datasets - Indian Pines, Pavia University and Salinas.



### BoLTVOS: Box-Level Tracking for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.04552v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04552v2)
- **Published**: 2019-04-09 09:16:26+00:00
- **Updated**: 2019-12-29 16:47:22+00:00
- **Authors**: Paul Voigtlaender, Jonathon Luiten, Bastian Leibe
- **Comment**: None
- **Journal**: None
- **Summary**: We approach video object segmentation (VOS) by splitting the task into two sub-tasks: bounding box level tracking, followed by bounding box segmentation. Following this paradigm, we present BoLTVOS (Box-Level Tracking for VOS), which consists of an R-CNN detector conditioned on the first-frame bounding box to detect the object of interest, a temporal consistency rescoring algorithm, and a Box2Seg network that converts bounding boxes to segmentation masks. BoLTVOS performs VOS using only the firstframe bounding box without the mask. We evaluate our approach on DAVIS 2017 and YouTube-VOS, and show that it outperforms all methods that do not perform first-frame fine-tuning. We further present BoLTVOS-ft, which learns to segment the object in question using the first-frame mask while it is being tracked, without increasing the runtime. BoLTVOS-ft outperforms PReMVOS, the previously best performing VOS method on DAVIS 2016 and YouTube-VOS, while running up to 45 times faster. Our bounding box tracker also outperforms all previous short-term and longterm trackers on the bounding box level tracking datasets OTB 2015 and LTB35. A newer version of this work can be found at arXiv:1911.12836.



### Assessing Capsule Networks With Biased Data
- **Arxiv ID**: http://arxiv.org/abs/1904.04555v1
- **DOI**: 10.1007/978-3-030-20205-7_8
- **Categories**: **cs.CV**, 00B25
- **Links**: [PDF](http://arxiv.org/pdf/1904.04555v1)
- **Published**: 2019-04-09 09:20:29+00:00
- **Updated**: 2019-04-09 09:20:29+00:00
- **Authors**: Bruno Ferrarini, Shoaib Ehsan, Adrien Bartoli, Aleš Leonardis, Klaus D. McDonald-Maier
- **Comment**: 15 pages, 4 figures, 2 tables, Capsule Networks, Evaluation, Biased
  Data
- **Journal**: Scandinavian Conference on Image Analysis. Springer, Cham, 2019
- **Summary**: Machine learning based methods achieves impressive results in object classification and detection. Utilizing representative data of the visual world during the training phase is crucial to achieve good performance with such data driven approaches. However, it not always possible to access bias-free datasets thus, robustness to biased data is a desirable property for a learning system. Capsule Networks have been introduced recently and their tolerance to biased data has received little attention. This paper aims to fill this gap and proposes two experimental scenarios to assess the tolerance to imbalanced training data and to determine the generalization performance of a model with unfamiliar affine transformations of the images. This paper assesses dynamic routing and EM routing based Capsule Networks and proposes a comparison with Convolutional Neural Networks in the two tested scenarios. The presented results provide new insights into the behaviour of capsule networks.



### Deep Virtual Networks for Memory Efficient Inference of Multiple Tasks
- **Arxiv ID**: http://arxiv.org/abs/1904.04562v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04562v1)
- **Published**: 2019-04-09 09:34:07+00:00
- **Updated**: 2019-04-09 09:34:07+00:00
- **Authors**: Eunwoo Kim, Chanho Ahn, Philip H. S. Torr, Songhwai Oh
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Deep networks consume a large amount of memory by their nature. A natural question arises can we reduce that memory requirement whilst maintaining performance. In particular, in this work we address the problem of memory efficient learning for multiple tasks. To this end, we propose a novel network architecture producing multiple networks of different configurations, termed deep virtual networks (DVNs), for different tasks. Each DVN is specialized for a single task and structured hierarchically. The hierarchical structure, which contains multiple levels of hierarchy corresponding to different numbers of parameters, enables multiple inference for different memory budgets. The building block of a deep virtual network is based on a disjoint collection of parameters of a network, which we call a unit. The lowest level of hierarchy in a deep virtual network is a unit, and higher levels of hierarchy contain lower levels' units and other additional units. Given a budget on the number of parameters, a different level of a deep virtual network can be chosen to perform the task. A unit can be shared by different DVNs, allowing multiple DVNs in a single network. In addition, shared units provide assistance to the target task with additional knowledge learned from another tasks. This cooperative configuration of DVNs makes it possible to handle different tasks in a memory-aware manner. Our experiments show that the proposed method outperforms existing approaches for multiple tasks. Notably, ours is more efficient than others as it allows memory-aware inference for all tasks.



### 3DPeople: Modeling the Geometry of Dressed Humans
- **Arxiv ID**: http://arxiv.org/abs/1904.04571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04571v1)
- **Published**: 2019-04-09 09:57:04+00:00
- **Updated**: 2019-04-09 09:57:04+00:00
- **Authors**: Albert Pumarola, Jordi Sanchez, Gary P. T. Choi, Alberto Sanfeliu, Francesc Moreno-Noguer
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape.   First, we present 3DPeople, a large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body, we annotate the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks.   We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete.   Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images.



### Rain O'er Me: Synthesizing real rain to derain with data distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.04605v2
- **DOI**: 10.1109/TIP.2020.3005517
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04605v2)
- **Published**: 2019-04-09 11:38:24+00:00
- **Updated**: 2019-04-10 02:47:39+00:00
- **Authors**: Huangxing Lin, Yanlong Li, Xinghao Ding, Weihong Zeng, Yue Huang, John Paisley
- **Comment**: None
- **Journal**: None
- **Summary**: We present a supervised technique for learning to remove rain from images without using synthetic rain software. The method is based on a two-stage data distillation approach: 1) A rainy image is first paired with a coarsely derained version using on a simple filtering technique ("rain-to-clean"). 2) Then a clean image is randomly matched with the rainy soft-labeled pair. Through a shared deep neural network, the rain that is removed from the first image is then added to the clean image to generate a second pair ("clean-to-rain"). The neural network simultaneously learns to map both images such that high resolution structure in the clean images can inform the deraining of the rainy images. Demonstrations show that this approach can address those visual characteristics of rain not easily synthesized by software in the usual way.



### Automated Search for Configurations of Deep Neural Network Architectures
- **Arxiv ID**: http://arxiv.org/abs/1904.04612v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04612v1)
- **Published**: 2019-04-09 11:56:28+00:00
- **Updated**: 2019-04-09 11:56:28+00:00
- **Authors**: Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view DNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for DNN architectures. Therefore, our contribution is threefold. First, we model the variability of DNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid DNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good DNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released %and are publicly available to support replication and future research.



### Exploring Uncertainty Measures for Image-Caption Embedding-and-Retrieval Task
- **Arxiv ID**: http://arxiv.org/abs/1904.08504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08504v1)
- **Published**: 2019-04-09 12:19:09+00:00
- **Updated**: 2019-04-09 12:19:09+00:00
- **Authors**: Kenta Hama, Takashi Matsubara, Kuniaki Uehara, Jianfei Cai
- **Comment**: None
- **Journal**: None
- **Summary**: With the wide development of black-box machine learning algorithms, particularly deep neural network (DNN), the practical demand for the reliability assessment is rapidly rising. On the basis of the concept that `Bayesian deep learning knows what it does not know,' the uncertainty of DNN outputs has been investigated as a reliability measure for the classification and regression tasks. However, in the image-caption retrieval task, well-known samples are not always easy-to-retrieve samples. This study investigates two aspects of image-caption embedding-and-retrieval systems. On one hand, we quantify feature uncertainty by considering image-caption embedding as a regression task, and use it for model averaging, which can improve the retrieval performance. On the other hand, we further quantify posterior uncertainty by considering the retrieval as a classification task, and use it as a reliability measure, which can greatly improve the retrieval performance by rejecting uncertain queries. The consistent performance of two uncertainty measures is observed with different datasets (MS COCO and Flickr30k), different deep learning architectures (dropout and batch normalization), and different similarity functions.



### Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1904.04620v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04620v2)
- **Published**: 2019-04-09 12:23:55+00:00
- **Updated**: 2019-08-12 11:11:02+00:00
- **Authors**: Jiwoong Choi, Dayoung Chun, Hyun Kim, Hyuk-Jae Lee
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.



### Towards Analyzing Semantic Robustness of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04621v4
- **DOI**: 10.1007/978-3-030-66415-2_2
- **Categories**: **cs.CV**, cs.CR, cs.LG, I.2.10; I.4; I.4; I.2.6; D.4.6; E.3
- **Links**: [PDF](http://arxiv.org/pdf/1904.04621v4)
- **Published**: 2019-04-09 12:26:55+00:00
- **Updated**: 2020-09-08 15:53:55+00:00
- **Authors**: Abdullah Hamdi, Bernard Ghanem
- **Comment**: Presented at European conference on computer vision (ECCV 2020)
  Workshop on Adversarial Robustness in the Real World (
  https://eccv20-adv-workshop.github.io/ ) [best paper award]. The code is
  available at https://github.com/ajhamdi/semantic-robustness
- **Journal**: ECCV 2020 Workshops
- **Summary**: Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNN robustness in the semantic space. We qualitatively analyze different DNNs' semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, we formalize the problem of finding robust semantic regions of the network as optimizing integral bounds and we develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different popular network architectures. We show through extensive experimentation that several networks, while trained on the same dataset and enjoying comparable accuracy, do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as a milestone towards understanding the semantic robustness of DNNs.



### Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds
- **Arxiv ID**: http://arxiv.org/abs/1904.08487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08487v1)
- **Published**: 2019-04-09 13:34:25+00:00
- **Updated**: 2019-04-09 13:34:25+00:00
- **Authors**: Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, Jian Zhuang
- **Comment**: IEEE Computer Society Conference on Computer Vision and Pattern
  Recognition(CVPR), Long Beach, CA, 2019
- **Journal**: None
- **Summary**: Cloud based medical image analysis has become popular recently due to the high computation complexities of various deep neural network (DNN) based frameworks and the increasingly large volume of medical images that need to be processed. It has been demonstrated that for medical images the transmission from local to clouds is much more expensive than the computation in the clouds itself. Towards this, 3D image compression techniques have been widely applied to reduce the data traffic. However, most of the existing image compression techniques are developed around human vision, i.e., they are designed to minimize distortions that can be perceived by human eyes. In this paper we will use deep learning based medical image segmentation as a vehicle and demonstrate that interestingly, machine and human view the compression quality differently. Medical images compressed with good quality w.r.t. human vision may result in inferior segmentation accuracy. We then design a machine vision oriented 3D image compression framework tailored for segmentation using DNNs. Our method automatically extracts and retains image features that are most important to the segmentation. Comprehensive experiments on widely adopted segmentation frameworks with HVSMR 2016 challenge dataset show that our method can achieve significantly higher segmentation accuracy at the same compression rate, or much better compression rate under the same segmentation accuracy, when compared with the existing JPEG 2000 method. To the best of the authors' knowledge, this is the first machine vision guided medical image compression framework for segmentation in the clouds.



### Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning from Radiology Reports and Label Ontology
- **Arxiv ID**: http://arxiv.org/abs/1904.04661v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04661v2)
- **Published**: 2019-04-09 13:34:31+00:00
- **Updated**: 2019-04-27 00:45:27+00:00
- **Authors**: Ke Yan, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, Ronald M. Summers
- **Comment**: CVPR 2019 oral, main paper + supplementary material
- **Journal**: None
- **Summary**: In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and describe them in the radiology report. In this paper, we study the lesion description or annotation problem. Given a lesion image, our aim is to predict a comprehensive set of relevant labels, such as the lesion's body part, type, and attributes, which may assist downstream fine-grained diagnosis. To address this task, we first design a deep learning module to extract relevant semantic labels from the radiology reports associated with the lesion images. With the images and text-mined labels, we propose a lesion annotation network (LesaNet) based on a multilabel convolutional neural network (CNN) to learn all labels holistically. Hierarchical relations and mutually exclusive relations between the labels are leveraged to improve the label prediction accuracy. The relations are utilized in a label expansion strategy and a relational hard example mining algorithm. We also attach a simple score propagation layer on LesaNet to enhance recall and explore implicit relation between labels. Multilabel metric learning is combined with classification to enable interpretable prediction. We evaluated LesaNet on the public DeepLesion dataset, which contains over 32K diverse lesion images. Experiments show that LesaNet can precisely annotate the lesions using an ontology of 171 fine-grained labels with an average AUC of 0.9344.



### Domain-Symmetric Networks for Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1904.04663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04663v2)
- **Published**: 2019-04-09 13:40:38+00:00
- **Updated**: 2019-07-03 11:07:23+00:00
- **Authors**: Yabin Zhang, Hui Tang, Kui Jia, Mingkui Tan
- **Comment**: CVPR 2019 camera ready. Codes are available at:
  https://github.com/YBZh/SymNets
- **Journal**: None
- **Summary**: Unsupervised domain adaptation aims to learn a model of classifier for unlabeled samples on the target domain, given training data of labeled samples on the source domain. Impressive progress is made recently by learning invariant features via domain-adversarial training of deep networks. In spite of the recent progress, domain adaptation is still limited in achieving the invariance of feature distributions at a finer category level. To this end, we propose in this paper a new domain adaptation method called Domain-Symmetric Networks (SymNets). The proposed SymNet is based on a symmetric design of source and target task classifiers, based on which we also construct an additional classifier that shares with them its layer neurons. To train the SymNet, we propose a novel adversarial learning objective whose key design is based on a two-level domain confusion scheme, where the category-level confusion loss improves over the domain-level one by driving the learning of intermediate network features to be invariant at the corresponding categories of the two domains. Both domain discrimination and domain confusion are implemented based on the constructed additional classifier. Since target samples are unlabeled, we also propose a scheme of cross-domain training to help learn the target classifier. Careful ablation studies show the efficacy of our proposed method. In particular, based on commonly used base networks, our SymNets achieve the new state of the art on three benchmark domain adaptation datasets.



### Multi-Target Embodied Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1904.04686v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04686v1)
- **Published**: 2019-04-09 14:10:40+00:00
- **Updated**: 2019-04-09 14:10:40+00:00
- **Authors**: Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, Dhruv Batra
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Embodied Question Answering (EQA) is a relatively new task where an agent is asked to answer questions about its environment from egocentric perception. EQA makes the fundamental assumption that every question, e.g., "what color is the car?", has exactly one target ("car") being inquired about. This assumption puts a direct limitation on the abilities of the agent. We present a generalization of EQA - Multi-Target EQA (MT-EQA). Specifically, we study questions that have multiple targets in them, such as "Is the dresser in the bedroom bigger than the oven in the kitchen?", where the agent has to navigate to multiple locations ("dresser in bedroom", "oven in kitchen") and perform comparative reasoning ("dresser" bigger than "oven") before it can answer a question. Such questions require the development of entirely new modules or components in the agent. To address this, we propose a modular architecture composed of a program generator, a controller, a navigator, and a VQA module. The program generator converts the given question into sequential executable sub-programs; the navigator guides the agent to multiple locations pertinent to the navigation-related sub-programs; and the controller learns to select relevant observations along its path. These observations are then fed to the VQA module to predict the answer. We perform detailed analysis for each of the model components and show that our joint model can outperform previous methods and strong baselines by a significant margin.



### Action Recognition from Single Timestamp Supervision in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.04689v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04689v1)
- **Published**: 2019-04-09 14:11:48+00:00
- **Updated**: 2019-04-09 14:11:48+00:00
- **Authors**: Davide Moltisanti, Sanja Fidler, Dima Damen
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Recognising actions in videos relies on labelled supervision during training, typically the start and end times of each action instance. This supervision is not only subjective, but also expensive to acquire. Weak video-level supervision has been successfully exploited for recognition in untrimmed videos, however it is challenged when the number of different actions in training videos increases. We propose a method that is supervised by single timestamps located around each action instance, in untrimmed videos. We replace expensive action bounds with sampling distributions initialised from these timestamps. We then use the classifier's response to iteratively update the sampling distributions. We demonstrate that these distributions converge to the location and extent of discriminative action segments. We evaluate our method on three datasets for fine-grained recognition, with increasing number of different actions per video, and show that single timestamps offer a reasonable compromise between recognition performance and labelling effort, performing comparably to full temporal supervision. Our update method improves top-1 test accuracy by up to 5.4%. across the evaluated datasets.



### Fast Enhanced CT Metal Artifact Reduction using Data Domain Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04691v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04691v3)
- **Published**: 2019-04-09 14:13:41+00:00
- **Updated**: 2019-08-01 16:16:35+00:00
- **Authors**: Muhammad Usman Ghani, W. Clem Karl
- **Comment**: Accepted for publication in IEEE Transactions on Computational
  Imaging
- **Journal**: None
- **Summary**: Filtered back projection (FBP) is the most widely used method for image reconstruction in X-ray computed tomography (CT) scanners. The presence of hyper-dense materials in a scene, such as metals, can strongly attenuate X-rays, producing severe streaking artifacts in the reconstruction. These metal artifacts can greatly limit subsequent object delineation and information extraction from the images, restricting their diagnostic value. This problem is particularly acute in the security domain, where there is great heterogeneity in the objects that can appear in a scene, highly accurate decisions must be made quickly. The standard practical approaches to reducing metal artifacts in CT imagery are either simplistic non-adaptive interpolation-based projection data completion methods or direct image post-processing methods. These standard approaches have had limited success. Motivated primarily by security applications, we present a new deep-learning-based metal artifact reduction (MAR) approach that tackles the problem in the projection data domain. We treat the projection data corresponding to metal objects as missing data and train an adversarial deep network to complete the missing data in the projection domain. The subsequent complete projection data is then used with FBP to reconstruct image intended to be free of artifacts. This new approach results in an end-to-end MAR algorithm that is computationally efficient so practical and fits well into existing CT workflows allowing easy adoption in existing scanners. Training deep networks can be challenging, and another contribution of our work is to demonstrate that training data generated using an accurate X-ray simulation can be used to successfully train the deep network when combined with transfer learning using limited real data sets. We demonstrate the effectiveness and potential of our algorithm on simulated and real examples.



### End-to-End Learning-Based Ultrasound Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.04696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04696v1)
- **Published**: 2019-04-09 14:24:28+00:00
- **Updated**: 2019-04-09 14:24:28+00:00
- **Authors**: Walter Simson, Rüdiger Göbl, Magdalini Paschali, Markus Krönke, Klemens Scheidhauer, Wolfgang Weber, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound imaging is caught between the quest for the highest image quality, and the necessity for clinical usability. Our contribution is two-fold: First, we propose a novel fully convolutional neural network for ultrasound reconstruction. Second, a custom loss function tailored to the modality is employed for end-to-end training of the network. We demonstrate that training a network to map time-delayed raw data to a minimum variance ground truth offers performance increases in a clinical environment. In doing so, a path is explored towards improved clinically viable ultrasound reconstruction. The proposed method displays both promising image reconstruction quality and acquisition frequency when integrated for live ultrasound scanning. A clinical evaluation is conducted to verify the diagnostic usefulness of the proposed method in a clinical setting.



### Label Propagation for Deep Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04717v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04717v1)
- **Published**: 2019-04-09 14:55:48+00:00
- **Updated**: 2019-04-09 14:55:48+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Semi-supervised learning is becoming increasingly important because it can combine data carefully labeled by humans with abundant unlabeled data to train deep neural networks. Classic methods on semi-supervised learning that have focused on transductive learning have not been fully exploited in the inductive framework followed by modern deep learning. The same holds for the manifold assumption---that similar examples should get the same prediction. In this work, we employ a transductive label propagation method that is based on the manifold assumption to make predictions on the entire dataset and use these predictions to generate pseudo-labels for the unlabeled data and train a deep neural network. At the core of the transductive method lies a nearest neighbor graph of the dataset that we create based on the embeddings of the same network.Therefore our learning process iterates between these two steps. We improve performance on several datasets especially in the few labels regime and show that our work is complementary to current state of the art.



### Generative Models for Novelty Detection: Applications in abnormal event and situational change detection from data series
- **Arxiv ID**: http://arxiv.org/abs/1904.04741v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04741v1)
- **Published**: 2019-04-09 15:40:15+00:00
- **Updated**: 2019-04-09 15:40:15+00:00
- **Authors**: Mahdyar Ravanbakhsh
- **Comment**: PhD thesis Feb. 2019
- **Journal**: None
- **Summary**: Novelty detection is a process for distinguishing the observations that differ in some respect from the observations that the model is trained on. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains observations that were not known at the training time. In other words, the novelty class is often is not presented during the training phase or not well defined.   In light of the above, one-class classifiers and generative methods can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end model is a challenging task itself. Therefore, detecting the Novel classes in unsupervised and semi-supervised settings is a crucial step in such tasks.   In this thesis, we propose several methods to model the novelty detection problem in unsupervised and semi-supervised fashion. The proposed frameworks applied to different related applications of anomaly and outlier detection tasks. The results show the superior of our proposed methods in compare to the baselines and state-of-the-art methods.



### Learning Across Tasks and Domains
- **Arxiv ID**: http://arxiv.org/abs/1904.04744v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04744v2)
- **Published**: 2019-04-09 15:48:44+00:00
- **Updated**: 2019-10-03 15:27:16+00:00
- **Authors**: Pierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, Luigi Di Stefano
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Recent works have proven that many relevant visual tasks are closely related one to another. Yet, this connection is seldom deployed in practice due to the lack of practical methodologies to transfer learned concepts across different training processes. In this work, we introduce a novel adaptation framework that can operate across both task and domains. Our framework learns to transfer knowledge across tasks in a fully supervised domain (e.g., synthetic data) and use this knowledge on a different domain where we have only partial supervision (e.g., real data). Our proposal is complementary to existing domain adaptation techniques and extends them to cross tasks scenarios providing additional performance gains. We prove the effectiveness of our framework across two challenging tasks (i.e., monocular depth estimation and semantic segmentation) and four different domains (Synthia, Carla, Kitti, and Cityscapes).



### Cross-Modal Self-Attention Network for Referring Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.04745v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1904.04745v1)
- **Published**: 2019-04-09 15:51:07+00:00
- **Updated**: 2019-04-09 15:51:07+00:00
- **Authors**: Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang
- **Comment**: Accepted to CVPR2019
- **Journal**: None
- **Summary**: We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.



### Segmentation of Skeletal Muscle in Thigh Dixon MRI Based on Texture Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.04747v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68U10, 92-08, I.4.6; I.4.9; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1904.04747v1)
- **Published**: 2019-04-09 15:56:06+00:00
- **Updated**: 2019-04-09 15:56:06+00:00
- **Authors**: Rafael Rodrigues, Antonio M. G. Pinheiro
- **Comment**: None
- **Journal**: None
- **Summary**: Segmentation of skeletal muscles in Magnetic Resonance Images (MRI) is essential for the study of muscle physiology and diagnosis of muscular pathologies. However, manual segmentation of large MRI volumes is a time-consuming task. The state-of-the-art on algorithms for muscle segmentation in MRI is still not very extensive and is somewhat database-dependent. In this paper, an automated segmentation method based on AdaBoost classification of local texture features is presented. The texture descriptor consists of the Histogram of Oriented Gradients (HOG), Wavelet-based features, and a set of statistical measures computed from both the original and the Laplacian of Gaussian filtering of the grayscale MRI. The classifier performance suggests that texture analysis may be a helpful tool for designing a generalized and automated MRI muscle segmentation framework. Furthermore, an atlas-based approach to individual muscle segmentation is also described in this paper. The atlas is obtained by overlaying the muscle segmentation ground truth, provided by a radiologist, after image alignment using an appropriate affine transformation. Then, it is used to define the muscle labels upon the AdaBoost binary segmentation. The developed atlas method provides reasonable results when an accurate muscle tissue segmentation was obtained.



### User-Controllable Multi-Texture Synthesis with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04751v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04751v2)
- **Published**: 2019-04-09 15:59:16+00:00
- **Updated**: 2019-04-24 12:07:22+00:00
- **Authors**: Aibek Alanov, Max Kochurov, Denis Volkhonskiy, Daniil Yashkov, Evgeny Burnaev, Dmitry Vetrov
- **Comment**: 8 pages paper, 17 pages supplementary material
- **Journal**: None
- **Summary**: We propose a novel multi-texture synthesis model based on generative adversarial networks (GANs) with a user-controllable mechanism. The user control ability allows to explicitly specify the texture which should be generated by the model. This property follows from using an encoder part which learns a latent representation for each texture from the dataset. To ensure a dataset coverage, we use an adversarial loss function that penalizes for incorrect reproductions of a given texture. In experiments, we show that our model can learn descriptive texture manifolds for large datasets and from raw data such as a collection of high-resolution photos. Moreover, we apply our method to produce 3D textures and show that it outperforms existing baselines.



### Adversarial Learning of Disentangled and Generalizable Representations for Visual Attributes
- **Arxiv ID**: http://arxiv.org/abs/1904.04772v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04772v3)
- **Published**: 2019-04-09 16:35:21+00:00
- **Updated**: 2021-01-30 14:16:30+00:00
- **Authors**: James Oldfield, Yannis Panagakis, Mihalis A. Nicolaou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a multitude of methods for image-to-image translation have demonstrated impressive results on problems such as multi-domain or multi-attribute transfer. The vast majority of such works leverages the strengths of adversarial learning and deep convolutional autoencoders to achieve realistic results by well-capturing the target data distribution. Nevertheless, the most prominent representatives of this class of methods do not facilitate semantic structure in the latent space, and usually rely on binary domain labels for test-time transfer. This leads to rigid models, unable to capture the variance of each domain label. In this light, we propose a novel adversarial learning method that (i) facilitates the emergence of latent structure by semantically disentangling sources of variation, and (ii) encourages learning generalizable, continuous, and transferable latent codes that enable flexible attribute mixing. This is achieved by introducing a novel loss function that encourages representations to result in uniformly distributed class posteriors for disentangled attributes. In tandem with an algorithm for inducing generalizable properties, the resulting representations can be utilized for a variety of tasks such as intensity-preserving multi-attribute image translation and synthesis, without requiring labelled test data. We demonstrate the merits of the proposed method by a set of qualitative and quantitative experiments on popular databases such as MultiPIE, RaFD, and BU-3DFE, where our method outperforms other, state-of-the-art methods in tasks such as intensity-preserving multi-attribute transfer and synthesis.



### Multi-Agent Tensor Fusion for Contextual Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.04776v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04776v2)
- **Published**: 2019-04-09 16:38:11+00:00
- **Updated**: 2019-07-28 13:38:00+00:00
- **Authors**: Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, Ying Nian Wu
- **Comment**: Presented in CVPR 19:
  http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Multi-Agent_Tensor_Fusion_for_Contextual_Trajectory_Prediction_CVPR_2019_paper.html
  ; Architecture details available:
  https://github.com/programmingLearner/MATF-architecture-details
- **Journal**: None
- **Summary**: Accurate prediction of others' trajectories is essential for autonomous driving. Trajectory prediction is challenging because it requires reasoning about agents' past movements, social interactions among varying numbers and kinds of agents, constraints from the scene context, and the stochasticity of human behavior. Our approach models these interactions and constraints jointly within a novel Multi-Agent Tensor Fusion (MATF) network. Specifically, the model encodes multiple agents' past trajectories and the scene context into a Multi-Agent Tensor, then applies convolutional fusion to capture multiagent interactions while retaining the spatial structure of agents and the scene context. The model decodes recurrently to multiple agents' future trajectories, using adversarial loss to learn stochastic predictions. Experiments on both highway driving and pedestrian crowd datasets show that the model achieves state-of-the-art prediction accuracy.



### CMIR-NET : A Deep Learning Based Model For Cross-Modal Retrieval In Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/1904.04794v2
- **DOI**: 10.1016/j.patrec.2020.02.006
- **Categories**: **eess.IV**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1904.04794v2)
- **Published**: 2019-04-09 17:16:54+00:00
- **Updated**: 2019-05-24 06:19:19+00:00
- **Authors**: Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, Mihai Datcu
- **Comment**: None
- **Journal**: None
- **Summary**: We address the problem of cross-modal information retrieval in the domain of remote sensing. In particular, we are interested in two application scenarios: i) cross-modal retrieval between panchromatic (PAN) and multi-spectral imagery, and ii) multi-label image retrieval between very high resolution (VHR) images and speech based label annotations. Notice that these multi-modal retrieval scenarios are more challenging than the traditional uni-modal retrieval approaches given the inherent differences in distributions between the modalities. However, with the growing availability of multi-source remote sensing data and the scarcity of enough semantic annotations, the task of multi-modal retrieval has recently become extremely important. In this regard, we propose a novel deep neural network based architecture which is considered to learn a discriminative shared feature space for all the input modalities, suitable for semantically coherent information retrieval. Extensive experiments are carried out on the benchmark large-scale PAN - multi-spectral DSRSID dataset and the multi-label UC-Merced dataset. Together with the Merced dataset, we generate a corpus of speech signals corresponding to the labels. Superior performance with respect to the current state-of-the-art is observed in all the cases.



### Unsupervised 3D Pose Estimation with Geometric Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1904.04812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04812v1)
- **Published**: 2019-04-09 17:53:50+00:00
- **Updated**: 2019-04-09 17:53:50+00:00
- **Authors**: Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dylan Drover, Rohith MV, Stefan Stojanov, James M. Rehg
- **Comment**: None
- **Journal**: None
- **Summary**: We present an unsupervised learning approach to recover 3D human pose from 2D skeletal joints extracted from a single image. Our method does not require any multi-view image data, 3D skeletons, correspondences between 2D-3D points, or use previously learned 3D priors during training. A lifting network accepts 2D landmarks as inputs and generates a corresponding 3D skeleton estimate. During training, the recovered 3D skeleton is reprojected on random camera viewpoints to generate new "synthetic" 2D poses. By lifting the synthetic 2D poses back to 3D and re-projecting them in the original camera view, we can define self-consistency loss both in 3D and in 2D. The training can thus be self supervised by exploiting the geometric self-consistency of the lift-reproject-lift process. We show that self-consistency alone is not sufficient to generate realistic skeletons, however adding a 2D pose discriminator enables the lifter to output valid 3D poses. Additionally, to learn from 2D poses "in the wild", we train an unsupervised 2D domain adapter network to allow for an expansion of 2D data. This improves results and demonstrates the usefulness of 2D pose data for unsupervised 3D lifting. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach improves upon the previous unsupervised methods by 30% and outperforms many weakly supervised approaches that explicitly use 3D data.



### Learning from Videos with Deep Convolutional LSTM Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04817v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04817v1)
- **Published**: 2019-04-09 17:57:17+00:00
- **Updated**: 2019-04-09 17:57:17+00:00
- **Authors**: Logan Courtney, Ramavarapu Sreenivas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper explores the use of convolution LSTMs to simultaneously learn spatial- and temporal-information in videos. A deep network of convolutional LSTMs allows the model to access the entire range of temporal information at all spatial scales of the data. We describe our experiments involving convolution LSTMs for lipreading that demonstrate the model is capable of selectively choosing which spatiotemporal scales are most relevant for a particular dataset. The proposed deep architecture also holds promise in other applications where spatiotemporal features play a vital role without having to specifically cater the design of the network for the particular spatiotemporal features existent within the problem. For the Lip Reading in the Wild (LRW) dataset, our model slightly outperforms the previous state of the art (83.4% vs. 83.0%) and sets the new state of the art at 85.2% when the model is pretrained on the Lip Reading Sentences (LRS2) dataset.



### Prime Sample Attention in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.04821v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04821v2)
- **Published**: 2019-04-09 17:59:18+00:00
- **Updated**: 2019-09-15 15:40:02+00:00
- **Authors**: Yuhang Cao, Kai Chen, Chen Change Loy, Dahua Lin
- **Comment**: None
- **Journal**: None
- **Summary**: It is a common paradigm in object detection frameworks to treat all samples equally and target at maximizing the performance on average. In this work, we revisit this paradigm through a careful study on how different samples contribute to the overall performance measured in terms of mAP. Our study suggests that the samples in each mini-batch are neither independent nor equally important, and therefore a better classifier on average does not necessarily mean higher mAP. Motivated by this study, we propose the notion of Prime Samples, those that play a key role in driving the detection performance. We further develop a simple yet effective sampling and learning strategy called PrIme Sample Attention (PISA) that directs the focus of the training process towards such samples. Our experiments demonstrate that it is often more effective to focus on prime samples than hard samples when training a detector. Particularly, On the MSCOCO dataset, PISA outperforms the random sampling baseline and hard mining schemes, e.g., OHEM and Focal Loss, consistently by around 2% on both single-stage and two-stage detectors, even with a strong backbone ResNeXt-101.



### A Non-linear Differential CNN-Rendering Module for 3D Data Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1904.04850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04850v1)
- **Published**: 2019-04-09 18:04:47+00:00
- **Updated**: 2019-04-09 18:04:47+00:00
- **Authors**: Yonatan Svirsky, Andrei Sharf
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we introduce a differential rendering module which allows neural networks to efficiently process cluttered data. The module is composed of continuous piecewise differentiable functions defined as a sensor array of cells embedded in 3D space. Our module is learnable and can be easily integrated into neural networks allowing to optimize data rendering towards specific learning tasks using gradient based methods in an end-to-end fashion. Essentially, the module's sensor cells are allowed to transform independently and locally focus and sense different parts of the 3D data. Thus, through their optimization process, cells learn to focus on important parts of the data, bypassing occlusions, clutter and noise. Since sensor cells originally lie on a grid, this equals to a highly non-linear rendering of the scene into a 2D image. Our module performs especially well in presence of clutter and occlusions. Similarly, it deals well with non-linear deformations and improves classification accuracy through proper rendering of the data. In our experiments, we apply our module to demonstrate efficient localization and classification tasks in cluttered data both 2D and 3D.



### 3D Object Instance Recognition and Pose Estimation Using Triplet Loss with Dynamic Margin
- **Arxiv ID**: http://arxiv.org/abs/1904.04854v1
- **DOI**: 10.1109/IROS.2017.8202207
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.04854v1)
- **Published**: 2019-04-09 18:09:12+00:00
- **Updated**: 2019-04-09 18:09:12+00:00
- **Authors**: Sergey Zakharov, Wadim Kehl, Benjamin Planche, Andreas Hutter, Slobodan Ilic
- **Comment**: None
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), pp. 552-559. IEEE, 2017
- **Summary**: In this paper, we address the problem of 3D object instance recognition and pose estimation of localized objects in cluttered environments using convolutional neural networks. Inspired by the descriptor learning approach of Wohlhart et al., we propose a method that introduces the dynamic margin in the manifold learning triplet loss function. Such a loss function is designed to map images of different objects under different poses to a lower-dimensional, similarity-preserving descriptor space on which efficient nearest neighbor search algorithms can be applied. Introducing the dynamic margin allows for faster training times and better accuracy of the resulting low-dimensional manifolds. Furthermore, we contribute the following: adding in-plane rotations (ignored by the baseline method) to the training, proposing new background noise types that help to better mimic realistic scenarios and improve accuracy with respect to clutter, adding surface normals as another powerful image modality representing an object surface leading to better performance than merely depth, and finally implementing an efficient online batch generation that allows for better variability during the training phase. We perform an exhaustive evaluation to demonstrate the effects of our contributions. Additionally, we assess the performance of the algorithm on the large BigBIRD dataset to demonstrate good scalability properties of the pipeline with respect to the number of models.



### POSEAMM: A Unified Framework for Solving Pose Problems using an Alternating Minimization Method
- **Arxiv ID**: http://arxiv.org/abs/1904.04858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.04858v1)
- **Published**: 2019-04-09 18:31:18+00:00
- **Updated**: 2019-04-09 18:31:18+00:00
- **Authors**: Joao Campos, Joao R. Cardoso, Pedro Miraldo
- **Comment**: 12 pages, 5 figures
- **Journal**: IEEE International Conference on Robotics and Automation (ICRA),
  2019
- **Summary**: Pose estimation is one of the most important problems in computer vision. It can be divided in two different categories -- absolute and relative -- and may involve two different types of camera models: central and non-central. State-of-the-art methods have been designed to solve separately these problems. This paper presents a unified framework that is able to solve any pose problem by alternating optimization techniques between two set of parameters, rotation and translation. In order to make this possible, it is necessary to define an objective function that captures the problem at hand. Since the objective function will depend on the rotation and translation it is not possible to solve it as a simple minimization problem. Hence the use of Alternating Minimization methods, in which the function will be alternatively minimized with respect to the rotation and the translation. We show how to use our framework in three distinct pose problems. Our methods are then benchmarked with both synthetic and real data, showing their better balance between computational time and accuracy.



### SWNet: Small-World Neural Networks and Rapid Convergence
- **Arxiv ID**: http://arxiv.org/abs/1904.04862v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04862v1)
- **Published**: 2019-04-09 18:41:26+00:00
- **Updated**: 2019-04-09 18:41:26+00:00
- **Authors**: Mojan Javaheripi, Bita Darvish Rouhani, Farinaz Koushanfar
- **Comment**: None
- **Journal**: None
- **Summary**: Training large and highly accurate deep learning (DL) models is computationally costly. This cost is in great part due to the excessive number of trained parameters, which are well-known to be redundant and compressible for the execution phase. This paper proposes a novel transformation which changes the topology of the DL architecture such that it reaches an optimal cross-layer connectivity. This transformation leverages our important observation that for a set level of accuracy, convergence is fastest when network topology reaches the boundary of a Small-World Network. Small-world graphs are known to possess a specific connectivity structure that enables enhanced signal propagation among nodes. Our small-world models, called SWNets, provide several intriguing benefits: they facilitate data (gradient) flow within the network, enable feature-map reuse by adding long-range connections and accommodate various network architectures/datasets. Compared to densely connected networks (e.g., DenseNets), SWNets require a substantially fewer number of training parameters while maintaining a similar level of classification accuracy. We evaluate our networks on various DL model architectures and image classification datasets, namely, CIFAR10, CIFAR100, and ILSVRC (ImageNet). Our experiments demonstrate an average of ~2.1x improvement in convergence speed to the desired accuracy



### Knowledge Distillation for Human Action Anticipation
- **Arxiv ID**: http://arxiv.org/abs/1904.04868v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04868v2)
- **Published**: 2019-04-09 18:55:44+00:00
- **Updated**: 2021-10-03 13:05:42+00:00
- **Authors**: Vinh Tran, Yang Wang, Minh Hoai
- **Comment**: 5 pages, 3 figures
- **Journal**: ICIP 2021
- **Summary**: We consider the task of training a neural network to anticipate human actions in video. This task is challenging given the complexity of video data, the stochastic nature of the future, and the limited amount of annotated training data. In this paper, we propose a novel knowledge distillation framework that uses an action recognition network to supervise the training of an action anticipation network, guiding the latter to attend to the relevant information needed for correctly anticipating the future actions. This framework is possible thanks to a novel loss function to account for positional shifts of semantic concepts in a dynamic video. The knowledge distillation framework is a form of self-supervised learning, and it takes advantage of unlabeled data. Experimental results on JHMDB and EPIC-KITCHENS dataset show the effectiveness of our approach.



### Non-Lambertian Surface Shape and Reflectance Reconstruction Using Concentric Multi-Spectral Light Field
- **Arxiv ID**: http://arxiv.org/abs/1904.04875v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04875v2)
- **Published**: 2019-04-09 19:12:29+00:00
- **Updated**: 2019-04-19 04:48:51+00:00
- **Authors**: Mingyuan Zhou, Yu Ji, Yuqi Ding, Jinwei Ye, S. Susan Young, Jingyi Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Recovering the shape and reflectance of non-Lambertian surfaces remains a challenging problem in computer vision since the view-dependent appearance invalidates traditional photo-consistency constraint. In this paper, we introduce a novel concentric multi-spectral light field (CMSLF) design that is able to recover the shape and reflectance of surfaces with arbitrary material in one shot. Our CMSLF system consists of an array of cameras arranged on concentric circles where each ring captures a specific spectrum. Coupled with a multi-spectral ring light, we are able to sample viewpoint and lighting variations in a single shot via spectral multiplexing. We further show that such concentric camera/light setting results in a unique pattern of specular changes across views that enables robust depth estimation. We formulate a physical-based reflectance model on CMSLF to estimate depth and multi-spectral reflectance map without imposing any surface prior. Extensive synthetic and real experiments show that our method outperforms state-of-the-art light field-based techniques, especially in non-Lambertian scenes.



### Contextual Attention for Hand Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1904.04882v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04882v1)
- **Published**: 2019-04-09 19:45:42+00:00
- **Updated**: 2019-04-09 19:45:42+00:00
- **Authors**: Supreeth Narasimhaswamy, Zhengwei Wei, Yang Wang, Justin Zhang, Minh Hoai
- **Comment**: 9 pages, 9 figures
- **Journal**: None
- **Summary**: We present Hand-CNN, a novel convolutional network architecture for detecting hand masks and predicting hand orientations in unconstrained images. Hand-CNN extends MaskRCNN with a novel attention mechanism to incorporate contextual cues in the detection process. This attention mechanism can be implemented as an efficient network module that captures non-local dependencies between features. This network module can be inserted at different stages of an object detection network, and the entire detector can be trained end-to-end.   We also introduce a large-scale annotated hand dataset containing hands in unconstrained images for training and evaluation. We show that Hand-CNN outperforms existing methods on several datasets, including our hand detection benchmark and the publicly available PASCAL VOC human layout challenge. We also conduct ablation studies on hand detection to show the effectiveness of the proposed contextual attention module.



### Few-Shot Learning with Localization in Realistic Settings
- **Arxiv ID**: http://arxiv.org/abs/1904.08502v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08502v2)
- **Published**: 2019-04-09 20:20:38+00:00
- **Updated**: 2019-07-01 18:12:02+00:00
- **Authors**: Davis Wertheimer, Bharath Hariharan
- **Comment**: Appearing in CVPR 2019; added references in covariance pooling
  sections, added link to code in supplementary
- **Journal**: None
- **Summary**: Traditional recognition methods typically require large, artificially-balanced training classes, while few-shot learning methods are tested on artificially small ones. In contrast to both extremes, real world recognition problems exhibit heavy-tailed class distributions, with cluttered scenes and a mix of coarse and fine-grained class distinctions. We show that prior methods designed for few-shot learning do not work out of the box in these challenging conditions, based on a new "meta-iNat" benchmark. We introduce three parameter-free improvements: (a) better training procedures based on adapting cross-validation to meta-learning, (b) novel architectures that localize objects using limited bounding box annotations before classification, and (c) simple parameter-free expansions of the feature space based on bilinear pooling. Together, these improvements double the accuracy of state-of-the-art models on meta-iNat while generalizing to prior benchmarks, complex neural architectures, and settings with substantial domain shift.



### Gait Recognition via Disentangled Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04925v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04925v1)
- **Published**: 2019-04-09 21:49:58+00:00
- **Updated**: 2019-04-09 21:49:58+00:00
- **Authors**: Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan, Nanxin Wang
- **Comment**: To appear at CVPR 2019 as an oral presentation
- **Journal**: None
- **Summary**: Gait, the walking pattern of individuals, is one of the most important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as the gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and view angle. To remedy this issue, we propose a novel AutoEncoder framework to explicitly disentangle pose and appearance features from RGB imagery and the LSTM-based integration of pose features over time produces the gait feature. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF and FVG datasets, our method demonstrates superior performance to the state of the arts quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency.



### Towards High-fidelity Nonlinear 3D Face Morphable Model
- **Arxiv ID**: http://arxiv.org/abs/1904.04933v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04933v1)
- **Published**: 2019-04-09 22:05:35+00:00
- **Updated**: 2019-04-09 22:05:35+00:00
- **Authors**: Luan Tran, Feng Liu, Xiaoming Liu
- **Comment**: CVPR 2019. Project webpage:
  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html
- **Journal**: None
- **Summary**: Embedding 3D morphable basis functions into deep neural networks opens great potential for models with better representation power. However, to faithfully learn those models from an image collection, it requires strong regularization to overcome ambiguities involved in the learning process. This critically prevents us from learning high fidelity face models which are needed to represent face images in high level of details. To address this problem, this paper presents a novel approach to learn additional proxies as means to side-step strong regularizations, as well as, leverages to promote detailed shape/albedo. To ease the learning, we also propose to use a dual-pathway network, a carefully-designed architecture that brings a balance between global and local-based models. By improving the nonlinear 3D morphable model in both learning objective and network architecture, we present a model which is superior in capturing higher level of details than the linear or its precedent nonlinear counterparts. As a result, our model achieves state-of-the-art performance on 3D face reconstruction by solely optimizing latent representations.



