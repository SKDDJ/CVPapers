# Arxiv Papers in cs.CV on 2019-04-11
### CNN-Based Deep Architecture for Reinforced Concrete Delamination Segmentation Through Thermography
- **Arxiv ID**: http://arxiv.org/abs/1904.05509v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.05509v1)
- **Published**: 2019-04-11 03:13:41+00:00
- **Updated**: 2019-04-11 03:13:41+00:00
- **Authors**: Chongsheng Cheng, Zhexiong Shang, Zhigang Shen
- **Comment**: Accepted for the 2019 ASCE International Conference on Computing in
  Civil Engineering
- **Journal**: None
- **Summary**: Delamination assessment of the bridge deck plays a vital role for bridge health monitoring. Thermography as one of the nondestructive technologies for delamination detection has the advantage of efficient data acquisition. But there are challenges on the interpretation of data for accurate delamination shape profiling. Due to the environmental variation and the irregular presence of delamination size and depth, conventional processing methods based on temperature contrast fall short in accurate segmentation of delamination. Inspired by the recent development of deep learning architecture for image segmentation, the Convolutional Neural Network (CNN) based framework was investigated for the applicability of delamination segmentation under variations in temperature contrast and shape diffusion. The models were developed based on Dense Convolutional Network (DenseNet) and trained on thermal images collected for mimicked delamination in concrete slabs with different depths under experimental setup. The results suggested satisfactory performance of accurate profiling the delamination shapes.



### Generalizing Monocular 3D Human Pose Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1904.05512v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05512v1)
- **Published**: 2019-04-11 03:18:15+00:00
- **Updated**: 2019-04-11 03:18:15+00:00
- **Authors**: Luyang Wang, Yan Chen, Zhenhua Guo, Keyuan Qian, Mude Lin, Hongsheng Li, Jimmy S. Ren
- **Comment**: None
- **Journal**: None
- **Summary**: The availability of the large-scale labeled 3D poses in the Human3.6M dataset plays an important role in advancing the algorithms for 3D human pose estimation from a still image. We observe that recent innovation in this area mainly focuses on new techniques that explicitly address the generalization issue when using this dataset, because this database is constructed in a highly controlled environment with limited human subjects and background variations. Despite such efforts, we can show that the results of the current methods are still error-prone especially when tested against the images taken in-the-wild. In this paper, we aim to tackle this problem from a different perspective. We propose a principled approach to generate high quality 3D pose ground truth given any in-the-wild image with a person inside. We achieve this by first devising a novel stereo inspired neural network to directly map any 2D pose to high quality 3D counterpart. We then perform a carefully designed geometric searching scheme to further refine the joints. Based on this scheme, we build a large-scale dataset with 400,000 in-the-wild images and their corresponding 3D pose ground truth. This enables the training of a high quality neural network model, without specialized training scheme and auxiliary loss function, which performs favorably against the state-of-the-art 3D pose estimation methods. We also evaluate the generalization ability of our model both quantitatively and qualitatively. Results show that our approach convincingly outperforms the previous methods. We make our dataset and code publicly available.



### Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach
- **Arxiv ID**: http://arxiv.org/abs/1904.05514v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.05514v1)
- **Published**: 2019-04-11 03:34:27+00:00
- **Updated**: 2019-04-11 03:34:27+00:00
- **Authors**: Proteek Chandan Roy, Vishnu Naresh Boddeti
- **Comment**: Accepted for oral presentation at CVPR 2019
- **Journal**: None
- **Summary**: Image recognition systems have demonstrated tremendous progress over the past few decades thanks, in part, to our ability of learning compact and robust representations of images. As we witness the wide spread adoption of these systems, it is imperative to consider the problem of unintended leakage of information from an image representation, which might compromise the privacy of the data owner. This paper investigates the problem of learning an image representation that minimizes such leakage of user information. We formulate the problem as an adversarial non-zero sum game of finding a good embedding function with two competing goals: to retain as much task dependent discriminative image information as possible, while simultaneously minimizing the amount of information, as measured by entropy, about other sensitive attributes of the user. We analyze the stability and convergence dynamics of the proposed formulation using tools from non-linear systems theory and compare to that of the corresponding adversarial zero-sum game formulation that optimizes likelihood as a measure of information content. Numerical experiments on UCI, Extended Yale B, CIFAR-10 and CIFAR-100 datasets indicate that our proposed approach is able to learn image representations that exhibit high task performance while mitigating leakage of predefined sensitive information.



### Efficient and Robust Registration on the 3D Special Euclidean Group
- **Arxiv ID**: http://arxiv.org/abs/1904.05519v2
- **DOI**: 10.1109/ICCV.2019.00598
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05519v2)
- **Published**: 2019-04-11 03:52:42+00:00
- **Updated**: 2021-07-31 16:05:51+00:00
- **Authors**: Uttaran Bhattacharya, Venu Madhav Govindu
- **Comment**: 18 pages, 7 figures, 7 tables
- **Journal**: IEEE/CVF International Conference on Computer Vision (ICCV), 2019,
  pp. 5884-5893
- **Summary**: We present an accurate, robust and fast method for registration of 3D scans. Our motion estimation optimizes a robust cost function on the intrinsic representation of rigid motions, i.e., the Special Euclidean group $\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as the robustness afforded by an iteratively reweighted least squares optimization. We also generalize our approach to a joint multiview method that simultaneously solves for the registration of a set of scans. We demonstrate the efficacy of our approach by thorough experimental validation. Our approach significantly outperforms the state-of-the-art robust 3D registration method based on a line process in terms of both speed and accuracy. We also show that this line process method is a special case of our principled geometric solution. Finally, we also present scenarios where global registration based on feature correspondences fails but multiview ICP based on our robust motion estimation is successful.



### UniVSE: Robust Visual Semantic Embeddings via Structured Semantic Representations
- **Arxiv ID**: http://arxiv.org/abs/1904.05521v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05521v2)
- **Published**: 2019-04-11 04:04:06+00:00
- **Updated**: 2019-04-28 03:21:28+00:00
- **Authors**: Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, Wei-Ying Ma
- **Comment**: v1 is the full version which is accepted by CVPR 2019. v2 is the
  short version accepted by NAACL 2019 SpLU-RoboNLP workshop (in non-archival
  proceedings)
- **Journal**: None
- **Summary**: We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint space of visual and textual concepts. The space unifies the concepts at different levels, including objects, attributes, relations, and full scenes. A contrastive learning approach is proposed for the fine-grained alignment from only image-caption pairs. Moreover, we present an effective approach for enforcing the coverage of semantic components that appear in the sentence. We demonstrate the robustness of Unified VSE in defending text-domain adversarial attacks on cross-modal retrieval tasks. Such robustness also empowers the use of visual cues to resolve word dependencies in novel sentences.



### Direct Fitting of Gaussian Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/1904.05537v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.05537v2)
- **Published**: 2019-04-11 05:19:52+00:00
- **Updated**: 2019-06-12 02:34:47+00:00
- **Authors**: Leonid Keselman, Martial Hebert
- **Comment**: Accepted to the Conference on Computer and Robot Vision 2019. 8 pages
- **Journal**: None
- **Summary**: When fitting Gaussian Mixture Models to 3D geometry, the model is typically fit to point clouds, even when the shapes were obtained as 3D meshes. Here we present a formulation for fitting Gaussian Mixture Models (GMMs) directly to a triangular mesh instead of using points sampled from its surface. Part of this work analyzes a general formulation for evaluating likelihood of geometric objects. This modification enables fitting higher-quality GMMs under a wider range of initialization conditions. Additionally, models obtained from this fitting method are shown to produce an improvement in 3D registration for both meshes and RGB-D frames. This result is general and applicable to arbitrary geometric objects, including representing uncertainty from sensor measurements.



### FrameRank: A Text Processing Approach to Video Summarization
- **Arxiv ID**: http://arxiv.org/abs/1904.05544v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1904.05544v2)
- **Published**: 2019-04-11 06:16:17+00:00
- **Updated**: 2019-04-12 10:45:19+00:00
- **Authors**: Zhuo Lei, Chao Zhang, Qian Zhang, Guoping Qiu
- **Comment**: accepted by ICME 2019 oral
- **Journal**: None
- **Summary**: Video summarization has been extensively studied in the past decades. However, user-generated video summarization is much less explored since there lack large-scale video datasets within which human-generated video summaries are unambiguously defined and annotated. Toward this end, we propose a user-generated video summarization dataset - UGSum52 - that consists of 52 videos (207 minutes). In constructing the dataset, because of the subjectivity of user-generated video summarization, we manually annotate 25 summaries for each video, which are in total 1300 summaries. To the best of our knowledge, it is currently the largest dataset for user-generated video summarization.   Based on this dataset, we present FrameRank, an unsupervised video summarization method that employs a frame-to-frame level affinity graph to identify coherent and informative frames to summarize a video. We use the Kullback-Leibler(KL)-divergence-based graph to rank temporal segments according to the amount of semantic information contained in their frames. We illustrate the effectiveness of our method by applying it to three datasets SumMe, TVSum and UGSum52 and show it achieves state-of-the-art results.



### Generating Multiple Hypotheses for 3D Human Pose Estimation with Mixture Density Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05547v1)
- **Published**: 2019-04-11 06:26:57+00:00
- **Updated**: 2019-04-11 06:26:57+00:00
- **Authors**: Chen Li, Gim Hee Lee
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: 3D human pose estimation from a monocular image or 2D joints is an ill-posed problem because of depth ambiguity and occluded joints. We argue that 3D human pose estimation from a monocular input is an inverse problem where multiple feasible solutions can exist. In this paper, we propose a novel approach to generate multiple feasible hypotheses of the 3D pose from 2D joints.In contrast to existing deep learning approaches which minimize a mean square error based on an unimodal Gaussian distribution, our method is able to generate multiple feasible hypotheses of 3D pose based on a multimodal mixture density networks. Our experiments show that the 3D poses estimated by our approach from an input of 2D joints are consistent in 2D reprojections, which supports our argument that multiple solutions exist for the 2D-to-3D inverse problem. Furthermore, we show state-of-the-art performance on the Human3.6M dataset in both best hypothesis and multi-view settings, and we demonstrate the generalization capacity of our model by testing on the MPII and MPI-INF-3DHP datasets. Our code is available at the project website.



### Reasoning Visual Dialogs with Structural and Partial Observations
- **Arxiv ID**: http://arxiv.org/abs/1904.05548v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05548v2)
- **Published**: 2019-04-11 06:46:15+00:00
- **Updated**: 2019-05-28 23:40:33+00:00
- **Authors**: Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu
- **Comment**: CVPR 2019 Oral paper
- **Journal**: None
- **Summary**: We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning.



### 3D Dense Face Alignment via Graph Convolution Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05562v1)
- **Published**: 2019-04-11 07:19:07+00:00
- **Updated**: 2019-04-11 07:19:07+00:00
- **Authors**: Huawei Wei, Shuang Liang, Yichen Wei
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, 3D face reconstruction and face alignment tasks are gradually combined into one task: 3D dense face alignment. Its goal is to reconstruct the 3D geometric structure of face with pose information. In this paper, we propose a graph convolution network to regress 3D face coordinates. Our method directly performs feature learning on the 3D face mesh, where the geometric structure and details are well preserved. Extensive experiments show that our approach gains superior performance over state-of-the-art methods on several challenging datasets.



### FRNET: Flattened Residual Network for Infant MRI Skull Stripping
- **Arxiv ID**: http://arxiv.org/abs/1904.05578v1
- **DOI**: 10.1109/ISBI.2019.8759167
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05578v1)
- **Published**: 2019-04-11 08:41:19+00:00
- **Updated**: 2019-04-11 08:41:19+00:00
- **Authors**: Qian Zhang, Li Wang, Xiaopeng Zong, Weili Lin, Gang Li, Dinggang Shen
- **Comment**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI)
- **Journal**: None
- **Summary**: Skull stripping for brain MR images is a basic segmentation task. Although many methods have been proposed, most of them focused mainly on the adult MR images. Skull stripping for infant MR images is more challenging due to the small size and dynamic intensity changes of brain tissues during the early ages. In this paper, we propose a novel CNN based framework to robustly extract brain region from infant MR image without any human assistance. Specifically, we propose a simplified but more robust flattened residual network architecture (FRnet). We also introduce a new boundary loss function to highlight ambiguous and low contrast regions between brain and non-brain regions. To make the whole framework more robust to MR images with different imaging quality, we further introduce an artifact simulator for data augmentation. We have trained and tested our proposed framework on a large dataset (N=343), covering newborns to 48-month-olds, and obtained performance better than the state-of-the-art methods in all age groups.



### Recurrent Space-time Graph Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05582v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05582v4)
- **Published**: 2019-04-11 08:51:48+00:00
- **Updated**: 2019-12-23 15:18:38+00:00
- **Authors**: Andrei Nicolicioiu, Iulia Duta, Marius Leordeanu
- **Comment**: None
- **Journal**: Advances in Neural Information Processing Systems 32 {NeurIPS
  2019} pages 12838-1285
- **Summary**: Learning in the space-time domain remains a very challenging problem in machine learning and computer vision. Current computational models for understanding spatio-temporal visual data are heavily rooted in the classical single-image based paradigm. It is not yet well understood how to integrate information in space and time into a single, general model. We propose a neural graph model, recurrent in space and time, suitable for capturing both the local appearance and the complex higher-level interactions of different entities and objects within the changing world scene. Nodes and edges in our graph have dedicated neural networks for processing information. Nodes operate over features extracted from local parts in space and time and previous memory states. Edges process messages between connected nodes at different locations and spatial scales or between past and present time. Messages are passed iteratively in order to transmit information globally and establish long range interactions. Our model is general and could learn to recognize a variety of high level spatio-temporal concepts and be applied to different learning tasks. We demonstrate, through extensive experiments and ablation studies, that our model outperforms strong baselines and top published methods on recognizing complex activities in video. Moreover, we obtain state-of-the-art performance on the challenging Something-Something human-object interaction dataset.



### Black-Box Decision based Adversarial Attack with Symmetric $α$-stable Distribution
- **Arxiv ID**: http://arxiv.org/abs/1904.05586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05586v1)
- **Published**: 2019-04-11 09:02:30+00:00
- **Updated**: 2019-04-11 09:02:30+00:00
- **Authors**: Vignesh Srinivasan, Ercan E. Kuruoglu, Klaus-Robert Müller, Wojciech Samek, Shinichi Nakajima
- **Comment**: None
- **Journal**: None
- **Summary**: Developing techniques for adversarial attack and defense is an important research field for establishing reliable machine learning and its applications. Many existing methods employ Gaussian random variables for exploring the data space to find the most adversarial (for attacking) or least adversarial (for defense) point. However, the Gaussian distribution is not necessarily the optimal choice when the exploration is required to follow the complicated structure that most real-world data distributions exhibit. In this paper, we investigate how statistics of random variables affect such random walk exploration. Specifically, we generalize the Boundary Attack, a state-of-the-art black-box decision based attacking strategy, and propose the L\'evy-Attack, where the random walk is driven by symmetric $\alpha$-stable random variables. Our experiments on MNIST and CIFAR10 datasets show that the L\'evy-Attack explores the image data space more efficiently, and significantly improves the performance. Our results also give an insight into the recently found fact in the whitebox attacking scenario that the choice of the norm for measuring the amplitude of the adversarial patterns is essential.



### Reducing Lateral Visual Biases in Displays
- **Arxiv ID**: http://arxiv.org/abs/1904.05614v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05614v1)
- **Published**: 2019-04-11 10:30:11+00:00
- **Updated**: 2019-04-11 10:30:11+00:00
- **Authors**: Inbar Huberman, Raanan Fattal
- **Comment**: None
- **Journal**: None
- **Summary**: The human visual system is composed of multiple physiological components that apply multiple mechanisms in order to cope with the rich visual content it encounters. The complexity of this system leads to non-trivial relations between what we see and what we perceive, and in particular, between the raw intensities of an image that we display and the ones we perceive where various visual biases and illusions are introduced. In this paper we describe a method for reducing a large class of biases related to the lateral inhibition mechanism in the human retina where neurons suppress the activity of neighboring receptors. Among these biases are the well-known Mach bands and halos that appear around smooth and sharp image gradients as well as the appearance of false contrasts between identical regions. The new method removes these visual biases by computing an image that contains counter biases such that when this laterally-compensated image is viewed on a display, the inserted biases cancel the ones created in the retina.



### Detecting Repeating Objects using Patch Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.05629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05629v1)
- **Published**: 2019-04-11 11:17:22+00:00
- **Updated**: 2019-04-11 11:17:22+00:00
- **Authors**: Inbar Huberman, Raanan Fattal
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we describe a new method for detecting and counting a repeating object in an image. While the method relies on a fairly sophisticated deformable part model, unlike existing techniques it estimates the model parameters in an unsupervised fashion thus alleviating the need for a user-annotated training data and avoiding the associated specificity. This automatic fitting process is carried out by exploiting the recurrence of small image patches associated with the repeating object and analyzing their spatial correlation. The analysis allows us to reject outlier patches, recover the visual and shape parameters of the part model, and detect the object instances efficiently. In order to achieve a practical system which is able to cope with diverse images, we describe a simple and intuitive active-learning procedure that updates the object classification by querying the user on very few carefully chosen marginal classifications. Evaluation of the new method against the state-of-the-art techniques demonstrates its ability to achieve higher accuracy through a better user experience.



### Retinal Vessels Segmentation Based on Dilated Multi-Scale Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05644v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.05644v1)
- **Published**: 2019-04-11 11:47:58+00:00
- **Updated**: 2019-04-11 11:47:58+00:00
- **Authors**: Yun Jiang, Ning Tan, Tingting Peng, Hai Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of retinal vessels is a basic step in Diabetic retinopathy(DR) detection. Most methods based on deep convolutional neural network (DCNN) have small receptive fields, and hence they are unable to capture global context information of larger regions, with difficult to identify lesions. The final segmented retina vessels contain more noise with low classification accuracy. Therefore, in this paper, we propose a DCNN structure named as D-Net. In the proposed D-Net, the dilation convolution is used in the backbone network to obtain a larger receptive field without losing spatial resolution, so as to reduce the loss of feature information and to reduce the difficulty of tiny thin vessels segmentation. The large receptive field can better distinguished between the lesion area and the blood vessel area. In the proposed Multi-Scale Information Fusion module (MSIF), parallel convolution layers with different dilation rates are used, so that the model can obtain more dense feature information and better capture retinal vessel information of different sizes. In the decoding module, the skip layer connection is used to propagate context information to higher resolution layers, so as to prevent low-level information from passing the entire network structure. Finally, our method was verified on DRIVE, STARE and CHASE dataset. The experimental results show that our network structure outperforms some state-of-art method, such as N4-fields, U-Net, and DRIU in terms of accuracy, sensitivity, specificity, and AUCROC. Particularly, D-Net outperforms U-Net by 1.04%, 1.23% and 2.79% in DRIVE, STARE, and CHASE three dataset, respectively.



### Topological signature for periodic motion recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.06210v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06210v1)
- **Published**: 2019-04-11 11:52:00+00:00
- **Updated**: 2019-04-11 11:52:00+00:00
- **Authors**: Javier Lamar-Leon, Rocio Gonzalez-Diaz, Edel Garcia-Reyes
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1707.06982
- **Journal**: None
- **Summary**: In this paper, we present an algorithm that computes the topological signature for a given periodic motion sequence. Such signature consists of a vector obtained by persistent homology which captures the topological and geometric changes of the object that models the motion. Two topological signatures are compared simply by the angle between the corresponding vectors. With respect to gait recognition, we have tested our method using only the lowest fourth part of the body's silhouette. In this way, the impact of variations in the upper part of the body, which are very frequent in real scenarios, decreases considerably. We have also tested our method using other periodic motions such as running or jumping. Finally, we formally prove that our method is robust to small perturbations in the input data and does not depend on the number of periods contained in the periodic motion sequence.



### C-MIL: Continuation Multiple Instance Learning for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.05647v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05647v1)
- **Published**: 2019-04-11 11:59:41+00:00
- **Updated**: 2019-04-11 11:59:41+00:00
- **Authors**: Fang Wan, Chang Liu, Wei Ke, Xiangyang Ji, Jianbin Jiao, Qixiang Ye
- **Comment**: Accept by CVPR2019
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD) is a challenging task when provided with image category supervision but required to simultaneously learn object locations and object detectors. Many WSOD approaches adopt multiple instance learning (MIL) and have non-convex loss functions which are prone to get stuck into local minima (falsely localize object parts) while missing full object extent during training. In this paper, we introduce a continuation optimization method into MIL and thereby creating continuation multiple instance learning (C-MIL), with the intention of alleviating the non-convexity problem in a systematic way. We partition instances into spatially related and class related subsets, and approximate the original loss function with a series of smoothed loss functions defined within the subsets. Optimizing smoothed loss functions prevents the training procedure falling prematurely into local minima and facilitates the discovery of Stable Semantic Extremal Regions (SSERs) which indicate full object extent. On the PASCAL VOC 2007 and 2012 datasets, C-MIL improves the state-of-the-art of weakly supervised object detection and weakly supervised object localization with large margins.



### Software Based Higher Order Structural Foot Abnormality Detection Using Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1904.05651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05651v1)
- **Published**: 2019-04-11 12:09:31+00:00
- **Updated**: 2019-04-11 12:09:31+00:00
- **Authors**: Arnesh Sen, Kaustav Sen, Jayoti Das
- **Comment**: None
- **Journal**: None
- **Summary**: The entire movement of human body undergoes through a periodic process named Gait Cycle. The structure of human foot is the key element to complete the cycle successfully. Abnormality of this foot structure is an alarming form of congenital disorder which results a classification based on the geometry of the human foot print image. Image processing is one of the most efficient way to determine a number of footprint parameter to detect the severeness of disorder. This paper aims to detect the Flatfoot and High Arch foot abnormalities using one of the footprint parameters named Modified Brucken Index by biomedical image processing.



### YUVMultiNet: Real-time YUV multi-task CNN for autonomous driving
- **Arxiv ID**: http://arxiv.org/abs/1904.05673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.05673v1)
- **Published**: 2019-04-11 13:08:05+00:00
- **Updated**: 2019-04-11 13:08:05+00:00
- **Authors**: Thomas Boulay, Said El-Hachimi, Mani Kumar Surisetti, Pullarao Maddu, Saranya Kandan
- **Comment**: This paper is accepted for CVPR workshop demo
- **Journal**: None
- **Summary**: In this paper, we propose a multi-task convolutional neural network (CNN) architecture optimized for a low power automotive grade SoC. We introduce a network based on a unified architecture where the encoder is shared among the two tasks namely detection and segmentation. The pro-posed network runs at 25FPS for 1280x800 resolution. We briefly discuss the methods used to optimize the network architecture such as using native YUV image directly, optimization of layers & feature maps and applying quantization. We also focus on memory bandwidth in our design as convolutions are data intensives and most SOCs are bandwidth bottlenecked. We then demonstrate the efficiency of our proposed network for a dedicated CNN accelerators presenting the key performance indicators (KPI) for the detection and segmentation tasks obtained from the hardware execution and the corresponding run-time.



### Elucidating image-to-set prediction: An analysis of models, losses and datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.05709v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05709v2)
- **Published**: 2019-04-11 14:10:53+00:00
- **Updated**: 2020-05-27 04:02:31+00:00
- **Authors**: Luis Pineda, Amaia Salvador, Michal Drozdzal, Adriana Romero
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we identify an important reproducibility challenge in the image-to-set prediction literature that impedes proper comparisons among published methods, namely, researchers use different evaluation protocols to assess their contributions. To alleviate this issue, we introduce an image-to-set prediction benchmark suite built on top of five public datasets of increasing task complexity that are suitable for multi-label classification (VOC, COCO, NUS-WIDE, ADE20k and Recipe1M). Using the benchmark, we provide an in-depth analysis where we study the key components of current models, namely the choice of the image representation backbone as well as the set predictor design. Our results show that (1) exploiting better image representation backbones leads to higher performance boosts than enhancing set predictors, and (2) modeling both the label co-occurrences and ordering has a slight positive impact in terms of performance, whereas explicit cardinality prediction only helps when training on complex datasets, such as Recipe1M. To facilitate future image-to-set prediction research, we make the code, best models and dataset splits publicly available at: https://github.com/facebookresearch/image-to-set.



### Reconstructing Network Inputs with Additive Perturbation Signatures
- **Arxiv ID**: http://arxiv.org/abs/1904.05712v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1904.05712v1)
- **Published**: 2019-04-11 14:17:52+00:00
- **Updated**: 2019-04-11 14:17:52+00:00
- **Authors**: Nick Moran, Chiraag Juvekar
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present preliminary results demonstrating the ability to recover a significant amount of information about secret model inputs given only very limited access to model outputs and the ability evaluate the model on additive perturbations to the input.



### FTGAN: A Fully-trained Generative Adversarial Networks for Text to Face Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.05729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05729v1)
- **Published**: 2019-04-11 14:38:35+00:00
- **Updated**: 2019-04-11 14:38:35+00:00
- **Authors**: Xiang Chen, Lingbo Qing, Xiaohai He, Xiaodong Luo, Yining Xu
- **Comment**: None
- **Journal**: None
- **Summary**: As a sub-domain of text-to-image synthesis, text-to-face generation has huge potentials in public safety domain. With lack of dataset, there are almost no related research focusing on text-to-face synthesis. In this paper, we propose a fully-trained Generative Adversarial Network (FTGAN) that trains the text encoder and image decoder at the same time for fine-grained text-to-face generation. With a novel fully-trained generative network, FTGAN can synthesize higher-quality images and urge the outputs of the FTGAN are more relevant to the input sentences. In addition, we build a dataset called SCU-Text2face for text-to-face synthesis. Through extensive experiments, the FTGAN shows its superiority in boosting both generated images' quality and similarity to the input descriptions. The proposed FTGAN outperforms the previous state of the art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On SCU-text2face, the face images generated by our proposed FTGAN just based on the input descriptions is of average 59% similarity to the ground-truth, which set a baseline for text-to-face synthesis.



### A Relation-Augmented Fully Convolutional Network for Semantic Segmentation in Aerial Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.05730v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05730v3)
- **Published**: 2019-04-11 14:40:03+00:00
- **Updated**: 2020-05-19 12:19:05+00:00
- **Authors**: Lichao Mou, Yuansheng Hua, Xiao Xiang Zhu
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: Most current semantic segmentation approaches fall back on deep convolutional neural networks (CNNs). However, their use of convolution operations with local receptive fields causes failures in modeling contextual spatial relations. Prior works have sought to address this issue by using graphical models or spatial propagation modules in networks. But such models often fail to capture long-range spatial relationships between entities, which leads to spatially fragmented predictions. Moreover, recent works have demonstrated that channel-wise information also acts a pivotal part in CNNs. In this work, we introduce two simple yet effective network units, the spatial relation module and the channel relation module, to learn and reason about global relationships between any two spatial positions or feature maps, and then produce relation-augmented feature representations. The spatial and channel relation modules are general and extensible, and can be used in a plug-and-play fashion with the existing fully convolutional network (FCN) framework. We evaluate relation module-equipped networks on semantic segmentation tasks using two aerial image datasets, which fundamentally depend on long-range spatial relational reasoning. The networks achieve very competitive results, bringing significant improvements over baselines.



### An In-Depth Study on Open-Set Camera Model Identification
- **Arxiv ID**: http://arxiv.org/abs/1904.08497v2
- **DOI**: 10.1109/ACCESS.2019.2921436
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08497v2)
- **Published**: 2019-04-11 14:48:55+00:00
- **Updated**: 2019-11-13 19:53:21+00:00
- **Authors**: Pedro Ribeiro Mendes Júnior, Luca Bondi, Paolo Bestagini, Stefano Tubaro, Anderson Rocha
- **Comment**: Published through IEEE Access journal
- **Journal**: None
- **Summary**: Camera model identification refers to the problem of linking a picture to the camera model used to shoot it. As this might be an enabling factor in different forensic applications to single out possible suspects (e.g., detecting the author of child abuse or terrorist propaganda material), many accurate camera model attribution methods have been developed in the literature. One of their main drawbacks, however, is the typical closed-set assumption of the problem. This means that an investigated photograph is always assigned to one camera model within a set of known ones present during investigation, i.e., training time, and the fact that the picture can come from a completely unrelated camera model during actual testing is usually ignored. Under realistic conditions, it is not possible to assume that every picture under analysis belongs to one of the available camera models. To deal with this issue, in this paper, we present the first in-depth study on the possibility of solving the camera model identification problem in open-set scenarios. Given a photograph, we aim at detecting whether it comes from one of the known camera models of interest or from an unknown one. We compare different feature extraction algorithms and classifiers specially targeting open-set recognition. We also evaluate possible open-set training protocols that can be applied along with any open-set classifier, observing that a simple of those alternatives obtains best results. Thorough testing on independent datasets shows that it is possible to leverage a recently proposed convolutional neural network as feature extractor paired with a properly trained open-set classifier aiming at solving the open-set camera model attribution problem even to small-scale image patches, improving over state-of-the-art available solutions.



### Learning joint reconstruction of hands and manipulated objects
- **Arxiv ID**: http://arxiv.org/abs/1904.05767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05767v1)
- **Published**: 2019-04-11 15:21:33+00:00
- **Updated**: 2019-04-11 15:21:33+00:00
- **Authors**: Yana Hasson, Gül Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, Cordelia Schmid
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.



### Difficulty-aware Image Super Resolution via Deep Adaptive Dual-Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05802v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05802v2)
- **Published**: 2019-04-11 16:00:57+00:00
- **Updated**: 2019-05-01 04:15:54+00:00
- **Authors**: Jinghui Qin, Ziwei Xie, Yukai Shi, Wushao Wen
- **Comment**: ICME2019(Oral), code and results are available at:
  https://github.com/xzwlx/Difficulty-SR
- **Journal**: None
- **Summary**: Recently, deep learning based single image super-resolution(SR) approaches have achieved great development. The state-of-the-art SR methods usually adopt a feed-forward pipeline to establish a non-linear mapping between low-res(LR) and high-res(HR) images. However, due to treating all image regions equally without considering the difficulty diversity, these approaches meet an upper bound for optimization. To address this issue, we propose a novel SR approach that discriminately processes each image region within an image by its difficulty. Specifically, we propose a dual-way SR network that one way is trained to focus on easy image regions and another is trained to handle hard image regions. To identify whether a region is easy or hard, we propose a novel image difficulty recognition network based on PSNR prior. Our SR approach that uses the region mask to adaptively enforce the dual-way SR network yields superior results. Extensive experiments on several standard benchmarks (e.g., Set5, Set14, BSD100, and Urban100) show that our approach achieves state-of-the-art performance.



### Probabilistic Permutation Synchronization using the Riemannian Structure of the Birkhoff Polytope
- **Arxiv ID**: http://arxiv.org/abs/1904.05814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.NA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.05814v1)
- **Published**: 2019-04-11 16:12:50+00:00
- **Updated**: 2019-04-11 16:12:50+00:00
- **Authors**: Tolga Birdal, Umut Şimşekli
- **Comment**: To appear as oral presentation at CVPR 2019. 20 pages including the
  supplementary material
- **Journal**: None
- **Summary**: We present an entirely new geometric and probabilistic approach to synchronization of correspondences across multiple sets of objects or images. In particular, we present two algorithms: (1) Birkhoff-Riemannian L-BFGS for optimizing the relaxed version of the combinatorially intractable cycle consistency loss in a principled manner, (2) Birkhoff-Riemannian Langevin Monte Carlo for generating samples on the Birkhoff Polytope and estimating the confidence of the found solutions. To this end, we first introduce the very recently developed Riemannian geometry of the Birkhoff Polytope. Next, we introduce a new probabilistic synchronization model in the form of a Markov Random Field (MRF). Finally, based on the first order retraction operators, we formulate our problem as simulating a stochastic differential equation and devise new integrators. We show on both synthetic and real datasets that we achieve high quality multi-graph matching results with faster convergence and reliable confidence/uncertainty estimates.



### Learning Single Camera Depth Estimation using Dual-Pixels
- **Arxiv ID**: http://arxiv.org/abs/1904.05822v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05822v3)
- **Published**: 2019-04-11 16:25:43+00:00
- **Updated**: 2019-08-14 17:52:05+00:00
- **Authors**: Rahul Garg, Neal Wadhwa, Sameer Ansari, Jonathan T. Barron
- **Comment**: Accepted to ICCV 2019 (oral)
- **Journal**: None
- **Summary**: Deep learning techniques have enabled rapid progress in monocular depth estimation, but their quality is limited by the ill-posed nature of the problem and the scarcity of high quality datasets. We estimate depth from a single camera by leveraging the dual-pixel auto-focus hardware that is increasingly common on modern camera sensors. Classic stereo algorithms and prior learning-based depth estimation techniques under-perform when applied on this dual-pixel data, the former due to too-strong assumptions about RGB image matching, and the latter due to not leveraging the understanding of optics of dual-pixel image formation. To allow learning based methods to work well on dual-pixel imagery, we identify an inherent ambiguity in the depth estimated from dual-pixel cues, and develop an approach to estimate depth up to this ambiguity. Using our approach, existing monocular depth estimation techniques can be effectively applied to dual-pixel data, and much smaller models can be constructed that still infer high quality depth. To demonstrate this, we capture a large dataset of in-the-wild 5-viewpoint RGB images paired with corresponding dual-pixel data, and show how view supervision with this data can be used to learn depth up to the unknown ambiguities. On our new task, our model is 30% more accurate than any prior work on learning-based monocular or stereoscopic depth estimation.



### Variational Information Distillation for Knowledge Transfer
- **Arxiv ID**: http://arxiv.org/abs/1904.05835v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05835v1)
- **Published**: 2019-04-11 16:39:19+00:00
- **Updated**: 2019-04-11 16:39:19+00:00
- **Authors**: Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen Dai
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.



### MAIN: Multi-Attention Instance Network for Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.05847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05847v1)
- **Published**: 2019-04-11 16:59:22+00:00
- **Updated**: 2019-04-11 16:59:22+00:00
- **Authors**: Juan Leon Alcazar, Maria A. Bravo, Ali K. Thabet, Guillaume Jeanneret, Thomas Brox, Pablo Arbelaez, Bernard Ghanem
- **Comment**: None
- **Journal**: None
- **Summary**: Instance-level video segmentation requires a solid integration of spatial and temporal information. However, current methods rely mostly on domain-specific information (online learning) to produce accurate instance-level segmentations. We propose a novel approach that relies exclusively on the integration of generic spatio-temporal attention cues. Our strategy, named Multi-Attention Instance Network (MAIN), overcomes challenging segmentation scenarios over arbitrary videos without modelling sequence- or instance-specific knowledge. We design MAIN to segment multiple instances in a single forward pass, and optimize it with a novel loss function that favors class agnostic predictions and assigns instance-specific penalties. We achieve state-of-the-art performance on the challenging Youtube-VOS dataset and benchmark, improving the unseen Jaccard and F-Metric by 6.8% and 12.7% respectively, while operating at real-time (30.3 FPS).



### Expressive Body Capture: 3D Hands, Face, and Body from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1904.05866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05866v1)
- **Published**: 2019-04-11 17:47:37+00:00
- **Updated**: 2019-04-11 17:47:37+00:00
- **Authors**: Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, Michael J. Black
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: To facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de.



### Improved training of binary networks for human pose estimation and image recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.05868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05868v1)
- **Published**: 2019-04-11 17:55:06+00:00
- **Updated**: 2019-04-11 17:55:06+00:00
- **Authors**: Adrian Bulat, Georgios Tzimiropoulos, Jean Kossaifi, Maja Pantic
- **Comment**: None
- **Journal**: None
- **Summary**: Big neural networks trained on large datasets have advanced the state-of-the-art for a large variety of challenging problems, improving performance by a large margin. However, under low memory and limited computational power constraints, the accuracy on the same problems drops considerable. In this paper, we propose a series of techniques that significantly improve the accuracy of binarized neural networks (i.e networks where both the features and the weights are binary). We evaluate the proposed improvements on two diverse tasks: fine-grained recognition (human pose estimation) and large-scale image recognition (ImageNet classification). Specifically, we introduce a series of novel methodological changes including: (a) more appropriate activation functions, (b) reverse-order initialization, (c) progressive quantization, and (d) network stacking and show that these additions improve existing state-of-the-art network binarization techniques, significantly. Additionally, for the first time, we also investigate the extent to which network binarization and knowledge distillation can be combined. When tested on the challenging MPII dataset, our method shows a performance improvement of more than 4% in absolute terms. Finally, we further validate our findings by applying the proposed techniques for large-scale object recognition on the Imagenet dataset, on which we report a reduction of error rate by 4%.



### Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning
- **Arxiv ID**: http://arxiv.org/abs/1904.05869v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.05869v2)
- **Published**: 2019-04-11 17:55:09+00:00
- **Updated**: 2020-05-08 00:53:23+00:00
- **Authors**: Karl Pertsch, Oleh Rybkin, Jingyun Yang, Shenghao Zhou, Konstantinos G. Derpanis, Kostas Daniilidis, Joseph Lim, Andrew Jaegle
- **Comment**: Conference on Learning for Dynamics and Control, 2020. Website:
  https://sites.google.com/view/keyin/home
- **Journal**: None
- **Summary**: Temporal observations such as videos contain essential information about the dynamics of the underlying scene, but they are often interleaved with inessential, predictable details. One way of dealing with this problem is by focusing on the most informative moments in a sequence. We propose a model that learns to discover these important events and the times when they occur and uses them to represent the full sequence. We do so using a hierarchical Keyframe-Inpainter (KeyIn) model that first generates a video's keyframes and then inpaints the rest by generating the frames at the intervening times. We propose a fully differentiable formulation to efficiently learn this procedure. We show that KeyIn finds informative keyframes in several datasets with different dynamics and visual properties. KeyIn outperforms other recent hierarchical predictive models for planning. For more details, please see the project website at \url{https://sites.google.com/view/keyin}.



### An Analysis of Pre-Training on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.05871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05871v1)
- **Published**: 2019-04-11 17:58:23+00:00
- **Updated**: 2019-04-11 17:58:23+00:00
- **Authors**: Hengduo Li, Bharat Singh, Mahyar Najibi, Zuxuan Wu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We provide a detailed analysis of convolutional neural networks which are pre-trained on the task of object detection. To this end, we train detectors on large datasets like OpenImagesV4, ImageNet Localization and COCO. We analyze how well their features generalize to tasks like image classification, semantic segmentation and object detection on small datasets like PASCAL-VOC, Caltech-256, SUN-397, Flowers-102 etc. Some important conclusions from our analysis are --- 1) Pre-training on large detection datasets is crucial for fine-tuning on small detection datasets, especially when precise localization is needed. For example, we obtain 81.1% mAP on the PASCAL-VOC dataset at 0.7 IoU after pre-training on OpenImagesV4, which is 7.6% better than the recently proposed DeformableConvNetsV2 which uses ImageNet pre-training. 2) Detection pre-training also benefits other localization tasks like semantic segmentation but adversely affects image classification. 3) Features for images (like avg. pooled Conv5) which are similar in the object detection feature space are likely to be similar in the image classification feature space but the converse is not true. 4) Visualization of features reveals that detection neurons have activations over an entire object, while activations for classification networks typically focus on parts. Therefore, detection networks are poor at classification when multiple instances are present in an image or when an instance only covers a small fraction of an image.



### An Empirical Study of Spatial Attention Mechanisms in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05873v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05873v1)
- **Published**: 2019-04-11 17:58:37+00:00
- **Updated**: 2019-04-11 17:58:37+00:00
- **Authors**: Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, Jifeng Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.



### Compressing deep neural networks by matrix product operators
- **Arxiv ID**: http://arxiv.org/abs/1904.06194v2
- **DOI**: 10.1103/PhysRevResearch.2.023300
- **Categories**: **cs.LG**, cs.CV, cs.NE, physics.comp-ph, quant-ph
- **Links**: [PDF](http://arxiv.org/pdf/1904.06194v2)
- **Published**: 2019-04-11 17:59:00+00:00
- **Updated**: 2020-06-10 03:26:01+00:00
- **Authors**: Ze-Feng Gao, Song Cheng, Rong-Qiang He, Z. Y. Xie, Hui-Hai Zhao, Zhong-Yi Lu, Tao Xiang
- **Comment**: 8+9 pages, 3+7 figures, 2+11 tables
- **Journal**: Phys. Rev. Research 2, 023300 (2020)
- **Summary**: A deep neural network is a parametrization of a multilayer mapping of signals in terms of many alternatively arranged linear and nonlinear transformations. The linear transformations, which are generally used in the fully connected as well as convolutional layers, contain most of the variational parameters that are trained and stored. Compressing a deep neural network to reduce its number of variational parameters but not its prediction power is an important but challenging problem toward the establishment of an optimized scheme in training efficiently these parameters and in lowering the risk of overfitting. Here we show that this problem can be effectively solved by representing linear transformations with matrix product operators (MPOs), which is a tensor network originally proposed in physics to characterize the short-range entanglement in one-dimensional quantum states. We have tested this approach in five typical neural networks, including FC2, LeNet-5, VGG, ResNet, and DenseNet on two widely used data sets, namely, MNIST and CIFAR-10, and found that this MPO representation indeed sets up a faithful and efficient mapping between input and output signals, which can keep or even improve the prediction accuracy with a dramatically reduced number of parameters. Our method greatly simplifies the representations in deep learning, and opens a possible route toward establishing a framework of modern neural networks which might be simpler and cheaper, but more efficient.



### A Simple Baseline for Audio-Visual Scene-Aware Dialog
- **Arxiv ID**: http://arxiv.org/abs/1904.05876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1904.05876v1)
- **Published**: 2019-04-11 17:59:51+00:00
- **Updated**: 2019-04-11 17:59:51+00:00
- **Authors**: Idan Schwartz, Alexander Schwing, Tamir Hazan
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: The recently proposed audio-visual scene-aware dialog task paves the way to a more data-driven way of learning virtual assistants, smart speakers and car navigation systems. However, very little is known to date about how to effectively extract meaningful information from a plethora of sensors that pound the computational engine of those devices. Therefore, in this paper, we provide and carefully analyze a simple baseline for audio-visual scene-aware dialog which is trained end-to-end. Our method differentiates in a data-driven manner useful signals from distracting ones using an attention mechanism. We evaluate the proposed approach on the recently introduced and challenging audio-visual scene-aware dataset, and demonstrate the key features that permit to outperform the current state-of-the-art by more than 20\% on CIDEr.



### Max-Sliced Wasserstein Distance and its use for GANs
- **Arxiv ID**: http://arxiv.org/abs/1904.05877v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.05877v1)
- **Published**: 2019-04-11 17:59:57+00:00
- **Updated**: 2019-04-11 17:59:57+00:00
- **Authors**: Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, Alexander Schwing
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Generative adversarial nets (GANs) and variational auto-encoders have significantly improved our distribution modeling capabilities, showing promise for dataset augmentation, image-to-image translation and feature learning. However, to model high-dimensional distributions, sequential training and stacked architectures are common, increasing the number of tunable hyper-parameters as well as the training time. Nonetheless, the sample complexity of the distance metrics remains one of the factors affecting GAN training. We first show that the recently proposed sliced Wasserstein distance has compelling sample complexity properties when compared to the Wasserstein distance. To further improve the sliced Wasserstein distance we then analyze its `projection complexity' and develop the max-sliced Wasserstein distance which enjoys compelling sample complexity while reducing projection complexity, albeit necessitating a max estimation. We finally illustrate that the proposed distance trains GANs on high-dimensional images up to a resolution of 256x256 easily.



### Two Body Problem: Collaborative Visual Task Completion
- **Arxiv ID**: http://arxiv.org/abs/1904.05879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1904.05879v1)
- **Published**: 2019-04-11 17:59:57+00:00
- **Updated**: 2019-04-11 17:59:57+00:00
- **Authors**: Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander Schwing, Aniruddha Kembhavi
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Collaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/two-body-problem



### Factor Graph Attention
- **Arxiv ID**: http://arxiv.org/abs/1904.05880v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05880v3)
- **Published**: 2019-04-11 17:59:58+00:00
- **Updated**: 2020-03-07 23:35:13+00:00
- **Authors**: Idan Schwartz, Seunghak Yu, Tamir Hazan, Alexander Schwing
- **Comment**: Accepted to CVPR 2019; revised version includes bottom-up features
- **Journal**: None
- **Summary**: Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.



### Synthetic Examples Improve Generalization for Rare Classes
- **Arxiv ID**: http://arxiv.org/abs/1904.05916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05916v2)
- **Published**: 2019-04-11 18:28:43+00:00
- **Updated**: 2019-05-14 04:27:29+00:00
- **Authors**: Sara Beery, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Markus Meister, Neel Joshi, Pietro Perona
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to detect and classify rare occurrences in images has important applications - for example, counting rare and endangered species when studying biodiversity, or detecting infrequent traffic scenarios that pose a danger to self-driving cars. Few-shot learning is an open problem: current computer vision systems struggle to categorize objects they have seen only rarely during training, and collecting a sufficient number of training examples of rare events is often challenging and expensive, and sometimes outright impossible. We explore in depth an approach to this problem: complementing the few available training images with ad-hoc simulated data.   Our testbed is animal species classification, which has a real-world long-tailed distribution. We analyze the effect of different axes of variation in simulation, such as pose, lighting, model, and simulation method, and we prescribe best practices for efficiently incorporating simulated data for real-world performance gain. Our experiments reveal that synthetic data can considerably reduce error rates for classes that are rare, that as the amount of simulated data is increased, accuracy on the target class improves, and that high variation of simulated data provides maximum performance gain.



### Learning Digital Camera Pipeline for Extreme Low-Light Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.05939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05939v1)
- **Published**: 2019-04-11 19:49:31+00:00
- **Updated**: 2019-04-11 19:49:31+00:00
- **Authors**: Syed Waqas Zamir, Aditya Arora, Salman Khan, Fahad Shahbaz Khan, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: In low-light conditions, a conventional camera imaging pipeline produces sub-optimal images that are usually dark and noisy due to a low photon count and low signal-to-noise ratio (SNR). We present a data-driven approach that learns the desired properties of well-exposed images and reflects them in images that are captured in extremely low ambient light environments, thereby significantly improving the visual quality of these low-light images. We propose a new loss function that exploits the characteristics of both pixel-wise and perceptual metrics, enabling our deep neural network to learn the camera processing pipeline to transform the short-exposure, low-light RAW sensor data to well-exposed sRGB images. The results show that our method outperforms the state-of-the-art according to psychophysical tests as well as pixel-wise standard metrics and recent learning-based perceptual image quality measures.



### Absolute Human Pose Estimation with Depth Prediction Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05947v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05947v1)
- **Published**: 2019-04-11 20:26:37+00:00
- **Updated**: 2019-04-11 20:26:37+00:00
- **Authors**: Márton Véges, András Lőrincz
- **Comment**: Accepted to IJCNN 2019
- **Journal**: None
- **Summary**: The common approach to 3D human pose estimation is predicting the body joint coordinates relative to the hip. This works well for a single person but is insufficient in the case of multiple interacting people. Methods predicting absolute coordinates first estimate a root-relative pose then calculate the translation via a secondary optimization task. We propose a neural network that predicts joints in a camera centered coordinate system instead of a root-relative one. Unlike previous methods, our network works in a single step without any post-processing. Our network beats previous methods on the MuPoTS-3D dataset and achieves state-of-the-art results.



### Automatic Pulmonary Nodule Detection in CT Scans Using Convolutional Neural Networks Based on Maximum Intensity Projection
- **Arxiv ID**: http://arxiv.org/abs/1904.05956v2
- **DOI**: 10.1109/TMI.2019.2935553
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05956v2)
- **Published**: 2019-04-11 21:12:33+00:00
- **Updated**: 2019-06-10 09:54:55+00:00
- **Authors**: Sunyi Zheng, Jiapan Guo, Xiaonan Cui, Raymond N. J. Veldhuis, Matthijs Oudkerk, Peter M. A. van Ooijen
- **Comment**: Submitted to IEEE TMI
- **Journal**: None
- **Summary**: Accurate pulmonary nodule detection is a crucial step in lung cancer screening. Computer-aided detection (CAD) systems are not routinely used by radiologists for pulmonary nodule detection in clinical practice despite their potential benefits. Maximum intensity projection (MIP) images improve the detection of pulmonary nodules in radiological evaluation with computed tomography (CT) scans. Inspired by the clinical methodology of radiologists, we aim to explore the feasibility of applying MIP images to improve the effectiveness of automatic lung nodule detection using convolutional neural networks (CNNs). We propose a CNN-based approach that takes MIP images of different slab thicknesses (5 mm, 10 mm, 15 mm) and 1 mm axial section slices as input. Such an approach augments the two-dimensional (2-D) CT slice images with more representative spatial information that helps discriminate nodules from vessels through their morphologies. Our proposed method achieves sensitivity of 92.67% with 1 false positive per scan and sensitivity of 94.19% with 2 false positives per scan for lung nodule detection on 888 scans in the LIDC-IDRI dataset. The use of thick MIP images helps the detection of small pulmonary nodules (3 mm-10 mm) and results in fewer false positives. Experimental results show that utilizing MIP images can increase the sensitivity and lower the number of false positives, which demonstrates the effectiveness and significance of the proposed MIP-based CNNs framework for automatic pulmonary nodule detection in CT scans. The proposed method also shows the potential that CNNs could gain benefits for nodule detection by combining the clinical procedure.



### TAFE-Net: Task-Aware Feature Embeddings for Low Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.05967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.05967v1)
- **Published**: 2019-04-11 22:14:45+00:00
- **Updated**: 2019-04-11 22:14:45+00:00
- **Authors**: Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, Joseph E. Gonzalez
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: Learning good feature embeddings for images often requires substantial training data. As a consequence, in settings where training data is limited (e.g., few-shot and zero-shot learning), we are typically forced to use a generic feature embedding across various tasks. Ideally, we want to construct feature embeddings that are tuned for the given task. In this work, we propose Task-Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt the image representation to a new task in a meta learning fashion. Our network is composed of a meta learner and a prediction network. Based on a task input, the meta learner generates parameters for the feature layers in the prediction network so that the feature embedding can be accurately adjusted for that task. We show that TAFE-Net is highly effective in generalizing to new tasks or concepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot and few-shot learning. Our model matches or exceeds the state-of-the-art on all tasks. In particular, our approach improves the prediction accuracy of unseen attribute-object pairs by 4 to 15 points on the challenging visual attribute-object composition task.



### The Sound of Motions
- **Arxiv ID**: http://arxiv.org/abs/1904.05979v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1904.05979v1)
- **Published**: 2019-04-11 23:05:52+00:00
- **Updated**: 2019-04-11 23:05:52+00:00
- **Authors**: Hang Zhao, Chuang Gan, Wei-Chiu Ma, Antonio Torralba
- **Comment**: None
- **Journal**: None
- **Summary**: Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.



### Adaptive Hierarchical Down-Sampling for Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.08506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08506v2)
- **Published**: 2019-04-11 23:10:13+00:00
- **Updated**: 2020-05-22 20:50:01+00:00
- **Authors**: Ehsan Nezhadarya, Ehsan Taghavi, Ryan Razani, Bingbing Liu, Jun Luo
- **Comment**: None
- **Journal**: 2020 Conference on Computer Vision and Pattern Recognition
- **Summary**: While several convolution-like operators have recently been proposed for extracting features out of point clouds, down-sampling an unordered point cloud in a deep neural network has not been rigorously studied. Existing methods down-sample the points regardless of their importance for the output. As a result, some important points in the point cloud may be removed, while less valuable points may be passed to the next layers. In contrast, adaptive down-sampling methods sample the points by taking into account the importance of each point, which varies based on the application, task and training data. In this paper, we propose a permutation-invariant learning-based adaptive down-sampling layer, called Critical Points Layer (CPL), which reduces the number of points in an unordered point cloud while retaining the important points. Unlike most graph-based point cloud down-sampling methods that use $k$-NN search algorithm to find the neighbouring points, CPL is a global down-sampling method, rendering it computationally very efficient. The proposed layer can be used along with any graph-based point cloud convolution layer to form a convolutional neural network, dubbed CP-Net in this paper. We introduce a CP-Net for $3$D object classification that achieves the best accuracy for the ModelNet$40$ dataset among point cloud-based methods, which validates the effectiveness of the CPL.



### Cramnet: Layer-wise Deep Neural Network Compression with Knowledge Transfer from a Teacher Network
- **Arxiv ID**: http://arxiv.org/abs/1904.05982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.05982v1)
- **Published**: 2019-04-11 23:28:05+00:00
- **Updated**: 2019-04-11 23:28:05+00:00
- **Authors**: Jon Hoffman
- **Comment**: Thesis for Masters degree
- **Journal**: None
- **Summary**: Neural Networks accomplish amazing things, but they suffer from computational and memory bottlenecks that restrict their usage. Nowhere can this be better seen than in the mobile space, where specialized hardware is being created just to satisfy the demand for neural networks. Previous studies have shown that neural networks have vastly more connections than they actually need to do their work. This thesis develops a method that can compress networks to less than 10% of memory and less than 25% of computational power, without loss of accuracy, and without creating sparse networks that require special code to run.



### The iWildCam 2018 Challenge Dataset
- **Arxiv ID**: http://arxiv.org/abs/1904.05986v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05986v2)
- **Published**: 2019-04-11 23:48:19+00:00
- **Updated**: 2019-04-24 20:21:23+00:00
- **Authors**: Sara Beery, Grant van Horn, Oisin Mac Aodha, Pietro Perona
- **Comment**: Challenge hosted at the fifth Fine-Grained Visual Categorization
  Workshop (FGVC5) at CVPR 2018
- **Journal**: None
- **Summary**: Camera traps are a valuable tool for studying biodiversity, but research using this data is limited by the speed of human annotation. With the vast amounts of data now available it is imperative that we develop automatic solutions for annotating camera trap data in order to allow this research to scale. A promising approach is based on deep networks trained on human-annotated images. We provide a challenge dataset to explore whether such solutions generalize to novel locations, since systems that are trained once and may be deployed to operate automatically in new locations would be most useful.



