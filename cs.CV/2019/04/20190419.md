# Arxiv Papers in cs.CV on 2019-04-19
### Automated Focal Loss for Image based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09048v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09048v1)
- **Published**: 2019-04-19 01:24:52+00:00
- **Updated**: 2019-04-19 01:24:52+00:00
- **Authors**: Michael Weber, Michael Fürst, J. Marius Zöllner
- **Comment**: None
- **Journal**: None
- **Summary**: Current state-of-the-art object detection algorithms still suffer the problem of imbalanced distribution of training data over object classes and background. Recent work introduced a new loss function called focal loss to mitigate this problem, but at the cost of an additional hyperparameter. Manually tuning this hyperparameter for each training task is highly time-consuming.   With automated focal loss we introduce a new loss function which substitutes this hyperparameter by a parameter that is automatically adapted during the training progress and controls the amount of focusing on hard training examples. We show on the COCO benchmark that this leads to an up to 30% faster training convergence. We further introduced a focal regression loss which on the more challenging task of 3D vehicle detection outperforms other loss functions by up to 1.8 AOS and can be used as a value range independent metric for regression.



### Three dimensional blind image deconvolution for fluorescence microscopy using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09974v1
- **DOI**: 10.1109/ISBI.2019.8759250
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09974v1)
- **Published**: 2019-04-19 02:02:13+00:00
- **Updated**: 2019-04-19 02:02:13+00:00
- **Authors**: Soonam Lee, Shuo Han, Paul Salama, Kenneth W. Dunn, Edward J. Delp
- **Comment**: IEEE International Symposium on Biomedical Imaging (ISBI) 2019
- **Journal**: None
- **Summary**: Due to image blurring image deconvolution is often used for studying biological structures in fluorescence microscopy. Fluorescence microscopy image volumes inherently suffer from intensity inhomogeneity, blur, and are corrupted by various types of noise which exacerbate image quality at deeper tissue depth. Therefore, quantitative analysis of fluorescence microscopy in deeper tissue still remains a challenge. This paper presents a three dimensional blind image deconvolution method for fluorescence microscopy using 3-way spatially constrained cycle-consistent adversarial networks. The restored volumes of the proposed deconvolution method and other well-known deconvolution methods, denoising methods, and an inhomogeneity correction method are visually and numerically evaluated. Experimental results indicate that the proposed method can restore and improve the quality of blurred and noisy deep depth microscopy image visually and quantitatively.



### Feature Fusion for Online Mutual Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.09058v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09058v2)
- **Published**: 2019-04-19 03:18:56+00:00
- **Updated**: 2020-07-21 11:51:14+00:00
- **Authors**: Jangho Kim, Minsung Hyun, Inseop Chung, Nojun Kwak
- **Comment**: International Conference on Pattern Recognition
- **Journal**: None
- **Summary**: We propose a learning framework named Feature Fusion Learning (FFL) that efficiently trains a powerful classifier through a fusion module which combines the feature maps generated from parallel neural networks. Specifically, we train a number of parallel neural networks as sub-networks, then we combine the feature maps from each sub-network using a fusion module to create a more meaningful feature map. The fused feature map is passed into the fused classifier for overall classification. Unlike existing feature fusion methods, in our framework, an ensemble of sub-network classifiers transfers its knowledge to the fused classifier and then the fused classifier delivers its knowledge back to each sub-network, mutually teaching one another in an online-knowledge distillation manner. This mutually teaching system not only improves the performance of the fused classifier but also obtains performance gain in each sub-network. Moreover, our model is more beneficial because different types of network can be used for each sub-network. We have performed a variety of experiments on multiple datasets such as CIFAR-10, CIFAR-100 and ImageNet and proved that our method is more effective than other alternative methods in terms of performance of both sub-networks and the fused classifier.



### Feature Forwarding for Efficient Single Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1904.09059v2
- **DOI**: None
- **Categories**: **cs.CV**, 65D19, I.4.4
- **Links**: [PDF](http://arxiv.org/pdf/1904.09059v2)
- **Published**: 2019-04-19 03:20:52+00:00
- **Updated**: 2019-05-03 14:45:50+00:00
- **Authors**: Peter Morales, Tzofi Klinghoffer, Seung Jae Lee
- **Comment**: Accepted to the NTIRE 2019 CVPR Workshop. Paper number 77. 8 Pages
- **Journal**: None
- **Summary**: Haze degrades content and obscures information of images, which can negatively impact vision-based decision-making in real-time systems. In this paper, we propose an efficient fully convolutional neural network (CNN) image dehazing method designed to run on edge graphical processing units (GPUs). We utilize three variants of our architecture to explore the dependency of dehazed image quality on parameter count and model design. The first two variants presented, a small and big version, make use of a single efficient encoder-decoder convolutional feature extractor. The final variant utilizes a pair of encoder-decoders for atmospheric light and transmission map estimation. Each variant ends with an image refinement pyramid pooling network to form the final dehazed image. For the big variant of the single-encoder network, we demonstrate state-of-the-art performance on the NYU Depth dataset. For the small variant, we maintain competitive performance on the super-resolution O/I-HAZE datasets without the need for image cropping. Finally, we examine some challenges presented by the Dense-Haze dataset when leveraging CNN architectures for dehazing of dense haze imagery and examine the impact of loss function selection on image quality. Benchmarks are included to show the feasibility of introducing this approach into real-time systems.



### Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts
- **Arxiv ID**: http://arxiv.org/abs/1904.09073v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09073v3)
- **Published**: 2019-04-19 04:28:17+00:00
- **Updated**: 2019-11-07 14:06:58+00:00
- **Authors**: Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, Ajay Divakaran
- **Comment**: Accepted at EMNLP'2019; Added dataset link
- **Journal**: None
- **Summary**: Computing author intent from multimodal data like Instagram posts requires modeling a complex relationship between text and image. For example, a caption might evoke an ironic contrast with the image, so neither caption nor image is a mere transcript of the other. Instead they combine -- via what has been called meaning multiplication -- to create a new meaning that has a more complex relation to the literal meanings of text and image. Here we introduce a multimodal dataset of 1299 Instagram posts labeled for three orthogonal taxonomies: the authorial intent behind the image-caption pair, the contextual relationship between the literal meanings of the image and caption, and the semiotic relationship between the signified meanings of the image and caption. We build a baseline deep multimodal classifier to validate the taxonomy, showing that employing both text and image improves intent detection by 9.6% compared to using only the image modality, demonstrating the commonality of non-intersective meaning multiplication. The gain with multimodality is greatest when the image and caption diverge semiotically. Our dataset offers a new resource for the study of the rich meanings that result from pairing text and image.



### Advanced Deep Convolutional Neural Network Approaches for Digital Pathology Image Analysis: a comprehensive evaluation with different use cases
- **Arxiv ID**: http://arxiv.org/abs/1904.09075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09075v1)
- **Published**: 2019-04-19 04:37:41+00:00
- **Updated**: 2019-04-19 04:37:41+00:00
- **Authors**: Md Zahangir Alom, Theus Aspiras, Tarek M. Taha, Vijayan K. Asari, TJ Bowen, Dave Billiter, Simon Arkell
- **Comment**: 25 pages, 28 figures, 9 tables
- **Journal**: None
- **Summary**: Deep Learning (DL) approaches have been providing state-of-the-art performance in different modalities in the field of medical imagining including Digital Pathology Image Analysis (DPIA). Out of many different DL approaches, Deep Convolutional Neural Network (DCNN) technique provides superior performance for classification, segmentation, and detection tasks. Most of the task in DPIA problems are somehow possible to solve with classification, segmentation, and detection approaches. In addition, sometimes pre and post-processing methods are applied for solving some specific type of problems. Recently, different DCNN models including Inception residual recurrent CNN (IRRCNN), Densely Connected Recurrent Convolution Network (DCRCN), Recurrent Residual U-Net (R2U-Net), and R2U-Net based regression model (UD-Net) have proposed and provide state-of-the-art performance for different computer vision and medical image analysis tasks. However, these advanced DCNN models have not been explored for solving different problems related to DPIA. In this study, we have applied these DCNN techniques for solving different DPIA problems and evaluated on different publicly available benchmark datasets for seven different tasks in digital pathology including lymphoma classification, Invasive Ductal Carcinoma (IDC) detection, nuclei segmentation, epithelium segmentation, tubule segmentation, lymphocyte detection, and mitosis detection. The experimental results are evaluated with different performance metrics such as sensitivity, specificity, accuracy, F1-score, Receiver Operating Characteristics (ROC) curve, dice coefficient (DC), and Means Squired Errors (MSE). The results demonstrate superior performance for classification, segmentation, and detection tasks compared to existing machine learning and DCNN based approaches.



### LATTE: Accelerating LiDAR Point Cloud Annotation via Sensor Fusion, One-Click Annotation, and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.09085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09085v1)
- **Published**: 2019-04-19 05:53:14+00:00
- **Updated**: 2019-04-19 05:53:14+00:00
- **Authors**: Bernie Wang, Virginia Wu, Bichen Wu, Kurt Keutzer
- **Comment**: None
- **Journal**: None
- **Summary**: LiDAR (Light Detection And Ranging) is an essential and widely adopted sensor for autonomous vehicles, particularly for those vehicles operating at higher levels (L4-L5) of autonomy. Recent work has demonstrated the promise of deep-learning approaches for LiDAR-based detection. However, deep-learning algorithms are extremely data hungry, requiring large amounts of labeled point-cloud data for training and evaluation. Annotating LiDAR point cloud data is challenging due to the following issues: 1) A LiDAR point cloud is usually sparse and has low resolution, making it difficult for human annotators to recognize objects. 2) Compared to annotation on 2D images, the operation of drawing 3D bounding boxes or even point-wise labels on LiDAR point clouds is more complex and time-consuming. 3) LiDAR data are usually collected in sequences, so consecutive frames are highly correlated, leading to repeated annotations. To tackle these challenges, we propose LATTE, an open-sourced annotation tool for LiDAR point clouds. LATTE features the following innovations: 1) Sensor fusion: We utilize image-based detection algorithms to automatically pre-label a calibrated image, and transfer the labels to the point cloud. 2) One-click annotation: Instead of drawing 3D bounding boxes or point-wise labels, we simplify the annotation to just one click on the target object, and automatically generate the bounding box for the target. 3) Tracking: we integrate tracking into sequence annotation such that we can transfer labels from one frame to subsequent ones and therefore significantly reduce repeated labeling. Experiments show the proposed features accelerate the annotation speed by 6.2x and significantly improve label quality with 23.6% and 2.2% higher instance-level precision and recall, and 2.0% higher bounding box IoU. LATTE is open-sourced at https://github.com/bernwang/latte.



### Weakly Supervised Adversarial Domain Adaptation for Semantic Segmentation in Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.09092v1
- **DOI**: 10.1109/TIP.2019.2910667
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09092v1)
- **Published**: 2019-04-19 06:30:36+00:00
- **Updated**: 2019-04-19 06:30:36+00:00
- **Authors**: Qi Wang, Junyu Gao, Xuelong Li
- **Comment**: To appear at TIP
- **Journal**: None
- **Summary**: Semantic segmentation, a pixel-level vision task, is developed rapidly by using convolutional neural networks (CNNs). Training CNNs requires a large amount of labeled data, but manually annotating data is difficult. For emancipating manpower, in recent years, some synthetic datasets are released. However, they are still different from real scenes, which causes that training a model on the synthetic data (source domain) cannot achieve a good performance on real urban scenes (target domain). In this paper, we propose a weakly supervised adversarial domain adaptation to improve the segmentation performance from synthetic data to real scenes, which consists of three deep neural networks. To be specific, a detection and segmentation ("DS" for short) model focuses on detecting objects and predicting segmentation map; a pixel-level domain classifier ("PDC" for short) tries to distinguish the image features from which domains; an object-level domain classifier ("ODC" for short) discriminates the objects from which domains and predicts the objects classes. PDC and ODC are treated as the discriminators, and DS is considered as the generator. By adversarial learning, DS is supposed to learn domain-invariant features. In experiments, our proposed method yields the new record of mIoU metric in the same problem.



### AMNet: Deep Atrous Multiscale Stereo Disparity Estimation Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09099v1)
- **Published**: 2019-04-19 07:05:59+00:00
- **Updated**: 2019-04-19 07:05:59+00:00
- **Authors**: Xianzhi Du, Mostafa El-Khamy, Jungwon Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a new deep learning architecture for stereo disparity estimation is proposed. The proposed atrous multiscale network (AMNet) adopts an efficient feature extractor with depthwise-separable convolutions and an extended cost volume that deploys novel stereo matching costs on the deep features. A stacked atrous multiscale network is proposed to aggregate rich multiscale contextual information from the cost volume which allows for estimating the disparity with high accuracy at multiple scales. AMNet can be further modified to be a foreground-background aware network, FBA-AMNet, which is capable of discriminating between the foreground and the background objects in the scene at multiple scales. An iterative multitask learning method is proposed to train FBA-AMNet end-to-end. The proposed disparity estimation networks, AMNet and FBA-AMNet, show accurate disparity estimates and advance the state of the art on the challenging Middlebury, KITTI 2012, KITTI 2015, and Sceneflow stereo disparity estimation benchmarks.



### Deep Likelihood Network for Image Restoration with Multiple Degradation Levels
- **Arxiv ID**: http://arxiv.org/abs/1904.09105v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.09105v4)
- **Published**: 2019-04-19 07:45:28+00:00
- **Updated**: 2021-01-10 02:07:26+00:00
- **Authors**: Yiwen Guo, Ming Lu, Wangmeng Zuo, Changshui Zhang, Yurong Chen
- **Comment**: Accepted by IEEE Transactions on Image Processing; 13 pages, 6
  figures
- **Journal**: None
- **Summary**: Convolutional neural networks have been proven effective in a variety of image restoration tasks. Most state-of-the-art solutions, however, are trained using images with a single particular degradation level, and their performance deteriorates drastically when applied to other degradation settings. In this paper, we propose deep likelihood network (DL-Net), aiming at generalizing off-the-shelf image restoration networks to succeed over a spectrum of degradation levels. We slightly modify an off-the-shelf network by appending a simple recursive module, which is derived from a fidelity term, for disentangling the computation for multiple degradation levels. Extensive experimental results on image inpainting, interpolation, and super-resolution show the effectiveness of our DL-Net.



### Automated Segmentation of Pulmonary Lobes using Coordination-Guided Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09106v1
- **DOI**: 10.1109/ISBI.2019.8759492
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09106v1)
- **Published**: 2019-04-19 07:49:03+00:00
- **Updated**: 2019-04-19 07:49:03+00:00
- **Authors**: Wenjia Wang, Junxuan Chen, Jie Zhao, Ying Chi, Xuansong Xie, Li Zhang, Xiansheng Hua
- **Comment**: ISBI 2019 (Oral)
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Summary**: The identification of pulmonary lobes is of great importance in disease diagnosis and treatment. A few lung diseases have regional disorders at lobar level. Thus, an accurate segmentation of pulmonary lobes is necessary. In this work, we propose an automated segmentation of pulmonary lobes using coordination-guided deep neural networks from chest CT images. We first employ an automated lung segmentation to extract the lung area from CT image, then exploit volumetric convolutional neural network (V-net) for segmenting the pulmonary lobes. To reduce the misclassification of different lobes, we therefore adopt coordination-guided convolutional layers (CoordConvs) that generate additional feature maps of the positional information of pulmonary lobes. The proposed model is trained and evaluated on a few publicly available datasets and has achieved the state-of-the-art accuracy with a mean Dice coefficient index of 0.947 $\pm$ 0.044.



### AnonymousNet: Natural Face De-Identification with Measurable Privacy
- **Arxiv ID**: http://arxiv.org/abs/1904.12620v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.12620v1)
- **Published**: 2019-04-19 07:57:32+00:00
- **Updated**: 2019-04-19 07:57:32+00:00
- **Authors**: Tao Li, Lei Lin
- **Comment**: CVPR-19 Workshop on Computer Vision: Challenges and Opportunities for
  Privacy and Security (CV-COPS 2019)
- **Journal**: None
- **Summary**: With billions of personal images being generated from social media and cameras of all sorts on a daily basis, security and privacy are unprecedentedly challenged. Although extensive attempts have been made, existing face image de-identification techniques are either insufficient in photo-reality or incapable of balancing privacy and usability qualitatively and quantitatively, i.e., they fail to answer counterfactual questions such as "is it private now?", "how private is it?", and "can it be more private?" In this paper, we propose a novel framework called AnonymousNet, with an effort to address these issues systematically, balance usability, and enhance privacy in a natural and measurable manner. The framework encompasses four stages: facial attribute estimation, privacy-metric-oriented face obfuscation, directed natural image synthesis, and adversarial perturbation. Not only do we achieve the state-of-the-arts in terms of image quality and attribute prediction accuracy, we are also the first to show that facial privacy is measurable, can be factorized, and accordingly be manipulated in a photo-realistic fashion to fulfill different requirements and application scenarios. Experiments further demonstrate the effectiveness of the proposed framework.



### Listen to the Image
- **Arxiv ID**: http://arxiv.org/abs/1904.09115v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1904.09115v1)
- **Published**: 2019-04-19 08:13:34+00:00
- **Updated**: 2019-04-19 08:13:34+00:00
- **Authors**: Di Hu, Dong Wang, Xuelong Li, Feiping Nie, Qi Wang
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Visual-to-auditory sensory substitution devices can assist the blind in sensing the visual environment by translating the visual information into a sound pattern. To improve the translation quality, the task performances of the blind are usually employed to evaluate different encoding schemes. In contrast to the toilsome human-based assessment, we argue that machine model can be also developed for evaluation, and more efficient. To this end, we firstly propose two distinct cross-modal perception model w.r.t. the late-blind and congenitally-blind cases, which aim to generate concrete visual contents based on the translated sound. To validate the functionality of proposed models, two novel optimization strategies w.r.t. the primary encoding scheme are presented. Further, we conduct sets of human-based experiments to evaluate and compare them with the conducted machine-based assessments in the cross-modal generation task. Their highly consistent results w.r.t. different encoding schemes indicate that using machine model to accelerate optimization evaluation and reduce experimental cost is feasible to some extent, which could dramatically promote the upgrading of encoding scheme then help the blind to improve their visual perception ability.



### SelFlow: Self-Supervised Learning of Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1904.09117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09117v1)
- **Published**: 2019-04-19 08:21:16+00:00
- **Updated**: 2019-04-19 08:21:16+00:00
- **Authors**: Pengpeng Liu, Michael Lyu, Irwin King, Jia Xu
- **Comment**: Published at the Conference on Computer Vision and Pattern
  Recognition (CVPR 2019)
- **Journal**: None
- **Summary**: We present a self-supervised learning approach for optical flow. Our method distills reliable flow estimations from non-occluded pixels, and uses these predictions as ground truth to learn optical flow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better flow estimation. These two principles lead to an approach that yields the best performance for unsupervised optical flow learning on the challenging benchmarks including MPI Sintel, KITTI 2012 and 2015. More notably, our self-supervised pre-trained model provides an excellent initialization for supervised fine-tuning. Our fine-tuned models achieve state-of-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.



### Deep Q Learning Driven CT Pancreas Segmentation with Geometry-Aware U-Net
- **Arxiv ID**: http://arxiv.org/abs/1904.09120v1
- **DOI**: 10.1109/TMI.2019.2911588
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.09120v1)
- **Published**: 2019-04-19 08:36:21+00:00
- **Updated**: 2019-04-19 08:36:21+00:00
- **Authors**: Yunze Man, Yangsibo Huang, Junyi Feng, Xi Li, Fei Wu
- **Comment**: in IEEE Transactions on Medical Imaging (2019)
- **Journal**: None
- **Summary**: Segmentation of pancreas is important for medical image analysis, yet it faces great challenges of class imbalance, background distractions and non-rigid geometrical features. To address these difficulties, we introduce a Deep Q Network(DQN) driven approach with deformable U-Net to accurately segment the pancreas by explicitly interacting with contextual information and extract anisotropic features from pancreas. The DQN based model learns a context-adaptive localization policy to produce a visually tightened and precise localization bounding box of the pancreas. Furthermore, deformable U-Net captures geometry-aware information of pancreas by learning geometrically deformable filters for feature extraction. Experiments on NIH dataset validate the effectiveness of the proposed framework in pancreas segmentation.



### Multiple receptive fields and small-object-focusing weakly-supervised segmentation network for fast object detection
- **Arxiv ID**: http://arxiv.org/abs/1904.12619v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12619v2)
- **Published**: 2019-04-19 10:26:38+00:00
- **Updated**: 2019-05-22 05:50:37+00:00
- **Authors**: Siyang Sun, Yingjie Yin, Xingang Wang, De Xu, Yuan Zhao, Haifeng Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection plays an important role in various visual applications. However, the precision and speed of detector are usually contradictory. One main reason for fast detectors' precision reduction is that small objects are hard to be detected. To address this problem, we propose a multiple receptive field and small-object-focusing weakly-supervised segmentation network (MRFSWSnet) to achieve fast object detection. In MRFSWSnet, multiple receptive fields block (MRF) is used to pay attention to the object and its adjacent background's different spatial location with different weights to enhance the feature's discriminability. In addition, in order to improve the accuracy of small object detection, a small-object-focusing weakly-supervised segmentation module which only focuses on small object instead of all objects is integrated into the detection network for auxiliary training to improve the precision of small object detection. Extensive experiments show the effectiveness of our method on both PASCAL VOC and MS COCO detection datasets. In particular, with a lower resolution version of 300x300, MRFSWSnet achieves 80.9% mAP on VOC2007 test with an inference speed of 15 milliseconds per frame, which is the state-of-the-art detector among real-time detectors.



### Simple yet efficient real-time pose-based action recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09140v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.09140v1)
- **Published**: 2019-04-19 10:36:28+00:00
- **Updated**: 2019-04-19 10:36:28+00:00
- **Authors**: Dennis Ludl, Thomas Gulde, Cristóbal Curio
- **Comment**: Submitted to IEEE Intelligent Transportation Systems Conference
  (ITSC) 2019. Code will be available soon at
  https://github.com/noboevbo/ehpi_action_recognition
- **Journal**: None
- **Summary**: Recognizing human actions is a core challenge for autonomous systems as they directly share the same space with humans. Systems must be able to recognize and assess human actions in real-time. In order to train corresponding data-driven algorithms, a significant amount of annotated training data is required. We demonstrated a pipeline to detect humans, estimate their pose, track them over time and recognize their actions in real-time with standard monocular camera sensors. For action recognition, we encode the human pose into a new data format called Encoded Human Pose Image (EHPI) that can then be classified using standard methods from the computer vision community. With this simple procedure we achieve competitive state-of-the-art performance in pose-based action detection and can ensure real-time performance. In addition, we show a use case in the context of autonomous driving to demonstrate how such a system can be trained to recognize human actions using simulation data.



### Deep Learning Based Automatic Video Annotation Tool for Self-Driving Car
- **Arxiv ID**: http://arxiv.org/abs/1904.12618v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12618v1)
- **Published**: 2019-04-19 10:48:18+00:00
- **Updated**: 2019-04-19 10:48:18+00:00
- **Authors**: N. S. Manikandan, K. Ganesan
- **Comment**: 8 pages, 9 Figures
- **Journal**: None
- **Summary**: In a self-driving car, objection detection, object classification, lane detection and object tracking are considered to be the crucial modules. In recent times, using the real time video one wants to narrate the scene captured by the camera fitted in our vehicle. To effectively implement this task, deep learning techniques and automatic video annotation tools are widely used. In the present paper, we compare the various techniques that are available for each module and choose the best algorithm among them by using appropriate metrics. For object detection, YOLO and Retinanet-50 are considered and the best one is chosen based on mean Average Precision (mAP). For object classification, we consider VGG-19 and Resnet-50 and select the best algorithm based on low error rate and good accuracy. For lane detection, Udacity's 'Finding Lane Line' and deep learning based LaneNet algorithms are compared and the best one that can accurately identify the given lane is chosen for implementation. As far as object tracking is concerned, we compare Udacity's 'Object Detection and Tracking' algorithm and deep learning based Deep Sort algorithm. Based on the accuracy of tracking the same object in many frames and predicting the movement of objects, the best algorithm is chosen. Our automatic video annotation tool is found to be 83% accurate when compared with a human annotator. We considered a video with 530 frames each of resolution 1035 x 1800 pixels. At an average each frame had about 15 objects. Our annotation tool consumed 43 minutes in a CPU based system and 2.58 minutes in a mid-level GPU based system to process all four modules. But the same video took nearly 3060 minutes for one human annotator to narrate the scene in the given video. Thus we claim that our proposed automatic video annotation tool is reasonably fast (about 1200 times in a GPU system) and accurate.



### Salient Object Detection in the Deep Learning Era: An In-Depth Survey
- **Arxiv ID**: http://arxiv.org/abs/1904.09146v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09146v5)
- **Published**: 2019-04-19 11:12:54+00:00
- **Updated**: 2021-01-08 22:58:36+00:00
- **Authors**: Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, Ruigang Yang
- **Comment**: Published on IEEE TPAMI. All the saliency prediction maps, our
  constructed dataset with annotations, and codes for evaluation are publicly
  available at \url{https://github.com/wenguanwang/SODsurvey}
- **Journal**: None
- **Summary**: As an essential problem in computer vision, salient object detection (SOD) has attracted an increasing amount of research attention over the years. Recent advances in SOD are predominantly led by deep learning-based solutions (named deep SOD). To enable in-depth understanding of deep SOD, in this paper, we provide a comprehensive survey covering various aspects, ranging from algorithm taxonomy to unsolved issues. In particular, we first review deep SOD algorithms from different perspectives, including network architecture, level of supervision, learning paradigm, and object-/instance-level detection. Following that, we summarize and analyze existing SOD datasets and evaluation metrics. Then, we benchmark a large group of representative SOD models, and provide detailed analyses of the comparison results. Moreover, we study the performance of SOD algorithms under different attribute settings, which has not been thoroughly explored previously, by constructing a novel SOD dataset with rich attribute annotations covering various salient object types, challenging factors, and scene categories. We further analyze, for the first time in the field, the robustness of SOD models to random input perturbations and adversarial attacks. We also look into the generalization and difficulty of existing SOD datasets. Finally, we discuss several open issues of SOD and outline future research directions.



### Knowledge Distillation via Route Constrained Optimization
- **Arxiv ID**: http://arxiv.org/abs/1904.09149v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.09149v1)
- **Published**: 2019-04-19 11:24:20+00:00
- **Updated**: 2019-04-19 11:24:20+00:00
- **Authors**: Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, Xiaolin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a high lower bound of congruence loss. In this work, inspired by curriculum learning we consider the knowledge distillation from the perspective of curriculum learning by routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR100 and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace.



### A Scalable Handwritten Text Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1904.09150v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09150v2)
- **Published**: 2019-04-19 11:35:27+00:00
- **Updated**: 2019-06-14 23:25:00+00:00
- **Authors**: R. Reeve Ingle, Yasuhisa Fujii, Thomas Deselaers, Jonathan Baccash, Ashok C. Popat
- **Comment**: ICDAR 2019
- **Journal**: None
- **Summary**: Many studies on (Offline) Handwritten Text Recognition (HTR) systems have focused on building state-of-the-art models for line recognition on small corpora. However, adding HTR capability to a large scale multilingual OCR system poses new challenges. This paper addresses three problems in building such systems: data, efficiency, and integration. Firstly, one of the biggest challenges is obtaining sufficient amounts of high quality training data. We address the problem by using online handwriting data collected for a large scale production online handwriting recognition system. We describe our image data generation pipeline and study how online data can be used to build HTR models. We show that the data improve the models significantly under the condition where only a small number of real images is available, which is usually the case for HTR models. It enables us to support a new script at substantially lower cost. Secondly, we propose a line recognition model based on neural networks without recurrent connections. The model achieves a comparable accuracy with LSTM-based models while allowing for better parallelism in training and inference. Finally, we present a simple way to integrate HTR models into an OCR system. These constitute a solution to bring HTR capability into a large scale OCR system.



### Efficient Blind Deblurring under High Noise Levels
- **Arxiv ID**: http://arxiv.org/abs/1904.09154v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09154v2)
- **Published**: 2019-04-19 11:49:21+00:00
- **Updated**: 2019-05-16 14:02:24+00:00
- **Authors**: Jérémy Anger, Mauricio Delbracio, Gabriele Facciolo
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of blind image deblurring is to recover a sharp image from a motion blurred one without knowing the camera motion. Current state-of-the-art methods have a remarkably good performance on images with no noise or very low noise levels. However, the noiseless assumption is not realistic considering that low light conditions are the main reason for the presence of motion blur due to requiring longer exposure times. In fact, motion blur and high to moderate noise often appear together. Most works approach this problem by first estimating the blur kernel $k$ and then deconvolving the noisy blurred image. In this work, we first show that current state-of-the-art kernel estimation methods based on the $\ell_0$ gradient prior can be adapted to handle high noise levels while keeping their efficiency. Then, we show that a fast non-blind deconvolution method can be significantly improved by first denoising the blurry image. The proposed approach yields results that are equivalent to those obtained with much more computationally demanding methods.



### Assessing the Sharpness of Satellite Images: Study of the PlanetScope Constellation
- **Arxiv ID**: http://arxiv.org/abs/1904.09159v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09159v1)
- **Published**: 2019-04-19 12:01:50+00:00
- **Updated**: 2019-04-19 12:01:50+00:00
- **Authors**: Jérémy Anger, Carlo de Franchis, Gabriele Facciolo
- **Comment**: Accepted at IGARSS 2019
- **Journal**: None
- **Summary**: New micro-satellite constellations enable unprecedented systematic monitoring applications thanks to their wide coverage and short revisit capabilities. However, the large volumes of images that they produce have uneven qualities, creating the need for automatic quality assessment methods. In this work, we quantify the sharpness of images from the PlanetScope constellation by estimating the blur kernel from each image. Once the kernel has been estimated, it is possible to compute an absolute measure of sharpness which allows to discard low quality images and deconvolve blurry images before any further processing. The method is fully blind and automatic, and since it does not require the knowledge of any satellite specifications it can be ported to other constellations.



### Realistic Hair Simulation Using Image Blending
- **Arxiv ID**: http://arxiv.org/abs/1904.09169v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09169v1)
- **Published**: 2019-04-19 12:40:12+00:00
- **Updated**: 2019-04-19 12:40:12+00:00
- **Authors**: Mohamed Attia, Mohammed Hossny, Saeid Nahavandi, Anousha Yazdabadi, Hamed Asadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this presented work, we propose a realistic hair simulator using image blending for dermoscopic images. This hair simulator can be used for benchmarking and validation of the hair removal methods and in data augmentation for improving computer aided diagnostic tools. We adopted one of the popular implementation of image blending to superimpose realistic hair masks to hair lesion. This method was able to produce realistic hair masks according to a predefined mask for hair. Thus, the produced hair images and masks can be used as ground truth for hair segmentation and removal methods by inpainting hair according to a pre-defined hair masks on hairfree areas. Also, we achieved a realism score equals to 1.65 in comparison to 1.59 for the state-of-the-art hair simulator.



### Video Object Segmentation and Tracking: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1904.09172v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09172v3)
- **Published**: 2019-04-19 12:49:22+00:00
- **Updated**: 2019-04-26 05:36:58+00:00
- **Authors**: Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, Yong Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Object segmentation and object tracking are fundamental research area in the computer vision community. These two topics are diffcult to handle some common challenges, such as occlusion, deformation, motion blur, and scale variation. The former contains heterogeneous object, interacting object, edge ambiguity, and shape complexity. And the latter suffers from difficulties in handling fast motion, out-of-view, and real-time processing. Combining the two problems of video object segmentation and tracking (VOST) can overcome their respective difficulties and improve their performance. VOST can be widely applied to many practical applications such as video summarization, high definition video compression, human computer interaction, and autonomous vehicles. This article aims to provide a comprehensive review of the state-of-the-art tracking methods, and classify these methods into different categories, and identify new trends. First, we provide a hierarchical categorization existing approaches, including unsupervised VOS, semi-supervised VOS, interactive VOS, weakly supervised VOS, and segmentation-based tracking methods. Second, we provide a detailed discussion and overview of the technical characteristics of the different methods. Third, we summarize the characteristics of the related video dataset, and provide a variety of evaluation metrics. Finally, we point out a set of interesting future works and draw our own conclusions.



### Assessing Architectural Similarity in Populations of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.09879v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.09879v1)
- **Published**: 2019-04-19 13:33:49+00:00
- **Updated**: 2019-04-19 13:33:49+00:00
- **Authors**: Audrey Chung, Paul Fieguth, Alexander Wong
- **Comment**: 3 pages. arXiv admin note: text overlap with arXiv:1811.07966
- **Journal**: None
- **Summary**: Evolutionary deep intelligence has recently shown great promise for producing small, powerful deep neural network models via the synthesis of increasingly efficient architectures over successive generations. Despite recent research showing the efficacy of multi-parent evolutionary synthesis, little has been done to directly assess architectural similarity between networks during the synthesis process for improved parent network selection. In this work, we present a preliminary study into quantifying architectural similarity via the percentage overlap of architectural clusters. Results show that networks synthesized using architectural alignment (via gene tagging) maintain higher architectural similarities within each generation, potentially restricting the search space of highly efficient network architectures.



### Visualizing the decision-making process in deep neural decision forest
- **Arxiv ID**: http://arxiv.org/abs/1904.09201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.09201v1)
- **Published**: 2019-04-19 14:10:03+00:00
- **Updated**: 2019-04-19 14:10:03+00:00
- **Authors**: Shichao Li, Kwang-Ting Cheng
- **Comment**: Accepted by CVPR 2019 workshops on explainable AI
- **Journal**: None
- **Summary**: Deep neural decision forest (NDF) achieved remarkable performance on various vision tasks via combining decision tree and deep representation learning. In this work, we first trace the decision-making process of this model and visualize saliency maps to understand which portion of the input influence it more for both classification and regression problems. We then apply NDF on a multi-task coordinate regression problem and demonstrate the distribution of routing probabilities, which is vital for interpreting NDF yet not shown for regression problems. The pre-trained model and code for visualization will be available at https://github.com/Nicholasli1995/VisualizingNDF



### GestARLite: An On-Device Pointing Finger Based Gestural Interface for Smartphones and Video See-Through Head-Mounts
- **Arxiv ID**: http://arxiv.org/abs/1904.09843v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09843v1)
- **Published**: 2019-04-19 14:32:40+00:00
- **Updated**: 2019-04-19 14:32:40+00:00
- **Authors**: Varun Jain, Gaurav Garg, Ramakrishna Perla, Ramya Hebbalaguppe
- **Comment**: The AAAI 2019 Workshop on Plan, Activity, and Intent Recognition.
  arXiv admin note: substantial text overlap with arXiv:1904.06122
- **Journal**: None
- **Summary**: Hand gestures form an intuitive means of interaction in Mixed Reality (MR) applications. However, accurate gesture recognition can be achieved only through state-of-the-art deep learning models or with the use of expensive sensors. Despite the robustness of these deep learning models, they are generally computationally expensive and obtaining real-time performance on-device is still a challenge. To this end, we propose a novel lightweight hand gesture recognition framework that works in First Person View for wearable devices. The models are trained on a GPU machine and ported on an Android smartphone for its use with frugal wearable devices such as the Google Cardboard and VR Box. The proposed hand gesture recognition framework is driven by a cascade of state-of-the-art deep learning models: MobileNetV2 for hand localisation, our custom fingertip regression architecture followed by a Bi-LSTM model for gesture classification. We extensively evaluate the framework on our EgoGestAR dataset. The overall framework works in real-time on mobile devices and achieves a classification accuracy of 80% on EgoGestAR video dataset with an average latency of only 0.12 s.



### XLSor: A Robust and Accurate Lung Segmentor on Chest X-Rays Using Criss-Cross Attention and Customized Radiorealistic Abnormalities Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.09229v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09229v1)
- **Published**: 2019-04-19 15:22:26+00:00
- **Updated**: 2019-04-19 15:22:26+00:00
- **Authors**: Youbao Tang, Yuxing Tang, Jing Xiao, Ronald M. Summers
- **Comment**: Accepted by MIDL 2019
- **Journal**: None
- **Summary**: This paper proposes a novel framework for lung segmentation in chest X-rays. It consists of two key contributions, a criss-cross attention based segmentation network and radiorealistic chest X-ray image synthesis (i.e. a synthesized radiograph that appears anatomically realistic) for data augmentation. The criss-cross attention modules capture rich global contextual information in both horizontal and vertical directions for all the pixels thus facilitating accurate lung segmentation. To reduce the manual annotation burden and to train a robust lung segmentor that can be adapted to pathological lungs with hazy lung boundaries, an image-to-image translation module is employed to synthesize radiorealistic abnormal CXRs from the source of normal ones for data augmentation. The lung masks of synthetic abnormal CXRs are propagated from the segmentation results of their normal counterparts, and then serve as pseudo masks for robust segmentor training. In addition, we annotate 100 CXRs with lung masks on a more challenging NIH Chest X-ray dataset containing both posterioranterior and anteroposterior views for evaluation. Extensive experiments validate the robustness and effectiveness of the proposed framework. The code and data can be found from https://github.com/rsummers11/CADLab/tree/master/Lung_Segmentation_XLSor .



### Fashion++: Minimal Edits for Outfit Improvement
- **Arxiv ID**: http://arxiv.org/abs/1904.09261v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09261v3)
- **Published**: 2019-04-19 16:43:10+00:00
- **Updated**: 2019-09-02 16:15:52+00:00
- **Authors**: Wei-Lin Hsiao, Isay Katsman, Chao-Yuan Wu, Devi Parikh, Kristen Grauman
- **Comment**: accepted to ICCV 2019
- **Journal**: None
- **Summary**: Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new vision challenge. We introduce Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. Our model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. We show how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. The edits suggested range from swapping in a new garment to tweaking its color, how it is worn (e.g., rolling up sleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that Fashion++ provides successful edits, both according to automated metrics and human opinion. Project page is at http://vision.cs.utexas.edu/projects/FashionPlus.



### STEP: Spatio-Temporal Progressive Learning for Video Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.09288v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09288v1)
- **Published**: 2019-04-19 17:59:39+00:00
- **Updated**: 2019-04-19 17:59:39+00:00
- **Authors**: Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao, Larry Davis, Jan Kautz
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: In this paper, we propose Spatio-TEmporal Progressive (STEP) action detector---a progressive learning framework for spatio-temporal action detection in videos. Starting from a handful of coarse-scale proposal cuboids, our approach progressively refines the proposals towards actions over a few steps. In this way, high-quality proposals (i.e., adhere to action movements) can be gradually obtained at later steps by leveraging the regression outputs from previous steps. At each step, we adaptively extend the proposals in time to incorporate more related temporal context. Compared to the prior work that performs action detection in one run, our progressive learning framework is able to naturally handle the spatial displacement within action tubes and therefore provides a more effective way for spatio-temporal modeling. We extensively evaluate our approach on UCF101 and AVA, and demonstrate superior detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two datasets with 3 progressive steps and using respectively only 11 and 34 initial proposals.



### Challenges and Prospects in Vision and Language Research
- **Arxiv ID**: http://arxiv.org/abs/1904.09317v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.09317v2)
- **Published**: 2019-04-19 19:04:12+00:00
- **Updated**: 2019-05-24 22:10:33+00:00
- **Authors**: Kushal Kafle, Robik Shrestha, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Language grounded image understanding tasks have often been proposed as a method for evaluating progress in artificial intelligence. Ideally, these tasks should test a plethora of capabilities that integrate computer vision, reasoning, and natural language understanding. However, rather than behaving as visual Turing tests, recent studies have demonstrated state-of-the-art systems are achieving good performance through flaws in datasets and evaluation procedures. We review the current state of affairs and outline a path forward.



### Context-Aware Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.09320v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09320v3)
- **Published**: 2019-04-19 19:28:48+00:00
- **Updated**: 2019-04-24 16:45:01+00:00
- **Authors**: Ruotian Luo, Ning Zhang, Bohyung Han, Linjie Yang
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel problem setting in zero-shot learning, zero-shot object recognition and detection in the context. Contrary to the traditional zero-shot learning methods, which simply infers unseen categories by transferring knowledge from the objects belonging to semantically similar seen categories, we aim to understand the identity of the novel objects in an image surrounded by the known objects using the inter-object relation prior. Specifically, we leverage the visual context and the geometric relationships between all pairs of objects in a single image, and capture the information useful to infer unseen categories. We integrate our context-aware zero-shot learning framework into the traditional zero-shot learning techniques seamlessly using a Conditional Random Field (CRF). The proposed algorithm is evaluated on both zero-shot region classification and zero-shot detection tasks. The results on Visual Genome (VG) dataset show that our model significantly boosts performance with the additional visual context compared to traditional methods.



### Compact Scene Graphs for Layout Composition and Patch Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.09348v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.09348v1)
- **Published**: 2019-04-19 21:21:56+00:00
- **Updated**: 2019-04-19 21:21:56+00:00
- **Authors**: Subarna Tripathi, Sharath Nittur Sridhar, Sairam Sundaresan, Hanlin Tang
- **Comment**: To appear in CVPRW 2019 (CEFRL)
- **Journal**: None
- **Summary**: Structured representations such as scene graphs serve as an efficient and compact representation that can be used for downstream rendering or retrieval tasks. However, existing efforts to generate realistic images from scene graphs perform poorly on scene composition for cluttered or complex scenes. We propose two contributions to improve the scene composition. First, we enhance the scene graph representation with heuristic-based relations, which add minimal storage overhead. Second, we use extreme points representation to supervise the learning of the scene composition network. These methods achieve significantly higher performance over existing work (69.0% vs 51.2% in relation score metric). We additionally demonstrate how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. Improving structured scene graph representations for rendering or retrieval is an important step towards realistic image generation.



### Temporal Unet: Sample Level Human Action Recognition using WiFi
- **Arxiv ID**: http://arxiv.org/abs/1904.11953v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1904.11953v1)
- **Published**: 2019-04-19 21:23:28+00:00
- **Updated**: 2019-04-19 21:23:28+00:00
- **Authors**: Fei Wang, Yunpeng Song, Jimuyang Zhang, Jinsong Han, Dong Huang
- **Comment**: 14 pages, 14 figures, 1 table
- **Journal**: None
- **Summary**: Human doing actions will result in WiFi distortion, which is widely explored for action recognition, such as the elderly fallen detection, hand sign language recognition, and keystroke estimation. As our best survey, past work recognizes human action by categorizing one complete distortion series into one action, which we term as series-level action recognition. In this paper, we introduce a much more fine-grained and challenging action recognition task into WiFi sensing domain, i.e., sample-level action recognition. In this task, every WiFi distortion sample in the whole series should be categorized into one action, which is a critical technique in precise action localization, continuous action segmentation, and real-time action recognition. To achieve WiFi-based sample-level action recognition, we fully analyze approaches in image-based semantic segmentation as well as in video-based frame-level action recognition, then propose a simple yet efficient deep convolutional neural network, i.e., Temporal Unet. Experimental results show that Temporal Unet achieves this novel task well. Codes have been made publicly available at https://github.com/geekfeiw/WiSLAR.



