# Arxiv Papers in cs.CV on 2019-04-29
### Mixture of Pre-processing Experts Model for Noise Robust Deep Learning on Resource Constrained Platforms
- **Arxiv ID**: http://arxiv.org/abs/1904.12426v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12426v1)
- **Published**: 2019-04-29 02:26:06+00:00
- **Updated**: 2019-04-29 02:26:06+00:00
- **Authors**: Taesik Na, Minah Lee, Burhan A. Mudassar, Priyabrata Saha, Jong Hwan Ko, Saibal Mukhopadhyay
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning on an edge device requires energy efficient operation due to ever diminishing power budget. Intentional low quality data during the data acquisition for longer battery life, and natural noise from the low cost sensor degrade the quality of target output which hinders adoption of deep learning on an edge device. To overcome these problems, we propose simple yet efficient mixture of pre-processing experts (MoPE) model to handle various image distortions including low resolution and noisy images. We also propose to use adversarially trained auto encoder as a pre-processing expert for the noisy images. We evaluate our proposed method for various machine learning tasks including object detection on MS-COCO 2014 dataset, multiple object tracking problem on MOT-Challenge dataset, and human activity classification on UCF 101 dataset. Experimental results show that the proposed method achieves better detection, tracking and activity classification accuracies under noise without sacrificing accuracies for the clean images. The overheads of our proposed MoPE are 0.67% and 0.17% in terms of memory and computation compared to the baseline object detection network.



### Attribute Guided Unpaired Image-to-Image Translation with Semi-supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.12428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12428v1)
- **Published**: 2019-04-29 02:50:32+00:00
- **Updated**: 2019-04-29 02:50:32+00:00
- **Authors**: Xinyang Li, Jie Hu, Shengchuan Zhang, Xiaopeng Hong, Qixiang Ye, Chenglin Wu, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired Image-to-Image Translation (UIT) focuses on translating images among different domains by using unpaired data, which has received increasing research focus due to its practical usage. However, existing UIT schemes defect in the need of supervised training, as well as the lack of encoding domain information. In this paper, we propose an Attribute Guided UIT model termed AGUIT to tackle these two challenges. AGUIT considers multi-modal and multi-domain tasks of UIT jointly with a novel semi-supervised setting, which also merits in representation disentanglement and fine control of outputs. Especially, AGUIT benefits from two-fold: (1) It adopts a novel semi-supervised learning process by translating attributes of labeled data to unlabeled data, and then reconstructing the unlabeled data by a cycle consistency operation. (2) It decomposes image representation into domain-invariant content code and domain-specific style code. The redesigned style code embeds image style into two variables drawn from standard Gaussian distribution and the distribution of domain label, which facilitates the fine control of translation due to the continuity of both variables. Finally, we introduce a new challenge, i.e., disentangled transfer, for UIT models, which adopts the disentangled representation to translate data less related with the training set. Extensive experiments demonstrate the capacity of AGUIT over existing state-of-the-art models.



### Automatic extrinsic calibration between a camera and a 3D Lidar using 3D point and plane correspondences
- **Arxiv ID**: http://arxiv.org/abs/1904.12433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.12433v1)
- **Published**: 2019-04-29 03:08:33+00:00
- **Updated**: 2019-04-29 03:08:33+00:00
- **Authors**: Surabhi Verma, Julie Stephany Berrio, Stewart Worrall, Eduardo Nebot
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an automated method to obtain the extrinsic calibration parameters between a camera and a 3D lidar with as low as 16 beams. We use a checkerboard as a reference to obtain features of interest in both sensor frames. The calibration board centre point and normal vector are automatically extracted from the lidar point cloud by exploiting the geometry of the board. The corresponding features in the camera image are obtained from the camera's extrinsic matrix. We explain the reasons behind selecting these features, and why they are more robust compared to other possibilities. To obtain the optimal extrinsic parameters, we choose a genetic algorithm to address the highly non-linear state space. The process is automated after defining the bounds of the 3D experimental region relative to the lidar, and the true board dimensions. In addition, the camera is assumed to be intrinsically calibrated. Our method requires a minimum of 3 checkerboard poses, and the calibration accuracy is demonstrated by evaluating our algorithm using real world and simulated features.



### HOG feature extraction from encrypted images for privacy-preserving machine learning
- **Arxiv ID**: http://arxiv.org/abs/1904.12434v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12434v1)
- **Published**: 2019-04-29 03:11:45+00:00
- **Updated**: 2019-04-29 03:11:45+00:00
- **Authors**: Masaki Kitayama, Hitoshi Kiya
- **Comment**: To appear in The 4th IEEE International Conference on Consumer
  Electronics (ICCE) Asia, Bankok, Thailand
- **Journal**: None
- **Summary**: In this paper, we propose an extraction method of HOG (histograms-of-oriented-gradients) features from encryption-then-compression (EtC) images for privacy-preserving machine learning, where EtC images are images encrypted by a block-based encryption method proposed for EtC systems with JPEG compression, and HOG is a feature descriptor used in computer vision for the purpose of object detection and image classification. Recently, cloud computing and machine learning have been spreading in many fields. However, the cloud computing has serious privacy issues for end users, due to unreliability of providers and some accidents. Accordingly, we propose a novel block-based extraction method of HOG features, and the proposed method enables us to carry out any machine learning algorithms without any influence, under some conditions. In an experiment, the proposed method is applied to a face image recognition problem under the use of two kinds of classifiers: linear support vector machine (SVM), gaussian SVM, to demonstrate the effectiveness.



### HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision
- **Arxiv ID**: http://arxiv.org/abs/1905.03696v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1905.03696v1)
- **Published**: 2019-04-29 06:49:08+00:00
- **Updated**: 2019-04-29 06:49:08+00:00
- **Authors**: Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, Kurt Keutzer
- **Comment**: ICCV 2019
- **Journal**: ICCV 2019 paper
- **Summary**: Model size and inference speed/power have become a major challenge in the deployment of Neural Networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers, based on second-order information. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with $8\times$ activation compression ratio on ResNet20, as compared to DNAS~\cite{wu2018mixed}, and up to $1\%$ higher accuracy with up to $14\%$ smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant~\cite{park2018value} and HAQ~\cite{wang2018haq}. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above $68\%$ top1 accuracy on ImageNet.



### Self-Attention Capsule Networks for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.12483v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12483v2)
- **Published**: 2019-04-29 08:04:46+00:00
- **Updated**: 2019-11-19 13:12:08+00:00
- **Authors**: Assaf Hoogi, Brian Wilcox, Yachee Gupta, Daniel L. Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel architecture for object classification, called Self-Attention Capsule Networks (SACN). SACN is the first model that incorporates the Self-Attention mechanism as an integral layer within the Capsule Network (CapsNet). While the Self-Attention mechanism supplies a long-range dependencies, results in selecting the more dominant image regions to focus on, the CapsNet analyzes the relevant features and their spatial correlations inside these regions only. The features are extracted in the convolutional layer. Then, the Self-Attention layer learns to suppress irrelevant regions based on features analysis and highlights salient features useful for a specific task. The attention map is then fed into the CapsNet primary layer that is followed by a classification layer. The proposed SACN model was designed to solve two main limitations of the baseline CapsNet - analysis of complex data and significant computational load. In this work, we use a shallow CapsNet architecture and compensates for the absence of a deeper network by using the Self-Attention module to significantly improve the results. The proposed Self-Attention CapsNet architecture was extensively evaluated on six different datasets, mainly on three different medical sets, in addition to the natural MNIST, SVHN and CIFAR10. The model was able to classify images and their patches with diverse and complex backgrounds better than the baseline CapsNet. As a result, the proposed Self-Attention CapsNet significantly improved classification performance within and across different datasets and outperformed the baseline CapsNet, ResNet-18 and DenseNet-40 not only in classification accuracy but also in robustness.



### Learning Meta Model for Zero- and Few-shot Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1904.12490v3
- **DOI**: 10.1609/aaai.v34i07.6866
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12490v3)
- **Published**: 2019-04-29 08:17:00+00:00
- **Updated**: 2019-12-02 03:59:24+00:00
- **Authors**: Yunxiao Qin, Chenxu Zhao, Xiangyu Zhu, Zezheng Wang, Zitong Yu, Tianyu Fu, Feng Zhou, Jingping Shi, Zhen Lei
- **Comment**: Accepted by AAAI2020
- **Journal**: AAAI-2020
- **Summary**: Face anti-spoofing is crucial to the security of face recognition systems. Most previous methods formulate face anti-spoofing as a supervised learning problem to detect various predefined presentation attacks, which need large scale training data to cover as many attacks as possible. However, the trained model is easy to overfit several common attacks and is still vulnerable to unseen attacks. To overcome this challenge, the detector should: 1) learn discriminative features that can generalize to unseen spoofing types from predefined presentation attacks; 2) quickly adapt to new spoofing types by learning from both the predefined attacks and a few examples of the new spoofing types. Therefore, we define face anti-spoofing as a zero- and few-shot learning problem. In this paper, we propose a novel Adaptive Inner-update Meta Face Anti-Spoofing (AIM-FAS) method to tackle this problem through meta-learning. Specifically, AIM-FAS trains a meta-learner focusing on the task of detecting unseen spoofing types by learning from predefined living and spoofing faces and a few examples of new attacks. To assess the proposed approach, we propose several benchmarks for zero- and few-shot FAS. Experiments show its superior performances on the presented benchmarks to existing methods in existing zero-shot FAS protocols.



### Casting Geometric Constraints in Semantic Segmentation as Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.12534v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.12534v3)
- **Published**: 2019-04-29 09:36:12+00:00
- **Updated**: 2020-01-08 08:54:00+00:00
- **Authors**: Sinisa Stekovic, Friedrich Fraundorfer, Vincent Lepetit
- **Comment**: To be presented at WACV 2020
- **Journal**: None
- **Summary**: We propose a simple yet effective method to learn to segment new indoor scenes from video frames: State-of-the-art methods trained on one dataset, even as large as the SUNRGB-D dataset, can perform poorly when applied to images that are not part of the dataset, because of the dataset bias, a common phenomenon in computer vision. To make semantic segmentation more useful in practice, one can exploit geometric constraints. Our main contribution is to show that these constraints can be cast conveniently as semi-supervised terms, which enforce the fact that the same class should be predicted for the projections of the same 3D location in different images. This is interesting as we can exploit general existing techniques developed for semi-supervised learning to efficiently incorporate the constraints. We show that this approach can efficiently and accurately learn to segment target sequences of ScanNet and our own target sequences using only annotations from SUNRGB-D, and geometric relations between the video frames of target sequences.



### Weakly and Semi Supervised Detection in Medical Imaging via Deep Dual Branch Net
- **Arxiv ID**: http://arxiv.org/abs/1904.12589v3
- **DOI**: 10.1016/j.neucom.2020.09.037
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12589v3)
- **Published**: 2019-04-29 12:16:45+00:00
- **Updated**: 2020-03-19 08:30:08+00:00
- **Authors**: Ran Bakalo, Jacob Goldberger, Rami Ben-Ari
- **Comment**: None
- **Journal**: Neurocomputing, Volume 421, 15 January 2021, Pages 15-25
- **Summary**: This study presents a novel deep learning architecture for multi-class classification and localization of abnormalities in medical imaging illustrated through experiments on mammograms. The proposed network combines two learning branches. One branch is for region classification with a newly added normal-region class. Second branch is region detection branch for ranking regions relative to one another. Our method enables detection of abnormalities at full mammogram resolution for both weakly and semi-supervised settings. A novel objective function allows for the incorporation of local annotations into the model. We present the impact of our schemes on several performance measures for classification and localization, to evaluate the cost effectiveness of the lesion annotation effort. Our evaluation was primarily conducted over a large multi-center mammography dataset of $\sim$3,000 mammograms with various findings. The results for weakly supervised learning showed significant improvement compared to previous approaches. We show that the time consuming local annotations involved in supervised learning can be addressed by a weakly supervised method that can leverage a subset of locally annotated data. Weakly and semi-supervised methods coupled with detection can produce a cost effective and explainable model to be adopted by radiologists in the field.



### DeLiO: Decoupled LiDAR Odometry
- **Arxiv ID**: http://arxiv.org/abs/1904.12667v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12667v1)
- **Published**: 2019-04-29 12:54:22+00:00
- **Updated**: 2019-04-29 12:54:22+00:00
- **Authors**: Queens Maria Thomas, Oliver Wasenmüller, Didier Stricker
- **Comment**: Accepted at IEEE Intelligent Vehicles Symposium (IV), 2019
- **Journal**: None
- **Summary**: Most LiDAR odometry algorithms estimate the transformation between two consecutive frames by estimating the rotation and translation in an intervening fashion. In this paper, we propose our Decoupled LiDAR Odometry (DeLiO), which -- for the first time -- decouples the rotation estimation completely from the translation estimation. In particular, the rotation is estimated by extracting the surface normals from the input point clouds and tracking their characteristic pattern on a unit sphere. Using this rotation the point clouds are unrotated so that the underlying transformation is pure translation, which can be easily estimated using a line cloud approach. An evaluation is performed on the KITTI dataset and the results are compared against state-of-the-art algorithms.



### Globally optimal vertical direction estimation in Atlanta World
- **Arxiv ID**: http://arxiv.org/abs/1904.12717v2
- **DOI**: 10.1109/TPAMI.2020.3027047
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.12717v2)
- **Published**: 2019-04-29 13:56:36+00:00
- **Updated**: 2020-04-03 11:52:36+00:00
- **Authors**: Yinlong Liu, Alois Knoll, Guang Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In man-made environments, such as indoor and urban scenes, most of the objects and structures are organized in the form of orthogonal and parallel planes. These planes can be approximated by the Atlanta world assumption, in which the normals of planes can be represented by the Atlanta frames. Atlanta world assumption, which can be considered as a generalized Manhattan world assumption, has one vertical frame and multiple horizontal frames. Conventionally, given a set of inputs such as surface normals, the Atlanta frame estimation problem can be solved in one-time by branch-and-bound (BnB). However, the runtime of the BnB algorithm will increase greatly when the dimensionality (i.e., the number of horizontal frames) increases. In this paper, we estimate only the vertical direction instead of all Atlanta frames at once. Accordingly, we propose a vertical direction estimation method by considering the relationship between the vertical frame and horizontal frames. Concretely, our approach employs a BnB algorithm to search the vertical direction guaranteeing global optimality without requiring prior knowledge of the number of Atlanta frames. Four novel bounds by mapping 3D-hemisphere to a 2D region are investigated to guarantee convergence. We verify the validity of the proposed method in various challenging synthetic and real-world data.



### Solo or Ensemble? Choosing a CNN Architecture for Melanoma Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.12724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12724v1)
- **Published**: 2019-04-29 14:06:19+00:00
- **Updated**: 2019-04-29 14:06:19+00:00
- **Authors**: Fábio Perez, Sandra Avila, Eduardo Valle
- **Comment**: ISIC Skin Image Analysis Workshop @ CVPR 2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) deliver exceptional results for computer vision, including medical image analysis. With the growing number of available architectures, picking one over another is far from obvious. Existing art suggests that, when performing transfer learning, the performance of CNN architectures on ImageNet correlates strongly with their performance on target tasks. We evaluate that claim for melanoma classification, over 9 CNNs architectures, in 5 sets of splits created on the ISIC Challenge 2017 dataset, and 3 repeated measures, resulting in 135 models. The correlations we found were, to begin with, much smaller than those reported by existing art, and disappeared altogether when we considered only the top-performing networks: uncontrolled nuisances (i.e., splits and randomness) overcome any of the analyzed factors. Whenever possible, the best approach for melanoma classification is still to create ensembles of multiple models. We compared two choices for selecting which models to ensemble: picking them at random (among a pool of high-quality ones) vs. using the validation set to determine which ones to pick first. For small ensembles, we found a slight advantage on the second approach but found that random choice was also competitive. Although our aim in this paper was not to maximize performance, we easily reached AUCs comparable to the first place on the ISIC Challenge 2017.



### DeepHMap++: Combined Projection Grouping and Correspondence Learning for Full DoF Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.12735v1
- **DOI**: 10.3390/s19051032
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12735v1)
- **Published**: 2019-04-29 14:22:07+00:00
- **Updated**: 2019-04-29 14:22:07+00:00
- **Authors**: Mingliang Fu, Weijia Zhou
- **Comment**: 21 pages, 10 figures
- **Journal**: Sensors 19.5 (2019): 1032
- **Summary**: In recent years, estimating the 6D pose of object instances with convolutional neural network (CNN) has received considerable attention. Depending on whether intermediate cues are used, the relevant literature can be roughly divided into two broad categories: direct methods and two stage pipelines. For the latter, intermediate cues, such as 3D object coordinates, semantic keypoints, or virtual control points instead of pose parameters are regressed by CNN in the first stage. Object pose can then be solved by correspondence constraints constructed with these intermediate cues. In this paper, we focus on the postprocessing of a two-stage pipeline and propose to combine two learning concepts for estimating object pose under challenging scenes: projection grouping on one side, and correspondence learning on the other. We firstly employ a local patch based method to predict projection heatmaps which denote the confidence distribution of projection of 3D bounding box's corners. A projection grouping module is then proposed to remove redundant local maxima from each layer of heatmaps. Instead of directly feeding 2D-3D correspondences to the perspective-n-point (PnP) algorithm, multiple correspondence hypotheses are sampled from local maxima and its corresponding neighborhood and ranked by a correspondence-evaluation network. Finally, correspondences with higher confidence are selected to determine object pose. Extensive experiments on three public datasets demonstrate that the proposed framework outperforms several state of the art methods.



### End-to-end Cloud Segmentation in High-Resolution Multispectral Satellite Imagery Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.12743v1
- **DOI**: 10.1109/INTERCON.2019.8853549
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12743v1)
- **Published**: 2019-04-29 14:28:32+00:00
- **Updated**: 2019-04-29 14:28:32+00:00
- **Authors**: Giorgio Morales, Alejandro Ramírez, Joel Telles
- **Comment**: Submitted to INTERCON2019 conference. Lima, Peru
- **Journal**: 2019 IEEE XXVI International Conference on Electronics, Electrical
  Engineering and Computing (INTERCON)
- **Summary**: Segmenting clouds in high-resolution satellite images is an arduous and challenging task due to the many types of geographies and clouds a satellite can capture. Therefore, it needs to be automated and optimized, specially for those who regularly process great amounts of satellite images, such as governmental institutions. In that sense, the contribution of this work is twofold: We present the CloudPeru2 dataset, consisting of 22,400 images of 512x512 pixels and their respective hand-drawn cloud masks, as well as the proposal of an end-to-end segmentation method for clouds using a Convolutional Neural Network (CNN) based on the Deeplab v3+ architecture. The results over the test set achieved an accuracy of 96.62%, precision of 96.46%, specificity of 98.53%, and sensitivity of 96.72% which is superior to the compared methods.



### Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1904.12760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.12760v1)
- **Published**: 2019-04-29 14:59:28+00:00
- **Updated**: 2019-04-29 14:59:28+00:00
- **Authors**: Xin Chen, Lingxi Xie, Jun Wu, Qi Tian
- **Comment**: 10 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time (~7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts.



### Style Transfer by Relaxed Optimal Transport and Self-Similarity
- **Arxiv ID**: http://arxiv.org/abs/1904.12785v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12785v2)
- **Published**: 2019-04-29 15:55:59+00:00
- **Updated**: 2019-10-09 19:00:22+00:00
- **Authors**: Nicholas Kolkin, Jason Salavon, Greg Shakhnarovich
- **Comment**: To Appear CVPR 2019, Webdemo Available at http://style.ttic.edu
- **Journal**: None
- **Summary**: Style transfer algorithms strive to render the content of one image using the style of another. We propose Style Transfer by Relaxed Optimal Transport and Self-Similarity (STROTSS), a new optimization-based style transfer algorithm. We extend our method to allow user-specified point-to-point or region-to-region control over visual similarity between the style image and the output. Such guidance can be used to either achieve a particular visual effect or correct errors made by unconstrained style transfer. In order to quantitatively compare our method to prior work, we conduct a large-scale user study designed to assess the style-content tradeoff across settings in style transfer algorithms. Our results indicate that for any desired level of content preservation, our method provides higher quality stylization than prior work. Code is available at https://github.com/nkolkin13/STROTSS



### TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures
- **Arxiv ID**: http://arxiv.org/abs/1904.12795v1
- **DOI**: 10.1145/3306346.3322993
- **Categories**: **cs.GR**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12795v1)
- **Published**: 2019-04-29 16:15:56+00:00
- **Updated**: 2019-04-29 16:15:56+00:00
- **Authors**: Anna Frühstück, Ibraheem Alhashim, Peter Wonka
- **Comment**: Code is available at http://github.com/afruehstueck/tileGAN
- **Journal**: ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)
- **Summary**: We tackle the problem of texture synthesis in the setting where many input images are given and a large-scale output is required. We build on recent generative adversarial networks and propose two extensions in this paper. First, we propose an algorithm to combine outputs of GANs trained on a smaller resolution to produce a large-scale plausible texture map with virtually no boundary artifacts. Second, we propose a user interface to enable artistic control. Our quantitative and qualitative results showcase the generation of synthesized high-resolution maps consisting of up to hundreds of megapixels as a case in point.



### Adversarial Training for Free!
- **Arxiv ID**: http://arxiv.org/abs/1904.12843v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12843v2)
- **Published**: 2019-04-29 17:50:32+00:00
- **Updated**: 2019-11-20 21:26:19+00:00
- **Authors**: Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin Taylor, Tom Goldstein
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our "free" adversarial training algorithm achieves comparable robustness to PGD adversarial training on the CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classification task that maintains 40% accuracy against PGD attacks. The code is available at https://github.com/ashafahi/free_adv_train.



### Unsupervised Data Augmentation for Consistency Training
- **Arxiv ID**: http://arxiv.org/abs/1904.12848v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.12848v6)
- **Published**: 2019-04-29 17:56:59+00:00
- **Updated**: 2020-11-05 15:11:02+00:00
- **Authors**: Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V. Le
- **Comment**: NeurIPS 2020
- **Journal**: None
- **Summary**: Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.



### DiamondGAN: Unified Multi-Modal Generative Adversarial Networks for MRI Sequences Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.12894v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12894v4)
- **Published**: 2019-04-29 18:21:35+00:00
- **Updated**: 2019-07-26 09:02:10+00:00
- **Authors**: Hongwei Li, Johannes C. Paetzold, Anjany Sekuboyina, Florian Kofler, Jianguo Zhang, Jan S. Kirschke, Benedikt Wiestler, Bjoern Menze
- **Comment**: accepted by miccai 2019
- **Journal**: None
- **Summary**: Synthesizing MR imaging sequences is highly relevant in clinical practice, as single sequences are often missing or are of poor quality (e.g. due to motion). Naturally, the idea arises that a target modality would benefit from multi-modal input, as proprietary information of individual modalities can be synergistic. However, existing methods fail to scale up to multiple non-aligned imaging modalities, facing common drawbacks of complex imaging sequences. We propose a novel, scalable and multi-modal approach called DiamondGAN. Our model is capable of performing exible non-aligned cross-modality synthesis and data infill, when given multiple modalities or any of their arbitrary subsets, learning structured information in an end-to-end fashion. We synthesize two MRI sequences with clinical relevance (i.e., double inversion recovery (DIR) and contrast-enhanced T1 (T1-c)), reconstructed from three common sequences. In addition, we perform a multi-rater visual evaluation experiment and find that trained radiologists are unable to distinguish synthetic DIR images from real ones.



### Learning to Find Common Objects Across Few Image Collections
- **Arxiv ID**: http://arxiv.org/abs/1904.12936v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12936v2)
- **Published**: 2019-04-29 20:26:40+00:00
- **Updated**: 2019-08-17 01:08:21+00:00
- **Authors**: Amirreza Shaban, Amir Rahimi, Shray Bansal, Stephen Gould, Byron Boots, Richard Hartley
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Given a collection of bags where each bag is a set of images, our goal is to select one image from each bag such that the selected images are from the same object class. We model the selection as an energy minimization problem with unary and pairwise potential functions. Inspired by recent few-shot learning algorithms, we propose an approach to learn the potential functions directly from the data. Furthermore, we propose a fast greedy inference algorithm for energy minimization. We evaluate our approach on few-shot common object recognition as well as object co-localization tasks. Our experiments show that learning the pairwise and unary terms greatly improves the performance of the model over several well-known methods for these tasks. The proposed greedy optimization algorithm achieves performance comparable to state-of-the-art structured inference algorithms while being ~10 times faster. The code is publicly available on https://github.com/haamoon/finding_common_object.



### Learning Raw Image Denoising with Bayer Pattern Unification and Bayer Preserving Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.12945v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.12945v2)
- **Published**: 2019-04-29 21:01:40+00:00
- **Updated**: 2019-07-30 18:31:13+00:00
- **Authors**: Jiaming Liu, Chi-Hao Wu, Yuzhi Wang, Qin Xu, Yuqian Zhou, Haibin Huang, Chuan Wang, Shaofan Cai, Yifan Ding, Haoqiang Fan, Jue Wang
- **Comment**: Accepted by CVPRW 2019
- **Journal**: None
- **Summary**: In this paper, we present new data pre-processing and augmentation techniques for DNN-based raw image denoising. Compared with traditional RGB image denoising, performing this task on direct camera sensor readings presents new challenges such as how to effectively handle various Bayer patterns from different data sources, and subsequently how to perform valid data augmentation with raw images. To address the first problem, we propose a Bayer pattern unification (BayerUnify) method to unify different Bayer patterns. This allows us to fully utilize a heterogeneous dataset to train a single denoising model instead of training one model for each pattern. Furthermore, while it is essential to augment the dataset to improve model generalization and performance, we discovered that it is error-prone to modify raw images by adapting augmentation methods designed for RGB images. Towards this end, we present a Bayer preserving augmentation (BayerAug) method as an effective approach for raw image augmentation. Combining these data processing technqiues with a modified U-Net, our method achieves a PSNR of 52.11 and a SSIM of 0.9969 in NTIRE 2019 Real Image Denoising Challenge, demonstrating the state-of-the-art performance. Our code is available at https://github.com/Jiaming-Liu/BayerUnifyAug.



### Convolutional nets for reconstructing neural circuits from brain images acquired by serial section electron microscopy
- **Arxiv ID**: http://arxiv.org/abs/1904.12966v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12966v1)
- **Published**: 2019-04-29 21:54:58+00:00
- **Updated**: 2019-04-29 21:54:58+00:00
- **Authors**: Kisuk Lee, Nicholas Turner, Thomas Macrina, Jingpeng Wu, Ran Lu, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: Neural circuits can be reconstructed from brain images acquired by serial section electron microscopy. Image analysis has been performed by manual labor for half a century, and efforts at automation date back almost as far. Convolutional nets were first applied to neuronal boundary detection a dozen years ago, and have now achieved impressive accuracy on clean images. Robust handling of image defects is a major outstanding challenge. Convolutional nets are also being employed for other tasks in neural circuit reconstruction: finding synapses and identifying synaptic partners, extending or pruning neuronal reconstructions, and aligning serial section images to create a 3D image stack. Computational systems are being engineered to handle petavoxel images of cubic millimeter brain volumes.



### A neural network based on SPD manifold learning for skeleton-based hand gesture recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.12970v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, 68T10, 68T05, 68T45, I.4.8; I.5.1; I.5.4; I.2.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1904.12970v1)
- **Published**: 2019-04-29 21:59:14+00:00
- **Updated**: 2019-04-29 21:59:14+00:00
- **Authors**: Xuan Son Nguyen, Luc Brun, Olivier Lézoray, Sébastien Bougleux
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: This paper proposes a new neural network based on SPD manifold learning for skeleton-based hand gesture recognition. Given the stream of hand's joint positions, our approach combines two aggregation processes on respectively spatial and temporal domains. The pipeline of our network architecture consists in three main stages. The first stage is based on a convolutional layer to increase the discriminative power of learned features. The second stage relies on different architectures for spatial and temporal Gaussian aggregation of joint features. The third stage learns a final SPD matrix from skeletal data. A new type of layer is proposed for the third stage, based on a variant of stochastic gradient descent on Stiefel manifolds. The proposed network is validated on two challenging datasets and shows state-of-the-art accuracies on both datasets.



### Learning Image Information for eCommerce Queries
- **Arxiv ID**: http://arxiv.org/abs/1904.12856v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.12856v1)
- **Published**: 2019-04-29 23:48:31+00:00
- **Updated**: 2019-04-29 23:48:31+00:00
- **Authors**: Utkarsh Porwal
- **Comment**: None
- **Journal**: None
- **Summary**: Computing similarity between a query and a document is fundamental in any information retrieval system. In search engines, computing query-document similarity is an essential step in both retrieval and ranking stages. In eBay search, document is an item and the query-item similarity can be computed by comparing different facets of the query-item pair. Query text can be compared with the text of the item title. Likewise, a category constraint applied on the query can be compared with the listing category of the item. However, images are one signal that are usually present in the items but are not present in the query. Images are one of the most intuitive signals used by users to determine the relevance of the item given a query. Including this signal in estimating similarity between the query-item pair is likely to improve the relevance of the search engine. We propose a novel way of deriving image information for queries. We attempt to learn image information for queries from item images instead of generating explicit image features or an image for queries. We use canonical correlation analysis (CCA) to learn a new subspace where projecting the original data will give us a new query and item representation. We hypothesize that this new query representation will also have image information about the query. We estimate the query-item similarity using a vector space model and report the performance of the proposed method on eBay's search data. We show 11.89\% relevance improvement over the baseline using area under the receiver operating characteristic curve (AUROC) as the evaluation metric. We also show 3.1\% relevance improvement over the baseline with area under the precision recall curve (AUPRC) .



### A Study on Action Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1904.12993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12993v2)
- **Published**: 2019-04-29 23:59:22+00:00
- **Updated**: 2019-06-09 19:55:45+00:00
- **Authors**: Yubo Zhang, Pavel Tokmakov, Martial Hebert, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: The recent introduction of the AVA dataset for action detection has caused a renewed interest to this problem. Several approaches have been recently proposed that improved the performance. However, all of them have ignored the main difficulty of the AVA dataset - its realistic distribution of training and test examples. This dataset was collected by exhaustive annotation of human action in uncurated videos. As a result, the most common categories, such as `stand' or `sit', contain tens of thousands of examples, whereas rare ones have only dozens. In this work we study the problem of action detection in a highly-imbalanced dataset. Differently from previous work on handling long-tail category distributions, we begin by analyzing the imbalance in the test set. We demonstrate that the standard AP metric is not informative for the categories in the tail, and propose an alternative one - Sampled AP. Armed with this new measure, we study the problem of transferring representations from the data-rich head to the rare tail categories and propose a simple but effective approach.



