# Arxiv Papers in cs.CV on 2019-04-01
### Toward Real-World Single Image Super-Resolution: A New Benchmark and A New Model
- **Arxiv ID**: http://arxiv.org/abs/1904.00523v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00523v1)
- **Published**: 2019-04-01 01:14:23+00:00
- **Updated**: 2019-04-01 01:14:23+00:00
- **Authors**: Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most of the existing learning-based single image superresolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degradation (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real-world LR images are far more complicated. As a consequence, the SISR models trained on simulated data become less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (RealSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a digital camera. An image registration algorithm is developed to progressively align the image pairs at different resolutions. Considering that the degradation kernels are naturally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our RealSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones.



### Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.00537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00537v1)
- **Published**: 2019-04-01 02:14:25+00:00
- **Updated**: 2019-04-01 02:14:25+00:00
- **Authors**: Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, Jian Sun
- **Comment**: 8 pages, 5 figures, accepted by CVPR2019
- **Journal**: None
- **Summary**: This paper considers a realistic problem in person re-identification (re-ID) task, i.e., partial re-ID. Under partial re-ID scenario, the images may contain a partial observation of a pedestrian. If we directly compare a partial pedestrian image with a holistic one, the extreme spatial misalignment significantly compromises the discriminative ability of the learned representation. We propose a Visibility-aware Part Model (VPM), which learns to perceive the visibility of regions through self-supervision. The visibility awareness allows VPM to extract region-level features and compare two images with focus on their shared regions (which are visible on both images). VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and benefits from fine-grained information. On the other hand, with visibility awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment. Experimental results confirm that our method significantly improves the learned representation and the achieved accuracy is on par with the state of the art.



### PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study
- **Arxiv ID**: http://arxiv.org/abs/1904.00540v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00540v2)
- **Published**: 2019-04-01 02:41:08+00:00
- **Updated**: 2019-05-01 07:12:43+00:00
- **Authors**: Mehrdad Shoeiby, Antonio Robles-Kelly, Ran Wei, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a newly collected and novel dataset (StereoMSI) for example-based single and colour-guided spectral image super-resolution. The dataset was first released and promoted during the PIRM2018 spectral image super-resolution challenge. To the best of our knowledge, the dataset is the first of its kind, comprising 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the challenge and, for each of these, we have provided a split into training, validation and testing. This arrangement is a result of the challenge structure and phases, with the first track focusing on example-based spectral image super-resolution and the second one aiming at exploiting the registered stereo colour imagery to improve the resolution of the spectral images. Each of the tracks and splits has been selected to be consistent across a number of image quality metrics. The dataset is quite general in nature and can be used for a wide variety of applications in addition to the development of spectral image super-resolution methods.



### Weakly Supervised Object Detection with Segmentation Collaboration
- **Arxiv ID**: http://arxiv.org/abs/1904.00551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00551v1)
- **Published**: 2019-04-01 03:53:49+00:00
- **Updated**: 2019-04-01 03:53:49+00:00
- **Authors**: Xiaoyan Li, Meina Kan, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Weakly supervised object detection aims at learning precise object detectors, given image category labels. In recent prevailing works, this problem is generally formulated as a multiple instance learning module guided by an image classification loss. The object bounding box is assumed to be the one contributing most to the classification among all proposals. However, the region contributing most is also likely to be a crucial part or the supporting context of an object. To obtain a more accurate detector, in this work we propose a novel end-to-end weakly supervised detection approach, where a newly introduced generative adversarial segmentation module interacts with the conventional detection module in a collaborative loop. The collaboration mechanism takes full advantages of the complementary interpretations of the weakly supervised localization task, namely detection and segmentation tasks, forming a more comprehensive solution. Consequently, our method obtains more precise object bounding boxes, rather than parts or irrelevant surroundings. Expectedly, the proposed method achieves an accuracy of 51.0% on the PASCAL VOC 2007 dataset, outperforming the state-of-the-arts and demonstrating its superiority for weakly supervised object detection.



### Defogging Kinect: Simultaneous Estimation of Object Region and Depth in Foggy Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.00558v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00558v1)
- **Published**: 2019-04-01 04:35:01+00:00
- **Updated**: 2019-04-01 04:35:01+00:00
- **Authors**: Yuki Fujimura, Motoharu Sonogashira, Masaaki Iiyama
- **Comment**: 16 pages
- **Journal**: None
- **Summary**: Three-dimensional (3D) reconstruction and scene depth estimation from 2-dimensional (2D) images are major tasks in computer vision. However, using conventional 3D reconstruction techniques gets challenging in participating media such as murky water, fog, or smoke. We have developed a method that uses a time-of-flight (ToF) camera to estimate an object region and depth in participating media simultaneously. The scattering component is saturated, so it does not depend on the scene depth, and received signals bouncing off distant points are negligible due to light attenuation in the participating media, so the observation of such a point contains only a scattering component. These phenomena enable us to estimate the scattering component in an object region from a background that only contains the scattering component. The problem is formulated as robust estimation where the object region is regarded as outliers, and it enables the simultaneous estimation of an object region and depth on the basis of an iteratively reweighted least squares (IRLS) optimization scheme. We demonstrate the effectiveness of the proposed method using captured images from a Kinect v2 in real foggy scenes and evaluate the applicability with synthesized data.



### Scene Graph Generation with External Knowledge and Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.00560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00560v1)
- **Published**: 2019-04-01 04:37:35+00:00
- **Updated**: 2019-04-01 04:37:35+00:00
- **Authors**: Jiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, Jianfei Cai, Mingyang Ling
- **Comment**: 10 pages, 5 figures, Accepted in CVPR 2019
- **Journal**: None
- **Summary**: Scene graph generation has received growing attention with the advancements in image understanding tasks such as object detection, attributes and relationship prediction,~\etc. However, existing datasets are biased in terms of object and relationship labels, or often come with noisy and missing annotations, which makes the development of a reliable scene graph prediction model very challenging. In this paper, we propose a novel scene graph generation algorithm with external knowledge and image reconstruction loss to overcome these dataset issues. In particular, we extract commonsense knowledge from the external knowledge base to refine object and phrase features for improving generalizability in scene graph generation. To address the bias of noisy object annotations, we introduce an auxiliary image reconstruction path to regularize the scene graph generation network. Extensive experiments show that our framework can generate better scene graphs, achieving the state-of-the-art performance on two benchmark datasets: Visual Relationship Detection and Visual Genome datasets.



### Co-regularized Multi-view Sparse Reconstruction Embedding for Dimension Reduction
- **Arxiv ID**: http://arxiv.org/abs/1904.08499v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.08499v1)
- **Published**: 2019-04-01 05:16:55+00:00
- **Updated**: 2019-04-01 05:16:55+00:00
- **Authors**: Huibing Wang, Jinjia Peng, Xianping Fu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of information technology, we have witnessed an age of data explosion which produces a large variety of data filled with redundant information. Because dimension reduction is an essential tool which embeds high-dimensional data into a lower-dimensional subspace to avoid redundant information, it has attracted interests from researchers all over the world. However, facing with features from multiple views, it's difficult for most dimension reduction methods to fully comprehended multi-view features and integrate compatible and complementary information from these features to construct low-dimensional subspace directly. Furthermore, most multi-view dimension reduction methods cannot handle features from nonlinear spaces with high dimensions. Therefore, how to construct a multi-view dimension reduction methods which can deal with multi-view features from high-dimensional nonlinear space is of vital importance but challenging. In order to address this problem, we proposed a novel method named Co-regularized Multi-view Sparse Reconstruction Embedding (CMSRE) in this paper. By exploiting correlations of sparse reconstruction from multiple views, CMSRE is able to learn local sparse structures of nonlinear manifolds from multiple views and constructs significative low-dimensional representations for them. Due to the proposed co-regularized scheme, correlations of sparse reconstructions from multiple views are preserved by CMSRE as much as possible. Furthermore, sparse representation produces more meaningful correlations between features from each single view, which helps CMSRE to gain better performances. Various evaluations based on the applications of document classification, face recognition and image retrieval can demonstrate the effectiveness of the proposed approach on multi-view dimension reduction.



### Multi-source weak supervision for saliency detection
- **Arxiv ID**: http://arxiv.org/abs/1904.00566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00566v1)
- **Published**: 2019-04-01 05:19:19+00:00
- **Updated**: 2019-04-01 05:19:19+00:00
- **Authors**: Yu Zeng, Yunzhi Zhuge, Huchuan Lu, Lihe Zhang, Mingyang Qian, Yizhou Yu
- **Comment**: cvpr2019
- **Journal**: None
- **Summary**: The high cost of pixel-level annotations makes it appealing to train saliency detection models with weak supervision. However, a single weak supervision source usually does not contain enough information to train a well-performing model. To this end, we propose a unified framework to train saliency detection models with diverse weak supervision sources. In this paper, we use category labels, captions, and unlabelled data for training, yet other supervision sources can also be plugged into this flexible framework. We design a classification network (CNet) and a caption generation network (PNet), which learn to predict object categories and generate captions, respectively, meanwhile highlight the most important regions for corresponding tasks. An attention transfer loss is designed to transmit supervision signal between networks, such that the network designed to be trained with one supervision source can benefit from another. An attention coherence loss is defined on unlabelled data to encourage the networks to detect generally salient regions instead of task-specific regions. We use CNet and PNet to generate pixel-level pseudo labels to train a saliency prediction network (SNet). During the testing phases, we only need SNet to predict saliency maps. Experiments demonstrate the performance of our method compares favourably against unsupervised and weakly supervised methods and even some supervised methods.



### Machine Vision for Natural Gas Methane Emissions Detection Using an Infrared Camera
- **Arxiv ID**: http://arxiv.org/abs/1904.08500v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.08500v1)
- **Published**: 2019-04-01 05:38:59+00:00
- **Updated**: 2019-04-01 05:38:59+00:00
- **Authors**: Jingfan Wang, Lyne P. Tchapmi, Arvind P. Ravikumara, Mike McGuire, Clay S. Bell, Daniel Zimmerle, Silvio Savarese, Adam R. Brandt
- **Comment**: This paper was submitted to Applied Energy
- **Journal**: None
- **Summary**: It is crucial to reduce natural gas methane emissions, which can potentially offset the climate benefits of replacing coal with gas. Optical gas imaging (OGI) is a widely-used method to detect methane leaks, but is labor-intensive and cannot provide leak detection results without operators' judgment. In this paper, we develop a computer vision approach to OGI-based leak detection using convolutional neural networks (CNN) trained on methane leak images to enable automatic detection. First, we collect ~1 M frames of labeled video of methane leaks from different leaking equipment for building CNN model, covering a wide range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m). Second, we examine different background subtraction methods to extract the methane plume in the foreground. Third, we then test three CNN model variants, collectively called GasNet, to detect plumes in videos taken at other pieces of leaking equipment. We assess the ability of GasNet to perform leak detection by comparing it to a baseline method that uses optical-flow based change detection algorithm. We explore the sensitivity of results to the CNN structure, with a moderate-complexity variant performing best across distances. We find that the detection accuracy can reach as high as 99%, the overall detection accuracy can exceed 95% for a case across all leak sizes and imaging distances. Binary detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely (~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater than 94% accuracy across all leak sizes. At farthest distances (~13-16 m), performance degrades rapidly, but it can achieve above 95% accuracy to detect large leaks (>950 gCH4/h). The GasNet-based computer vision approach could be deployed in OGI surveys to allow automatic vigilance of methane leak detection with high detection accuracy in the real world.



### Palmprint image registration using convolutional neural networks and Hough transform
- **Arxiv ID**: http://arxiv.org/abs/1904.00579v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00579v2)
- **Published**: 2019-04-01 06:27:10+00:00
- **Updated**: 2019-04-03 03:20:47+00:00
- **Authors**: Mohsen Ahmadi, Hossein Soleimani
- **Comment**: 6 figures, 8 pages
- **Journal**: None
- **Summary**: Minutia-based palmprint recognition systems has got lots of interest in last two decades. Due to the large number of minutiae in a palmprint, approximately 1000 minutiae, the matching process is time consuming which makes it unpractical for real time applications. One way to address this issue is aligning all palmprint images to a reference image and bringing them to a same coordinate system. Bringing all palmprint images to a same coordinate system, results in fewer computations during minutia matching. In this paper, using convolutional neural network (CNN) and generalized Hough transform (GHT), we propose a new method to register palmprint images accurately. This method, finds the corresponding rotation and displacement (in both x and y direction) between the palmprint and a reference image. Exact palmprint registration can enhance the speed and the accuracy of matching process. Proposed method is capable of distinguishing between left and right palmprint automatically which helps to speed up the matching process. Furthermore, designed structure of CNN in registration stage, gives us the segmented palmprint image from background which is a pre-processing step for minutia extraction. The proposed registration method followed by minutia-cylinder code (MCC) matching algorithm has been evaluated on the THUPALMLAB database, and the results show the superiority of our algorithm over most of the state-of-the-art algorithms.



### ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data
- **Arxiv ID**: http://arxiv.org/abs/1904.00592v3
- **DOI**: 10.1016/j.isprsjprs.2020.01.013
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00592v3)
- **Published**: 2019-04-01 06:54:29+00:00
- **Updated**: 2020-01-15 09:20:24+00:00
- **Authors**: Foivos I. Diakogiannis, François Waldner, Peter Caccetta, Chen Wu
- **Comment**: Accepted for publication to the ISPRS Journal of Photogrammetry and
  Remote Sensing
- **Journal**: None
- **Summary**: Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. \textcolor{black}{Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes.} The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9\% over all classes for our best model.



### Learning Combinatorial Embedding Networks for Deep Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.00597v3
- **DOI**: 10.1109/ICCV.2019.00315
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00597v3)
- **Published**: 2019-04-01 07:01:15+00:00
- **Updated**: 2019-09-26 14:35:05+00:00
- **Authors**: Runzhong Wang, Junchi Yan, Xiaokang Yang
- **Comment**: ICCV2019 oral. Code available at
  https://github.com/Thinklab-SJTU/PCA-GM
- **Journal**: None
- **Summary**: Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.



### Relative Attributing Propagation: Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.00605v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00605v4)
- **Published**: 2019-04-01 07:24:35+00:00
- **Updated**: 2019-11-13 07:27:10+00:00
- **Authors**: Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, Seong-Whan Lee
- **Comment**: 8 pages, 7 figures, Accepted paper in AAAI Conference on Artificial
  Intelligence (AAAI), 2020
- **Journal**: None
- **Summary**: As Deep Neural Networks (DNNs) have demonstrated superhuman performance in a variety of fields, there is an increasing interest in understanding the complex internal mechanisms of DNNs. In this paper, we propose Relative Attributing Propagation (RAP), which decomposes the output predictions of DNNs with a new perspective of separating the relevant (positive) and irrelevant (negative) attributions according to the relative influence between the layers. The relevance of each neuron is identified with respect to its degree of contribution, separated into positive and negative, while preserving the conservation rule. Considering the relevance assigned to neurons in terms of relative priority, RAP allows each neuron to be assigned with a bi-polar importance score concerning the output: from highly relevant to highly irrelevant. Therefore, our method makes it possible to interpret DNNs with much clearer and attentive visualizations of the separated attributions than the conventional explaining methods. To verify that the attributions propagated by RAP correctly account for each meaning, we utilize the evaluation metrics: (i) Outside-inside relevance ratio, (ii) Segmentation mIOU and (iii) Region perturbation. In all experiments and metrics, we present a sizable gap in comparison to the existing literature. Our source code is available in \url{https://github.com/wjNam/Relative_Attributing_Propagation}.



### Video Object Segmentation using Space-Time Memory Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.00607v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00607v2)
- **Published**: 2019-04-01 07:27:24+00:00
- **Updated**: 2019-08-12 07:19:59+00:00
- **Authors**: Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).



### Constructing Hierarchical Q&A Datasets for Video Story Understanding
- **Arxiv ID**: http://arxiv.org/abs/1904.00623v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.00623v1)
- **Published**: 2019-04-01 08:05:19+00:00
- **Updated**: 2019-04-01 08:05:19+00:00
- **Authors**: Yu-Jung Heo, Kyoung-Woon On, Seongho Choi, Jaeseo Lim, Jinah Kim, Jeh-Kwang Ryu, Byung-Chull Bae, Byoung-Tak Zhang
- **Comment**: Accepted to AAAI 2019 Spring Symposium Series : Story-Enabled
  Intelligence
- **Journal**: None
- **Summary**: Video understanding is emerging as a new paradigm for studying human-like AI. Question-and-Answering (Q&A) is used as a general benchmark to measure the level of intelligence for video understanding. While several previous studies have suggested datasets for video Q&A tasks, they did not really incorporate story-level understanding, resulting in highly-biased and lack of variance in degree of question difficulty. In this paper, we propose a hierarchical method for building Q&A datasets, i.e. hierarchical difficulty levels. We introduce three criteria for video story understanding, i.e. memory capacity, logical complexity, and DIKW (Data-Information-Knowledge-Wisdom) pyramid. We discuss how three-dimensional map constructed from these criteria can be used as a metric for evaluating the levels of intelligence relating to video story understanding.



### Med3D: Transfer Learning for 3D Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.00625v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00625v4)
- **Published**: 2019-04-01 08:14:29+00:00
- **Updated**: 2019-07-17 10:19:12+00:00
- **Authors**: Sihong Chen, Kai Ma, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The performance on deep learning is significantly affected by volume of training data. Models pre-trained from massive dataset such as ImageNet become a powerful weapon for speeding up training convergence and improving accuracy. Similarly, models based on large dataset are important for the development of deep learning in 3D medical images. However, it is extremely challenging to build a sufficiently large dataset due to difficulty of data acquisition and annotation in 3D medical imaging. We aggregate the dataset from several medical challenges to build 3DSeg-8 dataset with diverse modalities, target organs, and pathologies. To extract general medical three-dimension (3D) features, we design a heterogeneous 3D network called Med3D to co-train multi-domain 3DSeg-8 so as to make a series of pre-trained models. We transfer Med3D pre-trained models to lung segmentation in LIDC dataset, pulmonary nodule classification in LIDC dataset and liver segmentation on LiTS challenge. Experiments show that the Med3D can accelerate the training convergence speed of target 3D medical tasks 2 times compared with model pre-trained on Kinetics dataset, and 10 times compared with training from scratch as well as improve accuracy ranging from 3% to 20%. Transferring our Med3D model on state-the-of-art DenseASPP segmentation network, in case of single model, we achieve 94.6\% Dice coefficient which approaches the result of top-ranged algorithms on the LiTS challenge.



### TAN: Temporal Affine Network for Real-Time Left Ventricle Anatomical Structure Analysis Based on 2D Ultrasound Videos
- **Arxiv ID**: http://arxiv.org/abs/1904.00631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00631v1)
- **Published**: 2019-04-01 08:23:32+00:00
- **Updated**: 2019-04-01 08:23:32+00:00
- **Authors**: Sihong Chen, Kai Ma, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: With superiorities on low cost, portability, and free of radiation, echocardiogram is a widely used imaging modality for left ventricle (LV) function quantification. However, automatic LV segmentation and motion tracking is still a challenging task. In addition to fuzzy border definition, low contrast, and abounding artifacts on typical ultrasound images, the shape and size of the LV change significantly in a cardiac cycle. In this work, we propose a temporal affine network (TAN) to perform image analysis in a warped image space, where the shape and size variations due to the cardiac motion as well as other artifacts are largely compensated. Furthermore, we perform three frequent echocardiogram interpretation tasks simultaneously: standard cardiac plane recognition, LV landmark detection, and LV segmentation. Instead of using three networks with one dedicating to each task, we use a multi-task network to perform three tasks simultaneously. Since three tasks share the same encoder, the compact network improves the segmentation accuracy with more supervision. The network is further finetuned with optical flow adjusted annotations to enhance motion coherence in the segmentation result. Experiments on 1,714 2D echocardiographic sequences demonstrate that the proposed method achieves state-of-the-art segmentation accuracy with real-time efficiency.



### CFSNet: Toward a Controllable Feature Space for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1904.00634v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00634v2)
- **Published**: 2019-04-01 08:27:05+00:00
- **Updated**: 2019-08-20 02:34:43+00:00
- **Authors**: Wei Wang, Ruiming Guo, Yapeng Tian, Wenming Yang
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Deep learning methods have witnessed the great progress in image restoration with specific metrics (e.g., PSNR, SSIM). However, the perceptual quality of the restored image is relatively subjective, and it is necessary for users to control the reconstruction result according to personal preferences or image characteristics, which cannot be done using existing deterministic networks. This motivates us to exquisitely design a unified interactive framework for general image restoration tasks. Under this framework, users can control continuous transition of different objectives, e.g., the perception-distortion trade-off of image super-resolution, the trade-off between noise reduction and detail preservation. We achieve this goal by controlling the latent features of the designed network. To be specific, our proposed framework, named Controllable Feature Space Network (CFSNet), is entangled by two branches based on different objectives. Our framework can adaptively learn the coupling coefficients of different layers and channels, which provides finer control of the restored image quality. Experiments on several typical image restoration tasks fully validate the effective benefits of the proposed method. Code is available at https://github.com/qibao77/CFSNet.



### Single Image Reflection Removal Exploiting Misaligned Training Data and Network Enhancements
- **Arxiv ID**: http://arxiv.org/abs/1904.00637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00637v1)
- **Published**: 2019-04-01 08:38:37+00:00
- **Updated**: 2019-04-01 08:38:37+00:00
- **Authors**: Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, Hua Huang
- **Comment**: Accepted to CVPR2019; code is available at
  https://github.com/Vandermode/ERRNet
- **Journal**: None
- **Summary**: Removing undesirable reflections from a single image captured through a glass window is of practical importance to visual computing systems. Although state-of-the-art methods can obtain decent results in certain situations, performance declines significantly when tackling more general real-world cases. These failures stem from the intrinsic difficulty of single image reflection removal -- the fundamental ill-posedness of the problem, and the insufficiency of densely-labeled training data needed for resolving this ambiguity within learning-based neural network pipelines. In this paper, we address these issues by exploiting targeted network enhancements and the novel use of misaligned data. For the former, we augment a baseline network architecture by embedding context encoding modules that are capable of leveraging high-level contextual clues to reduce indeterminacy within areas containing strong reflections. For the latter, we introduce an alignment-invariant loss function that facilitates exploiting misaligned real-world training data that is much easier to collect. Experimental results collectively show that our method outperforms the state-of-the-art with aligned data, and that significant improvements are possible when using additional misaligned data.



### Harvesting Visual Objects from Internet Images via Deep Learning Based Objectness Assessment
- **Arxiv ID**: http://arxiv.org/abs/1904.00641v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00641v1)
- **Published**: 2019-04-01 08:56:00+00:00
- **Updated**: 2019-04-01 08:56:00+00:00
- **Authors**: Kan Wu, Guanbin Li, Haofeng Li, Jianjun Zhang, Yizhou Yu
- **Comment**: Accepted by ACM Transactions on Multimedia Computing, Communications
  and Applications
- **Journal**: None
- **Summary**: The collection of internet images has been growing in an astonishing speed. It is undoubted that these images contain rich visual information that can be useful in many applications, such as visual media creation and data-driven image synthesis. In this paper, we focus on the methodologies for building a visual object database from a collection of internet images. Such database is built to contain a large number of high-quality visual objects that can help with various data-driven image applications. Our method is based on dense proposal generation and objectness-based re-ranking. A novel deep convolutional neural network is designed for the inference of proposal objectness, the probability of a proposal containing optimally-located foreground object. In our work, the objectness is quantitatively measured in regard of completeness and fullness, reflecting two complementary features of an optimal proposal: a complete foreground and relatively small background. Our experiments indicate that object proposals re-ranked according to the output of our network generally achieve higher performance than those produced by other state-of-the-art methods. As a concrete example, a database of over 1.2 million visual objects has been built using the proposed method, and has been successfully used in various data-driven image applications.



### Deep Learning for Large-Scale Traffic-Sign Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.00649v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00649v1)
- **Published**: 2019-04-01 09:10:16+00:00
- **Updated**: 2019-04-01 09:10:16+00:00
- **Authors**: Domen Tabernik, Danijel Skočaj
- **Comment**: Accepted for publication in IEEE Transactions on Intelligent
  Transportation Systems
- **Journal**: None
- **Summary**: Automatic detection and recognition of traffic signs plays a crucial role in management of the traffic-sign inventory. It provides accurate and timely way to manage traffic-sign inventory with a minimal human effort. In the computer vision community the recognition and detection of traffic signs is a well-researched problem. A vast majority of existing approaches perform well on traffic signs needed for advanced drivers-assistance and autonomous systems. However, this represents a relatively small number of all traffic signs (around 50 categories out of several hundred) and performance on the remaining set of traffic signs, which are required to eliminate the manual labor in traffic-sign inventory management, remains an open question. In this paper, we address the issue of detecting and recognizing a large number of traffic-sign categories suitable for automating traffic-sign inventory management. We adopt a convolutional neural network (CNN) approach, the Mask R-CNN, to address the full pipeline of detection and recognition with automatic end-to-end learning. We propose several improvements that are evaluated on the detection of traffic signs and result in an improved overall performance. This approach is applied to detection of 200 traffic-sign categories represented in our novel dataset. Results are reported on highly challenging traffic-sign categories that have not yet been considered in previous works. We provide comprehensive analysis of the deep learning method for the detection of traffic signs with large intra-category appearance variation and show below 3% error rates with the proposed approach, which is sufficient for deployment in practical applications of traffic-sign inventory management.



### Learning Content-Weighted Deep Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1904.00664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00664v1)
- **Published**: 2019-04-01 09:40:37+00:00
- **Updated**: 2019-04-01 09:40:37+00:00
- **Authors**: Mu Li, Wangmeng Zuo, Shuhang Gu, Jane You, David Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Learning-based lossy image compression usually involves the joint optimization of rate-distortion performance. Most existing methods adopt spatially invariant bit length allocation and incorporate discrete entropy approximation to constrain compression rate. Nonetheless, the information content is spatially variant, where the regions with complex and salient structures generally are more essential to image compression. Taking the spatial variation of image content into account, this paper presents a content-weighted encoder-decoder model, which involves an importance map subnet to produce the importance mask for locally adaptive bit rate allocation. Consequently, the summation of importance mask can thus be utilized as an alternative of entropy estimation for compression rate control. Furthermore, the quantized representations of the learned code and importance map are still spatially dependent, which can be losslessly compressed using arithmetic coding. To compress the codes effectively and efficiently, we propose a trimmed convolutional network to predict the conditional probability of quantized codes. Experiments show that the proposed method can produce visually much better results, and performs favorably in comparison with deep and traditional lossy image compression approaches.



### Deep Built-Structure Counting in Satellite Imagery Using Attention Based Re-Weighting
- **Arxiv ID**: http://arxiv.org/abs/1904.00674v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00674v1)
- **Published**: 2019-04-01 09:57:08+00:00
- **Updated**: 2019-04-01 09:57:08+00:00
- **Authors**: Anza Shakeel, Waqas Sultani, Mohsen Ali
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we attempt to address the challenging problem of counting built-structures in the satellite imagery. Building density is a more accurate estimate of the population density, urban area expansion and its impact on the environment, than the built-up area segmentation. However, building shape variances, overlapping boundaries, and variant densities make this a complex task. To tackle this difficult problem, we propose a deep learning based regression technique for counting built-structures in satellite imagery. Our proposed framework intelligently combines features from different regions of satellite image using attention based re-weighting techniques. Multiple parallel convolutional networks are designed to capture information at different granulates. These features are combined into the FusionNet which is trained to weigh features from different granularity differently, allowing us to predict a precise building count. To train and evaluate the proposed method, we put forward a new large-scale and challenging built-structure-count dataset. Our dataset is constructed by collecting satellite imagery from diverse geographical areas (planes, urban centers, deserts, etc.,) across the globe (Asia, Europe, North America, and Africa) and captures the wide density of built structures. Detailed experimental results and analysis validate the proposed technique. FusionNet has Mean Absolute Error of 3.65 and R-squared measure of 88% over the testing data. Finally, we perform the test on the 274:3 ? 103 m2 of the unseen region, with the error of 19 buildings off the 656 buildings in that area.



### End-to-End Time-Lapse Video Synthesis from a Single Outdoor Image
- **Arxiv ID**: http://arxiv.org/abs/1904.00680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00680v1)
- **Published**: 2019-04-01 10:13:14+00:00
- **Updated**: 2019-04-01 10:13:14+00:00
- **Authors**: Seonghyeon Nam, Chongyang Ma, Menglei Chai, William Brendel, Ning Xu, Seon Joo Kim
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Time-lapse videos usually contain visually appealing content but are often difficult and costly to create. In this paper, we present an end-to-end solution to synthesize a time-lapse video from a single outdoor image using deep neural networks. Our key idea is to train a conditional generative adversarial network based on existing datasets of time-lapse videos and image sequences. We propose a multi-frame joint conditional generation framework to effectively learn the correlation between the illumination change of an outdoor scene and the time of the day. We further present a multi-domain training scheme for robust training of our generative models from two datasets with different distributions and missing timestamp labels. Compared to alternative time-lapse video synthesis algorithms, our method uses the timestamp as the control variable and does not require a reference video to guide the synthesis of the final output. We conduct ablation studies to validate our algorithm and compare with state-of-the-art techniques both qualitatively and quantitatively.



### Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1904.00682v1
- **DOI**: 10.1109/TMI.2019.2905770
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00682v1)
- **Published**: 2019-04-01 10:16:02+00:00
- **Updated**: 2019-04-01 10:16:02+00:00
- **Authors**: Hugo J. Kuijf, J. Matthijs Biesbroek, Jeroen de Bresser, Rutger Heinen, Simon Andermatt, Mariana Bento, Matt Berseth, Mikhail Belyaev, M. Jorge Cardoso, Adrià Casamitjana, D. Louis Collins, Mahsa Dadar, Achilleas Georgiou, Mohsen Ghafoorian, Dakai Jin, April Khademi, Jesse Knight, Hongwei Li, Xavier Lladó, Miguel Luna, Qaiser Mahmood, Richard McKinley, Alireza Mehrtash, Sébastien Ourselin, Bo-yong Park, Hyunjin Park, Sang Hyun Park, Simon Pezold, Elodie Puybareau, Leticia Rittner, Carole H. Sudre, Sergi Valverde, Verónica Vilaplana, Roland Wiest, Yongchao Xu, Ziyue Xu, Guodong Zeng, Jianguo Zhang, Guoyan Zheng, Christopher Chen, Wiesje van der Flier, Frederik Barkhof, Max A. Viergever, Geert Jan Biessels
- **Comment**: Accepted for publication in IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. Automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their method on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge (https://wmh.isi.uu.nl/).   Sixty T1+FLAIR images from three MR scanners were released with manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. Segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: (1) Dice similarity coefficient, (2) modified Hausdorff distance (95th percentile), (3) absolute log-transformed volume difference, (4) sensitivity for detecting individual lesions, and (5) F1-score for individual lesions. Additionally, methods were ranked on their inter-scanner robustness.   Twenty participants submitted their method for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all methods generalize to unseen scanners.   The challenge remains open for future submissions and provides a public platform for method evaluation.



### Dance with Flow: Two-in-One Stream Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.00696v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00696v3)
- **Published**: 2019-04-01 11:09:03+00:00
- **Updated**: 2019-06-11 11:29:06+00:00
- **Authors**: Jiaojiao Zhao, Cees G. M. Snoek
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single two-in-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance- or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.



### JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1904.00699v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00699v2)
- **Published**: 2019-04-01 11:21:33+00:00
- **Updated**: 2019-04-05 10:51:55+00:00
- **Authors**: Quang-Hieu Pham, Duc Thanh Nguyen, Binh-Son Hua, Gemma Roig, Sai-Kit Yeung
- **Comment**: CVPR 2019 (Oral). More information at
  https://pqhieu.github.io/cvpr19.html
- **Journal**: None
- **Summary**: Deep learning techniques have become the to-go models for most vision-related tasks on 2D images. However, their power has not been fully realised on several tasks in 3D space, e.g., 3D scene understanding. In this work, we jointly address the problems of semantic and instance segmentation of 3D point clouds. Specifically, we develop a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings. We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model. The proposed method is thoroughly evaluated and compared with existing methods on different indoor scene datasets including S3DIS and SceneNN. Experimental results showed the robustness of the proposed joint semantic-instance segmentation scheme over its single components. Our method also achieved state-of-the-art performance on semantic segmentation.



### Optimal Fusion of Elliptic Extended Target Estimates based on the Wasserstein Distance
- **Arxiv ID**: http://arxiv.org/abs/1904.00708v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.00708v3)
- **Published**: 2019-04-01 11:52:27+00:00
- **Updated**: 2019-10-08 13:03:32+00:00
- **Authors**: Kolja Thormann, Marcus Baum
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the fusion of multiple estimates of a spatially extended object, where the object extent is modeled as an ellipse parameterized by the orientation and semiaxes lengths. For this purpose, we propose a novel systematic approach that employs a distance measure for ellipses, i.e., the Gaussian Wasserstein distance, as a cost function. We derive an explicit approximate expression for the Minimum Mean Gaussian Wasserstein distance (MMGW) estimate. Based on the concept of a MMGW estimator, we develop efficient methods for the fusion of extended target estimates. The proposed fusion methods are evaluated in a simulated experiment and the benefits of the novel methods are discussed.



### GAN You Do the GAN GAN?
- **Arxiv ID**: http://arxiv.org/abs/1904.00724v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00724v1)
- **Published**: 2019-04-01 12:19:28+00:00
- **Updated**: 2019-04-01 12:19:28+00:00
- **Authors**: Joseph Suarez
- **Comment**: 3 pages
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have become a dominant class of generative models. In recent years, GAN variants have yielded especially impressive results in the synthesis of a variety of forms of data. Examples include compelling natural and artistic images, textures, musical sequences, and 3D object files. However, one obvious synthesis candidate is missing. In this work, we answer one of deep learning's most pressing questions: GAN you do the GAN GAN? That is, is it possible to train a GAN to model a distribution of GANs? We release the full source code for this project under the MIT license.



### Semantic Nearest Neighbor Fields Monocular Edge Visual-Odometry
- **Arxiv ID**: http://arxiv.org/abs/1904.00738v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00738v1)
- **Published**: 2019-04-01 12:25:50+00:00
- **Updated**: 2019-04-01 12:25:50+00:00
- **Authors**: Xiaolong Wu, Assia Benbihi, Antoine Richard, Cedric Pradalier
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning for edge detection and segmentation opens up a new path for semantic-edge-based ego-motion estimation. In this work, we propose a robust monocular visual odometry (VO) framework using category-aware semantic edges. It can reconstruct large-scale semantic maps in challenging outdoor environments. The core of our approach is a semantic nearest neighbor field that facilitates a robust data association of edges across frames using semantics. This significantly enlarges the convergence radius during tracking phases. The proposed edge registration method can be easily integrated into direct VO frameworks to estimate photometrically, geometrically, and semantically consistent camera motions. Different types of edges are evaluated and extensive experiments demonstrate that our proposed system outperforms state-of-art indirect, direct, and semantic monocular VO systems.



### COCO_TS Dataset: Pixel-level Annotations Based on Weak Supervision for Scene Text Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00818v6
- **DOI**: 10.1007/978-3-030-30490-4_26
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00818v6)
- **Published**: 2019-04-01 13:03:31+00:00
- **Updated**: 2019-09-24 10:22:35+00:00
- **Authors**: Simone Bonechi, Paolo Andreini, Monica Bianchini, Franco Scarselli
- **Comment**: None
- **Journal**: None
- **Summary**: The absence of large scale datasets with pixel-level supervisions is a significant obstacle for the training of deep convolutional networks for scene text segmentation. For this reason, synthetic data generation is normally employed to enlarge the training dataset. Nonetheless, synthetic data cannot reproduce the complexity and variability of natural images. In this paper, a weakly supervised learning approach is used to reduce the shift between training on real and synthetic data. Pixel-level supervisions for a text detection dataset (i.e. where only bounding-box annotations are available) are generated. In particular, the COCO-Text-Segmentation (COCO_TS) dataset, which provides pixel-level supervisions for the COCO-Text dataset, is created and released. The generated annotations are used to train a deep convolutional neural network for semantic segmentation. Experiments show that the proposed dataset can be used instead of synthetic data, allowing us to use only a fraction of the training samples and significantly improving the performances.



### Implementation of Fruits Recognition Classifier using Convolutional Neural Network Algorithm for Observation of Accuracies for Various Hidden Layers
- **Arxiv ID**: http://arxiv.org/abs/1904.00783v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00783v6)
- **Published**: 2019-04-01 13:03:33+00:00
- **Updated**: 2020-01-25 12:10:41+00:00
- **Authors**: Shadman Sakib, Zahidun Ashrafi, Md. Abu Bakr Siddique
- **Comment**: 4 Pages, 5 Figures
- **Journal**: None
- **Summary**: Fruit recognition using Deep Convolutional Neural Network (CNN) is one of the most promising applications in computer vision. In recent times, deep learning based classifications are making it possible to recognize fruits from images. However, fruit recognition is still a problem for the stacked fruits on weighing scale because of the complexity and similarity. In this paper, a fruit recognition system using CNN is proposed. The proposed method uses deep learning techniques for the classification. We have used Fruits-360 dataset for the evaluation purpose. From the dataset, we have established a dataset which contains 17,823 images from 25 different categories. The images are divided into training and test dataset. Moreover, for the classification accuracies, we have used various combinations of hidden layer and epochs for different cases and made a comparison between them. The overall performance losses of the network for different cases also observed. Finally, we have achieved the best test accuracy of 100% and a training accuracy of 99.79%.



### Depth-Aware Video Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1904.00830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00830v1)
- **Published**: 2019-04-01 13:19:59+00:00
- **Updated**: 2019-04-01 13:19:59+00:00
- **Authors**: Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang
- **Comment**: This work is accepted in CVPR 2019. The source code and pre-trained
  model are available on https://github.com/baowenbo/DAIN
- **Journal**: None
- **Summary**: Video frame interpolation aims to synthesize nonexistent frames in-between the original frames. While significant advances have been made from the recent deep convolutional neural networks, the quality of interpolation is often reduced due to large object motion or occlusion. In this work, we propose a video frame interpolation method which explicitly detects the occlusion by exploring the depth information. Specifically, we develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones. In addition, we learn hierarchical features to gather contextual information from neighboring pixels. The proposed model then warps the input frames, depth maps, and contextual features based on the optical flow and local interpolation kernels for synthesizing the output frame. Our model is compact, efficient, and fully differentiable. Quantitative and qualitative results demonstrate that the proposed model performs favorably against state-of-the-art frame interpolation methods on a wide variety of datasets.



### Precise Detection in Densely Packed Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.00853v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00853v3)
- **Published**: 2019-04-01 13:53:05+00:00
- **Updated**: 2019-04-30 17:46:20+00:00
- **Authors**: Eran Goldman, Roei Herzig, Aviv Eisenschtat, Oria Ratzon, Itsik Levi, Jacob Goldberger, Tal Hassner
- **Comment**: CVPR 2019
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition, 2019
- **Summary**: Man-made scenes can be densely packed, containing numerous objects, often identical, positioned in close proximity. We show that precise object detection in such scenes remains a challenging frontier even for state-of-the-art object detectors. We propose a novel, deep-learning based method for precise object detection, designed for such challenging settings. Our contributions include: (1) A layer for estimating the Jaccard index as a detection quality score; (2) a novel EM merging unit, which uses our quality scores to resolve detection overlap ambiguities; finally, (3) an extensive, annotated data set, SKU-110K, representing packed retail environments, released for training and testing under such extreme settings. Detection tests on SKU-110K and counting tests on the CARPK and PUCPR+ show our method to outperform existing state-of-the-art with substantial margins. The code and data will be made available on \url{www.github.com/eg4000/SKU110K_CVPR19}.



### DefectNET: multi-class fault detection on highly-imbalanced datasets
- **Arxiv ID**: http://arxiv.org/abs/1904.00863v2
- **DOI**: 10.1109/ICIP.2019.8803305
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00863v2)
- **Published**: 2019-04-01 14:05:29+00:00
- **Updated**: 2019-04-02 09:23:43+00:00
- **Authors**: N. Anantrasirichai, David Bull
- **Comment**: None
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP)
- **Summary**: As a data-driven method, the performance of deep convolutional neural networks (CNN) relies heavily on training data. The prediction results of traditional networks give a bias toward larger classes, which tend to be the background in the semantic segmentation task. This becomes a major problem for fault detection, where the targets appear very small on the images and vary in both types and sizes. In this paper we propose a new network architecture, DefectNet, that offers multi-class (including but not limited to) defect detection on highly-imbalanced datasets. DefectNet consists of two parallel paths, which are a fully convolutional network and a dilated convolutional network to detect large and small objects respectively. We propose a hybrid loss maximising the usefulness of a dice loss and a cross entropy loss, and we also employ the leaky rectified linear unit (ReLU) to deal with rare occurrence of some targets in training batches. The prediction results show that our DefectNet outperforms state-of-the-art networks for detecting multi-class defects with the average accuracy improvement of approximately 10% on a wind turbine.



### Non-linear aggregation of filters to improve image denoising
- **Arxiv ID**: http://arxiv.org/abs/1904.00865v3
- **DOI**: 10.1007/978-3-030-52246-9_22
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.00865v3)
- **Published**: 2019-04-01 14:10:21+00:00
- **Updated**: 2020-06-23 15:43:09+00:00
- **Authors**: Benjamin Guedj, Juliette Rengot
- **Comment**: To appear at Computing Conference 2020
- **Journal**: Computing Conference 2020
- **Summary**: We introduce a novel aggregation method to efficiently perform image denoising. Preliminary filters are aggregated in a non-linear fashion, using a new metric of pixel proximity based on how the pool of filters reaches a consensus. We provide a theoretical bound to support our aggregation scheme, its numerical performance is illustrated and we show that the aggregate significantly outperforms each of the preliminary filters.



### Significance-aware Information Bottleneck for Domain Adaptive Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.00876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00876v1)
- **Published**: 2019-04-01 14:19:28+00:00
- **Updated**: 2019-04-01 14:19:28+00:00
- **Authors**: Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang
- **Comment**: 12 pages, 9 figures
- **Journal**: None
- **Summary**: For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a "significance-aware information bottleneck (SIB)", to address the above problem. The new network structure, called SIBAN, enables a significance-aware feature purification before the adversarial adaptation, which eases the feature alignment and stabilizes the adversarial training course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method can yield leading results compared with other feature-space alternatives. Moreover, SIBAN can even match the state-of-the-art output-space methods in segmentation accuracy, while the latter are often considered to be better choices for domain adaptive segmentation task.



### Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.00887v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00887v4)
- **Published**: 2019-04-01 14:42:38+00:00
- **Updated**: 2019-07-28 08:53:05+00:00
- **Authors**: Aamir Mustafa, Salman Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, Ling Shao
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.



### Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters
- **Arxiv ID**: http://arxiv.org/abs/1904.00889v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00889v3)
- **Published**: 2019-04-01 14:47:24+00:00
- **Updated**: 2019-10-12 15:13:02+00:00
- **Authors**: Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, Krystian Mikolajczyk
- **Comment**: None
- **Journal**: International Conference on Computer Vision (ICCV) 2019
- **Summary**: We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity.



### Spherical U-Net on Cortical Surfaces: Methods and Applications
- **Arxiv ID**: http://arxiv.org/abs/1904.00906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00906v1)
- **Published**: 2019-04-01 15:18:53+00:00
- **Updated**: 2019-04-01 15:18:53+00:00
- **Authors**: Fenqiang Zhao, Shunren Xia, Zhengwang Wu, Dingna Duan, Li Wang, Weili Lin, John H Gilmore, Dinggang Shen, Gang Li
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been providing the state-of-the-art performance for learning-related problems involving 2D/3D images in Euclidean space. However, unlike in the Euclidean space, the shapes of many structures in medical imaging have a spherical topology in a manifold space, e.g., brain cortical or subcortical surfaces represented by triangular meshes, with large inter-subject and intrasubject variations in vertex number and local connectivity. Hence, there is no consistent neighborhood definition and thus no straightforward convolution/transposed convolution operations for cortical/subcortical surface data. In this paper, by leveraging the regular and consistent geometric structure of the resampled cortical surface mapped onto the spherical space, we propose a novel convolution filter analogous to the standard convolution on the image grid. Accordingly, we develop corresponding operations for convolution, pooling, and transposed convolution for spherical surface data and thus construct spherical CNNs. Specifically, we propose the Spherical U-Net architecture by replacing all operations in the standard U-Net with their spherical operation counterparts. We then apply the Spherical U-Net to two challenging and neuroscientifically important tasks in infant brains: cortical surface parcellation and cortical attribute map development prediction. Both applications demonstrate the competitive performance in the accuracy, computational efficiency, and effectiveness of our proposed Spherical U-Net, in comparison with the state-of-the-art methods.



### The RGB-D Triathlon: Towards Agile Visual Toolboxes for Robots
- **Arxiv ID**: http://arxiv.org/abs/1904.00912v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.00912v2)
- **Published**: 2019-04-01 15:33:02+00:00
- **Updated**: 2019-04-02 11:59:33+00:00
- **Authors**: Fabio Cermelli, Massimiliano Mancini, Elisa Ricci, Barbara Caputo
- **Comment**: This work has been submitted to IROS/RAL 2019
- **Journal**: None
- **Summary**: Deep networks have brought significant advances in robot perception, enabling to improve the capabilities of robots in several visual tasks, ranging from object detection and recognition to pose estimation, semantic scene segmentation and many others. Still, most approaches typically address visual tasks in isolation, resulting in overspecialized models which achieve strong performances in specific applications but work poorly in other (often related) tasks. This is clearly sub-optimal for a robot which is often required to perform simultaneously multiple visual recognition tasks in order to properly act and interact with the environment. This problem is exacerbated by the limited computational and memory resources typically available onboard to a robotic platform. The problem of learning flexible models which can handle multiple tasks in a lightweight manner has recently gained attention in the computer vision community and benchmarks supporting this research have been proposed. In this work we study this problem in the robot vision context, proposing a new benchmark, the RGB-D Triathlon, and evaluating state of the art algorithms in this novel challenging scenario. We also define a new evaluation protocol, better suited to the robot vision setting. Results shed light on the strengths and weaknesses of existing approaches and on open issues, suggesting directions for future research.



### Robustness of 3D Deep Learning in an Adversarial Setting
- **Arxiv ID**: http://arxiv.org/abs/1904.00923v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00923v1)
- **Published**: 2019-04-01 15:51:12+00:00
- **Updated**: 2019-04-01 15:51:12+00:00
- **Authors**: Matthew Wicker, Marta Kwiatkowska
- **Comment**: 10 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.



### Early Diagnosis of Pneumonia with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.00937v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.00937v1)
- **Published**: 2019-04-01 16:18:35+00:00
- **Updated**: 2019-04-01 16:18:35+00:00
- **Authors**: Can Jozef Saul, Deniz Yagmur Urey, Can Doruk Taktakoglu
- **Comment**: None
- **Journal**: None
- **Summary**: Pneumonia has been one of the fatal diseases and has the potential to result in severe consequences within a short period of time, due to the flow of fluid in lungs, which leads to drowning. If not acted upon by drugs at the right time, pneumonia may result in death of individuals. Therefore, the early diagnosis is a key factor along the progress of the disease. This paper focuses on the biological progress of pneumonia and its detection by x-ray imaging, overviews the studies conducted on enhancing the level of diagnosis, and presents the methodology and results of an automation of xray images based on various parameters in order to detect the disease at very early stages. In this study we propose our deep learning architecture for the classification task, which is trained with modified images, through multiple steps of preprocessing. Our classification method uses convolutional neural networks and residual network architecture for classifying the images. Our findings yield an accuracy of 78.73%, surpassing the previously top scoring accuracy of 76.8%.



### Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses
- **Arxiv ID**: http://arxiv.org/abs/1904.00979v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00979v2)
- **Published**: 2019-04-01 17:31:02+00:00
- **Updated**: 2020-07-31 01:42:37+00:00
- **Authors**: Yingwei Li, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan L. Yuille
- **Comment**: ECCV 2020. Project page:
  https://github.com/LiYingwei/Regional-Homogeneity
- **Journal**: None
- **Summary**: This paper focuses on learning transferable adversarial examples specifically against defense models (models to defense adversarial attacks). In particular, we show that a simple universal perturbation can fool a series of state-of-the-art defenses.   Adversarial examples generated by existing attacks are generally hard to transfer to defense models. We observe the property of regional homogeneity in adversarial perturbations and suggest that the defenses are less robust to regionally homogeneous perturbations. Therefore, we propose an effective transforming paradigm and a customized gradient transformer module to transform existing perturbations into regionally homogeneous ones. Without explicitly forcing the perturbations to be universal, we observe that a well-trained gradient transformer module tends to output input-independent gradients (hence universal) benefiting from the under-fitting phenomenon. Thorough experiments demonstrate that our work significantly outperforms the prior art attacking algorithms (either image-dependent or universal ones) by an average improvement of 14.0% when attacking 9 defenses in the transfer-based attack setting. In addition to the cross-model transferability, we also verify that regionally homogeneous perturbations can well transfer across different vision tasks (attacking with the semantic segmentation task and testing on the object detection task). The code is available here: https://github.com/LiYingwei/Regional-Homogeneity.



### Automatic Nonrigid Histological Image Registration with Adaptive Multistep Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1904.00982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00982v1)
- **Published**: 2019-04-01 17:38:05+00:00
- **Updated**: 2019-04-01 17:38:05+00:00
- **Authors**: Marek Wodzinski, Andrzej Skalski
- **Comment**: Submission to ANHIR challenge
- **Journal**: None
- **Summary**: In this paper, we present a short description of the method proposed to ANHIR challenge organized jointly with the IEEE ISBI 2019 conference. We propose a method consisting of preprocessing, initial alignment, nonrigid registration algorithms and a method to automatically choose the best result. The method turned out to be robust (99.792% robustness) and accurate (0.38% average median rTRE). The main drawback of the proposed method is relatively high computation time. However, this aspect can be easily improved by cleaning the code and proposing a GPU implementation.



### Equivariant Multi-View Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.00993v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.00993v2)
- **Published**: 2019-04-01 17:58:17+00:00
- **Updated**: 2019-10-27 14:48:43+00:00
- **Authors**: Carlos Esteves, Yinshuang Xu, Christine Allen-Blanchette, Kostas Daniilidis
- **Comment**: Camera-ready. Accepted to ICCV'19 as oral presentation
- **Journal**: None
- **Summary**: Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, achieving view permutation invariance through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification.



### Robust Alignment for Panoramic Stitching via an Exact Rank Constraint
- **Arxiv ID**: http://arxiv.org/abs/1904.04158v1
- **DOI**: 10.1109/TIP.2019.2909800
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04158v1)
- **Published**: 2019-04-01 18:25:08+00:00
- **Updated**: 2019-04-01 18:25:08+00:00
- **Authors**: Yuelong Li, Mohammad Tofighi, Vishal Monga
- **Comment**: Accepted for publication in IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: We study the problem of image alignment for panoramic stitching. Unlike most existing approaches that are feature-based, our algorithm works on pixels directly, and accounts for errors across the whole images globally. Technically, we formulate the alignment problem as rank-1 and sparse matrix decomposition over transformed images, and develop an efficient algorithm for solving this challenging non-convex optimization problem. The algorithm reduces to solving a sequence of subproblems, where we analytically establish exact recovery conditions, convergence and optimality, together with convergence rate and complexity. We generalize it to simultaneously align multiple images and recover multiple homographies, extending its application scope towards vast majority of practical scenarios. Experimental results demonstrate that the proposed algorithm is capable of more accurately aligning the images and generating higher quality stitched images than state-of-the-art methods.



### Learning Matchable Image Transformations for Long-term Metric Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1904.01080v5
- **DOI**: 10.1109/LRA.2020.2967659
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.01080v5)
- **Published**: 2019-04-01 19:38:56+00:00
- **Updated**: 2022-07-05 04:40:27+00:00
- **Authors**: Lee Clement, Mona Gridseth, Justin Tomasi, Jonathan Kelly
- **Comment**: In IEEE Robotics and Automation Letters (RA-L) and presented at the
  IEEE International Conference on Robotics and Automation (ICRA'20), Paris,
  France, May 31-June 4, 2020
- **Journal**: IEEE Robotics and Automation Letters (RA-L), Vol. 5, No. 2, pp.
  1492-1499, Apr. 2020
- **Summary**: Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.



### Probabilistic Regression of Rotations using Quaternion Averaging and a Deep Multi-Headed Network
- **Arxiv ID**: http://arxiv.org/abs/1904.03182v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.03182v2)
- **Published**: 2019-04-01 19:39:09+00:00
- **Updated**: 2020-05-08 16:27:56+00:00
- **Authors**: Valentin Peretroukhin, Brandon Wagstaff, Matthew Giamou, Jonathan Kelly
- **Comment**: A shortened version of this work appears in the Proceedings of the
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR'19) Workshop
  on Uncertainty and Robustness in Deep Visual Learning, Long Beach,
  California, USA, Jun. 16-20 2019, pp. 83-86
- **Journal**: None
- **Summary**: Accurate estimates of rotation are crucial to vision-based motion estimation in augmented reality and robotics. In this work, we present a method to extract probabilistic estimates of rotation from deep regression models. First, we build on prior work and argue that a multi-headed network structure we name HydraNet provides better calibrated uncertainty estimates than methods that rely on stochastic forward passes. Second, we extend HydraNet to targets that belong to the rotation group, SO(3), by regressing unit quaternions and using the tools of rotation averaging and uncertainty injection onto the manifold to produce three-dimensional covariances. Finally, we present results and analysis on a synthetic dataset, learn consistent orientation estimates on the 7-Scenes dataset, and show how we can use our learned covariances to fuse deep estimates of relative orientation with classical stereo visual odometry to improve localization on the KITTI dataset.



### Infant-Prints: Fingerprints for Reducing Infant Mortality
- **Arxiv ID**: http://arxiv.org/abs/1904.01091v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01091v1)
- **Published**: 2019-04-01 20:03:40+00:00
- **Updated**: 2019-04-01 20:03:40+00:00
- **Authors**: Joshua J. Engelsma, Debayan Deb, Anil K. Jain, Prem S. Sudhish, Anjoo Bhatnager
- **Comment**: None
- **Journal**: None
- **Summary**: In developing countries around the world, a multitude of infants continue to suffer and die from vaccine-preventable diseases, and malnutrition. Lamentably, the lack of any official identification documentation makes it exceedingly difficult to prevent these infant deaths. To solve this global crisis, we propose Infant-Prints which is comprised of (i) a custom, compact, low-cost (85 USD), high-resolution (1,900 ppi) fingerprint reader, (ii) a high-resolution fingerprint matcher, and (iii) a mobile application for search and verification for the infant fingerprint. Using Infant-Prints, we have collected a longitudinal database of infant fingerprints and demonstrate its ability to perform accurate and reliable recognition of infants enrolled at the ages 0-3 months, in time for effective delivery of critical vaccinations and nutritional supplements (TAR=90% @ FAR = 0.1% for infants older than 8 weeks).



### Fingerprints: Fixed Length Representation via Deep Networks and Domain Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1904.01099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01099v1)
- **Published**: 2019-04-01 20:41:43+00:00
- **Updated**: 2019-04-01 20:41:43+00:00
- **Authors**: Joshua J. Engelsma, Kai Cao, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We learn a discriminative fixed length feature representation of fingerprints which stands in contrast to commonly used unordered, variable length sets of minutiae points. To arrive at this fixed length representation, we embed fingerprint domain knowledge into a multitask deep convolutional neural network architecture. Empirical results, on two public-domain fingerprint databases (NIST SD4 and FVC 2004 DB1) show that compared to minutiae representations, extracted by two state-of-the-art commercial matchers (Verifinger v6.3 and Innovatrics v2.0.3), our fixed-length representations provide (i) higher search accuracy: Rank-1 accuracy of 97.9% vs. 97.3% on NIST SD4 against a gallery size of 2000 and (ii) significantly faster, large scale search: 682,594 matches per second vs. 22 matches per second for commercial matchers on an i5 3.3 GHz processor with 8 GB of RAM.



### Creativity Inspired Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01109v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01109v6)
- **Published**: 2019-04-01 21:05:23+00:00
- **Updated**: 2019-12-03 02:42:29+00:00
- **Authors**: Mohamed Elhoseiny, Mohamed Elfeki
- **Comment**: This paper was published at the International Conference on Computer
  Vision 2019, Seoul, South Korea,
  http://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf
- **Journal**: International Conference on Computer Vision-2019
- **Summary**: Zero-shot learning (ZSL) aims at understanding unseen categories with no training examples from class-level descriptions. To improve the discriminative power of zero-shot learning, we model the visual learning process of unseen categories with inspiration from the psychology of human creativity for producing novel art. We relate ZSL to human creativity by observing that zero-shot learning is about recognizing the unseen and creativity is about creating a likable unseen. We introduce a learning signal inspired by creativity literature that explores the unseen space with hallucinated class-descriptions and encourages careful deviation of their visual feature generations from seen classes while allowing knowledge transfer from seen to unseen classes. Empirically, we show consistent improvement over the state of the art of several percents on the largest available benchmarks on the challenging task or generalized ZSL from a noisy text that we focus on, using the CUB and NABirds datasets. We also show the advantage of our approach on Attribute-based ZSL on three additional datasets (AwA2, aPY, and SUN). Code is available.



### Deep Learning Methods for Parallel Magnetic Resonance Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1904.01112v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.01112v1)
- **Published**: 2019-04-01 21:23:23+00:00
- **Updated**: 2019-04-01 21:23:23+00:00
- **Authors**: Florian Knoll, Kerstin Hammernik, Chi Zhang, Steen Moeller, Thomas Pock, Daniel K. Sodickson, Mehmet Akcakaya
- **Comment**: 14 pages, 7 figures
- **Journal**: None
- **Summary**: Following the success of deep learning in a wide range of applications, neural network-based machine learning techniques have received interest as a means of accelerating magnetic resonance imaging (MRI). A number of ideas inspired by deep learning techniques from computer vision and image processing have been successfully applied to non-linear image reconstruction in the spirit of compressed sensing for both low dose computed tomography and accelerated MRI. The additional integration of multi-coil information to recover missing k-space lines in the MRI reconstruction process, is still studied less frequently, even though it is the de-facto standard for currently used accelerated MR acquisitions. This manuscript provides an overview of the recent machine learning approaches that have been proposed specifically for improving parallel imaging. A general background introduction to parallel MRI is given that is structured around the classical view of image space and k-space based methods. Both linear and non-linear methods are covered, followed by a discussion of recent efforts to further improve parallel imaging using machine learning, and specifically using artificial neural networks. Image-domain based techniques that introduce improved regularizers are covered as well as k-space based methods, where the focus is on better interpolation strategies using neural networks. Issues and open problems are discussed as well as recent efforts for producing open datasets and benchmarks for the community.



### Deep Industrial Espionage
- **Arxiv ID**: http://arxiv.org/abs/1904.01114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01114v1)
- **Published**: 2019-04-01 21:27:52+00:00
- **Updated**: 2019-04-01 21:27:52+00:00
- **Authors**: Samuel Albanie, James Thewlis, Sebastien Ehrhardt, Joao Henriques
- **Comment**: None
- **Journal**: None
- **Summary**: The theory of deep learning is now considered largely solved, and is well understood by researchers and influencers alike. To maintain our relevance, we therefore seek to apply our skills to under-explored, lucrative applications of this technology. To this end, we propose and Deep Industrial Espionage, an efficient end-to-end framework for industrial information propagation and productisation. Specifically, given a single image of a product or service, we aim to reverse-engineer, rebrand and distribute a copycat of the product at a profitable price-point to consumers in an emerging market---all within in a single forward pass of a Neural Network. Differently from prior work in machine perception which has been restricted to classifying, detecting and reasoning about object instances, our method offers tangible business value in a wide range of corporate settings. Our approach draws heavily on a promising recent arxiv paper until its original authors' names can no longer be read (we use felt tip pen). We then rephrase the anonymised paper, add the word "novel" to the title, and submit it a prestigious, closed-access espionage journal who assure us that someday, we will be entitled to some fraction of their extortionate readership fees.



### HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1904.01121v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01121v4)
- **Published**: 2019-04-01 21:48:41+00:00
- **Updated**: 2019-10-31 23:43:11+00:00
- **Authors**: Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Li Fei-Fei, Michael S. Bernstein
- **Comment**: https://hype.stanford.edu
- **Journal**: None
- **Summary**: Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g. 250ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements across training epochs, and we confirm via bootstrap sampling that HYPE rankings are consistent and replicable.



### Filling Factors of Sunspots in SODISM Images
- **Arxiv ID**: http://arxiv.org/abs/1904.01133v1
- **DOI**: 10.33166/AETiC.2019.02.001
- **Categories**: **astro-ph.SR**, astro-ph.IM, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.01133v1)
- **Published**: 2019-04-01 22:41:53+00:00
- **Updated**: 2019-04-01 22:41:53+00:00
- **Authors**: Amro F. Alasta, Abdrazag Algamudi, Fatma Almesrati, Mustapha Meftah, Rami Qahwaji
- **Comment**: 11 pages, 7 figures, 2 tables This article is an extension of our
  previous studies investigating the detection of sunspots using SODISM images.
  The paper presented in August 2018 at the IEEE International Conference on
  Computing, Electronics and Communications Engineering.
  http://aetic.theiaer.org/archive/v3/v3n2/p1.html
- **Journal**: Published by International Association of Educators and
  Researchers (IAER ) Annals of Emerging Technologies in Computing (AETiC) Vol.
  3, No. 2, 2019
- **Summary**: Received: 1st December 2018; Accepted: 18th February 2019; Published: 1st April 2019 Abstract: The calculated filling factors (FFs) for a feature reflect the fraction of the solar disc covered by that feature, and the assignment of reference synthetic spectra. In this paper, the FFs, specified as a function of radial position on the solar disc, are computed for each image in a tabular form. The filling factor (FF) is an important parameter and is defined as the fraction of area in a pixel covered with the magnetic field, whereas the rest of the area in the pixel is field-free. However, this does not provide extensive information about the experiments conducted on tens or hundreds of such images. This is the first time that filling factors for SODISM images have been catalogued in tabular formation. This paper presents a new method that provides the means to detect sunspots on full-disk solar images recorded by the Solar Diameter Imager and Surface Mapper (SODISM) on the PICARD satellite. The method is a totally automated detection process that achieves a sunspot recognition rate of 97.6%. The number of sunspots detected by this method strongly agrees with the NOAA catalogue. The sunspot areas calculated by this method have a 99% correlation with SOHO over the same period, and thus help to calculate the filling factor for wavelength (W.L.) 607nm.



### Surgical Gesture Recognition with Optical Flow only
- **Arxiv ID**: http://arxiv.org/abs/1904.01143v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01143v2)
- **Published**: 2019-04-01 23:40:21+00:00
- **Updated**: 2020-05-15 22:10:21+00:00
- **Authors**: Duygu Sarikaya, Pierre Jannin
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the open research problem of surgical gesture recognition using motion cues from video data only. We adapt Optical flow ConvNets initially proposed by Simonyan et al.. While Simonyan uses both RGB frames and dense optical flow, we use only dense optical flow representations as input to emphasize the role of motion in surgical gesture recognition, and present it as a robust alternative to kinematic data. We also overcome one of the limitations of Optical flow ConvNets by initializing our model with cross modality pre-training. A large number of promising studies that address surgical gesture recognition highly rely on kinematic data which requires additional recording devices. To our knowledge, this is the first paper that addresses surgical gesture recognition using dense optical flow information only. We achieve competitive results on JIGSAWS dataset, moreover, our model achieves more robust results with less standard deviation, which suggests optical flow information can be used as an alternative to kinematic data for the recognition of surgical gestures.



