# Arxiv Papers in cs.CV on 2019-04-07
### Long-Term Vehicle Localization by Recursive Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.03551v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03551v1)
- **Published**: 2019-04-07 00:16:51+00:00
- **Updated**: 2019-04-07 00:16:51+00:00
- **Authors**: Hiroki Tomoe, Tanaka Kanji
- **Comment**: 5 pages, 3 figures, technical report. arXiv admin note: text overlap
  with arXiv:1709.05470 by other authors
- **Journal**: None
- **Summary**: Most of the current state-of-the-art frameworks for cross-season visual place recognition (CS-VPR) focus on domain adaptation (DA) to a single specific season. From the viewpoint of long-term CS-VPR, such frameworks do not scale well to sequential multiple domains (e.g., spring - summer - autumn - winter - ... ). The goal of this study is to develop a novel long-term ensemble learning (LEL) framework that allows for a constant cost retraining in long-term sequential-multi-domain CS-VPR (SMD-VPR), which only requires the memorization of a small constant number of deep convolutional neural networks (CNNs) and can retrain the CNN ensemble of every season at a small constant time/space cost. We frame our task as the multi-teacher multi-student knowledge distillation (MTMS-KD), which recursively compresses all the previous season's knowledge into a current CNN ensemble. We further address the issue of teacher-student-assignment (TSA) to achieve a good generalization/specialization tradeoff. Experimental results on SMD-VPR tasks validate the efficacy of the proposed approach.



### Scalable Change Retrieval Using Deep 3D Neural Codes
- **Arxiv ID**: http://arxiv.org/abs/1904.03552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03552v1)
- **Published**: 2019-04-07 00:26:32+00:00
- **Updated**: 2019-04-07 00:26:32+00:00
- **Authors**: Kojima Yusuke, Tanaka Kanji, Yang Naiming, Hirota Yuji
- **Comment**: 5 pages, 1 figure, technical report
- **Journal**: None
- **Summary**: We present a novel scalable framework for image change detection (ICD) from an on-board 3D imagery system. We argue that existing ICD systems are constrained by the time required to align a given query image with individual reference image coordinates. We utilize an invariant coordinate system (ICS) to replace the time-consuming image alignment with an offline pre-processing procedure. Our key contribution is an extension of the traditional image comparison-based ICD tasks to setups of the image retrieval (IR) task. We replace each component of the 3D ICD system, i.e., (1) image modeling, (2) image alignment, and (3) image differencing, with significantly efficient variants from the bag-of-words (BoW) IR paradigm. Further, we train a deep 3D feature extractor in an unsupervised manner using an unsupervised Siamese network and automatically collected training data. We conducted experiments on a challenging cross-season ICD task using a publicly available dataset and thereby validate the efficacy of the proposed approach.



### Place-specific Background Modeling Using Recursive Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1904.03555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03555v1)
- **Published**: 2019-04-07 00:32:05+00:00
- **Updated**: 2019-04-07 00:32:05+00:00
- **Authors**: Yamaguchi Kousuke, Tanaka Kanji, Sugimoto Takuma, Ide Rino, Takeda Koji
- **Comment**: 6 pages, 3 figures, technical report
- **Journal**: None
- **Summary**: Image change detection (ICD) to detect changed objects in front of a vehicle with respect to a place-specific background model using an on-board monocular vision system is a fundamental problem in intelligent vehicle (IV). From the perspective of recent large-scale IV applications, it can be impractical in terms of space/time efficiency to train place-specific background models for every possible place. To address these issues, we introduce a new autoencoder (AE) based efficient ICD framework that combines the advantages of AE-based anomaly detection (AD) and AE-based image compression (IC). We propose a method that uses AE reconstruction errors as a single unified measure for training a minimal set of place-specific AEs and maintains detection accuracy. We introduce an efficient incremental recursive AE (rAE) training framework that recursively summarizes a large collection of background images into the AE set. The results of experiments on challenging cross-season ICD tasks validate the efficacy of the proposed approach.



### Image and Video Compression with Neural Networks: A Review
- **Arxiv ID**: http://arxiv.org/abs/1904.03567v2
- **DOI**: 10.1109/TCSVT.2019.2910119
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03567v2)
- **Published**: 2019-04-07 01:59:57+00:00
- **Updated**: 2019-04-10 09:40:37+00:00
- **Authors**: Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui Zhao, Shiqi Wang, Shanshe Wang
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (T-CSVT) as transactions paper
- **Journal**: IEEE Transactions on Circuits and Systems for Video Technology,
  2019
- **Summary**: In recent years, the image and video coding technologies have advanced by leaps and bounds. However, due to the popularization of image and video acquisition devices, the growth rate of image and video data is far beyond the improvement of the compression ratio. In particular, it has been widely recognized that there are increasing challenges of pursuing further coding performance improvement within the traditional hybrid coding framework. Deep convolution neural network (CNN) which makes the neural network resurge in recent years and has achieved great success in both artificial intelligent and signal processing fields, also provides a novel and promising solution for image and video compression. In this paper, we provide a systematic, comprehensive and up-to-date review of neural network based image and video compression techniques. The evolution and development of neural network based compression methodologies are introduced for images and video respectively. More specifically, the cutting-edge video coding techniques by leveraging deep learning and HEVC framework are presented and discussed, which promote the state-of-the-art video coding performance substantially. Moreover, the end-to-end image and video coding frameworks based on neural networks are also reviewed, revealing interesting explorations on next generation image and video coding frameworks/standards. The most significant research works on the image and video coding related topics using neural networks are highlighted, and future trends are also envisioned. In particular, the joint compression on semantic and visual information is tentatively explored to formulate high efficiency signal representation structure for both human vision and machine vision, which are the two dominant signal receptor in the age of artificial intelligence.



### A Dilated Inception Network for Visual Saliency Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.03571v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03571v2)
- **Published**: 2019-04-07 02:41:44+00:00
- **Updated**: 2019-05-09 02:34:24+00:00
- **Authors**: Sheng Yang, Guosheng Lin, Qiuping Jiang, Weisi Lin
- **Comment**: Accepted by IEEE Transactions on Multimedia. The source codes are
  available at https://github.com/ysyscool/DINet
- **Journal**: None
- **Summary**: Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we proposed an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved by using a set of linear normalization-based probability distribution distance metrics as loss functions. As such, we can formulate saliency prediction as a probability distribution prediction task for global saliency inference instead of a typical pixel-wise regression problem. Experimental results on several challenging saliency benchmark datasets demonstrate that our DINet with proposed loss functions can achieve state-of-the-art performance with shorter inference time.



### Adaptively Connected Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03579v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03579v1)
- **Published**: 2019-04-07 04:01:27+00:00
- **Updated**: 2019-04-07 04:01:27+00:00
- **Authors**: Guangrun Wang, Keze Wang, Liang Lin
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: This paper presents a novel adaptively connected neural network (ACNet) to improve the traditional convolutional neural networks (CNNs) {in} two aspects. First, ACNet employs a flexible way to switch global and local inference in processing the internal feature representations by adaptively determining the connection status among the feature nodes (e.g., pixels of the feature maps) \footnote{In a computer vision domain, a node refers to a pixel of a feature map{, while} in {the} graph domain, a node denotes a graph node.}. We can show that existing CNNs, the classical multilayer perceptron (MLP), and the recently proposed non-local network (NLN) \cite{nonlocalnn17} are all special cases of ACNet. Second, ACNet is also capable of handling non-Euclidean data. Extensive experimental analyses on {a variety of benchmarks (i.e.,} ImageNet-1k classification, COCO 2017 detection and segmentation, CUHK03 person re-identification, CIFAR analysis, and Cora document categorization) demonstrate that {ACNet} cannot only achieve state-of-the-art performance but also overcome the limitation of the conventional MLP and CNN \footnote{Corresponding author: Liang Lin (linliang@ieee.org)}. The code is available at \url{https://github.com/wanggrun/Adaptively-Connected-Neural-Networks}.



### Compare More Nuanced:Pairwise Alignment Bilinear Network For Few-shot Fine-grained Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.03580v2
- **DOI**: 10.1109/ICME.2019.00024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03580v2)
- **Published**: 2019-04-07 04:01:52+00:00
- **Updated**: 2019-04-11 10:50:00+00:00
- **Authors**: Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Jingsong Xu
- **Comment**: ICME 2019 Oral
- **Journal**: None
- **Summary**: The recognition ability of human beings is developed in a progressive way. Usually, children learn to discriminate various objects from coarse to fine-grained with limited supervision. Inspired by this learning process, we propose a simple yet effective model for the Few-Shot Fine-Grained (FSFG) recognition, which tries to tackle the challenging fine-grained recognition task using meta-learning. The proposed method, named Pairwise Alignment Bilinear Network (PABN), is an end-to-end deep neural network. Unlike traditional deep bilinear networks for fine-grained classification, which adopt the self-bilinear pooling to capture the subtle features of images, the proposed model uses a novel pairwise bilinear pooling to compare the nuanced differences between base images and query images for learning a deep distance metric. In order to match base image features with query image features, we design feature alignment losses before the proposed pairwise bilinear pooling. Experiment results on four fine-grained classification datasets and one generic few-shot dataset demonstrate that the proposed model outperforms both the state-ofthe-art few-shot fine-grained and general few-shot methods.



### Multi-Label Image Recognition with Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03582v1)
- **Published**: 2019-04-07 04:36:50+00:00
- **Updated**: 2019-04-07 04:36:50+00:00
- **Authors**: Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, Yanwen Guo
- **Comment**: To appear at CVPR 2019 (Source codes have been released on
  https://github.com/chenzhaomin123/ML_GCN)
- **Journal**: None
- **Summary**: The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned to map this label graph into a set of inter-dependent object classifiers. These classifiers are applied to the image descriptors extracted by another sub-net, enabling the whole network to be end-to-end trainable. Furthermore, we propose a novel re-weighted scheme to create an effective label correlation matrix to guide information propagation among the nodes in GCN. Experiments on two multi-label image recognition datasets show that our approach obviously outperforms other existing state-of-the-art methods. In addition, visualization analyses reveal that the classifiers learned by our model maintain meaningful semantic topology.



### Modularized Textual Grounding for Counterfactual Resilience
- **Arxiv ID**: http://arxiv.org/abs/1904.03589v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03589v2)
- **Published**: 2019-04-07 05:59:04+00:00
- **Updated**: 2019-07-01 04:42:34+00:00
- **Authors**: Zhiyuan Fang, Shu Kong, Charless Fowlkes, Yezhou Yang
- **Comment**: 13 pages, 12 figures, IEEE Conference on Computer Vision and Pattern
  Recognition, 2019
- **Journal**: None
- **Summary**: Computer Vision applications often require a textual grounding module with precision, interpretability, and resilience to counterfactual inputs/queries. To achieve high grounding precision, current textual grounding methods heavily rely on large-scale training data with manual annotations at the pixel level. Such annotations are expensive to obtain and thus severely narrow the model's scope of real-world applications. Moreover, most of these methods sacrifice interpretability, generalizability, and they neglect the importance of being resilient to counterfactual inputs. To address these issues, we propose a visual grounding system which is 1) end-to-end trainable in a weakly supervised fashion with only image-level annotations, and 2) counterfactually resilient owing to the modular design. Specifically, we decompose textual descriptions into three levels: entity, semantic attribute, color information, and perform compositional grounding progressively. We validate our model through a series of experiments and demonstrate its improvement over the state-of-the-art methods. In particular, our model's performance not only surpasses other weakly/un-supervised methods and even approaches the strongly supervised ones, but also is interpretable for decision making and performs much better in face of counterfactual classes than all the others.



### Self-supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics
- **Arxiv ID**: http://arxiv.org/abs/1904.03597v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03597v1)
- **Published**: 2019-04-07 07:27:37+00:00
- **Updated**: 2019-04-07 07:27:37+00:00
- **Authors**: Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, Wei Liu
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We address the problem of video representation learning without human-annotated labels. While previous efforts address the problem by designing novel self-supervised tasks using video data, the learned features are merely on a frame-by-frame basis, which are not applicable to many video analytic tasks where spatio-temporal features are prevailing. In this paper we propose a novel self-supervised approach to learn spatio-temporal features for video representation. Inspired by the success of two-stream approaches in video classification, we propose to learn visual features by regressing both motion and appearance statistics along spatial and temporal dimensions, given only the input video data. Specifically, we extract statistical concepts (fast-motion region and the corresponding dominant direction, spatio-temporal color diversity, dominant color, etc.) from simple patterns in both spatial and temporal domains. Unlike prior puzzles that are even hard for humans to solve, the proposed approach is consistent with human inherent visual habits and therefore easy to answer. We conduct extensive experiments with C3D to validate the effectiveness of our proposed approach. The experiments show that our approach can significantly improve the performance of C3D when applied to video classification tasks. Code is available at https://github.com/laura-wang/video_repres_mas.



### Cursive Multilingual Characters Recognition Based on Hard Geometric Features
- **Arxiv ID**: http://arxiv.org/abs/1904.08760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08760v1)
- **Published**: 2019-04-07 08:26:38+00:00
- **Updated**: 2019-04-07 08:26:38+00:00
- **Authors**: Amjad Rehman, Majid Harouni, Tanzila Saba
- **Comment**: 12
- **Journal**: None
- **Summary**: The cursive nature of multilingual characters segmentation and recognition of Arabic, Persian, Urdu languages have attracted researchers from academia and industry. However, despite several decades of research, still multilingual characters classification accuracy is not up to the mark. This paper presents an automated approach for multilingual characters segmentation and recognition. The proposed methodology explores character based on their geometric features. However, due to uncertainty and without dictionary support few characters are over-divided. To expand the productivity of the proposed methodology a BPN is prepared with countless division focuses for cursive multilingual characters. Prepared BPN separates off base portioned indicates effectively with rapid upgrade character acknowledgment precision. For reasonable examination, only benchmark dataset is utilized.



### An Ensemble of Neural Networks for Non-Linear Segmentation of Overlapped Cursive Script
- **Arxiv ID**: http://arxiv.org/abs/1904.12592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12592v1)
- **Published**: 2019-04-07 08:32:37+00:00
- **Updated**: 2019-04-07 08:32:37+00:00
- **Authors**: Amjad Rehman
- **Comment**: 15 Pages
- **Journal**: None
- **Summary**: Precise character segmentation is the only solution towards higher Optical Character Recognition (OCR) accuracy. In cursive script, overlapped characters are serious issue in the process of character segmentations as characters are deprived from their discriminative parts using conventional linear segmentation strategy. Hence, non-linear segmentation is an utmost need to avoid loss of characters parts and to enhance character/script recognition accuracy. This paper presents an improved approach for non-linear segmentation of the overlapped characters in handwritten roman script. The proposed technique is composed of a sequence of heuristic rules based on geometrical features of characters to locate possible non-linear character boundaries in a cursive script word. However, to enhance efficiency, heuristic approach is integrated with trained ensemble neural network validation strategy for verification of character boundaries. Accordingly, correct boundaries are retained and incorrect are removed based on ensemble neural networks vote. Finally, based on verified valid segmentation points, characters are segmented non-linearly. For fair comparison CEDAR benchmark database is experimented. The experimental results are much better than conventional linear character segmentation techniques reported in the state of art. Ensemble neural network play vital role to enhance character segmentation accuracy as compared to individual neural networks.



### Normalized Diversification
- **Arxiv ID**: http://arxiv.org/abs/1904.03608v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03608v3)
- **Published**: 2019-04-07 09:00:35+00:00
- **Updated**: 2021-10-04 21:00:03+00:00
- **Authors**: Shaohui Liu, Xiao Zhang, Jianqiao Wangni, Jianbo Shi
- **Comment**: 12 pages, 9 figures, to appear in CVPR 2019
- **Journal**: None
- **Summary**: Generating diverse yet specific data is the goal of the generative adversarial network (GAN), but it suffers from the problem of mode collapse. We introduce the concept of normalized diversity which force the model to preserve the normalized pairwise distance between the sparse samples from a latent parametric distribution and their corresponding high-dimensional outputs. The normalized diversification aims to unfold the manifold of unknown topology and non-uniform distribution, which leads to safe interpolation between valid latent variables. By alternating the maximization over the pairwise distance and updating the total distance (normalizer), we encourage the model to actively explore in the high-dimensional output space. We demonstrate that by combining the normalized diversity loss and the adversarial loss, we generate diverse data without suffering from mode collapsing. Experimental results show that our method achieves consistent improvement on unsupervised image generation, conditional image generation and hand pose estimation over strong baselines.



### Identity-preserving Face Recovery from Stylized Portraits
- **Arxiv ID**: http://arxiv.org/abs/1904.04241v1
- **DOI**: 10.1007/s11263-019-01169-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04241v1)
- **Published**: 2019-04-07 09:18:59+00:00
- **Updated**: 2019-04-07 09:18:59+00:00
- **Authors**: Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz
- **Comment**: International Journal of Computer Vision 2019. arXiv admin note:
  substantial text overlap with arXiv:1801.02279
- **Journal**: None
- **Summary**: Given an artistic portrait, recovering the latent photorealistic face that preserves the subject's identity is challenging because the facial details are often distorted or fully lost in artistic portraits. We develop an Identity-preserving Face Recovery from Portraits (IFRP) method that utilizes a Style Removal network (SRN) and a Discriminative Network (DN). Our SRN, composed of an autoencoder with residual block-embedded skip connections, is designed to transfer feature maps of stylized images to the feature maps of the corresponding photorealistic faces. Owing to the Spatial Transformer Network (STN), SRN automatically compensates for misalignments of stylized portraits to output aligned realistic face images. To ensure the identity preservation, we promote the recovered and ground truth faces to share similar visual features via a distance measure which compares features of recovered and ground truth faces extracted from a pre-trained FaceNet network. DN has multiple convolutional and fully-connected layers, and its role is to enforce recovered faces to be similar to authentic faces. Thus, we can recover high-quality photorealistic faces from unaligned portraits while preserving the identity of the face in an image. By conducting extensive evaluations on a large-scale synthesized dataset and a hand-drawn sketch dataset, we demonstrate that our method achieves superior face recovery and attains state-of-the-art results. In addition, our method can recover photorealistic faces from unseen stylized portraits, artistic paintings, and hand-drawn sketches.



### Recovering Faces from Portraits with Auxiliary Facial Attributes
- **Arxiv ID**: http://arxiv.org/abs/1904.03612v1
- **DOI**: 10.1109/WACV.2019.00049
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03612v1)
- **Published**: 2019-04-07 09:29:36+00:00
- **Updated**: 2019-04-07 09:29:36+00:00
- **Authors**: Fatemeh Shiri, Xin Yu, Fatih Porikli, Richard Hartley, Piotr Koniusz
- **Comment**: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV)
- **Journal**: None
- **Summary**: Recovering a photorealistic face from an artistic portrait is a challenging task since crucial facial details are often distorted or completely lost in artistic compositions. To handle this loss, we propose an Attribute-guided Face Recovery from Portraits (AFRP) that utilizes a Face Recovery Network (FRN) and a Discriminative Network (DN). FRN consists of an autoencoder with residual block-embedded skip-connections and incorporates facial attribute vectors into the feature maps of input portraits at the bottleneck of the autoencoder. DN has multiple convolutional and fully-connected layers, and its role is to enforce FRN to generate authentic face images with corresponding facial attributes dictated by the input attribute vectors. %Leveraging on the spatial transformer networks, FRN automatically compensates for misalignments of portraits. % and generates aligned face images. For the preservation of identities, we impose the recovered and ground-truth faces to share similar visual features. Specifically, DN determines whether the recovered image looks like a real face and checks if the facial attributes extracted from the recovered image are consistent with given attributes. %Our method can recover high-quality photorealistic faces from unaligned portraits while preserving the identity of the face images as well as it can reconstruct a photorealistic face image with a desired set of attributes. Our method can recover photorealistic identity-preserving faces with desired attributes from unseen stylized portraits, artistic paintings, and hand-drawn sketches. On large-scale synthesized and sketch datasets, we demonstrate that our face recovery method achieves state-of-the-art results.



### A Facial Affect Analysis System for Autism Spectrum Disorder
- **Arxiv ID**: http://arxiv.org/abs/1904.03616v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03616v1)
- **Published**: 2019-04-07 10:08:35+00:00
- **Updated**: 2019-04-07 10:08:35+00:00
- **Authors**: Beibin Li, Sachin Mehta, Deepali Aneja, Claire Foster, Pamela Ventola, Frederick Shic, Linda Shapiro
- **Comment**: 5 pages (including 1 page for reference), 3 figures
- **Journal**: None
- **Summary**: In this paper, we introduce an end-to-end machine learning-based system for classifying autism spectrum disorder (ASD) using facial attributes such as expressions, action units, arousal, and valence. Our system classifies ASD using representations of different facial attributes from convolutional neural networks, which are trained on images in the wild. Our experimental results show that different facial attributes used in our system are statistically significant and improve sensitivity, specificity, and F1 score of ASD classification by a large margin. In particular, the addition of different facial attributes improves the performance of ASD classification by about 7% which achieves a F1 score of 76%.



### Learning Metrics from Teachers: Compact Networks for Image Embedding
- **Arxiv ID**: http://arxiv.org/abs/1904.03624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03624v1)
- **Published**: 2019-04-07 11:12:07+00:00
- **Updated**: 2019-04-07 11:12:07+00:00
- **Authors**: Lu Yu, Vacit Oguz Yazici, Xialei Liu, Joost van de Weijer, Yongmei Cheng, Arnau Ramisa
- **Comment**: To appear at CVPR 2019
- **Journal**: None
- **Summary**: Metric learning networks are used to compute image embeddings, which are widely used in many applications such as image retrieval and face recognition. In this paper, we propose to use network distillation to efficiently compute image embeddings with small networks. Network distillation has been successfully applied to improve image classification, but has hardly been explored for metric learning. To do so, we propose two new loss functions that model the communication of a deep teacher network to a small student network. We evaluate our system in several datasets, including CUB-200-2011, Cars-196, Stanford Online Products and show that embeddings computed using small student networks perform significantly better than those computed using standard networks of similar size. Results on a very compact network (MobileNet-0.25), which can be used on mobile devices, show that the proposed method can greatly improve Recall@1 results from 27.5\% to 44.6\%. Furthermore, we investigate various aspects of distillation for embeddings, including hint and attention layers, semi-supervised learning and cross quality distillation. (Code is available at https://github.com/yulu0724/EmbeddingDistillation.)



### Adaptive NMS: Refining Pedestrian Detection in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1904.03629v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03629v1)
- **Published**: 2019-04-07 11:50:25+00:00
- **Updated**: 2019-04-07 11:50:25+00:00
- **Authors**: Songtao Liu, Di Huang, Yunhong Wang
- **Comment**: To appear at CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Pedestrian detection in a crowd is a very challenging issue. This paper addresses this problem by a novel Non-Maximum Suppression (NMS) algorithm to better refine the bounding boxes given by detectors. The contributions are threefold: (1) we propose adaptive-NMS, which applies a dynamic suppression threshold to an instance, according to the target density; (2) we design an efficient subnetwork to learn density scores, which can be conveniently embedded into both the single-stage and two-stage detectors; and (3) we achieve state of the art results on the CityPersons and CrowdHuman benchmarks.



### Learning to Learn Relation for Important People Detection in Still Images
- **Arxiv ID**: http://arxiv.org/abs/1904.03632v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03632v1)
- **Published**: 2019-04-07 12:11:21+00:00
- **Updated**: 2019-04-07 12:11:21+00:00
- **Authors**: Wei-Hong Li, Fa-Ting Hong, Wei-Shi Zheng
- **Comment**: Important people detection, Relation Network, POINT
- **Journal**: None
- **Summary**: Humans can easily recognize the importance of people in social event images, and they always focus on the most important individuals. However, learning to learn the relation between people in an image, and inferring the most important person based on this relation, remains undeveloped. In this work, we propose a deep imPOrtance relatIon NeTwork (POINT) that combines both relation modeling and feature learning. In particular, we infer two types of interaction modules: the person-person interaction module that learns the interaction between people and the event-person interaction module that learns to describe how a person is involved in the event occurring in an image. We then estimate the importance relations among people from both interactions and encode the relation feature from the importance relations. In this way, POINT automatically learns several types of relation features in parallel, and we aggregate these relation features and the person's feature to form the importance feature for important people classification. Extensive experimental results show that our method is effective for important people detection and verify the efficacy of learning to learn relations for important people detection.



### Real-Time Quality Assessment of Pediatric MRI via Semi-Supervised Deep Nonlocal Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03639v2
- **DOI**: 10.1109/TIP.2020.2992079
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03639v2)
- **Published**: 2019-04-07 12:37:13+00:00
- **Updated**: 2020-05-22 23:12:31+00:00
- **Authors**: Siyuan Liu, Kim-Han Thung, Weili Lin, Pew-Thian Yap, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce an image quality assessment (IQA) method for pediatric T1- and T2-weighted MR images. IQA is first performed slice-wise using a nonlocal residual neural network (NR-Net) and then volume-wise by agglomerating the slice QA results using random forest. Our method requires only a small amount of quality-annotated images for training and is designed to be robust to annotation noise that might occur due to rater errors and the inevitable mix of good and bad slices in an image volume. Using a small set of quality-assessed images, we pre-train NR-Net to annotate each image slice with an initial quality rating (i.e., pass, questionable, fail), which we then refine by semi-supervised learning and iterative self-training. Experimental results demonstrate that our method, trained using only samples of modest size, exhibit great generalizability, capable of real-time (milliseconds per volume) large-scale IQA with near-perfect accuracy.



### Robust Building-based Registration of Airborne LiDAR Data and Optical Imagery on Urban Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.03668v1
- **DOI**: 10.1109/IGARSS.2019.8898612
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03668v1)
- **Published**: 2019-04-07 15:13:58+00:00
- **Updated**: 2019-04-07 15:13:58+00:00
- **Authors**: Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes, Jean-Marc Le Caillec
- **Comment**: Copyright 2019 IEEE. Published in the IEEE 2019 International
  Geoscience & Remote Sensing Symposium (IGARSS 2019), scheduled for July 28 -
  August 2, 2019 in Yokohama, Japan
- **Journal**: IGARSS 2019 - 2019 IEEE International Geoscience and Remote
  Sensing Symposium, Yokohama, Japan, 2019, pp. 8474-8477
- **Summary**: The motivation of this paper is to address the problem of registering airborne LiDAR data and optical aerial or satellite imagery acquired from different platforms, at different times, with different points of view and levels of detail. In this paper, we present a robust registration method based on building regions, which are extracted from optical images using mean shift segmentation, and from LiDAR data using a 3D point cloud filtering process. The matching of the extracted building segments is then carried out using Graph Transformation Matching (GTM) which allows to determine a common pattern of relative positions of segment centers. Thanks to this registration, the relative shifts between the data sets are significantly reduced, which enables a subsequent fine registration and a resulting high-quality data fusion.



### Unsupervised Domain Adaptation for Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.03692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03692v1)
- **Published**: 2019-04-07 17:24:28+00:00
- **Updated**: 2019-04-07 17:24:28+00:00
- **Authors**: Dayan Guan, Xing Luo, Yanpeng Cao, Jiangxin Yang, Yanlong Cao, George Vosselman, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal information (e.g., visible and thermal) can generate robust pedestrian detections to facilitate around-the-clock computer vision applications, such as autonomous driving and video surveillance. However, it still remains a crucial challenge to train a reliable detector working well in different multispectral pedestrian datasets without manual annotations. In this paper, we propose a novel unsupervised domain adaptation framework for multispectral pedestrian detection, by iteratively generating pseudo annotations and updating the parameters of our designed multispectral pedestrian detector on target domain. Pseudo annotations are generated using the detector trained on source domain, and then updated by fixing the parameters of detector and minimizing the cross entropy loss without back-propagation. Training labels are generated using the pseudo annotations by considering the characteristics of similarity and complementarity between well-aligned visible and infrared image pairs. The parameters of detector are updated using the generated labels by minimizing our defined multi-detection loss function with back-propagation. The optimal parameters of detector can be obtained after iteratively updating the pseudo annotations and parameters. Experimental results show that our proposed unsupervised multimodal domain adaptation method achieves significantly higher detection performance than the approach without domain adaptation, and is competitive with the supervised multispectral pedestrian detectors.



### On-line and on-board planning and perception for quadrupedal locomotion
- **Arxiv ID**: http://arxiv.org/abs/1904.03693v1
- **DOI**: 10.1109/TePRA.2015.7219685
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03693v1)
- **Published**: 2019-04-07 17:27:14+00:00
- **Updated**: 2019-04-07 17:27:14+00:00
- **Authors**: Carlos Mastalli, Ioannis Havoutis, Alexander W. Winkler, Darwin G. Caldwell, Claudio Semini
- **Comment**: 7 pages, International Conference on Technologies for Practical Robot
  Applications
- **Journal**: published 2015
- **Summary**: We present a legged motion planning approach for quadrupedal locomotion over challenging terrain. We decompose the problem into body action planning and footstep planning. We use a lattice representation together with a set of defined body movement primitives for computing a body action plan. The lattice representation allows us to plan versatile movements that ensure feasibility for every possible plan. To this end, we propose a set of rules that define the footstep search regions and footstep sequence given a body action. We use Anytime Repairing A* (ARA*) search that guarantees bounded suboptimal plans. Our main contribution is a planning approach that generates on-line versatile movements. Experimental trials demonstrate the performance of our planning approach in a set of challenging terrain conditions. The terrain information and plans are computed on-line and on-board.



### A Novel Apex-Time Network for Cross-Dataset Micro-Expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.03699v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03699v7)
- **Published**: 2019-04-07 17:59:04+00:00
- **Updated**: 2019-08-17 12:41:09+00:00
- **Authors**: Min Peng, Chongyang Wang, Tao Bi, Tong Chen, XiangDong Zhou, Yu shi
- **Comment**: 6 pages, 3 figures, 3 tables, code available, accepted in ACII 2019
- **Journal**: None
- **Summary**: The automatic recognition of micro-expression has been boosted ever since the successful introduction of deep learning approaches. As researchers working on such topics are moving to learn from the nature of micro-expression, the practice of using deep learning techniques has evolved from processing the entire video clip of micro-expression to the recognition on apex frame. Using the apex frame is able to get rid of redundant video frames, but the relevant temporal evidence of micro-expression would be thereby left out. This paper proposes a novel Apex-Time Network (ATNet) to recognize micro-expression based on spatial information from the apex frame as well as on temporal information from the respective-adjacent frames. Through extensive experiments on three benchmarks, we demonstrate the improvement achieved by learning such temporal information. Specially, the model with such temporal information is more robust in cross-dataset validations.



### Planar Geometry and Image Recovery from Motion-Blur
- **Arxiv ID**: http://arxiv.org/abs/1904.03710v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03710v3)
- **Published**: 2019-04-07 18:58:47+00:00
- **Updated**: 2022-02-07 00:25:52+00:00
- **Authors**: Kuldeep Purohit, Subeesh Vasu, M. Purnachandra Rao, A. N. Rajagopalan
- **Comment**: None
- **Journal**: None
- **Summary**: Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a single motion blurred observation. We then develop an algorithm for automatic recovery of number of planes, the parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene. Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal that our proposed method achieves state-of-the-art results.



### Stokes Inversion based on Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03714v2
- **DOI**: 10.1051/0004-6361/201935628
- **Categories**: **astro-ph.SR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03714v2)
- **Published**: 2019-04-07 19:03:45+00:00
- **Updated**: 2019-05-23 09:42:38+00:00
- **Authors**: A. Asensio Ramos, C. Diaz Baso
- **Comment**: 18 pages, 14 figures, accepted for publication in Astronomy &
  Astrophysics
- **Journal**: None
- **Summary**: Spectropolarimetric inversions are routinely used in the field of Solar Physics for the extraction of physical information from observations. The application to two-dimensional fields of view often requires the use of supercomputers with parallelized inversion codes. Even in this case, the computing time spent on the process is still very large. Our aim is to develop a new inversion code based on the application of convolutional neural networks that can quickly provide a three-dimensional cube of thermodynamical and magnetic properties from the interpretation of two-dimensional maps of Stokes profiles. We train two different architectures of fully convolutional neural networks. To this end, we use the synthetic Stokes profiles obtained from two snapshots of three-dimensional magneto-hydrodynamic numerical simulations of different structures of the solar atmosphere. We provide an extensive analysis of the new inversion technique, showing that it infers the thermodynamical and magnetic properties with a precision comparable to that of standard inversion techniques. However, it provides several key improvements: our method is around one million times faster, it returns a three-dimensional view of the physical properties of the region of interest in geometrical height, it provides quantities that cannot be obtained otherwise (pressure and Wilson depression) and the inferred properties are decontaminated from the blurring effect of instrumental point spread functions for free. The code is provided for free on a specific repository, with options for training and evaluation.



### Deep Localization of Mixed Image Tampering Techniques
- **Arxiv ID**: http://arxiv.org/abs/1904.08484v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.08484v3)
- **Published**: 2019-04-07 19:39:17+00:00
- **Updated**: 2022-01-24 00:00:22+00:00
- **Authors**: Robin Elizabeth Yancey
- **Comment**: None
- **Journal**: None
- **Summary**: With technological advances leading to an increase in mechanisms for image tampering, fraud detection methods must continue to be upgraded to match their sophistication. One problem with current methods is that they require prior knowledge of the method of forgery in order to determine which features to extract from the image to localize the region of interest. When a machine learning algorithm is used to learn different types of tampering from a large set of various image types, with a large enough database we can easily classify which images are tampered. However, we still are left with the question of which features to train on, and how to localize the manipulation. In this work, deep learning for object detection is adapted to tampering detection to solve these two problems, while fusing features from multiple classic techniques for improved accuracy. A Multi-stream version of the Faster RCNN network will be employed with the second stream having an input of the element-wise sum of the ELA and BAG error maps to provide even higher accuracy than a single stream alone.



### Measuring Human Perception to Improve Handwritten Document Transcription
- **Arxiv ID**: http://arxiv.org/abs/1904.03734v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03734v5)
- **Published**: 2019-04-07 20:23:31+00:00
- **Updated**: 2021-06-22 22:00:27+00:00
- **Authors**: Samuel Grieggs, Bingyu Shen, Greta Rauch, Pei Li, Jiaqi Ma, David Chiang, Brian Price, Walter J. Scheirer
- **Comment**: None
- **Journal**: None
- **Summary**: The subtleties of human perception, as measured by vision scientists through the use of psychophysics, are important clues to the internal workings of visual recognition. For instance, measured reaction time can indicate whether a visual stimulus is easy for a subject to recognize, or whether it is hard. In this paper, we consider how to incorporate psychophysical measurements of visual perception into the loss function of a deep neural network being trained for a recognition task, under the assumption that such information can enforce consistency with human behavior. As a case study to assess the viability of this approach, we look at the problem of handwritten document transcription. While good progress has been made towards automatically transcribing modern handwriting, significant challenges remain in transcribing historical documents. Here we describe a general enhancement strategy, underpinned by the new loss formulation, which can be applied to the training regime of any deep learning-based document transcription system. Through experimentation, reliable performance improvement is demonstrated for the standard IAM and RIMES datasets for three different network architectures. Further, we go on to show feasibility for our approach on a new dataset of digitized Latin manuscripts, originally produced by scribes in the Cloister of St. Gall in the the 9th century.



### Surface Defect Classification in Real-Time Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04671v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04671v1)
- **Published**: 2019-04-07 21:22:38+00:00
- **Updated**: 2019-04-07 21:22:38+00:00
- **Authors**: Selim Arikan, Kiran Varanasi, Didier Stricker
- **Comment**: Supplementary material will follow
- **Journal**: None
- **Summary**: Surface inspection systems are an important application domain for computer vision, as they are used for defect detection and classification in the manufacturing industry. Existing systems use hand-crafted features which require extensive domain knowledge to create. Even though Convolutional neural networks (CNNs) have proven successful in many large-scale challenges, industrial inspection systems have yet barely realized their potential due to two significant challenges: real-time processing speed requirements and specialized narrow domain-specific datasets which are sometimes limited in size. In this paper, we propose CNN models that are specifically designed to handle capacity and real-time speed requirements of surface inspection systems. To train and evaluate our network models, we created a surface image dataset containing more than 22000 labeled images with many types of surface materials and achieved 98.0% accuracy in binary defect classification. To solve the class imbalance problem in our datasets, we introduce neural data augmentation methods which are also applicable to similar domains that suffer from the same problem. Our results show that deep learning based methods are feasible to be used in surface inspection systems and outperform traditional methods in accuracy and inference time by considerable margins.



### JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1904.03750v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03750v1)
- **Published**: 2019-04-07 21:47:45+00:00
- **Updated**: 2019-04-07 21:47:45+00:00
- **Authors**: N. Benjamin Erichson, Zhewei Yao, Michael W. Mahoney
- **Comment**: None
- **Journal**: None
- **Summary**: It has been demonstrated that very simple attacks can fool highly-sophisticated neural network architectures. In particular, so-called adversarial examples, constructed from perturbations of input data that are small or imperceptible to humans but lead to different predictions, may lead to an enormous risk in certain critical applications. In light of this, there has been a great deal of work on developing adversarial training strategies to improve model robustness. These training strategies are very expensive, in both human and computational time. To complement these approaches, we propose a very simple and inexpensive strategy which can be used to ``retrofit'' a previously-trained network to improve its resilience to adversarial attacks. More concretely, we propose a new activation function---the JumpReLU---which, when used in place of a ReLU in an already-trained model, leads to a trade-off between predictive accuracy and robustness. This trade-off is controlled by the jump size, a hyper-parameter which can be tuned during the validation stage. Our empirical results demonstrate that this increases model robustness, protecting against adversarial attacks with substantially increased levels of perturbations. This is accomplished simply by retrofitting existing networks with our JumpReLU activation function, without the need for retraining the model. Additionally, we demonstrate that adversarially trained (robust) models can greatly benefit from retrofitting.



### DeepGCNs: Can GCNs Go as Deep as CNNs?
- **Arxiv ID**: http://arxiv.org/abs/1904.03751v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03751v2)
- **Published**: 2019-04-07 21:49:26+00:00
- **Updated**: 2019-08-19 12:53:49+00:00
- **Authors**: Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem
- **Comment**: First two authors contributed equally. Accepted to ICCV'19 as oral
  presentation
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.



### ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact
- **Arxiv ID**: http://arxiv.org/abs/1904.03754v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.03754v3)
- **Published**: 2019-04-07 21:57:17+00:00
- **Updated**: 2019-07-25 05:04:43+00:00
- **Authors**: Samarth Brahmbhatt, Ankur Handa, James Hays, Dieter Fox
- **Comment**: IROS 2019 camera ready version
- **Journal**: None
- **Summary**: Grasping and manipulating objects is an important human skill. Since most objects are designed to be manipulated by human hands, anthropomorphic hands can enable richer human-robot interaction. Desirable grasps are not only stable, but also functional: they enable post-grasp actions with the object. However, functional grasp synthesis for high degree-of-freedom anthropomorphic hands from object shape alone is challenging because of the large optimization space. We present ContactGrasp, a framework for functional grasp synthesis from object shape and contact on the object surface. Contact can be manually specified or obtained through demonstrations. Our contact representation is object-centric and allows functional grasp synthesis even for hand models different than the one used for demonstration. Using a dataset of contact demonstrations from humans grasping diverse household objects, we synthesize functional grasps for three hand models and two functional intents. The project webpage is https://contactdb.cc.gatech.edu/contactgrasp.html.



### Meta-Learning with Differentiable Convex Optimization
- **Arxiv ID**: http://arxiv.org/abs/1904.03758v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03758v2)
- **Published**: 2019-04-07 22:23:42+00:00
- **Updated**: 2019-04-23 17:59:19+00:00
- **Authors**: Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, Stefano Soatto
- **Comment**: Accepted to CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. We propose to use these predictors as base learners to learn representations for few-shot learning and show they offer better tradeoffs between feature size and performance across a range of few-shot recognition benchmarks. Our objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, we exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows us to use high-dimensional embeddings with improved generalization at a modest increase in computational overhead. Our approach, named MetaOptNet, achieves state-of-the-art performance on miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Our code is available at https://github.com/kjunelee/MetaOptNet.



### ANTNets: Mobile Convolutional Neural Networks for Resource Efficient Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.03775v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03775v2)
- **Published**: 2019-04-07 23:43:24+00:00
- **Updated**: 2019-09-04 18:53:34+00:00
- **Authors**: Yunyang Xiong, Hyunwoo J. Kim, Varsha Hedau
- **Comment**: CVPR 2019 Workshop on Efficient Deep Learning for Computer Vision
  (Oral)
- **Journal**: None
- **Summary**: Deep convolutional neural networks have achieved remarkable success in computer vision. However, deep neural networks require large computing resources to achieve high performance. Although depthwise separable convolution can be an efficient module to approximate a standard convolution, it often leads to reduced representational power of networks. In this paper, under budget constraints such as computational cost (MAdds) and the parameter count, we propose a novel basic architectural block, ANTBlock. It boosts the representational power by modeling, in a high dimensional space, interdependency of channels between a depthwise convolution layer and a projection layer in the ANTBlocks. Our experiments show that ANTNet built by a sequence of ANTBlocks, consistently outperforms state-of-the-art low-cost mobile convolutional neural networks across multiple datasets. On CIFAR100, our model achieves 75.7% top-1 accuracy, which is 1.5% higher than MobileNetV2 with 8.3% fewer parameters and 19.6% less computational cost. On ImageNet, our model achieves 72.8% top-1 accuracy, which is 0.8% improvement, with 157.7ms (20% faster) on iPhone 5s over MobileNetV2.



