# Arxiv Papers in cs.CV on 2019-04-23
### Multiview Hessian Regularization for Image Annotation
- **Arxiv ID**: http://arxiv.org/abs/1904.10100v1
- **DOI**: 10.1109/TIP.2013.2255302
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.10100v1)
- **Published**: 2019-04-23 00:08:43+00:00
- **Updated**: 2019-04-23 00:08:43+00:00
- **Authors**: Weifeng Liu, Dacheng Tao
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 22, no. 7, pp. 2676 -
  2687, 2013
- **Summary**: The rapid development of computer hardware and Internet technology makes large scale data dependent models computationally tractable, and opens a bright avenue for annotating images through innovative machine learning algorithms. Semi-supervised learning (SSL) has consequently received intensive attention in recent years and has been successfully deployed in image annotation. One representative work in SSL is Laplacian regularization (LR), which smoothes the conditional distribution for classification along the manifold encoded in the graph Laplacian, however, it has been observed that LR biases the classification function towards a constant function which possibly results in poor generalization. In addition, LR is developed to handle uniformly distributed data (or single view data), although instances or objects, such as images and videos, are usually represented by multiview features, such as color, shape and texture. In this paper, we present multiview Hessian regularization (mHR) to address the above two problems in LR-based image annotation. In particular, mHR optimally combines multiple Hessian regularizations, each of which is obtained from a particular view of instances, and steers the classification function which varies linearly along the data manifold. We apply mHR to kernel least squares and support vector machines as two examples for image annotation. Extensive experiments on the PASCAL VOC'07 dataset validate the effectiveness of mHR by comparing it with baseline algorithms, including LR and HR.



### Learning Actor Relation Graphs for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.10117v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10117v1)
- **Published**: 2019-04-23 01:44:28+00:00
- **Updated**: 2019-04-23 01:44:28+00:00
- **Authors**: Jianchao Wu, Limin Wang, Li Wang, Jie Guo, Gangshan Wu
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Modeling relation between actors is important for recognizing group activity in a multi-person scene. This paper aims at learning discriminative relation between actors efficiently using deep models. To this end, we propose to build a flexible and efficient Actor Relation Graph (ARG) to simultaneously capture the appearance and position relation between actors. Thanks to the Graph Convolutional Network, the connections in ARG could be automatically learned from group activity videos in an end-to-end manner, and the inference on ARG could be efficiently performed with standard matrix operations. Furthermore, in practice, we come up with two variants to sparsify ARG for more effective modeling in videos: spatially localized ARG and temporal randomized ARG. We perform extensive experiments on two standard group activity recognition datasets: the Volleyball dataset and the Collective Activity dataset, where state-of-the-art performance is achieved on both datasets. We also visualize the learned actor graphs and relation features, which demonstrate that the proposed ARG is able to capture the discriminative relation information for group activity recognition.



### Lung Nodule Classification using Deep Local-Global Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.10126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.10126v1)
- **Published**: 2019-04-23 02:49:37+00:00
- **Updated**: 2019-04-23 02:49:37+00:00
- **Authors**: Mundher Al-Shabi, Boon Leong Lan, Wai Yee Chan, Kwan-Hoong Ng, Maxine Tan
- **Comment**: Code and dataset available here
  https://github.com/mundher/local-global
- **Journal**: None
- **Summary**: Purpose: Lung nodules have very diverse shapes and sizes, which makes classifying them as benign/malignant a challenging problem. In this paper, we propose a novel method to predict the malignancy of nodules that have the capability to analyze the shape and size of a nodule using a global feature extractor, as well as the density and structure of the nodule using a local feature extractor. Methods: We propose to use Residual Blocks with a 3x3 kernel size for local feature extraction, and Non-Local Blocks to extract the global features. The Non-Local Block has the ability to extract global features without using a huge number of parameters. The key idea behind the Non-Local Block is to apply matrix multiplications between features on the same feature maps. Results: We trained and validated the proposed method on the LIDC-IDRI dataset which contains 1,018 computed tomography (CT) scans. We followed a rigorous procedure for experimental setup namely, 10-fold cross-validation and ignored the nodules that had been annotated by less than 3 radiologists. The proposed method achieved state-of-the-art results with AUC=95.62%, while significantly outperforming other baseline methods. Conclusions: Our proposed Deep Local-Global network has the capability to accurately extract both local and global features. Our new method outperforms state-of-the-art architecture including Densenet and Resnet with transfer learning.



### Siamese Attentional Keypoint Network for High Performance Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.10128v2
- **DOI**: 10.1016/j.knosys.2019.105448
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.10128v2)
- **Published**: 2019-04-23 03:02:34+00:00
- **Updated**: 2019-12-29 03:03:41+00:00
- **Authors**: Peng Gao, Ruyue Yuan, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang
- **Comment**: Accepted by Knowledge-Based SYSTEMS
- **Journal**: None
- **Summary**: In this paper, we investigate the impacts of three main aspects of visual tracking, i.e., the backbone network, the attentional mechanism, and the detection component, and propose a Siamese Attentional Keypoint Network, dubbed SATIN, for efficient tracking and accurate localization. Firstly, a new Siamese lightweight hourglass network is specially designed for visual tracking. It takes advantage of the benefits of the repeated bottom-up and top-down inference to capture more global and local contextual information at multiple scales. Secondly, a novel cross-attentional module is utilized to leverage both channel-wise and spatial intermediate attentional information, which can enhance both discriminative and localization capabilities of feature maps. Thirdly, a keypoints detection approach is invented to trace any target object by detecting the top-left corner point, the centroid point, and the bottom-right corner point of its bounding box. Therefore, our SATIN tracker not only has a strong capability to learn more effective object representations, but also is computational and memory storage efficiency, either during the training or testing stages. To the best of our knowledge, we are the first to propose this approach. Without bells and whistles, experimental results demonstrate that our approach achieves state-of-the-art performance on several recent benchmark datasets, at a speed far exceeding 27 frames per second.



### Spatio-temporal crop classification of low-resolution satellite imagery with capsule layers and distributed attention
- **Arxiv ID**: http://arxiv.org/abs/1904.10130v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.10130v1)
- **Published**: 2019-04-23 03:05:31+00:00
- **Updated**: 2019-04-23 03:05:31+00:00
- **Authors**: John Brandt
- **Comment**: None
- **Journal**: None
- **Summary**: Land use classification of low resolution spatial imagery is one of the most extensively researched fields in remote sensing. Despite significant advancements in satellite technology, high resolution imagery lacks global coverage and can be prohibitively expensive to procure for extended time periods. Accurately classifying land use change without high resolution imagery offers the potential to monitor vital aspects of global development agenda including climate smart agriculture, drought resistant crops, and sustainable land management. Utilizing a combination of capsule layers and long-short term memory layers with distributed attention, the present paper achieves state-of-the-art accuracy on temporal crop type classification at a 30x30m resolution with Sentinel 2 imagery.



### DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1904.12634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.12634v1)
- **Published**: 2019-04-23 03:33:24+00:00
- **Updated**: 2019-04-23 03:33:24+00:00
- **Authors**: Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang, Sen Li
- **Comment**: Submitted to ITSC 2019
- **Journal**: None
- **Summary**: Driver attention prediction is currently becoming the focus in safe driving research community, such as the DR(eye)VE project and newly emerged Berkeley DeepDrive Attention (BDD-A) database in critical situations. In safe driving, an essential task is to predict the incoming accidents as early as possible. BDD-A was aware of this problem and collected the driver attention in laboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses the critical situations which do not encounter actual accidents, and just faces the driver attention prediction task, without a close step for accident prediction. In contrast to this, we explore the view of drivers' eyes for capturing multiple kinds of accidents, and construct a more diverse and larger video benchmark than ever before with the driver attention and the driving accident annotation simultaneously (named as DADA-2000), which has 2000 video clips owning about 658,476 frames on 54 kinds of accidents. These clips are crowd-sourced and captured in various occasions (highway, urban, rural, and tunnel), weather (sunny, rainy and snowy) and light conditions (daytime and nighttime). For the driver attention representation, we collect the maps of fixations, saccade scan path and focusing time. The accidents are annotated by their categories, the accident window in clips and spatial locations of the crash-objects. Based on the analysis, we obtain a quantitative and positive answer for the question in this paper.



### REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1904.10151v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1904.10151v2)
- **Published**: 2019-04-23 04:45:28+00:00
- **Updated**: 2020-01-06 01:38:43+00:00
- **Authors**: Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton van den Hengel
- **Comment**: None
- **Journal**: None
- **Summary**: One of the long-term challenges of robotics is to enable robots to interact with humans in the visual world via natural language, as humans are visual animals that communicate through language. Overcoming this challenge requires the ability to perform a wide variety of complex tasks in response to multifarious instructions from humans. In the hope that it might drive progress towards more flexible and powerful human interactions with robots, we propose a dataset of varied and complex robot tasks, described in natural language, in terms of objects visible in a large set of real images. Given an instruction, success requires navigating through a previously-unseen environment to identify an object. This represents a practical challenge, but one that closely reflects one of the core visual problems in robotics. Several state-of-the-art vision-and-language navigation, and referring-expression models are tested to verify the difficulty of this new task, but none of them show promising results because there are many fundamental differences between our task and previous ones. A novel Interactive Navigator-Pointer model is also proposed that provides a strong baseline on the task. The proposed model especially achieves the best performance on the unseen test split, but still leaves substantial room for improvement compared to the human performance.



### A Novel Multi-layer Framework for Tiny Obstacle Discovery
- **Arxiv ID**: http://arxiv.org/abs/1904.10161v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.10161v3)
- **Published**: 2019-04-23 05:54:30+00:00
- **Updated**: 2019-08-24 12:28:52+00:00
- **Authors**: Feng Xue, Anlong Ming, Menghan Zhou, Yu Zhou
- **Comment**: Accepted to 2019 International Conference on Robotics and Automation
  (ICRA)
- **Journal**: None
- **Summary**: For tiny obstacle discovery in a monocular image, edge is a fundamental visual element. Nevertheless, because of various reasons, e.g., noise and similar color distribution with background, it is still difficult to detect the edges of tiny obstacles at long distance. In this paper, we propose an obstacle-aware discovery method to recover the missing contours of these obstacles, which helps to obtain obstacle proposals as much as possible. First, by using visual cues in monocular images, several multi-layer regions are elaborately inferred to reveal the distances from the camera. Second, several novel obstacle-aware occlusion edge maps are constructed to well capture the contours of tiny obstacles, which combines cues from each layer. Third, to ensure the existence of the tiny obstacle proposals, the maps from all layers are used for proposals extraction. Finally, based on these proposals containing tiny obstacles, a novel obstacle-aware regressor is proposed to generate an obstacle occupied probability map with high confidence. The convincing experimental results with comparisons on the Lost and Found dataset demonstrate the effectiveness of our approach, achieving around 9.5% improvement on the accuracy than FPHT and PHT, it even gets comparable performance to MergeNet. Moreover, our method outperforms the state-of-the-art algorithms and significantly improves the discovery ability for tiny obstacles at long distance.



### T-SVD Based Non-convex Tensor Completion and Robust Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.10165v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.10165v2)
- **Published**: 2019-04-23 06:09:27+00:00
- **Updated**: 2021-05-25 06:08:52+00:00
- **Authors**: Tao Li, Jinwen Ma
- **Comment**: None
- **Journal**: None
- **Summary**: Tensor completion and robust principal component analysis have been widely used in machine learning while the key problem relies on the minimization of a tensor rank that is very challenging. A common way to tackle this difficulty is to approximate the tensor rank with the $\ell_1-$norm of singular values based on its Tensor Singular Value Decomposition (T-SVD). Besides, the sparsity of a tensor is also measured by its $\ell_1-$norm. However, the $\ell_1$ penalty is essentially biased and thus the result will deviate. In order to sidestep the bias, we propose a novel non-convex tensor rank surrogate function and a novel non-convex sparsity measure. In this new setting by using the concavity instead of the convexity, a majorization minimization algorithm is further designed for tensor completion and robust principal component analysis. Furthermore, we analyze its theoretical properties. Finally, the experiments on natural and hyperspectral images demonstrate the efficacy and efficiency of our proposed method.



### Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More
- **Arxiv ID**: http://arxiv.org/abs/1904.10167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10167v1)
- **Published**: 2019-04-23 06:12:58+00:00
- **Updated**: 2019-04-23 06:12:58+00:00
- **Authors**: Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, Mingli Song
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we investigate a novel deep-model reusing task. Our goal is to train a lightweight and versatile student model, without human-labelled annotations, that amalgamates the knowledge and masters the expertise of two pretrained teacher models working on heterogeneous problems, one on scene parsing and the other on depth estimation. To this end, we propose an innovative training strategy that learns the parameters of the student intertwined with the teachers, achieved by 'projecting' its amalgamated features onto each teacher's domain and computing the loss. We also introduce two options to generalize the proposed training strategy to handle three or more tasks simultaneously. The proposed scheme yields very encouraging results. As demonstrated on several benchmarks, the trained student model achieves results even superior to those of the teachers in their own expertise domains and on par with the state-of-the-art fully supervised models relying on human-labelled annotations.



### High-frequency crowd insights for public safety and congestion control
- **Arxiv ID**: http://arxiv.org/abs/1904.10180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10180v1)
- **Published**: 2019-04-23 07:11:48+00:00
- **Updated**: 2019-04-23 07:11:48+00:00
- **Authors**: Karthik Nandakumar, Sebastien Blandin, Laura Wynter
- **Comment**: None
- **Journal**: None
- **Summary**: We present results from several projects aimed at enabling the real-time understanding of crowds and their behaviour in the built environment. We make use of CCTV video cameras that are ubiquitous throughout the developed and developing world and as such are able to play the role of a reliable sensing mechanism. We outline the novel methods developed for our crowd insights engine, and illustrate examples of its use in different contexts in the urban landscape. Applications of the technology range from maintaining security in public spaces to quantifying the adequacy of public transport level of service.



### 3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs
- **Arxiv ID**: http://arxiv.org/abs/1904.10795v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10795v2)
- **Published**: 2019-04-23 08:18:56+00:00
- **Updated**: 2020-04-06 15:01:51+00:00
- **Authors**: Zeqing Fu, Wei Hu, Zongming Guo
- **Comment**: 7 pages, 5 figures, accepted by IEEE ICME 2020 at 2020.04.03. arXiv
  admin note: text overlap with arXiv:1810.03973
- **Journal**: None
- **Summary**: With the development of 3D laser scanning techniques and depth sensors, 3D dynamic point clouds have attracted increasing attention as a representation of 3D objects in motion, enabling various applications such as 3D immersive tele-presence, gaming and navigation. However, dynamic point clouds usually exhibit holes of missing data, mainly due to the fast motion, the limitation of acquisition and complicated structure. Leveraging on graph signal processing tools, we represent irregular point clouds on graphs and propose a novel inpainting method exploiting both intra-frame self-similarity and inter-frame consistency in 3D dynamic point clouds. Specifically, for each missing region in every frame of the point cloud sequence, we search for its self-similar regions in the current frame and corresponding ones in adjacent frames as references. Then we formulate dynamic point cloud inpainting as an optimization problem based on the two types of references, which is regularized by a graph-signal smoothness prior. Experimental results show the proposed approach outperforms three competing methods significantly, both in objective and subjective quality.



### A Personalized Affective Memory Neural Model for Improving Emotion Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.12632v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.12632v2)
- **Published**: 2019-04-23 09:42:26+00:00
- **Updated**: 2020-05-31 08:42:13+00:00
- **Authors**: Pablo Barros, German I. Parisi, Stefan Wermter
- **Comment**: Accepted by the International Conference on Machine Learning 2019
  (ICML2019)
- **Journal**: in PMLR 97:485-494 (2019)
- **Summary**: Recent models of emotion recognition strongly rely on supervised deep learning solutions for the distinction of general emotion expressions. However, they are not reliable when recognizing online and personalized facial expressions, e.g., for person-specific affective understanding. In this paper, we present a neural model based on a conditional adversarial autoencoder to learn how to represent and edit general emotion expressions. We then propose Grow-When-Required networks as personalized affective memories to learn individualized aspects of emotion expressions. Our model achieves state-of-the-art performance on emotion recognition when evaluated on \textit{in-the-wild} datasets. Furthermore, our experiments include ablation studies and neural visualizations in order to explain the behavior of our model.



### A Large RGB-D Dataset for Semi-supervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.10230v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10230v2)
- **Published**: 2019-04-23 10:02:39+00:00
- **Updated**: 2021-10-22 03:23:24+00:00
- **Authors**: Jaehoon Cho, Dongbo Min, Youngjung Kim, Kwanghoon Sohn
- **Comment**: https://dimlrgbd.github.io/
- **Journal**: None
- **Summary**: Current self-supervised methods for monocular depth estimation are largely based on deeply nested convolutional networks that leverage stereo image pairs or monocular sequences during a training phase. However, they often exhibit inaccurate results around occluded regions and depth boundaries. In this paper, we present a simple yet effective approach for monocular depth estimation using stereo image pairs. The study aims to propose a student-teacher strategy in which a shallow student network is trained with the auxiliary information obtained from a deeper and more accurate teacher network. Specifically, we first train the stereo teacher network by fully utilizing the binocular perception of 3-D geometry and then use the depth predictions of the teacher network to train the student network for monocular depth inference. This enables us to exploit all available depth data from massive unlabeled stereo pairs. We propose a strategy that involves the use of a data ensemble to merge the multiple depth predictions of the teacher network to improve the training samples by collecting non-trivial knowledge beyond a single prediction. To refine the inaccurate depth estimation that is used when training the student network, we further propose stereo confidence-guided regression loss that handles the unreliable pseudo depth values in occlusion, texture-less region, and repetitive pattern. To complement the existing dataset comprising outdoor driving scenes, we built a novel large-scale dataset consisting of one million outdoor stereo images taken using hand-held stereo cameras. Finally, we demonstrate that the monocular depth estimation network provides feature representations that are suitable for high-level vision tasks. The experimental results for various outdoor scenarios demonstrate the effectiveness and flexibility of our approach, which outperforms state-of-the-art approaches.



### A new Edge Detector Based on Parametric Surface Model: Regression Surface Descriptor
- **Arxiv ID**: http://arxiv.org/abs/1904.10235v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10235v1)
- **Published**: 2019-04-23 10:18:25+00:00
- **Updated**: 2019-04-23 10:18:25+00:00
- **Authors**: Rémi Cogranne, Rémi Slysz, Laurence Moreau, Houman Borouchaki
- **Comment**: 21 pages, 13 figures and 2 tables
- **Journal**: None
- **Summary**: In this paper we present a new methodology for edge detection in digital images. The first originality of the proposed method is to consider image content as a parametric surface. Then, an original parametric local model of this surface representing image content is proposed. The few parameters involved in the proposed model are shown to be very sensitive to discontinuities in surface which correspond to edges in image content. This naturally leads to the design of an efficient edge detector. Moreover, a thorough analysis of the proposed model also allows us to explain how these parameters can be used to obtain edge descriptors such as orientations and curvatures.   In practice, the proposed methodology offers two main advantages. First, it has high customization possibilities in order to be adjusted to a wide range of different problems, from coarse to fine scale edge detection. Second, it is very robust to blurring process and additive noise. Numerical results are presented to emphasis these properties and to confirm efficiency of the proposed method through a comparative study with other edge detectors.



### Free-form Video Inpainting with 3D Gated Convolution and Temporal PatchGAN
- **Arxiv ID**: http://arxiv.org/abs/1904.10247v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10247v3)
- **Published**: 2019-04-23 11:14:18+00:00
- **Updated**: 2019-07-23 05:53:03+00:00
- **Authors**: Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, Winston Hsu
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Free-form video inpainting is a very challenging task that could be widely used for video editing such as text removal. Existing patch-based methods could not handle non-repetitive structures such as faces, while directly applying image-based inpainting models to videos will result in temporal inconsistency (see http://bit.ly/2Fu1n6b ). In this paper, we introduce a deep learn-ing based free-form video inpainting model, with proposed 3D gated convolutions to tackle the uncertainty of free-form masks and a novel Temporal PatchGAN loss to enhance temporal consistency. In addition, we collect videos and design a free-form mask generation algorithm to build the free-form video inpainting (FVI) dataset for training and evaluation of video inpainting models. We demonstrate the benefits of these components and experiments on both the FaceForensics and our FVI dataset suggest that our method is superior to existing ones. Related source code, full-resolution result videos and the FVI dataset could be found on Github https://github.com/amjltc295/Free-Form-Video-Inpainting .



### End-to-end Sleep Staging with Raw Single Channel EEG using Deep Residual ConvNets
- **Arxiv ID**: http://arxiv.org/abs/1904.10255v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.10255v1)
- **Published**: 2019-04-23 11:32:46+00:00
- **Updated**: 2019-04-23 11:32:46+00:00
- **Authors**: Ahmed Imtiaz Humayun, Asif Shahriyar Sushmit, Taufiq Hasan, Mohammed Imamul Hassan Bhuiyan
- **Comment**: 5 pages, 3 Figures, Appendix, IEEE BHI 2019
- **Journal**: None
- **Summary**: Humans approximately spend a third of their life sleeping, which makes monitoring sleep an integral part of well-being. In this paper, a 34-layer deep residual ConvNet architecture for end-to-end sleep staging is proposed. The network takes raw single channel electroencephalogram (Fpz-Cz) signal as input and yields hypnogram annotations for each 30s segments as output. Experiments are carried out for two different scoring standards (5 and 6 stage classification) on the expanded PhysioNet Sleep-EDF dataset, which contains multi-source data from hospital and household polysomnography setups. The performance of the proposed network is compared with that of the state-of-the-art algorithms in patient independent validation tasks. The experimental results demonstrate the superiority of the proposed network compared to the best existing method, providing a relative improvement in epoch-wise average accuracy of 6.8% and 6.3% on the household data and multi-source data, respectively. Codes are made publicly available on Github.



### Detecting inter-sectional accuracy differences in driver drowsiness detection algorithms
- **Arxiv ID**: http://arxiv.org/abs/1904.12631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12631v1)
- **Published**: 2019-04-23 11:43:32+00:00
- **Updated**: 2019-04-23 11:43:32+00:00
- **Authors**: Mkhuseli Ngxande, Jule-Raymond Tapamo, Michael Burke
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been used successfully across a broad range of areas including data mining, object detection, and in business. The dominance of CNNs follows a breakthrough by Alex Krizhevsky which showed improvements by dramatically reducing the error rate obtained in a general image classification task from 26.2% to 15.4%. In road safety, CNNs have been applied widely to the detection of traffic signs, obstacle detection, and lane departure checking. In addition, CNNs have been used in data mining systems that monitor driving patterns and recommend rest breaks when appropriate. This paper presents a driver drowsiness detection system and shows that there are potential social challenges regarding the application of these techniques, by highlighting problems in detecting dark-skinned driver's faces. This is a particularly important challenge in African contexts, where there are more dark-skinned drivers. Unfortunately, publicly available datasets are often captured in different cultural contexts, and therefore do not cover all ethnicities, which can lead to false detections or racially biased models. This work evaluates the performance obtained when training convolutional neural network models on commonly used driver drowsiness detection datasets and testing on datasets specifically chosen for broader representation. Results show that models trained using publicly available datasets suffer extensively from over-fitting, and can exhibit racial bias, as shown by testing on a more representative dataset. We propose a novel visualisation technique that can assist in identifying groups of people where there might be the potential of discrimination, using Principal Component Analysis (PCA) to produce a grid of faces sorted by similarity, and combining these with a model accuracy overlay.



### Improving benchmarks for autonomous vehicles testing using synthetically generated images
- **Arxiv ID**: http://arxiv.org/abs/1904.10261v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML, 65K10
- **Links**: [PDF](http://arxiv.org/pdf/1904.10261v1)
- **Published**: 2019-04-23 11:59:36+00:00
- **Updated**: 2019-04-23 11:59:36+00:00
- **Authors**: Aleksander Lukashou
- **Comment**: 4 pages, 38 figures
- **Journal**: None
- **Summary**: Nowadays autonomous technologies are a very heavily explored area and particularly computer vision as the main component of vehicle perception. The quality of the whole vision system based on neural networks relies on the dataset it was trained on. It is extremely difficult to find traffic sign datasets from most of the counties of the world. Meaning autonomous vehicle from the USA will not be able to drive though Lithuania recognizing all road signs on the way. In this paper, we propose a solution on how to update model using a small dataset from the country vehicle will be used in. It is important to mention that is not panacea, rather small upgrade which can boost autonomous car development in countries with limited data access. We achieved about 10 percent quality raise and expect even better results during future experiments.



### Attention-guided Network for Ghost-free High Dynamic Range Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.10293v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10293v1)
- **Published**: 2019-04-23 13:04:58+00:00
- **Updated**: 2019-04-23 13:04:58+00:00
- **Authors**: Qingsen Yan, Dong Gong, Qinfeng Shi, Anton van den Hengel, Chunhua Shen, Ian Reid, Yanning Zhang
- **Comment**: Accepted to appear at CVPR 2019
- **Journal**: None
- **Summary**: Ghosting artifacts caused by moving objects or misalignments is a key challenge in high dynamic range (HDR) imaging for dynamic scenes. Previous methods first register the input low dynamic range (LDR) images using optical flow before merging them, which are error-prone and cause ghosts in results. A very recent work tries to bypass optical flows via a deep network with skip-connections, however, which still suffers from ghosting artifacts for severe movement. To avoid the ghosting from the source, we propose a novel attention-guided end-to-end deep neural network (AHDRNet) to produce high-quality ghost-free HDR images. Unlike previous methods directly stacking the LDR images or features for merging, we use attention modules to guide the merging according to the reference image. The attention modules automatically suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. In addition to the attention model, we use dilated residual dense block (DRDB) to make full use of the hierarchical features and increase the receptive field for hallucinating the missing details. The proposed AHDRNet is a non-flow-based method, which can also avoid the artifacts generated by optical-flow estimation error. Experiments on different datasets show that the proposed AHDRNet can achieve state-of-the-art quantitative and qualitative results.



### Transferable Semi-supervised 3D Object Detection from RGB-D Data
- **Arxiv ID**: http://arxiv.org/abs/1904.10300v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10300v1)
- **Published**: 2019-04-23 13:24:15+00:00
- **Updated**: 2019-04-23 13:24:15+00:00
- **Authors**: Yew Siang Tang, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.



### VITAMIN-E: VIsual Tracking And MappINg with Extremely Dense Feature Points
- **Arxiv ID**: http://arxiv.org/abs/1904.10324v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10324v2)
- **Published**: 2019-04-23 13:39:10+00:00
- **Updated**: 2019-12-16 09:09:57+00:00
- **Authors**: Masashi Yokozuka, Shuji Oishi, Thompson Simon, Atsuhiko Banno
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel indirect monocular SLAM algorithm called "VITAMIN-E," which is highly accurate and robust as a result of tracking extremely dense feature points. Typical indirect methods have difficulty in reconstructing dense geometry because of their careful feature point selection for accurate matching. Unlike conventional methods, the proposed method processes an enormous number of feature points by tracking the local extrema of curvature informed by dominant flow estimation. Because this may lead to high computational cost during bundle adjustment, we propose a novel optimization technique, the "subspace Gauss--Newton method", that significantly improves the computational efficiency of bundle adjustment by partially updating the variables. We concurrently generate meshes from the reconstructed points and merge them for an entire 3D model. The experimental results on the SLAM benchmark dataset EuRoC demonstrated that the proposed method outperformed state-of-the-art SLAM methods, such as DSO, ORB-SLAM, and LSD-SLAM, both in terms of accuracy and robustness in trajectory estimation. The proposed method simultaneously generated significantly detailed 3D geometry from the dense feature points in real time using only a CPU.



### Privacy Preserving Group Membership Verification and Identification
- **Arxiv ID**: http://arxiv.org/abs/1904.10327v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10327v1)
- **Published**: 2019-04-23 13:41:14+00:00
- **Updated**: 2019-04-23 13:41:14+00:00
- **Authors**: Marzieh Gheisari, Teddy Furon, Laurent Amsaleg
- **Comment**: Accepted at CVPR Workshops 2019
- **Journal**: None
- **Summary**: When convoking privacy, group membership verification checks if a biometric trait corresponds to one member of a group without revealing the identity of that member. Similarly, group membership identification states which group the individual belongs to, without knowing his/her identity. A recent contribution provides privacy and security for group membership protocols through the joint use of two mechanisms: quantizing biometric templates into discrete embeddings and aggregating several templates into one group representation. This paper significantly improves that contribution because it jointly learns how to embed and aggregate instead of imposing fixed and hard coded rules. This is demonstrated by exposing the mathematical underpinnings of the learning stage before showing the improvements through an extensive series of experiments targeting face recognition. Overall, experiments show that learning yields an excellent trade-off between security /privacy and verification /identification performances.



### Path-Restore: Learning Network Path Selection for Image Restoration
- **Arxiv ID**: http://arxiv.org/abs/1904.10343v2
- **DOI**: 10.1109/TPAMI.2021.3096255
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10343v2)
- **Published**: 2019-04-23 14:07:11+00:00
- **Updated**: 2021-07-27 04:05:42+00:00
- **Authors**: Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, Chen Change Loy
- **Comment**: IEEE TPAMI 2021. Project page:
  https://www.mmlab-ntu.com/project/pathrestore/
- **Journal**: None
- **Summary**: Very deep Convolutional Neural Networks (CNNs) have greatly improved the performance on various image restoration tasks. However, this comes at a price of increasing computational burden, hence limiting their practical usages. We observe that some corrupted image regions are inherently easier to restore than others since the distortion and content vary within an image. To leverage this, we propose Path-Restore, a multi-path CNN with a pathfinder that can dynamically select an appropriate route for each image region. We train the pathfinder using reinforcement learning with a difficulty-regulated reward. This reward is related to the performance, complexity and "the difficulty of restoring a region". A policy mask is further investigated to jointly process all the image regions. We conduct experiments on denoising and mixed restoration tasks. The results show that our method achieves comparable or superior performance to existing approaches with less computational cost. In particular, Path-Restore is effective for real-world denoising, where the noise distribution varies across different regions on a single image. Compared to the state-of-the-art RIDNet, our method achieves comparable performance and runs 2.7x faster on the realistic Darmstadt Noise Dataset.



### Monte-Carlo Tree Search for Efficient Visually Guided Rearrangement Planning
- **Arxiv ID**: http://arxiv.org/abs/1904.10348v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.10348v2)
- **Published**: 2019-04-23 14:15:37+00:00
- **Updated**: 2020-04-01 16:11:27+00:00
- **Authors**: Yann Labbé, Sergey Zagoruyko, Igor Kalevatykh, Ivan Laptev, Justin Carpentier, Mathieu Aubry, Josef Sivic
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)
- **Journal**: None
- **Summary**: We address the problem of visually guided rearrangement planning with many movable objects, i.e., finding a sequence of actions to move a set of objects from an initial arrangement to a desired one, while relying on visual inputs coming from an RGB camera. To do so, we introduce a complete pipeline relying on two key contributions. First, we introduce an efficient and scalable rearrangement planning method, based on a Monte-Carlo Tree Search exploration strategy. We demonstrate that because of its good trade-off between exploration and exploitation our method (i) scales well with the number of objects while (ii) finding solutions which require a smaller number of moves compared to the other state-of-the-art approaches. Note that on the contrary to many approaches, we do not require any buffer space to be available. Second, to precisely localize movable objects in the scene, we develop an integrated approach for robust multi-object workspace state estimation from a single uncalibrated RGB camera using a deep neural network trained only with synthetic data. We validate our multi-object visually guided manipulation pipeline with several experiments on a real UR-5 robotic arm by solving various rearrangement planning instances, requiring only 60 ms to compute the plan to rearrange 25 objects. In addition, we show that our system is insensitive to camera movements and can successfully recover from external perturbations. Supplementary video, source code and pre-trained models are available at https://ylabbe.github.io/rearrangement-planning.



### Drishtikon: An advanced navigational aid system for visually impaired people
- **Arxiv ID**: http://arxiv.org/abs/1904.10351v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, 97R40 97R40
- **Links**: [PDF](http://arxiv.org/pdf/1904.10351v1)
- **Published**: 2019-04-23 14:23:49+00:00
- **Updated**: 2019-04-23 14:23:49+00:00
- **Authors**: Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna Udutalapally
- **Comment**: Pre-print of the presented article at IEEE Conference on Information
  and Communication Technology (CICT-2018), 6 Pages, 7 Figures, 3 Tables
- **Journal**: None
- **Summary**: Today, many of the aid systems deployed for visually impaired people are mostly made for a single purpose. Be it navigation, object detection, or distance perceiving. Also, most of the deployed aid systems use indoor navigation which requires a pre-knowledge of the environment. These aid systems often fail to help visually impaired people in the unfamiliar scenario. In this paper, we propose an aid system developed using object detection and depth perceivement to navigate a person without dashing into an object. The prototype developed detects 90 different types of objects and compute their distances from the user. We also, implemented a navigation feature to get input from the user about the target destination and hence, navigate the impaired person to his/her destination using Google Directions API. With this system, we built a multi-feature, high accuracy navigational aid system which can be deployed in the wild and help the visually impaired people in their daily life by navigating them effortlessly to their desired destination.



### HAUAR: Home Automation Using Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.10354v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, 97R40
- **Links**: [PDF](http://arxiv.org/pdf/1904.10354v2)
- **Published**: 2019-04-23 14:26:18+00:00
- **Updated**: 2019-04-26 04:50:47+00:00
- **Authors**: Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna Udutalapally
- **Comment**: Pre-print of the presented article at IEEE Conference on Information
  and Communication Technology (CICT-2018)
- **Journal**: None
- **Summary**: Today, many of the home automation systems deployed are mostly controlled by humans. This control by humans restricts the automation of home appliances to an extent. Also, most of the deployed home automation systems use the Internet of Things technology to control the appliances. In this paper, we propose a system developed using action recognition to fully automate the home appliances. We recognize the three actions of a person (sitting, standing and lying) along with the recognition of an empty room. The accuracy of the system was 90% in the real-life test experiments. With this system, we remove the human intervention in home automation systems for controlling the home appliances and at the same time we ensure the data privacy and reduce the energy consumption by efficiently and optimally using home appliances.



### Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using Parametric Level Set Methods
- **Arxiv ID**: http://arxiv.org/abs/1904.10379v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1904.10379v2)
- **Published**: 2019-04-23 15:19:39+00:00
- **Updated**: 2019-12-20 12:52:17+00:00
- **Authors**: Moshe Eliasof, Andrei Sharf, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of 3D shape reconstruction from multi-modal data, given uncertain calibration parameters. Typically, 3D data modalities can be in diverse forms such as sparse point sets, volumetric slices, 2D photos and so on. To jointly process these data modalities, we exploit a parametric level set method that utilizes ellipsoidal radial basis functions. This method not only allows us to analytically and compactly represent the object, it also confers on us the ability to overcome calibration related noise that originates from inaccurate acquisition parameters. This essentially implicit regularization leads to a highly robust and scalable reconstruction, surpassing other traditional methods. In our results we first demonstrate the ability of the method to compactly represent complex objects. We then show that our reconstruction method is robust both to a small number of measurements and to noise in the acquisition parameters. Finally, we demonstrate our reconstruction abilities from diverse modalities such as volume slices obtained from liquid displacement (similar to CTscans and XRays), and visual measurements obtained from shape silhouettes.



### Minimizing Perceived Image Quality Loss Through Adversarial Attack Scoping
- **Arxiv ID**: http://arxiv.org/abs/1904.10390v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1904.10390v1)
- **Published**: 2019-04-23 15:42:00+00:00
- **Updated**: 2019-04-23 15:42:00+00:00
- **Authors**: Kostiantyn Khabarlak, Larysa Koriashkina
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are now actively being used for computer vision tasks in security critical areas such as robotics, face recognition, autonomous vehicles yet their safety is under question after the discovery of adversarial attacks. In this paper we develop simplified adversarial attack algorithms based on a scoping idea, which enables execution of fast adversarial attacks that minimize structural image quality (SSIM) loss, allows performing efficient transfer attacks with low target inference network call count and opens a possibility of an attack using pen-only drawings on a paper for the MNIST handwritten digit dataset. The presented adversarial attack analysis and the idea of attack scoping can be easily expanded to different datasets, thus making the paper's results applicable to a wide range of practical tasks.



### Corticospinal Tract (CST) reconstruction based on fiber orientation distributions(FODs) tractography
- **Arxiv ID**: http://arxiv.org/abs/1904.11136v1
- **DOI**: 10.1109/BIBE.2018.00066
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.11136v1)
- **Published**: 2019-04-23 16:19:06+00:00
- **Updated**: 2019-04-23 16:19:06+00:00
- **Authors**: Youshan Zhang
- **Comment**: None
- **Journal**: 2018 IEEE 18th International Conference on Bioinformatics and
  Bioengineering (BIBE), Taichung, 2018, pp. 305-310
- **Summary**: The Corticospinal Tract (CST) is a part of pyramidal tract (PT), and it can innervate the voluntary movement of skeletal muscle through spinal interneurons (the 4th layer of the Rexed gray board layers), and anterior horn motorneurons (which control trunk and proximal limb muscles). Spinal cord injury (SCI) is a highly disabling disease often caused by traffic accidents. The recovery of CST and the functional reconstruction of spinal anterior horn motor neurons play an essential role in the treatment of SCI. However, the localization and reconstruction of CST are still challenging issues; the accuracy of the geometric reconstruction can directly affect the results of the surgery. The main contribution of this paper is the reconstruction of the CST based on the fiber orientation distributions (FODs) tractography. Differing from tensor-based tractography in which the primary direction is a determined orientation, the direction of FODs tractography is determined by the probability. The spherical harmonics (SPHARM) can be used to approximate the efficiency of FODs tractography. We manually delineate the three ROIs (the posterior limb of the internal capsule, the cerebral peduncle, and the anterior pontine area) by the ITK-SNAP software, and use the pipeline software to reconstruct both the left and right sides of the CST fibers. Our results demonstrate that FOD-based tractography can show more and correct anatomical CST fiber bundles.



### BIT: Biologically Inspired Tracker
- **Arxiv ID**: http://arxiv.org/abs/1904.10411v1
- **DOI**: 10.1109/TIP.2016.2520358
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10411v1)
- **Published**: 2019-04-23 16:32:24+00:00
- **Updated**: 2019-04-23 16:32:24+00:00
- **Authors**: Bolun Cai, Xiangmin Xu, Xiaofen Xing, Kui Jia, Jie Miao, Dacheng Tao
- **Comment**: None
- **Journal**: IEEE Trans. on Image Proc. 25 (2016) 1327-1339
- **Summary**: Visual tracking is challenging due to image variations caused by various factors, such as object deformation, scale change, illumination change and occlusion. Given the superior tracking performance of human visual system (HVS), an ideal design of biologically inspired model is expected to improve computer visual tracking. This is however a difficult task due to the incomplete understanding of neurons' working mechanism in HVS. This paper aims to address this challenge based on the analysis of visual cognitive mechanism of the ventral stream in the visual cortex, which simulates shallow neurons (S1 units and C1 units) to extract low-level biologically inspired features for the target appearance and imitates an advanced learning mechanism (S2 units and C2 units) to combine generative and discriminative models for target location. In addition, fast Gabor approximation (FGA) and fast Fourier transform (FFT) are adopted for real-time learning and detection in this framework. Extensive experiments on large-scale benchmark datasets show that the proposed biologically inspired tracker performs favorably against state-of-the-art methods in terms of efficiency, accuracy, and robustness. The acceleration technique in particular ensures that BIT maintains a speed of approximately 45 frames per second.



### Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting
- **Arxiv ID**: http://arxiv.org/abs/1904.10424v4
- **DOI**: 10.1007/978-3-030-58621-8_27
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10424v4)
- **Published**: 2019-04-23 17:03:13+00:00
- **Updated**: 2020-07-20 07:36:28+00:00
- **Authors**: Shengcai Liao, Ling Shao
- **Comment**: This is the ECCV 2020 version, including the appendix
- **Journal**: Vedaldi A., Bischof H., Brox T., Frahm JM. (eds). European
  Conference on Computer Vision. ECCV 2020. Lecture Notes in Computer Science,
  vol 12356. Springer, Cham
- **Summary**: For person re-identification, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identification. Code is available at https://github.com/ShengcaiLiao/QAConv.



### DenseNet Models for Tiny ImageNet Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.10429v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.10429v2)
- **Published**: 2019-04-23 17:20:35+00:00
- **Updated**: 2020-06-01 11:40:40+00:00
- **Authors**: Zoheb Abai, Nishad Rajmalwar
- **Comment**: 7 pages, 8 figures, 2 tables
- **Journal**: None
- **Summary**: In this paper, we present two image classification models on the Tiny ImageNet dataset. We built two very different networks from scratch based on the idea of Densely Connected Convolution Networks. The architecture of the networks is designed based on the image resolution of this specific dataset and by calculating the Receptive Field of the convolution layers. We also used some non-conventional techniques related to image augmentation and Cyclical Learning Rate to improve the accuracy of our models. The networks are trained under high constraints and low computation resources. We aimed to achieve top-1 validation accuracy of 60%; the results and error analysis are also presented.



### Chunkflow: Distributed Hybrid Cloud Processing of Large 3D Images by Convolutional Nets
- **Arxiv ID**: http://arxiv.org/abs/1904.10489v3
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1904.10489v3)
- **Published**: 2019-04-23 18:47:57+00:00
- **Updated**: 2019-05-02 13:48:34+00:00
- **Authors**: Jingpeng Wu, William M. Silversmith, Kisuk Lee, H. Sebastian Seung
- **Comment**: None
- **Journal**: None
- **Summary**: It is now common to process volumetric biomedical images using 3D Convolutional Networks (ConvNets). This can be challenging for the teravoxel and even petavoxel images that are being acquired today by light or electron microscopy. Here we introduce chunkflow, a software framework for distributing ConvNet processing over local and cloud GPUs and CPUs. The image volume is divided into overlapping chunks, each chunk is processed by a ConvNet, and the results are blended together to yield the output image. The frontend submits ConvNet tasks to a cloud queue. The tasks are executed by local and cloud GPUs and CPUs. Thanks to the fault-tolerant architecture of Chunkflow, cost can be greatly reduced by utilizing cheap unstable cloud instances. Chunkflow currently supports PyTorch for GPUs and PZnet for CPUs. To illustrate its usage, a large 3D brain image from serial section electron microscopy was processed by a 3D ConvNet with a U-Net style architecture. Chunkflow provides some chunk operations for general use, and the operations can be composed flexibly in a command line interface.



### Comparing Samples from the $\mathcal{G}^0$ Distribution using a Geodesic Distance
- **Arxiv ID**: http://arxiv.org/abs/1904.10499v1
- **DOI**: None
- **Categories**: **stat.ME**, cs.CV, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1904.10499v1)
- **Published**: 2019-04-23 19:13:29+00:00
- **Updated**: 2019-04-23 19:13:29+00:00
- **Authors**: Alejandro C. Frery, Juliana Gambini
- **Comment**: None
- **Journal**: None
- **Summary**: The $\mathcal{G}^0$ distribution is widely used for monopolarized SAR image modeling because it can characterize regions with different degree of texture accurately. It is indexed by three parameters: the number of looks (which can be estimated for the whole image), a scale parameter and a texture parameter. This paper presents a new proposal for comparing samples from the $\mathcal{G}^0$ distribution using a Geodesic Distance (GD) as a measure of dissimilarity between models. The objective is quantifying the difference between pairs of samples from SAR data using both local parameters (scale and texture) of the $\mathcal{G}^0$ distribution. We propose three tests based on the GD which combine the tests presented in~\cite{GeodesicDistanceGI0JSTARS}, and we estimate their probability distributions using permutation methods.



