# Arxiv Papers in cs.CV on 2019-04-12
### An Introduction to Person Re-identification with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.05992v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.05992v2)
- **Published**: 2019-04-12 00:33:05+00:00
- **Updated**: 2019-04-17 23:27:48+00:00
- **Authors**: Hamed Alqahtani, Manolya Kavakli-Thorne, Charles Z. Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification is a basic subject in the field of computer vision. The traditional methods have several limitations in solving the problems of person illumination like occlusion, pose variation and feature variation under complex background. Fortunately, deep learning paradigm opens new ways of the person re-identification research and becomes a hot spot in this field. Generative Adversarial Nets (GANs) in the past few years attracted lots of attention in solving these problems. This paper reviews the GAN based methods for person re-identification focuses on the related papers about different GAN based frameworks and discusses their advantages and disadvantages. Finally, it proposes the direction of future research, especially the prospect of person re-identification methods based on GANs.



### A New Loss Function for CNN Classifier Based on Pre-defined Evenly-Distributed Class Centroids
- **Arxiv ID**: http://arxiv.org/abs/1904.06008v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06008v2)
- **Published**: 2019-04-12 02:19:45+00:00
- **Updated**: 2019-04-26 11:50:59+00:00
- **Authors**: Qiuyu Zhu, Pengju Zhang, Xin Ye
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: With the development of convolutional neural networks (CNNs) in recent years, the network structure has become more and more complex and varied, and has achieved very good results in pattern recognition, image classification, object detection and tracking. For CNNs used for image classification, in addition to the network structure, more and more research is now focusing on the improvement of the loss function, so as to enlarge the inter-class feature differences, and reduce the intra-class feature variations as soon as possible. Besides the traditional Softmax, typical loss functions include L-Softmax, AM-Softmax, ArcFace, and Center loss, etc. Based on the concept of predefined evenly-distributed class centroids (PEDCC) in CSAE network, this paper proposes a PEDCC-based loss function called PEDCC-Loss, which can make the inter-class distance maximal and intra-class distance small enough in hidden feature space. Multiple experiments on image classification and face recognition have proved that our method achieve the best recognition accuracy, and network training is stable and easy to converge. Code is available in https://github.com/ZLeopard/PEDCC-Loss



### Real-Time Dense Stereo Embedded in A UAV for Road Inspection
- **Arxiv ID**: http://arxiv.org/abs/1904.06017v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.06017v1)
- **Published**: 2019-04-12 02:45:32+00:00
- **Updated**: 2019-04-12 02:45:32+00:00
- **Authors**: Rui Fan, Jianhao Jiao, Jie Pan, Huaiyang Huang, Shaojie Shen, Ming Liu
- **Comment**: 9 pages, 8 figures, In Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR) Workshops, June 16-20, 2019, Long
  Beach, USA
- **Journal**: None
- **Summary**: The condition assessment of road surfaces is essential to ensure their serviceability while still providing maximum road traffic safety. This paper presents a robust stereo vision system embedded in an unmanned aerial vehicle (UAV). The perspective view of the target image is first transformed into the reference view, and this not only improves the disparity accuracy, but also reduces the algorithm's computational complexity. The cost volumes generated from stereo matching are then filtered using a bilateral filter. The latter has been proved to be a feasible solution for the functional minimisation problem in a fully connected Markov random field model. Finally, the disparity maps are transformed by minimising an energy function with respect to the roll angle and disparity projection model. This makes the damaged road areas more distinguishable from the road surface. The proposed system is implemented on an NVIDIA Jetson TX2 GPU with CUDA for real-time purposes. It is demonstrated through experiments that the damaged road areas can be easily distinguished from the transformed disparity maps.



### Tensor Sparse PCA and Face Recognition: A Novel Approach
- **Arxiv ID**: http://arxiv.org/abs/1904.08496v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1904.08496v4)
- **Published**: 2019-04-12 03:43:57+00:00
- **Updated**: 2020-08-11 08:08:48+00:00
- **Authors**: Loc Hoang Tran, Linh Hoang Tran
- **Comment**: It has some errors in the experimental section
- **Journal**: None
- **Summary**: Face recognition is the important field in machine learning and pattern recognition research area. It has a lot of applications in military, finance, public security, to name a few. In this paper, the combination of the tensor sparse PCA with the nearest-neighbor method (and with the kernel ridge regression method) will be proposed and applied to the face dataset. Experimental results show that the combination of the tensor sparse PCA with any classification system does not always reach the best accuracy performance measures. However, the accuracy of the combination of the sparse PCA method and one specific classification system is always better than the accuracy of the combination of the PCA method and one specific classification system and is always better than the accuracy of the classification system itself.



### A Light Dual-Task Neural Network for Haze Removal
- **Arxiv ID**: http://arxiv.org/abs/1904.06024v1
- **DOI**: 10.1109/LSP.2018.2849681
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06024v1)
- **Published**: 2019-04-12 03:51:56+00:00
- **Updated**: 2019-04-12 03:51:56+00:00
- **Authors**: Yu Zhang, Xinchao Wang, Xiaojun Bi, Dacheng Tao
- **Comment**: 6 pages, 4 figures
- **Journal**: IEEE Signal Processing Letters, 2018, 25(8): 1231-1235
- **Summary**: Single-image dehazing is a challenging problem due to its ill-posed nature. Existing methods rely on a suboptimal two-step approach, where an intermediate product like a depth map is estimated, based on which the haze-free image is subsequently generated using an artificial prior formula. In this paper, we propose a light dual-task Neural Network called LDTNet that restores the haze-free image in one shot. We use transmission map estimation as an auxiliary task to assist the main task, haze removal, in feature extraction and to enhance the generalization of the network. In LDTNet, the haze-free image and the transmission map are produced simultaneously. As a result, the artificial prior is reduced to the smallest extent. Extensive experiments demonstrate that our algorithm achieves superior performance against the state-of-the-art methods on both synthetic and real-world images.



### Cycle-Consistent Adversarial GAN: the integration of adversarial attack and defense
- **Arxiv ID**: http://arxiv.org/abs/1904.06026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06026v1)
- **Published**: 2019-04-12 04:03:42+00:00
- **Updated**: 2019-04-12 04:03:42+00:00
- **Authors**: Lingyun Jiang, Kai Qiao, Ruoxi Qin, Linyuan Wang, Jian Chen, Haibing Bu, Bin Yan
- **Comment**: 13 pages,7 tables, 1 figure
- **Journal**: None
- **Summary**: In image classification of deep learning, adversarial examples where inputs intended to add small magnitude perturbations may mislead deep neural networks (DNNs) to incorrect results, which means DNNs are vulnerable to them. Different attack and defense strategies have been proposed to better research the mechanism of deep learning. However, those research in these networks are only for one aspect, either an attack or a defense, not considering that attacks and defenses should be interdependent and mutually reinforcing, just like the relationship between spears and shields. In this paper, we propose Cycle-Consistent Adversarial GAN (CycleAdvGAN) to generate adversarial examples, which can learn and approximate the distribution of original instances and adversarial examples. For CycleAdvGAN, once the Generator and are trained, can generate adversarial perturbations efficiently for any instance, so as to make DNNs predict wrong, and recovery adversarial examples to clean instances, so as to make DNNs predict correct. We apply CycleAdvGAN under semi-white box and black-box settings on two public datasets MNIST and CIFAR10. Using the extensive experiments, we show that our method has achieved the state-of-the-art adversarial attack method and also efficiently improve the defense ability, which make the integration of adversarial attack and defense come true. In additional, it has improved attack effect only trained on the adversarial dataset generated by any kind of adversarial attack.



### EvalNorm: Estimating Batch Normalization Statistics for Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1904.06031v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06031v2)
- **Published**: 2019-04-12 04:54:56+00:00
- **Updated**: 2019-08-13 22:17:03+00:00
- **Authors**: Saurabh Singh, Abhinav Shrivastava
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Batch normalization (BN) has been very effective for deep learning and is widely used. However, when training with small minibatches, models using BN exhibit a significant degradation in performance. In this paper we study this peculiar behavior of BN to gain a better understanding of the problem, and identify a cause. We propose 'EvalNorm' to address the issue by estimating corrected normalization statistics to use for BN during evaluation. EvalNorm supports online estimation of the corrected statistics while the model is being trained, and does not affect the training scheme of the model. As a result, EvalNorm can also be used with existing pre-trained models allowing them to benefit from our method. EvalNorm yields large gains for models trained with smaller batches. Our experiments show that EvalNorm performs 6.18% (absolute) better than vanilla BN for a batchsize of 2 on ImageNet validation set and from 1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across a variety of setups.



### Evaluating the Representational Hub of Language and Vision Models
- **Arxiv ID**: http://arxiv.org/abs/1904.06038v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.06038v1)
- **Published**: 2019-04-12 05:18:35+00:00
- **Updated**: 2019-04-12 05:18:35+00:00
- **Authors**: Ravi Shekhar, Ece Takmaz, Raquel Fernández, Raffaella Bernardi
- **Comment**: Accepted to IWCS 2019
- **Journal**: None
- **Summary**: The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the `Hub and Spoke' architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing diagnostic task designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.



### Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology
- **Arxiv ID**: http://arxiv.org/abs/1904.06040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06040v1)
- **Published**: 2019-04-12 05:32:33+00:00
- **Updated**: 2019-04-12 05:32:33+00:00
- **Authors**: Hiroki Tokunaga, Yuki Teramoto, Akihiko Yoshizawa, Ryoma Bise
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Automated digital histopathology image segmentation is an important task to help pathologists diagnose tumors and cancer subtypes. For pathological diagnosis of cancer subtypes, pathologists usually change the magnification of whole-slide images (WSI) viewers. A key assumption is that the importance of the magnifications depends on the characteristics of the input image, such as cancer subtypes. In this paper, we propose a novel semantic segmentation method, called Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), that can adaptively use image features from images with different magnifications to segment multiple cancer subtype regions in the input image. The proposed method aggregates several expert CNNs for images of different magnifications by adaptively changing the weight of each expert depending on the input image. It leverages information in the images with different magnifications that might be useful for identifying the subtypes. It outperformed other state-of-the-art methods in experiments.



### Unsupervised Method to Localize Masses in Mammograms
- **Arxiv ID**: http://arxiv.org/abs/1904.06044v1
- **DOI**: 10.1109/access.2021.3094768
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06044v1)
- **Published**: 2019-04-12 05:50:51+00:00
- **Updated**: 2019-04-12 05:50:51+00:00
- **Authors**: Bilal Ahmed Lodhi
- **Comment**: None
- **Journal**: IEEE Access, vol. 9, pp. 99327-99338, 2021
- **Summary**: Breast cancer is one of the most common and prevalent type of cancer that mainly affects the women population. chances of effective treatment increases with early diagnosis. Mammography is considered one of the effective and proven techniques for early diagnosis of breast cancer. Tissues around masses look identical in mammogram, which makes automatic detection process a very challenging task. They are indistinguishable from the surrounding parenchyma. In this paper, we present an efficient and automated approach to segment masses in mammograms. The proposed method uses hierarchical clustering to isolate the salient area, and then features are extracted to reject false detection. We applied our method on two popular publicly available datasets (mini-MIAS and DDSM). A total of 56 images from mini-mias database, and 76 images from DDSM were randomly selected. Results are explained in-terms of ROC (Receiver Operating Characteristics) curves and compared with the other techniques. Experimental results demonstrate the efficiency and advantages of the proposed system in automatic mass identification in mammograms.



### Unifying Heterogeneous Classifiers with Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.06062v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06062v1)
- **Published**: 2019-04-12 06:51:41+00:00
- **Updated**: 2019-04-12 06:51:41+00:00
- **Authors**: Jayakorn Vongkulbhisal, Phongtharin Vinayavekhin, Marco Visentini-Scarzanella
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we study the problem of unifying knowledge from a set of classifiers with different architectures and target classes into a single classifier, given only a generic set of unlabelled data. We call this problem Unifying Heterogeneous Classifiers (UHC). This problem is motivated by scenarios where data is collected from multiple sources, but the sources cannot share their data, e.g., due to privacy concerns, and only privately trained models can be shared. In addition, each source may not be able to gather data to train all classes due to data availability at each source, and may not be able to train the same classification model due to different computational resources. To tackle this problem, we propose a generalisation of knowledge distillation to merge HCs. We derive a probabilistic relation between the outputs of HCs and the probability over all classes. Based on this relation, we propose two classes of methods based on cross-entropy minimisation and matrix factorisation, which allow us to estimate soft labels over all classes from unlabelled samples and use them in lieu of ground truth labels to train a unified classifier. Our extensive experiments on ImageNet, LSUN, and Places365 datasets show that our approaches significantly outperform a naive extension of distillation and can achieve almost the same accuracy as classifiers that are trained in a centralised, supervised manner.



### Multi-View Region Adaptive Multi-temporal DMM and RGB Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.06074v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06074v1)
- **Published**: 2019-04-12 07:23:30+00:00
- **Updated**: 2019-04-12 07:23:30+00:00
- **Authors**: Mahmoud Al-Faris, John P. Chiverton, Yanyan Yang, David L. Ndzi
- **Comment**: 14 pages, 6 figures, 13 tables. Submitted
- **Journal**: None
- **Summary**: Human action recognition remains an important yet challenging task. This work proposes a novel action recognition system. It uses a novel Multiple View Region Adaptive Multi-resolution in time Depth Motion Map (MV-RAMDMM) formulation combined with appearance information. Multiple stream 3D Convolutional Neural Networks (CNNs) are trained on the different views and time resolutions of the region adaptive Depth Motion Maps. Multiple views are synthesised to enhance the view invariance. The region adaptive weights, based on localised motion, accentuate and differentiate parts of actions possessing faster motion. Dedicated 3D CNN streams for multi-time resolution appearance information (RGB) are also included. These help to identify and differentiate between small object interactions. A pre-trained 3D-CNN is used here with fine-tuning for each stream along with multiple class Support Vector Machines (SVM)s. Average score fusion is used on the output. The developed approach is capable of recognising both human action and human-object interaction. Three public domain datasets including: MSR 3D Action,Northwestern UCLA multi-view actions and MSR 3D daily activity are used to evaluate the proposed solution. The experimental results demonstrate the robustness of this approach compared with state-of-the-art algorithms.



### Digging Deeper into Egocentric Gaze Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.06090v1
- **DOI**: 10.1109/WACV.2019.00035
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06090v1)
- **Published**: 2019-04-12 08:08:12+00:00
- **Updated**: 2019-04-12 08:08:12+00:00
- **Authors**: Hamed R. Tavakoli, Esa Rahtu, Juho Kannala, Ali Borji
- **Comment**: presented at WACV 2019
- **Journal**: None
- **Summary**: This paper digs deeper into factors that influence egocentric gaze. Instead of training deep models for this purpose in a blind manner, we propose to inspect factors that contribute to gaze guidance during daily tasks. Bottom-up saliency and optical flow are assessed versus strong spatial prior baselines. Task-specific cues such as vanishing point, manipulation point, and hand regions are analyzed as representatives of top-down information. We also look into the contribution of these factors by investigating a simple recurrent neural model for ego-centric gaze prediction. First, deep features are extracted for all input video frames. Then, a gated recurrent unit is employed to integrate information over time and to predict the next fixation. We also propose an integrated model that combines the recurrent model with several top-down and bottom-up cues. Extensive experiments over multiple datasets reveal that (1) spatial biases are strong in egocentric videos, (2) bottom-up saliency models perform poorly in predicting gaze and underperform spatial biases, (3) deep features perform better compared to traditional features, (4) as opposed to hand regions, the manipulation point is a strong influential cue for gaze prediction, (5) combining the proposed recurrent model with bottom-up cues, vanishing points and, in particular, manipulation point results in the best gaze prediction accuracy over egocentric videos, (6) the knowledge transfer works best for cases where the tasks or sequences are similar, and (7) task and activity recognition can benefit from gaze prediction. Our findings suggest that (1) there should be more emphasis on hand-object interaction and (2) the egocentric vision community should consider larger datasets including diverse stimuli and more subjects.



### Evaluating Robustness of Deep Image Super-Resolution against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1904.06097v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06097v2)
- **Published**: 2019-04-12 08:37:17+00:00
- **Updated**: 2019-10-02 05:11:06+00:00
- **Authors**: Jun-Ho Choi, Huan Zhang, Jun-Hyuk Kim, Cho-Jui Hsieh, Jong-Seok Lee
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many computer vision applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks.



### Face De-occlusion using 3D Morphable Model and Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1904.06109v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06109v2)
- **Published**: 2019-04-12 09:06:47+00:00
- **Updated**: 2019-09-06 14:29:45+00:00
- **Authors**: Xiaowei Yuan, In Kyu Park
- **Comment**: Presented in ICCV 2019
- **Journal**: None
- **Summary**: In recent decades, 3D morphable model (3DMM) has been commonly used in image-based photorealistic 3D face reconstruction. However, face images are often corrupted by serious occlusion by non-face objects including eyeglasses, masks, and hands. Such objects block the correct capture of landmarks and shading information. Therefore, the reconstructed 3D face model is hardly reusable. In this paper, a novel method is proposed to restore de-occluded face images based on inverse use of 3DMM and generative adversarial network. We utilize the 3DMM prior to the proposed adversarial network and combine a global and local adversarial convolutional neural network to learn face de-occlusion model. The 3DMM serves not only as geometric prior but also proposes the face region for the local discriminator. Experiment results confirm the effectiveness and robustness of the proposed algorithm in removing challenging types of occlusions with various head poses and illumination. Furthermore, the proposed method reconstructs the correct 3D face model with de-occluded textures.



### PWOC-3D: Deep Occlusion-Aware End-to-End Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.06116v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06116v1)
- **Published**: 2019-04-12 09:20:51+00:00
- **Updated**: 2019-04-12 09:20:51+00:00
- **Authors**: Rohan Saxena, René Schuster, Oliver Wasenmüller, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, convolutional neural networks (CNNs) have demonstrated increasing success at learning many computer vision tasks including dense estimation problems such as optical flow and stereo matching. However, the joint prediction of these tasks, called scene flow, has traditionally been tackled using slow classical methods based on primitive assumptions which fail to generalize. The work presented in this paper overcomes these drawbacks efficiently (in terms of speed and accuracy) by proposing PWOC-3D, a compact CNN architecture to predict scene flow from stereo image sequences in an end-to-end supervised setting. Further, large motion and occlusions are well-known problems in scene flow estimation. PWOC-3D employs specialized design decisions to explicitly model these challenges. In this regard, we propose a novel self-supervised strategy to predict occlusions from images (learned without any labeled occlusion data). Leveraging several such constructs, our network achieves competitive results on the KITTI benchmark and the challenging FlyingThings3D dataset. Especially on KITTI, PWOC-3D achieves the second place among end-to-end deep learning methods with 48 times fewer parameters than the top-performing method.



### Towards Photographic Image Manipulation with Balanced Growing of Generative Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1904.06145v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.06145v2)
- **Published**: 2019-04-12 10:31:45+00:00
- **Updated**: 2020-02-20 18:36:05+00:00
- **Authors**: Ari Heljakka, Arno Solin, Juho Kannala
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: We present a generative autoencoder that provides fast encoding, faithful reconstructions (eg. retaining the identity of a face), sharp generated/reconstructed samples in high resolutions, and a well-structured latent space that supports semantic manipulation of the inputs. There are no current autoencoder or GAN models that satisfactorily achieve all of these. We build on the progressively growing autoencoder model PIONEER, for which we completely alter the training dynamics based on a careful analysis of recently introduced normalization schemes. We show significantly improved visual and quantitative results for face identity conservation in CelebAHQ. Our model achieves state-of-the-art disentanglement of latent space, both quantitatively and via realistic image attribute manipulations. On the LSUN Bedrooms dataset, we improve the disentanglement performance of the vanilla PIONEER, despite having a simpler model. Overall, our results indicate that the PIONEER networks provide a way towards photorealistic face manipulation.



### An Empirical Evaluation Study on the Training of SDC Features for Dense Pixel Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.06167v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06167v1)
- **Published**: 2019-04-12 11:50:07+00:00
- **Updated**: 2019-04-12 11:50:07+00:00
- **Authors**: René Schuster, Oliver Wasenmüller, Christian Unger, Didier Stricker
- **Comment**: None
- **Journal**: None
- **Summary**: Training a deep neural network is a non-trivial task. Not only the tuning of hyperparameters, but also the gathering and selection of training data, the design of the loss function, and the construction of training schedules is important to get the most out of a model. In this study, we perform a set of experiments all related to these issues. The model for which different training strategies are investigated is the recently presented SDC descriptor network (stacked dilated convolution). It is used to describe images on pixel-level for dense matching tasks. Our work analyzes SDC in more detail, validates some best practices for training deep neural networks, and provides insights into training with multiple domain data.



### Generalized Presentation Attack Detection: a face anti-spoofing evaluation proposal
- **Arxiv ID**: http://arxiv.org/abs/1904.06213v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06213v1)
- **Published**: 2019-04-12 12:57:54+00:00
- **Updated**: 2019-04-12 12:57:54+00:00
- **Authors**: Artur Costa-Pazo, David Jimenez-Cabello, Esteban Vazquez-Fernandez, Jose L. Alba-Castro, Roberto J. López-Sastre
- **Comment**: 8 pages, to appear at International Conference on Biometrics (ICB19)
- **Journal**: None
- **Summary**: Over the past few years, Presentation Attack Detection (PAD) has become a fundamental part of facial recognition systems. Although much effort has been devoted to anti-spoofing research, generalization in real scenarios remains a challenge. In this paper we present a new open-source evaluation framework to study the generalization capacity of face PAD methods, coined here as face-GPAD. This framework facilitates the creation of new protocols focused on the generalization problem establishing fair procedures of evaluation and comparison between PAD solutions. We also introduce a large aggregated and categorized dataset to address the problem of incompatibility between publicly available datasets. Finally, we propose a benchmark adding two novel evaluation protocols: one for measuring the effect introduced by the variations in face resolution, and the second for evaluating the influence of adversarial operating conditions.



### Multimodal Machine Learning-based Knee Osteoarthritis Progression Prediction from Plain Radiographs and Clinical Data
- **Arxiv ID**: http://arxiv.org/abs/1904.06236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06236v2)
- **Published**: 2019-04-12 13:57:50+00:00
- **Updated**: 2019-05-06 13:56:21+00:00
- **Authors**: Aleksei Tiulpin, Stefan Klein, Sita M. A. Bierma-Zeinstra, Jérôme Thevenot, Esa Rahtu, Joyce van Meurs, Edwin H. G. Oei, Simo Saarakkala
- **Comment**: None
- **Journal**: None
- **Summary**: Knee osteoarthritis (OA) is the most common musculoskeletal disease without a cure, and current treatment options are limited to symptomatic relief. Prediction of OA progression is a very challenging and timely issue, and it could, if resolved, accelerate the disease modifying drug development and ultimately help to prevent millions of total joint replacement surgeries performed annually. Here, we present a multi-modal machine learning-based OA progression prediction model that utilizes raw radiographic data, clinical examination results and previous medical history of the patient. We validated this approach on an independent test set of 3,918 knee images from 2,129 subjects. Our method yielded area under the ROC curve (AUC) of 0.79 (0.78-0.81) and Average Precision (AP) of 0.68 (0.66-0.70). In contrast, a reference approach, based on logistic regression, yielded AUC of 0.75 (0.74-0.77) and AP of 0.62 (0.60-0.64). The proposed method could significantly improve the subject selection process for OA drug-development trials and help the development of personalized therapeutic plans.



### Generative Hybrid Representations for Activity Forecasting with No-Regret Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.06250v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06250v2)
- **Published**: 2019-04-12 14:22:37+00:00
- **Updated**: 2020-04-03 18:27:39+00:00
- **Authors**: Jiaqi Guan, Ye Yuan, Kris M. Kitani, Nicholas Rhinehart
- **Comment**: Oral presentation at CVPR 2020
- **Journal**: None
- **Summary**: Automatically reasoning about future human behaviors is a difficult problem but has significant practical applications to assistive systems. Part of this difficulty stems from learning systems' inability to represent all kinds of behaviors. Some behaviors, such as motion, are best described with continuous representations, whereas others, such as picking up a cup, are best described with discrete representations. Furthermore, human behavior is generally not fixed: people can change their habits and routines. This suggests these systems must be able to learn and adapt continuously. In this work, we develop an efficient deep generative model to jointly forecast a person's future discrete actions and continuous motions. On a large-scale egocentric dataset, EPIC-KITCHENS, we observe our method generates high-quality and diverse samples while exhibiting better generalization than related generative models. Finally, we propose a variant to continually learn our model from streaming data, observe its practical effectiveness, and theoretically justify its learning efficiency.



### MAANet: Multi-view Aware Attention Networks for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1904.06252v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06252v1)
- **Published**: 2019-04-12 14:32:10+00:00
- **Updated**: 2019-04-12 14:32:10+00:00
- **Authors**: Jingcai Guo, Shiheng Ma, Song Guo
- **Comment**: None
- **Journal**: None
- **Summary**: In most recent years, deep convolutional neural networks (DCNNs) based image super-resolution (SR) has gained increasing attention in multimedia and computer vision communities, focusing on restoring the high-resolution (HR) image from a low-resolution (LR) image. However, one nonnegligible flaw of DCNNs based methods is that most of them are not able to restore high-resolution images containing sufficient high-frequency information from low-resolution images with low-frequency information redundancy. Worse still, as the depth of DCNNs increases, the training easily encounters the problem of vanishing gradients, which makes the training more difficult. These problems hinder the effectiveness of DCNNs in image SR task. To solve these problems, we propose the Multi-view Aware Attention Networks (MAANet) for image SR task. Specifically, we propose the local aware (LA) and global aware (GA) attention to deal with LR features in unequal manners, which can highlight the high-frequency components and discriminate each feature from LR images in the local and the global views, respectively. Furthermore, we propose the local attentive residual-dense (LARD) block, which combines the LA attention with multiple residual and dense connections, to fit a deeper yet easy to train architecture. The experimental results show that our proposed approach can achieve remarkable performance compared with other state-of-the-art methods.



### Variational Inference for Computational Imaging Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1904.06264v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.06264v3)
- **Published**: 2019-04-12 15:10:57+00:00
- **Updated**: 2020-08-21 15:18:09+00:00
- **Authors**: Francesco Tonolini, Jack Radford, Alex Turpin, Daniele Faccio, Roderick Murray-Smith
- **Comment**: 29+15 Pages
- **Journal**: None
- **Summary**: Machine learning methods for computational imaging require uncertainty estimation to be reliable in real settings. While Bayesian models offer a computationally tractable way of recovering uncertainty, they need large data volumes to be trained, which in imaging applications implicates prohibitively expensive collections with specific imaging instruments. This paper introduces a novel framework to train variational inference for inverse problems exploiting in combination few experimentally collected data, domain expertise and existing image data sets. In such a way, Bayesian machine learning models can solve imaging inverse problems with minimal data collection efforts. Extensive simulated experiments show the advantages of the proposed framework. The approach is then applied to two real experimental optics settings: holographic image reconstruction and imaging through highly scattering media. In both settings, state of the art reconstructions are achieved with little collection of training data.



### ACE: Adapting to Changing Environments for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.06268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06268v1)
- **Published**: 2019-04-12 15:15:15+00:00
- **Updated**: 2019-04-12 15:15:15+00:00
- **Authors**: Zuxuan Wu, Xin Wang, Joseph E. Gonzalez, Tom Goldstein, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks exhibit exceptional accuracy when they are trained and tested on the same data distributions. However, neural classifiers are often extremely brittle when confronted with domain shift---changes in the input distribution that occur over time. We present ACE, a framework for semantic segmentation that dynamically adapts to changing environments over the time. By aligning the distribution of labeled training data from the original source domain with the distribution of incoming data in a shifted domain, ACE synthesizes labeled training data for environments as it sees them. This stylized data is then used to update a segmentation model so that it performs well in new environments. To avoid forgetting knowledge from past environments, we introduce a memory that stores feature statistics from previously seen domains. These statistics can be used to replay images in any of the previously observed domains, thus preventing catastrophic forgetting. In addition to standard batch training using stochastic gradient decent (SGD), we also experiment with fast adaptation methods based on adaptive meta-learning. Extensive experiments are conducted on two datasets from SYNTHIA, the results demonstrate the effectiveness of the proposed approach when adapting to a number of tasks.



### GeoCapsNet: Aerial to Ground view Image Geo-localization using Capsule Network
- **Arxiv ID**: http://arxiv.org/abs/1904.06281v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06281v1)
- **Published**: 2019-04-12 15:34:01+00:00
- **Updated**: 2019-04-12 15:34:01+00:00
- **Authors**: Bin Sun, Chen Chen, Yingying Zhu, Jianmin Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: The task of cross-view image geo-localization aims to determine the geo-location (GPS coordinates) of a query ground-view image by matching it with the GPS-tagged aerial (satellite) images in a reference dataset. Due to the dramatic changes of viewpoint, matching the cross-view images is challenging. In this paper, we propose the GeoCapsNet based on the capsule network for ground-to-aerial image geo-localization. The network first extracts features from both ground-view and aerial images via standard convolution layers and the capsule layers further encode the features to model the spatial feature hierarchies and enhance the representation power. Moreover, we introduce a simple and effective weighted soft-margin triplet loss with online batch hard sample mining, which can greatly improve image retrieval accuracy. Experimental results show that our GeoCapsNet significantly outperforms the state-of-the-art approaches on two benchmark datasets.



### Boundary-Preserved Deep Denoising of the Stochastic Resonance Enhanced Multiphoton Images
- **Arxiv ID**: http://arxiv.org/abs/1904.06329v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.06329v2)
- **Published**: 2019-04-12 17:17:40+00:00
- **Updated**: 2019-04-15 03:00:57+00:00
- **Authors**: Sheng-Yong Niu, Lun-Zhang Guo, Yue Li, Tzung-Dau Wang, Yu Tsao, Tzu-Ming Liu
- **Comment**: None
- **Journal**: None
- **Summary**: As the rapid growth of high-speed and deep-tissue imaging in biomedical research, it is urgent to find a robust and effective denoising method to retain morphological features for further texture analysis and segmentation. Conventional denoising filters and models can easily suppress perturbative noises in high contrast images. However, for low photon budget multi-photon images, high detector gain will not only boost signals, but also bring huge background noises. In such stochastic resonance regime of imaging, sub-threshold signals may be detectable with the help of noises. Therefore, a denoising filter that can smartly remove noises without sacrificing the important cellular features such as cell boundaries is highly desired. In this paper, we propose a convolutional neural network based autoencoder method, Fully Convolutional Deep Denoising Autoencoder (DDAE), to improve the quality of Three-Photon Fluorescence (3PF) and Third Harmonic Generation (THG) microscopy images. The average of the acquired 200 images of a given location served as the low-noise answer for DDAE training. Compared with other widely used denoising methods, our DDAE model shows better signal-to-noise ratio (26.6 and 29.9 for 3PF and THG, respectively), structure similarity (0.86 and 0.87 for 3PF and THG, respectively), and preservation of nuclear or cellular boundaries.



### Incremental multi-domain learning with network latent tensor factorization
- **Arxiv ID**: http://arxiv.org/abs/1904.06345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06345v2)
- **Published**: 2019-04-12 17:57:05+00:00
- **Updated**: 2019-11-22 14:04:15+00:00
- **Authors**: Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, Maja Pantic
- **Comment**: AAAI20
- **Journal**: None
- **Summary**: The prominence of deep learning, large amount of annotated data and increasingly powerful hardware made it possible to reach remarkable performance for supervised classification tasks, in many cases saturating the training sets. However the resulting models are specialized to a single very specific task and domain. Adapting the learned classification to new domains is a hard problem due to at least three reasons: (1) the new domains and the tasks might be drastically different; (2) there might be very limited amount of annotated data on the new domain and (3) full training of a new model for each new task is prohibitive in terms of computation and memory, due to the sheer number of parameters of deep CNNs. In this paper, we present a method to learn new-domains and tasks incrementally, building on prior knowledge from already learned tasks and without catastrophic forgetting. We do so by jointly parametrizing weights across layers using low-rank Tucker structure. The core is task agnostic while a set of task specific factors are learnt on each new domain. We show that leveraging tensor structure enables better performance than simply using matrix operations. Joint tensor modelling also naturally leverages correlations across different layers. Compared with previous methods which have focused on adapting each layer separately, our approach results in more compact representations for each new task/domain. We apply the proposed method to the 10 datasets of the Visual Decathlon Challenge and show that our method offers on average about 7.5x reduction in number of parameters and competitive performance in terms of both classification accuracy and Decathlon score.



### Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.06346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06346v2)
- **Published**: 2019-04-12 17:57:40+00:00
- **Updated**: 2019-08-21 06:23:42+00:00
- **Authors**: Yuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei Han, Elliot Fishman, Alan Yuille
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Accurate multi-organ abdominal CT segmentation is essential to many clinical applications such as computer-aided intervention. As data annotation requires massive human labor from experienced radiologists, it is common that training data are partially labeled, e.g., pancreas datasets only have the pancreas labeled while leaving the rest marked as background. However, these background labels can be misleading in multi-organ segmentation since the "background" usually contains some other organs of interest. To address the background ambiguity in these partially-labeled datasets, we propose Prior-aware Neural Network (PaNN) via explicitly incorporating anatomical priors on abdominal organ sizes, guiding the training process with domain-specific knowledge. More specifically, PaNN assumes that the average organ size distributions in the abdomen should approximate their empirical distributions, a prior statistics obtained from the fully-labeled dataset. As our training objective is difficult to be directly optimized using stochastic gradient descent [20], we propose to reformulate it in a min-max form and optimize it via the stochastic primal-dual gradient algorithm. PaNN achieves state-of-the-art performance on the MICCAI2015 challenge "Multi-Atlas Labeling Beyond the Cranial Vault", a competition on organ segmentation in the abdomen. We report an average Dice score of 84.97%, surpassing the prior art by a large margin of 3.27%.



### Unrestricted Adversarial Examples via Semantic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1904.06347v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06347v2)
- **Published**: 2019-04-12 17:59:30+00:00
- **Updated**: 2020-03-20 17:59:15+00:00
- **Authors**: Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, D. A. Forsyth
- **Comment**: Accepted to ICLR 2020. First two authors contributed equally. Code:
  https://github.com/aisecure/Big-but-Invisible-Adversarial-Attack and
  Openreview: https://openreview.net/forum?id=Sye_OgHFwH
- **Journal**: None
- **Summary**: Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce "unrestricted" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG compression, feature squeezing and adversarially trained model. We also show that the proposed methods can effectively be applied to both image classification and image captioning tasks on complex datasets such as ImageNet and MSCOCO. In addition, we conduct comprehensive user studies to show that our generated semantic adversarial examples are photorealistic to humans despite large magnitude perturbations when compared to other attacks.



### Towards Accurate One-Stage Object Detection with AP-Loss
- **Arxiv ID**: http://arxiv.org/abs/1904.06373v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06373v3)
- **Published**: 2019-04-12 18:58:55+00:00
- **Updated**: 2020-02-13 18:23:50+00:00
- **Authors**: Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang, Lingyu Duan, Zhibo Chen, Changwei He, Junni Zou
- **Comment**: 13 pages, 7 figures, 4 tables, main paper + supplementary material,
  accepted to CVPR 2019
- **Journal**: None
- **Summary**: One-stage object detectors are trained by optimizing classification-loss and localization-loss simultaneously, with the former suffering much from extreme foreground-background class imbalance issue due to the large number of anchors. This paper alleviates this issue by proposing a novel framework to replace the classification task in one-stage detectors with a ranking task, and adopting the Average-Precision loss (AP-loss) for the ranking problem. Due to its non-differentiability and non-convexity, the AP-loss cannot be optimized directly. For this purpose, we develop a novel optimization algorithm, which seamlessly combines the error-driven update scheme in perceptron learning and backpropagation algorithm in deep networks. We verify good convergence property of the proposed algorithm theoretically and empirically. Experimental results demonstrate notable performance improvement in state-of-the-art one-stage detectors based on AP-loss over different kinds of classification-losses on various benchmarks, without changing the network architectures. Code is available at https://github.com/cccorn/AP-loss.



### Macrocanonical Models for Texture Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.06396v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.06396v1)
- **Published**: 2019-04-12 20:08:39+00:00
- **Updated**: 2019-04-12 20:08:39+00:00
- **Authors**: De Bortoli Valentin, Desolneux Agnès, Galerne Bruno, Leclaire Arthur
- **Comment**: Accepted to Scale Space and Variational Methods in Computer Vision
  2019
- **Journal**: None
- **Summary**: In this article we consider macrocanonical models for texture synthesis. In these models samples are generated given an input texture image and a set of features which should be matched in expectation. It is known that if the images are quantized, macrocanonical models are given by Gibbs measures, using the maximum entropy principle. We study conditions under which this result extends to real-valued images. If these conditions hold, finding a macrocanonical model amounts to minimizing a convex function and sampling from an associated Gibbs measure. We analyze an algorithm which alternates between sampling and minimizing. We present experiments with neural network features and study the drawbacks and advantages of using this sampling scheme.



### Multi-View Stereo by Temporal Nonparametric Fusion
- **Arxiv ID**: http://arxiv.org/abs/1904.06397v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06397v2)
- **Published**: 2019-04-12 20:13:23+00:00
- **Updated**: 2019-08-16 09:59:21+00:00
- **Authors**: Yuxin Hou, Juho Kannala, Arno Solin
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder--decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from previous views. We train the encoder--decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.



### Distributed Deep Learning Model for Intelligent Video Surveillance Systems with Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/1904.06400v1
- **DOI**: 10.1109/TII.2019.2909473
- **Categories**: **cs.CV**, cs.LG, F.2.2; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/1904.06400v1)
- **Published**: 2019-04-12 20:17:05+00:00
- **Updated**: 2019-04-12 20:17:05+00:00
- **Authors**: Jianguo Chen, Kenli Li, Qingying Deng, Keqin Li, Philip S. Yu
- **Comment**: IEEE Transactions on Industrial Informatics. 2019
- **Journal**: None
- **Summary**: In this paper, we propose a Distributed Intelligent Video Surveillance (DIVS) system using Deep Learning (DL) algorithms and deploy it in an edge computing environment. We establish a multi-layer edge computing architecture and a distributed DL training model for the DIVS system. The DIVS system can migrate computing workloads from the network center to network edges to reduce huge network communication overhead and provide low-latency and accurate video analysis solutions. We implement the proposed DIVS system and address the problems of parallel training, model synchronization, and workload balancing. Task-level parallel and model-level parallel training methods are proposed to further accelerate the video analysis process. In addition, we propose a model parameter updating method to achieve model synchronization of the global DL model in a distributed EC environment. Moreover, a dynamic data migration approach is proposed to address the imbalance of workload and computational power of edge nodes. Experimental results showed that the EC architecture can provide elastic and scalable computing power, and the proposed DIVS system can efficiently handle video surveillance and analysis tasks.



### Patch redundancy in images: a statistical testing framework and some applications
- **Arxiv ID**: http://arxiv.org/abs/1904.06428v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06428v1)
- **Published**: 2019-04-12 21:36:14+00:00
- **Updated**: 2019-04-12 21:36:14+00:00
- **Authors**: De Bortoli Valentin, Desolneux Agnès, Galerne Bruno, Leclaire Arthur
- **Comment**: Submitted to SIIMS
- **Journal**: None
- **Summary**: In this work we introduce a statistical framework in order to analyze the spatial redundancy in natural images. This notion of spatial redundancy must be defined locally and thus we give some examples of functions (auto-similarity and template similarity) which, given one or two images, computes a similarity measurement between patches. Two patches are said to be similar if the similarity measurement is small enough. To derive a criterion for taking a decision on the similarity between two patches we present an a contrario model. Namely, two patches are said to be similar if the associated similarity measurement is unlikely to happen in a background model. Choosing Gaussian random fields as background models we derive non-asymptotic expressions for the probability distribution function of similarity measurements. We introduce a fast algorithm in order to assess redundancy in natural images and present applications in denoising, periodicity analysis and texture ranking.



### Detecting Anemia from Retinal Fundus Images
- **Arxiv ID**: http://arxiv.org/abs/1904.06435v1
- **DOI**: 10.1038/s41551-019-0487-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06435v1)
- **Published**: 2019-04-12 22:25:48+00:00
- **Updated**: 2019-04-12 22:25:48+00:00
- **Authors**: Akinori Mitani, Yun Liu, Abigail Huang, Greg S. Corrado, Lily Peng, Dale R. Webster, Naama Hammel, Avinash V. Varadarajan
- **Comment**: 31 pages, 5 figures, 3 tables
- **Journal**: Nature Biomedical Engineering (2019)
- **Summary**: Despite its high prevalence, anemia is often undetected due to the invasiveness and cost of screening and diagnostic tests. Though some non-invasive approaches have been developed, they are less accurate than invasive methods, resulting in an unmet need for more accurate non-invasive methods. Here, we show that deep learning-based algorithms can detect anemia and quantify several related blood measurements using retinal fundus images both in isolation and in combination with basic metadata such as patient demographics. On a validation dataset of 11,388 patients from the UK Biobank, our algorithms achieved a mean absolute error of 0.63 g/dL (95% confidence interval (CI) 0.62-0.64) in quantifying hemoglobin concentration and an area under receiver operating characteristic curve (AUC) of 0.88 (95% CI 0.86-0.89) in detecting anemia. This work shows the potential of automated non-invasive anemia screening based on fundus images, particularly in diabetic patients, who may have regular retinal imaging and are at increased risk of further morbidity and mortality from anemia.



### Learning Shape Templates with Structured Implicit Functions
- **Arxiv ID**: http://arxiv.org/abs/1904.06447v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.06447v1)
- **Published**: 2019-04-12 23:15:47+00:00
- **Updated**: 2019-04-12 23:15:47+00:00
- **Authors**: Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T. Freeman, Thomas Funkhouser
- **Comment**: 12 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image.



