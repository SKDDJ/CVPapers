# Arxiv Papers in cs.CV on 2019-04-08
### Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?
- **Arxiv ID**: http://arxiv.org/abs/1904.03786v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03786v2)
- **Published**: 2019-04-08 00:39:27+00:00
- **Updated**: 2019-09-04 19:07:43+00:00
- **Authors**: Yunyang Xiong, Ronak Mehta, Vikas Singh
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.



### FoveaBox: Beyond Anchor-based Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1904.03797v2
- **DOI**: 10.1109/TIP.2020.3002345
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03797v2)
- **Published**: 2019-04-08 01:43:48+00:00
- **Updated**: 2020-07-16 15:38:26+00:00
- **Authors**: Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Lei Li, Jianbo Shi
- **Comment**: IEEE Transactions on Image Processing, code at:
  https://github.com/taokong/FoveaBox
- **Journal**: IEEE Trans. Image Process. pp. 7389-7398 (2020)
- **Summary**: We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox .



### Visual Localization Using Sparse Semantic 3D Map
- **Arxiv ID**: http://arxiv.org/abs/1904.03803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03803v2)
- **Published**: 2019-04-08 02:36:58+00:00
- **Updated**: 2019-05-17 01:24:21+00:00
- **Authors**: Tianxin Shi, Shuhan Shen, Xiang Gao, Lingjie Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate and robust visual localization under a wide range of viewing condition variations including season and illumination changes, as well as weather and day-night variations, is the key component for many computer vision and robotics applications. Under these conditions, most traditional methods would fail to locate the camera. In this paper we present a visual localization algorithm that combines structure-based method and image-based method with semantic information. Given semantic information about the query and database images, the retrieved images are scored according to the semantic consistency of the 3D model and the query image. Then the semantic matching score is used as weight for RANSAC's sampling and the pose is solved by a standard PnP solver. Experiments on the challenging long-term visual localization benchmark dataset demonstrate that our method has significant improvement compared with the state-of-the-arts.



### Towards Real-Time Automatic Portrait Matting on Mobile Devices
- **Arxiv ID**: http://arxiv.org/abs/1904.03816v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.03816v1)
- **Published**: 2019-04-08 03:21:25+00:00
- **Updated**: 2019-04-08 03:21:25+00:00
- **Authors**: Seokjun Seo, Seungwoo Choi, Martin Kersner, Beomjun Shin, Hyungsuk Yoon, Hyeongmin Byun, Sungjoo Ha
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of automatic portrait matting on mobile devices. The proposed model is aimed at attaining real-time inference on mobile devices with minimal degradation of model performance. Our model MMNet, based on multi-branch dilated convolution with linear bottleneck blocks, outperforms the state-of-the-art model and is orders of magnitude faster. The model can be accelerated four times to attain 30 FPS on Xiaomi Mi 5 device with moderate increase in the gradient error. Under the same conditions, our model has an order of magnitude less number of parameters and is faster than Mobile DeepLabv3 while maintaining comparable performance. The accompanied implementation can be found at \url{https://github.com/hyperconnect/MMNet}.



### Ensemble Teaching for Hybrid Label Propagation
- **Arxiv ID**: http://arxiv.org/abs/1904.03828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03828v1)
- **Published**: 2019-04-08 04:10:40+00:00
- **Updated**: 2019-04-08 04:10:40+00:00
- **Authors**: Chen Gong, Dacheng Tao, Xiaojun Chang, Jian Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Label propagation aims to iteratively diffuse the label information from labeled examples to unlabeled examples over a similarity graph. Current label propagation algorithms cannot consistently yield satisfactory performance due to two reasons: one is the instability of single propagation method in dealing with various practical data, and the other one is the improper propagation sequence ignoring the labeling difficulties of different examples. To remedy above defects, this paper proposes a novel propagation algorithm called hybrid diffusion under ensemble teaching (HyDEnT). Specifically, HyDEnT integrates multiple propagation methods as base learners to fully exploit their individual wisdom, which helps HyDEnT to be stable and obtain consistent encouraging results. More importantly, HyDEnT conducts propagation under the guidance of an ensemble of teachers. That is to say, in every propagation round the simplest curriculum examples are wisely designated by a teaching algorithm, so that their labels can be reliably and accurately decided by the learners. To optimally choose these simplest examples, every teacher in the ensemble should comprehensively consider the examples' difficulties from its own viewpoint, as well as the common knowledge shared by all the teachers. This is accomplished by a designed optimization problem, which can be efficiently solved via the block coordinate descent method. Thanks to the efforts of the teachers, all the unlabeled examples are logically propagated from simple to difficult, leading to better propagation quality of HyDEnT than the existing methods.



### Weakly Supervised Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1904.03832v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03832v2)
- **Published**: 2019-04-08 04:21:54+00:00
- **Updated**: 2019-05-28 00:46:11+00:00
- **Authors**: Jingke Meng, Sheng Wu, Wei-Shi Zheng
- **Comment**: to appear at CVPR19
- **Journal**: None
- **Summary**: In the conventional person re-id setting, it is assumed that the labeled images are the person images within the bounding box for each individual; this labeling across multiple nonoverlapping camera views from raw video surveillance is costly and time-consuming. To overcome this difficulty, we consider weakly supervised person re-id modeling. The weak setting refers to matching a target person with an untrimmed gallery video where we only know that the identity appears in the video without the requirement of annotating the identity in any frame of the video during the training procedure. Hence, for a video, there could be multiple video-level labels. We cast this weakly supervised person re-id challenge into a multi-instance multi-label learning (MIML) problem. In particular, we develop a Cross-View MIML (CV-MIML) method that is able to explore potential intraclass person images from all the camera views by incorporating the intra-bag alignment and the cross-view bag alignment. Finally, the CV-MIML method is embedded into an existing deep neural network for developing the Deep Cross-View MIML (Deep CV-MIML) model. We have performed extensive experiments to show the feasibility of the proposed weakly supervised setting and verify the effectiveness of our method compared to related methods on four weakly labeled datasets.



### Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure
- **Arxiv ID**: http://arxiv.org/abs/1904.03837v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03837v1)
- **Published**: 2019-04-08 04:48:02+00:00
- **Updated**: 2019-04-08 04:48:02+00:00
- **Authors**: Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: The redundancy is widely recognized in Convolutional Neural Networks (CNNs), which enables to remove unimportant filters from convolutional layers so as to slim the network with acceptable performance drop. Inspired by the linear and combinational properties of convolution, we seek to make some filters increasingly close and eventually identical for network slimming. To this end, we propose Centripetal SGD (C-SGD), a novel optimization method, which can train several filters to collapse into a single point in the parameter hyperspace. When the training is completed, the removal of the identical filters can trim the network with NO performance loss, thus no finetuning is needed. By doing so, we have partly solved an open problem of constrained filter pruning on CNNs with complicated structure, where some layers must be pruned following others. Our experimental results on CIFAR-10 and ImageNet have justified the effectiveness of C-SGD-based filter pruning. Moreover, we have provided empirical evidences for the assumption that the redundancy in deep neural networks helps the convergence of training by showing that a redundant CNN trained using C-SGD outperforms a normally trained counterpart with the equivalent width.



### Image-based reconstruction for the impact problems by using DPNNs
- **Arxiv ID**: http://arxiv.org/abs/1905.03229v3
- **DOI**: None
- **Categories**: **cs.OH**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1905.03229v3)
- **Published**: 2019-04-08 04:48:36+00:00
- **Updated**: 2019-06-03 05:43:12+00:00
- **Authors**: Yu Li, Hu Wang, Wenquan Shuai, Honghao Zhang, Yong Peng
- **Comment**: None
- **Journal**: None
- **Summary**: With the improvement of the pattern recognition and feature extraction of Deep Neural Networks (DPNNs), image-based design and optimization have been widely used in multidisciplinary researches. Recently, a Reconstructive Neural Network (ReConNN) has been proposed to obtain an image-based model from an analysis-based model [1, 2], and a steady-state heat transfer of a heat sink has been successfully reconstructed. Commonly, this method is suitable to handle stable-state problems. However, it has difficulties handling nonlinear transient impact problems, due to the bottlenecks of the Deep Neural Network (DPNN). For example, nonlinear transient problems make it difficult for the Generative Adversarial Network (GAN) to generate various reasonable images. Therefore, in this study, an improved ReConNN method is proposed to address the mentioned weaknesses. Time-dependent ordered images can be generated. Furthermore, the improved method is successfully applied in impact simulation case and engineering experiment. Through the experiments, comparisons and analyses, the improved method is demonstrated to outperform the former one in terms of its accuracy, efficiency and costs.



### Weakly Supervised Person Re-ID: Differentiable Graphical Learning and A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1904.03845v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.03845v3)
- **Published**: 2019-04-08 05:27:53+00:00
- **Updated**: 2020-07-15 08:16:31+00:00
- **Authors**: Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang Lai, Zhengtao Yu, Liang Lin
- **Comment**: Accepted by TNNLS 2020
- **Journal**: None
- **Summary**: Person re-identification (Re-ID) benefits greatly from the accurate annotations of existing datasets (e.g., CUHK03 [1] and Market-1501 [2]), which are quite expensive because each image in these datasets has to be assigned with a proper label. In this work, we ease the annotation of Re-ID by replacing the accurate annotation with inaccurate annotation, i.e., we group the images into bags in terms of time and assign a bag-level label for each bag. This greatly reduces the annotation effort and leads to the creation of a large-scale Re-ID benchmark called SYSU-30$k$. The new benchmark contains $30k$ individuals, which is about $20$ times larger than CUHK03 ($1.3k$ individuals) and Market-1501 ($1.5k$ individuals), and $30$ times larger than ImageNet ($1k$ categories). It sums up to 29,606,918 images. Learning a Re-ID model with bag-level annotation is called the weakly supervised Re-ID problem. To solve this problem, we introduce a differentiable graphical model to capture the dependencies from all images in a bag and generate a reliable pseudo label for each person image. The pseudo label is further used to supervise the learning of the Re-ID model. When compared with the fully supervised Re-ID models, our method achieves state-of-the-art performance on SYSU-30$k$ and other datasets. The code, dataset, and pretrained model will be available at \url{https://github.com/wanggrun/SYSU-30k}.



### Decomposition-Based Transfer Distance Metric Learning for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.03846v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03846v1)
- **Published**: 2019-04-08 05:39:46+00:00
- **Updated**: 2019-04-08 05:39:46+00:00
- **Authors**: Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Distance metric learning (DML) is a critical factor for image analysis and pattern recognition. To learn a robust distance metric for a target task, we need abundant side information (i.e., the similarity/dissimilarity pairwise constraints over the labeled data), which is usually unavailable in practice due to the high labeling cost. This paper considers the transfer learning setting by exploiting the large quantity of side information from certain related, but different source tasks to help with target metric learning (with only a little side information). The state-of-the-art metric learning algorithms usually fail in this setting because the data distributions of the source task and target task are often quite different. We address this problem by assuming that the target distance metric lies in the space spanned by the eigenvectors of the source metrics (or other randomly generated bases). The target metric is represented as a combination of the base metrics, which are computed using the decomposed components of the source metrics (or simply a set of random bases); we call the proposed method, decomposition-based transfer DML (DTDML). In particular, DTDML learns a sparse combination of the base metrics to construct the target metric by forcing the target metric to be close to an integration of the source metrics. The main advantage of the proposed method compared with existing transfer metric learning approaches is that we directly learn the base metric coefficients instead of the target metric. To this end, far fewer variables need to be learned. We therefore obtain more reliable solutions given the limited side information and the optimization tends to be faster. Experiments on the popular handwritten image (digit, letter) classification and challenge natural image annotation tasks demonstrate the effectiveness of the proposed method.



### Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1904.03848v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03848v2)
- **Published**: 2019-04-08 05:41:48+00:00
- **Updated**: 2022-07-14 12:27:15+00:00
- **Authors**: Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, Hongdong Li
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Unsupervised deep learning for optical flow computation has achieved promising results. Most existing deep-net based methods rely on image brightness consistency and local smoothness constraint to train the networks. Their performance degrades at regions where repetitive textures or occlusions occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical flow method which incorporates global geometric constraints into network learning. In particular, we investigate multiple ways of enforcing the epipolar constraint in flow estimation. To alleviate a "chicken-and-egg" type of problem encountered in dynamic scenes where multiple motions may be present, we propose a low-rank constraint as well as a union-of-subspaces constraint for training. Experimental results on various benchmarking datasets show that our method achieves competitive performance compared with supervised methods and outperforms state-of-the-art unsupervised deep-learning methods.



### Noise-Aware Unsupervised Deep Lidar-Stereo Fusion
- **Arxiv ID**: http://arxiv.org/abs/1904.03868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03868v1)
- **Published**: 2019-04-08 07:16:16+00:00
- **Updated**: 2019-04-08 07:16:16+00:00
- **Authors**: Xuelian Cheng, Yiran Zhong, Yuchao Dai, Pan Ji, Hongdong Li
- **Comment**: Accepted at CVPR2019
- **Journal**: None
- **Summary**: In this paper, we present LidarStereoNet, the first unsupervised Lidar-stereo fusion network, which can be trained in an end-to-end manner without the need of ground truth depth maps. By introducing a novel "Feedback Loop'' to connect the network input with output, LidarStereoNet could tackle both noisy Lidar points and misalignment between sensors that have been ignored in existing Lidar-stereo fusion studies. Besides, we propose to incorporate a piecewise planar model into network learning to further constrain depths to conform to the underlying 3D geometry. Extensive quantitative and qualitative evaluations on both real and synthetic datasets demonstrate the superiority of our method, which outperforms state-of-the-art stereo matching, depth completion and Lidar-Stereo fusion approaches significantly.



### Streamlined Dense Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.03870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03870v1)
- **Published**: 2019-04-08 07:17:30+00:00
- **Updated**: 2019-04-08 07:17:30+00:00
- **Authors**: Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, Bohyung Han
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Dense video captioning is an extremely challenging task since accurate and coherent description of events in a video requires holistic understanding of video contents as well as contextual reasoning of individual events. Most existing approaches handle this problem by first detecting event proposals from a video and then captioning on a subset of the proposals. As a result, the generated sentences are prone to be redundant or inconsistent since they fail to consider temporal dependency between events. To tackle this challenge, we propose a novel dense video captioning framework, which models temporal dependency across events in a video explicitly and leverages visual and linguistic context from prior events for coherent storytelling. This objective is achieved by 1) integrating an event sequence generation network to select a sequence of event proposals adaptively, and 2) feeding the sequence of event proposals to our sequential video captioning network, which is trained by reinforcement learning with two-level rewards at both event and episode levels for better context modeling. The proposed technique achieves outstanding performances on ActivityNet Captions dataset in most metrics.



### Referring to Objects in Videos using Spatio-Temporal Identifying Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1904.03885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03885v1)
- **Published**: 2019-04-08 08:28:54+00:00
- **Updated**: 2019-04-08 08:28:54+00:00
- **Authors**: Peratham Wiriyathammabhum, Abhinav Shrivastava, Vlad I. Morariu, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.



### From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images
- **Arxiv ID**: http://arxiv.org/abs/1904.03892v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03892v2)
- **Published**: 2019-04-08 08:59:12+00:00
- **Updated**: 2019-06-18 11:14:20+00:00
- **Authors**: Taibou Birgui Sekou, Moncef Hidane, Julien Olivier, Hubert Cardot
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning based models, generally, require a large number of samples for appropriate training, a requirement that is difficult to satisfy in the medical field. This issue can usually be avoided with a proper initialization of the weights. On the task of medical image segmentation in general, two techniques are oftentimes employed to tackle the training of a deep network $f_T$. The first one consists in reusing some weights of a network $f_S$ pre-trained on a large scale database ($e.g.$ ImageNet). This procedure, also known as $transfer$ $learning$, happens to reduce the flexibility when it comes to new network design since $f_T$ is constrained to match some parts of $f_S$. The second commonly used technique consists in working on image patches to benefit from the large number of available patches. This paper brings together these two techniques and propose to train $arbitrarily$ $designed$ $networks$ that segment an image in one forward pass, with a focus on relatively small databases. An experimental work have been carried out on the tasks of retinal blood vessel segmentation and the optic disc one, using four publicly available databases. Furthermore, three types of network are considered, going from a very light weighted network to a densely connected one. The final results show the efficiency of the proposed framework along with state of the art results on all the databases.



### Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation
- **Arxiv ID**: http://arxiv.org/abs/1904.03895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03895v2)
- **Published**: 2019-04-08 09:01:48+00:00
- **Updated**: 2019-04-09 11:00:57+00:00
- **Authors**: Fengda Zhu, Linchao Zhu, Yi Yang
- **Comment**: Paper has been accepted by CVPR 2019
- **Journal**: None
- **Summary**: There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by rendering. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. Therefore two types of information, i.e. visual representation and policy behavior, need to be adapted in the reinforcement model. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We propose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer and a policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.



### Multi-View Matrix Completion for Multi-Label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.03901v1
- **DOI**: 10.1109/TIP.2015.2421309
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03901v1)
- **Published**: 2019-04-08 09:17:56+00:00
- **Updated**: 2019-04-08 09:17:56+00:00
- **Authors**: Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (Volume: 24, Issue: 8, Aug.
  2015)
- **Summary**: There is growing interest in multi-label image classification due to its critical role in web-based image analytics-based applications, such as large-scale image retrieval and browsing. Matrix completion has recently been introduced as a method for transductive (semi-supervised) multi-label classification, and has several distinct advantages, including robustness to missing data and background noise in both feature and label space. However, it is limited by only considering data represented by a single-view feature, which cannot precisely characterize images containing several semantic concepts. To utilize multiple features taken from different views, we have to concatenate the different features as a long vector. But this concatenation is prone to over-fitting and often leads to very high time complexity in MC based image classification. Therefore, we propose to weightedly combine the MC outputs of different views, and present the multi-view matrix completion (MVMC) framework for transductive multi-label image classification. To learn the view combination weights effectively, we apply a cross validation strategy on the labeled set. In the learning process, we adopt the average precision (AP) loss, which is particular suitable for multi-label image classification. A least squares loss formulation is also presented for the sake of efficiency, and the robustness of the algorithm based on the AP loss compared with the other losses is investigated. Experimental evaluation on two real world datasets (PASCAL VOC' 07 and MIR Flickr) demonstrate the effectiveness of MVMC for transductive (semi-supervised) multi-label image classification, and show that MVMC can exploit complementary properties of different features and output-consistent labels for improved multi-label image classification.



### The EntOptLayout Cytoscape plug-in for the efficient visualization of major protein complexes in protein-protein interaction and signalling networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03910v2
- **DOI**: 10.1093/bioinformatics/btz257
- **Categories**: **q-bio.MN**, cond-mat.dis-nn, cs.CV, physics.bio-ph
- **Links**: [PDF](http://arxiv.org/pdf/1904.03910v2)
- **Published**: 2019-04-08 09:35:20+00:00
- **Updated**: 2019-11-01 11:47:10+00:00
- **Authors**: Bence Agg, Andrea Csaszar, Mate Szalay-Beko, Daniel V. Veres, Reka Mizsei, Peter Ferdinandy, Peter Csermely, Istvan A. Kovacs
- **Comment**: None
- **Journal**: Bioinformatics 2019 35, 4490-4492
- **Summary**: Motivation: Network visualizations of complex biological datasets usually result in 'hairball' images, which do not discriminate network modules. Results: We present the EntOptLayout Cytoscape plug-in based on a recently developed network representation theory. The plug-in provides an efficient visualization of network modules, which represent major protein complexes in protein-protein interaction and signalling networks. Importantly, the tool gives a quality score of the network visualization by calculating the information loss between the input data and the visual representation showing a 3- to 25-fold improvement over conventional methods. Availability and implementation: The plug-in (running on Windows, Linux, or Mac OS) and its tutorial (both in written and video forms) can be downloaded freely under the terms of the MIT license from: http://apps.cytoscape.org/apps/entoptlayout. Supplementary data are available at Bioinformatics online. Contact: csermely.peter@med.semmelweis-univ.hu



### On Learning Density Aware Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1904.03911v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03911v1)
- **Published**: 2019-04-08 09:35:23+00:00
- **Updated**: 2019-04-08 09:35:23+00:00
- **Authors**: Soumyadeep Ghosh, Richa Singh, Mayank Vatsa
- **Comment**: Accepted in IEEE CVPR 2019
- **Journal**: None
- **Summary**: Deep metric learning algorithms have been utilized to learn discriminative and generalizable models which are effective for classifying unseen classes. In this paper, a novel noise tolerant deep metric learning algorithm is proposed. The proposed method, termed as Density Aware Metric Learning, enforces the model to learn embeddings that are pulled towards the most dense region of the clusters for each class. It is achieved by iteratively shifting the estimate of the center towards the dense region of the cluster thereby leading to faster convergence and higher generalizability. In addition to this, the approach is robust to noisy samples in the training data, often present as outliers. Detailed experiments and analysis on two challenging cross-modal face recognition databases and two popular object recognition databases exhibit the efficacy of the proposed approach. It has superior convergence, requires lesser training time, and yields better accuracies than several popular deep metric learning methods.



### Multi-view Vector-valued Manifold Regularization for Multi-label Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.03921v1
- **DOI**: 10.1109/TNNLS.2013.2238682
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03921v1)
- **Published**: 2019-04-08 09:57:18+00:00
- **Updated**: 2019-04-08 09:57:18+00:00
- **Authors**: Yong Luo, Dacheng Tao, Chang Xu, Chao Xu, Hong Liu, Yonggang Wen
- **Comment**: None
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems (Volume:
  24, Issue: 5, May 2013)
- **Summary**: In computer vision, image datasets used for classification are naturally associated with multiple labels and comprised of multiple views, because each image may contain several objects (e.g. pedestrian, bicycle and tree) and is properly characterized by multiple visual features (e.g. color, texture and shape). Currently available tools ignore either the label relationship or the view complementary. Motivated by the success of the vector-valued function that constructs matrix-valued kernels to explore the multi-label structure in the output space, we introduce multi-view vector-valued manifold regularization (MV$\mathbf{^3}$MR) to integrate multiple features. MV$\mathbf{^3}$MR exploits the complementary property of different features and discovers the intrinsic local geometry of the compact support shared by different features under the theme of manifold regularization. We conducted extensive experiments on two challenging, but popular datasets, PASCAL VOC' 07 (VOC) and MIR Flickr (MIR), and validated the effectiveness of the proposed MV$\mathbf{^3}$MR for image classification.



### Wasserstein Adversarial Regularization (WAR) on label noise
- **Arxiv ID**: http://arxiv.org/abs/1904.03936v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.03936v3)
- **Published**: 2019-04-08 10:28:12+00:00
- **Updated**: 2021-06-29 07:45:32+00:00
- **Authors**: Kilian Fatras, Bharath Bhushan Damodaran, Sylvain Lobry, Rémi Flamary, Devis Tuia, Nicolas Courty
- **Comment**: In Press, IEEE Transactions on Pattern Analysis and Machine
  Intelligence (PAMI)
- **Journal**: None
- **Summary**: Noisy labels often occur in vision datasets, especially when they are obtained from crowdsourcing or Web scraping. We propose a new regularization method, which enables learning robust classifiers in presence of noisy data. To achieve this goal, we propose a new adversarial regularization scheme based on the Wasserstein distance. Using this distance allows taking into account specific relations between classes by leveraging the geometric properties of the labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a selective regularization, which promotes smoothness of the classifier between some classes, while preserving sufficient complexity of the decision boundary between others. We first discuss how and why adversarial regularization can be used in the context of label noise and then show the effectiveness of our method on five datasets corrupted with noisy labels: in both benchmarks and real datasets, WAR outperforms the state-of-the-art competitors.



### Minimal Solvers for Mini-Loop Closures in 3D Multi-Scan Alignment
- **Arxiv ID**: http://arxiv.org/abs/1904.03941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03941v1)
- **Published**: 2019-04-08 10:46:53+00:00
- **Updated**: 2019-04-08 10:46:53+00:00
- **Authors**: Pedro Miraldo, Surojit Saha, Srikumar Ramalingam
- **Comment**: 10 pages, 5 figures, 5 tables
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019
- **Summary**: 3D scan registration is a classical, yet a highly useful problem in the context of 3D sensors such as Kinect and Velodyne. While there are several existing methods, the techniques are usually incremental where adjacent scans are registered first to obtain the initial poses, followed by motion averaging and bundle-adjustment refinement. In this paper, we take a different approach and develop minimal solvers for jointly computing the initial poses of cameras in small loops such as 3-, 4-, and 5-cycles. Note that the classical registration of 2 scans can be done using a minimum of 3 point matches to compute 6 degrees of relative motion. On the other hand, to jointly compute the 3D registrations in n-cycles, we take 2 point matches between the first n-1 consecutive pairs (i.e., Scan 1 & Scan 2, ... , and Scan n-1 & Scan n) and 1 or 2 point matches between Scan 1 and Scan n. Overall, we use 5, 7, and 10 point matches for 3-, 4-, and 5-cycles, and recover 12, 18, and 24 degrees of transformation variables, respectively. Using simulations and real-data we show that the 3D registration using mini n-cycles are computationally efficient, and can provide alternate and better initial poses compared to standard pairwise methods.



### Variational Uncalibrated Photometric Stereo under General Lighting
- **Arxiv ID**: http://arxiv.org/abs/1904.03942v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03942v2)
- **Published**: 2019-04-08 10:48:53+00:00
- **Updated**: 2019-08-27 18:38:49+00:00
- **Authors**: Bjoern Haefner, Zhenzhang Ye, Maolin Gao, Tao Wu, Yvain Quéau, Daniel Cremers
- **Comment**: Haefner and Ye contributed equally
- **Journal**: The IEEE International Conference on Computer Vision (ICCV), 2019
- **Summary**: Photometric stereo (PS) techniques nowadays remain constrained to an ideal laboratory setup where modeling and calibration of lighting is amenable. To eliminate such restrictions, we propose an efficient principled variational approach to uncalibrated PS under general illumination. To this end, the Lambertian reflectance model is approximated through a spherical harmonic expansion, which preserves the spatial invariance of the lighting. The joint recovery of shape, reflectance and illumination is then formulated as a single variational problem. There the shape estimation is carried out directly in terms of the underlying perspective depth map, thus implicitly ensuring integrability and bypassing the need for a subsequent normal integration. To tackle the resulting nonconvex problem numerically, we undertake a two-phase procedure to initialize a balloon-like perspective depth map, followed by a "lagged" block coordinate descent scheme. The experiments validate efficiency and robustness of this approach. Across a variety of evaluations, we are able to reduce the mean angular error consistently by a factor of 2-3 compared to the state-of-the-art.



### Improving Image Classification Robustness through Selective CNN-Filters Fine-Tuning
- **Arxiv ID**: http://arxiv.org/abs/1904.03949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.03949v1)
- **Published**: 2019-04-08 11:02:05+00:00
- **Updated**: 2019-04-08 11:02:05+00:00
- **Authors**: Alessandro Bianchi, Moreno Raimondo Vendra, Pavlos Protopapas, Marco Brambilla
- **Comment**: arXiv admin note: text overlap with arXiv:1705.02406 by other authors
- **Journal**: None
- **Summary**: Image quality plays a big role in CNN-based image classification performance. Fine-tuning the network with distorted samples may be too costly for large networks. To solve this issue, we propose a transfer learning approach optimized to keep into account that in each layer of a CNN some filters are more susceptible to image distortion than others. Our method identifies the most susceptible filters and applies retraining only to the filters that show the highest activation maps distance between clean and distorted images. Filters are ranked using the Borda count election method and then only the most affected filters are fine-tuned. This significantly reduces the number of parameters to retrain. We evaluate this approach on the CIFAR-10 and CIFAR-100 datasets, testing it on two different models and two different types of distortion. Results show that the proposed transfer learning technique recovers most of the lost performance due to input data distortion, at a considerably faster pace with respect to existing methods, thanks to the reduced number of parameters to fine-tune. When few noisy samples are provided for training, our filter-level fine tuning performs particularly well, also outperforming state of the art layer-level transfer learning approaches.



### Kervolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.03955v2
- **DOI**: 10.1109/CVPR.2019.00012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03955v2)
- **Published**: 2019-04-08 11:10:51+00:00
- **Updated**: 2020-01-06 21:27:01+00:00
- **Authors**: Chen Wang, Jianfei Yang, Lihua Xie, Junsong Yuan
- **Comment**: oral paper in CVPR 2019
- **Journal**: 2019 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR 2019) 31-40
- **Summary**: Convolutional neural networks (CNNs) have enabled the state-of-the-art performance in many computer vision tasks. However, little effort has been devoted to establishing convolution in non-linear space. Existing works mainly leverage on the activation layers, which can only provide point-wise non-linearity. To solve this problem, a new operation, kervolution (kernel convolution), is introduced to approximate complex behaviors of human perception systems leveraging on the kernel trick. It generalizes convolution, enhances the model capacity, and captures higher order interactions of features, via patch-wise kernel functions, but without introducing additional parameters. Extensive experiments show that kervolutional neural networks (KNN) achieve higher accuracy and faster convergence than baseline CNN.



### Filter Pruning by Switching to Neighboring CNNs with Good Attributes
- **Arxiv ID**: http://arxiv.org/abs/1904.03961v2
- **DOI**: 10.1109/TNNLS.2022.3149332
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03961v2)
- **Published**: 2019-04-08 11:24:21+00:00
- **Updated**: 2022-02-11 14:42:41+00:00
- **Authors**: Yang He, Ping Liu, Linchao Zhu, Yi Yang
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Filter pruning is effective to reduce the computational costs of neural networks. Existing methods show that updating the previous pruned filter would enable large model capacity and achieve better performance. However, during the iterative pruning process, even if the network weights are updated to new values, the pruning criterion remains the same. In addition, when evaluating the filter importance, only the magnitude information of the filters is considered. However, in neural networks, filters do not work individually, but they would affect other filters. As a result, the magnitude information of each filter, which merely reflects the information of an individual filter itself, is not enough to judge the filter importance. To solve the above problems, we propose Meta-attribute-based Filter Pruning (MFP). First, to expand the existing magnitude information based pruning criteria, we introduce a new set of criteria to consider the geometric distance of filters. Additionally, to explicitly assess the current state of the network, we adaptively select the most suitable criteria for pruning via a meta-attribute, a property of the neural network at the current state. Experiments on two image classification benchmarks validate our method. For ResNet-50 on ILSVRC-2012, we could reduce more than 50% FLOPs with only 0.44% top-5 accuracy loss.



### Adaptive Morphological Reconstruction for Seeded Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.03973v1
- **DOI**: 10.1109/TIP.2019.2920514
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03973v1)
- **Published**: 2019-04-08 11:56:07+00:00
- **Updated**: 2019-04-08 11:56:07+00:00
- **Authors**: Tao Lei, Xiaohong Jia, Tongliang Liu, Shigang Liu, Hongying Meng, Asoke K. Nandi
- **Comment**: None
- **Journal**: None
- **Summary**: Morphological reconstruction (MR) is often employed by seeded image segmentation algorithms such as watershed transform and power watershed as it is able to filter seeds (regional minima) to reduce over-segmentation. However, MR might mistakenly filter meaningful seeds that are required for generating accurate segmentation and it is also sensitive to the scale because a single-scale structuring element is employed. In this paper, a novel adaptive morphological reconstruction (AMR) operation is proposed that has three advantages. Firstly, AMR can adaptively filter useless seeds while preserving meaningful ones. Secondly, AMR is insensitive to the scale of structuring elements because multiscale structuring elements are employed. Finally, AMR has two attractive properties: monotonic increasingness and convergence that help seeded segmentation algorithms to achieve a hierarchical segmentation. Experiments clearly demonstrate that AMR is useful for improving algorithms of seeded image segmentation and seed-based spectral segmentation. Compared to several state-of-the-art algorithms, the proposed algorithms provide better segmentation results requiring less computing time. Source code is available at https://github.com/SUST-reynole/AMR.



### VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1904.03977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03977v1)
- **Published**: 2019-04-08 12:02:03+00:00
- **Updated**: 2019-04-08 12:02:03+00:00
- **Authors**: Divyam Madaan, Radhika Dua, Prerana Mukherjee, Brejesh Lall
- **Comment**: None
- **Journal**: None
- **Summary**: Air pollution is the leading environmental health hazard globally due to various sources which include factory emissions, car exhaust and cooking stoves. As a precautionary measure, air pollution forecast serves as the basis for taking effective pollution control measures, and accurate air pollution forecasting has become an important task. In this paper, we forecast fine-grained ambient air quality information for 5 prominent locations in Delhi based on the historical and real-time ambient air quality and meteorological data reported by Central Pollution Control board. We present VayuAnukulani system, a novel end-to-end solution to predict air quality for next 24 hours by estimating the concentration and level of different air pollutants including nitrogen dioxide ($NO_2$), particulate matter ($PM_{2.5}$ and $PM_{10}$) for Delhi. Extensive experiments on data sources obtained in Delhi demonstrate that the proposed adaptive attention based Bidirectional LSTM Network outperforms several baselines for classification and regression models. The accuracy of the proposed adaptive system is $\sim 15 - 20\%$ better than the same offline trained model. We compare the proposed methodology on several competing baselines, and show that the network outperforms conventional methods by $\sim 3 - 5 \%$.



### Simultaneous Spectral-Spatial Feature Selection and Extraction for Hyperspectral Images
- **Arxiv ID**: http://arxiv.org/abs/1904.03982v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03982v1)
- **Published**: 2019-04-08 12:05:59+00:00
- **Updated**: 2019-04-08 12:05:59+00:00
- **Authors**: Lefei Zhang, Qian Zhang, Bo Du, Xin Huang, Yuan Yan Tang, Dacheng Tao
- **Comment**: None
- **Journal**: None
- **Summary**: In hyperspectral remote sensing data mining, it is important to take into account of both spectral and spatial information, such as the spectral signature, texture feature and morphological property, to improve the performances, e.g., the image classification accuracy. In a feature representation point of view, a nature approach to handle this situation is to concatenate the spectral and spatial features into a single but high dimensional vector and then apply a certain dimension reduction technique directly on that concatenated vector before feed it into the subsequent classifier. However, multiple features from various domains definitely have different physical meanings and statistical properties, and thus such concatenation hasn't efficiently explore the complementary properties among different features, which should benefit for boost the feature discriminability. Furthermore, it is also difficult to interpret the transformed results of the concatenated vector. Consequently, finding a physically meaningful consensus low dimensional feature representation of original multiple features is still a challenging task. In order to address the these issues, we propose a novel feature learning framework, i.e., the simultaneous spectral-spatial feature selection and extraction algorithm, for hyperspectral images spectral-spatial feature representation and classification. Specifically, the proposed method learns a latent low dimensional subspace by projecting the spectral-spatial feature into a common feature space, where the complementary information has been effectively exploited, and simultaneously, only the most significant original features have been transformed. Encouraging experimental results on three public available hyperspectral remote sensing datasets confirm that our proposed method is effective and efficient.



### Weakly Supervised Semantic Segmentation of Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1904.03983v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.03983v1)
- **Published**: 2019-04-08 12:09:11+00:00
- **Updated**: 2019-04-08 12:09:11+00:00
- **Authors**: Adrien Nivaggioli, Hicham Randrianarivo
- **Comment**: None
- **Journal**: Joint Urban Remote Sensing Event (JURSE), May 2019, Vannes, France
- **Summary**: When one wants to train a neural network to perform semantic segmentation, creating pixel-level annotations for each of the images in the database is a tedious task. If he works with aerial or satellite images, which are usually very large, it is even worse. With that in mind, we investigate how to use image-level annotations in order to perform semantic segmentation. Image-level annotations are much less expensive to acquire than pixel-level annotations, but we lose a lot of information for the training of the model. From the annotations of the images, the model must find by itself how to classify the different regions of the image. In this work, we use the method proposed by Anh and Kwak [1] to produce pixel-level annotation from image level annotation. We compare the overall quality of our generated dataset with the original dataset. In addition, we propose an adaptation of the AffinityNet that allows us to directly perform a semantic segmentation. Our results show that the generated labels lead to the same performances for the training of several segmentation networks. Also, the quality of semantic segmentation performed directly by the AffinityNet and the Random Walk is close to the one of the best fully-supervised approaches.



### Nucleus Neural Network: A Data-driven Self-organized Architecture
- **Arxiv ID**: http://arxiv.org/abs/1904.04036v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.04036v2)
- **Published**: 2019-04-08 13:03:50+00:00
- **Updated**: 2019-05-14 14:06:05+00:00
- **Authors**: Jia Liu, Maoguo Gong, Haibo He
- **Comment**: None
- **Journal**: None
- **Summary**: Artificial neural networks which are inspired from the learning mechanism of brain have achieved great successes in many problems, especially those with deep layers. In this paper, we propose a nucleus neural network (NNN) and corresponding connecting architecture learning method. In a nucleus, there are no regular layers, i.e., a neuron may connect to all the neurons in the nucleus. This type of architecture gets rid of layer limitation and may lead to more powerful learning capability. It is crucial to determine the connections between them given numerous neurons. Based on the principle that more relevant input and output neuron pair deserves higher connecting density, we propose an efficient architecture learning model for the nucleus. Moreover, we improve the learning method for connecting weights and biases given the optimized architecture. We find that this novel architecture is robust to irrelevant components in test data. So we reconstruct a new dataset based on the MNIST dataset where the types of digital backgrounds in training and test sets are different. Experiments demonstrate that the proposed learner achieves significant improvement over traditional learners on the reconstructed data set.



### Transferring Knowledge Fragments for Learning Distance Metric from A Heterogeneous Domain
- **Arxiv ID**: http://arxiv.org/abs/1904.04061v1
- **DOI**: 10.1109/TPAMI.2018.2824309
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04061v1)
- **Published**: 2019-04-08 13:44:22+00:00
- **Updated**: 2019-04-08 13:44:22+00:00
- **Authors**: Yong Luo, Yonggang Wen, Tongliang Liu, Dacheng Tao
- **Comment**: None
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (Volume: 41, Issue: 4, April 1 2019)
- **Summary**: The goal of transfer learning is to improve the performance of target learning task by leveraging information (or transferring knowledge) from other related tasks. In this paper, we examine the problem of transfer distance metric learning (DML), which usually aims to mitigate the label information deficiency issue in the target DML. Most of the current Transfer DML (TDML) methods are not applicable to the scenario where data are drawn from heterogeneous domains. Some existing heterogeneous transfer learning (HTL) approaches can learn target distance metric by usually transforming the samples of source and target domain into a common subspace. However, these approaches lack flexibility in real-world applications, and the learned transformations are often restricted to be linear. This motivates us to develop a general flexible heterogeneous TDML (HTDML) framework. In particular, any (linear/nonlinear) DML algorithms can be employed to learn the source metric beforehand. Then the pre-learned source metric is represented as a set of knowledge fragments to help target metric learning. We show how generalization error in the target domain could be reduced using the proposed transfer strategy, and develop novel algorithm to learn either linear or nonlinear target metric. Extensive experiments on various applications demonstrate the effectiveness of the proposed method.



### ContextDesc: Local Descriptor Augmentation with Cross-Modality Context
- **Arxiv ID**: http://arxiv.org/abs/1904.04084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04084v1)
- **Published**: 2019-04-08 14:12:36+00:00
- **Updated**: 2019-04-08 14:12:36+00:00
- **Authors**: Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan
- **Comment**: Accepted to CVPR 2019 (oral), supplementary materials included.
  (https://github.com/lzx551402/contextdesc)
- **Journal**: None
- **Summary**: Most existing studies on learning local features focus on the patch-based descriptions of individual keypoints, whereas neglecting the spatial relations established from their keypoint locations. In this paper, we go beyond the local detail representation by introducing context awareness to augment off-the-shelf local feature descriptors. Specifically, we propose a unified learning framework that leverages and aggregates the cross-modality contextual information, including (i) visual context from high-level image representation, and (ii) geometric context from 2D keypoint distribution. Moreover, we propose an effective N-pair loss that eschews the empirical hyper-parameter search and improves the convergence. The proposed augmentation scheme is lightweight compared with the raw local feature description, meanwhile improves remarkably on several large-scale benchmarks with diversified scenes, which demonstrates both strong practicality and generalization ability in geometric matching applications.



### Large Margin Multi-modal Multi-task Feature Extraction for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.04088v1
- **DOI**: 10.1109/TIP.2015.2495116
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04088v1)
- **Published**: 2019-04-08 14:14:19+00:00
- **Updated**: 2019-04-08 14:14:19+00:00
- **Authors**: Yong Luo, Yonggang Wen, Dacheng Tao, Jie Gui, Chao Xu
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing (Volume: 25, Issue: 1, Jan.
  2016)
- **Summary**: The features used in many image analysis-based applications are frequently of very high dimension. Feature extraction offers several advantages in high-dimensional cases, and many recent studies have used multi-task feature extraction approaches, which often outperform single-task feature extraction approaches. However, most of these methods are limited in that they only consider data represented by a single type of feature, even though features usually represent images from multiple modalities. We therefore propose a novel large margin multi-modal multi-task feature extraction (LM3FE) framework for handling multi-modal features for image classification. In particular, LM3FE simultaneously learns the feature extraction matrix for each modality and the modality combination coefficients. In this way, LM3FE not only handles correlated and noisy features, but also utilizes the complementarity of different modalities to further help reduce feature redundancy in each modality. The large margin principle employed also helps to extract strongly predictive features so that they are more suitable for prediction (e.g., classification). An alternating algorithm is developed for problem optimization and each sub-problem can be efficiently solved. Experiments on two challenging real-world image datasets demonstrate the effectiveness and superiority of the proposed method.



### Leveraging the Invariant Side of Generative Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04092v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04092v1)
- **Published**: 2019-04-08 14:28:24+00:00
- **Updated**: 2019-04-08 14:28:24+00:00
- **Authors**: Jingjing Li, Mengmeng Jin, Ke Lu, Zhengming Ding, Lei Zhu, Zi Huang
- **Comment**: Accepted in CVPR 2019
- **Journal**: None
- **Summary**: Conventional zero-shot learning (ZSL) methods generally learn an embedding, e.g., visual-semantic mapping, to handle the unseen visual samples via an indirect manner. In this paper, we take the advantage of generative adversarial networks (GANs) and propose a novel method, named leveraging invariant side GAN (LisGAN), which can directly generate the unseen features from random noises which are conditioned by the semantic descriptions. Specifically, we train a conditional Wasserstein GANs in which the generator synthesizes fake unseen features from noises and the discriminator distinguishes the fake from real via a minimax game. Considering that one semantic description can correspond to various synthesized visual samples, and the semantic description, figuratively, is the soul of the generated features, we introduce soul samples as the invariant side of generative zero-shot learning in this paper. A soul sample is the meta-representation of one class. It visualizes the most semantically-meaningful aspects of each sample in the same category. We regularize that each generated sample (the varying side of generative ZSL) should be close to at least one soul sample (the invariant side) which has the same class label with it. At the zero-shot recognition stage, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. Experiments on five popular benchmarks verify that our proposed approach can outperform state-of-the-art methods with significant improvements.



### Weighted Point Cloud Augmentation for Neural Network Training Data Class-Imbalance
- **Arxiv ID**: http://arxiv.org/abs/1904.04094v2
- **DOI**: 10.5194/isprs-archives-XLII-2-W13-981-2019
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04094v2)
- **Published**: 2019-04-08 14:32:27+00:00
- **Updated**: 2019-04-09 07:31:37+00:00
- **Authors**: David Griffiths, Jan Boehm
- **Comment**: 7 pages, 6 figures, submitted for ISPRS Geospatial Week conference
  2019
- **Journal**: None
- **Summary**: Recent developments in the field of deep learning for 3D data have demonstrated promising potential for end-to-end learning directly from point clouds. However, many real-world point clouds contain a large class im-balance due to the natural class im-balance observed in nature. For example, a 3D scan of an urban environment will consist mostly of road and facade, whereas other objects such as poles will be under-represented. In this paper we address this issue by employing a weighted augmentation to increase classes that contain fewer points. By mitigating the class im-balance present in the data we demonstrate that a standard PointNet++ deep neural network can achieve higher performance at inference on validation data. This was observed as an increase of F1 score of 19% and 25% on two test benchmark datasets; ScanNet and Semantic3D respectively where no class im-balance pre-processing had been performed. Our networks performed better on both highly-represented and under-represented classes, which indicates that the network is learning more robust and meaningful features when the loss function is not overly exposed to only a few classes.



### Learning monocular depth estimation infusing traditional stereo knowledge
- **Arxiv ID**: http://arxiv.org/abs/1904.04144v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04144v1)
- **Published**: 2019-04-08 15:59:07+00:00
- **Updated**: 2019-04-08 15:59:07+00:00
- **Authors**: Fabio Tosi, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia
- **Comment**: accepted at CVPR 2019. Code available at
  https://github.com/fabiotosi92/monoResMatch-Tensorflow
- **Journal**: None
- **Summary**: Depth estimation from a single image represents a fascinating, yet challenging problem with countless applications. Recent works proved that this task could be learned without direct supervision from ground truth labels leveraging image synthesis on sequences or stereo pairs. Focusing on this second case, in this paper we leverage stereo matching in order to improve monocular depth estimation. To this aim we propose monoResMatch, a novel deep architecture designed to infer depth from a single input image by synthesizing features from a different point of view, horizontally aligned with the input image, performing stereo matching between the two cues. In contrast to previous works sharing this rationale, our network is the first trained end-to-end from scratch. Moreover, we show how obtaining proxy ground truth annotation through traditional stereo algorithms, such as Semi-Global Matching, enables more accurate monocular depth estimation still countering the need for expensive depth labels by keeping a self-supervised approach. Exhaustive experimental results prove how the synergy between i) the proposed monoResMatch architecture and ii) proxy-supervision attains state-of-the-art for self-supervised monocular depth estimation. The code is publicly available at https://github.com/fabiotosi92/monoResMatch-Tensorflow.



### Revisiting EmbodiedQA: A Simple Baseline and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1904.04166v2
- **DOI**: 10.1109/TIP.2020.2967584
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04166v2)
- **Published**: 2019-04-08 16:23:24+00:00
- **Updated**: 2020-09-04 15:19:25+00:00
- **Authors**: Yu Wu, Lu Jiang, Yi Yang
- **Comment**: Accepted to IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: In Embodied Question Answering (EmbodiedQA), an agent interacts with an environment to gather necessary information for answering user questions. Existing works have laid a solid foundation towards solving this interesting problem. But the current performance, especially in navigation, suggests that EmbodiedQA might be too challenging for the contemporary approaches. In this paper, we empirically study this problem and introduce 1) a simple yet effective baseline that achieves promising performance; 2) an easier and practical setting for EmbodiedQA where an agent has a chance to adapt the trained model to a new environment before it actually answers users questions. In this new setting, we randomly place a few objects in new environments, and upgrade the agent policy by a distillation network to retain the generalization ability from the trained model. On the EmbodiedQA v1 benchmark, under the standard setting, our simple baseline achieves very competitive results to the-state-of-the-art; in the new setting, we found the introduced small change in settings yields a notable gain in navigation.



### Data-Driven Design for Fourier Ptychographic Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1904.04175v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04175v1)
- **Published**: 2019-04-08 16:30:21+00:00
- **Updated**: 2019-04-08 16:30:21+00:00
- **Authors**: Michael Kellman, Emrah Bostan, Michael Chen, Laura Waller
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Fourier Ptychographic Microscopy (FPM) is a computational imaging method that is able to super-resolve features beyond the diffraction-limit set by the objective lens of a traditional microscope. This is accomplished by using synthetic aperture and phase retrieval algorithms to combine many measurements captured by an LED array microscope with programmable source patterns. FPM provides simultaneous large field-of-view and high resolution imaging, but at the cost of reduced temporal resolution, thereby limiting live cell applications. In this work, we learn LED source pattern designs that compress the many required measurements into only a few, with negligible loss in reconstruction quality or resolution. This is accomplished by recasting the super-resolution reconstruction as a Physics-based Neural Network and learning the experimental design to optimize the network's overall performance. Specifically, we learn LED patterns for different applications (e.g. amplitude contrast and quantitative phase imaging) and show that the designs we learn through simulation generalize well in the experimental setting. Further, we discuss a context-specific loss function, practical memory limitations, and interpretability of our learned designs.



### Unsupervised learning of action classes with continuous temporal embedding
- **Arxiv ID**: http://arxiv.org/abs/1904.04189v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04189v1)
- **Published**: 2019-04-08 17:05:31+00:00
- **Updated**: 2019-04-08 17:05:31+00:00
- **Authors**: Anna Kukleva, Hilde Kuehne, Fadime Sener, Juergen Gall
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: The task of temporally detecting and segmenting actions in untrimmed videos has seen an increased attention recently. One problem in this context arises from the need to define and label action boundaries to create annotations for training which is very time and cost intensive. To address this issue, we propose an unsupervised approach for learning action classes from untrimmed video sequences. To this end, we use a continuous temporal embedding of framewise features to benefit from the sequential nature of activities. Based on the latent space created by the embedding, we identify clusters of temporal segments across all videos that correspond to semantic meaningful action classes. The approach is evaluated on three challenging datasets, namely the Breakfast dataset, YouTube Instructions, and the 50Salads dataset. While previous works assumed that the videos contain the same high level activity, we furthermore show that the proposed approach can also be applied to a more general setting where the content of the videos is unknown.



### Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout
- **Arxiv ID**: http://arxiv.org/abs/1904.04195v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04195v1)
- **Published**: 2019-04-08 17:14:52+00:00
- **Updated**: 2019-04-08 17:14:52+00:00
- **Authors**: Hao Tan, Licheng Yu, Mohit Bansal
- **Comment**: NAACL 2019 (12 pages)
- **Journal**: None
- **Summary**: A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced 'unseen' triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective 'environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropped-out environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.



### Pushing the Envelope for RGB-based Dense 3D Hand Pose Estimation via Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/1904.04196v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04196v2)
- **Published**: 2019-04-08 17:15:55+00:00
- **Updated**: 2019-04-09 15:57:08+00:00
- **Authors**: Seungryul Baek, Kwang In Kim, Tae-Kyun Kim
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Estimating 3D hand meshes from single RGB images is challenging, due to intrinsic 2D-3D mapping ambiguities and limited training data. We adopt a compact parametric 3D hand model that represents deformable and articulated hand meshes. To achieve the model fitting to RGB images, we investigate and contribute in three ways: 1) Neural rendering: inspired by recent work on human body, our hand mesh estimator (HME) is implemented by a neural network and a differentiable renderer, supervised by 2D segmentation masks and 3D skeletons. HME demonstrates good performance for estimating diverse hand shapes and improves pose estimation accuracies. 2) Iterative testing refinement: Our fitting function is differentiable. We iteratively refine the initial estimate using the gradients, in the spirit of iterative model fitting methods like ICP. The idea is supported by the latest research on human body. 3) Self-data augmentation: collecting sized RGB-mesh (or segmentation mask)-skeleton triplets for training is a big hurdle. Once the model is successfully fitted to input RGB images, its meshes i.e. shapes and articulations, are realistic, and we augment view-points on top of estimated dense hand poses. Experiments using three RGB-based benchmarks show that our framework offers beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand shapes. Each technical component above meaningfully improves the accuracy in the ablation study.



### Dynamics of Pedestrian Crossing Decisions Based on Vehicle Trajectories in Large-Scale Simulated and Real-World Data
- **Arxiv ID**: http://arxiv.org/abs/1904.04202v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04202v1)
- **Published**: 2019-04-08 17:19:54+00:00
- **Updated**: 2019-04-08 17:19:54+00:00
- **Authors**: Jack Terwilliger, Michael Glazer, Henri Schmidt, Josh Domeyer, Heishiro Toyoda, Bruce Mehler, Bryan Reimer, Lex Fridman
- **Comment**: Will appear in Proceedings of 2019 Driving Assessment Conference
- **Journal**: None
- **Summary**: Humans, as both pedestrians and drivers, generally skillfully navigate traffic intersections. Despite the uncertainty, danger, and the non-verbal nature of communication commonly found in these interactions, there are surprisingly few collisions considering the total number of interactions. As the role of automation technology in vehicles grows, it becomes increasingly critical to understand the relationship between pedestrian and driver behavior: how pedestrians perceive the actions of a vehicle/driver and how pedestrians make crossing decisions. The relationship between time-to-arrival (TTA) and pedestrian gap acceptance (i.e., whether a pedestrian chooses to cross under a given window of time to cross) has been extensively investigated. However, the dynamic nature of vehicle trajectories in the context of non-verbal communication has not been systematically explored. Our work provides evidence that trajectory dynamics, such as changes in TTA, can be powerful signals in the non-verbal communication between drivers and pedestrians. Moreover, we investigate these effects in both simulated and real-world datasets, both larger than have previously been considered in literature to the best of our knowledge.



### Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions
- **Arxiv ID**: http://arxiv.org/abs/1904.04205v5
- **DOI**: 10.23919/EUSIPCO55093.2022.9909927
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04205v5)
- **Published**: 2019-04-08 17:25:46+00:00
- **Updated**: 2022-03-08 19:53:58+00:00
- **Authors**: Hoel Kervadec, Jose Dolz, Jing Yuan, Christian Desrosiers, Eric Granger, Ismail Ben Ayed
- **Comment**: None
- **Journal**: 30th European Signal Processing Conference (EUSIPCO), Belgrade,
  Serbia, 2022, pp. 962-966
- **Summary**: This study investigates imposing hard inequality constraints on the outputs of convolutional neural networks (CNN) during training. Several recent works showed that the theoretical and practical advantages of Lagrangian optimization over simple penalties do not materialize in practice when dealing with modern CNNs involving millions of parameters. Therefore, constrained CNNs are typically handled with penalties. We propose *log-barrier extensions*, which approximate Lagrangian optimization of constrained-CNN problems with a sequence of unconstrained losses. Unlike standard interior-point and log-barrier methods, our formulation does not need an initial feasible solution. The proposed extension yields an upper bound on the duality gap -- generalizing the result of standard log-barriers -- and yielding sub-optimality certificates for feasible solutions. While sub-optimality is not guaranteed for non-convex problems, this result shows that log-barrier extensions are a principled way to approximate Lagrangian optimization for constrained CNNs via implicit dual variables. We report weakly supervised image segmentation experiments, with various constraints, showing that our formulation outperforms substantially the existing constrained-CNN methods, in terms of accuracy, constraint satisfaction and training stability, more so when dealing with a large number of constraints.



### Least-squares registration of point sets over SE (d) using closed-form projections
- **Arxiv ID**: http://arxiv.org/abs/1904.04218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04218v2)
- **Published**: 2019-04-08 17:38:37+00:00
- **Updated**: 2019-04-12 10:12:50+00:00
- **Authors**: Sk. Miraj Ahmed, Niladri Ranjan Das, Kunal Narayan Chaudhury
- **Comment**: To appear in Computer Vision and Image Understanding (Elsevier)
- **Journal**: None
- **Summary**: Consider the problem of registering multiple point sets in some $d$-dimensional space using rotations and translations. Assume that there are sets with common points, and moreover the pairwise correspondences are known for such sets. We consider a least-squares formulation of this problem, where the variables are the transforms associated with the point sets. The present novelty is that we reduce this nonconvex problem to an optimization over the positive semidefinite cone, where the objective is linear but the constraints are nevertheless nonconvex. We propose to solve this using variable splitting and the alternating directions method of multipliers (ADMM). Due to the linearity of the objective and the structure of constraints, the ADMM subproblems are given by projections with closed-form solutions. In particular, for $m$ point sets, the dominant cost per iteration is the partial eigendecomposition of an $md \times md$ matrix, and $m-1$ singular value decompositions of $d \times d$ matrices. We empirically show that for appropriate parameter settings, the proposed solver has a large convergence basin and is stable under perturbations. As applications, we use our method for $2$D shape matching and $3$D multiview registration. In either application, we model the shapes/scans as point sets and determine the pairwise correspondences using ICP. In particular, our algorithm compares favorably with existing methods for multiview reconstruction in terms of timing and accuracy.



### Relational Action Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1904.04231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04231v1)
- **Published**: 2019-04-08 17:57:27+00:00
- **Updated**: 2019-04-08 17:57:27+00:00
- **Authors**: Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, Cordelia Schmid
- **Comment**: CVPR 2019 (oral)
- **Journal**: None
- **Summary**: This paper focuses on multi-person action forecasting in videos. More precisely, given a history of H previous frames, the goal is to detect actors and to predict their future actions for the next T frames. Our approach jointly models temporal and spatial interactions among different actors by constructing a recurrent graph, using actor proposals obtained with Faster R-CNN as nodes. Our method learns to select a subset of discriminative relations without requiring explicit supervision, thus enabling us to tackle challenging visual data. We refer to our model as Discriminative Relational Recurrent Network (DRRN). Evaluation of action prediction on AVA demonstrates the effectiveness of our proposed method compared to simpler baselines. Furthermore, we significantly improve performance on the task of early action classification on J-HMDB, from the previous SOTA of 48% to 60%.



### A Closer Look at Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1904.04232v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04232v2)
- **Published**: 2019-04-08 17:59:07+00:00
- **Updated**: 2020-01-12 16:25:06+00:00
- **Authors**: Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, Jia-Bin Huang
- **Comment**: ICLR 2019. Code: https://github.com/wyharveychen/CloserLookFewShot .
  Project: https://sites.google.com/view/a-closer-look-at-few-shot/
- **Journal**: None
- **Summary**: Few-shot classification aims to learn a classifier to recognize unseen classes during training with limited labeled examples. While significant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difficult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the \miniI and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic cross-domain evaluation setting, we show that a baseline method with a standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms.



### SoDeep: a Sorting Deep net to learn ranking loss surrogates
- **Arxiv ID**: http://arxiv.org/abs/1904.04272v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04272v1)
- **Published**: 2019-04-08 18:02:43+00:00
- **Updated**: 2019-04-08 18:02:43+00:00
- **Authors**: Martin Engilberge, Louis Chevallier, Patrick Pérez, Matthieu Cord
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Several tasks in machine learning are evaluated using non-differentiable metrics such as mean average precision or Spearman correlation. However, their non-differentiability prevents from using them as objective functions in a learning framework. Surrogate and relaxation methods exist but tend to be specific to a given metric.   In the present work, we introduce a new method to learn approximations of such non-differentiable objective functions. Our approach is based on a deep architecture that approximates the sorting of arbitrary sets of scores. It is trained virtually for free using synthetic data. This sorting deep (SoDeep) net can then be combined in a plug-and-play manner with existing deep architectures. We demonstrate the interest of our approach in three different tasks that require ranking: Cross-modal text-image retrieval, multi-label image classification and visual memorability ranking. Our approach yields very competitive results on these three tasks, which validates the merit and the flexibility of SoDeep as a proxy for sorting operation in ranking-based losses.



### 3D Local Features for Direct Pairwise Registration
- **Arxiv ID**: http://arxiv.org/abs/1904.04281v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.04281v1)
- **Published**: 2019-04-08 18:17:36+00:00
- **Updated**: 2019-04-08 18:17:36+00:00
- **Authors**: Haowen Deng, Tolga Birdal, Slobodan Ilic
- **Comment**: To appear in CVPR 2019. 16 pages, identical to the camera ready
  submission
- **Journal**: None
- **Summary**: We present a novel, data driven approach for solving the problem of registration of two point cloud scans. Our approach is direct in the sense that a single pair of corresponding local patches already provides the necessary transformation cue for the global registration. To achieve that, we first endow the state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling, where the discrepancy between the two leads to pose-specific descriptors. Based upon this, we introduce RelativeNet, a relative pose estimation network to assign correspondence-specific orientations to the keypoints, eliminating any local reference frame computations. Finally, we devise a simple yet effective hypothesize-and-verify algorithm to quickly use the predictions and align two point sets. Our extensive quantitative and qualitative experiments suggests that our approach outperforms the state of the art in challenging real datasets of pairwise registration and that augmenting the keypoints with local pose information leads to better generalization and a dramatic speed-up.



### SCSampler: Sampling Salient Clips from Video for Efficient Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.04289v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04289v2)
- **Published**: 2019-04-08 18:28:12+00:00
- **Updated**: 2019-08-30 12:15:46+00:00
- **Authors**: Bruno Korbar, Du Tran, Lorenzo Torresani
- **Comment**: None
- **Journal**: None
- **Summary**: While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight "clip-sampling" model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly/uniformly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost.



### Neural Rerendering in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1904.04290v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.04290v1)
- **Published**: 2019-04-08 18:30:14+00:00
- **Updated**: 2019-04-08 18:30:14+00:00
- **Authors**: Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, Ricardo Martin-Brualla
- **Comment**: To be presented at CVPR 2019 (oral). Supplementary video available at
  http://youtu.be/E1crWQn_kmY
- **Journal**: None
- **Summary**: We explore total scene capture -- recording, modeling, and rerendering a scene under varying appearance such as season and time of day. Starting from internet photos of a tourist landmark, we apply traditional 3D reconstruction to register the photos and approximate the scene as a point cloud. For each photo, we render the scene points into a deep framebuffer, and train a neural network to learn the mapping of these initial renderings to the actual photos. This rerendering network also takes as input a latent appearance vector and a semantic mask indicating the location of transient objects like pedestrians. The model is evaluated on several datasets of publicly available images spanning a broad range of illumination conditions. We create short videos demonstrating realistic manipulation of the image viewpoint, appearance, and semantic labeling. We also compare results with prior work on scene reconstruction from internet photos.



### Learned 3D Shape Representations Using Fused Geometrically Augmented Images: Application to Facial Expression and Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.04297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04297v1)
- **Published**: 2019-04-08 19:05:57+00:00
- **Updated**: 2019-04-08 19:05:57+00:00
- **Authors**: Bilal Taha, Munawar Hayat, Stefano Berretti, Naoufel Werghi
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an approach to learn generic multi-modal mesh surface representations using a novel scheme for fusing texture and geometric data. Our approach defines an inverse mapping between different geometric descriptors computed on the mesh surface or its down-sampled version, and the corresponding 2D texture image of the mesh, allowing the construction of fused geometrically augmented images (FGAI). This new fused modality enables us to learn feature representations from 3D data in a highly efficient manner by simply employing standard convolutional neural networks in a transfer-learning mode. In contrast to existing methods, the proposed approach is both computationally and memory efficient, preserves intrinsic geometric information and learns highly discriminative feature representation by effectively fusing shape and texture information at data level. The efficacy of our approach is demonstrated for the tasks of facial action unit detection and expression classification. The extensive experiments conducted on the Bosphorus and BU-4DFE datasets, show that our method produces a significant boost in the performance when compared to state-of-the-art solutions



### $\mathcal{G}$-softmax: Improving Intra-class Compactness and Inter-class Separability of Features
- **Arxiv ID**: http://arxiv.org/abs/1904.04317v2
- **DOI**: 10.1109/TNNLS.2019.2909737
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04317v2)
- **Published**: 2019-04-08 19:31:25+00:00
- **Updated**: 2019-07-15 15:21:07+00:00
- **Authors**: Yan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao
- **Comment**: 15 pages, published in TNNLS
- **Journal**: IEEE Transactions on Neural Networks and Learning Systems in 2019
- **Summary**: Intra-class compactness and inter-class separability are crucial indicators to measure the effectiveness of a model to produce discriminative features, where intra-class compactness indicates how close the features with the same label are to each other and inter-class separability indicates how far away the features with different labels are. In this work, we investigate intra-class compactness and inter-class separability of features learned by convolutional networks and propose a Gaussian-based softmax ($\mathcal{G}$-softmax) function that can effectively improve intra-class compactness and inter-class separability. The proposed function is simple to implement and can easily replace the softmax function. We evaluate the proposed $\mathcal{G}$-softmax function on classification datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) and on multi-label classification datasets (i.e., MS COCO and NUS-WIDE). The experimental results show that the proposed $\mathcal{G}$-softmax function improves the state-of-the-art models across all evaluated datasets. In addition, analysis of the intra-class compactness and inter-class separability demonstrates the advantages of the proposed function over the softmax function, which is consistent with the performance improvement. More importantly, we observe that high intra-class compactness and inter-class separability are linearly correlated to average precision on MS COCO and NUS-WIDE. This implies that improvement of intra-class compactness and inter-class separability would lead to improvement of average precision.



### Automated Monitoring Cropland Using Remote Sensing Data: Challenges and Opportunities for Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.04329v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04329v1)
- **Published**: 2019-04-08 19:54:27+00:00
- **Updated**: 2019-04-08 19:54:27+00:00
- **Authors**: Xiaowei Jia, Ankush Khandelwal, Vipin Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper provides an overview of how recent advances in machine learning and the availability of data from earth observing satellites can dramatically improve our ability to automatically map croplands over long period and over large regions. It discusses three applications in the domain of crop monitoring where ML approaches are beginning to show great promise. For each application, it highlights machine learning challenges, proposed approaches, and recent results. The paper concludes with discussion of major challenges that need to be addressed before ML approaches will reach their full potential for this problem of great societal relevance.



### End-to-end Projector Photometric Compensation
- **Arxiv ID**: http://arxiv.org/abs/1904.04335v1
- **DOI**: 10.1109/CVPR.2019.00697
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.04335v1)
- **Published**: 2019-04-08 20:07:27+00:00
- **Updated**: 2019-04-08 20:07:27+00:00
- **Authors**: Bingyao Huang, Haibin Ling
- **Comment**: To appear in the 2019 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR). Source code and dataset are available at
  https://github.com/BingyaoHuang/compennet
- **Journal**: None
- **Summary**: Projector photometric compensation aims to modify a projector input image such that it can compensate for disturbance from the appearance of projection surface. In this paper, for the first time, we formulate the compensation problem as an end-to-end learning problem and propose a convolutional neural network, named CompenNet, to implicitly learn the complex compensation function. CompenNet consists of a UNet-like backbone network and an autoencoder subnet. Such architecture encourages rich multi-level interactions between the camera-captured projection surface image and the input image, and thus captures both photometric and environment information of the projection surface. In addition, the visual details and interaction information are carried to deeper layers along the multi-level skip convolution layers. The architecture is of particular importance for the projector compensation task, for which only a small training dataset is allowed in practice. Another contribution we make is a novel evaluation benchmark, which is independent of system setup and thus quantitatively verifiable. Such benchmark is not previously available, to our best knowledge, due to the fact that conventional evaluation requests the hardware system to actually project the final results. Our key idea, motivated from our end-to-end problem formulation, is to use a reasonable surrogate to avoid such projection process so as to be setup-independent. Our method is evaluated carefully on the benchmark, and the results show that our end-to-end learning solution outperforms state-of-the-arts both qualitatively and quantitatively by a significant margin.



### Quantifying the presence of graffiti in urban environments
- **Arxiv ID**: http://arxiv.org/abs/1904.04336v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04336v1)
- **Published**: 2019-04-08 20:08:09+00:00
- **Updated**: 2019-04-08 20:08:09+00:00
- **Authors**: Eric K. Tokuda, Claudio T. Silva, Roberto M. Cesar-Jr
- **Comment**: This article was presented at the IEEE Big Data and Smart Computing
  2019
- **Journal**: None
- **Summary**: Graffiti is a common phenomenon in urban scenarios. Differently from urban art, graffiti tagging is a vandalism act and many local governments are putting great effort to combat it. The graffiti map of a region can be a very useful resource because it may allow one to potentially combat vandalism in locations with high level of graffiti and also to cleanup saturated regions to discourage future acts. There is currently no automatic way of obtaining a graffiti map of a region and it is obtained by manual inspection by the police or by popular participation. In this sense, we describe an ongoing work where we propose an automatic way of obtaining a graffiti map of a neighbourhood. It consists of the systematic collection of street view images followed by the identification of graffiti tags in the collected dataset and finally, in the calculation of the proposed graffiti level of that location. We validate the proposed method by evaluating the geographical distribution of graffiti in a city known to have high concentration of graffiti -- Sao Paulo, Brazil.



### L2AE-D: Learning to Aggregate Embeddings for Few-shot Learning with Meta-level Dropout
- **Arxiv ID**: http://arxiv.org/abs/1904.04339v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.04339v1)
- **Published**: 2019-04-08 20:11:39+00:00
- **Updated**: 2019-04-08 20:11:39+00:00
- **Authors**: Heda Song, Mercedes Torres Torres, Ender Özcan, Isaac Triguero
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning focuses on learning a new visual concept with very limited labelled examples. A successful approach to tackle this problem is to compare the similarity between examples in a learned metric space based on convolutional neural networks. However, existing methods typically suffer from meta-level overfitting due to the limited amount of training tasks and do not normally consider the importance of the convolutional features of different examples within the same channel. To address these limitations, we make the following two contributions: (a) We propose a novel meta-learning approach for aggregating useful convolutional features and suppressing noisy ones based on a channel-wise attention mechanism to improve class representations. The proposed model does not require fine-tuning and can be trained in an end-to-end manner. The main novelty lies in incorporating a shared weight generation module that learns to assign different weights to the feature maps of different examples within the same channel. (b) We also introduce a simple meta-level dropout technique that reduces meta-level overfitting in several few-shot learning approaches. In our experiments, we find that this simple technique significantly improves the performance of the proposed method as well as various state-of-the-art meta-learning algorithms. Applying our method to few-shot image recognition using Omniglot and miniImageNet datasets shows that it is capable of delivering a state-of-the-art classification performance.



### What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1904.04346v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04346v2)
- **Published**: 2019-04-08 20:43:52+00:00
- **Updated**: 2019-06-14 16:46:51+00:00
- **Authors**: Paritosh Parmar, Brendan Tran Morris
- **Comment**: CVPR 2019. Dataset temporarily made available at
  https://github.com/ParitoshParmar/MTL-AQA
- **Journal**: None
- **Summary**: Can performance on the task of action quality assessment (AQA) be improved by exploiting a description of the action and its quality? Current AQA and skills assessment approaches propose to learn features that serve only one task - estimating the final score. In this paper, we propose to learn spatio-temporal features that explain three related tasks - fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach (https://github.com/ParitoshParmar/MTL-AQA). We show that our MTL approach outperforms STL approach using two different kinds of architectures: C3D-AVG and MSCADC. The C3D-AVG-MTL approach achieves the new state-of-the-art performance with a rank correlation of 90.44%. Detailed experiments were performed to show that MTL offers better generalization than STL, and representations from action recognition models are not sufficient for the AQA task and instead should be learned.



### Relational Reasoning Network (RRN) for Anatomical Landmarking
- **Arxiv ID**: http://arxiv.org/abs/1904.04354v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.04354v2)
- **Published**: 2019-04-08 20:58:47+00:00
- **Updated**: 2022-09-18 04:36:09+00:00
- **Authors**: Neslisah Torosdagli, Syed Anwar, Payal Verma, Denise K Liberton, Janice S. Lee, Wade W. Han, Ulas Bagci
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: We perform anatomical landmarking for craniomaxillofacial (CMF) bones without explicitly segmenting them. Towards this, we propose a new simple yet efficient deep network architecture, called \textit{relational reasoning network (RRN)}, to accurately learn the local and the global relations among the landmarks in CMF bones; specifically, mandible, maxilla, and nasal bones.   Approach: The proposed RRN works in an end-to-end manner, utilizing learned relations of the landmarks based on dense-block units. For a given few landmarks as input, RRN treats the landmarking process similar to a data imputation problem where predicted landmarks are considered missing.   Results: We applied RRN to cone beam computed tomography scans obtained from 250 patients. With a 4-fold cross validation technique, we obtained an average root mean squared error of less than 2 mm per landmark. Our proposed RRN has revealed unique relationships among the landmarks that help us in inferring several \textit{reasoning} about informativeness of the landmark points. The proposed system identifies the missing landmark locations accurately even when severe pathology or deformation are present in the bones.   Conclusions: Accurately identifying anatomical landmarks is a crucial step in deformation analysis and surgical planning for CMF surgeries. Achieving this goal without the need for explicit bone segmentation addresses a major limitation of segmentation based approaches, where segmentation failure (as often the case in bones with severe pathology or deformation) could easily lead to incorrect landmarking. To the best of our knowledge, this is the first of its kind algorithm finding anatomical relations of the objects using deep learning.



### Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1904.04357v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04357v1)
- **Published**: 2019-04-08 21:10:16+00:00
- **Updated**: 2019-04-08 21:10:16+00:00
- **Authors**: Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, Heng Huang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end trainable Video Question Answering (VideoQA) framework with three major components: 1) a new heterogeneous memory which can effectively learn global context information from appearance and motion features; 2) a redesigned question memory which helps understand the complex semantics of question and highlights queried subjects; and 3) a new multimodal fusion layer which performs multi-step reasoning by attending to relevant visual and textual hints with self-updated attention. Our VideoQA model firstly generates the global context-aware visual and textual features respectively by interacting current inputs with memory contents. After that, it makes the attentional fusion of the multimodal visual and textual representations to infer the correct answer. Multiple cycles of reasoning can be made to iteratively refine attention weights of the multimodal data and improve the final representation of the QA pair. Experimental results demonstrate our approach achieves state-of-the-art performance on four VideoQA benchmark datasets.



### A Robust Visual System for Small Target Motion Detection Against Cluttered Moving Backgrounds
- **Arxiv ID**: http://arxiv.org/abs/1904.04363v1
- **DOI**: 10.1109/TNNLS.2019.2910418
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04363v1)
- **Published**: 2019-04-08 21:21:53+00:00
- **Updated**: 2019-04-08 21:21:53+00:00
- **Authors**: Hongxin Wang, Jigen Peng, Xuqiang Zheng, Shigang Yue
- **Comment**: 14 pages, 21 figures
- **Journal**: None
- **Summary**: Monitoring small objects against cluttered moving backgrounds is a huge challenge to future robotic vision systems. As a source of inspiration, insects are quite apt at searching for mates and tracking prey -- which always appear as small dim speckles in the visual field. The exquisite sensitivity of insects for small target motion, as revealed recently, is coming from a class of specific neurons called small target motion detectors (STMDs). Although a few STMD-based models have been proposed, these existing models only use motion information for small target detection and cannot discriminate small targets from small-target-like background features (named as fake features). To address this problem, this paper proposes a novel visual system model (STMD+) for small target motion detection, which is composed of four subsystems -- ommatidia, motion pathway, contrast pathway and mushroom body. Compared to existing STMD-based models, the additional contrast pathway extracts directional contrast from luminance signals to eliminate false positive background motion. The directional contrast and the extracted motion information by the motion pathway are integrated in the mushroom body for small target discrimination. Extensive experiments showed the significant and consistent improvements of the proposed visual system model over existing STMD-based models against fake features.



### Improved Embeddings with Easy Positive Triplet Mining
- **Arxiv ID**: http://arxiv.org/abs/1904.04370v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.04370v2)
- **Published**: 2019-04-08 21:41:28+00:00
- **Updated**: 2020-03-18 18:58:28+00:00
- **Authors**: Hong Xuan, Abby Stylianou, Robert Pless
- **Comment**: None
- **Journal**: None
- **Summary**: Deep metric learning seeks to define an embedding where semantically similar images are embedded to nearby locations, and semantically dissimilar images are embedded to distant locations. Substantial work has focused on loss functions and strategies to learn these embeddings by pushing images from the same class as close together in the embedding space as possible. In this paper, we propose an alternative, loosened embedding strategy that requires the embedding function only map each training image to the most similar examples from the same class, an approach we call "Easy Positive" mining. We provide a collection of experiments and visualizations that highlight that this Easy Positive mining leads to embeddings that are more flexible and generalize better to new unseen data. This simple mining strategy yields recall performance that exceeds state of the art approaches (including those with complicated loss functions and ensemble methods) on image retrieval datasets including CUB, Stanford Online Products, In-Shop Clothes and Hotels-50K.



### Controlling Steering Angle for Cooperative Self-driving Vehicles utilizing CNN and LSTM-based Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.04375v3
- **DOI**: 10.1109/IVS.2019.8814260
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.04375v3)
- **Published**: 2019-04-08 21:51:49+00:00
- **Updated**: 2019-05-12 02:38:13+00:00
- **Authors**: Rodolfo Valiente, Mahdi Zaman, Sedat Ozer, Yaser P. Fallah
- **Comment**: Accepted in IV 2019, 6 pages, 9 figures
- **Journal**: None
- **Summary**: A fundamental challenge in autonomous vehicles is adjusting the steering angle at different road conditions. Recent state-of-the-art solutions addressing this challenge include deep learning techniques as they provide end-to-end solution to predict steering angles directly from the raw input images with higher accuracy. Most of these works ignore the temporal dependencies between the image frames. In this paper, we tackle the problem of utilizing multiple sets of images shared between two autonomous vehicles to improve the accuracy of controlling the steering angle by considering the temporal dependencies between the image frames. This problem has not been studied in the literature widely. We present and study a new deep architecture to predict the steering angle automatically by using Long-Short-Term-Memory (LSTM) in our deep architecture. Our deep architecture is an end-to-end network that utilizes CNN, LSTM and fully connected (FC) layers and it uses both present and futures images (shared by a vehicle ahead via Vehicle-to-Vehicle (V2V) communication) as input to control the steering angle. Our model demonstrates the lowest error when compared to the other existing approaches in the literature.



