# Arxiv Papers in cs.CV on 2019-04-15
### Universal Bounding Box Regression and Its Applications
- **Arxiv ID**: http://arxiv.org/abs/1904.06805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06805v1)
- **Published**: 2019-04-15 01:21:22+00:00
- **Updated**: 2019-04-15 01:21:22+00:00
- **Authors**: Seungkwan Lee, Suha Kwak, Minsu Cho
- **Comment**: ACCV 2018 accepted paper
- **Journal**: None
- **Summary**: Bounding-box regression is a popular technique to refine or predict localization boxes in recent object detection approaches. Typically, bounding-box regressors are trained to regress from either region proposals or fixed anchor boxes to nearby bounding boxes of a pre-defined target object classes. This paper investigates whether the technique is generalizable to unseen classes and is transferable to other tasks beyond supervised object detection. To this end, we propose a class-agnostic and anchor-free box regressor, dubbed Universal Bounding-Box Regressor (UBBR), which predicts a bounding box of the nearest object from any given box. Trained on a relatively small set of annotated images, UBBR successfully generalizes to unseen classes, and can be used to improve localization in many vision problems. We demonstrate its effectivenss on weakly supervised object detection and object discovery.



### Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.06807v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.06807v2)
- **Published**: 2019-04-15 02:04:15+00:00
- **Updated**: 2019-04-16 20:36:07+00:00
- **Authors**: Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan
- **Comment**: 20 pages, 16 figures, accepted to CVPR 2019 as an oral paper
- **Journal**: CVPR 2019
- **Summary**: Cross-view image translation is challenging because it involves images with drastically different views and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top datasets show that our model is able to generate significantly better results than the state-of-the-art methods. The source code, data and trained models are available at https://github.com/Ha0Tang/SelectionGAN.



### Learning Spatiotemporal Features of Ride-sourcing Services with Fusion Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1904.06823v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.06823v2)
- **Published**: 2019-04-15 03:10:45+00:00
- **Updated**: 2020-04-24 08:48:30+00:00
- **Authors**: Feng Xiao, Dapeng Zhang, Gang Kou, Lu Li
- **Comment**: None
- **Journal**: None
- **Summary**: To collectively forecast the demand for ride-sourcing services in all regions of a city, the deep learning approaches have been applied with commendable results. However, the local statistical differences throughout the geographical layout of the city make the spatial stationarity assumption of the convolution invalid, which limits the performance of CNNs on the demand forecasting task. In this paper, we propose a novel deep learning framework called LC-ST-FCN (locally connected spatiotemporal fully-convolutional neural network) to address the unique challenges of the region-level demand forecasting problem within one end-to-end architecture (E2E). We first employ the 3D convolutional layers to fuse the spatial and temporal information existed in the input and then feed the spatiotemporal features extracted by the 3D convolutional layers to the subsequent 2D convolutional layers. Afterward, the prediction value of each region is obtained by the locally connected convolutional layers which relax the parameter sharing scheme. We evaluate the proposed model on a real dataset from a ride-sourcing service platform (DiDiChuxing) and observe significant improvements compared with a bunch of baseline models. Besides, we also illustrate the effectiveness of our proposed model by visualizing how different types of convolutional layers transform their input and capture useful features. The visualization results show that fully convolutional architecture enables the model to better localize the related regions. And the locally connected layers play an important role in dealing with the local statistical differences and activating useful regions.



### Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces
- **Arxiv ID**: http://arxiv.org/abs/1904.06827v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06827v1)
- **Published**: 2019-04-15 03:31:31+00:00
- **Updated**: 2019-04-15 03:31:31+00:00
- **Authors**: Senthil Purushwalkam, Abhinav Gupta, Danny M. Kaufman, Bryan Russell
- **Comment**: Accepted for publication at the International Conference on Learning
  Representations (ICLR) 2019
- **Journal**: None
- **Summary**: We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing - restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories. To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show on our newly collected dataset that our model out-performs baselines, including trajectory fitting with Newtonian physics, in predicting post-bounce trajectories and inferring physical properties of a scene.



### ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging
- **Arxiv ID**: http://arxiv.org/abs/1904.06830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06830v1)
- **Published**: 2019-04-15 03:50:16+00:00
- **Updated**: 2019-04-15 03:50:16+00:00
- **Authors**: Samarth Brahmbhatt, Cusuh Ham, Charles C. Kemp, James Hays
- **Comment**: CVPR 2019 Oral
- **Journal**: None
- **Summary**: Grasping and manipulating objects is an important human skill. Since hand-object contact is fundamental to grasping, capturing it can lead to important insights. However, observing contact through external sensors is challenging because of occlusion and the complexity of the human hand. We present ContactDB, a novel dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. Participants in our study grasped 3D printed objects with a post-grasp functional intent. ContactDB includes 3750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images. To the best of our knowledge, this is the first large-scale dataset that records detailed contact maps for human grasps. Analysis of this data shows the influence of functional intent and object size on grasping, the tendency to touch/avoid 'active areas', and the high frequency of palm and proximal finger contact. Finally, we train state-of-the-art image translation and 3D convolution algorithms to predict diverse contact patterns from object shape. Data, code and models are available at https://contactdb.cc.gatech.edu.



### Deep CNNs Meet Global Covariance Pooling: Better Representation and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1904.06836v2
- **DOI**: 10.1109/TPAMI.2020.2974833
- **Categories**: **cs.CV**, cs.AI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.06836v2)
- **Published**: 2019-04-15 04:30:01+00:00
- **Updated**: 2020-08-11 02:49:48+00:00
- **Authors**: Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang, Peihua Li
- **Comment**: Accepted to IEEE TPAMI. Code is at http://peihuali.org/MPN-COV/
- **Journal**: None
- **Summary**: Compared with global average pooling in existing deep convolutional neural networks (CNNs), global covariance pooling can capture richer statistics of deep features, having potential for improving representation and generalization abilities of deep CNNs. However, integration of global covariance pooling into deep CNNs brings two challenges: (1) robust covariance estimation given deep features of high dimension and small sample size; (2) appropriate usage of geometry of covariances. To address these challenges, we propose a global Matrix Power Normalized COVariance (MPN-COV) Pooling. Our MPN-COV conforms to a robust covariance estimator, very suitable for scenario of high dimension and small sample size. It can also be regarded as Power-Euclidean metric between covariances, effectively exploiting their geometry. Furthermore, a global Gaussian embedding network is proposed to incorporate first-order statistics into MPN-COV. For fast training of MPN-COV networks, we implement an iterative matrix square root normalization, avoiding GPU unfriendly eigen-decomposition inherent in MPN-COV. Additionally, progressive 1x1 convolutions and group convolution are introduced to compress covariance representations. The proposed methods are highly modular, readily plugged into existing deep CNNs. Extensive experiments are conducted on large-scale object classification, scene categorization, fine-grained visual recognition and texture classification, showing our methods outperform the counterparts and obtain state-of-the-art performance.



### PIV-Based 3D Fluid Flow Reconstruction Using Light Field Camera
- **Arxiv ID**: http://arxiv.org/abs/1904.06841v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06841v2)
- **Published**: 2019-04-15 05:00:28+00:00
- **Updated**: 2020-03-23 17:54:50+00:00
- **Authors**: Zhong Li, Jinwei Ye, Yu Ji, Hao Sheng, Jingyi Yu
- **Comment**: This submission need to be withdrawn due to an unresolved conflict
  between authors. This article was submitted without consent of Jinwei Ye
- **Journal**: None
- **Summary**: Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearance and tracking a large number of particles is particularly difficult. In this paper, we present a PIV solution that uses densely sampled light field to reconstruct and track 3D particles. We exploit the refocusing capability and focal symmetry constraint of the light field for reliable particle depth estimation. We further propose a new motion-constrained optical flow estimation scheme by enforcing local motion rigidity and the Navier-Stoke constraint. Comprehensive experiments on synthetic and real experiments show that using a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.



### Robust Visual Tracking Revisited: From Correlation Filter to Template Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.06842v1
- **DOI**: 10.1109/TIP.2018.2813161
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06842v1)
- **Published**: 2019-04-15 05:06:59+00:00
- **Updated**: 2019-04-15 05:06:59+00:00
- **Authors**: Fanghui Liu, Chen Gong, Xiaolin Huang, Tao Zhou, Jie Yang, Dacheng Tao
- **Comment**: has been published on IEEE TIP
- **Journal**: None
- **Summary**: In this paper, we propose a novel matching based tracker by investigating the relationship between template matching and the recent popular correlation filter based trackers (CFTs). Compared to the correlation operation in CFTs, a sophisticated similarity metric termed "mutual buddies similarity" (MBS) is proposed to exploit the relationship of multiple reciprocal nearest neighbors for target matching. By doing so, our tracker obtains powerful discriminative ability on distinguishing target and background as demonstrated by both empirical and theoretical analyses. Besides, instead of utilizing single template with the improper updating scheme in CFTs, we design a novel online template updating strategy named "memory filtering" (MF), which aims to select a certain amount of representative and reliable tracking results in history to construct the current stable and expressive template set. This scheme is beneficial for the proposed tracker to comprehensively "understand" the target appearance variations, "recall" some stable results. Both qualitative and quantitative evaluations on two benchmarks suggest that the proposed tracking method performs favorably against some recently developed CFTs and other competitive trackers.



### Pedestrian Detection in Thermal Images using Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1904.06859v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1904.06859v1)
- **Published**: 2019-04-15 05:42:44+00:00
- **Updated**: 2019-04-15 05:42:44+00:00
- **Authors**: Debasmita Ghose, Shasvat Mukeshkumar Desai, Sneha Bhattacharya, Deep Chakraborty, Madalina Fiterau, Tauhidur Rahman
- **Comment**: Accepted at CVPR 2019 Workshop (PBVS), 10 pages, 7 figures
- **Journal**: None
- **Summary**: Thermal images are mainly used to detect the presence of people at night or in bad lighting conditions, but perform poorly at daytime. To solve this problem, most state-of-the-art techniques employ a fusion network that uses features from paired thermal and color images. Instead, we propose to augment thermal images with their saliency maps, to serve as an attention mechanism for the pedestrian detector especially during daytime. We investigate how such an approach results in improved performance for pedestrian detection using only thermal images, eliminating the need for paired color images. For our experiments, we train the Faster R-CNN for pedestrian detection and report the added effect of saliency maps generated using static and deep methods (PiCA-Net and R3-Net). Our best performing model results in an absolute reduction of miss rate by 13.4% and 19.4% over the baseline in day and night images respectively. We also annotate and release pixel level masks of pedestrians on a subset of the KAIST Multispectral Pedestrian Detection dataset, which is a first publicly available dataset for salient pedestrian detection.



### Self-critical n-step Training for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1904.06861v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06861v1)
- **Published**: 2019-04-15 05:47:23+00:00
- **Updated**: 2019-04-15 05:47:23+00:00
- **Authors**: Junlong Gao, Shiqi Wang, Shanshe Wang, Siwei Ma, Wen Gao
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: Existing methods for image captioning are usually trained by cross entropy loss, which leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. Recently it has been shown that these two issues can be addressed by incorporating techniques from reinforcement learning, where one of the popular techniques is the advantage actor-critic algorithm that calculates per-token advantage by estimating state value with a parametrized estimator at the cost of introducing estimation bias. In this paper, we estimate state value without using a parametrized value estimator. With the properties of image captioning, namely, the deterministic state transition function and the sparse reward, state value is equivalent to its preceding state-action value, and we reformulate advantage function by simply replacing the former with the latter. Moreover, the reformulated advantage is extended to n-step, which can generally increase the absolute value of the mean of reformulated advantage while lowering variance. Then two kinds of rollout are adopted to estimate state-action value, which we call self-critical n-step training. Empirically we find that our method can obtain better performance compared to the state-of-the-art methods that use the sequence level advantage and parametrized estimator respectively on the widely used MSCOCO benchmark.



### Geometric Image Correspondence Verification by Dense Pixel Matching
- **Arxiv ID**: http://arxiv.org/abs/1904.06882v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06882v3)
- **Published**: 2019-04-15 07:25:36+00:00
- **Updated**: 2020-08-17 13:50:30+00:00
- **Authors**: Zakaria Laskar, Iaroslav Melekhov, Hamed R. Tavakoli, Juha Ylioinas, Juho Kannala
- **Comment**: The appendix has been updated by adding some clarifications
- **Journal**: None
- **Summary**: This paper addresses the problem of determining dense pixel correspondences between two images and its application to geometric correspondence verification in image retrieval. The main contribution is a geometric correspondence verification approach for re-ranking a shortlist of retrieved database images based on their dense pair-wise matching with the query image at a pixel level. We determine a set of cyclically consistent dense pixel matches between the pair of images and evaluate local similarity of matched pixels using neural network based image descriptors. Final re-ranking is based on a novel similarity function, which fuses the local similarity metric with a global similarity metric and a geometric consistency measure computed for the matched pixels. For dense matching our approach utilizes a modified version of a recently proposed dense geometric correspondence network (DGC-Net), which we also improve by optimizing the architecture. The proposed model and similarity metric compare favourably to the state-of-the-art image retrieval methods. In addition, we apply our method to the problem of long-term visual localization demonstrating promising results and generalization across datasets.



### DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors
- **Arxiv ID**: http://arxiv.org/abs/1904.06883v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06883v2)
- **Published**: 2019-04-15 07:32:08+00:00
- **Updated**: 2019-04-16 05:37:49+00:00
- **Authors**: Shuai Chen, Jinpeng Li, Chuanqi Yao, Wenbo Hou, Shuo Qin, Wenyao Jin, Xu Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm's ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will increase the computational redundancy of the detection algorithm. In this study, we introduce Dubox, a new one-stage approach that detects the objects without prior box. Working with multi-scale features, the designed dual scale residual unit makes dual scale detectors no longer run independently. The second scale detector learns the residual of the first. Dubox has enhanced the capacity of heuristic-guided that can further enable the first scale detector to maximize the detection of small targets and the second to detect objects that cannot be identified by the first one. Besides, for each scale detector, with the new classification-regression progressive strapped loss makes our process not based on prior boxes. Integrating these strategies, our detection algorithm has achieved excellent performance in terms of speed and accuracy. Extensive experiments on the VOC, COCO object detection benchmark have confirmed the effectiveness of this algorithm.



### Algorithms used for the Cell Segmentation Benchmark Competition at ISBI 2019 by RWTH-GE
- **Arxiv ID**: http://arxiv.org/abs/1904.06890v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/1904.06890v1)
- **Published**: 2019-04-15 07:45:47+00:00
- **Updated**: 2019-04-15 07:45:47+00:00
- **Authors**: Dennis Eschweiler, Johannes Stegmaier
- **Comment**: 4 pages, algorithms used for the Cell Segmentation Benchmark
  competition at IEEE International Symposium on Biomedical Imaging (ISBI) 2019
  in Venice, Italy
- **Journal**: None
- **Summary**: The presented algorithms for segmentation and tracking follow a 3-step approach where we detect, track and finally segment nuclei. In the preprocessing phase, we detect centroids of the cell nuclei using a convolutional neural network (CNN) for the 2D images and a Laplacian-of-Gaussian Scale Space Maximum Projection approach for the 3D data sets. Tracking was performed in a backwards fashion on the predicted seed points, i.e., starting at the last frame and sequentially connecting corresponding objects until the first frame was reached. Correspondences were identified by propagating detections of a frame t to its preceding frame t-1 and by combining redundant detections using a hierarchical clustering approach. The tracked centroids were then used as input to variants of the seeded watershed algorithm to obtain the final segmentation.



### Learning Deformable Kernels for Image and Video Denoising
- **Arxiv ID**: http://arxiv.org/abs/1904.06903v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1904.06903v1)
- **Published**: 2019-04-15 08:15:09+00:00
- **Updated**: 2019-04-15 08:15:09+00:00
- **Authors**: Xiangyu Xu, Muchen Li, Wenxiu Sun
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Most of the classical denoising methods restore clear results by selecting and averaging pixels in the noisy input. Instead of relying on hand-crafted selecting and averaging strategies, we propose to explicitly learn this process with deep neural networks. Specifically, we propose deformable 2D kernels for image denoising where the sampling locations and kernel weights are both learned. The proposed kernel naturally adapts to image structures and could effectively reduce the oversmoothing artifacts. Furthermore, we develop 3D deformable kernels for video denoising to more efficiently sample pixels across the spatial-temporal space. Our method is able to solve the misalignment issues of large motion from dynamic scenes. For better training our video denoising model, we introduce the trilinear sampler and a new regularization term. We demonstrate that the proposed method performs favorably against the state-of-the-art image and video denoising approaches on both synthetic and real-world data.



### Implicit Pairs for Boosting Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1904.06913v4
- **DOI**: 10.1016/j.visinf.2020.10.001
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06913v4)
- **Published**: 2019-04-15 08:52:25+00:00
- **Updated**: 2020-12-03 17:37:23+00:00
- **Authors**: Yiftach Ginger, Dov Danon, Hadar Averbuch-Elor, Daniel Cohen-Or
- **Comment**: None
- **Journal**: None
- **Summary**: In image-to-image translation the goal is to learn a mapping from one image domain to another. In the case of supervised approaches the mapping is learned from paired samples. However, collecting large sets of image pairs is often either prohibitively expensive or not possible. As a result, in recent years more attention has been given to techniques that learn the mapping from unpaired sets.   In our work, we show that injecting implicit pairs into unpaired sets strengthens the mapping between the two domains, improves the compatibility of their distributions, and leads to performance boosting of unsupervised techniques by over 14% across several measurements.   The competence of the implicit pairs is further displayed with the use of pseudo-pairs, i.e., paired samples which only approximate a real pair. We demonstrate the effect of the approximated implicit samples on image-to-image translation problems, where such pseudo-pairs may be synthesized in one direction, but not in the other. We further show that pseudo-pairs are significantly more effective as implicit pairs in an unpaired setting, than directly using them explicitly in a paired setting.



### Estimation of Linear Motion in Dense Crowd Videos using Langevin Model
- **Arxiv ID**: http://arxiv.org/abs/1904.07233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07233v1)
- **Published**: 2019-04-15 09:17:16+00:00
- **Updated**: 2019-04-15 09:17:16+00:00
- **Authors**: Shreetam Behera, Debi Prosad Dogra, Malay Kumar Bandyopadhyay, Partha Pratim Roy
- **Comment**: None
- **Journal**: None
- **Summary**: Crowd gatherings at social and cultural events are increasing in leaps and bounds with the increase in population. Surveillance through computer vision and expert decision making systems can help to understand the crowd phenomena at large gatherings. Understanding crowd phenomena can be helpful in early identification of unwanted incidents and their prevention. Motion flow is one of the important crowd phenomena that can be instrumental in describing the crowd behavior. Flows can be useful in understanding instabilities in the crowd. However, extracting motion flows is a challenging task due to randomness in crowd movement and limitations of the sensing device. Moreover, low-level features such as optical flow can be misleading if the randomness is high. In this paper, we propose a new model based on Langevin equation to analyze the linear dominant flows in videos of densely crowded scenarios. We assume a force model with three components, namely external force, confinement/drift force, and disturbance force. These forces are found to be sufficient to describe the linear or near-linear motion in dense crowd videos. The method is significantly faster as compared to existing popular crowd segmentation methods. The evaluation of the proposed model has been carried out on publicly available datasets as well as using our dataset. It has been observed that the proposed method is able to estimate and segment the linear flows in the dense crowd with better accuracy as compared to state-of-the-art techniques with substantial decrease in the computational overhead.



### Deep Comprehensive Correlation Mining for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1904.06925v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06925v3)
- **Published**: 2019-04-15 09:26:58+00:00
- **Updated**: 2019-08-12 06:02:44+00:00
- **Authors**: Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, Hongbin Zha
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Recent developed deep unsupervised methods allow us to jointly learn representation and cluster unlabelled data. These deep clustering methods mainly focus on the correlation among samples, e.g., selecting high precision pairs to gradually tune the feature representation, which neglects other useful correlations. In this paper, we propose a novel clustering framework, named deep comprehensive correlation mining(DCCM), for exploring and taking full advantage of various kinds of correlations behind the unlabeled data from three aspects: 1) Instead of only using pair-wise information, pseudo-label supervision is proposed to investigate category information and learn discriminative features. 2) The features' robustness to image transformation of input space is fully explored, which benefits the network learning and significantly improves the performance. 3) The triplet mutual information among features is presented for clustering problem to lift the recently discovered instance-level deep mutual information to a triplet-level formation, which further helps to learn more discriminative features. Extensive experiments on several challenging datasets show that our method achieves good performance, e.g., attaining $62.3\%$ clustering accuracy on CIFAR-10, which is $10.1\%$ higher than the state-of-the-art results.



### Influence of Control Parameters and the Size of Biomedical Image Datasets on the Success of Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1904.06964v1
- **DOI**: None
- **Categories**: **cs.CV**, 49-04, I.4.10; I.2.10; K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/1904.06964v1)
- **Published**: 2019-04-15 11:07:22+00:00
- **Updated**: 2019-04-15 11:07:22+00:00
- **Authors**: Vassili Kovalev, Dmitry Voynov
- **Comment**: 10 pages, 3 figures, 1 table, 3746 words
- **Journal**: None
- **Summary**: In this paper, we study dependence of the success rate of adversarial attacks to the Deep Neural Networks on the biomedical image type, control parameters, and image dataset size. With this work, we are going to contribute towards accumulation of experimental results on adversarial attacks for the community dealing with biomedical images. The white-box Projected Gradient Descent attacks were examined based on 8 classification tasks and 13 image datasets containing a total of 605,080 chest X-ray and 317,000 histology images of malignant tumors. We concluded that: (1) An increase of the amplitude of perturbation in generating malicious adversarial images leads to a growth of the fraction of successful attacks for the majority of image types examined in this study. (2) Histology images tend to be less sensitive to the growth of amplitude of adversarial perturbations. (3) Percentage of successful attacks is growing with an increase of the number of iterations of the algorithm of generating adversarial perturbations with an asymptotic stabilization. (4) It was found that the success of attacks dropping dramatically when the original confidence of predicting image class exceeds 0.95. (5) The expected dependence of the percentage of successful attacks on the size of image training set was not confirmed.



### Segmenting Potentially Cancerous Areas in Prostate Biopsies using Semi-Automatically Annotated Data
- **Arxiv ID**: http://arxiv.org/abs/1904.06969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.06969v1)
- **Published**: 2019-04-15 11:18:27+00:00
- **Updated**: 2019-04-15 11:18:27+00:00
- **Authors**: Nikolay Burlutskiy, Nicolas Pinchaud, Feng Gu, Daniel Hägg, Mats Andersson, Lars Björk, Kristian Eurén, Cristina Svensson, Lena Kajland Wilén, Martin Hedlund
- **Comment**: Accepted as oral presentation at Medical Imaging with Deep Learning
  (MIDL) 2019, July, London, England
- **Journal**: None
- **Summary**: Gleason grading specified in ISUP 2014 is the clinical standard in staging prostate cancer and the most important part of the treatment decision. However, the grading is subjective and suffers from high intra and inter-user variability. To improve the consistency and objectivity in the grading, we introduced glandular tissue WithOut Basal cells (WOB) as the ground truth. The presence of basal cells is the most accepted biomarker for benign glandular tissue and the absence of basal cells is a strong indicator of acinar prostatic adenocarcinoma, the most common form of prostate cancer. Glandular tissue can objectively be assessed as WOB or not WOB by using specific immunostaining for glandular tissue (Cytokeratin 8/18) and for basal cells (Cytokeratin 5/6 + p63). Even more, WOB allowed us to develop a semi-automated data generation pipeline to speed up the tremendously time consuming and expensive process of annotating whole slide images by pathologists. We generated 295 prostatectomy images exhaustively annotated with WOB. Then we used our Deep Learning Framework, which achieved the $2^{nd}$ best reported score in Camelyon17 Challenge, to train networks for segmenting WOB in needle biopsies. Evaluation of the model on 63 needle biopsies showed promising results which were improved further by finetuning the model on 118 biopsies annotated with WOB, achieving F1-score of 0.80 and Precision-Recall AUC of 0.89 at the pixel-level. Then we compared the performance of the model against 17 biopsies annotated independently by 3 pathologists using only H\&E staining. The comparison demonstrated that the model performed on a par with the pathologists. Finally, the model detected and accurately outlined existing WOB areas in two biopsies incorrectly annotated as totally WOB-free biopsies by three pathologists and in one biopsy by two pathologists.



### Three scenarios for continual learning
- **Arxiv ID**: http://arxiv.org/abs/1904.07734v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07734v1)
- **Published**: 2019-04-15 12:22:36+00:00
- **Updated**: 2019-04-15 12:22:36+00:00
- **Authors**: Gido M. van de Ven, Andreas S. Tolias
- **Comment**: Extended version of work presented at the NeurIPS Continual Learning
  workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635
- **Journal**: None
- **Summary**: Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.



### Tightly Coupled 3D Lidar Inertial Odometry and Mapping
- **Arxiv ID**: http://arxiv.org/abs/1904.06993v1
- **DOI**: 10.1109/ICRA.2019.8793511
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.06993v1)
- **Published**: 2019-04-15 12:26:12+00:00
- **Updated**: 2019-04-15 12:26:12+00:00
- **Authors**: Haoyang Ye, Yuying Chen, Ming Liu
- **Comment**: Accepted by ICRA 2019
- **Journal**: None
- **Summary**: Ego-motion estimation is a fundamental requirement for most mobile robotic applications. By sensor fusion, we can compensate the deficiencies of stand-alone sensors and provide more reliable estimations. We introduce a tightly coupled lidar-IMU fusion method in this paper. By jointly minimizing the cost derived from lidar and IMU measurements, the lidar-IMU odometry (LIO) can perform well with acceptable drift after long-term experiment, even in challenging cases where the lidar measurements can be degraded. Besides, to obtain more reliable estimations of the lidar poses, a rotation-constrained refinement algorithm (LIO-mapping) is proposed to further align the lidar poses with the global map. The experiment results demonstrate that the proposed method can estimate the poses of the sensor pair at the IMU update rate with high precision, even under fast motion conditions or with insufficient features.



### SR-GAN: Semantic Rectifying Generative Adversarial Network for Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.06996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.06996v1)
- **Published**: 2019-04-15 12:30:09+00:00
- **Updated**: 2019-04-15 12:30:09+00:00
- **Authors**: Zihan Ye, Fan Lyu, Linyan Li, Qiming Fu, Jinchang Ren, Fuyuan Hu
- **Comment**: ICME 2019 Oral
- **Journal**: None
- **Summary**: The existing Zero-Shot learning (ZSL) methods may suffer from the vague class attributes that are highly overlapped for different classes. Unlike these methods that ignore the discrimination among classes, in this paper, we propose to classify unseen image by rectifying the semantic space guided by the visual space. First, we pre-train a Semantic Rectifying Network (SRN) to rectify semantic space with a semantic loss and a rectifying loss. Then, a Semantic Rectifying Generative Adversarial Network (SR-GAN) is built to generate plausible visual feature of unseen class from both semantic feature and rectified semantic feature. To guarantee the effectiveness of rectified semantic features and synthetic visual features, a pre-reconstruction and a post reconstruction networks are proposed, which keep the consistency between visual feature and semantic feature. Experimental results demonstrate that our approach significantly outperforms the state-of-the-arts on four benchmark datasets.



### Synthesising 3D Facial Motion from "In-the-Wild" Speech
- **Arxiv ID**: http://arxiv.org/abs/1904.07002v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07002v1)
- **Published**: 2019-04-15 12:42:43+00:00
- **Updated**: 2019-04-15 12:42:43+00:00
- **Authors**: Panagiotis Tzirakis, Athanasios Papaioannou, Alexander Lattas, Michail Tarasiou, Björn Schuller, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Synthesising 3D facial motion from speech is a crucial problem manifesting in a multitude of applications such as computer games and movies. Recently proposed methods tackle this problem in controlled conditions of speech. In this paper, we introduce the first methodology for 3D facial motion synthesis from speech captured in arbitrary recording conditions ("in-the-wild") and independent of the speaker. For our purposes, we captured 4D sequences of people uttering 500 words, contained in the Lip Reading Words (LRW) a publicly available large-scale in-the-wild dataset, and built a set of 3D blendshapes appropriate for speech. We correlate the 3D shape parameters of the speech blendshapes to the LRW audio samples by means of a novel time-warping technique, named Deep Canonical Attentional Warping (DCAW), that can simultaneously learn hierarchical non-linear representations and a warping path in an end-to-end manner. We thoroughly evaluate our proposed methods, and show the ability of a deep learning model to synthesise 3D facial motion in handling different speakers and continuous speech signals in uncontrolled conditions.



### A deep learning framework for quality assessment and restoration in video endoscopy
- **Arxiv ID**: http://arxiv.org/abs/1904.07073v1
- **DOI**: 10.1016/j.media.2020.101900
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.07073v1)
- **Published**: 2019-04-15 14:26:38+00:00
- **Updated**: 2019-04-15 14:26:38+00:00
- **Authors**: Sharib Ali, Felix Zhou, Adam Bailey, Barbara Braden, James East, Xin Lu, Jens Rittscher
- **Comment**: 14 pages
- **Journal**: Medical Image Analysis, 101900(2020)
- **Summary**: Endoscopy is a routine imaging technique used for both diagnosis and minimally invasive surgical treatment. Artifacts such as motion blur, bubbles, specular reflections, floating objects and pixel saturation impede the visual interpretation and the automated analysis of endoscopy videos. Given the widespread use of endoscopy in different clinical applications, we contend that the robust and reliable identification of such artifacts and the automated restoration of corrupted video frames is a fundamental medical imaging problem. Existing state-of-the-art methods only deal with the detection and restoration of selected artifacts. However, typically endoscopy videos contain numerous artifacts which motivates to establish a comprehensive solution.   We propose a fully automatic framework that can: 1) detect and classify six different primary artifacts, 2) provide a quality score for each frame and 3) restore mildly corrupted frames. To detect different artifacts our framework exploits fast multi-scale, single stage convolutional neural network detector. We introduce a quality metric to assess frame quality and predict image restoration success. Generative adversarial networks with carefully chosen regularization are finally used to restore corrupted frames.   Our detector yields the highest mean average precision (mAP at 5% threshold) of 49.0 and the lowest computational time of 88 ms allowing for accurate real-time processing. Our restoration models for blind deblurring, saturation correction and inpainting demonstrate significant improvements over previous methods. On a set of 10 test videos we show that our approach preserves an average of 68.7% which is 25% more frames than that retained from the raw videos.



### Painting on Placement: Forecasting Routing Congestion using Conditional Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1904.07077v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07077v1)
- **Published**: 2019-04-15 14:35:14+00:00
- **Updated**: 2019-04-15 14:35:14+00:00
- **Authors**: Cunxi Yu, Zhiru Zhang
- **Comment**: 6 pages, 9 figures, to appear at DAC'19
- **Journal**: None
- **Summary**: Physical design process commonly consumes hours to days for large designs, and routing is known as the most critical step. Demands for accurate routing quality prediction raise to a new level to accelerate hardware innovation with advanced technology nodes. This work presents an approach that forecasts the density of all routing channels over the entire floorplan, with features collected up to placement, using conditional GANs. Specifically, forecasting the routing congestion is constructed as an image translation (colorization) problem. The proposed approach is applied to a) placement exploration for minimum congestion, b) constrained placement exploration and c) forecasting congestion in real-time during incremental placement, using eight designs targeting a fixed FPGA architecture.



### Saliency Prediction on Omnidirectional Images with Generative Adversarial Imitation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.07080v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.07080v1)
- **Published**: 2019-04-15 14:36:40+00:00
- **Updated**: 2019-04-15 14:36:40+00:00
- **Authors**: Mai Xu, Li Yang, Xiaoming Tao, Yiping Duan, Zulin Wang
- **Comment**: None
- **Journal**: None
- **Summary**: When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects' head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and determine three findings: (1) The consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) The head fixations exist with a front center bias (FCB); and (3) The magnitude of head movement is similar across subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 10 state-of-the-art approaches.



### Recurrent Neural Network for (Un-)supervised Learning of Monocular VideoVisual Odometry and Depth
- **Arxiv ID**: http://arxiv.org/abs/1904.07087v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07087v1)
- **Published**: 2019-04-15 14:48:43+00:00
- **Updated**: 2019-04-15 14:48:43+00:00
- **Authors**: Rui Wang, Stephen M. Pizer, Jan-Michael Frahm
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based, single-view depth estimation methods have recently shown highly promising results. However, such methods ignore one of the most important features for determining depth in the human vision system, which is motion. We propose a learning-based, multi-view dense depth map and odometry estimation method that uses Recurrent Neural Networks (RNN) and trains utilizing multi-view image reprojection and forward-backward flow-consistency losses. Our model can be trained in a supervised or even unsupervised mode. It is designed for depth and visual odometry estimation from video where the input frames are temporally correlated. However, it also generalizes to single-view depth estimation. Our method produces superior results to the state-of-the-art approaches for single-view and multi-view learning-based depth estimation on the KITTI driving dataset.



### SIMCO: SIMilarity-based object COunting
- **Arxiv ID**: http://arxiv.org/abs/1904.07092v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07092v2)
- **Published**: 2019-04-15 14:52:31+00:00
- **Updated**: 2020-10-13 08:16:53+00:00
- **Authors**: Marco Godi, Christian Joppi, Andrea Giachetti, Marco Cristani
- **Comment**: Accepted at ICPR 2020
- **Journal**: None
- **Summary**: We present SIMCO, the first agnostic multi-class object counting approach. SIMCO starts by detecting foreground objects through a novel Mask RCNN-based architecture trained beforehand (just once) on a brand-new synthetic 2D shape dataset, InShape; the idea is to highlight every object resembling a primitive 2D shape (circle, square, rectangle, etc.). Each object detected is described by a low-dimensional embedding, obtained from a novel similarity-based head branch; this latter implements a triplet loss, encouraging similar objects (same 2D shape + color and scale) to map close. Subsequently, SIMCO uses this embedding for clustering, so that different types of objects can emerge and be counted, making SIMCO the very first multi-class unsupervised counter. Experiments show that SIMCO provides state-of-the-art scores on counting benchmarks and that it can also help in many challenging image understanding tasks.



### Processsing Simple Geometric Attributes with Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1904.07099v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.07099v1)
- **Published**: 2019-04-15 15:01:27+00:00
- **Updated**: 2019-04-15 15:01:27+00:00
- **Authors**: Alasdair Newson, Andrés Almansa, Yann Gousseau, Saïd Ladjal
- **Comment**: None
- **Journal**: None
- **Summary**: Image synthesis is a core problem in modern deep learning, and many recent architectures such as autoencoders and Generative Adversarial networks produce spectacular results on highly complex data, such as images of faces or landscapes. While these results open up a wide range of new, advanced synthesis applications, there is also a severe lack of theoretical understanding of how these networks work. This results in a wide range of practical problems, such as difficulties in training, the tendency to sample images with little or no variability, and generalisation problems. In this paper, we propose to analyse the ability of the simplest generative network, the autoencoder, to encode and decode two simple geometric attributes : size and position. We believe that, in order to understand more complicated tasks, it is necessary to first understand how these networks process simple attributes. For the first property, we analyse the case of images of centred disks with variable radii. We explain how the autoencoder projects these images to and from a latent space of smallest possible dimension, a scalar. In particular, we describe a closed-form solution to the decoding training problem in a network without biases, and show that during training, the network indeed finds this solution. We then investigate the best regularisation approaches which yield networks that generalise well. For the second property, position, we look at the encoding and decoding of Dirac delta functions, also known as `one-hot' vectors. We describe a hand-crafted filter that achieves encoding perfectly, and show that the network naturally finds this filter during training. We also show experimentally that the decoding can be achieved if the dataset is sampled in an appropriate manner.



### Focus Is All You Need: Loss Functions For Event-based Vision
- **Arxiv ID**: http://arxiv.org/abs/1904.07235v1
- **DOI**: 10.1109/CVPR.2019.01256
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.07235v1)
- **Published**: 2019-04-15 15:40:56+00:00
- **Updated**: 2019-04-15 15:40:56+00:00
- **Authors**: Guillermo Gallego, Mathias Gehrig, Davide Scaramuzza
- **Comment**: 29 pages, 19 figures, 4 tables
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Long Beach, 2019
- **Summary**: Event cameras are novel vision sensors that output pixel-level brightness changes ("events") instead of traditional video frames. These asynchronous sensors offer several advantages over traditional cameras, such as, high temporal resolution, very high dynamic range, and no motion blur. To unlock the potential of such sensors, motion compensation methods have been recently proposed. We present a collection and taxonomy of twenty two objective functions to analyze event alignment in motion compensation approaches (Fig. 1). We call them Focus Loss Functions since they have strong connections with functions used in traditional shape-from-focus applications. The proposed loss functions allow bringing mature computer vision tools to the realm of event cameras. We compare the accuracy and runtime performance of all loss functions on a publicly available dataset, and conclude that the variance, the gradient and the Laplacian magnitudes are among the best loss functions. The applicability of the loss functions is shown on multiple tasks: rotational motion, depth and optical flow estimation. The proposed focus loss functions allow to unlock the outstanding properties of event cameras.



### Learning to Generate Unambiguous Spatial Referring Expressions for Real-World Environments
- **Arxiv ID**: http://arxiv.org/abs/1904.07165v4
- **DOI**: 10.1109/IROS40897.2019.8968510
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07165v4)
- **Published**: 2019-04-15 16:25:49+00:00
- **Updated**: 2019-08-05 09:46:52+00:00
- **Authors**: Fethiye Irmak Doğan, Sinan Kalkan, Iolanda Leite
- **Comment**: International Conference on Intelligent Robots and Systems (IROS
  2019), Demo 1: Finding the described object (https://youtu.be/BE6-F6chW0w),
  Demo 2: Referring to the pointed object (https://youtu.be/nmmv6JUpy8M),
  Supplementary Video (https://youtu.be/sFjBa_MHS98)
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (2019) 4992-4999
- **Summary**: Referring to objects in a natural and unambiguous manner is crucial for effective human-robot interaction. Previous research on learning-based referring expressions has focused primarily on comprehension tasks, while generating referring expressions is still mostly limited to rule-based methods. In this work, we propose a two-stage approach that relies on deep learning for estimating spatial relations to describe an object naturally and unambiguously with a referring expression. We compare our method to the state of the art algorithm in ambiguous environments (e.g., environments that include very similar objects with similar relationships). We show that our method generates referring expressions that people find to be more accurate ($\sim$30% better) and would prefer to use ($\sim$32% more often).



### Deep Iterative Surface Normal Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.07172v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07172v3)
- **Published**: 2019-04-15 16:40:38+00:00
- **Updated**: 2020-06-23 13:01:36+00:00
- **Authors**: Jan Eric Lenssen, Christian Osendorfer, Jonathan Masci
- **Comment**: Presented at CVPR 2020
- **Journal**: None
- **Summary**: This paper presents an end-to-end differentiable algorithm for robust and detail-preserving surface normal estimation on unstructured point-clouds. We utilize graph neural networks to iteratively parameterize an adaptive anisotropic kernel that produces point weights for weighted least-squares plane fitting in local neighborhoods. The approach retains the interpretability and efficiency of traditional sequential plane fitting while benefiting from adaptation to data set statistics through deep learning. This results in a state-of-the-art surface normal estimator that is robust to noise, outliers and point density variation, preserves sharp features through anisotropic kernels and equivariance through a local quaternion-based spatial transformer. Contrary to previous deep learning methods, the proposed approach does not require any hand-crafted features or preprocessing. It improves on the state-of-the-art results while being more than two orders of magnitude faster and more parameter efficient.



### Explicit Spatial Encoding for Deep Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1904.07190v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07190v1)
- **Published**: 2019-04-15 17:03:50+00:00
- **Updated**: 2019-04-15 17:03:50+00:00
- **Authors**: Arun Mukundan, Giorgos Tolias, Ondrej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a kernelized deep local-patch descriptor based on efficient match kernels of neural network activations. Response of each receptive field is encoded together with its spatial location using explicit feature maps. Two location parametrizations, Cartesian and polar, are used to provide robustness to a different types of canonical patch misalignment. Additionally, we analyze how the conventional architecture, i.e. a fully connected layer attached after the convolutional part, encodes responses in a spatially variant way. In contrary, explicit spatial encoding is used in our descriptor, whose potential applications are not limited to local-patches. We evaluate the descriptor on standard benchmarks. Both versions, encoding 32x32 or 64x64 patches, consistently outperform all other methods on all benchmarks. The number of parameters of the model is independent of the input patch resolution.



### Low-Power Computer Vision: Status, Challenges, Opportunities
- **Arxiv ID**: http://arxiv.org/abs/1904.07714v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1904.07714v1)
- **Published**: 2019-04-15 17:48:48+00:00
- **Updated**: 2019-04-15 17:48:48+00:00
- **Authors**: Sergei Alyamkin, Matthew Ardi, Alexander C. Berg, Achille Brighton, Bo Chen, Yiran Chen, Hsin-Pai Cheng, Zichen Fan, Chen Feng, Bo Fu, Kent Gauen, Abhinav Goel, Alexander Goncharenko, Xuyang Guo, Soonhoi Ha, Andrew Howard, Xiao Hu, Yuanjun Huang, Donghyun Kang, Jaeyoun Kim, Jong Gook Ko, Alexander Kondratyev, Junhyeok Lee, Seungjae Lee, Suwoong Lee, Zichao Li, Zhiyu Liang, Juzheng Liu, Xin Liu, Yang Lu, Yung-Hsiang Lu, Deeptanshu Malik, Hong Hanh Nguyen, Eunbyung Park, Denis Repin, Liang Shen, Tao Sheng, Fei Sun, David Svitov, George K. Thiruvathukal, Baiwu Zhang, Jingchi Zhang, Xiaopeng Zhang, Shaojie Zhuo
- **Comment**: Preprint, Accepted by IEEE Journal on Emerging and Selected Topics in
  Circuits and Systems. arXiv admin note: substantial text overlap with
  arXiv:1810.01732
- **Journal**: None
- **Summary**: Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries and energy efficiency is critical. This article serves two main purposes: (1) Examine the state-of-the-art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This article summarizes 2018 winners' solutions. (2) Suggest directions for research as well as opportunities for low-power computer vision.



### Learning Discriminative Model Prediction for Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.07220v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07220v2)
- **Published**: 2019-04-15 17:57:09+00:00
- **Updated**: 2020-06-08 18:31:34+00:00
- **Authors**: Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte
- **Comment**: None
- **Journal**: None
- **Summary**: The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability.   We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking.



### Joint Discriminative and Generative Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.07223v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07223v3)
- **Published**: 2019-04-15 17:59:43+00:00
- **Updated**: 2021-05-19 06:36:47+00:00
- **Authors**: Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan Kautz
- **Comment**: CVPR 2019 (Oral). Add transfer learning results
- **Journal**: None
- **Summary**: Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.



### A deep learning model for early prediction of Alzheimer's disease dementia based on hippocampal MRI
- **Arxiv ID**: http://arxiv.org/abs/1904.07282v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1904.07282v1)
- **Published**: 2019-04-15 18:37:09+00:00
- **Updated**: 2019-04-15 18:37:09+00:00
- **Authors**: Hongming Li, Mohamad Habes, David A. Wolk, Yong Fan
- **Comment**: Accepted for publication in Alzheimer's & Dementia
- **Journal**: None
- **Summary**: Introduction: It is challenging at baseline to predict when and which individuals who meet criteria for mild cognitive impairment (MCI) will ultimately progress to Alzheimer's disease (AD) dementia. Methods: A deep learning method is developed and validated based on MRI scans of 2146 subjects (803 for training and 1343 for validation) to predict MCI subjects' progression to AD dementia in a time-to-event analysis setting. Results: The deep learning time-to-event model predicted individual subjects' progression to AD dementia with a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects with follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a C-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from 18-54 months (quartiles: [18, 36,54]). The predicted progression risk also clustered individual subjects into subgroups with significant differences in their progression time to AD dementia (p<0.0002). Improved performance for predicting progression to AD dementia (C-index=0.864) was obtained when the deep learning based progression risk was combined with baseline clinical measures. Conclusion: Our method provides a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period.



### Brain Tumor Segmentation on MRI with Missing Modalities
- **Arxiv ID**: http://arxiv.org/abs/1904.07290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07290v1)
- **Published**: 2019-04-15 19:00:24+00:00
- **Updated**: 2019-04-15 19:00:24+00:00
- **Authors**: Yan Shen, Mingchen Gao
- **Comment**: Will appear in IPMI 2019
- **Journal**: None
- **Summary**: Brain Tumor Segmentation from magnetic resonance imaging (MRI) is a critical technique for early diagnosis. However, rather than having complete four modalities as in BraTS dataset, it is common to have missing modalities in clinical scenarios. We design a brain tumor segmentation algorithm that is robust to the absence of any modality. Our network includes a channel-independent encoding path and a feature-fusion decoding path. We use self-supervised training through channel dropout and also propose a novel domain adaptation method on feature maps to recover the information from the missing channel. Our results demonstrate that the quality of the segmentation depends on which modality is missing. Furthermore, we also discuss and visualize the contribution of each modality to the segmentation results. Their contributions are along well with the expert screening routine.



### MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.08492v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.08492v2)
- **Published**: 2019-04-15 19:25:59+00:00
- **Updated**: 2019-04-22 13:21:27+00:00
- **Authors**: Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, Samir A Rawashdeh
- **Comment**: Accepted for CVPR 2019 Workshop on Autonomous Driving (WAD). Demo
  Video can be accessed at https://youtu.be/E378PzLq7lQ
- **Journal**: None
- **Summary**: Multi-task learning is commonly used in autonomous driving for solving various visual perception tasks. It offers significant benefits in terms of both performance and computational complexity. Current work on multi-task learning networks focus on processing a single input image and there is no known implementation of multi-task learning handling a sequence of images. In this work, we propose a multi-stream multi-task network to take advantage of using feature representations from preceding frames in a video sequence for joint learning of segmentation, depth, and motion. The weights of the current and previous encoder are shared so that features computed in the previous frame can be leveraged without additional computation. In addition, we propose to use the geometric mean of task losses as a better alternative to the weighted average of task losses. The proposed loss function facilitates better handling of the difference in convergence rates of different tasks. Experimental results on KITTI, Cityscapes and SYNTHIA datasets demonstrate that the proposed strategies outperform various existing multi-task learning solutions.



### Fast Inference in Capsule Networks Using Accumulated Routing Coefficients
- **Arxiv ID**: http://arxiv.org/abs/1904.07304v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07304v1)
- **Published**: 2019-04-15 19:44:52+00:00
- **Updated**: 2019-04-15 19:44:52+00:00
- **Authors**: Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P. Unnikrishnan
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for fast inference in Capsule Networks (CapsNets) by taking advantage of a key insight regarding the routing coefficients that link capsules between adjacent network layers. Since the routing coefficients are responsible for assigning object parts to wholes, and an object whole generally contains similar intra-class and dissimilar inter-class parts, the routing coefficients tend to form a unique signature for each object class. For fast inference, a network is first trained in the usual manner using examples from the training dataset. Afterward, the routing coefficients associated with the training examples are accumulated offline and used to create a set of "master" routing coefficients. During inference, these master routing coefficients are used in place of the dynamically calculated routing coefficients. Our method effectively replaces the for-loop iterations in the dynamic routing procedure with a single matrix multiply operation, providing a significant boost in inference speed. Compared with the dynamic routing procedure, fast inference decreases the test accuracy for the MNIST, Background MNIST, Fashion MNIST, and Rotated MNIST datasets by less than 0.5% and by approximately 5% for CIFAR10.



### Automatic adaptation of object detectors to new domains using self-training
- **Arxiv ID**: http://arxiv.org/abs/1904.07305v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.07305v1)
- **Published**: 2019-04-15 19:46:18+00:00
- **Updated**: 2019-04-15 19:46:18+00:00
- **Authors**: Aruni RoyChowdhury, Prithvijit Chakrabarty, Ashish Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, Erik Learned-Miller
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: This work addresses the unsupervised adaptation of an existing object detector to a new target domain. We assume that a large number of unlabeled videos from this domain are readily available. We automatically obtain labels on the target data by using high-confidence detections from the existing detector, augmented with hard (misclassified) examples acquired by exploiting temporal cues using a tracker. These automatically-obtained labels are then used for re-training the original model. A modified knowledge distillation loss is proposed, and we investigate several ways of assigning soft-labels to the training examples from the target domain. Our approach is empirically evaluated on challenging face and pedestrian detection tasks: a face detector trained on WIDER-Face, which consists of high-quality images crawled from the web, is adapted to a large-scale surveillance data set; a pedestrian detector trained on clear, daytime images from the BDD-100K driving data set is adapted to all other scenarios such as rainy, foggy, night-time. Our results demonstrate the usefulness of incorporating hard examples obtained from tracking, the advantage of using soft-labels via distillation loss versus hard-labels, and show promising performance as a simple method for unsupervised domain adaptation of object detectors, with minimal dependence on hyper-parameters.



### A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.07312v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07312v1)
- **Published**: 2019-04-15 20:10:47+00:00
- **Updated**: 2019-04-15 20:10:47+00:00
- **Authors**: Reza Ghoddoosian, Marnim Galib, Vassilis Athitsos
- **Comment**: Computer Vision and Pattern Recognition Workshops (CVPRW 2019)
- **Journal**: None
- **Summary**: Drowsiness can put lives of many drivers and workers in danger. It is important to design practical and easy-to-deploy real-world systems to detect the onset of drowsiness.In this paper, we address early drowsiness detection, which can provide early alerts and offer subjects ample time to react. We present a large and public real-life dataset of 60 subjects, with video segments labeled as alert, low vigilant, or drowsy. This dataset consists of around 30 hours of video, with contents ranging from subtle signs of drowsiness to more obvious ones. We also benchmark a temporal model for our dataset, which has low computational and storage demands. The core of our proposed method is a Hierarchical Multiscale Long Short-Term Memory (HM-LSTM) network, that is fed by detected blink features in sequence. Our experiments demonstrate the relationship between the sequential blink features and drowsiness. In the experimental results, our baseline method produces higher accuracy than human judgment.



### Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics
- **Arxiv ID**: http://arxiv.org/abs/1904.07318v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07318v1)
- **Published**: 2019-04-15 20:15:46+00:00
- **Updated**: 2019-04-15 20:15:46+00:00
- **Authors**: David Schlangen
- **Comment**: Presented at the 13th International Conference on Computational
  Semantics (IWCS 2019), Gothenburg
- **Journal**: None
- **Summary**: Propelling, and propelled by, the "deep learning revolution", recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the "linked to same image" relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.



### Characterizing the Variability in Face Recognition Accuracy Relative to Race
- **Arxiv ID**: http://arxiv.org/abs/1904.07325v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07325v3)
- **Published**: 2019-04-15 20:46:29+00:00
- **Updated**: 2019-05-08 04:19:44+00:00
- **Authors**: KS Krishnapriya, Kushal Vangara, Michael C. King, Vitor Albiero, Kevin Bowyer
- **Comment**: Paper will appear in the BEFA workshop at CVPR 2019
- **Journal**: None
- **Summary**: Many recent news headlines have labeled face recognition technology as biased or racist. We report on a methodical investigation into differences in face recognition accuracy between African-American and Caucasian image cohorts of the MORPH dataset. We find that, for all four matchers considered, the impostor and the genuine distributions are statistically significantly different between cohorts. For a fixed decision threshold, the African-American image cohort has a higher false match rate and a lower false non-match rate. ROC curves compare verification rates at the same false match rate, but the different cohorts achieve the same false match rate at different thresholds. This means that ROC comparisons are not relevant to operational scenarios that use a fixed decision threshold. We show that, for the ResNet matcher, the two cohorts have approximately equal separation of impostor and genuine distributions. Using ICAO compliance as a standard of image quality, we find that the initial image cohorts have unequal rates of good quality images. The ICAO-compliant subsets of the original image cohorts show improved accuracy, with the main effect being to reducing the low-similarity tail of the genuine distributions.



### Polarimetric Thermal to Visible Face Verification via Self-Attention Guided Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.07344v1
- **DOI**: 10.1109/ICB45273.2019.8987329
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07344v1)
- **Published**: 2019-04-15 21:58:40+00:00
- **Updated**: 2019-04-15 21:58:40+00:00
- **Authors**: Xing Di, Benjamin S. Riggan, Shuowen Hu, Nathaniel J. Short, Vishal M. Patel
- **Comment**: This work is accepted at the 12th IAPR International Conference On
  Biometrics (ICB 2019)
- **Journal**: None
- **Summary**: Polarimetric thermal to visible face verification entails matching two images that contain significant domain differences. Several recent approaches have attempted to synthesize visible faces from thermal images for cross-modal matching. In this paper, we take a different approach in which rather than focusing only on synthesizing visible faces from thermal faces, we also propose to synthesize thermal faces from visible faces. Our intuition is based on the fact that thermal images also contain some discriminative information about the person for verification. Deep features from a pre-trained Convolutional Neural Network (CNN) are extracted from the original as well as the synthesized images. These features are then fused to generate a template which is then used for verification. The proposed synthesis network is based on the self-attention generative adversarial network (SAGAN) which essentially allows efficient attention-guided image synthesis. Extensive experiments on the ARL polarimetric thermal face dataset demonstrate that the proposed method achieves state-of-the-art performance.



### DLBC: A Deep Learning-Based Consensus in Blockchains for Deep Learning Services
- **Arxiv ID**: http://arxiv.org/abs/1904.07349v2
- **DOI**: None
- **Categories**: **cs.DC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.07349v2)
- **Published**: 2019-04-15 22:28:45+00:00
- **Updated**: 2020-01-31 03:44:56+00:00
- **Authors**: Boyang Li, Changhao Chenli, Xiaowei Xu, Yiyu Shi, Taeho Jung
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing artificial intelligence application, deep neural network (DNN) has become an emerging task. However, to train a good deep learning model will suffer from enormous computation cost and energy consumption. Recently, blockchain has been widely used, and during its operation, a huge amount of computation resources are wasted for the Proof of Work (PoW) consensus. In this paper, we propose DLBC to exploit the computation power of miners for deep learning training as proof of useful work instead of calculating hash values. it distinguishes itself from recent proof of useful work mechanisms by addressing various limitations of them. Specifically, DLBC handles multiple tasks, larger model and training datasets, and introduces a comprehensive ranking mechanism that considers tasks difficulty(e.g., model complexity, network burden, data size, queue length). We also applied DNN-watermark [1] to improve the robustness. In Section V, the average overhead of digital signature is 1.25, 0.001, 0.002 and 0.98 seconds, respectively, and the average overhead of network is 3.77, 3.01, 0.37 and 0.41 seconds, respectively. Embedding a watermark takes 3 epochs and removing a watermark takes 30 epochs. This penalty of removing watermark will prevent attackers from stealing, improving, and resubmitting DL models from honest miners.



### Custom Video-Oculography Device and Its Application to Fourth Purkinje Image Detection during Saccades
- **Arxiv ID**: http://arxiv.org/abs/1904.07361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.07361v1)
- **Published**: 2019-04-15 23:10:25+00:00
- **Updated**: 2019-04-15 23:10:25+00:00
- **Authors**: Evgeniy Abdulin, Lee Friedman, Oleg Komogortsev
- **Comment**: 8 pages, 8 figures, appendix
- **Journal**: None
- **Summary**: We built a custom video-based eye-tracker that saves every video frame as a full resolution image (MJPEG). Images can be processed offline for the detection of ocular features, including the pupil and corneal reflection (First Purkinje Image, P1) position. A comparison of multiple algorithms for detection of pupil and corneal reflection can be performed. The system provides for highly flexible stimulus creation, with mixing of graphic, image, and video stimuli. We can change cameras and infrared illuminators depending on the image qualities and frame rate desired. Using this system, we have detected the position of the Fourth Purkinje image (P4) in the frames. We show that when we estimate gaze by calculating P1-P4, signal compares well with gaze estimated with a DPI eye-tracker, which natively detects and tracks the P1 and P4.



