# Arxiv Papers in cs.CV on 2019-04-03
### Deep Policy Hashing Network with Listwise Supervision
- **Arxiv ID**: http://arxiv.org/abs/1904.01728v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01728v1)
- **Published**: 2019-04-03 01:08:18+00:00
- **Updated**: 2019-04-03 01:08:18+00:00
- **Authors**: Shaoying Wang, Haijiang Lai, Yifan Yang, Jian Yin
- **Comment**: 8 pages, accepted by ACM ICMR
- **Journal**: None
- **Summary**: Deep-networks-based hashing has become a leading approach for large-scale image retrieval, which learns a similarity-preserving network to map similar images to nearby hash codes. The pairwise and triplet losses are two widely used similarity preserving manners for deep hashing. These manners ignore the fact that hashing is a prediction task on the list of binary codes. However, learning deep hashing with listwise supervision is challenging in 1) how to obtain the rank list of whole training set when the batch size of the deep network is always small and 2) how to utilize the listwise supervision. In this paper, we present a novel deep policy hashing architecture with two systems are learned in parallel: a query network and a shared and slowly changing database network. The following three steps are repeated until convergence: 1) the database network encodes all training samples into binary codes to obtain a whole rank list, 2) the query network is trained based on policy learning to maximize a reward that indicates the performance of the whole ranking list of binary codes, e.g., mean average precision (MAP), and 3) the database network is updated as the query network. Extensive evaluations on several benchmark datasets show that the proposed method brings substantial improvements over state-of-the-art hashing methods.



### SADIH: Semantic-Aware DIscrete Hashing
- **Arxiv ID**: http://arxiv.org/abs/1904.01739v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1904.01739v2)
- **Published**: 2019-04-03 01:45:05+00:00
- **Updated**: 2019-04-16 04:51:18+00:00
- **Authors**: Zheng Zhang, Guo-sen Xie, Yang Li, Sheng Li, Zi Huang
- **Comment**: Accepted by The Thirty-Third AAAI Conference on Artificial
  Intelligence (AAAI-19)
- **Journal**: None
- **Summary**: Due to its low storage cost and fast query speed, hashing has been recognized to accomplish similarity search in large-scale multimedia retrieval applications. Particularly supervised hashing has recently received considerable research attention by leveraging the label information to preserve the pairwise similarities of data points in the Hamming space. However, there still remain two crucial bottlenecks: 1) the learning process of the full pairwise similarity preservation is computationally unaffordable and unscalable to deal with big data; 2) the available category information of data are not well-explored to learn discriminative hash functions. To overcome these challenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH) framework, which aims to directly embed the transformed semantic information into the asymmetric similarity approximation and discriminative hashing function learning. Specifically, a semantic-aware latent embedding is introduced to asymmetrically preserve the full pairwise similarities while skillfully handle the cumbersome n times n pairwise similarity matrix. Meanwhile, a semantic-aware autoencoder is developed to jointly preserve the data structures in the discriminative latent semantic space and perform data reconstruction. Moreover, an efficient alternating optimization algorithm is proposed to solve the resulting discrete optimization problem. Extensive experimental results on multiple large-scale datasets demonstrate that our SADIH can clearly outperform the state-of-the-art baselines with the additional benefit of lower computational costs.



### FaceQnet: Quality Assessment for Face Recognition based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01740v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1904.01740v2)
- **Published**: 2019-04-03 02:12:31+00:00
- **Updated**: 2019-04-04 07:05:42+00:00
- **Authors**: Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Rudolf Haraksim, Laurent Beslay
- **Comment**: Preprint version of a paper accepted at ICB 2019
- **Journal**: None
- **Summary**: In this paper we develop a Quality Assessment approach for face recognition based on deep learning. The method consists of a Convolutional Neural Network, FaceQnet, that is used to predict the suitability of a specific input image for face recognition purposes. The training of FaceQnet is done using the VGGFace2 database. We employ the BioLab-ICAO framework for labeling the VGGFace2 images with quality information related to their ICAO compliance level. The groundtruth quality labels are obtained using FaceNet to generate comparison scores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making it capable of returning a numerical quality measure for each input image. Finally, we verify if the FaceQnet scores are suitable to predict the expected performance when employing a specific image for face recognition with a COTS face recognition system. Several conclusions can be drawn from this work, most notably: 1) we managed to employ an existing ICAO compliance framework and a pretrained CNN to automatically label data with quality information, 2) we trained FaceQnet for quality estimation by fine-tuning a pre-trained face recognition network (ResNet-50), and 3) we have shown that the predictions from FaceQnet are highly correlated with the face recognition accuracy of a state-of-the-art commercial system not used during development. FaceQnet is publicly available in GitHub.



### Evaluation of the Spatio-Temporal features and GAN for Micro-expression Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1904.01748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01748v1)
- **Published**: 2019-04-03 03:29:48+00:00
- **Updated**: 2019-04-03 03:29:48+00:00
- **Authors**: Sze-Teng Liong, Y. S. Gan, Danna Zheng, Shu-Meng Lic, Hao-Xuan Xua, Han-Zhe Zhang, Ran-Ke Lyu, Kun-Hong Liu
- **Comment**: 15 pages, 16 figures, 6 tables
- **Journal**: None
- **Summary**: Owing to the development and advancement of artificial intelligence, numerous works were established in the human facial expression recognition system. Meanwhile, the detection and classification of micro-expressions are attracting attentions from various research communities in the recent few years. In this paper, we first review the processes of a conventional optical-flow-based recognition system, which comprised of facial landmarks annotations, optical flow guided images computation, features extraction and emotion class categorization. Secondly, a few approaches have been proposed to improve the feature extraction part, such as exploiting GAN to generate more image samples. Particularly, several variations of optical flow are computed in order to generate optimal images to lead to high recognition accuracy. Next, GAN, a combination of Generator and Discriminator, is utilized to generate new "fake" images to increase the sample size. Thirdly, a modified state-of-the-art Convolutional neural networks is proposed. To verify the effectiveness of the the proposed method, the results are evaluated on spontaneous micro-expression databases, namely SMIC, CASME II and SAMM. Both the F1-score and accuracy performance metrics are reported in this paper.



### Fully Using Classifiers for Weakly Supervised Semantic Segmentation with Modified Cues
- **Arxiv ID**: http://arxiv.org/abs/1904.01749v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01749v2)
- **Published**: 2019-04-03 03:30:51+00:00
- **Updated**: 2019-06-14 06:56:51+00:00
- **Authors**: Ting Sun, Lei Tai, Zhihan Gao, Ming Liu, Dit-Yan Yeung
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel weakly-supervised semantic segmentation method using image-level label only. The class-specific activation maps from the well-trained classifiers are used as cues to train a segmentation network. The well-known defects of these cues are coarseness and incompleteness. We use super-pixel to refine them, and fuse the cues extracted from both a color image trained classifier and a gray image trained classifier to compensate for their incompleteness. The conditional random field is adapted to regulate the training process and to refine the outputs further. Besides initializing the segmentation network, the previously trained classifier is also used in the testing phase to suppress the non-existing classes. Experimental results on the PASCAL VOC 2012 dataset illustrate the effectiveness of our method.



### Cross-Entropy Adversarial View Adaptation for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.01755v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01755v2)
- **Published**: 2019-04-03 03:52:21+00:00
- **Updated**: 2019-07-23 02:25:53+00:00
- **Authors**: Lin Wu, Richang Hong, Yang Wang, Meng Wang
- **Comment**: Appearing at IEEE Transactions on Circuits and Systems for Video
  Technology
- **Journal**: None
- **Summary**: Person re-identification (re-ID) is a task of matching pedestrians under disjoint camera views. To recognise paired snapshots, it has to cope with large cross-view variations caused by the camera view shift. Supervised deep neural networks are effective in producing a set of non-linear projections that can transform cross-view images into a common feature space. However, they typically impose a symmetric architecture, yielding the network ill-conditioned on its optimisation. In this paper, we learn view-invariant subspace for person re-ID, and its corresponding similarity metric using an adversarial view adaptation approach. The main contribution is to learn coupled asymmetric mappings regarding view characteristics which are adversarially trained to address the view discrepancy by optimising the cross-entropy view confusion objective. To determine the similarity value, the network is empowered with a similarity discriminator to promote features that are highly discriminant in distinguishing positive and negative pairs. The other contribution includes an adaptive weighing on the most difficult samples to address the imbalance of within/between-identity pairs. Our approach achieves notable improved performance in comparison to state-of-the-arts on benchmark datasets.



### Do not Omit Local Minimizer: a Complete Solution for Pose Estimation from 3D Correspondences
- **Arxiv ID**: http://arxiv.org/abs/1904.01759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01759v2)
- **Published**: 2019-04-03 04:23:13+00:00
- **Updated**: 2019-04-04 13:46:54+00:00
- **Authors**: Lipu Zhou, Shengze Wang, Jiamin Ye, Michael Kaess
- **Comment**: None
- **Journal**: None
- **Summary**: Estimating pose from given 3D correspondences, including point-to-point, point-to-line and point-to-plane correspondences, is a fundamental task in computer vision with many applications. We present a complete solution for this task, including a solution for the minimal problem and the least-squares problem of this task. Previous works mainly focused on finding the global minimizer to address the least-squares problem. However, existing works that show the ability to achieve global minimizer are still unsuitable for real-time applications. Furthermore, as one of contributions of this paper, we prove that there exist ambiguous configurations for any number of lines and planes. These configurations have several solutions in theory, which makes the correct solution may come from a local minimizer. Our algorithm is efficient and able to reveal local minimizers. We employ the Cayley-Gibbs-Rodriguez (CGR) parameterization of the rotation to derive a general rational cost for the three cases of 3D correspondences. The main contribution of this paper is to solve the resulting equation system of the minimal problem and the first-order optimality conditions of the least-squares problem, both of which are of complicated rational forms. The central idea of our algorithm is to introduce intermediate unknowns to simplify the problem. Extensive experimental results show that our algorithm significantly outperforms previous algorithms when the number of correspondences is small. Besides, when the global minimizer is the solution, our algorithm achieves the same accuracy as previous algorithms that have guaranteed global optimality, but our algorithm is applicable to real-time applications.



### VideoBERT: A Joint Model for Video and Language Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01766v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.01766v2)
- **Published**: 2019-04-03 04:40:16+00:00
- **Updated**: 2019-09-11 19:52:54+00:00
- **Authors**: Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid
- **Comment**: ICCV 2019 camera ready
- **Journal**: None
- **Summary**: Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.



### M2KD: Multi-model and Multi-level Knowledge Distillation for Incremental Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.01769v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01769v2)
- **Published**: 2019-04-03 04:54:01+00:00
- **Updated**: 2020-09-05 04:41:31+00:00
- **Authors**: Peng Zhou, Long Mai, Jianming Zhang, Ning Xu, Zuxuan Wu, Larry S. Davis
- **Comment**: None
- **Journal**: BMVC 2020
- **Summary**: Incremental learning targets at achieving good performance on new categories without forgetting old ones. Knowledge distillation has been shown critical in preserving the performance on old classes. Conventional methods, however, sequentially distill knowledge only from the last model, leading to performance degradation on the old classes in later incremental learning steps. In this paper, we propose a multi-model and multi-level knowledge distillation strategy. Instead of sequentially distilling knowledge only from the last model, we directly leverage all previous model snapshots. In addition, we incorporate an auxiliary distillation to further preserve knowledge encoded at the intermediate feature levels. To make the model more memory efficient, we adapt mask based pruning to reconstruct all previous models with a small memory footprint. Experiments on standard incremental learning benchmarks show that our method preserves the knowledge on old classes better and improves the overall performance over standard distillation techniques.



### Target-Aware Deep Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.01772v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01772v1)
- **Published**: 2019-04-03 05:06:39+00:00
- **Updated**: 2019-04-03 05:06:39+00:00
- **Authors**: Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Existing deep trackers mainly use convolutional neural networks pre-trained for generic object recognition task for representations. Despite demonstrated successes for numerous vision tasks, the contributions of using pre-trained deep features for visual tracking are not as significant as that for object recognition. The key issue is that in visual tracking the targets of interest can be arbitrary object class with arbitrary forms. As such, pre-trained deep features are less effective in modeling these targets of arbitrary forms for distinguishing them from the background. In this paper, we propose a novel scheme to learn target-aware features, which can better recognize the targets undergoing significant appearance variations than pre-trained deep features. To this end, we develop a regression loss and a ranking loss to guide the generation of target-active and scale-sensitive features. We identify the importance of each convolutional filter according to the back-propagated gradients and select the target-aware features based on activations for representing the targets. The target-aware features are integrated with a Siamese matching network for visual tracking. Extensive experimental results show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of accuracy and speed.



### Image Generation From Small Datasets via Batch Statistics Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1904.01774v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01774v4)
- **Published**: 2019-04-03 05:24:02+00:00
- **Updated**: 2019-10-23 08:28:38+00:00
- **Authors**: Atsuhiro Noguchi, Tatsuya Harada
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.



### Conditional Adversarial Generative Flow for Controllable Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1904.01782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01782v1)
- **Published**: 2019-04-03 05:58:01+00:00
- **Updated**: 2019-04-03 05:58:01+00:00
- **Authors**: Rui Liu, Yu Liu, Xinyu Gong, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Flow-based generative models show great potential in image synthesis due to its reversible pipeline and exact log-likelihood target, yet it suffers from weak ability for conditional image synthesis, especially for multi-label or unaware conditions. This is because the potential distribution of image conditions is hard to measure precisely from its latent variable $z$. In this paper, based on modeling a joint probabilistic density of an image and its conditions, we propose a novel flow-based generative model named conditional adversarial generative flow (CAGlow). Instead of disentangling attributes from latent space, we blaze a new trail for learning an encoder to estimate the mapping from condition space to latent space in an adversarial manner. Given a specific condition $c$, CAGlow can encode it to a sampled $z$, and then enable robust conditional image synthesis in complex situations like combining person identity with multiple attributes. The proposed CAGlow can be implemented in both supervised and unsupervised manners, thus can synthesize images with conditional information like categories, attributes, and even some unknown properties. Extensive experiments show that CAGlow ensures the independence of different conditions and outperforms regular Glow to a significant extent.



### Patchwork: A Patch-wise Attention Network for Efficient Object Detection and Segmentation in Video Streams
- **Arxiv ID**: http://arxiv.org/abs/1904.01784v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01784v2)
- **Published**: 2019-04-03 05:58:42+00:00
- **Updated**: 2019-08-20 17:11:31+00:00
- **Authors**: Yuning Chai
- **Comment**: ICCV 2019 Camera Ready + Supplementary
- **Journal**: None
- **Summary**: Recent advances in single-frame object detection and segmentation techniques have motivated a wide range of works to extend these methods to process video streams. In this paper, we explore the idea of hard attention aimed for latency-sensitive applications. Instead of reasoning about every frame separately, our method selects and only processes a small sub-window of the frame. Our technique then makes predictions for the full frame based on the sub-windows from previous frames and the update from the current sub-window. The latency reduction by this hard attention mechanism comes at the cost of degraded accuracy. We made two contributions to address this. First, we propose a specialized memory cell that recovers lost context when processing sub-windows. Secondly, we adopt a Q-learning-based policy training strategy that enables our approach to intelligently select the sub-windows such that the staleness in the memory hurts the performance the least. Our experiments suggest that our approach reduces the latency by approximately four times without significantly sacrificing the accuracy on the ImageNet VID video object detection dataset and the DAVIS video object segmentation dataset. We further demonstrate that we can reinvest the saved computation into other parts of the network, and thus resulting in an accuracy increase at a comparable computational cost as the original system and beating other recently proposed state-of-the-art methods in the low latency range.



### Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1904.01786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01786v1)
- **Published**: 2019-04-03 06:06:43+00:00
- **Updated**: 2019-04-03 06:06:43+00:00
- **Authors**: Shichen Liu, Tianye Li, Weikai Chen, Hao Li
- **Comment**: This is a substantially revised version of previous submission:
  arXiv:1901.05567
- **Journal**: None
- **Summary**: Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers.



### MAVNet: an Effective Semantic Segmentation Micro-Network for MAV-based Tasks
- **Arxiv ID**: http://arxiv.org/abs/1904.01795v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01795v2)
- **Published**: 2019-04-03 06:36:26+00:00
- **Updated**: 2019-06-08 23:03:57+00:00
- **Authors**: Ty Nguyen, Shreyas S. Shivakumar, Ian D. Miller, James Keller, Elijah S. Lee, Alex Zhou, Tolga Ozaslan, Giuseppe Loianno, Joseph H. Harwood, Jennifer Wozencraft, Camillo J. Taylor, Vijay Kumar
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Real-time semantic image segmentation on platforms subject to size, weight and power (SWaP) constraints is a key area of interest for air surveillance and inspection. In this work, we propose MAVNet: a small, light-weight, deep neural network for real-time semantic segmentation on micro Aerial Vehicles (MAVs). MAVNet, inspired by ERFNet, features 400 times fewer parameters and achieves comparable performance with some reference models in empirical experiments. Our model achieves a trade-off between speed and accuracy, achieving up to 48 FPS on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high resolution imagery. Additionally, we provide two novel datasets that represent challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks and verify MAVNet on these datasets. Our algorithm and datasets are made publicly available.



### Correlation Congruence for Knowledge Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.01802v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01802v1)
- **Published**: 2019-04-03 06:58:10+00:00
- **Updated**: 2019-04-03 06:58:10+00:00
- **Authors**: Baoyun Peng, Xiao Jin, Jiaheng Liu, Shunfeng Zhou, Yichao Wu, Yu Liu, Dongsheng Li, Zhaoning Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information, but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and achieves state-of-the-art accuracy compared with other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.



### GFF: Gated Fully Fusion for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.01803v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01803v2)
- **Published**: 2019-04-03 07:00:16+00:00
- **Updated**: 2020-02-24 09:50:37+00:00
- **Authors**: Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Kuiyuan Yang
- **Comment**: accepted by AAAI-2020(oral)
- **Journal**: None
- **Summary**: Semantic segmentation generates comprehensive understanding of scenes through densely predicting the category for each pixel. High-level features from Deep Convolutional Neural Networks already demonstrate their effectiveness in semantic segmentation tasks, however the coarse resolution of high-level features often leads to inferior results for small/thin objects where detailed information is important. It is natural to consider importing low level features to compensate for the lost detailed information in high-level features.Unfortunately, simply combining multi-level features suffers from the semantic gap among them. In this paper, we propose a new architecture, named Gated Fully Fusion (GFF), to selectively fuse features from multiple levels using gates in a fully connected way. Specifically, features at each level are enhanced by higher-level features with stronger semantics and lower-level features with more details, and gates are used to control the propagation of useful information which significantly reduces the noises during fusion. We achieve the state of the art results on four challenging scene parsing datasets including Cityscapes, Pascal Context, COCO-stuff and ADE20K.



### Robust object extraction from remote sensing data
- **Arxiv ID**: http://arxiv.org/abs/1904.12586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.12586v1)
- **Published**: 2019-04-03 07:08:53+00:00
- **Updated**: 2019-04-03 07:08:53+00:00
- **Authors**: Sophie Crommelinck, Mila Koeva, Michael Ying Yang, George Vosselman
- **Comment**: unpublished study (15 pages)
- **Journal**: None
- **Summary**: The extraction of object outlines has been a research topic during the last decades. In spite of advances in photogrammetry, remote sensing and computer vision, this task remains challenging due to object and data complexity. The development of object extraction approaches is promoted through publically available benchmark datasets and evaluation frameworks. Many aspects of performance evaluation have already been studied. This study collects the best practices from literature, puts the various aspects in one evaluation framework, and demonstrates its usefulness to a case study on mapping object outlines. The evaluation framework includes five dimensions: the robustness to changes in resolution, input, location, parameters, and application. Examples for investigating these dimensions are provided, as well as accuracy measures for their qualitative analysis. The measures consist of time efficiency and a procedure for line-based accuracy assessment regarding quantitative completeness and spatial correctness. The delineation approach to which the evaluation framework is applied, was previously introduced and is substantially improved in this study.



### SFNet: Learning Object-aware Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1904.01810v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01810v2)
- **Published**: 2019-04-03 07:33:21+00:00
- **Updated**: 2019-04-05 01:10:03+00:00
- **Authors**: Junghyup Lee, Dohyung Kim, Jean Ponce, Bumsub Ham
- **Comment**: cvpr 2019 oral paper
- **Journal**: None
- **Summary**: We address the problem of semantic correspondence, that is, establishing a dense flow field between images depicting different instances of the same object or scene category. We propose to use images annotated with binary foreground masks and subjected to synthetic geometric deformations to train a convolutional neural network (CNN) for this task. Using these masks as part of the supervisory signal offers a good compromise between semantic flow methods, where the amount of training data is limited by the cost of manually selecting point correspondences, and semantic alignment ones, where the regression of a single global geometric transformation between images may be sensitive to image-specific details such as background clutter. We propose a new CNN architecture, dubbed SFNet, which implements this idea. It leverages a new and differentiable version of the argmax function for end-to-end training, with a loss that combines mask and flow consistency with smoothness terms. Experimental results demonstrate the effectiveness of our approach, which significantly outperforms the state of the art on standard benchmarks.



### Unsupervised Deep Tracking
- **Arxiv ID**: http://arxiv.org/abs/1904.01828v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01828v1)
- **Published**: 2019-04-03 08:14:11+00:00
- **Updated**: 2019-04-03 08:14:11+00:00
- **Authors**: Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li
- **Comment**: to appear in CVPR 2019
- **Journal**: None
- **Summary**: We propose an unsupervised visual tracking method in this paper. Different from existing approaches using extensive annotated data for supervised learning, our CNN model is trained on large-scale unlabeled videos in an unsupervised manner. Our motivation is that a robust tracker should be effective in both the forward and backward predictions (i.e., the tracker can forward localize the target object in successive frames and backtrace to its initial position in the first frame). We build our framework on a Siamese correlation filter network, which is trained using unlabeled raw videos. Meanwhile, we propose a multiple-frame validation method and a cost-sensitive loss to facilitate unsupervised learning. Without bells and whistles, the proposed unsupervised tracker achieves the baseline accuracy of fully supervised trackers, which require complete and accurate labels during training. Furthermore, unsupervised framework exhibits a potential in leveraging unlabeled or weakly labeled data to further improve the tracking accuracy.



### Learning Context Graph for Person Search
- **Arxiv ID**: http://arxiv.org/abs/1904.01830v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01830v1)
- **Published**: 2019-04-03 08:16:11+00:00
- **Updated**: 2019-04-03 08:16:11+00:00
- **Authors**: Yichao Yan, Qiang Zhang, Bingbing Ni, Wendong Zhang, Minghao Xu, Xiaokang Yang
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: Person re-identification has achieved great progress with deep convolutional neural networks. However, most previous methods focus on learning individual appearance feature embedding, and it is hard for the models to handle difficult situations with different illumination, large pose variance and occlusion. In this work, we take a step further and consider employing context information for person search. For a probe-gallery pair, we first propose a contextual instance expansion module, which employs a relative attention module to search and filter useful context information in the scene. We also build a graph learning framework to effectively employ context pairs to update target similarity. These two modules are built on top of a joint detection and instance feature learning framework, which improves the discriminativeness of the learned features. The proposed framework achieves state-of-the-art performance on two widely used person search datasets.



### A Comprehensive Overhaul of Feature Distillation
- **Arxiv ID**: http://arxiv.org/abs/1904.01866v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01866v2)
- **Published**: 2019-04-03 09:17:59+00:00
- **Updated**: 2019-08-09 05:07:09+00:00
- **Authors**: Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, Jin Young Choi
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at https://sites.google.com/view/byeongho-heo/overhaul



### Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1904.01870v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01870v1)
- **Published**: 2019-04-03 09:23:03+00:00
- **Updated**: 2019-04-03 09:23:03+00:00
- **Authors**: Shanshan Zhao, Huan Fu, Mingming Gong, Dacheng Tao
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Supervised depth estimation has achieved high accuracy due to the advanced deep network architectures. Since the groundtruth depth labels are hard to obtain, recent methods try to learn depth estimation networks in an unsupervised way by exploring unsupervised cues, which are effective but less reliable than true labels. An emerging way to resolve this dilemma is to transfer knowledge from synthetic images with ground truth depth via domain adaptation techniques. However, these approaches overlook specific geometric structure of the natural images in the target domain (i.e., real data), which is important for high-performing depth prediction. Motivated by the observation, we propose a geometry-aware symmetric domain adaptation framework (GASDA) to explore the labels in the synthetic data and epipolar geometry in the real data jointly. Moreover, by training two image style translators and depth estimators symmetrically in an end-to-end network, our model achieves better image style transfer and generates high-quality depth maps. The experimental results demonstrate the effectiveness of our proposed method and comparable performance against the state-of-the-art. Code will be publicly available at: https://github.com/sshan-zhao/GASDA.



### DADA: Depth-aware Domain Adaptation in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.01886v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01886v3)
- **Published**: 2019-04-03 09:59:06+00:00
- **Updated**: 2019-08-19 09:22:00+00:00
- **Authors**: Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick Pérez
- **Comment**: Accepted in ICCV'19
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real "target domain" data models that are trained on annotated images from a different "source domain", notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks.



### Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1904.01892v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01892v2)
- **Published**: 2019-04-03 10:11:22+00:00
- **Updated**: 2019-04-05 03:16:52+00:00
- **Authors**: Fei Xue, Xin Wang, Shunkai Li, Qiuyuan Wang, Junqiu Wang, Hongbin Zha
- **Comment**: Accepted to CVPR2019 (oral presentation)
- **Journal**: None
- **Summary**: Most previous learning-based visual odometry (VO) methods take VO as a pure tracking problem. In contrast, we present a VO framework by incorporating two additional components called Memory and Refining. The Memory component preserves global information by employing an adaptive and efficient selection strategy. The Refining component ameliorates previous results with the contexts stored in the Memory by adopting a spatial-temporal attention mechanism for feature distilling. Experiments on the KITTI and TUM-RGBD benchmark datasets demonstrate that our method outperforms state-of-the-art learning-based methods by a large margin and produces competitive results against classic monocular VO approaches. Especially, our model achieves outstanding performance in challenging scenarios such as texture-less regions and abrupt motions, where classic VO algorithms tend to fail.



### Semantic Bilinear Pooling for Fine-Grained Recognition
- **Arxiv ID**: http://arxiv.org/abs/1904.01893v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01893v4)
- **Published**: 2019-04-03 10:14:57+00:00
- **Updated**: 2021-11-25 13:41:07+00:00
- **Authors**: Xinjie Li, Chun Yang, Songlu Chen, Chao Zhu, Xu-Cheng Yin
- **Comment**: Accepted by ICPR2020
- **Journal**: None
- **Summary**: Naturally, fine-grained recognition, e.g., vehicle identification or bird classification, has specific hierarchical labels, where fine categories are always harder to be classified than coarse categories. However, most of the recent deep learning based methods neglect the semantic structure of fine-grained objects and do not take advantage of the traditional fine-grained recognition techniques (e.g. coarse-to-fine classification). In this paper, we propose a novel framework with a two-branch network (coarse branch and fine branch), i.e., semantic bilinear pooling, for fine-grained recognition with a hierarchical label tree. This framework can adaptively learn the semantic information from the hierarchical levels. Specifically, we design a generalized cross-entropy loss for the training of the proposed framework to fully exploit the semantic priors via considering the relevance between adjacent levels and enlarge the distance between samples of different coarse classes. Furthermore, our method leverages only the fine branch when testing so that it adds no overhead to the testing time. Experimental results show that our proposed method achieves state-of-the-art performance on four public datasets.



### What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.01906v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01906v4)
- **Published**: 2019-04-03 10:45:29+00:00
- **Updated**: 2019-12-18 11:40:03+00:00
- **Authors**: Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, Hwalsuk Lee
- **Comment**: Oral paper at ICCV'19. Our code is publicly available.
  (https://github.com/clovaai/deep-text-recognition-benchmark)
- **Journal**: None
- **Summary**: Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules.



### Multi-layered Spiking Neural Network with Target Timestamp Threshold Adaptation and STDP
- **Arxiv ID**: http://arxiv.org/abs/1904.01908v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1904.01908v1)
- **Published**: 2019-04-03 10:47:34+00:00
- **Updated**: 2019-04-03 10:47:34+00:00
- **Authors**: Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, Pierre Boulet
- **Comment**: None
- **Journal**: None
- **Summary**: Spiking neural networks (SNNs) are good candidates to produce ultra-energy-efficient hardware. However, the performance of these models is currently behind traditional methods. Introducing multi-layered SNNs is a promising way to reduce this gap. We propose in this paper a new threshold adaptation system which uses a timestamp objective at which neurons should fire. We show that our method leads to state-of-the-art classification rates on the MNIST dataset (98.60%) and the Faces/Motorbikes dataset (99.46%) with an unsupervised SNN followed by a linear SVM. We also investigate the sparsity level of the network by testing different inhibition policies and STDP rules.



### ICface: Interpretable and Controllable Face Reenactment Using GANs
- **Arxiv ID**: http://arxiv.org/abs/1904.01909v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01909v2)
- **Published**: 2019-04-03 10:49:03+00:00
- **Updated**: 2020-01-17 14:30:59+00:00
- **Authors**: Soumya Tripathy, Juho Kannala, Esa Rahtu
- **Comment**: Accepted in WACV-2020
- **Journal**: None
- **Summary**: This paper presents a generic face animator that is able to control the pose and expressions of a given face image. The animation is driven by human interpretable control signals consisting of head pose angles and the Action Unit (AU) values. The control information can be obtained from multiple sources including external driving videos and manual controls. Due to the interpretable nature of the driving signal, one can easily mix the information between multiple sources (e.g. pose from one image and expression from another) and apply selective post-production editing. The proposed face animator is implemented as a two-stage neural network model that is learned in a self-supervised manner using a large video collection. The proposed Interpretable and Controllable face reenactment network (ICface) is compared to the state-of-the-art neural network-based face animation techniques in multiple tasks. The results indicate that ICface produces better visual quality while being more versatile than most of the comparison methods. The introduced model could provide a lightweight and easy to use tool for a multitude of advanced image and video editing tasks.



### CubiCasa5K: A Dataset and an Improved Multi-Task Model for Floorplan Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1904.01920v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01920v1)
- **Published**: 2019-04-03 11:23:59+00:00
- **Updated**: 2019-04-03 11:23:59+00:00
- **Authors**: Ahti Kalervo, Juha Ylioinas, Markus Häikiö, Antti Karhu, Juho Kannala
- **Comment**: None
- **Journal**: None
- **Summary**: Better understanding and modelling of building interiors and the emergence of more impressive AR/VR technology has brought up the need for automatic parsing of floorplan images. However, there is a clear lack of representative datasets to investigate the problem further. To address this shortcoming, this paper presents a novel image dataset called CubiCasa5K, a large-scale floorplan image dataset containing 5000 samples annotated into over 80 floorplan object categories. The dataset annotations are performed in a dense and versatile manner by using polygons for separating the different objects. Diverging from the classical approaches based on strong heuristics and low-level pixel operations, we present a method relying on an improved multi-task convolutional neural network. By releasing the novel dataset and our implementations, this study significantly boosts the research on automatic floorplan image analysis as it provides a richer set of tools for investigating the problem in a more comprehensive manner.



### Character Region Awareness for Text Detection
- **Arxiv ID**: http://arxiv.org/abs/1904.01941v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01941v1)
- **Published**: 2019-04-03 12:00:33+00:00
- **Updated**: 2019-04-03 12:00:33+00:00
- **Authors**: Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, Hwalsuk Lee
- **Comment**: 12 pages, 11 figures, Accepted by CVPR 2019
- **Journal**: None
- **Summary**: Scene text detection methods based on neural networks have emerged recently and have shown promising results. Previous methods trained with rigid word-level bounding boxes exhibit limitations in representing the text region in an arbitrary shape. In this paper, we propose a new scene text detection method to effectively detect text area by exploring each character and affinity between characters. To overcome the lack of individual character level annotations, our proposed framework exploits both the given character-level annotations for synthetic images and the estimated character-level ground-truths for real images acquired by the learned interim model. In order to estimate affinity between characters, the network is trained with the newly proposed representation for affinity. Extensive experiments on six benchmarks, including the TotalText and CTW-1500 datasets which contain highly curved texts in natural images, demonstrate that our character-level text detection significantly outperforms the state-of-the-art detectors. According to the results, our proposed method guarantees high flexibility in detecting complicated scene text images, such as arbitrarily-oriented, curved, or deformed texts.



### Stacked Semantic-Guided Network for Zero-Shot Sketch-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1904.01971v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01971v2)
- **Published**: 2019-04-03 12:33:47+00:00
- **Updated**: 2019-10-18 02:31:42+00:00
- **Authors**: Hao Wang, Cheng Deng, Xinxu Xu, Wei Liu, Xinbo Gao, Dacheng Tao
- **Comment**: withdraw this paper due to minor problem in problem defination
- **Journal**: None
- **Summary**: Zero-shot sketch-based image retrieval (ZS-SBIR) is a task of cross-domain image retrieval from a natural image gallery with free-hand sketch under a zero-shot scenario. Previous works mostly focus on a generative approach that takes a highly abstract and sparse sketch as input and then synthesizes the corresponding natural image. However, the intrinsic visual sparsity and large intra-class variance of the sketch make the learning of the conditional decoder more difficult and hence achieve unsatisfactory retrieval performance. In this paper, we propose a novel stacked semantic-guided network to address the unique characteristics of sketches in ZS-SBIR. Specifically, we devise multi-layer feature fusion networks that incorporate different intermediate feature representation information in a deep neural network to alleviate the intrinsic sparsity of sketches. In order to improve visual knowledge transfer from seen to unseen classes, we elaborate a coarse-to-fine conditional decoder that generates coarse-grained category-specific corresponding features first (taking auxiliary semantic information as conditional input) and then generates fine-grained instance-specific corresponding features (taking sketch representation as conditional input). Furthermore, regression loss and classification loss are utilized to preserve the semantic and discriminative information of the synthesized features respectively. Extensive experiments on the large-scale Sketchy dataset and TU-Berlin dataset demonstrate that our proposed approach outperforms state-of-the-art methods by more than 20\% in retrieval performance.



### D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1904.01975v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.01975v2)
- **Published**: 2019-04-03 12:40:08+00:00
- **Updated**: 2019-06-03 06:42:25+00:00
- **Authors**: Zhengping Che, Guangyu Li, Tracy Li, Bo Jiang, Xuefeng Shi, Xinsheng Zhang, Ying Lu, Guobin Wu, Yan Liu, Jieping Ye
- **Comment**: None
- **Journal**: None
- **Summary**: Driving datasets accelerate the development of intelligent driving and related computer vision technologies, while substantial and detailed annotations serve as fuels and powers to boost the efficacy of such datasets to improve learning-based models. We propose D$^2$-City, a large-scale comprehensive collection of dashcam videos collected by vehicles on DiDi's platform. D$^2$-City contains more than 10000 video clips which deeply reflect the diversity and complexity of real-world traffic scenarios in China. We also provide bounding boxes and tracking annotations of 12 classes of objects in all frames of 1000 videos and detection annotations on keyframes for the remainder of the videos. Compared with existing datasets, D$^2$-City features data in varying weather, road, and traffic conditions and a huge amount of elaborate detection and tracking annotations. By bringing a diverse set of challenging cases to the community, we expect the D$^2$-City dataset will advance the perception and related areas of intelligent driving.



### Hybrid Cosine Based Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1904.01987v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.01987v1)
- **Published**: 2019-04-03 13:06:43+00:00
- **Updated**: 2019-04-03 13:06:43+00:00
- **Authors**: Adrià Ciurana, Albert Mosella-Montoro, Javier Ruiz-Hidalgo
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have demonstrated their capability to solve different kind of problems in a very huge number of applications. However, CNNs are limited for their computational and storage requirements. These limitations make difficult to implement these kind of neural networks on embedded devices such as mobile phones, smart cameras or advanced driving assistance systems. In this paper, we present a novel layer named Hybrid Cosine Based Convolution that replaces standard convolutional layers using cosine basis to generate filter weights. The proposed layers provide several advantages: faster convergence in training, the receptive field can be increased at no cost and substantially reduce the number of parameters. We evaluate our proposed layers on three competitive classification tasks where our proposed layers can achieve similar (and in some cases better) performances than VGG and ResNet architectures.



### Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1904.01990v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.01990v1)
- **Published**: 2019-04-03 13:11:59+00:00
- **Updated**: 2019-04-03 13:11:59+00:00
- **Authors**: Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, Yi Yang
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: This paper considers the domain adaptive person re-identification (re-ID) problem: learning a re-ID model from a labeled source domain and an unlabeled target domain. Conventional methods are mainly to reduce feature distribution gap between the source and target domains. However, these studies largely neglect the intra-domain variations in the target domain, which contain critical factors influencing the testing performance on the target domain. In this work, we comprehensively investigate into the intra-domain variations of the target domain and propose to generalize the re-ID model w.r.t three types of the underlying invariance, i.e., exemplar-invariance, camera-invariance and neighborhood-invariance. To achieve this goal, an exemplar memory is introduced to store features of the target domain and accommodate the three invariance properties. The memory allows us to enforce the invariance constraints over global training batch without significantly increasing computation cost. Experiment demonstrates that the three invariance properties and the proposed memory are indispensable towards an effective domain adaptation system. Results on three re-ID domains show that our domain adaptation accuracy outperforms the state of the art by a large margin. Code is available at: https://github.com/zhunzhong07/ECN



### Deep Landscape Features for Improving Vector-borne Disease Prediction
- **Arxiv ID**: http://arxiv.org/abs/1904.01994v1
- **DOI**: None
- **Categories**: **cs.CY**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.01994v1)
- **Published**: 2019-04-03 13:29:58+00:00
- **Updated**: 2019-04-03 13:29:58+00:00
- **Authors**: Nabeel Abdur Rehman, Umar Saif, Rumi Chunara
- **Comment**: 10 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: The global population at risk of mosquito-borne diseases such as dengue, yellow fever, chikungunya and Zika is expanding. Infectious disease models commonly incorporate environmental measures like temperature and precipitation. Given increasing availability of high-resolution satellite imagery, here we consider including landscape features from satellite imagery into infectious disease prediction models. To do so, we implement a Convolutional Neural Network (CNN) model trained on Imagenet data and labelled landscape features in satellite data from London. We then incorporate landscape features from satellite image data from Pakistan, labelled using the CNN, in a well-known Susceptible-Infectious-Recovered epidemic model, alongside dengue case data from 2012-2016 in Pakistan. We study improvement of the prediction model for each of the individual landscape features, and assess the feasibility of using image labels from a different place. We find that incorporating satellite-derived landscape features can improve prediction of outbreaks, which is important for proactive and strategic surveillance and control programmes.



### Super accurate low latency object detection on a surveillance UAV
- **Arxiv ID**: http://arxiv.org/abs/1904.02024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02024v1)
- **Published**: 2019-04-03 14:29:07+00:00
- **Updated**: 2019-04-03 14:29:07+00:00
- **Authors**: Maarten Vandersteegen, Kristof Vanbeeck, Toon goedeme
- **Comment**: Conference MVA2019, Source code: https://gitlab.com/EAVISE/jetnet
- **Journal**: None
- **Summary**: Drones have proven to be useful in many industry segments such as security and surveillance, where e.g. on-board real-time object tracking is a necessity for autonomous flying guards. Tracking and following suspicious objects is therefore required in real-time on limited hardware. With an object detector in the loop, low latency becomes extremely important. In this paper, we propose a solution to make object detection for UAVs both fast and super accurate. We propose a multi-dataset learning strategy yielding top eye-sky object detection accuracy. Our model generalizes well on unseen data and can cope with different flying heights, optically zoomed-in shots and different viewing angles. We apply optimization steps such that we achieve minimal latency on embedded on-board hardware by fusing layers, quantizing calculations to 16-bit floats and 8-bit integers, with negligible loss in accuracy. We validate on NVIDIA's Jetson TX2 and Jetson Xavier platforms where we achieve a speed-wise performance boost of more than 10x.



### CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth
- **Arxiv ID**: http://arxiv.org/abs/1904.02028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02028v1)
- **Published**: 2019-04-03 14:31:35+00:00
- **Updated**: 2019-04-03 14:31:35+00:00
- **Authors**: Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, Javier Civera
- **Comment**: Camera ready version for CVPR 2019. Project page:
  http://webdiis.unizar.es/~jmfacil/camconvs/
- **Journal**: None
- **Summary**: Single-view depth estimation suffers from the problem that a network trained on images from one camera does not generalize to images taken with a different camera model. Thus, changing the camera model requires collecting an entirely new training dataset. In this work, we propose a new type of convolution that can take the camera parameters into account, thus allowing neural networks to learn calibration-aware patterns. Experiments confirm that this improves the generalization capabilities of depth prediction networks considerably, and clearly outperforms the state of the art when the train and test images are acquired with different cameras.



### Towards Computational Models and Applications of Insect Visual Systems for Motion Perception: A Review
- **Arxiv ID**: http://arxiv.org/abs/1904.02048v1
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1904.02048v1)
- **Published**: 2019-04-03 15:10:29+00:00
- **Updated**: 2019-04-03 15:10:29+00:00
- **Authors**: Qinbing Fu, Hongxin Wang, Cheng Hu, Shigang Yue
- **Comment**: 90 pages, 34 figures, a comprehensive review paper
- **Journal**: None
- **Summary**: Motion perception is a critical capability determining a variety of aspects of insects' life, including avoiding predators, foraging and so forth. A good number of motion detectors have been identified in the insects' visual pathways. Computational modelling of these motion detectors has not only been providing effective solutions to artificial intelligence, but also benefiting the understanding of complicated biological visual systems. These biological mechanisms through millions of years of evolutionary development will have formed solid modules for constructing dynamic vision systems for future intelligent machines. This article reviews the computational motion perception models originating from biological research of insects' visual systems in the literature. These motion perception models or neural networks comprise the looming sensitive neuronal models of lobula giant movement detectors (LGMDs) in locusts, the translation sensitive neural systems of direction selective neurons (DSNs) in fruit flies, bees and locusts, as well as the small target motion detectors (STMDs) in dragonflies and hover flies. We also review the applications of these models to robots and vehicles. Through these modelling studies, we summarise the methodologies that generate different direction and size selectivity in motion perception. At last, we discuss about multiple systems integration and hardware realisation of these bio-inspired motion perception models.



### Estimating Chlorophyll a Concentrations of Several Inland Waters with Hyperspectral Data and Machine Learning Models
- **Arxiv ID**: http://arxiv.org/abs/1904.02052v1
- **DOI**: 10.5194/isprs-annals-IV-2-W5-609-2019
- **Categories**: **cs.CV**, q-bio.QM, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1904.02052v1)
- **Published**: 2019-04-03 15:16:36+00:00
- **Updated**: 2019-04-03 15:16:36+00:00
- **Authors**: Philipp M. Maier, Sina Keller
- **Comment**: Accepted at ISPRS Geospatial Week 2019 in Enschede
- **Journal**: None
- **Summary**: Water is a key component of life, the natural environment and human health. For monitoring the conditions of a water body, the chlorophyll a concentration can serve as a proxy for nutrients and oxygen supply. In situ measurements of water quality parameters are often time-consuming, expensive and limited in areal validity. Therefore, we apply remote sensing techniques. During field campaigns, we collected hyperspectral data with a spectrometer and in situ measured chlorophyll a concentrations of 13 inland water bodies with different spectral characteristics. One objective of this study is to estimate chlorophyll a concentrations of these inland waters by applying three machine learning regression models: Random Forest, Support Vector Machine and an Artificial Neural Network. Additionally, we simulate four different hyperspectral resolutions of the spectrometer data to investigate the effects on the estimation performance. Furthermore, the application of first order derivatives of the spectra is evaluated in turn to the regression performance. This study reveals the potential of combining machine learning approaches and remote sensing data for inland waters. Each machine learning model achieves an R2-score between 80 % to 90 % for the regression on chlorophyll a concentrations. The random forest model benefits clearly from the applied derivatives of the spectra. In further studies, we will focus on the application of machine learning models on spectral satellite data to enhance the area-wide estimation of chlorophyll a concentration for inland waters.



### Interpreting Adversarial Examples by Activation Promotion and Suppression
- **Arxiv ID**: http://arxiv.org/abs/1904.02057v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1904.02057v2)
- **Published**: 2019-04-03 15:25:21+00:00
- **Updated**: 2019-09-30 18:32:03+00:00
- **Authors**: Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan, Chuang Gan, Xue Lin
- **Comment**: None
- **Journal**: None
- **Summary**: It is widely known that convolutional neural networks (CNNs) are vulnerable to adversarial examples: images with imperceptible perturbations crafted to fool classifiers. However, interpretability of these perturbations is less explored in the literature. This work aims to better understand the roles of adversarial perturbations and provide visual explanations from pixel, image and network perspectives. We show that adversaries have a promotion-suppression effect (PSE) on neurons' activations and can be primarily categorized into three types: i) suppression-dominated perturbations that mainly reduce the classification score of the true label, ii) promotion-dominated perturbations that focus on boosting the confidence of the target label, and iii) balanced perturbations that play a dual role in suppression and promotion. We also provide image-level interpretability of adversarial examples. This links PSE of pixel-level perturbations to class-specific discriminative image regions localized by class activation mapping (Zhou et al. 2016). Further, we examine the adversarial effect through network dissection (Bau et al. 2017), which offers concept-level interpretability of hidden units. We show that there exists a tight connection between the units' sensitivity to adversarial attacks and their interpretability on semantic concepts. Lastly, we provide some new insights from our interpretation to improve the adversarial robustness of networks.



### A Visual Neural Network for Robust Collision Perception in Vehicle Driving Scenarios
- **Arxiv ID**: http://arxiv.org/abs/1904.02074v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1904.02074v1)
- **Published**: 2019-04-03 16:05:56+00:00
- **Updated**: 2019-04-03 16:05:56+00:00
- **Authors**: Qinbing Fu, Nicola Bellotto, Huatian Wang, F. Claire Rind, Hongxin Wang, Shigang Yue
- **Comment**: 12 pages, 7 figures, conference, springer format
- **Journal**: None
- **Summary**: This research addresses the challenging problem of visual collision detection in very complex and dynamic real physical scenes, specifically, the vehicle driving scenarios. This research takes inspiration from a large-field looming sensitive neuron, i.e., the lobula giant movement detector (LGMD) in the locust's visual pathways, which represents high spike frequency to rapid approaching objects. Building upon our previous models, in this paper we propose a novel inhibition mechanism that is capable of adapting to different levels of background complexity. This adaptive mechanism works effectively to mediate the local inhibition strength and tune the temporal latency of local excitation reaching the LGMD neuron. As a result, the proposed model is effective to extract colliding cues from complex dynamic visual scenes. We tested the proposed method using a range of stimuli including simulated movements in grating backgrounds and shifting of a natural panoramic scene, as well as vehicle crash video sequences. The experimental results demonstrate the proposed method is feasible for fast collision perception in real-world situations with potential applications in future autonomous vehicles.



### Learning for Multi-Type Subspace Clustering
- **Arxiv ID**: http://arxiv.org/abs/1904.02075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02075v1)
- **Published**: 2019-04-03 16:08:38+00:00
- **Updated**: 2019-04-03 16:08:38+00:00
- **Authors**: Xun Xu, Loong-Fah Cheong, Zhuwen Li
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1901.10254
- **Journal**: None
- **Summary**: Subspace clustering has been extensively studied from the hypothesis-and-test, algebraic, and spectral clustering based perspectives. Most assume that only a single type/class of subspace is present. Generalizations to multiple types are non-trivial, plagued by challenges such as choice of types and numbers of models, sampling imbalance and parameter tuning. In this work, we formulate the multi-type subspace clustering problem as one of learning non-linear subspace filters via deep multi-layer perceptrons (mlps). The response to the learnt subspace filters serve as the feature embedding that is clustering-friendly, i.e., points of the same clusters will be embedded closer together through the network. For inference, we apply K-means to the network output to cluster the data. Experiments are carried out on both synthetic and real world multi-type fitting problems, producing state-of-the-art results.



### Graph based Nearest Neighbor Search: Promises and Failures
- **Arxiv ID**: http://arxiv.org/abs/1904.02077v5
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV, cs.DS, cs.MM, cs.SI
- **Links**: [PDF](http://arxiv.org/pdf/1904.02077v5)
- **Published**: 2019-04-03 16:12:55+00:00
- **Updated**: 2019-06-18 09:07:06+00:00
- **Authors**: Peng-Cheng Lin, Wan-Lei Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, graph based nearest neighbor search gets more and more popular on large-scale retrieval tasks. The attractiveness of this type of approaches lies in its superior performance over most of the known nearest neighbor search approaches as well as its genericness to various metrics. In this paper, the role of two strategies, namely hierarchical structure and graph diversification that are adopted as the key steps in the graph based approaches, is investigated. We find the hierarchical structure could not achieve "much better logarithmic complexity scaling" as it was claimed in the original paper, particularly on high dimensional cases. Moreover, we find that similar high search speed efficiency as the one with hierarchical structure could be achieved with the support of flat k-NN graph after graph diversification. Finally, we point out the difficulty, that is faced by most of the graph based search approaches, is directly linked to "curse of dimensionality".



### FatSegNet : A Fully Automated Deep Learning Pipeline for Adipose Tissue Segmentation on Abdominal Dixon MRI
- **Arxiv ID**: http://arxiv.org/abs/1904.02082v2
- **DOI**: 10.1002/mrm.28022
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02082v2)
- **Published**: 2019-04-03 16:22:37+00:00
- **Updated**: 2019-11-05 13:17:50+00:00
- **Authors**: Santiago Estrada, Ran Lu, Sailesh Conjeti, Ximena Orozco-Ruiz, Joana Panos-Willuhn, Monique M. B Breteler, Martin Reuter
- **Comment**: Submitted to Magnetic Resonance in Medicine
- **Journal**: None
- **Summary**: Purpose: Development of a fast and fully automated deep learning pipeline (FatSegNet) to accurately identify, segment, and quantify abdominal adipose tissue on Dixon MRI from the Rhineland Study - a large prospective population-based study. Method: FatSegNet is composed of three stages: (i) consistent localization of the abdominal region using two 2D-Competitive Dense Fully Convolutional Networks (CDFNet), (ii) segmentation of adipose tissue on three views by independent CDFNets, and (iii) view aggregation. FatSegNet is trained with 33 manually annotated subjects, and validated by: 1) comparison of segmentation accuracy against a testingset covering a wide range of body mass index (BMI), 2) test-retest reliability, and 3) robustness in a large cohort study. Results: The CDFNet demonstrates increased robustness compared to traditional deep learning networks. FatSegNet dice score outperforms manual raters on the abdominal visceral adipose tissue (VAT, 0.828 vs. 0.788), and produces comparable results on subcutaneous adipose tissue (SAT, 0.973 vs. 0.982). The pipeline has very small test-retest absolute percentage difference and excellent agreement between scan sessions (VAT: APD = 2.957%, ICC=0.998 and SAT: APD= 3.254%, ICC=0.996). Conclusion: FatSegNet can reliably analyze a 3D Dixon MRI in1 min. It generalizes well to different body shapes, sensitively replicates known VAT and SAT volume effects in a large cohort study, and permits localized analysis of fat compartments.



### Automatic alignment of surgical videos using kinematic data
- **Arxiv ID**: http://arxiv.org/abs/1904.07302v2
- **DOI**: 10.1007/978-3-030-21642-9_14
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.07302v2)
- **Published**: 2019-04-03 16:46:08+00:00
- **Updated**: 2019-04-26 12:27:13+00:00
- **Authors**: Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, François Petitjean, Lhassane Idoumghar, Pierre-Alain Muller
- **Comment**: Accepted at AIME 2019
- **Journal**: None
- **Summary**: Over the past one hundred years, the classic teaching methodology of "see one, do one, teach one" has governed the surgical education systems worldwide. With the advent of Operation Room 2.0, recording video, kinematic and many other types of data during the surgery became an easy task, thus allowing artificial intelligence systems to be deployed and used in surgical and medical practice. Recently, surgical videos has been shown to provide a structure for peer coaching enabling novice trainees to learn from experienced surgeons by replaying those videos. However, the high inter-operator variability in surgical gesture duration and execution renders learning from comparing novice to expert surgical videos a very difficult task. In this paper, we propose a novel technique to align multiple videos based on the alignment of their corresponding kinematic multivariate time series data. By leveraging the Dynamic Time Warping measure, our algorithm synchronizes a set of videos in order to show the same gesture being performed at different speed. We believe that the proposed approach is a valuable addition to the existing learning tools for surgery.



### Target-Tailored Source-Transformation for Scene Graph Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.02104v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02104v2)
- **Published**: 2019-04-03 16:59:49+00:00
- **Updated**: 2020-05-27 14:33:40+00:00
- **Authors**: Wentong Liao, Cuiling Lan, Wenjun Zeng, Michael Ying Yang, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: Scene graph generation aims to provide a semantic and structural description of an image, denoting the objects (with nodes) and their relationships (with edges). The best performing works to date are based on exploiting the context surrounding objects or relations,e.g., by passing information among objects. In these approaches, to transform the representation of source objects is a critical process for extracting information for the use by target objects. In this work, we argue that a source object should give what tar-get object needs and give different objects different information rather than contributing common information to all targets. To achieve this goal, we propose a Target-TailoredSource-Transformation (TTST) method to efficiently propagate information among object proposals and relations. Particularly, for a source object proposal which will contribute information to other target objects, we transform the source object feature to the target object feature domain by simultaneously taking both the source and target into account. We further explore more powerful representations by integrating language prior with the visual context in the transformation for the scene graph generation. By doing so the target object is able to extract target-specific information from the source object and source relation accordingly to refine its representation. Our framework is validated on the Visual Genome bench-mark and demonstrated its state-of-the-art performance for the scene graph generation. The experimental results show that the performance of object detection and visual relation-ship detection are promoted mutually by our method.



### Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1904.02113v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02113v1)
- **Published**: 2019-04-03 17:18:26+00:00
- **Updated**: 2019-04-03 17:18:26+00:00
- **Authors**: Loic Landrieu, Mohamed Boussaha
- **Comment**: CVPR2019
- **Journal**: None
- **Summary**: We propose a new supervized learning framework for oversegmenting 3D point clouds into superpoints. We cast this problem as learning deep embeddings of the local geometry and radiometry of 3D points, such that the border of objects presents high contrasts. The embeddings are computed using a lightweight neural network operating on the points' local neighborhood. Finally, we formulate point cloud oversegmentation as a graph partition problem with respect to the learned embeddings.   This new approach allows us to set a new state-of-the-art in point cloud oversegmentation by a significant margin, on a dense indoor dataset (S3DIS) and a sparse outdoor one (vKITTI). Our best solution requires over five times fewer superpoints to reach similar performance than previously published methods on S3DIS. Furthermore, we show that our framework can be used to improve superpoint-based semantic segmentation algorithms, setting a new state-of-the-art for this task as well.



### Constrained Generative Adversarial Networks for Interactive Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1904.02526v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02526v1)
- **Published**: 2019-04-03 17:59:41+00:00
- **Updated**: 2019-04-03 17:59:41+00:00
- **Authors**: Eric Heim
- **Comment**: To Appear in the Proceedings of the 2019 Conference on Computer
  Vision and Pattern Recognition
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have received a great deal of attention due in part to recent success in generating original, high-quality samples from visual domains. However, most current methods only allow for users to guide this image generation process through limited interactions. In this work we develop a novel GAN framework that allows humans to be "in-the-loop" of the image generation process. Our technique iteratively accepts relative constraints of the form "Generate an image more like image A than image B". After each constraint is given, the user is presented with new outputs from the GAN, informing the next round of feedback. This feedback is used to constrain the output of the GAN with respect to an underlying semantic space that can be designed to model a variety of different notions of similarity (e.g. classes, attributes, object relationships, color, etc.). In our experiments, we show that our GAN framework is able to generate images that are of comparable quality to equivalent unsupervised GANs while satisfying a large number of the constraints provided by users, effectively changing a GAN into one that allows users interactive control over image generation without sacrificing image quality.



### Understanding the efficacy, reliability and resiliency of computer vision techniques for malware detection and future research directions
- **Arxiv ID**: http://arxiv.org/abs/1904.10504v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.10504v1)
- **Published**: 2019-04-03 18:34:20+00:00
- **Updated**: 2019-04-03 18:34:20+00:00
- **Authors**: Li Chen
- **Comment**: Report
- **Journal**: None
- **Summary**: My research lies in the intersection of security and machine learning. This overview summarizes one component of my research: combining computer vision with malware exploit detection for enhanced security solutions. I will present the perspectives of efficacy, reliability and resiliency to formulate threat detection as computer vision problems and develop state-of-the-art image-based malware classification. Representing malware binary as images provides a direct visualization of data samples, reduces the efforts for feature extraction, and consumes the whole binary for holistic structural analysis. Employing transfer learning of deep neural networks effective for large scale image classification to malware classification demonstrates superior classification efficacy compared with classical machine learning algorithms. To enhance reliability of these vision-based malware detectors, interpretation frameworks can be constructed on the malware visual representations and useful for extracting faithful explanation, so that security practitioners have confidence in the model before deployment. In cyber-security applications, we should always assume that a malware writer constantly modifies code to bypass detection. Addressing the resiliency of the malware detectors is equivalently important as efficacy and reliability. Via understanding the attack surfaces of machine learning models used for malware detection, we can greatly improve the robustness of the algorithms to combat malware adversaries in the wild. Finally I will discuss future research directions worth pursuing in this research community.



### 3D-BEVIS: Bird's-Eye-View Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.02199v3
- **DOI**: 10.1007/978-3-030-33676-9_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02199v3)
- **Published**: 2019-04-03 18:51:53+00:00
- **Updated**: 2019-08-01 10:02:00+00:00
- **Authors**: Cathrin Elich, Francis Engelmann, Theodora Kontogianni, Bastian Leibe
- **Comment**: camera-ready version for GCPR '19
- **Journal**: None
- **Summary**: Recent deep learning models achieve impressive results on 3D scene analysis tasks by operating directly on unstructured point clouds. A lot of progress was made in the field of object classification and semantic segmentation. However, the task of instance segmentation is less explored. In this work, we present 3D-BEVIS, a deep learning framework for 3D semantic instance segmentation on point clouds. Following the idea of previous proposal-free instance segmentation approaches, our model learns a feature embedding and groups the obtained feature space into semantic instances. Current point-based methods scale linearly with the number of points by processing local sub-parts of a scene individually. However, to perform instance segmentation by clustering, globally consistent features are required. Therefore, we propose to combine local point geometry with global context information from an intermediate bird's-eye view representation.



### PaintBot: A Reinforcement Learning Approach for Natural Media Painting
- **Arxiv ID**: http://arxiv.org/abs/1904.02201v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02201v1)
- **Published**: 2019-04-03 18:56:02+00:00
- **Updated**: 2019-04-03 18:56:02+00:00
- **Authors**: Biao Jia, Chen Fang, Jonathan Brandt, Byungmoon Kim, Dinesh Manocha
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new automated digital painting framework, based on a painting agent trained through reinforcement learning. To synthesize an image, the agent selects a sequence of continuous-valued actions representing primitive painting strokes, which are accumulated on a digital canvas. Action selection is guided by a given reference image, which the agent attempts to replicate subject to the limitations of the action space and the agent's learned policy. The painting agent policy is determined using a variant of proximal policy optimization reinforcement learning. During training, our agent is presented with patches sampled from an ensemble of reference images. To accelerate training convergence, we adopt a curriculum learning strategy, whereby reference patches are sampled according to how challenging they are using the current policy. We experiment with differing loss functions, including pixel-wise and perceptual loss, which have consequent differing effects on the learned policy. We demonstrate that our painting agent can learn an effective policy with a high dimensional continuous action space comprising pen pressure, width, tilt, and color, for a variety of painting styles. Through a coarse-to-fine refinement process our agent can paint arbitrarily complex images in the desired style.



### Semantics-Aware Image to Image Translation and Domain Transfer
- **Arxiv ID**: http://arxiv.org/abs/1904.02203v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02203v2)
- **Published**: 2019-04-03 19:06:39+00:00
- **Updated**: 2021-03-01 18:35:45+00:00
- **Authors**: Pravakar Roy, Nicolai Häni, Jun-Jee Chao, Volkan Isler
- **Comment**: None
- **Journal**: None
- **Summary**: Image to image translation is the problem of transferring an image from a source domain to a different (but related) target domain. We present a new unsupervised image to image translation technique that leverages the underlying semantic information for object transfiguration and domain transfer tasks. Specifically, we present a generative adversarial learning approach that jointly translates images and labels from a source domain to a target domain. Our main technical contribution is an encoder-decoder based network architecture that jointly encodes the image and its underlying semantics and translates both individually to the target domain. Additionally, we propose object transfiguration and cross-domain semantic consistency losses that preserve semantic labels. Through extensive experimental evaluation, we demonstrate the effectiveness of our approach as compared to the state-of-the-art methods on unsupervised image-to-image translation, domain adaptation, and object transfiguration.



### Linearly Converging Quasi Branch and Bound Algorithms for Global Rigid Registration
- **Arxiv ID**: http://arxiv.org/abs/1904.02204v2
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1904.02204v2)
- **Published**: 2019-04-03 19:07:25+00:00
- **Updated**: 2019-04-14 04:30:34+00:00
- **Authors**: Nadav Dym, Shahar Ziv Kovalsky
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, several branch-and-bound (BnB) algorithms have been proposed to globally optimize rigid registration problems. In this paper, we suggest a general framework to improve upon the BnB approach, which we name Quasi BnB. Quasi BnB replaces the linear lower bounds used in BnB algorithms with quadratic quasi-lower bounds which are based on the quadratic behavior of the energy in the vicinity of the global minimum. While quasi-lower bounds are not truly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we prove that it exhibits linear convergence -- it achieves $\epsilon$-accuracy in $~O(\log(1/\epsilon)) $ time while the time complexity of other rigid registration BnB algorithms is polynomial in $1/\epsilon $. Our experiments verify that Quasi-BnB is significantly more efficient than state-of-the-art BnB algorithms, especially for problems where high accuracy is desired.



### DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1904.02216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02216v1)
- **Published**: 2019-04-03 19:45:17+00:00
- **Updated**: 2019-04-03 19:45:17+00:00
- **Authors**: Hanchao Li, Pengfei Xiong, Haoqiang Fan, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces an extremely efficient CNN architecture named DFANet for semantic segmentation under resource constraints. Our proposed network starts from a single lightweight backbone and aggregates discriminative features through sub-network and sub-stage cascade respectively. Based on the multi-scale feature propagation, DFANet substantially reduces the number of parameters, but still obtains sufficient receptive field and enhances the model learning ability, which strikes a balance between the speed and segmentation performance. Experiments on Cityscapes and CamVid datasets demonstrate the superior performance of DFANet with 8$\times$ less FLOPs and 2$\times$ faster than the existing state-of-the-art real-time semantic segmentation methods while providing comparable accuracy. Specifically, it achieves 70.3\% Mean IOU on the Cityscapes test dataset with only 1.7 GFLOPs and a speed of 160 FPS on one NVIDIA Titan X card, and 71.3\% Mean IOU with 3.4 GFLOPs while inferring on a higher resolution image.



### Decomposing Temperature Time Series with Non-Negative Matrix Factorization
- **Arxiv ID**: http://arxiv.org/abs/1904.02217v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.app-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1904.02217v1)
- **Published**: 2019-04-03 19:46:56+00:00
- **Updated**: 2019-04-03 19:46:56+00:00
- **Authors**: Peter Weiderer, Ana Maria Tomé, Elmar Wolfgang Lang
- **Comment**: None
- **Journal**: None
- **Summary**: During the fabrication of casting parts sensor data is typically automatically recorded and accumulated for process monitoring and defect diagnosis. As casting is a thermal process with many interacting process parameters, root cause analysis tends to be tedious and ineffective. We show how a decomposition based on non-negative matrix factorization (NMF), which is guided by a knowledge-based initialization strategy, is able to extract physical meaningful sources from temperature time series collected during a thermal manufacturing process. The approach assumes the time series to be generated by a superposition of several simultaneously acting component processes. NMF is able to reverse the superposition and to identify the hidden component processes. The latter can be linked to ongoing physical phenomena and process variables, which cannot be monitored directly. Our approach provides new insights into the underlying physics and offers a tool, which can assist in diagnosing defect causes. We demonstrate our method by applying it to real world data, collected in a foundry during the series production of casting parts for the automobile industry.



### Revisiting Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1904.02225v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02225v1)
- **Published**: 2019-04-03 20:11:02+00:00
- **Updated**: 2019-04-03 20:11:02+00:00
- **Authors**: Erik Conser, Kennedy Hahn, Chandler M. Watson, Melanie Mitchell
- **Comment**: To appear in Proceedings of the Workshop on Shortcomings in Vision
  and Language, NAACL-2019, ACL
- **Journal**: None
- **Summary**: We revisit a particular visual grounding method: the "Image Retrieval Using Scene Graphs" (IRSG) system of Johnson et al. (2015). Our experiments indicate that the system does not effectively use its learned object-relationship models. We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it. We find that these datasets exhibit biases that allow methods that ignore relationships to perform relatively well. We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed. Our studies contribute to a more general effort: that of better understanding what machine learning methods that combine language and vision actually learn and what popular datasets actually test.



### Hyperbolic Image Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1904.02239v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1904.02239v2)
- **Published**: 2019-04-03 21:10:12+00:00
- **Updated**: 2020-03-30 20:35:39+00:00
- **Authors**: Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, Victor Lempitsky
- **Comment**: None
- **Journal**: None
- **Summary**: Computer vision tasks such as image classification, image retrieval and few-shot learning are currently dominated by Euclidean and spherical embeddings, so that the final decisions about class belongings or the degree of similarity are made using linear hyperplanes, Euclidean distances, or spherical geodesic distances (cosine similarity). In this work, we demonstrate that in many practical scenarios hyperbolic embeddings provide a better alternative.



### Unpaired Thermal to Visible Spectrum Transfer using Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1904.02242v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02242v1)
- **Published**: 2019-04-03 21:20:10+00:00
- **Updated**: 2019-04-03 21:20:10+00:00
- **Authors**: Adam Nyberg, Abdelrahman Eldesokey, David Bergström, David Gustafsson
- **Comment**: Published in Computer Vision ECCV 2018 Workshops
- **Journal**: None
- **Summary**: Thermal Infrared (TIR) cameras are gaining popularity in many computer vision applications due to their ability to operate under low-light conditions. Images produced by TIR cameras are usually difficult for humans to perceive visually, which limits their usability. Several methods in the literature were proposed to address this problem by transforming TIR images into realistic visible spectrum (VIS) images. However, existing TIR-VIS datasets suffer from imperfect alignment between TIR-VIS image pairs which degrades the performance of supervised methods. We tackle this problem by learning this transformation using an unsupervised Generative Adversarial Network (GAN) which trains on unpaired TIR and VIS images. When trained and evaluated on KAIST-MS dataset, our proposed methods was shown to produce significantly more realistic and sharp VIS images than the existing state-of-the-art supervised methods. In addition, our proposed method was shown to generalize very well when evaluated on a new dataset of new environments.



### StereoDRNet: Dilated Residual Stereo Net
- **Arxiv ID**: http://arxiv.org/abs/1904.02251v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1904.02251v3)
- **Published**: 2019-04-03 21:58:39+00:00
- **Updated**: 2019-06-02 06:48:50+00:00
- **Authors**: Rohan Chabra, Julian Straub, Chris Sweeney, Richard Newcombe, Henry Fuchs
- **Comment**: Accepted at CVPR 2019
- **Journal**: None
- **Summary**: We propose a system that uses a convolution neural network (CNN) to estimate depth from a stereo pair followed by volumetric fusion of the predicted depth maps to produce a 3D reconstruction of a scene. Our proposed depth refinement architecture, predicts view-consistent disparity and occlusion maps that helps the fusion system to produce geometrically consistent reconstructions. We utilize 3D dilated convolutions in our proposed cost filtering network that yields better filtering while almost halving the computational cost in comparison to state of the art cost filtering architectures.For feature extraction we use the Vortex Pooling architecture. The proposed method achieves state of the art results in KITTI 2012, KITTI 2015 and ETH 3D stereo benchmarks. Finally, we demonstrate that our system is able to produce high fidelity 3D scene reconstructions that outperforms the state of the art stereo system.



### Continuous Direct Sparse Visual Odometry from RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1904.02266v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, 53B21, 46C05, 46C07, 68T40, 68T45, 93C85, I.2.9; I.2.10; I.4.10; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/1904.02266v3)
- **Published**: 2019-04-03 23:25:01+00:00
- **Updated**: 2019-08-23 23:21:18+00:00
- **Authors**: Maani Ghaffari, William Clark, Anthony Bloch, Ryan M. Eustice, Jessy W. Grizzle
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: This paper reports on a novel formulation and evaluation of visual odometry from RGB-D images. Assuming a static scene, the developed theoretical framework generalizes the widely used direct energy formulation (photometric error minimization) technique for obtaining a rigid body transformation that aligns two overlapping RGB-D images to a continuous formulation. The continuity is achieved through functional treatment of the problem and representing the process models over RGB-D images in a reproducing kernel Hilbert space; consequently, the registration is not limited to the specific image resolution and the framework is fully analytical with a closed-form derivation of the gradient. We solve the problem by maximizing the inner product between two functions defined over RGB-D images, while the continuous action of the rigid body motion Lie group is captured through the integration of the flow in the corresponding Lie algebra. Energy-based approaches have been extremely successful and the developed framework in this paper shares many of their desired properties such as the parallel structure on both CPUs and GPUs, sparsity, semi-dense tracking, avoiding explicit data association which is computationally expensive, and possible extensions to the simultaneous localization and mapping frameworks. The evaluations on experimental data and comparison with the equivalent energy-based formulation of the problem confirm the effectiveness of the proposed technique, especially, when the lack of structure and texture in the environment is evident.



