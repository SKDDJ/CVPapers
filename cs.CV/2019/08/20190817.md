# Arxiv Papers in cs.CV on 2019-08-17
### Parametric Majorization for Data-Driven Energy Minimization Methods
- **Arxiv ID**: http://arxiv.org/abs/1908.06209v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.LG, 90C06, 68U10, 68T45, 65K10, I.4; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/1908.06209v1)
- **Published**: 2019-08-17 00:10:41+00:00
- **Updated**: 2019-08-17 00:10:41+00:00
- **Authors**: Jonas Geiping, Michael Moeller
- **Comment**: 16 pages, 5 figures, accepted for ICCV 2019
- **Journal**: None
- **Summary**: Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric energy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.



### Zero Shot Learning for Multi-Modal Real Time Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1908.06213v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06213v1)
- **Published**: 2019-08-17 00:37:25+00:00
- **Updated**: 2019-08-17 00:37:25+00:00
- **Authors**: Avinash Kori, Ganapathi Krishnamurthi
- **Comment**: None
- **Journal**: None
- **Summary**: In this report we present an unsupervised image registration framework, using a pre-trained deep neural network as a feature extractor. We refer this to zero-shot learning, due to nonoverlap between training and testing dataset (none of the network modules in the processing pipeline were trained specifically for the task of medical image registration). Highlights of our technique are: (a) No requirement of a training dataset (b) Keypoints i.e.locations of important features are automatically estimated (c) The number of key points in this model is fixed and can possibly be tuned as a hyperparameter. (d) Uncertaintycalculation of the proposed, transformation estimates (e) Real-time registration of images. Our technique was evaluated on BraTS, ALBERT, and collaborative hospital Brain MRI data. Results suggest that the method proved to be robust for affine transformation models and the results are practically instantaneous, irrespective of the size of the input image



### Neural Re-Simulation for Generating Bounces in Single Images
- **Arxiv ID**: http://arxiv.org/abs/1908.06217v3
- **DOI**: None
- **Categories**: **cs.CV**, 68T45, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1908.06217v3)
- **Published**: 2019-08-17 01:19:19+00:00
- **Updated**: 2019-08-24 17:34:38+00:00
- **Authors**: Carlo Innamorati, Bryan Russell, Danny M. Kaufman, and Niloy J. Mitra
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: We introduce a method to generate videos of dynamic virtual objects plausibly interacting via collisions with a still image's environment. Given a starting trajectory, physically simulated with the estimated geometry of a single, static input image, we learn to 'correct' this trajectory to a visually plausible one via a neural network. The neural network can then be seen as learning to 'correct' traditional simulation output, generated with incomplete and imprecise world information, to obtain context-specific, visually plausible re-simulated output, a process we call neural re-simulation. We train our system on a set of 50k synthetic scenes where a virtual moving object (ball) has been physically simulated. We demonstrate our approach on both our synthetic dataset and a collection of real-life images depicting everyday scenes, obtaining consistent improvement over baseline alternatives throughout.



### CompenNet++: End-to-end Full Projector Compensation
- **Arxiv ID**: http://arxiv.org/abs/1908.06246v1
- **DOI**: 10.1109/ICCV.2019.00726
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.06246v1)
- **Published**: 2019-08-17 05:28:21+00:00
- **Updated**: 2019-08-17 05:28:21+00:00
- **Authors**: Bingyao Huang, Haibin Ling
- **Comment**: To appear in ICCV 2019. High-res supplementary material:
  https://www3.cs.stonybrook.edu/~hling/publication/CompenNet++_sup-high-res.pdf.
  Code: https://github.com/BingyaoHuang/CompenNet-plusplus
- **Journal**: None
- **Summary**: Full projector compensation aims to modify a projector input image such that it can compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately, although they are known to correlate with each other. In this paper, we propose the first end-to-end solution, named CompenNet++, to solve the two problems jointly. Our work non-trivially extends CompenNet, which was recently proposed for photometric compensation with promising performance. First, we propose a novel geometric correction subnet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from photometric sampling images. Second, by concatenating the geometric correction subset with CompenNet, CompenNet++ accomplishes full projector compensation and is end-to-end trainable. Third, after training, we significantly simplify both geometric and photometric compensation parts, and hence largely improves the running time efficiency. Moreover, we construct the first setup-independent full compensation benchmark to facilitate the study on this topic. In our thorough experiments, our method shows clear advantages over previous arts with promising compensation quality and meanwhile being practically convenient.



### Attentional Feature-Pair Relation Networks for Accurate Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.06255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06255v1)
- **Published**: 2019-08-17 07:32:49+00:00
- **Updated**: 2019-08-17 07:32:49+00:00
- **Authors**: Bong-Nam Kang, Yonghyun Kim, Bongjin Jun, Daijin Kim
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9x9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets.



### OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1908.06257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06257v1)
- **Published**: 2019-08-17 07:57:03+00:00
- **Updated**: 2019-08-17 07:57:03+00:00
- **Authors**: Changhee Won, Jongbin Ryu, Jongwoo Lim
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.



### Deep Meta Functionals for Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/1908.06277v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06277v1)
- **Published**: 2019-08-17 09:47:47+00:00
- **Updated**: 2019-08-17 09:47:47+00:00
- **Authors**: Gidi Littwin, Lior Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network \textcolor{black}{parametrized by} these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code is available at: https://github.com/gidilittwin/Deep-Meta



### No-Reference Light Field Image Quality Assessment Based on Spatial-Angular Measurement
- **Arxiv ID**: http://arxiv.org/abs/1908.06280v2
- **DOI**: 10.1109/TCSVT.2019.2955011
- **Categories**: **eess.IV**, cs.CG, cs.CV, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1908.06280v2)
- **Published**: 2019-08-17 09:56:54+00:00
- **Updated**: 2022-02-18 01:49:48+00:00
- **Authors**: Likun Shi, Wei Zhou, Zhibo Chen, Jinglin Zhang
- **Comment**: Published on IEEE TCSVT
- **Journal**: None
- **Summary**: Light field image quality assessment (LFI-QA) is a significant and challenging research problem. It helps to better guide light field acquisition, processing and applications. However, only a few objective models have been proposed and none of them completely consider intrinsic factors affecting the LFI quality. In this paper, we propose a No-Reference Light Field image Quality Assessment (NR-LFQA) scheme, where the main idea is to quantify the LFI quality degradation through evaluating the spatial quality and angular consistency. We first measure the spatial quality deterioration by capturing the naturalness distribution of the light field cyclopean image array, which is formed when human observes the LFI. Then, as a transformed representation of LFI, the Epipolar Plane Image (EPI) contains the slopes of lines and involves the angular information. Therefore, EPI is utilized to extract the global and local features from LFI to measure angular consistency degradation. Specifically, the distribution of gradient direction map of EPI is proposed to measure the global angular consistency distortion in the LFI. We further propose the weighted local binary pattern to capture the characteristics of local angular consistency degradation. Extensive experimental results on four publicly available LFI quality datasets demonstrate that the proposed method outperforms state-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.



### Occlusion Robust Face Recognition Based on Mask Learning with PairwiseDifferential Siamese Network
- **Arxiv ID**: http://arxiv.org/abs/1908.06290v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06290v1)
- **Published**: 2019-08-17 10:49:50+00:00
- **Updated**: 2019-08-17 10:49:50+00:00
- **Authors**: Lingxue Song, Dihong Gong, Zhifeng Li, Changsong Liu, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of the face recognition research in the past years. However, existing general CNN face models generalize poorly to the scenario of occlusions on variable facial areas. Inspired by the fact that a human visual system explicitly ignores occlusions and only focuses on non-occluded facial areas, we propose a mask learning strategy to find and discard the corrupted feature elements for face recognition. A mask dictionary is firstly established by exploiting the differences between the top convoluted features of occluded and occlusion-free face pairs using an innovatively designed Pairwise Differential Siamese Network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed approach significantly outperforms the state-of-the-arts.



### Improved Techniques for Training Adaptive Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.06294v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06294v1)
- **Published**: 2019-08-17 10:53:53+00:00
- **Updated**: 2019-08-17 10:53:53+00:00
- **Authors**: Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, Gao Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Adaptive inference is a promising technique to improve the computational efficiency of deep models at test time. In contrast to static models which use the same computation graph for all instances, adaptive networks can dynamically adjust their structure conditioned on each input. While existing research on adaptive inference mainly focuses on designing more advanced architectures, this paper investigates how to train such networks more effectively. Specifically, we consider a typical adaptive deep network with multiple intermediate classifiers. We present three techniques to improve its training efficacy from two aspects: 1) a Gradient Equilibrium algorithm to resolve the conflict of learning of different classifiers; 2) an Inline Subnetwork Collaboration approach and a One-for-all Knowledge Distillation algorithm to enhance the collaboration among classifiers. On multiple datasets (CIFAR-10, CIFAR-100 and ImageNet), we show that the proposed approach consistently leads to further improved efficiency on top of state-of-the-art adaptive deep networks.



### ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics
- **Arxiv ID**: http://arxiv.org/abs/1908.06295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06295v1)
- **Published**: 2019-08-17 12:08:52+00:00
- **Updated**: 2019-08-17 12:08:52+00:00
- **Authors**: Zhiyuan Zhang, Binh-Son Hua, Sai-Kit Yeung
- **Comment**: International Conference on Computer Vision (ICCV) 2019 Oral
- **Journal**: None
- **Summary**: Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient end-to-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train.



### Rotation Invariant Convolutions for 3D Point Clouds Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.06297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06297v1)
- **Published**: 2019-08-17 12:31:57+00:00
- **Updated**: 2019-08-17 12:31:57+00:00
- **Authors**: Zhiyuan Zhang, Binh-Son Hua, David W. Rosen, Sai-Kit Yeung
- **Comment**: International Conference on 3D Vision (3DV) 2019
- **Journal**: None
- **Summary**: Recent progresses in 3D deep learning has shown that it is possible to design special convolution operators to consume point cloud data. However, a typical drawback is that rotation invariance is often not guaranteed, resulting in networks being trained with data augmented with rotations. In this paper, we introduce a novel convolution operator for point clouds that achieves rotation invariance. Our core idea is to use low-level rotation invariant geometric features such as distances and angles to design a convolution operator for point cloud learning. The well-known point ordering problem is also addressed by a binning approach seamlessly built into the convolution. This convolution operator then serves as the basic building block of a neural network that is robust to point clouds under 6DoF transformations such as translation and rotation. Our experiment shows that our method performs with high accuracy in common scene understanding tasks such as object classification and segmentation. Compared to previous works, most importantly, our method is able to generalize and achieve consistent results across different scenarios in which training and testing can contain arbitrary rotations.



### U-CAM: Visual Explanation using Uncertainty based Class Activation Maps
- **Arxiv ID**: http://arxiv.org/abs/1908.06306v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06306v4)
- **Published**: 2019-08-17 14:39:36+00:00
- **Updated**: 2019-10-17 07:20:32+00:00
- **Authors**: Badri N. Patro, Mayank Lunayach, Shivansh Patel, Vinay P. Namboodiri
- **Comment**: ICCV 2019 (accepted)
- **Journal**: None
- **Summary**: Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods.



### Multi-Kernel Filtering for Nonstationary Noise: An Extension of Bilateral Filtering Using Image Context
- **Arxiv ID**: http://arxiv.org/abs/1908.06307v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06307v4)
- **Published**: 2019-08-17 14:43:00+00:00
- **Updated**: 2019-12-23 10:03:22+00:00
- **Authors**: Feihong Liu, Jun Feng, Pew-Thian Yap, Dinggang Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Bilateral filtering (BF) is one of the most classical denoising filters, however, the manually initialized filtering kernel hampers its adaptivity across images with various characteristics. To deal with image variation (i.e., non-stationary noise), in this paper, we propose multi-kernel filter (MKF) which adapts filtering kernels to specific image characteristics automatically. The design of MKF takes inspiration from adaptive mechanisms of human vision that make full use of information in a visual context. More specifically, for simulating the visual context and its adaptive function, we construct the image context based on which we simulate the contextual impact on filtering kernels. We first design a hierarchically clustering algorithm to generate a hierarchy of large to small coherent image patches, organized as a cluster tree, so that obtain multi-scale image representation. The leaf cluster and corresponding predecessor clusters are used to generate one of multiple range kernels that are capable of catering to image variation. At first, we design a hierarchically clustering framework to generate a hierarchy of large to small coherent image patches that organized as a cluster tree, so that obtain multi-scale image representation, i.e., the image context. Next, a leaf cluster is used to generate one of the multiple kernels, and two corresponding predecessor clusters are used to fine-tune the adopted kernel. Ultimately, the single spatially-invariant kernel in BF becomes multiple spatially-varying ones. We evaluate MKF on two public datasets, BSD300 and BrainWeb which are added integrally-varying noise and spatially-varying noise, respectively. Extensive experiments show that MKF outperforms state-of-the-art filters w.r.t. both mean absolute error and structural similarity.



### Bayesian Optimized 1-Bit CNNs
- **Arxiv ID**: http://arxiv.org/abs/1908.06314v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06314v1)
- **Published**: 2019-08-17 15:35:51+00:00
- **Updated**: 2019-08-17 15:35:51+00:00
- **Authors**: Jiaxin Gu, Junhe Zhao, Xiaolong Jiang, Baochang Zhang, Jianzhuang Liu, Guodong Guo, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Deep convolutional neural networks (DCNNs) have dominated the recent developments in computer vision through making various record-breaking models. However, it is still a great challenge to achieve powerful DCNNs in resource-limited environments, such as on embedded devices and smart phones. Researchers have realized that 1-bit CNNs can be one feasible solution to resolve the issue; however, they are baffled by the inferior performance compared to the full-precision DCNNs. In this paper, we propose a novel approach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the advantage of Bayesian learning, a well-established strategy for hard problems, to significantly improve the performance of extreme 1-bit CNNs. We incorporate the prior distributions of full-precision kernels and features into the Bayesian framework to construct 1-bit CNNs in an end-to-end manner, which have not been considered in any previous related methods. The Bayesian losses are achieved with a theoretical support to optimize the network simultaneously in both continuous and discrete spaces, aggregating different losses jointly to improve the model capacity. Extensive experiments on the ImageNet and CIFAR datasets show that BONNs achieve the best classification performance compared to state-of-the-art 1-bit CNNs.



### Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/1908.06316v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06316v1)
- **Published**: 2019-08-17 15:44:12+00:00
- **Updated**: 2019-08-17 15:44:12+00:00
- **Authors**: Fabian Brickwedde, Steffen Abraham, Rudolf Mester
- **Comment**: accepted to IEEE International Conference on Computer Vision 2019
  (ICCV 2019)
- **Journal**: None
- **Summary**: Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design.



### Structural Health Monitoring of Cantilever Beam, a Case Study -- Using Bayesian Neural Network AND Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.06326v1
- **DOI**: 10.1007/978-981-13-8767-8_64
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.06326v1)
- **Published**: 2019-08-17 17:47:24+00:00
- **Updated**: 2019-08-17 17:47:24+00:00
- **Authors**: Rahul Vashisht, H. Viji, T. Sundararajan, D. Mohankumar, S. Sumitra
- **Comment**: 10 Pages
- **Journal**: None
- **Summary**: The advancement of machine learning algorithms has opened a wide scope for vibration-based SHM (Structural Health Monitoring). Vibration-based SHM is based on the fact that damage will alter the dynamic properties viz., structural response, frequencies, mode shapes, etc of the structure. The responses measured using sensors, which are high dimensional in nature, can be intelligently analyzed using machine learning techniques for damage assessment. Neural networks employing multilayer architectures are expressive models capable of capturing complex relationships between input-output pairs but do not account for uncertainty in network outputs. A BNN (Bayesian Neural Network) refers to extending standard networks with posterior inference. It is a neural network with a prior distribution on its weights. Deep learning architectures like CNN (Convolutional neural network) and LSTM(Long Short Term Memory) are good candidates for representation learning from high dimensional data. The advantage of using CNN over multi-layer neural networks is that they are good feature extractors as well as classifiers, which eliminates the need for generating hand-engineered features. LSTM networks are mainly used for sequence modeling. This paper presents both a Bayesian multi-layer perceptron and deep learning-based approach for damage detection and location identification in beam-like structures. Raw frequency response data simulated using finite element analysis is fed as the input of the network. As part of this, frequency response was generated for a series of simulations in the cantilever beam involving different damage scenarios. This case study shows the effectiveness of the above approaches to predict bending rigidity with an acceptable error rate.



### Language Features Matter: Effective Language Representations for Vision-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.06327v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.06327v1)
- **Published**: 2019-08-17 18:01:27+00:00
- **Updated**: 2019-08-17 18:01:27+00:00
- **Authors**: Andrea Burns, Reuben Tan, Kate Saenko, Stan Sclaroff, Bryan A. Plummer
- **Comment**: ICCV 2019 accepted paper
- **Journal**: None
- **Summary**: Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We believe that language features deserve more attention, and conduct experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms an LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we propose a set of best practices for incorporating the language component of VL tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome, providing a ready-to-use vision-language embedding: http://ai.bu.edu/grovle.



### What is needed for simple spatial language capabilities in VQA?
- **Arxiv ID**: http://arxiv.org/abs/1908.06336v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06336v2)
- **Published**: 2019-08-17 20:12:39+00:00
- **Updated**: 2019-10-22 19:03:21+00:00
- **Authors**: Alexander Kuhnle, Ann Copestake
- **Comment**: None
- **Journal**: None
- **Summary**: Visual question answering (VQA) comprises a variety of language capabilities. The diagnostic benchmark dataset CLEVR has fueled progress by helping to better assess and distinguish models in basic abilities like counting, comparing and spatial reasoning in vitro. Following this approach, we focus on spatial language capabilities and investigate the question: what are the key ingredients to handle simple visual-spatial relations? We look at the SAN, RelNet, FiLM and MC models and evaluate their learning behavior on diagnostic data which is solely focused on spatial relations. Via comparative analysis and targeted model modification we identify what really is required to substantially improve upon the CNN-LSTM baseline.



### EigenRank by Committee: A Data Subset Selection and Failure Prediction paradigm for Robust Deep Learning based Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.06337v2
- **DOI**: 10.1016/j.media.2020.101834
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, 68T45 (Primary) 68T05, 68T20 (Secondary), I.5.4; I.4.6
- **Links**: [PDF](http://arxiv.org/pdf/1908.06337v2)
- **Published**: 2019-08-17 20:16:07+00:00
- **Updated**: 2021-01-18 19:40:32+00:00
- **Authors**: Bilwaj Gaonkar, Joel Beckett, Mark Attiah, Christine Ahn, Matthew Edwards, Bayard Wilson, Azim Laiwalla, Banafsheh Salehi, Bryan Yoo, Alex Bui, Luke Macyszyn
- **Comment**: None
- **Journal**: Medical Image Analysis, Volume 67, 2021, Medical Image Analysis,
  Volume 67,2021,101834,ISSN 1361-8415,
- **Summary**: Translation of fully automated deep learning based medical image segmentation technologies to clinical workflows face two main algorithmic challenges. The first, is the collection and archival of large quantities of manually annotated ground truth data for both training and validation. The second is the relative inability of the majority of deep learning based segmentation techniques to alert physicians to a likely segmentation failure. Here we propose a novel algorithm, named `Eigenrank' which addresses both of these challenges. Eigenrank can select for manual labeling, a subset of medical images from a large database, such that a U-Net trained on this subset is superior to one trained on a randomly selected subset of the same size. Eigenrank can also be used to pick out, cases in a large database, where deep learning segmentation will fail. We present our algorithm, followed by results and a discussion of how Eigenrank exploits the Von Neumann information to perform both data subset selection and failure prediction for medical image segmentation using deep learning.



### Assessment of gait normality using a depth camera and mirrors
- **Arxiv ID**: http://arxiv.org/abs/1908.07418v1
- **DOI**: 10.1109/BHI.2018.8333364
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07418v1)
- **Published**: 2019-08-17 20:52:14+00:00
- **Updated**: 2019-08-17 20:52:14+00:00
- **Authors**: Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier
- **Comment**: 2018 IEEE EMBS International Conference on Biomedical & Health
  Informatics (BHI)
- **Journal**: None
- **Summary**: This paper presents an initial work on assessment of gait normality in which the human body motion is represented by a sequence of enhanced depth maps. The input data is provided by a system consisting of a Time-of-Flight (ToF) depth camera and two mirrors. This approach proposes two feature types to describe characteristics of localized points of interest and the level of posture symmetry. These two features are processed on a sequence of enhanced depth maps with the support of a sliding window to provide two corresponding scores. The gait assessment is finally performed based on a weighted combination of these two scores. The evaluation is performed by experimenting on 6 simulated abnormal gaits.



### Skeleton-based Gait Index Estimation with LSTMs
- **Arxiv ID**: http://arxiv.org/abs/1908.07416v1
- **DOI**: 10.1109/ICIS.2018.8466522
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07416v1)
- **Published**: 2019-08-17 21:22:55+00:00
- **Updated**: 2019-08-17 21:22:55+00:00
- **Authors**: Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier
- **Comment**: 2018 IEEE/ACIS 17th International Conference on Computer and
  Information Science (ICIS)
- **Journal**: None
- **Summary**: In this paper, we propose a method that estimates a gait index for a sequence of skeletons. Our system is a stack of an encoder and a decoder that are formed by Long Short-Term Memories (LSTMs). In the encoding stage, the characteristics of an input are automatically determined and are compressed into a latent space. The decoding stage then attempts to reconstruct the input according to such intermediate representation. The reconstruction error is thus considered as a weak gait index. By combining such weak indices over a long-time movement, our system can provide a good estimation for the gait index. Our experiments on a large dataset (nearly one hundred thousand skeletons) showed that the index given by the proposed method outperformed some recent works on gait analysis.



### Estimating skeleton-based gait abnormality index by sparse deep auto-encoder
- **Arxiv ID**: http://arxiv.org/abs/1908.07415v1
- **DOI**: 10.1109/CCE.2018.8465714
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07415v1)
- **Published**: 2019-08-17 21:42:40+00:00
- **Updated**: 2019-08-17 21:42:40+00:00
- **Authors**: Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier
- **Comment**: 2018 IEEE Seventh International Conference on Communications and
  Electronics (ICCE)
- **Journal**: None
- **Summary**: This paper proposes an approach estimating a gait abnormality index based on skeletal information provided by a depth camera. Differently from related works where the extraction of hand-crafted features is required to describe gait characteristics, our method automatically performs that stage with the support of a deep auto-encoder. In order to get visually interpretable features, we embedded a constraint of sparsity into the model. Similarly to most gait-related studies, the temporal factor is also considered as a post-processing in our system. This method provided promising results when experimenting on a dataset containing nearly one hundred thousand skeleton samples.



### Matching-based Depth Camera and Mirrors for 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.06342v1
- **DOI**: 10.1117/12.2304427
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06342v1)
- **Published**: 2019-08-17 22:32:45+00:00
- **Updated**: 2019-08-17 22:32:45+00:00
- **Authors**: Trong Nguyen Nguyen, Huu Hung Huynh, Jean Meunier
- **Comment**: None
- **Journal**: Proc. SPIE 10666, Three-Dimensional Imaging, Visualization, and
  Display 2018, 1066610 (16 May 2018)
- **Summary**: Reconstructing 3D object models is playing an important role in many applications in the field of computer vision. Instead of employing a collection of cameras and/or sensors as in many studies, this paper proposes a simple way to build a cheaper system for 3D reconstruction using only one depth camera and 2 or more mirrors. Each mirror is equivalently considered as a depth camera at another viewpoint. Since all scene data are provided by only one depth sensor, our approach can be applied to moving objects and does not require any synchronization protocol as with a set of cameras. Some experiments were performed on easy-to-evaluate objects to confirm the reconstruction accuracy of our proposed system.



### Hybrid Deep Network for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.06347v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.06347v1)
- **Published**: 2019-08-17 23:08:30+00:00
- **Updated**: 2019-08-17 23:08:30+00:00
- **Authors**: Trong Nguyen Nguyen, Jean Meunier
- **Comment**: Paper accepted for BMVC 2019
- **Journal**: None
- **Summary**: In this paper, we propose a deep convolutional neural network (CNN) for anomaly detection in surveillance videos. The model is adapted from a typical auto-encoder working on video patches under the perspective of sparse combination learning. Our CNN focuses on (unsupervisedly) learning common characteristics of normal events with the emphasis of their spatial locations (by supervised losses). To our knowledge, this is the first work that directly adapts the patch position as the target of a classification sub-network. The model is capable to provide a score of anomaly assessment for each video frame. Our experiments were performed on 4 benchmark datasets with various anomalous events and the obtained results were competitive with state-of-the-art studies.



### Anomaly Detection in Video Sequence with Appearance-Motion Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1908.06351v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.06351v1)
- **Published**: 2019-08-17 23:52:22+00:00
- **Updated**: 2019-08-17 23:52:22+00:00
- **Authors**: Trong Nguyen Nguyen, Jean Meunier
- **Comment**: Paper accepted for ICCV 2019
- **Journal**: None
- **Summary**: Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods.



