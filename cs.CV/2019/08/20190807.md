# Arxiv Papers in cs.CV on 2019-08-07
### Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.02422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02422v1)
- **Published**: 2019-08-07 02:33:18+00:00
- **Updated**: 2019-08-07 02:33:18+00:00
- **Authors**: Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Yi Niu, Shiliang Pu, Fei Wu, Futai Zou
- **Comment**: To be appeared in ACM MM2019
- **Journal**: None
- **Summary**: Temporal action localization is an important yet challenging research topic due to its various applications. Since the frame-level or segment-level annotations of untrimmed videos require amounts of labor expenditure, studies on the weakly-supervised action detection have been springing up. However, most of existing frameworks rely on Class Activation Sequence (CAS) to localize actions by minimizing the video-level classification loss, which exploits the most discriminative parts of actions but ignores the minor regions. In this paper, we propose a novel weakly-supervised framework by adversarial learning of two modules for eliminating such demerits. Specifically, the first module is designed as a well-designed Seeded Sequence Growing (SSG) Network for progressively extending seed regions (namely the highly reliable regions initialized by a CAS-based framework) to their expected boundaries. The second module is a specific classifier for mining trivial or incomplete action regions, which is trained on the shared features after erasing the seeded regions activated by SSG. In this way, a whole network composed of these two modules can be trained in an adversarial manner. The goal of the adversary is to mine features that are difficult for the action classifier. That is, erasion from SSG will force the classifier to discover minor or even new action regions on the input feature sequence, and the classifier will drive the seeds to grow, alternately. At last, we could obtain the action locations and categories from the well-trained SSG and the classifier. Extensive experiments on two public benchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance of our proposed method compared with the state-of-the-arts.



### Model Learning: Primal Dual Networks for Fast MR imaging
- **Arxiv ID**: http://arxiv.org/abs/1908.02426v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, physics.med-ph, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02426v1)
- **Published**: 2019-08-07 02:59:08+00:00
- **Updated**: 2019-08-07 02:59:08+00:00
- **Authors**: Jing Cheng, Haifeng Wang, Leslie Ying, Dong Liang
- **Comment**: accepted in MICCAI2019. arXiv admin note: text overlap with
  arXiv:1906.08143
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is known to be a slow imaging modality and undersampling in k-space has been used to increase the imaging speed. However, image reconstruction from undersampled k-space data is an ill-posed inverse problem. Iterative algorithms based on compressed sensing have been used to address the issue. In this work, we unroll the iterations of the primal-dual hybrid gradient algorithm to a learnable deep network architecture, and gradually relax the constraints to reconstruct MR images from highly undersampled k-space data. The proposed method combines the theoretical convergence guarantee of optimi-zation methods with the powerful learning capability of deep networks. As the constraints are gradually relaxed, the reconstruction model is finally learned from the training data by updating in k-space and image domain alternatively. Experi-ments on in vivo MR data demonstrate that the proposed method achieves supe-rior MR reconstructions from highly undersampled k-space data over other state-of-the-art image reconstruction methods.



### Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations
- **Arxiv ID**: http://arxiv.org/abs/1908.02435v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02435v1)
- **Published**: 2019-08-07 04:11:01+00:00
- **Updated**: 2019-08-07 04:11:01+00:00
- **Authors**: Andras Rozsa, Terrance E. Boult
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial examples contain small perturbations that can remain imperceptible to human observers but alter the behavior of even the best performing deep learning models and yield incorrect outputs. Since their discovery, adversarial examples have drawn significant attention in machine learning: researchers try to reveal the reasons for their existence and improve the robustness of machine learning models to adversarial perturbations. The state-of-the-art defense is the computationally expensive and very time consuming adversarial training via projected gradient descent (PGD). We hypothesize that adversarial attacks exploit the open space risk of classic monotonic activation functions. This paper introduces the tent activation function with bounded open space risk and shows that tents make deep learning models more robust to adversarial attacks. We demonstrate on the MNIST dataset that a classifier with tents yields an average accuracy of 91.8% against six white-box adversarial attacks, which is more than 15 percentage points above the state of the art. On the CIFAR-10 dataset, our approach improves the average accuracy against the six white-box adversarial attacks to 73.5% from 41.8% achieved by adversarial training via PGD.



### Continuous Graph Flow
- **Arxiv ID**: http://arxiv.org/abs/1908.02436v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02436v2)
- **Published**: 2019-08-07 04:24:48+00:00
- **Updated**: 2019-09-28 04:34:55+00:00
- **Authors**: Zhiwei Deng, Megha Nawhal, Lili Meng, Greg Mori
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data. Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to state-of-the-art models.



### Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.02441v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02441v1)
- **Published**: 2019-08-07 05:08:15+00:00
- **Updated**: 2019-08-07 05:08:15+00:00
- **Authors**: Jiwoong Park, Minsik Lee, Hyung Jin Chang, Kyuewang Lee, Jin Young Choi
- **Comment**: 10 pages, 3 figures, ICCV 2019 accepted
- **Journal**: None
- **Summary**: We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.



### An Adaptive Supervision Framework for Active Learning in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.02454v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02454v3)
- **Published**: 2019-08-07 06:20:33+00:00
- **Updated**: 2019-10-15 09:18:26+00:00
- **Authors**: Sai Vikas Desai, Akshay L Chandra, Wei Guo, Seishi Ninomiya, Vineeth N Balasubramanian
- **Comment**: Accepted in BMVC 2019
- **Journal**: None
- **Summary**: Active learning approaches in computer vision generally involve querying strong labels for data. However, previous works have shown that weak supervision can be effective in training models for vision tasks while greatly reducing annotation costs. Using this knowledge, we propose an adaptive supervision framework for active learning and demonstrate its effectiveness on the task of object detection. Instead of directly querying bounding box annotations (strong labels) for the most informative samples, we first query weak labels and optimize the model. Using a switching condition, the required supervision level can be increased. Our framework requires little to no change in model architecture. Our extensive experiments show that the proposed framework can be used to train good generalizable models with much lesser annotation costs than the state of the art active learning approaches for object detection.



### Edge-guided Non-local Fully Convolutional Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.02460v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02460v2)
- **Published**: 2019-08-07 06:51:52+00:00
- **Updated**: 2019-08-11 08:25:39+00:00
- **Authors**: Zhengzheng Tu, Yan Ma, Chenglong Li, Jin Tang, Bin Luo
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Fully Convolutional Neural Network (FCN) has been widely applied to salient object detection recently by virtue of high-level semantic feature extraction, but existing FCN based methods still suffer from continuous striding and pooling operations leading to loss of spatial structure and blurred edges. To maintain the clear edge structure of salient objects, we propose a novel Edge-guided Non-local FCN (ENFNet) to perform edge guided feature learning for accurate salient object detection. In a specific, we extract hierarchical global and local information in FCN to incorporate non-local features for effective feature representations. To preserve good boundaries of salient objects, we propose a guidance block to embed edge prior knowledge into hierarchical feature maps. The guidance block not only performs feature-wise manipulation but also spatial-wise transformation for effective edge embeddings. Our model is trained on the MSRA-B dataset and tested on five popular benchmark datasets. Comparing with the state-of-the-art methods, the proposed method achieves the best performance on all datasets.



### Expert Sample Consensus Applied to Camera Re-Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.02484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02484v1)
- **Published**: 2019-08-07 08:23:03+00:00
- **Updated**: 2019-08-07 08:23:03+00:00
- **Authors**: Eric Brachmann, Carsten Rother
- **Comment**: ICCV 2019. Supplementary materials included
- **Journal**: None
- **Summary**: Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets.



### STM: SpatioTemporal and Motion Encoding for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.02486v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02486v2)
- **Published**: 2019-08-07 08:28:38+00:00
- **Updated**: 2019-08-16 16:12:24+00:00
- **Authors**: Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, Junjie Yan
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose an STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.



### Understanding Optical Music Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.03608v3
- **DOI**: 10.1145/3397499
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1908.03608v3)
- **Published**: 2019-08-07 08:37:16+00:00
- **Updated**: 2020-07-29 08:59:52+00:00
- **Authors**: Jorge Calvo-Zaragoza, Jan Hajič Jr., Alexander Pacha
- **Comment**: None
- **Journal**: ACM Comput. Surv. 53, 4 (2020) Article 77
- **Summary**: For over 50 years, researchers have been trying to teach computers to read music notation, referred to as Optical Music Recognition (OMR). However, this field is still difficult to access for new researchers, especially those without a significant musical background: few introductory materials are available, and furthermore the field has struggled with defining itself and building a shared terminology. In this tutorial, we address these shortcomings by (1) providing a robust definition of OMR and its relationship to related fields, (2) analyzing how OMR inverts the music encoding process to recover the musical notation and the musical semantics from documents, (3) proposing a taxonomy of OMR, with most notably a novel taxonomy of applications. Additionally, we discuss how deep learning affects modern OMR research, as opposed to the traditional pipeline. Based on this work, the reader should be able to attain a basic understanding of OMR: its objectives, its inherent structure, its relationship to other fields, the state of the art, and the research opportunities it affords.



### Progressive Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.02492v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02492v3)
- **Published**: 2019-08-07 08:52:26+00:00
- **Updated**: 2020-11-06 08:22:17+00:00
- **Authors**: Zhengxu Yu, Dong Shen, Zhongming Jin, Jianqiang Huang, Deng Cai, Xian-Sheng Hua
- **Comment**: 10 pages, 4 figures, journel verison of our published short paper on
  IJCAI2019
- **Journal**: None
- **Summary**: Model fine-tuning is a widely used transfer learning approach in person Re-identification (ReID) applications, which fine-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the significant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between the distribution of each mini-batch and the whole dataset's distribution when using mini-batch training. In this paper, we study model fine-tuning from the perspective of the aggregation and utilization of the global information of the dataset when using mini-batch training. Specifically, we introduce a novel network structure called Batch-related Convolutional Cell (BConv-Cell), which progressively collects the global information of the dataset into a latent state and uses it to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning process by jointly optimizing the BConv-Cells and the pre-trained ReID model. Empirical experiments show that our proposal can improve the performance of the ReID model greatly on MSMT17, Market-1501, CUHK03 and DukeMTMC-reID datasets. Moreover, we extend our proposal to the general image classification task. The experiments in several image classification benchmark datasets demonstrate that our proposal can significantly improve the performance of baseline models. The code has been released at \url{https://github.com/ZJULearning/PTL}



### Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.02498v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02498v1)
- **Published**: 2019-08-07 09:33:03+00:00
- **Updated**: 2019-08-07 09:33:03+00:00
- **Authors**: Gihyun Kwon, Chihye Han, Dae-shik Kim
- **Comment**: 8.5 pages, 4 figures, Accepted by the 22nd International Conference
  on Medical Image Computing and Computer Assisted Intervention (MICCAI 2019)
- **Journal**: None
- **Summary**: As deep learning is showing unprecedented success in medical image analysis tasks, the lack of sufficient medical data is emerging as a critical problem. While recent attempts to solve the limited data problem using Generative Adversarial Networks (GAN) have been successful in generating realistic images with diversity, most of them are based on image-to-image translation and thus require extensive datasets from different domains. Here, we propose a novel model that can successfully generate 3D brain MRI data from random vectors by learning the data distribution. Our 3D GAN model solves both image blurriness and mode collapse problems by leveraging alpha-GAN that combines the advantages of Variational Auto-Encoder (VAE) and GAN with an additional code discriminator network. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss to lower the training instability. To demonstrate the effectiveness of our model, we generate new images of normal brain MRI and show that our model outperforms baseline models in both quantitative and qualitative measurements. We also train the model to synthesize brain disorder MRI data to demonstrate the wide applicability of our model. Our results suggest that the proposed model can successfully generate various types and modalities of 3D whole brain volumes from a small set of training data.



### Mesh Variational Autoencoders with Edge Contraction Pooling
- **Arxiv ID**: http://arxiv.org/abs/1908.02507v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02507v1)
- **Published**: 2019-08-07 09:59:08+00:00
- **Updated**: 2019-08-07 09:59:08+00:00
- **Authors**: Yu-Jie Yuan, Yu-Kun Lai, Jie Yang, Hongbo Fu, Lin Gao
- **Comment**: None
- **Journal**: None
- **Summary**: 3D shape analysis is an important research topic in computer vision and graphics. While existing methods have generalized image-based deep learning to meshes using graph-based convolutions, the lack of an effective pooling operation restricts the learning capability of their networks. In this paper, we propose a novel pooling operation for mesh datasets with the same connectivity but different geometry, by building a mesh hierarchy using mesh simplification. For this purpose, we develop a modified mesh simplification method to avoid generating highly irregularly sized triangles. Our pooling operation effectively encodes the correspondence between coarser and finer meshes in the hierarchy. We then present a variational auto-encoder structure with the edge contraction pooling and graph-based convolutions, to explore probability latent spaces of 3D surfaces. Our network requires far fewer parameters than the original mesh VAE and thus can handle denser models thanks to our new pooling operation and convolutional kernels. Our evaluation also shows that our method has better generalization ability and is more reliable in various applications, including shape generation, shape interpolation and shape embedding.



### Free-Lunch Saliency via Attention in Atari Agents
- **Arxiv ID**: http://arxiv.org/abs/1908.02511v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02511v2)
- **Published**: 2019-08-07 10:10:45+00:00
- **Updated**: 2019-10-30 17:42:50+00:00
- **Authors**: Dmitry Nikulin, Anastasia Ianina, Vladimir Aliev, Sergey Nikolenko
- **Comment**: 2019 ICCV Workshop on Interpreting and Explaining Visual Artificial
  Intelligence Models. 15 pages, 14 figures, 5 tables
- **Journal**: None
- **Summary**: We propose a new approach to visualize saliency maps for deep neural network models and apply it to deep reinforcement learning agents trained on Atari environments. Our method adds an attention module that we call FLS (Free Lunch Saliency) to the feature extractor from an established baseline (Mnih et al., 2015). This addition results in a trainable model that can produce saliency maps, i.e., visualizations of the importance of different parts of the input for the agent's current decision making. We show experimentally that a network with an FLS module exhibits performance similar to the baseline (i.e., it is "free", with no performance cost) and can be used as a drop-in replacement for reinforcement learning agents. We also design another feature extractor that scores slightly lower but provides higher-fidelity visualizations. In addition to attained scores, we report saliency metrics evaluated on the Atari-HEAD dataset of human gameplay.



### Grasp Type Estimation for Myoelectric Prostheses using Point Cloud Feature Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.02564v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02564v1)
- **Published**: 2019-08-07 12:23:56+00:00
- **Updated**: 2019-08-07 12:23:56+00:00
- **Authors**: Ghazal Ghazaei, Federico Tombari, Nassir Navab, Kianoush Nazarpour
- **Comment**: None
- **Journal**: Workshop on Human-aiding Robotics, International Conference on
  Intelligent Robots and Systems (IROS) 2018
- **Summary**: Prosthetic hands can help people with limb difference to return to their life routines. Commercial prostheses, however have several limitations in providing an acceptable dexterity. We approach these limitations by augmenting the prosthetic hands with an off-the-shelf depth sensor to enable the prosthesis to see the object's depth, record a single view (2.5-D) snapshot, and estimate an appropriate grasp type; using a deep network architecture based on 3D point clouds called PointNet. The human can act as the supervisor throughout the procedure by accepting or refusing the suggested grasp type. We achieved the grasp classification accuracy of up to 88%. Contrary to the case of the RGB data, the depth data provides all the necessary object shape information, which is required for grasp recognition. The PointNet not only enables using 3-D data in practice, but it also prevents excessive computations. Augmentation of the prosthetic hands with such a semi-autonomous system can lead to better differentiation of grasp types, less burden on user, and better performance.



### Confident Head Circumference Measurement from Ultrasound with Real-time Feedback for Sonographers
- **Arxiv ID**: http://arxiv.org/abs/1908.02582v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02582v1)
- **Published**: 2019-08-07 12:35:54+00:00
- **Updated**: 2019-08-07 12:35:54+00:00
- **Authors**: Samuel Budd, Matthew Sinclair, Bishesh Khanal, Jacqueline Matthew, David Lloyd, Alberto Gomez, Nicolas Toussaint, Emma Robinson, Bernhard Kainz
- **Comment**: Accepted at MICCAI 2019; Demo video available on Twitter
  (@sambuddinc)
- **Journal**: None
- **Summary**: Manual estimation of fetal Head Circumference (HC) from Ultrasound (US) is a key biometric for monitoring the healthy development of fetuses. Unfortunately, such measurements are subject to large inter-observer variability, resulting in low early-detection rates of fetal abnormalities. To address this issue, we propose a novel probabilistic Deep Learning approach for real-time automated estimation of fetal HC. This system feeds back statistics on measurement robustness to inform users how confident a deep neural network is in evaluating suitable views acquired during free-hand ultrasound examination. In real-time scenarios, this approach may be exploited to guide operators to scan planes that are as close as possible to the underlying distribution of training images, for the purpose of improving inter-operator consistency. We train on free-hand ultrasound data from over 2000 subjects (2848 training/540 test) and show that our method is able to predict HC measurements within 1.81$\pm$1.65mm deviation from the ground truth, with 50% of the test images fully contained within the predicted confidence margins, and an average of 1.82$\pm$1.78mm deviation from the margin for the remaining cases that are not fully contained.



### Kidney and Kidney Tumor Segmentation using a Logical Ensemble of U-nets with Volumetric Validation
- **Arxiv ID**: http://arxiv.org/abs/1908.02625v1
- **DOI**: 10.24926/548719.082
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02625v1)
- **Published**: 2019-08-07 13:28:26+00:00
- **Updated**: 2019-08-07 13:28:26+00:00
- **Authors**: Jamie A. O'Reilly, Manas Sangworasil, Takenobu Matsuura
- **Comment**: 9 pages, 4 figures, 1 table, competition submission manuscript
- **Journal**: None
- **Summary**: Automated medical image segmentation is a priority research area for computational methods. In particular, detection of cancerous tumors represents a current challenge in this area with potential for real-world impact. This paper describes a method developed in response to the 2019 Kidney Tumor Segmentation Challenge (KiTS19). Axial computed tomography (CT) scans from 210 kidney cancer patients were used to develop and evaluate this automatic segmentation method based on a logical ensemble of fully-convolutional network (FCN) architectures, followed by volumetric validation. Data was pre-processed using conventional computer vision techniques, thresholding, histogram equalization, morphological operations, centering, zooming and resizing. Three binary FCN segmentation models were trained to classify kidney and tumor (2), and only tumor (1), respectively. Model output images were stacked and volumetrically validated to produce the final segmentation for each patient scan. The average F1 score from kidney and tumor pixel classifications was calculated as 0.6758 using preprocessed images and annotations; although restoring to the original image format reduced this score. It remains to be seen how this compares to other solutions.



### Structuring Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1908.02626v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02626v1)
- **Published**: 2019-08-07 13:29:11+00:00
- **Updated**: 2019-08-07 13:29:11+00:00
- **Authors**: Marco Rudolph, Bastian Wandt, Bodo Rosenhahn
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.



### Scene-based Factored Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.02632v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02632v3)
- **Published**: 2019-08-07 13:43:25+00:00
- **Updated**: 2019-09-02 16:11:16+00:00
- **Authors**: Chen Shen, Rongrong Ji, Fuhai Chen, Xiaoshuai Sun, Xiangming Li
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Image captioning has attracted ever-increasing research attention in the multimedia community. To this end, most cutting-edge works rely on an encoder-decoder framework with attention mechanisms, which have achieved remarkable progress. However, such a framework does not consider scene concepts to attend visual information, which leads to sentence bias in caption generation and defects the performance correspondingly. We argue that such scene concepts capture higher-level visual semantics and serve as an important cue in describing images. In this paper, we propose a novel scene-based factored attention module for image captioning. Specifically, the proposed module first embeds the scene concepts into factored weights explicitly and attends the visual information extracted from the input image. Then, an adaptive LSTM is used to generate captions for specific scene types. Experimental results on Microsoft COCO benchmark show that the proposed scene-based attention module improves model performance a lot, which outperforms the state-of-the-art approaches under various evaluation metrics.



### Mono-Stixels: Monocular depth reconstruction of dynamic street scenes
- **Arxiv ID**: http://arxiv.org/abs/1908.02635v1
- **DOI**: 10.1109/ICRA.2018.8460490
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02635v1)
- **Published**: 2019-08-07 13:54:51+00:00
- **Updated**: 2019-08-07 13:54:51+00:00
- **Authors**: Fabian Brickwedde, Steffen Abraham, Rudolf Mester
- **Comment**: 2018 IEEE International Conference on Robotics and Automation (ICRA
  2018)
- **Journal**: None
- **Summary**: In this paper we present mono-stixels, a compact environment representation specially designed for dynamic street scenes. Mono-stixels are a novel approach to estimate stixels from a monocular camera sequence instead of the traditionally used stereo depth measurements. Our approach jointly infers the depth, motion and semantic information of the dynamic scene as a 1D energy minimization problem based on optical flow estimates, pixel-wise semantic segmentation and camera motion. The optical flow of a stixel is described by a homography. By applying the mono-stixel model the degrees of freedom of a stixel-homography are reduced to only up to two degrees of freedom. Furthermore, we exploit a scene model and semantic information to handle moving objects. In our experiments we use the public available DeepFlow for optical flow estimation and FCN8s for the semantic information as inputs and show on the KITTI 2015 dataset that mono-stixels provide a compact and reliable depth reconstruction of both the static and moving parts of the scene. Thereby, mono-stixels overcome the limitation to static scenes of previous structure-from-motion approaches.



### Attention-Aware Linear Depthwise Convolution for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1908.02648v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02648v3)
- **Published**: 2019-08-07 14:09:46+00:00
- **Updated**: 2019-11-29 06:17:16+00:00
- **Authors**: Seongmin Hwang, Gwanghuyn Yu, Cheolkon Jung, Jinyoung Kim
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Although deep convolutional neural networks (CNNs) have obtained outstanding performance in image superresolution (SR), their computational cost increases geometrically as CNN models get deeper and wider. Meanwhile, the features of intermediate layers are treated equally across the channel, thus hindering the representational capability of CNNs. In this paper, we propose an attention-aware linear depthwise network to address the problems for single image SR, named ALDNet. Specifically, linear depthwise convolution allows CNN-based SR models to preserve useful information for reconstructing a super-resolved image while reducing computational burden. Furthermore, we design an attention-aware branch that enhances the representation ability of depthwise convolution layers by making full use of depthwise filter interdependency. Experiments on publicly available benchmark datasets show that ALDNet achieves superior performance to traditional depthwise separable convolutions in terms of quantitative measurements and visual quality.



### Regression Constraint for an Explainable Cervical Cancer Classifier
- **Arxiv ID**: http://arxiv.org/abs/1908.02650v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02650v2)
- **Published**: 2019-08-07 14:12:04+00:00
- **Updated**: 2019-08-19 07:52:40+00:00
- **Authors**: Antoine Pirovano, Leandro G. Almeida, Said Ladjal
- **Comment**: 5 pages, 9 figures, accepted at GRETSI 2019
- **Journal**: None
- **Summary**: This article adresses the problem of automatic squamous cells classification for cervical cancer screening using Deep Learning methods. We study different architectures on a public dataset called Herlev dataset, which consists in classifying cells, obtained by cervical pap smear, regarding the severity of the abnormalities they represent. Furthermore, we use an attribution method to understand which cytomorphological features are actually learned as discriminative to classify severity of the abnormalities. Through this paper, we show how we trained a performant classifier: 74.5\% accuracy on severity classification and 94\% accuracy on normal/abnormal classification.



### SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.02660v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02660v2)
- **Published**: 2019-08-07 14:41:30+00:00
- **Updated**: 2019-08-29 20:30:38+00:00
- **Authors**: Kaiyu Yang, Olga Russakovsky, Jia Deng
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be "behind" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be "next to" each other if there is a third student between them.   We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at https://github.com/princeton-vl/SpatialSense.



### Visual Coin-Tracking: Tracking of Planar Double-Sided Objects
- **Arxiv ID**: http://arxiv.org/abs/1908.02664v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02664v1)
- **Published**: 2019-08-07 14:53:44+00:00
- **Updated**: 2019-08-07 14:53:44+00:00
- **Authors**: Jonáš Šerých, Jiří Matas
- **Comment**: Accepted to GCPR 2019. Dataset available at
  http://cmp.felk.cvut.cz/coin-tracking/
- **Journal**: None
- **Summary**: We introduce a new video analysis problem -- tracking of rigid planar objects in sequences where both their sides are visible. Such coin-like objects often rotate fast with respect to an arbitrary axis producing unique challenges, such as fast incident light and aspect ratio change and rotational motion blur. Despite being common, neither tracking sequences containing coin-like objects nor suitable algorithm have been published. As a second contribution, we present a novel coin-tracking benchmark containing 17 video sequences annotated with object segmentation masks. Experiments show that the sequences differ significantly from the ones encountered in standard tracking datasets. We propose a baseline coin-tracking method based on convolutional neural network segmentation and explicit pose modeling. Its performance confirms that coin-tracking is an open and challenging problem.



### The Northumberland Dolphin Dataset: A Multimedia Individual Cetacean Dataset for Fine-Grained Categorisation
- **Arxiv ID**: http://arxiv.org/abs/1908.02669v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02669v1)
- **Published**: 2019-08-07 15:00:27+00:00
- **Updated**: 2019-08-07 15:00:27+00:00
- **Authors**: Cameron Trotter, Georgia Atkinson, Matthew Sharpe, A. Stephen McGough, Nick Wright, Per Berggren
- **Comment**: 4 pages, 4 figures, submitted to FGVC6 Workshop at CVPR2019
- **Journal**: None
- **Summary**: Methods for cetacean research include photo-identification (photo-id) and passive acoustic monitoring (PAM) which generate thousands of images per expedition that are currently hand categorised by researchers into the individual dolphins sighted. With the vast amount of data obtained it is crucially important to develop a system that is able to categorise this quickly. The Northumberland Dolphin Dataset (NDD) is an on-going novel dataset project made up of above and below water images of, and spectrograms of whistles from, white-beaked dolphins. These are produced by photo-id and PAM data collection methods applied off the coast of Northumberland, UK. This dataset will aid in building cetacean identification models, reducing the number of human-hours required to categorise images. Example use cases and areas identified for speed up are examined.



### Dual-reference Age Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1908.02671v4
- **DOI**: 10.1016/j.neucom.2020.06.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02671v4)
- **Published**: 2019-08-07 15:06:08+00:00
- **Updated**: 2020-04-14 06:05:30+00:00
- **Authors**: Yuan Zhou, Bingzhang Hu, and Jun He, Yu Guan, Ling Shao
- **Comment**: None
- **Journal**: Neurocomputing(2020)
- **Summary**: Age synthesis methods typically take a single image as input and use a specific number to control the age of the generated image. In this paper, we propose a novel framework taking two images as inputs, named dual-reference age synthesis (DRAS), which approaches the task differently; instead of using "hard" age information, i.e. a fixed number, our model determines the target age in a "soft" way, by employing a second reference image. Specifically, the proposed framework consists of an identity agent, an age agent and a generative adversarial network. It takes two images as input - an identity reference and an age reference - and outputs a new image that shares corresponding features with each. Experimental results on two benchmark datasets (UTKFace and CACD) demonstrate the appealing performance and flexibility of the proposed framework.



### Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.02686v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02686v1)
- **Published**: 2019-08-07 15:39:55+00:00
- **Updated**: 2019-08-07 15:39:55+00:00
- **Authors**: Jörg Wagner, Jan Mathias Köhler, Tobias Gindele, Leon Hetzel, Jakob Thaddäus Wiedemer, Sven Behnke
- **Comment**: In Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), Long Beach, CA, USA, June 2019
- **Journal**: None
- **Summary**: To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a specific prediction. Our approach is based on a novel technique to defend against adversarial evidence (i.e. faulty evidence due to artefacts) by filtering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.



### Fine-Tuning Models Comparisons on Garbage Classification for Recyclability
- **Arxiv ID**: http://arxiv.org/abs/1908.04393v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04393v1)
- **Published**: 2019-08-07 15:58:51+00:00
- **Updated**: 2019-08-07 15:58:51+00:00
- **Authors**: Umut Ozkaya, Levent Seyfi
- **Comment**: published in ISAS 2018-Winter
- **Journal**: None
- **Summary**: In this study, it is aimed to develop a deep learning application which detects types of garbage into trash in order to provide recyclability with vision system. Training and testing will be performed with image data consisting of several classes on different garbage types. The data set used during training and testing will be generated from original frames taken from garbage images. The data set used for deep learning structures has a total of 2527 images with 6 different classes. Half of these images in the data set were used for training process and remaining part were used for testing procedure. Also, transfer learning was used to obtain shorter training and test procedures with and higher accuracy. As fine-tuned models, Alexnet, VGG16, Googlenet and Resnet structures were carried. In order to test performance of classifiers, two different classifiers are used as Softmax and Support Vector Machines. 6 different type of trash images were correctly classified the highest accuracy with GoogleNet+SVM as 97.86%.



### I Bet You Are Wrong: Gambling Adversarial Networks for Structured Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.02711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02711v1)
- **Published**: 2019-08-07 16:27:50+00:00
- **Updated**: 2019-08-07 16:27:50+00:00
- **Authors**: Laurens Samson, Nanne van Noord, Olaf Booij, Michael Hofmann, Efstratios Gavves, Mohsen Ghafoorian
- **Comment**: 13 pages, 8 figures
- **Journal**: None
- **Summary**: Adversarial training has been recently employed for realizing structured semantic segmentation, in which the aim is to preserve higher-level scene structural consistencies in dense predictions. However, as we show, value-based discrimination between the predictions from the segmentation network and ground-truth annotations can hinder the training process from learning to improve structural qualities as well as disabling the network from properly expressing uncertainties. In this paper, we rethink adversarial training for semantic segmentation and propose to formulate the fake/real discrimination framework with a correct/incorrect training objective. More specifically, we replace the discriminator with a "gambler" network that learns to spot and distribute its budget in areas where the predictions are clearly wrong, while the segmenter network tries to leave no clear clues for the gambler where to bet. Empirical evaluation on two road-scene semantic segmentation tasks shows that not only does the proposed method re-enable expressing uncertainties, it also improves pixel-wise and structure-based metrics.



### Relighting Humans: Occlusion-Aware Inverse Rendering for Full-Body Human Images
- **Arxiv ID**: http://arxiv.org/abs/1908.02714v1
- **DOI**: 10.1145/3272127.3275104
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02714v1)
- **Published**: 2019-08-07 16:35:27+00:00
- **Updated**: 2019-08-07 16:35:27+00:00
- **Authors**: Yoshihiro Kanamori, Yuki Endo
- **Comment**: Published at SIGGRAPH Asia 2018 (ACM Transactions on Graphics).
  Project page with codes, pretrained models, and human model lists is at
  http://kanamori.cs.tsukuba.ac.jp/projects/relighting_human/
- **Journal**: None
- **Summary**: Relighting of human images has various applications in image synthesis. For relighting, we must infer albedo, shape, and illumination from a human portrait. Previous techniques rely on human faces for this inference, based on spherical harmonics (SH) lighting. However, because they often ignore light occlusion, inferred shapes are biased and relit images are unnaturally bright particularly at hollowed regions such as armpits, crotches, or garment wrinkles. This paper introduces the first attempt to infer light occlusion in the SH formulation directly. Based on supervised learning using convolutional neural networks (CNNs), we infer not only an albedo map, illumination but also a light transport map that encodes occlusion as nine SH coefficients per pixel. The main difficulty in this inference is the lack of training datasets compared to unlimited variations of human portraits. Surprisingly, geometric information including occlusion can be inferred plausibly even with a small dataset of synthesized human figures, by carefully preparing the dataset so that the CNNs can exploit the data coherency. Our method accomplishes more realistic relighting than the occlusion-ignored formulation.



### Advocacy Learning: Learning through Competition and Class-Conditional Representations
- **Arxiv ID**: http://arxiv.org/abs/1908.02723v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02723v1)
- **Published**: 2019-08-07 16:55:44+00:00
- **Updated**: 2019-08-07 16:55:44+00:00
- **Authors**: Ian Fox, Jenna Wiens
- **Comment**: Accepted IJCAI 2019
- **Journal**: None
- **Summary**: We introduce advocacy learning, a novel supervised training scheme for attention-based classification problems. Advocacy learning relies on a framework consisting of two connected networks: 1) $N$ Advocates (one for each class), each of which outputs an argument in the form of an attention map over the input, and 2) a Judge, which predicts the class label based on these arguments. Each Advocate produces a class-conditional representation with the goal of convincing the Judge that the input example belongs to their class, even when the input belongs to a different class. Applied to several different classification tasks, we show that advocacy learning can lead to small improvements in classification accuracy over an identical supervised baseline. Though a series of follow-up experiments, we analyze when and how such class-conditional representations improve discriminative performance. Though somewhat counter-intuitive, a framework in which subnetworks are trained to competitively provide evidence in support of their class shows promise, in many cases performing on par with standard learning approaches. This provides a foundation for further exploration into competition and class-conditional representations in supervised learning.



### Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1908.02735v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02735v1)
- **Published**: 2019-08-07 17:22:01+00:00
- **Updated**: 2019-08-07 17:22:01+00:00
- **Authors**: Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein
- **Comment**: Camera-ready for our ICCV 2019 paper (poster)
- **Journal**: None
- **Summary**: Learning an effective similarity measure between image representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval).



### Learning Conditional Deformable Templates with Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.02738v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02738v2)
- **Published**: 2019-08-07 17:29:36+00:00
- **Updated**: 2019-10-11 16:04:22+00:00
- **Authors**: Adrian V. Dalca, Marianne Rakic, John Guttag, Mert R. Sabuncu
- **Comment**: NeurIPS 2019: Neural Information Processing Systems. Keywords:
  deformable templates, conditional atlases, diffeomorphic image registration,
  probabilistic models, neuroimaging
- **Journal**: NeurIPS: Thirty-third Conference on Neural Information Processing
  Systems, 2019
- **Summary**: We develop a learning framework for building deformable templates, which play a fundamental role in many image analysis and computational anatomy tasks. Conventional methods for template creation and image alignment to the template have undergone decades of rich technical development. In these frameworks, templates are constructed using an iterative process of template estimation and alignment, which is often computationally very expensive. Due in part to this shortcoming, most methods compute a single template for the entire population of images, or a few templates for specific sub-groups of the data. In this work, we present a probabilistic model and efficient learning strategy that yields either universal or conditional templates, jointly with a neural network that provides efficient alignment of the images to these templates. We demonstrate the usefulness of this method on a variety of domains, with a special focus on neuroimaging. This is particularly useful for clinical applications where a pre-existing template does not exist, or creating a new one with traditional methods can be prohibitively expensive. Our code and atlases are available online as part of the VoxelMorph library at http://voxelmorph.csail.mit.edu.



### Hierarchy-of-Visual-Words: a Learning-based Approach for Trademark Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1908.02786v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02786v1)
- **Published**: 2019-08-07 18:19:43+00:00
- **Updated**: 2019-08-07 18:19:43+00:00
- **Authors**: Vítor N. Lourenço, Gabriela G. Silva, Leandro A. F. Fernandes
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the Hierarchy-of-Visual-Words (HoVW), a novel trademark image retrieval (TIR) method that decomposes images into simpler geometric shapes and defines a descriptor for binary trademark image representation by encoding the hierarchical arrangement of component shapes. The proposed hierarchical organization of visual data stores each component shape as a visual word. It is capable of representing the geometry of individual elements and the topology of the trademark image, making the descriptor robust against linear as well as to some level of nonlinear transformation. Experiments show that HoVW outperforms previous TIR methods on the MPEG-7 CE-1 and MPEG-7 CE-2 image databases.



### Attend To Count: Crowd Counting with Adaptive Capacity Multi-scale CNNs
- **Arxiv ID**: http://arxiv.org/abs/1908.02797v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02797v2)
- **Published**: 2019-08-07 18:57:35+00:00
- **Updated**: 2019-08-26 05:37:24+00:00
- **Authors**: Zhikang Zou, Yu Cheng, Xiaoye Qu, Shouling Ji, Xiaoxiao Guo, Pan Zhou
- **Comment**: Accepted to Neurocomputing, code will be released soon
- **Journal**: None
- **Summary**: Crowd counting is a challenging task due to the large variations in crowd distributions. Previous methods tend to tackle the whole image with a single fixed structure, which is unable to handle diverse complicated scenes with different crowd densities. Hence, we propose the Adaptive Capacity Multi-scale convolutional neural networks (ACM-CNN), a novel crowd counting approach which can assign different capacities to different portions of the input. The intuition is that the model should focus on important regions of the input image and optimize its capacity allocation conditioning on the crowd intensive degree. ACM-CNN consists of three types of modules: a coarse network, a fine network, and a smooth network. The coarse network is used to explore the areas that need to be focused via count attention mechanism, and generate a rough feature map. Then the fine network processes the areas of interest into a fine feature map. To alleviate the sense of division caused by fusion, the smooth network is designed to combine two feature maps organically to produce high-quality density maps. Extensive experiments are conducted on five mainstream datasets. The results demonstrate the effectiveness of the proposed model for both density estimation and crowd counting tasks.



### GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1908.02809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02809v1)
- **Published**: 2019-08-07 19:44:09+00:00
- **Updated**: 2019-08-07 19:44:09+00:00
- **Authors**: Alexander Grabner, Peter M. Roth, Vincent Lepetit
- **Comment**: Accepted to International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics.



### Location Field Descriptors: Single Image 3D Model Retrieval in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1908.02853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02853v1)
- **Published**: 2019-08-07 21:51:19+00:00
- **Updated**: 2019-08-07 21:51:19+00:00
- **Authors**: Alexander Grabner, Peter M. Roth, Vincent Lepetit
- **Comment**: Accepted to International Conference on 3D Vision (3DV) 2019 (Oral)
- **Journal**: None
- **Summary**: We present Location Field Descriptors, a novel approach for single image 3D model retrieval in the wild. In contrast to previous methods that directly map 3D models and RGB images to an embedding space, we establish a common low-level representation in the form of location fields from which we compute pose invariant 3D shape descriptors. Location fields encode correspondences between 2D pixels and 3D surface coordinates and, thus, explicitly capture 3D shape and 3D pose information without appearance variations which are irrelevant for the task. This early fusion of 3D models and RGB images results in three main advantages: First, the bottleneck location field prediction acts as a regularizer during training. Second, major parts of the system benefit from training on a virtually infinite amount of synthetic data. Finally, the predicted location fields are visually interpretable and unblackbox the system. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple 3D retrieval metrics.



### Unsupervised Feature Learning in Remote Sensing
- **Arxiv ID**: http://arxiv.org/abs/1908.02877v1
- **DOI**: 10.1117/12.2529791
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02877v1)
- **Published**: 2019-08-07 23:48:49+00:00
- **Updated**: 2019-08-07 23:48:49+00:00
- **Authors**: Aaron Reite, Scott Kangas, Zackery Steck, Steven Goley, Jonathan Von Stroh, Steven Forsyth
- **Comment**: None
- **Journal**: None
- **Summary**: The need for labeled data is among the most common and well-known practical obstacles to deploying deep learning algorithms to solve real-world problems. The current generation of learning algorithms requires a large volume of data labeled according to a static and pre-defined schema. Conversely, humans can quickly learn generalizations based on large quantities of unlabeled data, and turn these generalizations into classifications using spontaneous labels, often including labels not seen before. We apply a state-of-the-art unsupervised learning algorithm to the noisy and extremely imbalanced xView data set to train a feature extractor that adapts to several tasks: visual similarity search that performs well on both common and rare classes; identifying outliers within a labeled data set; and learning a natural class hierarchy automatically.



