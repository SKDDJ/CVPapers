# Arxiv Papers in cs.CV on 2019-08-24
### EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User Understanding
- **Arxiv ID**: http://arxiv.org/abs/1908.09060v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09060v1)
- **Published**: 2019-08-24 00:47:39+00:00
- **Updated**: 2019-08-24 00:47:39+00:00
- **Authors**: Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich
- **Comment**: None
- **Journal**: None
- **Summary**: Eye gaze estimation and simultaneous semantic understanding of a user through eye images is a crucial component in Virtual and Mixed Reality; enabling energy efficient rendering, multi-focal displays and effective interaction with 3D content. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation and semantic user understanding for an off-axis camera setting. The tasks include eye segmentation, blink detection, emotive expression classification, IR LED glints detection, pupil and cornea center estimation. To train EyeNet end-to-end we employ both hand labelled supervision and model based supervision. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions.



### Robust Regression via Deep Negative Correlation Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.09066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09066v1)
- **Published**: 2019-08-24 01:26:03+00:00
- **Updated**: 2019-08-24 01:26:03+00:00
- **Authors**: Le Zhang, Zenglin Shi, Ming-Ming Cheng, Yun Liu, Jia-Wang Bian, Joey Tianyi Zhou, Guoyan Zheng, Zeng Zeng
- **Comment**: None
- **Journal**: None
- **Summary**: Nonlinear regression has been extensively employed in many computer vision problems (e.g., crowd counting, age estimation, affective computing). Under the umbrella of deep learning, two common solutions exist i) transforming nonlinear regression to a robust loss function which is jointly optimizable with the deep convolutional network, and ii) utilizing ensemble of deep networks. Although some improved performance is achieved, the former may be lacking due to the intrinsic limitation of choosing a single hypothesis and the latter usually suffers from much larger computational complexity. To cope with those issues, we propose to regress via an efficient "divide and conquer" manner. The core of our approach is the generalization of negative correlation learning that has been shown, both theoretically and empirically, to work well for non-deep regression problems. Without extra parameters, the proposed method controls the bias-variance-covariance trade-off systematically and usually yields a deep regression ensemble where each base model is both "accurate" and "diversified". Moreover, we show that each sub-problem in the proposed method has less Rademacher Complexity and thus is easier to optimize. Extensive experiments on several diverse and challenging tasks including crowd counting, personality analysis, age estimation, and image super-resolution demonstrate the superiority over challenging baselines as well as the versatility of the proposed method.



### Plexus Convolutional Neural Network (PlexusNet): A novel neural network architecture for histologic image analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.09067v2
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.AI, cs.CV, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1908.09067v2)
- **Published**: 2019-08-24 01:29:34+00:00
- **Updated**: 2020-06-03 04:43:21+00:00
- **Authors**: Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Andreas M. Loening, Jeanne Shen, James D. Brooks, Curtis P. Langlotz, Daniel L. Rubin
- **Comment**: None
- **Journal**: None
- **Summary**: Different convolutional neural network (CNN) models have been tested for their application in histological image analyses. However, these models are prone to overfitting due to their large parameter capacity, requiring more data or valuable computational resources for model training. Given these limitations, we introduced a novel architecture (termed PlexusNet). We utilized 310 Hematoxylin and Eosin stained (H&E) annotated histological images of prostate cancer cases from TCGA-PRAD and Stanford University and 398 H&E whole slides images from the Camelyon 2016 challenge. PlexusNet-architecture -derived models were compared to models derived from several existing "state of the art" architectures. We measured discrimination accuracy, calibration, and clinical utility. An ablation study was conducted to study the effect of each component of PlexusNet on model performance. A well-fitted PlexusNet-based model delivered comparable classification performance (AUC: 0.963) in distinguishing prostate cancer from healthy tissues, although it was at least 23 times smaller, had a better model calibration and clinical utility than the comparison models. A separate smaller PlexusNet model accurately detected slides with breast cancer metastases (AUC: 0.978); it helped reduce the slide number to examine by 43.8% without consequences, although its parameter capacity was 200 times smaller than ResNet18. We found that the partitioning of the development set influences the model calibration for all models. However, with PlexusNet architecture, we could achieve comparable well-calibrated models trained on different partitions. In conclusion, PlexusNet represents a novel model architecture for histological image analysis that achieves classification performance comparable to other models while providing orders-of-magnitude parameter reduction.



### Camera Pose Correction in SLAM Based on Bias Values of Map Points
- **Arxiv ID**: http://arxiv.org/abs/1908.09072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09072v1)
- **Published**: 2019-08-24 02:07:51+00:00
- **Updated**: 2019-08-24 02:07:51+00:00
- **Authors**: Zhaobing Kang, Wei Zou, Zheng Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate camera pose estimation result is essential for visual SLAM (VSLAM). This paper presents a novel pose correction method to improve the accuracy of the VSLAM system. Firstly, the relationship between the camera pose estimation error and bias values of map points is derived based on the optimized function in VSLAM. Secondly, the bias value of the map point is calculated by a statistical method. Finally, the camera pose estimation error is compensated according to the first derived relationship. After the pose correction, procedures of the original system, such as the bundle adjustment (BA) optimization, can be executed as before. Compared with existing methods, our algorithm is compact and effective and can be easily generalized to different VSLAM systems. Additionally, the robustness to system noise of our method is better than feature selection methods, due to all original system information is preserved in our algorithm while only a subset is employed in the latter. Experimental results on benchmark datasets show that our approach leads to considerable improvements over state-of-the-art algorithms for absolute pose estimation.



### Situational Fusion of Visual Representation for Visual Navigation
- **Arxiv ID**: http://arxiv.org/abs/1908.09073v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09073v2)
- **Published**: 2019-08-24 02:20:43+00:00
- **Updated**: 2021-08-03 18:49:31+00:00
- **Authors**: Bokui Shen, Danfei Xu, Yuke Zhu, Leonidas J. Guibas, Li Fei-Fei, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: A complex visual navigation task puts an agent in different situations which call for a diverse range of visual perception abilities. For example, to "go to the nearest chair", the agent might need to identify a chair in a living room using semantics, follow along a hallway using vanishing point cues, and avoid obstacles using depth. Therefore, utilizing the appropriate visual perception abilities based on a situational understanding of the visual environment can empower these navigation models in unseen visual environments. We propose to train an agent to fuse a large set of visual representations that correspond to diverse visual perception abilities. To fully utilize each representation, we develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action. Furthermore, we employ a data-driven inter-task affinity regularization to reduce redundancies and improve generalization. Our approach leads to a significantly improved performance in novel environments over ImageNet-pretrained baseline and other fusion methods.



### Residual Objectness for Imbalance Reduction
- **Arxiv ID**: http://arxiv.org/abs/1908.09075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09075v1)
- **Published**: 2019-08-24 02:24:25+00:00
- **Updated**: 2019-08-24 02:24:25+00:00
- **Authors**: Joya Chen, Dong Liu, Bin Luo, Xuezheng Peng, Tong Xu, Enhong Chen
- **Comment**: Tech report
- **Journal**: None
- **Summary**: For a long time, object detectors have suffered from extreme imbalance between foregrounds and backgrounds. While several sampling/reweighting schemes have been explored to alleviate the imbalance, they are usually heuristic and demand laborious hyper-parameters tuning, which is hard to achieve the optimality. In this paper, we first reveal that such the imbalance could be addressed in a learning-based manner. Guided by this illuminating observation, we propose a novel Residual Objectness (ResObj) mechanism that addresses the imbalance by end-to-end optimization, while no further hand-crafted sampling/reweighting is required. Specifically, by applying multiple cascaded objectness-related modules with residual connections, we formulate an elegant consecutive refinement procedure for distinguishing the foregrounds from backgrounds, thereby progressively addressing the imbalance. Extensive experiments present the effectiveness of our method, as well as its compatibility and adaptivity for both region-based and one-stage detectors, namely, the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3.6%, 3.9%, 3.2% Average Precision (AP) improvements compared with their vanilla models on COCO, respectively.



### SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.09086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09086v1)
- **Published**: 2019-08-24 03:48:28+00:00
- **Updated**: 2019-08-24 03:48:28+00:00
- **Authors**: Yan Huang, Qiang Wu, JingSong Xu, Yi Zhong
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, ie., Market-1501, DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.



### Where Is My Mirror?
- **Arxiv ID**: http://arxiv.org/abs/1908.09101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09101v2)
- **Published**: 2019-08-24 06:57:04+00:00
- **Updated**: 2019-10-03 10:44:28+00:00
- **Authors**: Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin, Rynson W. H. Lau
- **Comment**: Accepted by ICCV 2019. Project homepage:
  https://mhaiyang.github.io/ICCV2019_MirrorNet/index.html
- **Journal**: None
- **Summary**: Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods.



### Improving Outfit Recommendation with Co-supervision of Fashion Generation
- **Arxiv ID**: http://arxiv.org/abs/1908.09104v1
- **DOI**: 10.1145/3308558.3313614
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09104v1)
- **Published**: 2019-08-24 07:37:57+00:00
- **Updated**: 2019-08-24 07:37:57+00:00
- **Authors**: Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, Maarten de Rijke
- **Comment**: None
- **Journal**: None
- **Summary**: The task of fashion recommendation includes two main challenges: visual understanding and visual matching. Visual understanding aims to extract effective visual features. Visual matching aims to model a human notion of compatibility to compute a match between fashion items. Most previous studies rely on recommendation loss alone to guide visual understanding and matching. Although the features captured by these methods describe basic characteristics (e.g., color, texture, shape) of the input items, they are not directly related to the visual signals of the output items (to be recommended). This is problematic because the aesthetic characteristics (e.g., style, design), based on which we can directly infer the output items, are lacking. Features are learned under the recommendation loss alone, where the supervision signal is simply whether the given two items are matched or not. To address this problem, we propose a neural co-supervision learning framework, called the FAshion Recommendation Machine (FARM). FARM improves visual understanding by incorporating the supervision of generation loss, which we hypothesize to be able to better encode aesthetic information. FARM enhances visual matching by introducing a novel layer-to-layer matching mechanism to fuse aesthetic information more effectively, and meanwhile avoiding paying too much attention to the generation quality and ignoring the recommendation performance. Extensive experiments on two publicly available datasets show that FARM outperforms state-of-the-art models on outfit recommendation, in terms of AUC and MRR. Detailed analyses of generated and recommended items demonstrate that FARM can encode better features and generate high quality images as references to improve recommendation performance.



### Conditional Flow Variational Autoencoders for Structured Sequence Prediction
- **Arxiv ID**: http://arxiv.org/abs/1908.09008v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09008v3)
- **Published**: 2019-08-24 08:02:34+00:00
- **Updated**: 2020-08-18 09:55:31+00:00
- **Authors**: Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele, Christoph-Nikolas Straehle
- **Comment**: To appear at Bayesian Deep Learning and Machine Learning for
  Autonomous Driving @NeurIPS 2019
- **Journal**: None
- **Summary**: Prediction of future states of the environment and interacting agents is a key competence required for autonomous agents to operate successfully in the real world. Prior work for structured sequence prediction based on latent variable models imposes a uni-modal standard Gaussian prior on the latent variables. This induces a strong model bias which makes it challenging to fully capture the multi-modality of the distribution of the future states. In this work, we introduce Conditional Flow Variational Autoencoders (CF-VAE) using our novel conditional normalizing flow based prior to capture complex multi-modal conditional distributions for effective structured sequence prediction. Moreover, we propose two novel regularization schemes which stabilizes training and deals with posterior collapse for stable training and better fit to the target data distribution. Our experiments on three multi-modal structured sequence prediction datasets -- MNIST Sequences, Stanford Drone and HighD -- show that the proposed method obtains state of art results across different evaluation metrics.



### Generator evaluator-selector net for panoptic image segmentation and splitting unfamiliar objects into parts
- **Arxiv ID**: http://arxiv.org/abs/1908.09108v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.09108v4)
- **Published**: 2019-08-24 09:01:27+00:00
- **Updated**: 2020-04-13 08:55:36+00:00
- **Authors**: Sagi Eppel, Alan Aspuru-Guzik
- **Comment**: None
- **Journal**: None
- **Summary**: In machine learning and other fields, suggesting a good solution to a problem is usually a harder task than evaluating the quality of such a solution. This asymmetry is the basis for a large number of selection oriented methods that use a generator system to guess a set of solutions and an evaluator system to rank and select the best solutions. This work examines the use of this approach to the problem of panoptic image segmentation and class agnostic parts segmentation. The generator/evaluator approach for this case consists of two independent convolutional neural nets: a generator net that suggests variety segments corresponding to objects, stuff and parts regions in the image, and an evaluator net that chooses the best segments to be merged into the segmentation map. The result is a trial and error evolutionary approach in which a generator that guesses segments with low average accuracy, but with wide variability, can still produce good results when coupled with an accurate evaluator. The generator consists of a Pointer net that receives an image and a point in the image, and predicts the region of the segment containing the point. Generating and evaluating each segment separately is essential in this case since it demands exponentially fewer guesses compared to a system that guesses and evaluates the full segmentation map in each try. The classification of the selected segments is done by an independent region-specific classification net. This allows the segmentation to be class agnostic and hence, capable of segmenting unfamiliar categories that were not part of the training set. The method was examined on the COCO Panoptic segmentation benchmark and gave results comparable to those of the basic semantic segmentation and Mask-RCNN methods. In addition, the system was used for the task of splitting objects of unseen classes (that did not appear in the training set) into parts.



### SeesawFaceNets: sparse and robust face verification model for mobile platform
- **Arxiv ID**: http://arxiv.org/abs/1908.09124v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.09124v3)
- **Published**: 2019-08-24 11:21:38+00:00
- **Updated**: 2019-12-01 12:52:26+00:00
- **Authors**: Jintao Zhang
- **Comment**: 8 pages, 2 figures. All source code and proposed models will be
  released publicly later
- **Journal**: None
- **Summary**: Deep Convolutional Neural Network (DCNNs) come to be the most widely used solution for most computer vision related tasks, and one of the most important application scenes is face verification. Due to its high-accuracy performance, deep face verification models of which the inference stage occurs on cloud platform through internet plays the key role on most prectical scenes. However, two critical issues exist: First, individual privacy may not be well protected since they have to upload their personal photo and other private information to the online cloud backend. Secondly, either training or inference stage is time-comsuming and the latency may affect customer experience, especially when the internet link speed is not so stable or in remote areas where mobile reception is not so good, but also in cities where building and other construction may block mobile signals. Therefore, designing lightweight networks with low memory requirement and computational cost is one of the most practical solutions for face verification on mobile platform. In this paper, a novel mobile network named SeesawFaceNets, a simple but effective model, is proposed for productively deploying face recognition for mobile devices. Dense experimental results have shown that our proposed model SeesawFaceNets outperforms the baseline MobilefaceNets, with only {\bf66\%}(146M VS 221M MAdds) computational cost, smaller batch size and less training steps, and SeesawFaceNets achieve comparable performance with other SOTA model e.g. mobiface with only {\bf54.2\%}(1.3M VS 2.4M) parameters and {\bf31.6\%}(146M VS 462M MAdds) computational cost, It is also eventually competitive against large-scale deep-networks face recognition on all 5 listed public validation datasets, with {\bf6.5\%}(4.2M VS 65M) parameters and {\bf4.35\%}(526M VS 12G MAdds) computational cost.



### LANTERN: learn analysis transform network for dynamic magnetic resonance imaging with small dataset
- **Arxiv ID**: http://arxiv.org/abs/1908.09140v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09140v1)
- **Published**: 2019-08-24 14:04:58+00:00
- **Updated**: 2019-08-24 14:04:58+00:00
- **Authors**: Shanshan Wang, Yanxia Chen, Taohui Xiao, Ziwen Ke, Qiegen Liu, Hairong Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes to learn analysis transform network for dynamic magnetic resonance imaging (LANTERN) with small dataset. Integrating the strength of CS-MRI and deep learning, the proposed framework is highlighted in three components: (i) The spatial and temporal domains are sparsely constrained by using adaptively trained CNN. (ii) We introduce an end-to-end framework to learn the parameters in LANTERN to solve the difficulty of parameter selection in traditional methods. (iii) Compared to existing deep learning reconstruction methods, our reconstruction accuracy is better when the amount of data is limited. Our model is able to fully exploit the redundancy in spatial and temporal of dynamic MR images. We performed quantitative and qualitative analysis of cardiac datasets at different acceleration factors (2x-11x) and different undersampling modes. In comparison with state-of-the-art methods, extensive experiments show that our method achieves consistent better reconstruction performance on the MRI reconstruction in terms of three quantitative metrics (PSNR, SSIM and HFEN) under different undersamling patterns and acceleration factors.



### Estimation of preterm birth markers with U-Net segmentation network
- **Arxiv ID**: http://arxiv.org/abs/1908.09148v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09148v1)
- **Published**: 2019-08-24 15:14:11+00:00
- **Updated**: 2019-08-24 15:14:11+00:00
- **Authors**: Tomasz Włodarczyk, Szymon Płotka, Tomasz Trzciński, Przemysław Rokita, Nicole Sochacki-Wójcicka, Michał Lipa, Jakub Wójcicki
- **Comment**: Accepted at MICCAI Workshop on Perinatal, Preterm and Paediatric
  Image analysis (PIPPI) 2019
- **Journal**: None
- **Summary**: Preterm birth is the most common cause of neonatal death. Current diagnostic methods that assess the risk of preterm birth involve the collection of maternal characteristics and transvaginal ultrasound imaging conducted in the first and second trimester of pregnancy. Analysis of the ultrasound data is based on visual inspection of images by gynaecologist, sometimes supported by hand-designed image features such as cervical length. Due to the complexity of this process and its subjective component, approximately 30% of spontaneous preterm deliveries are not correctly predicted. Moreover, 10% of the predicted preterm deliveries are false-positives. In this paper, we address the problem of predicting spontaneous preterm delivery using machine learning. To achieve this goal, we propose to first use a deep neural network architecture for segmenting prenatal ultrasound images and then automatically extract two biophysical ultrasound markers, cervical length (CL) and anterior cervical angle (ACA), from the resulting images. Our method allows to estimate ultrasound markers without human oversight. Furthermore, we show that CL and ACA markers, when combined, allow us to decrease false-negative ratio from 30% to 18%. Finally, contrary to the current approaches to diagnostics methods that rely only on gynaecologist's expertise, our method introduce objectively obtained results.



### Fast Dynamic Perfusion and Angiography Reconstruction using an end-to-end 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1908.08947v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08947v2)
- **Published**: 2019-08-24 15:51:20+00:00
- **Updated**: 2019-09-04 08:16:39+00:00
- **Authors**: Sahar Yousefi, Lydiane Hirschler, Merlijn van der Plas, Mohamed S. Elmahdy, Hessam Sokooti, Matthias Van Osch, Marius Staring
- **Comment**: 11 pages, 4 figures, 1 table, conference paper, accepted in MLMIR2019
- **Journal**: None
- **Summary**: Hadamard time-encoded pseudo-continuous arterial spin labeling (te-pCASL) is a signal-to-noise ratio (SNR)-efficient MRI technique for acquiring dynamic pCASL signals that encodes the temporal information into the labeling according to a Hadamard matrix. In the decoding step, the contribution of each sub-bolus can be isolated resulting in dynamic perfusion scans. When acquiring te-ASL both with and without flow-crushing, the ASL-signal in the arteries can be isolated resulting in 4D-angiographic information. However, obtaining multi-timepoint perfusion and angiographic data requires two acquisitions. In this study, we propose a 3D Dense-Unet convolutional neural network with a multi-level loss function for reconstructing multi-timepoint perfusion and angiographic information from an interleaved $50\%$-sampled crushed and $50\%$-sampled non-crushed data, thereby negating the additional scan time. We present a framework to generate dynamic pCASL training and validation data, based on models of the intravascular and extravascular te-pCASL signals. The proposed network achieved SSIM values of $97.3 \pm 1.1$ and $96.2 \pm 11.1$ respectively for 4D perfusion and angiographic data reconstruction for 313 test data-sets.



### Don't ignore Dropout in Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.09162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09162v1)
- **Published**: 2019-08-24 16:28:40+00:00
- **Updated**: 2019-08-24 16:28:40+00:00
- **Authors**: Thomas Spilsbury, Paavo Camps
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Data for Image segmentation models can be costly to obtain due to the precision required by human annotators. We run a series of experiments showing the effect of different kinds of Dropout training on the DeepLabv3+ Image segmentation model when trained using a small dataset. We find that when appropriate forms of Dropout are applied in the right place in the model architecture that non-insignificant improvement in Mean Intersection over Union (mIoU) score can be observed. In our best case, we find that applying Dropout scheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49 to 0.59. This result shows that even where a model architecture makes extensive use of Batch Normalization, Dropout can still be an effective way of improving performance in low data situations.



### Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve the Tower
- **Arxiv ID**: http://arxiv.org/abs/1908.09163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09163v1)
- **Published**: 2019-08-24 16:30:44+00:00
- **Updated**: 2019-08-24 16:30:44+00:00
- **Authors**: Giorgos Tolias, Filip Radenovic, Ondřej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: Access to online visual search engines implies sharing of private user content - the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.



### Efficient Learning on Point Clouds with Basis Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1908.09186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09186v1)
- **Published**: 2019-08-24 18:53:52+00:00
- **Updated**: 2019-08-24 18:53:52+00:00
- **Authors**: Sergey Prokudin, Christoph Lassner, Javier Romero
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: With the increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to their unordered structure. One common approach is to apply occupancy grid mapping, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures often use an increased number of parameters and are computationally inefficient. In this work, we propose basis point sets (BPS) as a highly efficient and fully general way to process point clouds with machine learning algorithms. The basis point set representation is a residual representation that can be computed efficiently and can be used with standard neural network architectures and other machine learning algorithms. Using the proposed representation as the input to a simple fully connected network allows us to match the performance of PointNet on a shape classification task while using three orders of magnitude less floating-point operations. In a second experiment, we show how the proposed representation can be used for registering high-resolution meshes to noisy 3D scans. Here, we present the first method for single-pass high-resolution mesh registration, avoiding time-consuming per-scan optimization and allowing real-time execution.



### Deep Camera: A Fully Convolutional Neural Network for Image Signal Processing
- **Arxiv ID**: http://arxiv.org/abs/1908.09191v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09191v1)
- **Published**: 2019-08-24 19:31:15+00:00
- **Updated**: 2019-08-24 19:31:15+00:00
- **Authors**: Sivalogeswaran Ratnasingam
- **Comment**: 11 pages, 6 figures, conference:ICCV 2019 workshop: Learning for
  Computational Imaging (LCI)
- **Journal**: None
- **Summary**: A conventional camera performs various signal processing steps sequentially to reconstruct an image from a raw Bayer image. When performing these processing in multiple stages the residual error from each stage accumulates in the image and degrades the quality of the final reconstructed image. In this paper, we present a fully convolutional neural network (CNN) to perform defect pixel correction, denoising, white balancing, exposure correction, demosaicing, color transform, and gamma encoding. To our knowledge, this is the first CNN trained end-to-end to perform the entire image signal processing pipeline in a camera. The neural network was trained using a large image database of raw Bayer images. Through extensive experiments, we show that the proposed CNN based image signal processing system performs better than the conventional signal processing pipelines that perform the processing sequentially.



### Customized OCT images compression scheme with deep neural network
- **Arxiv ID**: http://arxiv.org/abs/1908.09215v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09215v2)
- **Published**: 2019-08-24 21:38:29+00:00
- **Updated**: 2019-08-27 15:57:12+00:00
- **Authors**: Pengfei Guo, Dawei Li, Xingde Li
- **Comment**: One of author disagrees to release this paper at Arxiv
- **Journal**: None
- **Summary**: We customize an end-to-end image compression framework for retina OCT images based on deep convolutional neural networks (CNNs). The customized compression scheme consists of three parts: data Preprocessing, compression CNNs, and reconstruction CNNs. Data preprocessing module reduces the speckle noise of the OCT images and the segments out the region of interest. We added customized skip connections between the compression CNNs and the reconstruction CNNs to reserve the detail information and trained the two nets together with the semantic segmented image patches from data preprocessing module. To train the two networks sensitive to both low frequency information and high frequency information, we adopted an objective function with two parts: A PatchGAN discriminator to judge the high frequency information and a differentiable MS-SSIM penalty to evaluate the low frequency information. The proposed framework was trained and evaluated on a publicly available OCT dataset. The evaluation showed above 99% similarity in terms of multi-scale structural similarity (MS-SSIM) when the compression ratio is as high as 40. Furthermore, the reconstructed images of compression ratio 80 from the proposed framework even have better quality than that of compression ratio 20 from JPEG by visual comparison. The testing result outperforms JPEG in term of both of MS-SSIM and visualization, which is more obvious as the increase of compression ratio. Our preliminary result indicates the huge potential of deep neural networks on customized medical image compression.



### Dynamic Kernel Distillation for Efficient Pose Estimation in Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.09216v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09216v1)
- **Published**: 2019-08-24 21:44:02+00:00
- **Updated**: 2019-08-24 21:44:02+00:00
- **Authors**: Xuecheng Nie, Yuncheng Li, Linjie Luo, Ning Zhang, Jiashi Feng
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy.



### Single-Stage Multi-Person Pose Machines
- **Arxiv ID**: http://arxiv.org/abs/1908.09220v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09220v1)
- **Published**: 2019-08-24 22:05:51+00:00
- **Updated**: 2019-08-24 22:05:51+00:00
- **Authors**: Xuecheng Nie, Jianfeng Zhang, Shuicheng Yan, Jiashi Feng
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Multi-person pose estimation is a challenging problem. Existing methods are mostly two-stage based--one stage for proposal generation and the other for allocating poses to corresponding persons. However, such two-stage methods generally suffer low efficiency. In this work, we present the first single-stage model, Single-stage multi-person Pose Machine (SPM), to simplify the pipeline and lift the efficiency for multi-person pose estimation. To achieve this, we propose a novel Structured Pose Representation (SPR) that unifies person instance and body joint position representations. Based on SPR, we develop the SPM model that can directly predict structured poses for multiple persons in a single stage, and thus offer a more compact pipeline and attractive efficiency advantage over two-stage methods. In particular, SPR introduces the root joints to indicate different person instances and human body joint positions are encoded into their displacements w.r.t. the roots. To better predict long-range displacements for some joints, SPR is further extended to hierarchical representations. Based on SPR, SPM can efficiently perform multi-person poses estimation by simultaneously predicting root joints (location of instances) and body joint displacements via CNNs. Moreover, to demonstrate the generality of SPM, we also apply it to multi-person 3D pose estimation. Comprehensive experiments on benchmarks MPII, extended PASCAL-Person-Part, MSCOCO and CMU Panoptic clearly demonstrate the state-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation, together with outstanding accuracy.



### Towards Unconstrained End-to-End Text Spotting
- **Arxiv ID**: http://arxiv.org/abs/1908.09231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09231v1)
- **Published**: 2019-08-24 23:41:07+00:00
- **Updated**: 2019-08-24 23:41:07+00:00
- **Authors**: Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, Ying Xiao
- **Comment**: Accepted to ICCV 2019 as oral presentation
- **Journal**: None
- **Summary**: We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectification. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to significant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the Total-Text (curved) benchmark by more than 16%.



