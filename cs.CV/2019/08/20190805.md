# Arxiv Papers in cs.CV on 2019-08-05
### Inference of visual field test performance from OCT volumes using deep learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01428v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01428v3)
- **Published**: 2019-08-05 00:35:18+00:00
- **Updated**: 2019-10-10 04:43:27+00:00
- **Authors**: Stefan Maetschke, Bhavna Antony, Hiroshi Ishikawa, Gadi Wollstein, Joel Schuman, Rahil Garnavi
- **Comment**: 12 pages, 3 figures
- **Journal**: None
- **Summary**: Visual field tests (VFT) are pivotal for glaucoma diagnosis and conducted regularly to monitor disease progression. Here we address the question to what degree aggregate VFT measurements such as Visual Field Index (VFI) and Mean Deviation (MD) can be inferred from Optical Coherence Tomography (OCT) scans of the Optic Nerve Head (ONH) or the macula. Accurate inference of VFT measurements from OCT could reduce examination time and cost. We propose a novel 3D Convolutional Neural Network (CNN) for this task and compare its accuracy with classical machine learning (ML) algorithms trained on common, segmentation-based OCT, features employed for glaucoma diagnostics. Peak accuracies were achieved on ONH scans when inferring VFI with a Pearson Correlation (PC) of 0.88$\pm$0.035 for the CNN and a significantly lower (p $<$ 0.01) PC of 0.74$\pm$0.090 for the best performing, classical ML algorithm - a Random Forest regressor. Estimation of MD was equally accurate with a PC of 0.88$\pm$0.023 on ONH scans for the CNN.



### Restricted Linearized Augmented Lagrangian Method for Euler's Elastica Model
- **Arxiv ID**: http://arxiv.org/abs/1908.01429v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01429v1)
- **Published**: 2019-08-05 00:40:41+00:00
- **Updated**: 2019-08-05 00:40:41+00:00
- **Authors**: Yinghui Zhang, Xiaojuan Deng, Jun Zhang, Hongwei Li
- **Comment**: None
- **Journal**: None
- **Summary**: Euler's elastica model has been extensively studied and applied to image processing tasks. However, due to the high nonlinearity and nonconvexity of the involved curvature term, conventional algorithms suffer from slow convergence and high computational cost. Various fast algorithms have been proposed, among which, the augmented Lagrangian based ones are very popular in the community. However, parameter tuning might be very challenging for these methods. In this paper, a simple cutting-off strategy is introduced into the augmented Lagrangian based algorithms for minimizing the Euler's elastica energy, which leads to easy parameter tuning and fast convergence. The cutting-off strategy is based on an observation of inconsistency inside the augmented Lagrangian based algorithms. When the weighting parameter of the curvature term goes to zero, the energy functional boils down to the ROF model. So, a natural requirement is that its augmented Lagrangian based algorithms should also approach the augmented Lagrangian based algorithms formulated directly for solving the ROF model from the very beginning. Unfortunately, this is not the case for certain existing augmented Lagrangian based algorithms. The proposed cutting-off strategy helps to decouple the tricky dependence between the auxiliary splitting variables, so as to remove the observed inconsistency. Numerical experiments suggest that the proposed algorithm enjoys easier parameter-tuning, faster convergence and even higher quality of image restorations.



### Architecture-aware Network Pruning for Vision Quality Applications
- **Arxiv ID**: http://arxiv.org/abs/1908.02125v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02125v1)
- **Published**: 2019-08-05 01:54:22+00:00
- **Updated**: 2019-08-05 01:54:22+00:00
- **Authors**: Wei-Ting Wang, Han-Lin Li, Wei-Shiang Lin, Cheng-Ming Chiang, Yi-Min Tsai
- **Comment**: Accepted to be Published in the 26th IEEE International Conference on
  Image Processing (ICIP 2019). Updated to contain the IEEE copyright notice
- **Journal**: None
- **Summary**: Convolutional neural network (CNN) delivers impressive achievements in computer vision and machine learning field. However, CNN incurs high computational complexity, especially for vision quality applications because of large image resolution. In this paper, we propose an iterative architecture-aware pruning algorithm with adaptive magnitude threshold while cooperating with quality-metric measurement simultaneously. We show the performance improvement applied on vision quality applications and provide comprehensive analysis with flexible pruning configuration. With the proposed method, the Multiply-Accumulate (MAC) of state-of-the-art low-light imaging (SID) and super-resolution (EDSR) are reduced by 58% and 37% without quality drop, respectively. The memory bandwidth (BW) requirements of convolutional layer can be also reduced by 20% to 40%.



### Learning Compact Target-Oriented Feature Representations for Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.01442v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01442v1)
- **Published**: 2019-08-05 02:01:02+00:00
- **Updated**: 2019-08-05 02:01:02+00:00
- **Authors**: Chenglong Li, Yan Huang, Liang Wang, Jin Tang, Liang Lin
- **Comment**: 10 pages, 4 figures,6 tables
- **Journal**: None
- **Summary**: Many state-of-the-art trackers usually resort to the pretrained convolutional neural network (CNN) model for correlation filtering, in which deep features could usually be redundant, noisy and less discriminative for some certain instances, and the tracking performance might thus be affected. To handle this problem, we propose a novel approach, which takes both advantages of good generalization of generative models and excellent discrimination of discriminative models, for visual tracking. In particular, we learn compact, discriminative and target-oriented feature representations using the Laplacian coding algorithm that exploits the dependence among the input local features in a discriminative correlation filter framework. The feature representations and the correlation filter are jointly learnt to enhance to each other via a fast solver which only has very slight computational burden on the tracking speed. Extensive experiments on three benchmark datasets demonstrate that this proposed framework clearly outperforms baseline trackers with a modest impact on the frame rate, and performs comparably against the state-of-the-art methods.



### Image to Video Domain Adaptation Using Web Supervision
- **Arxiv ID**: http://arxiv.org/abs/1908.01449v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01449v1)
- **Published**: 2019-08-05 02:50:59+00:00
- **Updated**: 2019-08-05 02:50:59+00:00
- **Authors**: Andrew Kae, Yale Song
- **Comment**: None
- **Journal**: None
- **Summary**: Training deep neural networks typically requires large amounts of labeled data which may be scarce or expensive to obtain for a particular target domain. As an alternative, we can leverage webly-supervised data (i.e. results from a public search engine) which are relatively plentiful but may contain noisy results. In this work, we propose a novel two-stage approach to learn a video classifier using webly-supervised data. We argue that learning appearance features and then temporal features sequentially, rather than simultaneously, is an easier optimization for this task. We show this by first learning an image model from web images, which is used to initialize and train a video model. Our model applies domain adaptation to account for potential domain shift present between the source domain (webly-supervised data) and target domain and also accounts for noise by adding a novel attention component. We report results competitive with state-of-the-art for webly-supervised approaches on UCF-101 (while simplifying the training process) and also evaluate on Kinetics for comparison.



### TopoTag: A Robust and Scalable Topological Fiducial Marker System
- **Arxiv ID**: http://arxiv.org/abs/1908.01450v3
- **DOI**: 10.1109/TVCG.2020.2988466
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.01450v3)
- **Published**: 2019-08-05 02:57:50+00:00
- **Updated**: 2020-04-15 09:58:06+00:00
- **Authors**: Guoxing Yu, Yongtao Hu, Jingwen Dai
- **Comment**: Accepted to TVCG
- **Journal**: None
- **Summary**: Fiducial markers have been playing an important role in augmented reality (AR), robot navigation, and general applications where the relative pose between a camera and an object is required. Here we introduce TopoTag, a robust and scalable topological fiducial marker system, which supports reliable and accurate pose estimation from a single image. TopoTag uses topological and geometrical information in marker detection to achieve higher robustness. Topological information is extensively used for 2D marker detection, and further corresponding geometrical information for ID decoding. Robust 3D pose estimation is achieved by taking advantage of all TopoTag vertices. Without sacrificing bits for higher recall and precision like previous systems, TopoTag can use full bits for ID encoding. TopoTag supports tens of thousands unique IDs and easily extends to millions of unique tags resulting in massive scalability. We collected a large test dataset including in total 169,713 images for evaluation, involving in-plane and out-of-plane rotation, image blur, different distances and various backgrounds, etc. Experiments on the dataset and real indoor and outdoor scene tests with a rolling shutter camera both show that TopoTag significantly outperforms previous fiducial marker systems in terms of various metrics, including detection accuracy, vertex jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion as long as the main tag topological structure is maintained and allows for flexible shape design where users can customize internal and external marker shapes. Code for our marker design/generation, marker detection, and dataset are available at http://herohuyongtao.github.io/research/publications/topo-tag/.



### Mass Estimation from Images using Deep Neural Network and Sparse Ground Truth
- **Arxiv ID**: http://arxiv.org/abs/1908.04387v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04387v3)
- **Published**: 2019-08-05 02:59:18+00:00
- **Updated**: 2019-09-10 17:46:09+00:00
- **Authors**: Muhammad K A Hamdan, Daine T. Rover, Matthew J. Darr, John Just
- **Comment**: 9 pages, 19 figures, pre-print NIPS2019
- **Journal**: None
- **Summary**: Supervised learning is the workhorse for regression and classification tasks, but the standard approach presumes ground truth for every measurement. In real world applications, limitations due to expense or general in-feasibility due to the specific application are common. In the context of agriculture applications, yield monitoring is one such example where simple-physics based measurements such as volume or force-impact have been used to quantify mass flow, which incur error due to sensor calibration. By utilizing semi-supervised deep learning with gradient aggregation and a sequence of images, in this work we can accurately estimate a physical quantity (mass) with complex data structures and sparse ground truth. Using a vision system capturing images of a sugarcane elevator and running bamboo under controlled testing as a surrogate material to harvesting sugarcane, mass is accurately predicted from images by training a DNN using only final load weights. The DNN succeeds in capturing the complex density physics of random stacking of slender rods internally as part of the mass prediction model, and surpasses older volumetric-based methods for mass prediction. Furthermore, by incorporating knowledge about the system physics through the DNN architecture and penalty terms, improvements in prediction accuracy and stability, as well as faster learning are obtained. It is shown that the classic nonlinear regression optimization can be reformulated with an aggregation term with some independence assumptions to achieve this feat. Since the number of images for any given run are too large to fit on typical GPU vRAM, an implementation is shown that compensates for the limited memory but still achieve fast training times. The same approach presented herein could be applied to other applications like yield monitoring on grain combines or other harvesters using vision or other instrumentation.



### Automated Detection System for Adversarial Examples with High-Frequency Noises Sieve
- **Arxiv ID**: http://arxiv.org/abs/1908.01469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01469v1)
- **Published**: 2019-08-05 05:05:29+00:00
- **Updated**: 2019-08-05 05:05:29+00:00
- **Authors**: Dang Duy Thang, Toshihiro Matsui
- **Comment**: Appear to 11th International Symposium on Cyberspace Safety and
  Security CSS 2019, Guangzhou, China
- **Journal**: None
- **Summary**: Deep neural networks are being applied in many tasks with encouraging results, and have often reached human-level performance. However, deep neural networks are vulnerable to well-designed input samples called adversarial examples. In particular, neural networks tend to misclassify adversarial examples that are imperceptible to humans. This paper introduces a new detection system that automatically detects adversarial examples on deep neural networks. Our proposed system can mostly distinguish adversarial samples and benign images in an end-to-end manner without human intervention. We exploit the important role of the frequency domain in adversarial samples and propose a method that detects malicious samples in observations. When evaluated on two standard benchmark datasets (MNIST and ImageNet), our method achieved an out-detection rate of 99.7 - 100% in many settings.



### GDRQ: Group-based Distribution Reshaping for Quantization
- **Arxiv ID**: http://arxiv.org/abs/1908.01477v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01477v1)
- **Published**: 2019-08-05 05:44:52+00:00
- **Updated**: 2019-08-05 05:44:52+00:00
- **Authors**: Haibao Yu, Tuopu Wen, Guangliang Cheng, Jiankai Sun, Qi Han, Jianping Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Low-bit quantization is challenging to maintain high performance with limited model capacity (e.g., 4-bit for both weights and activations). Naturally, the distribution of both weights and activations in deep neural network are Gaussian-like. Nevertheless, due to the limited bitwidth of low-bit model, uniform-like distributed weights and activations have been proved to be more friendly to quantization while preserving accuracy~\cite{Han2015Learning}. Motivated by this, we propose Scale-Clip, a Distribution Reshaping technique that can reshape weights or activations into a uniform-like distribution in a dynamic manner. Furthermore, to increase the model capability for a low-bit model, a novel Group-based Quantization algorithm is proposed to split the filters into several groups. Different groups can learn different quantization parameters, which can be elegantly merged in to batch normalization layer without extra computational cost in the inference stage. Finally, we integrate Scale-Clip technique with Group-based Quantization algorithm and propose the Group-based Distribution Reshaping Quantization (GDQR) framework to further improve the quantization performance. Experiments on various networks (e.g. VGGNet and ResNet) and vision tasks (e.g. classification, detection and segmentation) demonstrate that our framework achieves good performance.



### CameraNet: A Two-Stage Framework for Effective Camera ISP Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01481v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01481v2)
- **Published**: 2019-08-05 06:13:31+00:00
- **Updated**: 2019-08-08 02:20:25+00:00
- **Authors**: Zhetong Liang, Jianrui Cai, Zisheng Cao, Lei Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Traditional image signal processing (ISP) pipeline consists of a set of individual image processing components onboard a camera to reconstruct a high-quality sRGB image from the sensor raw data. Due to the hand-crafted nature of the ISP components, traditional ISP pipeline has limited reconstruction quality under challenging scenes. Recently, the convolutional neural networks (CNNs) have demonstrated their competitiveness in solving many individual image processing problems, such as image denoising, demosaicking, white balance and contrast enhancement. However, it remains a question whether a CNN model can address the multiple tasks inside an ISP pipeline simultaneously. We make a good attempt along this line and propose a novel framework, which we call CameraNet, for effective and general ISP pipeline learning. The CameraNet is composed of two CNN modules to account for two sets of relatively uncorrelated subtasks in an ISP pipeline: restoration and enhancement. To train the two-stage CameraNet model, we specify two groundtruths that can be easily created in the common workflow of photography. CameraNet is trained to progressively address the restoration and the enhancement subtasks with its two modules. Experiments show that the proposed CameraNet achieves consistently compelling reconstruction quality on three benchmark datasets and outperforms traditional ISP pipelines.



### Walking with MIND: Mental Imagery eNhanceD Embodied QA
- **Arxiv ID**: http://arxiv.org/abs/1908.01482v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.01482v1)
- **Published**: 2019-08-05 06:17:03+00:00
- **Updated**: 2019-08-05 06:17:03+00:00
- **Authors**: Juncheng Li, Siliang Tang, Fei Wu, Yueting Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: The EmbodiedQA is a task of training an embodied agent by intelligently navigating in a simulated environment and gathering visual information to answer questions. Existing approaches fail to explicitly model the mental imagery function of the agent, while the mental imagery is crucial to embodied cognition, and has a close relation to many high-level meta-skills such as generalization and interpretation. In this paper, we propose a novel Mental Imagery eNhanceD (MIND) module for the embodied agent, as well as a relevant deep reinforcement framework for training. The MIND module can not only model the dynamics of the environment (e.g. 'what might happen if the agent passes through a door') but also help the agent to create a better understanding of the environment (e.g. 'The refrigerator is usually in the kitchen'). Such knowledge makes the agent a faster and better learner in locating a feasible policy with only a few trails. Furthermore, the MIND module can generate mental images that are treated as short-term subgoals by our proposed deep reinforcement framework. These mental images facilitate policy learning since short-term subgoals are easy to achieve and reusable. This yields better planning efficiency than other algorithms that learn a policy directly from primitive actions. Finally, the mental images visualize the agent's intentions in a way that human can understand, and this endows our agent's actions with more interpretability. The experimental results and further analysis prove that the agent with the MIND module is superior to its counterparts not only in EQA performance but in many other aspects such as route planning, behavioral interpretation, and the ability to generalize from a few examples.



### Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation
- **Arxiv ID**: http://arxiv.org/abs/1908.01491v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01491v2)
- **Published**: 2019-08-05 07:00:11+00:00
- **Updated**: 2019-08-16 16:24:37+00:00
- **Authors**: Chao Wen, Yinda Zhang, Zhuwen Li, Yanwei Fu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.



### NeuroMask: Explaining Predictions of Deep Neural Networks through Mask Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.04389v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04389v1)
- **Published**: 2019-08-05 07:33:30+00:00
- **Updated**: 2019-08-05 07:33:30+00:00
- **Authors**: Moustafa Alzantot, Amy Widdicombe, Simon Julier, Mani Srivastava
- **Comment**: None
- **Journal**: Published in the DAIS 2019 - Workshop on Distributed Analytics
  InfraStructure and Algorithms for Multi-Organization Federations
- **Summary**: Deep Neural Networks (DNNs) deliver state-of-the-art performance in many image recognition and understanding applications. However, despite their outstanding performance, these models are black-boxes and it is hard to understand how they make their decisions. Over the past few years, researchers have studied the problem of providing explanations of why DNNs predicted their results. However, existing techniques are either obtrusive, requiring changes in model training, or suffer from low output quality. In this paper, we present a novel method, NeuroMask, for generating an interpretable explanation of classification model results. When applied to image classification models, NeuroMask identifies the image parts that are most important to classifier results by applying a mask that hides/reveals different parts of the image, before feeding it back into the model. The mask values are tuned by minimizing a properly designed cost function that preserves the classification result and encourages producing an interpretable mask. Experiments using state-of-the-art Convolutional Neural Networks for image recognition on different datasets (CIFAR-10 and ImageNet) show that NeuroMask successfully localizes the parts of the input image which are most relevant to the DNN decision. By showing a visual quality comparison between NeuroMask explanations and those of other methods, we find NeuroMask to be both accurate and interpretable.



### Part Segmentation for Highly Accurate Deformable Tracking in Occlusions via Fully Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.01504v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.01504v1)
- **Published**: 2019-08-05 08:08:56+00:00
- **Updated**: 2019-08-05 08:08:56+00:00
- **Authors**: Weilin Wan, Aaron Walsman, Dieter Fox
- **Comment**: None
- **Journal**: IEEE International Conference on Robotics and Automation 2019
- **Summary**: Successfully tracking the human body is an important perceptual challenge for robots that must work around people. Existing methods fall into two broad categories: geometric tracking and direct pose estimation using machine learning. While recent work has shown direct estimation techniques can be quite powerful, geometric tracking methods using point clouds can provide a very high level of 3D accuracy which is necessary for many robotic applications. However these approaches can have difficulty in clutter when large portions of the subject are occluded. To overcome this limitation, we propose a solution based on fully convolutional neural networks (FCN). We develop an optimized Fast-FCN network architecture for our application which allows us to filter observed point clouds and improve tracking accuracy while maintaining interactive frame rates. We also show that this model can be trained with a limited number of examples and almost no manual labelling by using an existing geometric tracker and data augmentation to automatically generate segmentation maps. We demonstrate the accuracy of our full system by comparing it against an existing geometric tracker, and show significant improvement in these challenging scenarios.



### A Fast Content-Based Image Retrieval Method Using Deep Visual Features
- **Arxiv ID**: http://arxiv.org/abs/1908.01505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.01505v1)
- **Published**: 2019-08-05 08:09:36+00:00
- **Updated**: 2019-08-05 08:09:36+00:00
- **Authors**: Hiroki Tanioka
- **Comment**: accepted in ICDAR-WML: The 2nd International Workshop on Machine
  Learning 2019
- **Journal**: None
- **Summary**: Fast and scalable Content-Based Image Retrieval using visual features is required for document analysis, Medical image analysis, etc. in the present age. Convolutional Neural Network (CNN) activations as features achieved their outstanding performance in this area. Deep Convolutional representations using the softmax function in the output layer are also ones among visual features. However, almost all the image retrieval systems hold their index of visual features on main memory in order to high responsiveness, limiting their applicability for big data applications. In this paper, we propose a fast calculation method of cosine similarity with L2 norm indexed in advance on Elasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16 pre-trained model. The evaluation results show the effectiveness and efficiency of our proposed method.



### Difficulty Classification of Mountainbike Downhill Trails utilizing Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.04390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04390v1)
- **Published**: 2019-08-05 08:11:07+00:00
- **Updated**: 2019-08-05 08:11:07+00:00
- **Authors**: Stefan Langer, Robert Müller, Kyrill Schmid, Claudia Linnhoff-Popien
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: The difficulty of mountainbike downhill trails is a subjective perception. However, sports-associations and mountainbike park operators attempt to group trails into different levels of difficulty with scales like the Singletrail-Skala (S0-S5) or colored scales (blue, red, black, ...) as proposed by The International Mountain Bicycling Association. Inconsistencies in difficulty grading occur due to the various scales, different people grading the trails, differences in topography, and more. We propose an end-to-end deep learning approach to classify trails into three difficulties easy, medium, and hard by using sensor data. With mbientlab Meta Motion r0.2 sensor units, we record accelerometer- and gyroscope data of one rider on multiple trail segments. A 2D convolutional neural network is trained with a stacked and concatenated representation of the aforementioned data as its input. We run experiments with five different sample- and five different kernel sizes and achieve a maximum Sparse Categorical Accuracy of 0.9097. To the best of our knowledge, this is the first work targeting computational difficulty classification of mountainbike downhill trails.



### Adversarial Self-Defense for Cycle-Consistent GANs
- **Arxiv ID**: http://arxiv.org/abs/1908.01517v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01517v1)
- **Published**: 2019-08-05 08:37:40+00:00
- **Updated**: 2019-08-05 08:37:40+00:00
- **Authors**: Dina Bashkirova, Ben Usman, Kate Saenko
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of unsupervised image-to-image translation is to map images from one domain to another without the ground truth correspondence between the two domains. State-of-art methods learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. In order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycle-consistency loss that penalizes incorrect reconstruction of the input image from the translated one. However, if the target mapping is many-to-one, e.g. aerial photos to maps, such a restriction forces the generator to hide information in low-amplitude structured noise that is undetectable by human eye or by the discriminator. In this paper, we show how such self-attacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. We perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations.



### 3D Reconstruction of Deformable Revolving Object under Heavy Hand Interaction
- **Arxiv ID**: http://arxiv.org/abs/1908.01523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1908.01523v1)
- **Published**: 2019-08-05 09:00:54+00:00
- **Updated**: 2019-08-05 09:00:54+00:00
- **Authors**: Raoul de Charette, Sotiris Manitsaris
- **Comment**: 7 pages, 10 figures. Submitted to journal
- **Journal**: None
- **Summary**: We reconstruct 3D deformable object through time, in the context of a live pottery making process where the crafter molds the object. Because the object suffers from heavy hand interaction, and is being deformed, classical techniques cannot be applied. We use particle energy optimization to estimate the object profile and benefit of the object radial symmetry to increase the robustness of the reconstruction to both occlusion and noise. Our method works with an unconstrained scalable setup with one or more depth sensors. We evaluate on our database (released upon publication) on a per-frame and temporal basis and shows it significantly outperforms state-of-the-art achieving 7.60mm average object reconstruction error. Further ablation studies demonstrate the effectiveness of our method.



### Discriminating Spatial and Temporal Relevance in Deep Taylor Decompositions for Explainable Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.01536v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01536v2)
- **Published**: 2019-08-05 09:42:25+00:00
- **Updated**: 2019-08-14 14:36:13+00:00
- **Authors**: Liam Hiley, Alun Preece, Yulia Hicks, David Marshall, Harrison Taylor
- **Comment**: 5 pages, 2 figures, published at IJCAI19 ExAI workshop
- **Journal**: None
- **Summary**: Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work n deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a na\"ive representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.



### Precise Estimation of Renal Vascular Dominant Regions Using Spatially Aware Fully Convolutional Networks, Tensor-Cut and Voronoi Diagrams
- **Arxiv ID**: http://arxiv.org/abs/1908.01543v1
- **DOI**: 10.1016/j.compmedimag.2019.101642
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01543v1)
- **Published**: 2019-08-05 10:11:41+00:00
- **Updated**: 2019-08-05 10:11:41+00:00
- **Authors**: Chenglong Wang, Holger R. Roth, Takayuki Kitasaka, Masahiro Oda, Yuichiro Hayashi, Yasushi Yoshino, Tokunori Yamamoto, Naoto Sassa, Momokazu Goto, Kensaku Mori
- **Comment**: None
- **Journal**: Computerized Medical Imaging and Graphics 77 (2019): 101642
- **Summary**: This paper presents a new approach for precisely estimating the renal vascular dominant region using a Voronoi diagram. To provide computer-assisted diagnostics for the pre-surgical simulation of partial nephrectomy surgery, we must obtain information on the renal arteries and the renal vascular dominant regions. We propose a fully automatic segmentation method that combines a neural network and tensor-based graph-cut methods to precisely extract the kidney and renal arteries. First, we use a convolutional neural network to localize the kidney regions and extract tiny renal arteries with a tensor-based graph-cut method. Then we generate a Voronoi diagram to estimate the renal vascular dominant regions based on the segmented kidney and renal arteries. The accuracy of kidney segmentation in 27 cases with 8-fold cross validation reached a Dice score of 95%. The accuracy of renal artery segmentation in 8 cases obtained a centerline overlap ratio of 80%. Each partition region corresponds to a renal vascular dominant region. The final dominant-region estimation accuracy achieved a Dice coefficient of 80%. A clinical application showed the potential of our proposed estimation approach in a real clinical surgical environment. Further validation using large-scale database is our future work.



### Revisiting Feature Alignment for One-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.01570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01570v1)
- **Published**: 2019-08-05 12:00:48+00:00
- **Updated**: 2019-08-05 12:00:48+00:00
- **Authors**: Yuntao Chen, Chenxia Han, Naiyan Wang, Zhaoxiang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, one-stage object detectors gain much attention due to their simplicity in practice. Its fully convolutional nature greatly reduces the difficulty of training and deployment compared with two-stage detectors which require NMS and sorting for the proposal stage. However, a fundamental issue lies in all one-stage detectors is the misalignment between anchor boxes and convolutional features, which significantly hinders the performance of one-stage detectors. In this work, we first reveal the deep connection between the widely used im2col operator and the RoIAlign operator. Guided by this illuminating observation, we propose a RoIConv operator which aligns the features and its corresponding anchors in one-stage detection in a principled way. We then design a fully convolutional AlignDet architecture which combines the flexibility of learned anchors and the preciseness of aligned features. Specifically, our AlignDet achieves a state-of-the-art mAP of 44.1 on the COCO test-dev with ResNeXt-101 backbone.



### Knowledge Consistency between Neural Networks and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1908.01581v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01581v2)
- **Published**: 2019-08-05 12:25:37+00:00
- **Updated**: 2020-01-14 17:30:39+00:00
- **Authors**: Ruofan Liang, Tianlin Li, Longfei Li, Jing Wang, Quanshi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic definition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to refine pre-trained networks and boost performance.



### Knee menisci segmentation and relaxometry of 3D ultrashort echo time (UTE) cones MR imaging using attention U-Net with transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01594v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1908.01594v1)
- **Published**: 2019-08-05 12:56:11+00:00
- **Updated**: 2019-08-05 12:56:11+00:00
- **Authors**: Michal Byra, Mei Wu, Xiaodong Zhang, Hyungseok Jang, Ya-Jun Ma, Eric Y Chang, Sameer Shah, Jiang Du
- **Comment**: 30 pages, 7 figures
- **Journal**: None
- **Summary**: The purpose of this work is to develop a deep learning-based method for knee menisci segmentation in 3D ultrashort echo time (UTE) cones magnetic resonance (MR) imaging, and to automatically determine MR relaxation times, namely the T1, T1$\rho$, and T2* parameters, which can be used to assess knee osteoarthritis (OA). Whole knee joint imaging was performed using 3D UTE cones sequences to collect data from 61 human subjects. Regions of interest (ROIs) were outlined by two experienced radiologists based on subtracted T1$\rho$-weighted MR images. Transfer learning was applied to develop 2D attention U-Net convolutional neural networks for the menisci segmentation based on each radiologist's ROIs separately. Dice scores were calculated to assess segmentation performance. Next, the T1, T1$\rho$, T2* relaxations, and ROI areas were determined for the manual and automatic segmentations, then compared.The models developed using ROIs provided by two radiologists achieved high Dice scores of 0.860 and 0.833, while the radiologists' manual segmentations achieved a Dice score of 0.820. Linear correlation coefficients for the T1, T1$\rho$, and T2* relaxations calculated using the automatic and manual segmentations ranged between 0.90 and 0.97, and there were no associated differences between the estimated average meniscal relaxation parameters. The deep learning models achieved segmentation performance equivalent to the inter-observer variability of two radiologists. The proposed deep learning-based approach can be used to efficiently generate automatic segmentations and determine meniscal relaxations times. The method has the potential to help radiologists with the assessment of meniscal diseases, such as OA.



### Model Decay in Long-Term Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.01603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01603v1)
- **Published**: 2019-08-05 13:16:29+00:00
- **Updated**: 2019-08-05 13:16:29+00:00
- **Authors**: Efstratios Gavves, Ran Tao, Deepak K. Gupta, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: Updating the tracker model with adverse bounding box predictions adds an unavoidable bias term to the learning. This bias term, which we refer to as model decay, offsets the learning and causes tracking drift. While its adverse affect might not be visible in short-term tracking, accumulation of this bias over a long-term can eventually lead to a permanent loss of the target. In this paper, we look at the problem of model bias from a mathematical perspective. Further, we briefly examine the effect of various sources of tracking error on model decay, using a correlation filter (ECO) and a Siamese (SINT) tracker. Based on observations and insights, we propose simple additions that help to reduce model decay in long-term tracking. The proposed tracker is evaluated on four long-term and one short term tracking benchmarks, demonstrating superior accuracy and robustness, even in 30 minute long videos.



### One-shot Face Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1908.03251v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03251v1)
- **Published**: 2019-08-05 13:46:59+00:00
- **Updated**: 2019-08-05 13:46:59+00:00
- **Authors**: Yunxuan Zhang, Siwei Zhang, Yue He, Cheng Li, Chen Change Loy, Ziwei Liu
- **Comment**: To appear in BMVC 2019 as a spotlight presentation. Code and models
  are available at: https://github.com/bj80heyue/One_Shot_Face_Reenactment
- **Journal**: None
- **Summary**: To enable realistic shape (e.g. pose and expression) transfer, existing face reenactment methods rely on a set of target faces for learning subject-specific traits. However, in real-world scenario end-users often only have one target face at hand, rendering existing methods inapplicable. In this work, we bridge this gap by proposing a novel one-shot face reenactment learning framework. Our key insight is that the one-shot learner should be able to disentangle and compose appearance and shape information for effective modeling. Specifically, the target face appearance and the source face shape are first projected into latent spaces with their corresponding encoders. Then these two latent spaces are associated by learning a shared decoder that aggregates multi-level features to produce the final reenactment results. To further improve the synthesizing quality on mustache and hair regions, we additionally propose FusionNet which combines the strengths of our learned decoder and the traditional warping method. Extensive experiments show that our one-shot face reenactment system achieves superior transfer fidelity as well as identity preserving capability than alternatives. More remarkably, our approach trained with only one target image per subject achieves competitive results to those using a set of target images, demonstrating the practical merit of this work. Code, models and an additional set of reenacted faces have been publicly released at the project page.



### Review of Algorithms for Compressive Sensing of Images
- **Arxiv ID**: http://arxiv.org/abs/1908.01642v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01642v1)
- **Published**: 2019-08-05 14:24:57+00:00
- **Updated**: 2019-08-05 14:24:57+00:00
- **Authors**: Yoni Sher
- **Comment**: 14 pages, 8 figures, all data available in appendix
- **Journal**: None
- **Summary**: We provide a comprehensive review of classical algorithms for compressive sensing of images, focused on Total variation methods, with a view to application in LiDAR systems. Our primary focus is providing a full review for beginners in the field, as well as simulating the kind of noise found in real LiDAR systems. To this end, we provide an overview of the theoretical background, a brief discussion of various considerations that come in to play in compressive sensing, and a standardized comparison of off-the-shelf methods, intended as a quick-start guide to choosing algorithms for compressive sensing applications.



### A principled approach for generating adversarial images under non-smooth dissimilarity metrics
- **Arxiv ID**: http://arxiv.org/abs/1908.01667v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01667v2)
- **Published**: 2019-08-05 14:57:01+00:00
- **Updated**: 2019-10-08 17:21:21+00:00
- **Authors**: Aram-Alexandre Pooladian, Chris Finlay, Tim Hoheisel, Adam Oberman
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks perform well on real world data but are prone to adversarial perturbations: small changes in the input easily lead to misclassification. In this work, we propose an attack methodology not only for cases where the perturbations are measured by $\ell_p$ norms, but in fact any adversarial dissimilarity metric with a closed proximal form. This includes, but is not limited to, $\ell_1, \ell_2$, and $\ell_\infty$ perturbations; the $\ell_0$ counting "norm" (i.e. true sparseness); and the total variation seminorm, which is a (non-$\ell_p$) convolutional dissimilarity measuring local pixel changes. Our approach is a natural extension of a recent adversarial attack method, and eliminates the differentiability requirement of the metric. We demonstrate our algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and ImageNet-1k datasets. We consider undefended and defended models, and show that our algorithm easily transfers to various datasets. We observe that ProxLogBarrier outperforms a host of modern adversarial attacks specialized for the $\ell_0$ case. Moreover, by altering images in the total variation seminorm, we shed light on a new class of perturbations that exploit neighboring pixel information.



### Spatially and Temporally Efficient Non-local Attention Network for Video-based Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.01683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01683v1)
- **Published**: 2019-08-05 15:13:06+00:00
- **Updated**: 2019-08-05 15:13:06+00:00
- **Authors**: Chih-Ting Liu, Chih-Wei Wu, Yu-Chiang Frank Wang, Shao-Yi Chien
- **Comment**: This paper was accepted by 2019 British Machine Vision Conference
  (BMVC)
- **Journal**: BMVC2019
- **Summary**: Video-based person re-identification (Re-ID) aims at matching video sequences of pedestrians across non-overlapping cameras. It is a practical yet challenging task of how to embed spatial and temporal information of a video into its feature representation. While most existing methods learn the video characteristics by aggregating image-wise features and designing attention mechanisms in Neural Networks, they only explore the correlation between frames at high-level features. In this work, we target at refining the intermediate features as well as high-level features with non-local attention operations and make two contributions. (i) We propose a Non-local Video Attention Network (NVAN) to incorporate video characteristics into the representation at multiple feature levels. (ii) We further introduce a Spatially and Temporally Efficient Non-local Video Attention Network (STE-NVAN) to reduce the computation complexity by exploring spatial and temporal redundancy presented in pedestrian videos. Extensive experiments show that our NVAN outperforms state-of-the-arts by 3.8% in rank-1 accuracy on MARS dataset and confirms our STE-NVAN displays a much superior computation footprint compared to existing methods.



### SESF-Fuse: An Unsupervised Deep Model for Multi-Focus Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1908.01703v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01703v2)
- **Published**: 2019-08-05 15:54:09+00:00
- **Updated**: 2019-08-21 17:03:46+00:00
- **Authors**: Boyuan Ma, Xiaojuan Ban, Haiyou Huang, Yu Zhu
- **Comment**: technological report and fix some errors
- **Journal**: None
- **Summary**: In this work, we propose a novel unsupervised deep learning model to address multi-focus image fusion problem. First, we train an encoder-decoder network in unsupervised manner to acquire deep feature of input images. And then we utilize these features and spatial frequency to measure activity level and decision map. Finally, we apply some consistency verification methods to adjust the decision map and draw out fused result. The key point behind of proposed method is that only the objects within the depth-of-field (DOF) have sharp appearance in the photograph while other objects are likely to be blurred. In contrast to previous works, our method analyzes sharp appearance in deep feature instead of original image. Experimental results demonstrate that the proposed method achieves the state-of-art fusion performance compared to existing 16 fusion methods in objective and subjective assessment.



### Learning a Unified Embedding for Visual Search at Pinterest
- **Arxiv ID**: http://arxiv.org/abs/1908.01707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01707v1)
- **Published**: 2019-08-05 16:08:12+00:00
- **Updated**: 2019-08-05 16:08:12+00:00
- **Authors**: Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, Charles Rosenberg
- **Comment**: in Proceedings of the 25th ACM SIGKDD International Conference on
  Knowledge and Discovery and Data Mining, 2019
- **Journal**: None
- **Summary**: At Pinterest, we utilize image embeddings throughout our search and recommendation systems to help our users navigate through visual content by powering experiences like browsing of related content and searching for exact products for shopping. In this work we describe a multi-task deep metric learning system to learn a single unified image embedding which can be used to power our multiple visual search products. The solution we present not only allows us to train for multiple application objectives in a single deep neural network architecture, but takes advantage of correlated information in the combination of all training data from each application to generate a unified embedding that outperforms all specialized embeddings previously deployed for each product. We discuss the challenges of handling images from different domains such as camera photos, high quality web images, and clean product catalog images. We also detail how to jointly train for multiple product objectives and how to leverage both engagement data and human labeled data. In addition, our trained embeddings can also be binarized for efficient storage and retrieval without compromising precision and recall. Through comprehensive evaluations on offline metrics, user studies, and online A/B experiments, we demonstrate that our proposed unified embedding improves both relevance and engagement of our visual search products for both browsing and searching purposes when compared to existing specialized embeddings. Finally, the deployment of the unified embedding at Pinterest has drastically reduced the operation and engineering cost of maintaining multiple embeddings while improving quality.



### Visual-Relation Conscious Image Generation from Structured-Text
- **Arxiv ID**: http://arxiv.org/abs/1908.01741v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01741v3)
- **Published**: 2019-08-05 17:33:00+00:00
- **Updated**: 2020-07-18 05:26:53+00:00
- **Authors**: Duc Minh Vo, Akihiro Sugimoto
- **Comment**: accepted at ECCV 2020
- **Journal**: None
- **Summary**: We propose an end-to-end network for image generation from given structured-text that consists of the visual-relation layout module and the pyramid of GANs, namely stacking-GANs. Our visual-relation layout module uses relations among entities in the structured-text in two ways: comprehensive usage and individual usage. We comprehensively use all available relations together to localize initial bounding-boxes of all the entities. We also use individual relation separately to predict from the initial bounding-boxes relation-units for all the relations in the input text. We then unify all the relation-units to produce the visual-relation layout, i.e., bounding-boxes for all the entities so that each of them uniquely corresponds to each entity while keeping its involved relations. Our visual-relation layout reflects the scene structure given in the input text. The stacking-GANs is the stack of three GANs conditioned on the visual-relation layout and the output of previous GAN, consistently capturing the scene structure. Our network realistically renders entities' details in high resolution while keeping the scene structure. Experimental results on two public datasets show outperformances of our method against state-of-the-art methods.



### SqueezeNAS: Fast neural architecture search for faster semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.01748v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.01748v2)
- **Published**: 2019-08-05 17:46:36+00:00
- **Updated**: 2019-08-08 06:46:25+00:00
- **Authors**: Albert Shaw, Daniel Hunter, Forrest Iandola, Sammy Sidhu
- **Comment**: 11 pages, 10 figures, 3 tables, 3 pages of appendix; Added found
  networks to Appendix tables
- **Journal**: None
- **Summary**: For real time applications utilizing Deep Neural Networks (DNNs), it is critical that the models achieve high-accuracy on the target task and low-latency inference on the target computing platform. While Neural Architecture Search (NAS) has been effectively used to develop low-latency networks for image classification, there has been relatively little effort to use NAS to optimize DNN architectures for other vision tasks. In this work, we present what we believe to be the first proxyless hardware-aware search targeted for dense semantic segmentation. With this approach, we advance the state-of-the-art accuracy for latency-optimized networks on the Cityscapes semantic segmentation dataset. Our latency-optimized small SqueezeNAS network achieves 68.02% validation class mIOU with less than 35 ms inference times on the NVIDIA AGX Xavier. Our latency-optimized large SqueezeNAS network achieves 73.62% class mIOU with less than 100 ms inference times. We demonstrate that significant performance gains are possible by utilizing NAS to find networks optimized for both the specific task and inference hardware. We also present detailed analysis comparing our networks to recent state-of-the-art architectures.



### Attribute-Guided Coupled GAN for Cross-Resolution Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.01790v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01790v1)
- **Published**: 2019-08-05 18:10:55+00:00
- **Updated**: 2019-08-05 18:10:55+00:00
- **Authors**: Veeru Talreja, Fariborz Taherkhani, Matthew C Valenti, Nasser M Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel attribute-guided cross-resolution (low-resolution to high-resolution) face recognition framework that leverages a coupled generative adversarial network (GAN) structure with adversarial training to find the hidden relationship between the low-resolution and high-resolution images in a latent common embedding subspace. The coupled GAN framework consists of two sub-networks, one dedicated to the low-resolution domain and the other dedicated to the high-resolution domain. Each sub-network aims to find a projection that maximizes the pair-wise correlation between the two feature domains in a common embedding subspace. In addition to projecting the images into a common subspace, the coupled network also predicts facial attributes to improve the cross-resolution face recognition. Specifically, our proposed coupled framework exploits facial attributes to further maximize the pair-wise correlation by implicitly matching facial attributes of the low and high-resolution images during the training, which leads to a more discriminative embedding subspace resulting in performance enhancement for cross-resolution face recognition. The efficacy of our approach compared with the state-of-the-art is demonstrated using the LFWA, Celeb-A, SCFace and UCCS datasets.



### Hybrid Camera Pose Estimation with Online Partitioning for SLAM
- **Arxiv ID**: http://arxiv.org/abs/1908.01797v2
- **DOI**: 10.1109/LRA.2020.2967688
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.01797v2)
- **Published**: 2019-08-05 18:26:03+00:00
- **Updated**: 2020-11-02 19:40:08+00:00
- **Authors**: Xinyi Li, Haibin Ling
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters ( Volume: 5, Issue: 2, April
  2020)
- **Summary**: This paper presents a hybrid real-time camera pose estimation framework with a novel partitioning scheme and introduces motion averaging to monocular Simultaneous Localization and Mapping (SLAM) systems. Breaking through the limitations of fixed-size temporal partitioning in many conventional SLAM pipelines, our approach significantly improves the accuracy of local bundle adjustment by gathering spatially-strongly-connected cameras into each block. With the dynamic initialization using intermediate computation values, \XL{we improve the Levenberg-Marquardt solver to further enhance the efficiency of the local optimization.} Moreover, the dense data association between blocks by our co-visibility-based partitioning enables us to explore and implement motion averaging to efficiently align the blocks globally, updating camera motion estimations on-the-fly. Experiments on benchmarks convincingly demonstrate the practicality and robustness of our proposed approach by significantly outperforming conventional approaches.



### Answering Questions about Data Visualizations using Efficient Bimodal Fusion
- **Arxiv ID**: http://arxiv.org/abs/1908.01801v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.01801v2)
- **Published**: 2019-08-05 18:47:30+00:00
- **Updated**: 2020-07-22 15:10:29+00:00
- **Authors**: Kushal Kafle, Robik Shrestha, Brian Price, Scott Cohen, Christopher Kanan
- **Comment**: Presented at WACV, 2020
- **Journal**: None
- **Summary**: Chart question answering (CQA) is a newly proposed visual question answering (VQA) task where an algorithm must answer questions about data visualizations, e.g. bar charts, pie charts, and line graphs. CQA requires capabilities that natural-image VQA algorithms lack: fine-grained measurements, optical character recognition, and handling out-of-vocabulary words in both questions and answers. Without modifications, state-of-the-art VQA algorithms perform poorly on this task. Here, we propose a novel CQA algorithm called parallel recurrent fusion of image and language (PReFIL). PReFIL first learns bimodal embeddings by fusing question and image features and then intelligently aggregates these learned embeddings to answer the given question. Despite its simplicity, PReFIL greatly surpasses state-of-the art systems and human baselines on both the FigureQA and DVQA datasets. Additionally, we demonstrate that PReFIL can be used to reconstruct tables by asking a series of questions about a chart.



### Zero-Shot Deep Hashing and Neural Network Based Error Correction for Face Template Protection
- **Arxiv ID**: http://arxiv.org/abs/1908.02706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02706v1)
- **Published**: 2019-08-05 19:39:25+00:00
- **Updated**: 2019-08-05 19:39:25+00:00
- **Authors**: Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: arXiv admin note: text overlap with arXiv:1902.04149
- **Journal**: None
- **Summary**: In this paper, we present a novel architecture that integrates a deep hashing framework with a neural network decoder (NND) for application to face template protection. It improves upon existing face template protection techniques to provide better matching performance with one-shot and multi-shot enrollment. A key novelty of our proposed architecture is that the framework can also be used with zero-shot enrollment. This implies that our architecture does not need to be re-trained even if a new subject is to be enrolled into the system. The proposed architecture consists of two major components: a deep hashing (DH) component, which is used for robust mapping of face images to their corresponding intermediate binary codes, and a NND component, which corrects errors in the intermediate binary codes that are caused by differences in the enrollment and probe biometrics due to factors such as variation in pose, illumination, and other factors. The final binary code generated by the NND is then cryptographically hashed and stored as a secure face template in the database. The efficacy of our approach with zero-shot, one-shot, and multi-shot enrollments is shown for CMU-PIE, Extended Yale B, WVU multimodal and Multi-PIE face databases. With zero-shot enrollment, the system achieves approximately 85% genuine accept rates (GAR) at 0.01% false accept rate (FAR), and with one-shot and multi-shot enrollments, it achieves approximately 99.95% GAR at 0.01% FAR, while providing a high level of template security.



### Semi-Automatic Labeling for Deep Learning in Robotics
- **Arxiv ID**: http://arxiv.org/abs/1908.01862v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01862v1)
- **Published**: 2019-08-05 21:10:12+00:00
- **Updated**: 2019-08-05 21:10:12+00:00
- **Authors**: Daniele De Gregorio, Alessio Tonioni, Gianluca Palli, Luigi Di Stefano
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a semi-automatic method which leverages on moving a 2D camera by means of a robot, proving precise camera tracking, and an augmented reality pen to define initial object bounding box, to create large labeled datasets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the Deep Learning technique applied to computer vision, that typically requires very large datasets, truly automated and reliable. With the ARS pipeline, we created effortlessly two novel datasets, one on electromechanical components (industrial scenario) and one on fruits (daily-living scenario), and trained robustly two state-of-the-art object detectors, based on convolutional neural networks, such as YOLO and SSD. With respect to the conventional manual annotation of 1000 frames that takes us slightly more than 10 hours, the proposed approach based on ARS allows annotating 9 sequences of about 35000 frames in less than one hour, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15\% with respect to manual labeling. All our software is available as a ROS package in a public repository alongside the novel annotated datasets.



### Unsupervised Representations of Pollen in Bright-Field Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1908.01866v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1908.01866v1)
- **Published**: 2019-08-05 21:29:51+00:00
- **Updated**: 2019-08-05 21:29:51+00:00
- **Authors**: Chloe He, Gerard Glowacki, Alexis Gkantiragas
- **Comment**: Accepted at the Workshop on Computational Biology at the
  International Conference on Machine Learning (ICML) in Long Beach, CA, USA on
  June 14, 2019
- **Journal**: None
- **Summary**: We present the first unsupervised deep learning method for pollen analysis using bright-field microscopy. Using a modest dataset of 650 images of pollen grains collected from honey, we achieve family level identification of pollen. We embed images of pollen grains into a low-dimensional latent space and compare Euclidean and Riemannian metrics on these spaces for clustering. We propose this system for automated analysis of pollen and other microscopic biological structures which have only small or unlabelled datasets available.



### Attention Control with Metric Learning Alignment for Image Set-based Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.01872v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01872v1)
- **Published**: 2019-08-05 21:48:05+00:00
- **Updated**: 2019-08-05 21:48:05+00:00
- **Authors**: Xiaofeng Liu, Zhenhua Guo, Jane You, B. V. K Vijaya Kumar
- **Comment**: Accepted to IEEE T-IFS (Extension of ECCV 2018 paper:
  Dependency-aware Attention Control for Unconstrained Face Recognition with
  Image Sets). arXiv admin note: substantial text overlap with
  arXiv:1907.03030; text overlap with arXiv:1707.00130 by other authors
- **Journal**: None
- **Summary**: This paper considers the problem of image set-based face verification and identification. Unlike traditional single sample (an image or a video) setting, this situation assumes the availability of a set of heterogeneous collection of orderless images and videos. The samples can be taken at different check points, different identity documents $etc$. The importance of each image is usually considered either equal or based on a quality assessment of that image independent of other images and/or videos in that image set. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in a latent space. Specifically, we first propose a dependency-aware attention control (DAC) network, which uses actor-critic reinforcement learning for attention decision of each image to exploit the correlations among the unordered images. An off-policy experience replay is introduced to speed up the learning process. Moreover, the DAC is combined with a temporal model for videos using divide and conquer strategies. We also introduce a pose-guided representation (PGR) scheme that can further boost the performance at extreme poses. We propose a parameter-free PGR without the need for training as well as a novel metric learning-based PGR for pose alignment without the need for pose detection in testing stage. Extensive evaluations on IJB-A/B/C, YTF, Celebrity-1000 datasets demonstrate that our method outperforms many state-of-art approaches on the set-based as well as video-based face recognition databases.



