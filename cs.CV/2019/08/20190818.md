# Arxiv Papers in cs.CV on 2019-08-18
### A Fast and Accurate One-Stage Approach to Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1908.06354v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06354v1)
- **Published**: 2019-08-18 00:25:02+00:00
- **Updated**: 2019-08-18 00:25:02+00:00
- **Authors**: Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo
- **Comment**: ICCV 2019 Oral
- **Journal**: None
- **Summary**: We propose a simple, fast, and accurate one-stage approach to visual grounding, inspired by the following insight. The performances of existing propose-and-rank two-stage methods are capped by the quality of the region candidates they propose in the first stage --- if none of the candidates could cover the ground truth region, there is no hope in the second stage to rank the right region to the top. To avoid this caveat, we propose a one-stage model that enables end-to-end joint optimization. The main idea is as straightforward as fusing a text query's embedding into the YOLOv3 object detector, augmented by spatial features so as to account for spatial mentions in the query. Despite being simple, this one-stage approach shows great potential in terms of both accuracy and speed for both phrase localization and referring expression comprehension, according to our experiments. Given these results along with careful investigations into some popular region proposals, we advocate for visual grounding a paradigm shift from the conventional two-stage methods to the one-stage framework.



### A Delay Metric for Video Object Detection: What Average Precision Fails to Tell
- **Arxiv ID**: http://arxiv.org/abs/1908.06368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06368v2)
- **Published**: 2019-08-18 03:36:23+00:00
- **Updated**: 2019-11-06 22:50:02+00:00
- **Authors**: Huizi Mao, Xiaodong Yang, William J. Dally
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze object detection from videos and point out that AP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, average delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve AP well. In other words, AP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be additionally evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception.



### Asynchronous Single-Photon 3D Imaging
- **Arxiv ID**: http://arxiv.org/abs/1908.06372v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06372v1)
- **Published**: 2019-08-18 04:03:36+00:00
- **Updated**: 2019-08-18 04:03:36+00:00
- **Authors**: Anant Gupta, Atul Ingle, Mohit Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight depth-ranging due to their unique ability to capture individual photons with picosecond timing resolution. However, ambient light (e.g., sunlight) incident on a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in the measured waveform, resulting in large depth errors. We propose asynchronous single-photon 3D imaging, a family of acquisition schemes to mitigate pileup during data acquisition itself. Asynchronous acquisition temporally misaligns SPAD measurement windows and the laser cycles through deterministically predefined or randomized offsets. Our key insight is that pileup distortions can be "averaged out" by choosing a sequence of offsets that span the entire depth range. We develop a generalized image formation model and perform theoretical analysis to explore the space of asynchronous acquisition schemes and design high-performance schemes. Our simulations and experiments demonstrate an improvement in depth accuracy of up to an order of magnitude as compared to the state-of-the-art, across a wide range of imaging scenarios, including those with high ambient flux.



### Distill Knowledge from NRSfM for Weakly Supervised 3D Pose Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.06377v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06377v1)
- **Published**: 2019-08-18 04:48:49+00:00
- **Updated**: 2019-08-18 04:48:49+00:00
- **Authors**: Chaoyang Wang, Chen Kong, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision.



### Multivariate Spatial Data Visualization: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1908.11344v1
- **DOI**: 10.1007/s12650-019-00584-3
- **Categories**: **cs.HC**, cs.CV, 68U99, 76M27, J.2; I.6.0
- **Links**: [PDF](http://arxiv.org/pdf/1908.11344v1)
- **Published**: 2019-08-18 06:07:17+00:00
- **Updated**: 2019-08-18 06:07:17+00:00
- **Authors**: Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin
- **Comment**: 16 pages, 5 figures. Corresponding author: Yubo Tao
- **Journal**: Journal of Visualization, (2019), 1-16
- **Summary**: Multivariate spatial data plays an important role in computational science and engineering simulations. The potential features and hidden relationships in multivariate data can assist scientists to gain an in-depth understanding of a scientific process, verify a hypothesis and further discover a new physical or chemical law. In this paper, we present a comprehensive survey of the state-of-the-art techniques for multivariate spatial data visualization. We first introduce the basic concept and characteristics of multivariate spatial data, and describe three main tasks in multivariate data visualization: feature classification, fusion visualization, and correlation analysis. Finally, we prospect potential research topics for multivariate data visualization according to the current research.



### Long-Duration Fully Autonomous Operation of Rotorcraft Unmanned Aerial Systems for Remote-Sensing Data Acquisition
- **Arxiv ID**: http://arxiv.org/abs/1908.06381v1
- **DOI**: 10.1002/rob.21898
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1908.06381v1)
- **Published**: 2019-08-18 06:33:52+00:00
- **Updated**: 2019-08-18 06:33:52+00:00
- **Authors**: Danylo Malyuta, Christian Brommer, Daniel Hentzen, Thomas Stastny, Roland Siegwart, Roland Brockers
- **Comment**: 38 pages, 28 figures
- **Journal**: J Field Robotics (2019) 1-21
- **Summary**: Recent applications of unmanned aerial systems (UAS) to precision agriculture have shown increased ease and efficiency in data collection at precise remote locations. However, further enhancement of the field requires operation over long periods of time, e.g. days or weeks. This has so far been impractical due to the limited flight times of such platforms and the requirement of humans in the loop for operation. To overcome these limitations, we propose a fully autonomous rotorcraft UAS that is capable of performing repeated flights for long-term observation missions without any human intervention. We address two key technologies that are critical for such a system: full platform autonomy to enable mission execution independently from human operators and the ability of vision-based precision landing on a recharging station for automated energy replenishment. High-level autonomous decision making is implemented as a hierarchy of master and slave state machines. Vision-based precision landing is enabled by estimating the landing pad's pose using a bundle of AprilTag fiducials configured for detection from a wide range of altitudes. We provide an extensive evaluation of the landing pad pose estimation accuracy as a function of the bundle's geometry. The functionality of the complete system is demonstrated through two indoor experiments with a duration of 11 and 10.6 hours, and one outdoor experiment with a duration of 4 hours. The UAS executed 16, 48 and 22 flights respectively during these experiments. In the outdoor experiment, the ratio between flying to collect data and charging was 1 to 10, which is similar to past work in this domain. All flights were fully autonomous with no human in the loop. To our best knowledge this is the first research publication about the long-term outdoor operation of a quadrotor system with no human interaction.



### RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1908.06382v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06382v2)
- **Published**: 2019-08-18 06:38:55+00:00
- **Updated**: 2019-08-26 10:14:40+00:00
- **Authors**: Wenlong Zhang, Yihao Liu, Chao Dong, Yu Qiao
- **Comment**: ICCV 2019 (Oral) camera-ready + supplementary; Project page:
  https://wenlongzhang0724.github.io/Projects/RankSRGAN
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN



### Geometric Disentanglement for Generative Latent Shape Models
- **Arxiv ID**: http://arxiv.org/abs/1908.06386v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, I.2.10; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/1908.06386v1)
- **Published**: 2019-08-18 07:05:39+00:00
- **Updated**: 2019-08-18 07:05:39+00:00
- **Authors**: Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jepson, Sven Dickinson
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Representing 3D shape is a fundamental problem in artificial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shape that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information. Our method makes use of tools from spectral differential geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner, including a novel one that penalizes the Jacobian of the latent representation of the decoded output with respect to the latent encoding. We show that the resulting representation exhibits intuitive and interpretable behavior, enabling tasks such as pose transfer and pose-aware shape retrieval that cannot easily be performed by models with an entangled representation.



### Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.06387v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1908.06387v1)
- **Published**: 2019-08-18 07:13:26+00:00
- **Updated**: 2019-08-18 07:13:26+00:00
- **Authors**: Måns Larsson, Erik Stenborg, Carl Toft, Lars Hammarstrand, Torsten Sattler, Fredrik Kahl
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Long-term visual localization is the problem of estimating the camera pose of a given query image in a scene whose appearance changes over time. It is an important problem in practice, for example, encountered in autonomous driving. In order to gain robustness to such changes, long-term localization approaches often use segmantic segmentations as an invariant scene representation, as the semantic meaning of each scene part should not be affected by seasonal and other changes. However, these representations are typically not very discriminative due to the limited number of available classes. In this paper, we propose a new neural network, the Fine-Grained Segmentation Network (FGSN), that can be used to provide image segmentations with a larger number of labels and can be trained in a self-supervised fashion. In addition, we show how FGSNs can be trained to output consistent labels across seasonal changes. We demonstrate through extensive experiments that integrating the fine-grained segmentations produced by our FGSNs into existing localization algorithms leads to substantial improvements in localization performance.



### PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment
- **Arxiv ID**: http://arxiv.org/abs/1908.06391v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06391v2)
- **Published**: 2019-08-18 07:56:19+00:00
- **Updated**: 2020-02-07 03:20:01+00:00
- **Authors**: Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, Jiashi Feng
- **Comment**: 10 pages, 6 figures, ICCV 2019, code available at
  https://github.com/kaixin96/PANet
- **Journal**: None
- **Summary**: Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%.



### Evaluation of an AI System for the Detection of Diabetic Retinopathy from Images Captured with a Handheld Portable Fundus Camera: the MAILOR AI study
- **Arxiv ID**: http://arxiv.org/abs/1908.06399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06399v1)
- **Published**: 2019-08-18 08:43:59+00:00
- **Updated**: 2019-08-18 08:43:59+00:00
- **Authors**: T W Rogers, J Gonzalez-Bueno, R Garcia Franco, E Lopez Star, D Méndez Marín, J Vassallo, V C Lansingh, S Trikha, N Jaccard
- **Comment**: None
- **Journal**: None
- **Summary**: Objectives: To evaluate the performance of an Artificial Intelligence (AI) system (Pegasus, Visulytix Ltd., UK), at the detection of Diabetic Retinopathy (DR) from images captured by a handheld portable fundus camera.   Methods: A cohort of 6,404 patients (~80% with diabetes mellitus) was screened for retinal diseases using a handheld portable fundus camera (Pictor Plus, Volk Optical Inc., USA) at the Mexican Advanced Imaging Laboratory for Ocular Research. The images were graded for DR by specialists according to the Scottish DR grading scheme. The performance of the AI system was evaluated, retrospectively, in assessing Referable DR (RDR) and Proliferative DR (PDR) and compared to the performance on a publicly available desktop camera benchmark dataset.   Results: For RDR detection, Pegasus performed with an 89.4% (95% CI: 88.0-90.7) Area Under the Receiver Operating Characteristic (AUROC) curve for the MAILOR cohort, compared to an AUROC of 98.5% (95% CI: 97.8-99.2) on the benchmark dataset. This difference was statistically significant. Moreover, no statistically significant difference was found in performance for PDR detection with Pegasus achieving an AUROC of 94.3% (95% CI: 91.0-96.9) on the MAILOR cohort and 92.2% (95% CI: 89.4-94.8) on the benchmark dataset.   Conclusions: Pegasus showed good transferability for the detection of PDR from a curated desktop fundus camera dataset to real-world clinical practice with a handheld portable fundus camera. However, there was a substantial, and statistically significant, decrease in the diagnostic performance for RDR when using the handheld device.



### On the Robustness of Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.06401v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06401v2)
- **Published**: 2019-08-18 09:04:26+00:00
- **Updated**: 2021-06-10 05:27:07+00:00
- **Authors**: Sahil Shah, Naman Jain, Abhishek Sharma, Arjun Jain
- **Comment**: Accepted in CVPR-W
- **Journal**: None
- **Summary**: This paper provides a comprehensive and exhaustive study of adversarial attacks on human pose estimation models and the evaluation of their robustness. Besides highlighting the important differences between well-studied classification and human pose-estimation systems w.r.t. adversarial attacks, we also provide deep insights into the design choices of pose-estimation systems to shape future work. We benchmark the robustness of several 2D single person pose-estimation architectures trained on multiple datasets, MPII and COCO. In doing so, we also explore the problem of attacking non-classification networks including regression based networks, which has been virtually unexplored in the past.   \par We find that compared to classification and semantic segmentation, human pose estimation architectures are relatively robust to adversarial attacks with the single-step attacks being surprisingly ineffective. Our study shows that the heatmap-based pose-estimation models are notably robust than their direct regression-based systems and that the systems which explicitly model anthropomorphic semantics of human body fare better than their other counterparts. Besides, targeted attacks are more difficult to obtain than un-targeted ones and some body-joints are easier to fool than the others. We present visualizations of universal perturbations to facilitate unprecedented insights into their workings on pose-estimation. Additionally, we show them to generalize well across different networks. Finally we perform a user study about perceptibility of these examples.



### Word and character segmentation directly in run-length compressed handwritten document images
- **Arxiv ID**: http://arxiv.org/abs/1909.05146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05146v1)
- **Published**: 2019-08-18 09:48:52+00:00
- **Updated**: 2019-08-18 09:48:52+00:00
- **Authors**: Amarnath R, P. Nagabhushan, Mohammed Javed
- **Comment**: 17 pages,19 figures
- **Journal**: None
- **Summary**: From the literature, it is demonstrated that performing text-line segmentation directly in the run-length compressed handwritten document images significantly reduces the computational time and memory space. In this paper, we investigate the issues of word and character segmentation directly on the run-length compressed document images. Primarily, the spreads of the characters are intelligently extracted from the foreground runs of the compressed data and subsequently connected components are established. The spacing between the connected components would be larger between the adjacent words when compared to that of intra-words. With this knowledge, a threshold is empirically chosen for inter-word separation. Every connected component within a word is further analysed for character segmentation. Here, min-cut graph concept is used for separating the touching characters. Over-segmentation and under-segmentation issues are addressed by insertion and deletion operations respectively. The approach has been developed particularly for compressed handwritten English document images. However, the model has been tested on non-English document images.



### Investigating Convolutional Neural Networks using Spatial Orderness
- **Arxiv ID**: http://arxiv.org/abs/1908.06416v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06416v2)
- **Published**: 2019-08-18 10:05:24+00:00
- **Updated**: 2019-11-29 16:35:10+00:00
- **Authors**: Rohan Ghosh, Anupam K. Gupta, Mehul Motani
- **Comment**: Presented at BMVC 2019: Workshop on Interpretable and Explainable
  Machine Vision, Cardiff, UK
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have been pivotal to the success of many state-of-the-art classification problems, in a wide variety of domains (for e.g. vision, speech, graphs and medical imaging). A commonality within those domains is the presence of hierarchical, spatially agglomerative local-to-global interactions within the data. For two-dimensional images, such interactions may induce an a priori relationship between the pixel data and the underlying spatial ordering of the pixels. For instance in natural images, neighboring pixels are more likely contain similar values than non-neighboring pixels which are further apart. To that end, we propose a statistical metric called spatial orderness, which quantifies the extent to which the input data (2D) obeys the underlying spatial ordering at various scales. In our experiments, we mainly find that adding convolutional layers to a CNN could be counterproductive for data bereft of spatial order at higher scales. We also observe, quite counter-intuitively, that the spatial orderness of CNN feature maps show a synchronized increase during the intial stages of training, and validation performance only improves after spatial orderness of feature maps start decreasing. Lastly, we present a theoretical analysis (and empirical validation) of the spatial orderness of network weights, where we find that using smaller kernel sizes leads to kernels of greater spatial orderness and vice-versa.



### Creation of digital elevation models for river floodplains
- **Arxiv ID**: http://arxiv.org/abs/1908.09005v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, physics.geo-ph, 86-08, 76D99
- **Links**: [PDF](http://arxiv.org/pdf/1908.09005v1)
- **Published**: 2019-08-18 10:09:47+00:00
- **Updated**: 2019-08-18 10:09:47+00:00
- **Authors**: Anna Klikunova, Alexander Khoperskov
- **Comment**: 10 pages, 9 figures, V International conference Information
  Technology and Nanotechnology. Session Image Processing and Earth Remote
  Sensing (IPERS-ITNT 2019), Samara, Russia, May 21-24, 2019
- **Journal**: CEUR Workshop Proceedings, 2019, vol.2391, pp.275-284
- **Summary**: A procedure for constructing a digital elevation model (DEM) of the northern part of the Volga-Akhtuba interfluve is described. The basis of our DEM is the elevation matrix of Shuttle Radar Topography Mission (SRTM) for which we carried out the refinement and updating of spatial data using satellite imagery, GPS data, depth measurements of the River Volga and River Akhtuba stream beds. The most important source of high-altitude data for the Volga-Akhtuba floodplain (VAF) can be the results of observations of the coastlines dynamics of small reservoirs (lakes, eriks, small channels) arising in the process of spring flooding and disappearing during low-flow periods. A set of digitized coastlines at different times of flooding can significantly improve the quality of the DEM. The method of constructing a digital elevation model includes an iterative procedure that uses the results of morphostructural analysis of the DEM and the numerical hydrodynamic simulations of the VAF flooding based on the shallow water model.



### Scene Classification in Indoor Environments for Robots using Context Based Word Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1908.06422v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06422v1)
- **Published**: 2019-08-18 11:17:19+00:00
- **Updated**: 2019-08-18 11:17:19+00:00
- **Authors**: Bao Xin Chen, Raghavender Sahdev, Dekun Wu, Xing Zhao, Manos Papagelis, John K. Tsotsos
- **Comment**: None
- **Journal**: 2018 IEEE International Conference of Robotics and Automation
  (ICRA) Workshop
- **Summary**: Scene Classification has been addressed with numerous techniques in computer vision literature. However, with the increasing number of scene classes in datasets in the field, it has become difficult to achieve high accuracy in the context of robotics. In this paper, we implement an approach which combines traditional deep learning techniques with natural language processing methods to generate a word embedding based Scene Classification algorithm. We use the key idea that context (objects in the scene) of an image should be representative of the scene label meaning a group of objects could assist to predict the scene class. Objects present in the scene are represented by vectors and the images are re-classified based on the objects present in the scene to refine the initial classification by a Convolutional Neural Network (CNN). In our approach we address indoor Scene Classification task using a model trained with a reduced pre-processed version of the Places365 dataset and an empirical analysis is done on a real-world dataset that we built by capturing image sequences using a GoPro camera. We also report results obtained on a subset of the Places365 dataset using our approach and additionally show a deployment of our approach on a robot operating in a real-world environment.



### Unsupervised Learning of Landmarks by Descriptor Vector Exchange
- **Arxiv ID**: http://arxiv.org/abs/1908.06427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06427v1)
- **Published**: 2019-08-18 11:35:45+00:00
- **Updated**: 2019-08-18 11:35:45+00:00
- **Authors**: James Thewlis, Samuel Albanie, Hakan Bilen, Andrea Vedaldi
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities. In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advantages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at www.robots.ox.ac.uk/~vgg/research/DVE/.



### Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation
- **Arxiv ID**: http://arxiv.org/abs/1908.06440v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06440v1)
- **Published**: 2019-08-18 13:08:39+00:00
- **Updated**: 2019-08-18 13:08:39+00:00
- **Authors**: Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, Jiaya Jia
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign.



### Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1908.06442v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06442v2)
- **Published**: 2019-08-18 13:48:11+00:00
- **Updated**: 2019-08-20 02:42:18+00:00
- **Authors**: Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, Chen Change Loy
- **Comment**: To appear in ICCV 2019. Code and models are available at
  https://penincillin.github.io/dct_iccv2019
- **Journal**: None
- **Summary**: Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.



### Image Formation Model Guided Deep Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1908.06444v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06444v3)
- **Published**: 2019-08-18 13:55:17+00:00
- **Updated**: 2020-03-29 12:46:42+00:00
- **Authors**: Jinshan Pan, Yang Liu, Deqing Sun, Jimmy Ren, Ming-Ming Cheng, Jian Yang, Jinhui Tang
- **Comment**: AAAI 2020. The training code and models are available at
  https://github.com/jspan/PHYSICS SR
- **Journal**: None
- **Summary**: We present a simple and effective image super-resolution algorithm that imposes an image formation constraint on the deep neural networks via pixel substitution. The proposed algorithm first uses a deep neural network to estimate intermediate high-resolution images, blurs the intermediate images using known blur kernels, and then substitutes values of the pixels at the un-decimated positions with those of the corresponding pixels from the low-resolution images. The output of the pixel substitution process strictly satisfies the image formation model and is further refined by the same deep neural network in a cascaded manner. The proposed framework is trained in an end-to-end fashion and can work with existing feed-forward deep neural networks for super-resolution and converges fast in practice. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods.



### Convolutional Neural Network with Median Layers for Denoising Salt-and-Pepper Contaminations
- **Arxiv ID**: http://arxiv.org/abs/1908.06452v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06452v1)
- **Published**: 2019-08-18 14:56:14+00:00
- **Updated**: 2019-08-18 14:56:14+00:00
- **Authors**: Luming Liang, Sen Deng, Lionel Gueguen, Mingqiang Wei, Xinming Wu, Jing Qin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a deep fully convolutional neural network with a new type of layer, named median layer, to restore images contaminated by the salt-and-pepper (s&p) noise. A median layer simply performs median filtering on all feature channels. By adding this kind of layer into some widely used fully convolutional deep neural networks, we develop an end-to-end network that removes the extremely high-level s&p noise without performing any non-trivial preprocessing tasks, which is different from all the existing literature in s&p noise removal. Experiments show that inserting median layers into a simple fully-convolutional network with the L2 loss significantly boosts the signal-to-noise ratio. Quantitative comparisons testify that our network outperforms the state-of-the-art methods with a limited amount of training data. The source code has been released for public evaluation and use (https://github.com/llmpass/medianDenoise).



### Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1908.06472v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06472v1)
- **Published**: 2019-08-18 16:25:14+00:00
- **Updated**: 2019-08-18 16:25:14+00:00
- **Authors**: Andreas Kamilaris, Corjan van den Brink, Savvas Karatsiolis
- **Comment**: Workshop on Deep-learning based computer vision for UAV in
  conjunction with CAIP 2019, Salerno, italy, September 2019
- **Journal**: None
- **Summary**: This paper describes preliminary work in the recent promising approach of generating synthetic training data for facilitating the learning procedure of deep learning (DL) models, with a focus on aerial photos produced by unmanned aerial vehicles (UAV). The general concept and methodology are described, and preliminary results are presented, based on a classification problem of fire identification in forests as well as a counting problem of estimating number of houses in urban areas. The proposed technique constitutes a new possibility for the DL community, especially related to UAV-based imagery analysis, with much potential, promising results, and unexplored ground for further research.



### A Software to Detect OCC Emotion, Big-Five Personality and Hofstede Cultural Dimensions of Pedestrians from Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1908.06484v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1908.06484v1)
- **Published**: 2019-08-18 17:17:03+00:00
- **Updated**: 2019-08-18 17:17:03+00:00
- **Authors**: Rodolfo Migon Favaretto, Victor Araujo, Soraia Raupp Musse, Felipe Vilanova, Angelo Brandelli Costa
- **Comment**: 13 pages, 9 figures
- **Journal**: None
- **Summary**: This paper presents a video analysis application to detect personality, emotion and cultural aspects from pedestrians in video sequences, along with a visualizer of features. The proposed model considers a series of characteristics of the pedestrians and the crowd, such as number and size of groups, distances, speeds, among others, and performs the mapping of these characteristics in personalities, emotions and cultural aspects, considering the Cultural Dimensions of Hofstede (HCD), the Big-Five Personality Model (OCEAN) and the OCC Emotional Model. The main hypothesis is that there is a relationship between so-called intrinsic human variables (such as emotion) and the way people behave in space and time. The software was tested in a set of videos from different countries and results seem promising in order to identify these three different levels of psychological traits in the filmed sequences. In addition, the data of the people present in the videos can be seen in a crowd viewer.



### Weakly Supervised Segmentation by A Deep Geodesic Prior
- **Arxiv ID**: http://arxiv.org/abs/1908.06498v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06498v1)
- **Published**: 2019-08-18 18:43:44+00:00
- **Updated**: 2019-08-18 18:43:44+00:00
- **Authors**: Aliasghar Mortazi, Naji Khosravan, Drew A. Torigian, Sila Kurugol, Ulas Bagci
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2019)
- **Journal**: None
- **Summary**: The performance of the state-of-the-art image segmentation methods heavily relies on the high-quality annotations, which are not easily affordable, particularly for medical data. To alleviate this limitation, in this study, we propose a weakly supervised image segmentation method based on a deep geodesic prior. We hypothesize that integration of this prior information can reduce the adverse effects of weak labels in segmentation accuracy. Our proposed algorithm is based on a prior information, extracted from an auto-encoder, trained to map objects geodesic maps to their corresponding binary maps. The obtained information is then used as an extra term in the loss function of the segmentor. In order to show efficacy of the proposed strategy, we have experimented segmentation of cardiac substructures with clean and two levels of noisy labels (L1, L2). Our experiments showed that the proposed algorithm boosted the performance of baseline deep learning-based segmentation for both clean and noisy labels by 4.4%, 4.6%(L1), and 6.3%(L2) in dice score, respectively. We also showed that the proposed method was more robust in the presence of high-level noise due to the existence of shape priors.



### Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features
- **Arxiv ID**: http://arxiv.org/abs/1908.06537v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06537v1)
- **Published**: 2019-08-18 23:23:54+00:00
- **Updated**: 2019-08-18 23:23:54+00:00
- **Authors**: Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Establishing visual correspondences under large intra-class variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details. To tackle these challenges, we represent images by "hyperpixels" that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis.



### A New Technique of Camera Calibration: A Geometric Approach Based on Principal Lines
- **Arxiv ID**: http://arxiv.org/abs/1908.06539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06539v1)
- **Published**: 2019-08-18 23:28:05+00:00
- **Updated**: 2019-08-18 23:28:05+00:00
- **Authors**: Jen-Hui Chuang, Chih-Hui Ho, Ardian Umam, HsinYi Chen, Mu-Tien Lu, Jenq-Neng Hwang, Tai-An Chen
- **Comment**: Under review
- **Journal**: None
- **Summary**: Camera calibration is a crucial prerequisite in many applications of computer vision. In this paper, a new, geometry-based camera calibration technique is proposed, which resolves two main issues associated with the widely used Zhang's method: (i) the lack of guidelines to avoid outliers in the computation and (ii) the assumption of fixed camera focal length. The proposed approach is based on the closed-form solution of principal lines (PLs), with their intersection being the principal point while each PL can concisely represent relative orientation/position (up to one degree of freedom for both) between a special pair of coordinate systems of image plane and calibration pattern. With such analytically tractable image features, computations associated with the calibration are greatly simplified, while the guidelines in (i) can be established intuitively. Experimental results for synthetic and real data show that the proposed approach does compare favorably with Zhang's method, in terms of correctness, robustness, and flexibility, and addresses issues (i) and (ii) satisfactorily.



