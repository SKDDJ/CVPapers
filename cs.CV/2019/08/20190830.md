# Arxiv Papers in cs.CV on 2019-08-30
### Learned reconstructions for practical mask-based lensless imaging
- **Arxiv ID**: http://arxiv.org/abs/1908.11502v1
- **DOI**: 10.1364/OE.27.028075
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11502v1)
- **Published**: 2019-08-30 01:45:05+00:00
- **Updated**: 2019-08-30 01:45:05+00:00
- **Authors**: Kristina Monakhova, Joshua Yurtsever, Grace Kuo, Nick Antipa, Kyrollos Yanny, Laura Waller
- **Comment**: None
- **Journal**: None
- **Summary**: Mask-based lensless imagers are smaller and lighter than traditional lensed cameras. In these imagers, the sensor does not directly record an image of the scene; rather, a computational algorithm reconstructs it. Typically, mask-based lensless imagers use a model-based reconstruction approach that suffers from long compute times and a heavy reliance on both system calibration and heuristically chosen denoisers. In this work, we address these limitations using a bounded-compute, trainable neural network to reconstruct the image. We leverage our knowledge of the physical system by unrolling a traditional model-based optimization algorithm, whose parameters we optimize using experimentally gathered ground-truth data. Optionally, images produced by the unrolled network are then fed into a jointly-trained denoiser. As compared to traditional methods, our architecture achieves better perceptual image quality and runs 20x faster, enabling interactive previewing of the scene. We explore a spectrum between model-based and deep learning methods, showing the benefits of using an intermediate approach. Finally, we test our network on images taken in the wild with a prototype mask-based camera, demonstrating that our network generalizes to natural images.



### EventCap: Monocular 3D Capture of High-Speed Human Motions using an Event Camera
- **Arxiv ID**: http://arxiv.org/abs/1908.11505v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.11505v1)
- **Published**: 2019-08-30 01:59:41+00:00
- **Updated**: 2019-08-30 01:59:41+00:00
- **Authors**: Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Habermann, Lu Fang, Christian Theobalt
- **Comment**: 10 pages, 11 figures, 2 tables
- **Journal**: None
- **Summary**: The high frame rate is a critical requirement for capturing fast human motions. In this setting, existing markerless image-based methods are constrained by the lighting requirement, the high data bandwidth and the consequent high computation overhead. In this paper, we propose EventCap --- the first approach for 3D capturing of high-speed human motions using a single event camera. Our method combines model-based optimization and CNN-based human pose detection to capture high-frequency motion details and to reduce the drifting in the tracking. As a result, we can capture fast motions at millisecond resolution with significantly higher data efficiency than using high frame rate videos. Experiments on our new event-based fast human motion dataset demonstrate the effectiveness and accuracy of our method, as well as its robustness to challenging lighting conditions.



### Virtual Thin Slice: 3D Conditional GAN-based Super-resolution for CT Slice Interval
- **Arxiv ID**: http://arxiv.org/abs/1908.11506v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11506v2)
- **Published**: 2019-08-30 02:01:04+00:00
- **Updated**: 2019-09-02 01:37:08+00:00
- **Authors**: Akira Kudo, Yoshiro Kitamura, Yuanzhong Li, Satoshi Iizuka, Edgar Simo-Serra
- **Comment**: 10 pages, 6 figures, Accepted to Machine Learning for Medical Image
  Reconstruction (MLMIR) at MICCAI 2019
- **Journal**: None
- **Summary**: Many CT slice images are stored with large slice intervals to reduce storage size in clinical practice. This leads to low resolution perpendicular to the slice images (i.e., z-axis), which is insufficient for 3D visualization or image analysis. In this paper, we present a novel architecture based on conditional Generative Adversarial Networks (cGANs) with the goal of generating high resolution images of main body parts including head, chest, abdomen and legs. However, GANs are known to have a difficulty with generating a diversity of patterns due to a phenomena known as mode collapse. To overcome the lack of generated pattern variety, we propose to condition the discriminator on the different body parts. Furthermore, our generator networks are extended to be three dimensional fully convolutional neural networks, allowing for the generation of high resolution images from arbitrary fields of view. In our verification tests, we show that the proposed method obtains the best scores by PSNR/SSIM metrics and Visual Turing Test, allowing for accurate reproduction of the principle anatomy in high resolution. We expect that the proposed method contribute to effective utilization of the existing vast amounts of thick CT images stored in hospitals.



### Class-Based Styling: Real-time Localized Style Transfer with Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.11525v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11525v1)
- **Published**: 2019-08-30 04:08:15+00:00
- **Updated**: 2019-08-30 04:08:15+00:00
- **Authors**: Lironne Kurzman, David Vazquez, Issam Laradji
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a Class-Based Styling method (CBS) that can map different styles for different object classes in real-time. CBS achieves real-time performance by carrying out two steps simultaneously. While a semantic segmentation method is used to obtain the mask of each object class in a video frame, a styling method is used to style that frame globally. Then an object class can be styled by combining the segmentation mask and the styled image. The user can also select multiple styles so that different object classes can have different styles in a single frame. For semantic segmentation, we leverage DABNet that achieves high accuracy, yet only has 0.76 million parameters and runs at 104 FPS. For the style transfer step, we use a popular real-time method proposed by Johnson et al. [7]. We evaluated CBS on a video of the CityScapes dataset and observed high-quality localized style transfer results for different object classes and real-time performance.



### MVS^2: Deep Unsupervised Multi-view Stereo with Multi-View Symmetry
- **Arxiv ID**: http://arxiv.org/abs/1908.11526v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11526v1)
- **Published**: 2019-08-30 04:09:02+00:00
- **Updated**: 2019-08-30 04:09:02+00:00
- **Authors**: Yuchao Dai, Zhidong Zhu, Zhibo Rao, Bo Li
- **Comment**: Accepted by International Conference on 3D Vision (3DV 2019) as ORAL
  presentation
- **Journal**: None
- **Summary**: The success of existing deep-learning based multi-view stereo (MVS) approaches greatly depends on the availability of large-scale supervision in the form of dense depth maps. Such supervision, while not always possible, tends to hinder the generalization ability of the learned models in never-seen-before scenarios. In this paper, we propose the first unsupervised learning based MVS network, which learns the multi-view depth maps from the input multi-view images and does not need ground-truth 3D training data. Our network is symmetric in predicting depth maps for all views simultaneously, where we enforce cross-view consistency of multi-view depth maps during both training and testing stages. Thus, the learned multi-view depth maps naturally comply with the underlying 3D scene geometry. Besides, our network also learns the multi-view occlusion maps, which further improves the robustness of our network in handling real-world occlusions. Experimental results on multiple benchmarking datasets demonstrate the effectiveness of our network and the excellent generalization ability.



### Bin-wise Temperature Scaling (BTS): Improvement in Confidence Calibration Performance through Simple Scaling Techniques
- **Arxiv ID**: http://arxiv.org/abs/1908.11528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11528v2)
- **Published**: 2019-08-30 04:18:27+00:00
- **Updated**: 2019-09-23 07:30:26+00:00
- **Authors**: Byeongmoon Ji, Hyemin Jung, Jihyeun Yoon, Kyungyul Kim, Younghak Shin
- **Comment**: None
- **Journal**: ICCV 2019 Workshop on Interpreting and Explaining Visual
  Artificial Intelligence Models
- **Summary**: The prediction reliability of neural networks is important in many applications. Specifically, in safety-critical domains, such as cancer prediction or autonomous driving, a reliable confidence of model's prediction is critical for the interpretation of the results. Modern deep neural networks have achieved a significant improvement in performance for many different image classification tasks. However, these networks tend to be poorly calibrated in terms of output confidence. Temperature scaling is an efficient post-processing-based calibration scheme and obtains well calibrated results. In this study, we leverage the concept of temperature scaling to build a sophisticated bin-wise scaling. Furthermore, we adopt augmentation of validation samples for elaborated scaling. The proposed methods consistently improve calibration performance with various datasets and deep convolutional neural network models.



### Satellite Pose Estimation with Deep Landmark Regression and Nonlinear Pose Refinement
- **Arxiv ID**: http://arxiv.org/abs/1908.11542v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11542v1)
- **Published**: 2019-08-30 05:49:45+00:00
- **Updated**: 2019-08-30 05:49:45+00:00
- **Authors**: Bo Chen, Jiewei Cao, Alvaro Parra, Tat-Jun Chin
- **Comment**: Accepted by ICCVW 2019
- **Journal**: None
- **Summary**: We propose an approach to estimate the 6DOF pose of a satellite, relative to a canonical pose, from a single image. Such a problem is crucial in many space proximity operations, such as docking, debris removal, and inter-spacecraft communications. Our approach combines machine learning and geometric optimisation, by predicting the coordinates of a set of landmarks in the input image, associating the landmarks to their corresponding 3D points on an a priori reconstructed 3D model, then solving for the object pose using non-linear optimisation. Our approach is not only novel for this specific pose estimation task, which helps to further open up a relatively new domain for machine learning and computer vision, but it also demonstrates superior accuracy and won the first place in the recent Kelvins Pose Estimation Challenge organised by the European Space Agency (ESA).



### Context Aware Road-user Importance Estimation (iCARE)
- **Arxiv ID**: http://arxiv.org/abs/1909.05152v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05152v1)
- **Published**: 2019-08-30 05:54:44+00:00
- **Updated**: 2019-08-30 05:54:44+00:00
- **Authors**: Alireza Rahimpour, Sujitha Martin, Ashish Tawari, Hairong Qi
- **Comment**: Published in: IEEE Intelligent Vehicles (IV), 2019
- **Journal**: None
- **Summary**: Road-users are a critical part of decision-making for both self-driving cars and driver assistance systems. Some road-users, however, are more important for decision-making than others because of their respective intentions, ego vehicle's intention and their effects on each other. In this paper, we propose a novel architecture for road-user importance estimation which takes advantage of the local and global context of the scene. For local context, the model exploits the appearance of the road users (which captures orientation, intention, etc.) and their location relative to ego-vehicle. The global context in our model is defined based on the feature map of the convolutional layer of the module which predicts the future path of the ego-vehicle and contains rich global information of the scene (e.g., infrastructure, road lanes, etc.), as well as the ego vehicle's intention information. Moreover, this paper introduces a new data set of real-world driving, concentrated around inter-sections and includes annotations of important road users. Systematic evaluations of our proposed method against several baselines show promising results.



### Handwritten Chinese Character Recognition by Convolutional Neural Network and Similarity Ranking
- **Arxiv ID**: http://arxiv.org/abs/1908.11550v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11550v1)
- **Published**: 2019-08-30 06:21:52+00:00
- **Updated**: 2019-08-30 06:21:52+00:00
- **Authors**: Junyi Zou, Jinliang Zhang, Ludi Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Convolution Neural Networks (CNN) have recently achieved state-of-the art performance on handwritten Chinese character recognition (HCCR). However, most of CNN models employ the SoftMax activation function and minimize cross entropy loss, which may cause loss of inter-class information. To cope with this problem, we propose to combine cross entropy with similarity ranking function and use it as loss function. The experiments results show that the combination loss functions produce higher accuracy in HCCR. This report briefly reviews cross entropy loss function, a typical similarity ranking function: Euclidean distance, and also propose a new similarity ranking function: Average variance similarity. Experiments are done to compare the performances of a CNN model with three different loss functions. In the end, SoftMax cross entropy with Average variance similarity produce the highest accuracy on handwritten Chinese characters recognition.



### Revisiting CycleGAN for semi-supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.11569v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11569v1)
- **Published**: 2019-08-30 07:14:43+00:00
- **Updated**: 2019-08-30 07:14:43+00:00
- **Authors**: Arnab Kumar Mondal, Aniket Agarwal, Jose Dolz, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study the problem of training deep networks for semantic image segmentation using only a fraction of annotated images, which may significantly reduce human annotation efforts. Particularly, we propose a strategy that exploits the unpaired image style transfer capabilities of CycleGAN in semi-supervised segmentation. Unlike recent works using adversarial learning for semi-supervised segmentation, we enforce cycle consistency to learn a bidirectional mapping between unpaired images and segmentation masks. This adds an unsupervised regularization effect that boosts the segmentation performance when annotated data is limited. Experiments on three different public segmentation benchmarks (PASCAL VOC 2012, Cityscapes and ACDC) demonstrate the effectiveness of the proposed method. The proposed model achieves 2-4% of improvement with respect to the baseline and outperforms recent approaches for this task, particularly in low labeled data regime.



### ORBSLAM-Atlas: a robust and accurate multi-map system
- **Arxiv ID**: http://arxiv.org/abs/1908.11585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11585v1)
- **Published**: 2019-08-30 08:09:46+00:00
- **Updated**: 2019-08-30 08:09:46+00:00
- **Authors**: Richard Elvira, Juan D. Tardós, J. M. M. Montiel
- **Comment**: 2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
- **Journal**: None
- **Summary**: We propose ORBSLAM-Atlas, a system able to handle an unlimited number of disconnected sub-maps, that includes a robust map merging algorithm able to detect sub-maps with common regions and seamlessly fuse them. The outstanding robustness and accuracy of ORBSLAM are due to its ability to detect wide-baseline matches between keyframes, and to exploit them by means of non-linear optimization, however it only can handle a single map. ORBSLAM-Atlas brings the wide-baseline matching detection and exploitation to the multiple map arena. The result is a SLAM system significantly more general and robust, able to perform multi-session mapping. If tracking is lost during exploration, instead of freezing the map, a new sub-map is launched, and it can be fused with the previous map when common parts are visited. Our criteria to declare the camera lost contrast with previous approaches that simply count the number of tracked points, we propose to discard also inaccurately estimated camera poses due to bad geometrical conditioning. As a result, the map is split into more accurate sub-maps, that are eventually merged in a more accurate global map, thanks to the multi-mapping capabilities.   We provide extensive experimental validation in the EuRoC datasets, where ORBSLAM-Atlas obtains accurate monocular and stereo results in the difficult sequences where ORBSLAM failed. We also build global maps after multiple sessions in the same room, obtaining the best results to date, between 2 and 3 times more accurate than competing multi-map approaches. We also show the robustness and capability of our system to deal with dynamic scenes, quantitatively in the EuRoC datasets and qualitatively in a densely populated corridor where camera occlusions and tracking losses are frequent.



### Copy-and-Paste Networks for Deep Video Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1908.11587v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11587v1)
- **Published**: 2019-08-30 08:11:06+00:00
- **Updated**: 2019-08-30 08:11:06+00:00
- **Authors**: Sungho Lee, Seoung Wug Oh, DaeYeun Won, Seon Joo Kim
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes affine matrices between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos.



### Recursive Visual Sound Separation Using Minus-Plus Net
- **Arxiv ID**: http://arxiv.org/abs/1908.11602v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1908.11602v2)
- **Published**: 2019-08-30 09:05:26+00:00
- **Updated**: 2019-10-23 07:32:46+00:00
- **Authors**: Xudong Xu, Bo Dai, Dahua Lin
- **Comment**: accepted by ICCV2019
- **Journal**: None
- **Summary**: Sounds provide rich semantics, complementary to visual data, for many tasks. However, in practice, sounds from multiple sources are often mixed together. In this paper we propose a novel framework, referred to as MinusPlus Network (MP-Net), for the task of visual sound separation. MP-Net separates sounds recursively in the order of average energy, removing the separated sound from the mixture at the end of each prediction, until the mixture becomes empty or contains only noise. In this way, MP-Net could be applied to sound mixtures with arbitrary numbers and types of sounds. Moreover, while MP-Net keeps removing sounds with large energy from the mixture, sounds with small energy could emerge and become clearer, so that the separation is more accurate. Compared to previous methods, MP-Net obtains state-of-the-art results on two large scale datasets, across mixtures with different types and numbers of sounds.



### Multi-Grained Spatio-temporal Modeling for Lip-reading
- **Arxiv ID**: http://arxiv.org/abs/1908.11618v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1908.11618v2)
- **Published**: 2019-08-30 09:50:20+00:00
- **Updated**: 2019-09-02 02:22:32+00:00
- **Authors**: Chenhao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Lip-reading aims to recognize speech content from videos via visual analysis of speakers' lip movements. This is a challenging task due to the existence of homophemes-words which involve identical or highly similar lip movements, as well as diverse lip appearances and motion patterns among the speakers. To address these challenges, we propose a novel lip-reading model which captures not only the nuance between words but also styles of different speakers, by a multi-grained spatio-temporal modeling of the speaking process. Specifically, we first extract both frame-level fine-grained features and short-term medium-grained features by the visual front-end, which are then combined to obtain discriminative representations for words with similar phonemes. Next, a bidirectional ConvLSTM augmented with temporal attention aggregates spatio-temporal information in the entire input sequence, which is expected to be able to capture the coarse-gained patterns of each word and robust to various conditions in speaker identity, lighting conditions, and so on. By making full use of the information from different levels in a unified framework, the model is not only able to distinguish words with similar pronunciations, but also becomes robust to appearance changes. We evaluate our method on two challenging word-level lip-reading benchmarks and show the effectiveness of the proposed method, which also demonstrate the above claims.



### Semi-supervised Learning of Fetal Anatomy from Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1908.11624v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11624v1)
- **Published**: 2019-08-30 10:03:32+00:00
- **Updated**: 2019-08-30 10:03:32+00:00
- **Authors**: Jeremy Tan, Anselm Au, Qingjie Meng, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning methods have achieved excellent performance on standard benchmark datasets using very few labelled images. Anatomy classification in fetal 2D ultrasound is an ideal problem setting to test whether these results translate to non-ideal data. Our results indicate that inclusion of a challenging background class can be detrimental and that semi-supervised learning mostly benefits classes that are already distinct, sometimes at the expense of more similar classes.



### Domain Intersection and Domain Difference
- **Arxiv ID**: http://arxiv.org/abs/1908.11628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11628v1)
- **Published**: 2019-08-30 10:08:43+00:00
- **Updated**: 2019-08-30 10:08:43+00:00
- **Authors**: Sagie Benaim, Michael Khaitov, Tomer Galanti, Lior Wolf
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: We present a method for recovering the shared content between two visual domains as well as the content that is unique to each domain. This allows us to map from one domain to the other, in a way in which the content that is specific for the first domain is removed and the content that is specific for the second is imported from any image in the second domain. In addition, our method enables generation of images from the intersection of the two domains as well as their union, despite having no such samples during training. The method is shown analytically to contain all the sufficient and necessary constraints. It also outperforms the literature methods in an extensive set of experiments. Our code is available at https://github.com/sagiebenaim/DomainIntersectionDifference.



### EBPC: Extended Bit-Plane Compression for Deep Neural Network Inference and Training Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1908.11645v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11645v2)
- **Published**: 2019-08-30 10:47:31+00:00
- **Updated**: 2019-10-25 14:47:32+00:00
- **Authors**: Lukas Cavigelli, Georg Rutishauser, Luca Benini
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1810.03979
- **Journal**: None
- **Summary**: In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I/O bandwidth, power consumption is dominated by I/O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly, and lossless compression scheme for the feature maps present within convolutional neural networks. We present hardware architectures and synthesis results for the compressor and decompressor in 65nm. With a throughput of one 8-bit word/cycle at 600MHz, they fit into 2.8kGE and 3.0kGE of silicon area, respectively - together the size of less than seven 8-bit multiply-add units at the same throughput. We show that an average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for ResNet-34 and 2.2x for MobileNetV2 can be achieved - a gain of 45-70% over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.



### LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net
- **Arxiv ID**: http://arxiv.org/abs/1908.11656v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11656v1)
- **Published**: 2019-08-30 11:28:59+00:00
- **Updated**: 2019-08-30 11:28:59+00:00
- **Authors**: Pierre Biasutti, Vincent Lepetit, Jean-François Aujol, Mathieu Brédif, Aurélie Bugeau
- **Comment**: 9 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:1905.08748
- **Journal**: None
- **Summary**: We propose LU-Net -- for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efficiently solves the problem as an image processing problem. We first extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can finally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efficient. In this way, we can exploit both the 3D nature of the data and the specificity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.



### Small Obstacle Avoidance Based on RGB-D Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.11675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11675v1)
- **Published**: 2019-08-30 12:10:15+00:00
- **Updated**: 2019-08-30 12:10:15+00:00
- **Authors**: Minjie Hua, Yibing Nan, Shiguo Lian
- **Comment**: Accepted by CVRSUAD 2019 (ICCV Workshop)
- **Journal**: None
- **Summary**: This paper presents a novel obstacle avoidance system for road robots equipped with RGB-D sensor that captures scenes of its way forward. The purpose of the system is to have road robots move around autonomously and constantly without any collision even with small obstacles, which are often missed by existing solutions. For each input RGB-D image, the system uses a new two-stage semantic segmentation network followed by the morphological processing to generate the accurate semantic map containing road and obstacles. Based on the map, the local path planning is applied to avoid possible collision. Additionally, optical flow supervision and motion blurring augmented training scheme is applied to improve temporal consistency between adjacent frames and overcome the disturbance caused by camera shake. Various experiments are conducted to show that the proposed architecture obtains high performance both in indoor and outdoor scenarios.



### Motion Capture from Pan-Tilt Cameras with Unknown Orientation
- **Arxiv ID**: http://arxiv.org/abs/1908.11676v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11676v1)
- **Published**: 2019-08-30 12:12:54+00:00
- **Updated**: 2019-08-30 12:12:54+00:00
- **Authors**: Roman Bachmann, Jörg Spörri, Pascal Fua, Helge Rhodin
- **Comment**: International Conference on 3D Vision 2019
- **Journal**: None
- **Summary**: In sports, such as alpine skiing, coaches would like to know the speed and various biomechanical variables of their athletes and competitors. Existing methods use either body-worn sensors, which are cumbersome to setup, or manual image annotation, which is time consuming. We propose a method for estimating an athlete's global 3D position and articulated pose using multiple cameras. By contrast to classical markerless motion capture solutions, we allow cameras to rotate freely so that large capture volumes can be covered. In a first step, tight crops around the skier are predicted and fed to a 2D pose estimator network. The 3D pose is then reconstructed using a bundle adjustment method. Key to our solution is the rotation estimation of Pan-Tilt cameras in a joint optimization with the athlete pose and conditioning on relative background motion computed with feature tracking. Furthermore, we created a new alpine skiing dataset and annotated it with 2D pose labels, to overcome shortcomings of existing ones. Our method estimates accurate global 3D poses from images only and provides coaches with an automatic and fast tool for measuring and improving an athlete's performance.



### Multi-Modal Fusion for End-to-End RGB-T Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.11714v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11714v1)
- **Published**: 2019-08-30 12:58:10+00:00
- **Updated**: 2019-08-30 12:58:10+00:00
- **Authors**: Lichao Zhang, Martin Danelljan, Abel Gonzalez-Garcia, Joost van de Weijer, Fahad Shahbaz Khan
- **Comment**: Accepted at ICCVW (VOT) 2019
- **Journal**: None
- **Summary**: We propose an end-to-end tracking framework for fusing the RGB and TIR modalities in RGB-T tracking. Our baseline tracker is DiMP (Discriminative Model Prediction), which employs a carefully designed target prediction network trained end-to-end using a discriminative loss. We analyze the effectiveness of modality fusion in each of the main components in DiMP, i.e. feature extractor, target estimation network, and classifier. We consider several fusion mechanisms acting at different levels of the framework, including pixel-level, feature-level and response-level. Our tracker is trained in an end-to-end manner, enabling the components to learn how to fuse the information from both modalities. As data to train our model, we generate a large-scale RGB-T dataset by considering an annotated RGB tracking dataset (GOT-10k) and synthesizing paired TIR images using an image-to-image translation approach. We perform extensive experiments on VOT-RGBT2019 dataset and RGBT210 dataset, evaluating each type of modality fusing on each model component. The results show that the proposed fusion mechanisms improve the performance of the single modality counterparts. We obtain our best results when fusing at the feature-level on both the IoU-Net and the model predictor, obtaining an EAO score of 0.391 on VOT-RGBT2019 dataset. With this fusion mechanism we achieve the state-of-the-art performance on RGBT210 dataset.



### Fact-Checking Meets Fauxtography: Verifying Claims About Images
- **Arxiv ID**: http://arxiv.org/abs/1908.11722v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.IR, 68T50, I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/1908.11722v1)
- **Published**: 2019-08-30 13:12:21+00:00
- **Updated**: 2019-08-30 13:12:21+00:00
- **Authors**: Dimitrina Zlatkova, Preslav Nakov, Ivan Koychev
- **Comment**: Claims about Images; Fauxtography; Fact-Checking; Veracity; Fake News
- **Journal**: EMNLP-2019
- **Summary**: The recent explosion of false claims in social media and on the Web in general has given rise to a lot of manual fact-checking initiatives. Unfortunately, the number of claims that need to be fact-checked is several orders of magnitude larger than what humans can handle manually. Thus, there has been a lot of research aiming at automating the process. Interestingly, previous work has largely ignored the growing number of claims about images. This is despite the fact that visual imagery is more influential than text and naturally appears alongside fake news. Here we aim at bridging this gap. In particular, we create a new dataset for this problem, and we explore a variety of features modeling the claim, the image, and the relationship between the claim and the image. The evaluation results show sizable improvements over the baseline. We release our dataset, hoping to enable further research on fact-checking claims about images.



### Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid
- **Arxiv ID**: http://arxiv.org/abs/1908.11754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11754v1)
- **Published**: 2019-08-30 14:19:24+00:00
- **Updated**: 2019-08-30 14:19:24+00:00
- **Authors**: Zhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang Lin, Wayne Zhang
- **Comment**: ICCV 2019 (oral)
- **Journal**: None
- **Summary**: Matching clothing images from customers and online shopping stores has rich applications in E-commerce. Existing algorithms encoded an image as a global feature vector and performed retrieval with the global representation. However, discriminative local information on clothes are submerged in this global representation, resulting in sub-optimal performance. To address this issue, we propose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which learns similarities between a query and a gallery cloth by using both global and local representations in multiple scales. The similarity pyramid is represented by a Graph of similarity, where nodes represent similarities between clothing components at different scales, and the final matching score is obtained by message passing along edges. In GRNet, graph reasoning is solved by training a graph convolutional network, enabling to align salient clothing components to improve clothing retrieval. To facilitate future researches, we introduce a new benchmark FindFashion, containing rich annotations of bounding boxes, views, occlusions, and cropping. Extensive experiments show that GRNet obtains new state-of-the-art results on two challenging benchmarks, e.g., pushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%, and 75% (i.e., 4%, 10%, and 10% absolute improvements), outperforming competitors with large margins. On FindFashion, GRNet achieves considerable improvements on all empirical settings.



### Temporal Coherence for Active Learning in Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.11757v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.11757v1)
- **Published**: 2019-08-30 14:20:36+00:00
- **Updated**: 2019-08-30 14:20:36+00:00
- **Authors**: Javad Zolfaghari Bengar, Abel Gonzalez-Garcia, Gabriel Villalonga, Bogdan Raducanu, Hamed H. Aghdam, Mikhail Mozerov, Antonio M. Lopez, Joost van de Weijer
- **Comment**: Accepted at ICCVW 2019 (CVRSUAD-Road Scene Understanding and
  Autonomous Driving)
- **Journal**: None
- **Summary**: Autonomous driving systems require huge amounts of data to train. Manual annotation of this data is time-consuming and prohibitively expensive since it involves human resources. Therefore, active learning emerged as an alternative to ease this effort and to make data annotation more manageable. In this paper, we introduce a novel active learning approach for object detection in videos by exploiting temporal coherence. Our active learning criterion is based on the estimated number of errors in terms of false positives and false negatives. The detections obtained by the object detector are used to define the nodes of a graph and tracked forward and backward to temporally link the nodes. Minimizing an energy function defined on this graphical model provides estimates of both false positives and false negatives. Additionally, we introduce a synthetic video dataset, called SYNTHIA-AL, specially designed to evaluate active learning for video object detection in road scenes. Finally, we show that our approach outperforms active learning baselines tested on two datasets.



### FisheyeMODNet: Moving Object detection on Surround-view Cameras for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1908.11789v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11789v1)
- **Published**: 2019-08-30 15:29:46+00:00
- **Updated**: 2019-08-30 15:29:46+00:00
- **Authors**: Marie Yahiaoui, Hazem Rashed, Letizia Mariotti, Ganesh Sistu, Ian Clancy, Lucie Yahiaoui, Varun Ravi Kumar, Senthil Yogamani
- **Comment**: Accepted for ICCV 2019 Workshop on 360{\deg} Perception and
  Interaction. A shorter version was presented at IMVIP 2019
- **Journal**: None
- **Summary**: Moving Object Detection (MOD) is an important task for achieving robust autonomous driving. An autonomous vehicle has to estimate collision risk with other interacting objects in the environment and calculate an optional trajectory. Collision risk is typically higher for moving objects than static ones due to the need to estimate the future states and poses of the objects for decision making. This is particularly important for near-range objects around the vehicle which are typically detected by a fisheye surround-view system that captures a 360{\deg} view of the scene. In this work, we propose a CNN architecture for moving object detection using fisheye images that were captured in autonomous driving environment. As motion geometry is highly non-linear and unique for fisheye cameras, we will make an improved version of the current dataset public to encourage further research. To target embedded deployment, we design a lightweight encoder sharing weights across sequential images. The proposed network runs at 15 fps on a 1 teraflops automotive embedded system at accuracy of 40% IoU and 69.5% mIoU.



### Dense Dilated Convolutions Merging Network for Semantic Mapping of Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1908.11799v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11799v1)
- **Published**: 2019-08-30 15:47:15+00:00
- **Updated**: 2019-08-30 15:47:15+00:00
- **Authors**: Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg
- **Comment**: JURSE 2019
- **Journal**: None
- **Summary**: We propose a network for semantic mapping called the Dense Dilated Convolutions Merging Network (DDCM-Net) to provide a deep learning approach that can recognize multi-scale and complex shaped objects with similar color and textures, such as buildings, surfaces/roads, and trees in very high resolution remote sensing images. The proposed DDCM-Net consists of dense dilated convolutions merged with varying dilation rates. This can effectively enlarge the kernels' receptive fields, and, more importantly, obtain fused local and global context information to promote surrounding discriminative capability. We demonstrate the effectiveness of the proposed DDCM-Net on the publicly available ISPRS Potsdam dataset and achieve a performance of 92.3% F1-score and 86.0% mean intersection over union accuracy by only using the RGB bands, without any post-processing. We also show results on the ISPRS Vaihingen dataset, where the DDCM-Net trained with IRRG bands, also obtained better mapping accuracy (89.8% F1-score) than previous state-of-the-art approaches.



### Learning Rich Representations For Structured Visual Prediction Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.11820v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.11820v1)
- **Published**: 2019-08-30 16:18:26+00:00
- **Updated**: 2019-08-30 16:18:26+00:00
- **Authors**: Mohammadreza Mostajabi
- **Comment**: PhD Thesis
- **Journal**: None
- **Summary**: We describe an approach to learning rich representations for images, that enables simple and effective predictors in a range of vision tasks involving spatially structured maps. Our key idea is to map small image elements to feature representations extracted from a sequence of nested regions of increasing spatial extent. These regions are obtained by "zooming out" from the pixel/superpixel all the way to scene-level resolution, and hence we call these zoom-out features. Applied to semantic segmentation and other structured prediction tasks, our approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead image elements are classified by a feedforward multilayer network with skip-layer connections spanning the zoom-out levels. When used in conjunction with modern neural architectures such as ResNet, DenseNet and NASNet (to which it is complementary) our approach achieves competitive accuracy on segmentation benchmarks.   In addition, we propose an approach for learning category-level semantic segmentation purely from image-level classification tag. It exploits localization cues that emerge from training a modified zoom-out architecture tailored for classification tasks, to drive a weakly supervised process that automatically labels a sparse, diverse training set of points likely to belong to classes of interest. Finally, we introduce data-driven regularization functions for the supervised training of CNNs. Our innovation takes the form of a regularizer derived by learning an autoencoder over the set of annotations. This approach leverages an improved representation of label space to inform extraction of features from images



### Dual Attention MobDenseNet(DAMDNet) for Robust 3D Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1908.11821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11821v1)
- **Published**: 2019-08-30 16:20:08+00:00
- **Updated**: 2019-08-30 16:20:08+00:00
- **Authors**: Lei Jiang Xiao-Jun Wu Josef Kittler
- **Comment**: 10 pages
- **Journal**: ICCV2019 workshop
- **Summary**: 3D face alignment of monocular images is a crucial process in the recognition of faces with disguise.3D face reconstruction facilitated by alignment can restore the face structure which is helpful in detcting disguise interference.This paper proposes a dual attention mechanism and an efficient end-to-end 3D face alignment framework.We build a stable network model through Depthwise Separable Convolution, Densely Connected Convolutional and Lightweight Channel Attention Mechanism. In order to enhance the ability of the network model to extract the spatial features of the face region, we adopt Spatial Group-wise Feature enhancement module to improve the representation ability of the network. Different loss functions are applied jointly to constrain the 3D parameters of a 3D Morphable Model (3DMM) and its 3D vertices. We use a variety of data enhancement methods and generate large virtual pose face data sets to solve the data imbalance problem. The experiments on the challenging AFLW,AFLW2000-3D datasets show that our algorithm significantly improves the accuracy of 3D face alignment. Our experiments using the field DFW dataset show that DAMDNet exhibits excellent performance in the 3D alignment and reconstruction of challenging disguised faces.The model parameters and the complexity of the proposed method are also reduced significantly.The code is publicly available at https:// github.com/LeiJiangJNU/DAMDNet



### Multi-Temporal Aerial Image Registration Using Semantic Features
- **Arxiv ID**: http://arxiv.org/abs/1908.11822v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11822v2)
- **Published**: 2019-08-30 16:20:28+00:00
- **Updated**: 2019-09-19 13:23:35+00:00
- **Authors**: Ananya Gupta, Yao Peng, Simon Watson, Hujun Yin
- **Comment**: Accepted to 20th International Conference on Intelligent Data
  Engineering and Automated Learning (IDEAL)
- **Journal**: None
- **Summary**: A semantic feature extraction method for multitemporal high resolution aerial image registration is proposed in this paper. These features encode properties or information about temporally invariant objects such as roads and help deal with issues such as changing foliage in image registration, which classical handcrafted features are unable to address. These features are extracted from a semantic segmentation network and have shown good robustness and accuracy in registering aerial images across years and seasons in the experiments.



### Reflective Decoding Network for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.11824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11824v1)
- **Published**: 2019-08-30 16:25:55+00:00
- **Updated**: 2019-08-30 16:25:55+00:00
- **Authors**: Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, Yu-Wing Tai
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions.



### Rethinking Irregular Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.11834v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11834v2)
- **Published**: 2019-08-30 16:47:08+00:00
- **Updated**: 2019-11-11 17:25:43+00:00
- **Authors**: Shangbang Long, Yushuo Guan, Bingxuan Wang, Kaigui Bian, Cong Yao
- **Comment**: Technical report for participation in ICDAR2019-ArT recognition track
- **Journal**: None
- **Summary**: Reading text from natural images is challenging due to the great variety in text font, color, size, complex background and etc.. The perspective distortion and non-linear spatial arrangement of characters make it further difficult. While rectification based method is intuitively grounded and has pushed the envelope by far, its potential is far from being well exploited. In this paper, we present a bag of tricks that prove to significantly improve the performance of rectification based method. On curved text dataset, our method achieves an accuracy of 89.6% on CUTE-80 and 76.3% on Total-Text, an improvement over previous state-of-the-art by 6.3% and 14.7% respectively. Furthermore, our combination of tricks helps us win the ICDAR 2019 Arbitrary-Shaped Text Challenge (Latin script), achieving an accuracy of 74.3% on the held-out test set. We release our code as well as data samples for further exploration at https://github.com/Jyouhou/ICDAR2019-ArT-Recognition-Alchemy



### Systematic Analysis of Image Generation using GANs
- **Arxiv ID**: http://arxiv.org/abs/1908.11863v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11863v1)
- **Published**: 2019-08-30 17:48:05+00:00
- **Updated**: 2019-08-30 17:48:05+00:00
- **Authors**: Rohan Akut, Sumukh Marathe, Rucha Apte, Ishan Joshi, Siddhivinayak Kulkarni
- **Comment**: Accepted in IEEE ICMLDS 2018
- **Journal**: None
- **Summary**: Generative Adversarial Networks have been crucial in the developments made in unsupervised learning in recent times. Exemplars of image synthesis from text or other images, these networks have shown remarkable improvements over conventional methods in terms of performance. Trained on the adversarial training philosophy, these networks aim to estimate the potential distribution from the real data and then use this as input to generate the synthetic data. Based on this fundamental principle, several frameworks can be generated that are paragon implementations in several real-life applications such as art synthesis, generation of high resolution outputs and synthesis of images from human drawn sketches, to name a few. While theoretically GANs present better results and prove to be an improvement over conventional methods in many factors, the implementation of these frameworks for dedicated applications remains a challenge. This study explores and presents a taxonomy of these frameworks and their use in various image to image synthesis and text to image synthesis applications. The basic GANs, as well as a variety of different niche frameworks, are critically analyzed. The advantages of GANs for image generation over conventional methods as well their disadvantages amongst other frameworks are presented. The future applications of GANs in industries such as healthcare, art and entertainment are also discussed.



### Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane through a Direct Closed-Loop Alignment
- **Arxiv ID**: http://arxiv.org/abs/1909.00032v1
- **DOI**: 10.1109/TVCG.2019.2932248
- **Categories**: **cs.GR**, cs.CV, H.5.1; I.4.8; B.4.2
- **Links**: [PDF](http://arxiv.org/pdf/1909.00032v1)
- **Published**: 2019-08-30 18:37:19+00:00
- **Updated**: 2019-08-30 18:37:19+00:00
- **Authors**: Shingo Kagami, Koichi Hashimoto
- **Comment**: To appear in IEEE Transactions on Visualization and Computer Graphics
  (ISMAR 2019 Special Issue)
- **Journal**: None
- **Summary**: This paper presents a fast projection mapping method for moving image content projected onto a markerless planar surface using a low-latency Digital Micromirror Device (DMD) projector. By adopting a closed-loop alignment approach, in which not only the surface texture but also the projected image is tracked by a camera, the proposed method is free from a calibration or position adjustment between the camera and projector. We designed fiducial patterns to be inserted into a fast flapping sequence of binary frames of the DMD projector, which allows the simultaneous tracking of the surface texture and a fiducial geometry separate from a single image captured by the camera. The proposed method implemented on a CPU runs at 400 fps and enables arbitrary video contents to be "stuck" onto a variety of textured surfaces.



### Learning Digital Circuits: A Journey Through Weight Invariant Self-Pruning Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.00052v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.00052v3)
- **Published**: 2019-08-30 20:07:39+00:00
- **Updated**: 2020-05-03 22:23:59+00:00
- **Authors**: Amey Agrawal, Rohit Karlupia
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, in the paper "Weight Agnostic Neural Networks" Gaier & Ha utilized architecture search to find networks where the topology completely encodes the knowledge. However, architecture search in topology space is expensive. We use the existing framework of binarized networks to find performant topologies by constraining the weights to be either, zero or one. We show that such topologies achieve performance similar to standard networks while pruning more than 99% weights. We further demonstrate that these topologies can perform tasks using constant weights without any explicit tuning. Finally, we discover that in our setup each neuron acts like a NOR gate, virtually learning a digital circuit. We demonstrate the efficacy of our approach on computer vision datasets.



### Robust Online Video Super-Resolution Using an Efficient Alternating Projections Scheme
- **Arxiv ID**: http://arxiv.org/abs/1909.00073v2
- **DOI**: 10.1016/j.sigpro.2020.107575
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00073v2)
- **Published**: 2019-08-30 21:15:58+00:00
- **Updated**: 2020-03-11 02:31:28+00:00
- **Authors**: Ricardo Augusto Borsoi
- **Comment**: None
- **Journal**: None
- **Summary**: Video super-resolution reconstruction (SRR) algorithms attempt to reconstruct high-resolution (HR) video sequences from low-resolution observations. Although recent progress in video SRR has significantly improved the quality of the reconstructed HR sequences, it remains challenging to design SRR algorithms that achieve good quality and robustness at a small computational complexity, being thus suitable for online applications. In this paper, we propose a new adaptive video SRR algorithm that achieves state-of-the-art performance at a very small computational cost. Using a nonlinear cost function constructed considering characteristics of typical innovation outliers in natural image sequences and an edge-preserving regularization strategy, we achieve state-of-the-art reconstructed image quality and robustness. This cost function is optimized using a specific alternating projections strategy over non-convex sets that is able to converge in a very few iterations. An accurate and very efficient approximation for the projection operations is also obtained using tools from multidimensional multirate signal processing. This solves the slow convergence issue of stochastic gradient-based methods while keeping a small computational complexity. Simulation results with both synthetic and real image sequences show that the performance of the proposed algorithm is similar or better than state-of-the-art SRR algorithms, while requiring only a small fraction of their computational cost.



### Deep Plug-and-Play Prior for Parallel MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1909.00089v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00089v2)
- **Published**: 2019-08-30 23:16:08+00:00
- **Updated**: 2019-09-06 22:12:51+00:00
- **Authors**: Ali Pour Yazdanpanah, Onur Afacan, Simon K. Warfield
- **Comment**: None
- **Journal**: None
- **Summary**: Fast data acquisition in Magnetic Resonance Imaging (MRI) is vastly in demand and scan time directly depends on the number of acquired k-space samples. Conventional MRI reconstruction methods for fast MRI acquisition mostly relied on different regularizers which represent analytical models of sparsity. However, recent data-driven methods based on deep learning has resulted in promising improvements in image reconstruction algorithms. In this paper, we propose a deep plug-and-play prior framework for parallel MRI reconstruction problems which utilize a deep neural network (DNN) as an advanced denoiser within an iterative method. This, in turn, enables rapid acquisition of MR images with improved image quality. The proposed method was compared with the reconstructions using the clinical gold standard GRAPPA method. Our results with undersampled data demonstrate that our method can deliver considerably higher quality images at high acceleration factors in comparison to clinical gold standard method for MRI reconstructions. Our proposed reconstruction enables an increase in acceleration factor, and a reduction in acquisition time while maintaining high image quality.



