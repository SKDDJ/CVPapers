# Arxiv Papers in cs.CV on 2019-08-16
### Cross-Domain Adaptation for Animal Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.05806v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05806v2)
- **Published**: 2019-08-16 01:16:30+00:00
- **Updated**: 2019-08-19 00:20:06+00:00
- **Authors**: Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, Yu-Wing Tai
- **Comment**: accepted by ICCV'2019 for oral presentation
- **Journal**: None
- **Summary**: In this paper, we are interested in pose estimation of animals. Animals usually exhibit a wide range of variations on poses and there is no available animal pose dataset for training and testing. To address this problem, we build an animal pose dataset to facilitate training and evaluation. Considering the heavy labor needed to label dataset and it is impossible to label data for all concerned animal species, we, therefore, proposed a novel cross-domain adaptation method to transform the animal pose knowledge from labeled animal classes to unlabeled animal classes. We use the modest animal pose dataset to adapt learned knowledge to multiple animals species. Moreover, humans also share skeleton similarities with some animals (especially four-footed mammals). Therefore, the easily available human pose dataset, which is of a much larger scale than our labeled animal dataset, provides important prior knowledge to boost up the performance on animal pose estimation. Experiments show that our proposed method leverages these pieces of prior knowledge well and achieves convincing results on animal pose estimation.



### Mixed High-Order Attention Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.05819v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05819v1)
- **Published**: 2019-08-16 02:38:21+00:00
- **Updated**: 2019-08-16 02:38:21+00:00
- **Authors**: Binghui Chen, Weihong Deng, Jiani Hu
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Attention has become more attractive in person reidentification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn/.



### A Cooperative Autoencoder for Population-Based Regularization of CNN Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1908.05825v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05825v2)
- **Published**: 2019-08-16 03:06:19+00:00
- **Updated**: 2019-08-19 14:44:47+00:00
- **Authors**: Riddhish Bhalodia, Shireen Y. Elhabian, Ladislav Kavan, Ross T. Whitaker
- **Comment**: To appear in MICCAI 2019
- **Journal**: None
- **Summary**: Spatial transformations are enablers in a variety of medical image analysis applications that entail aligning images to a common coordinate systems. Population analysis of such transformations is expected to capture the underlying image and shape variations, and hence these transformations are required to produce anatomically feasible correspondences. This is usually enforced through some smoothness-based generic regularization on deformation field. Alternatively, population-based regularization has been shown to produce anatomically accurate correspondences in cases where anatomically unaware (i.e., data independent) fail. Recently, deep networks have been for unsupervised image registration, these methods are computationally faster and maintains the accuracy of state of the art methods. However, these networks use smoothness penalty on deformation fields and ignores population-level statistics of the transformations. We propose a novel neural network architecture that simultaneously learns and uses the population-level statistics of the spatial transformations to regularize the neural networks for unsupervised image registration. This regularization is in the form of a bottleneck autoencoder, which encodes the population level information of the deformation fields in a low-dimensional manifold. The proposed architecture produces deformation fields that describe the population-level features and associated correspondences in an anatomically relevant manner and are statistically compact relative to the state-of-the-art approaches while maintaining computational efficiency. We demonstrate the efficacy of the proposed architecture on synthetic data sets, as well as 2D and 3D medical data.



### Transferable Contrastive Network for Generalized Zero-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.05832v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05832v1)
- **Published**: 2019-08-16 03:35:36+00:00
- **Updated**: 2019-08-16 03:35:36+00:00
- **Authors**: Huajie Jiang, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is a challenging problem that aims to recognize the target categories without seen data, where semantic information is leveraged to transfer knowledge from some source classes. Although ZSL has made great progress in recent years, most existing approaches are easy to overfit the sources classes in generalized zero-shot learning (GZSL) task, which indicates that they learn little knowledge about target classes. To tackle such problem, we propose a novel Transferable Contrastive Network (TCN) that explicitly transfers knowledge from the source classes to the target classes. It automatically contrasts one image with different classes to judge whether they are consistent or not. By exploiting the class similarities to make knowledge transfer from source images to similar target classes, our approach is more robust to recognize the target images. Experiments on five benchmark datasets show the superiority of our approach for GZSL.



### Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing Loss
- **Arxiv ID**: http://arxiv.org/abs/1908.05840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05840v1)
- **Published**: 2019-08-16 04:24:38+00:00
- **Updated**: 2019-08-16 04:24:38+00:00
- **Authors**: Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, Sungjoo Yoo
- **Comment**: Accepted to ICCV 2019
- **Journal**: Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV), 2019, pp. 9056-9065
- **Summary**: Line art colorization is expensive and challenging to automate. A GAN approach is proposed, called Tag2Pix, of line art colorization which takes as input a grayscale line art and color tag information and produces a quality colored image. First, we present the Tag2Pix line art colorization dataset. A generator network is proposed which consists of convolutional layers to transform the input line art, a pre-trained semantic extraction network, and an encoder for input color information. The discriminator is based on an auxiliary classifier GAN to classify the tag information as well as genuineness. In addition, we propose a novel network structure called SECat, which makes the generator properly colorize even small features such as eyes, and also suggest a novel two-step training method where the generator and discriminator first learn the notion of object and shape and then, based on the learned notion, learn colorization, such as where and how to place which color. We present both quantitative and qualitative evaluations which prove the effectiveness of the proposed method.



### Recurrent U-net: Deep learning to predict daily summertime ozone in the United States
- **Arxiv ID**: http://arxiv.org/abs/1908.05841v1
- **DOI**: None
- **Categories**: **physics.ao-ph**, cs.CV, cs.LG, physics.chem-ph, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1908.05841v1)
- **Published**: 2019-08-16 04:32:00+00:00
- **Updated**: 2019-08-16 04:32:00+00:00
- **Authors**: Tai-Long He, Dylan B. A. Jones, Binxuan Huang, Yuyang Liu, Kazuyuki Miyazaki, Zhe Jiang, E. Charlie White, Helen M. Worden, John R. Worden
- **Comment**: None
- **Journal**: None
- **Summary**: We use a hybrid deep learning model to predict June-July-August (JJA) daily maximum 8-h average (MDA8) surface ozone concentrations in the US. A set of meteorological fields from the ERA-Interim reanalysis as well as monthly mean NO$_x$ emissions from the Community Emissions Data System (CEDS) inventory are selected as predictors. Ozone measurements from the US Environmental Protection Agency (EPA) Air Quality System (AQS) from 1980 to 2009 are used to train the model, whereas data from 2010 to 2014 are used to evaluate the performance of the model. The model captures well daily, seasonal and interannual variability in MDA8 ozone across the US. Feature maps show that the model captures teleconnections between MDA8 ozone and the meteorological fields, which are responsible for driving the ozone dynamics. We used the model to evaluate recent trends in NO$_x$ emissions in the US and found that the trend in the EPA emission inventory produced the largest negative bias in MDA8 ozone between 2010-2016. The top-down emission trends from the Tropospheric Chemistry Reanalysis (TCR-2), which is based on satellite observations, produced predictions in best agreement with observations. In urban regions, the trend in AQS NO$_2$ observations provided ozone predictions in agreement with observations, whereas in rural regions the satellite-derived trends produced the best agreement. In both rural and urban regions the EPA trend resulted in the largest negative bias in predicted ozone. Our results suggest that the EPA inventory is overestimating the reductions in NO$_x$ emissions and that the satellite-derived trend reflects the influence of reductions in NO$_x$ emissions as well as changes in background NO$_x$. Our results demonstrate the significantly greater predictive capability that the deep learning model provides over conventional atmospheric chemical transport models for air quality analyses.



### daBNN: A Super Fast Inference Framework for Binary Neural Networks on ARM devices
- **Arxiv ID**: http://arxiv.org/abs/1908.05858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05858v1)
- **Published**: 2019-08-16 06:07:57+00:00
- **Updated**: 2019-08-16 06:07:57+00:00
- **Authors**: Jianhao Zhang, Yingwei Pan, Ting Yao, He Zhao, Tao Mei
- **Comment**: Accepted by 2019 ACMMM Open Source Software Competition. Source code:
  https://github.com/JDAI-CV/dabnn
- **Journal**: None
- **Summary**: It is always well believed that Binary Neural Networks (BNNs) could drastically accelerate the inference efficiency by replacing the arithmetic operations in float-valued Deep Neural Networks (DNNs) with bit-wise operations. Nevertheless, there has not been open-source implementation in support of this idea on low-end ARM devices (e.g., mobile phones and embedded devices). In this work, we propose daBNN --- a super fast inference framework that implements BNNs on ARM devices. Several speed-up and memory refinement strategies for bit-packing, binarized convolution, and memory layout are uniquely devised to enhance inference efficiency. Compared to the recent open-source BNN inference framework, BMXNet, our daBNN is $7\times$$\sim$$23\times$ faster on a single binary convolution, and about $6\times$ faster on Bi-Real Net 18 (a BNN variant of ResNet-18). The daBNN is a BSD-licensed inference framework, and its source code, sample projects and pre-trained models are available on-line: https://github.com/JDAI-CV/dabnn.



### Learning Deep Representations by Mutual Information for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.05860v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05860v1)
- **Published**: 2019-08-16 06:15:30+00:00
- **Updated**: 2019-08-16 06:15:30+00:00
- **Authors**: Peng Chen, Tong Jia, Pengfei Wu, Jianjun Wu, Dongyue Chen
- **Comment**: 8 pages, 9 figures
- **Journal**: None
- **Summary**: Most existing person re-identification (ReID) methods have good feature representations to distinguish pedestrians with deep convolutional neural network (CNN) and metric learning methods. However, these works concentrate on the similarity between encoder output and ground-truth, ignoring the correlation between input and encoder output, which affects the performance of identifying different pedestrians. To address this limitation, We design a Deep InfoMax (DIM) network to maximize the mutual information (MI) between the input image and encoder output, which doesn't need any auxiliary labels. To evaluate the effectiveness of the DIM network, we propose end-to-end Global-DIM and Local-DIM models. Additionally, the DIM network provides a new solution for cross-dataset unsupervised ReID issue as it needs no extra labels. The experiments prove the superiority of MI theory on the ReID issue, which achieves the state-of-the-art results.



### The Angel is in the Priors: Improving GAN based Image and Sequence Inpainting with Better Noise and Structural Priors
- **Arxiv ID**: http://arxiv.org/abs/1908.05861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05861v1)
- **Published**: 2019-08-16 06:30:40+00:00
- **Updated**: 2019-08-16 06:30:40+00:00
- **Authors**: Avisek Lahiri, Arnav Kumar Jain, Prabir Kumar Biswas
- **Comment**: None
- **Journal**: None
- **Summary**: Contemporary deep learning based inpainting algorithms are mainly based on a hybrid dual stage training policy of supervised reconstruction loss followed by an unsupervised adversarial critic loss. However, there is a dearth of literature for a fully unsupervised GAN based inpainting framework. The primary aversion towards the latter genre is due to its prohibitively slow iterative optimization requirement during inference to find a matching noise prior for a masked image. In this paper, we show that priors matter in GAN: we learn a data driven parametric network to predict a matching prior for a given image. This converts an iterative paradigm to a single feed forward inference pipeline with a massive 1500X speedup and simultaneous improvement in reconstruction quality. We show that an additional structural prior imposed on GAN model results in higher fidelity outputs. To extend our model for sequence inpainting, we propose a recurrent net based grouped noise prior learning. To our knowledge, this is the first demonstration of an unsupervised GAN based sequence inpainting. A further improvement in sequence inpainting is achieved with an additional subsequence consistency loss. These contributions improve the spatio-temporal characteristics of reconstructed sequences. Extensive experiments conducted on SVHN, Standford Cars, CelebA and CelebA-HQ image datasets, synthetic sequences and ViDTIMIT video datasets reveal that we consistently improve upon previous unsupervised baseline and also achieve comparable performances(sometimes also better) to hybrid benchmarks.



### 3D Rigid Motion Segmentation with Mixed and Unknown Number of Models
- **Arxiv ID**: http://arxiv.org/abs/1908.06087v1
- **DOI**: 10.1109/TPAMI.2019.2929146
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06087v1)
- **Published**: 2019-08-16 06:32:30+00:00
- **Updated**: 2019-08-16 06:32:30+00:00
- **Authors**: Xun Xu, Loong-Fah Cheong, Zhuwen Li
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI) 2019. arXiv admin note: substantial text overlap with
  arXiv:1804.02142
- **Journal**: None
- **Summary**: Many real-world video sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation on video sequences would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-model spectral clustering framework that synergistically combines multiple models (homography and fundamental matrix) together. We show that the performance can be substantially improved in this way. For general motion segmentation tasks, the number of independently moving objects is often unknown a priori and needs to be estimated from the observations. This is referred to as model selection and it is essentially still an open research problem. In this work, we propose a set of model selection criteria balancing data fidelity and model complexity. We perform extensive testing on existing motion segmentation datasets with both segmentation and model selection tasks, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.



### Differentiable Learning-to-Group Channels via Groupable Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.05867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.05867v2)
- **Published**: 2019-08-16 06:50:33+00:00
- **Updated**: 2019-08-19 08:06:53+00:00
- **Authors**: Zhaoyang Zhang, Jingyu Li, Wenqi Shao, Zhanglin Peng, Ruimao Zhang, Xiaogang Wang, Ping Luo
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Group convolution, which divides the channels of ConvNets into groups, has achieved impressive improvement over the regular convolution operation. However, existing models, eg. ResNeXt, still suffers from the sub-optimal performance due to manually defining the number of groups as a constant over all of the layers. Toward addressing this issue, we present Groupable ConvNet (GroupNet) built by using a novel dynamic grouping convolution (DGConv) operation, which is able to learn the number of groups in an end-to-end manner. The proposed approach has several appealing benefits. (1) DGConv provides a unified convolution representation and covers many existing convolution operations such as regular dense convolution, group convolution, and depthwise convolution. (2) DGConv is a differentiable and flexible operation which learns to perform various convolutions from training data. (3) GroupNet trained with DGConv learns different number of groups for different convolution layers. Extensive experiments demonstrate that GroupNet outperforms its counterparts such as ResNet and ResNeXt in terms of accuracy and computational complexity. We also present introspection and reproducibility study, for the first time, showing the learning dynamics of training group numbers.



### See Clearer at Night: Towards Robust Nighttime Semantic Segmentation through Day-Night Image Conversion
- **Arxiv ID**: http://arxiv.org/abs/1908.05868v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05868v1)
- **Published**: 2019-08-16 06:56:55+00:00
- **Updated**: 2019-08-16 06:56:55+00:00
- **Authors**: Lei Sun, Kaiwei Wang, Kailun Yang, Kaite Xiang
- **Comment**: 13 pages, 7 figures, 2 tables, 2 equations. Artificial Intelligence
  and Machine Learning in Defense Applications, SPIE Security + Defence 2019,
  Strasbourg, France, September 2019
- **Journal**: None
- **Summary**: Currently, semantic segmentation shows remarkable efficiency and reliability in standard scenarios such as daytime scenes with favorable illumination conditions. However, in face of adverse conditions such as the nighttime, semantic segmentation loses its accuracy significantly. One of the main causes of the problem is the lack of sufficient annotated segmentation datasets of nighttime scenes. In this paper, we propose a framework to alleviate the accuracy decline when semantic segmentation is taken to adverse conditions by using Generative Adversarial Networks (GANs). To bridge the daytime and nighttime image domains, we made key observation that compared to datasets in adverse conditions, there are considerable amount of segmentation datasets in standard conditions such as BDD and our collected ZJU datasets. Our GAN-based nighttime semantic segmentation framework includes two methods. In the first method, GANs were used to translate nighttime images to the daytime, thus semantic segmentation can be performed using robust models already trained on daytime datasets. In another method, we use GANs to translate different ratio of daytime images in the dataset to the nighttime but still with their labels. In this sense, synthetic nighttime segmentation datasets can be generated to yield models prepared to operate at nighttime conditions robustly. In our experiment, the later method significantly boosts the performance at the nighttime evidenced by quantitative results using Intersection over Union (IoU) and Pixel Accuracy (Acc). We show that the performance varies with respect to the proportion of synthetic nighttime images in the dataset, where the sweet spot corresponds to most robust performance across the day and night.



### Incorporating human and learned domain knowledge into training deep neural networks: A differentiable dose volume histogram and adversarial inspired framework for generating Pareto optimal dose distributions in radiation therapy
- **Arxiv ID**: http://arxiv.org/abs/1908.05874v2
- **DOI**: 10.1002/mp.13955
- **Categories**: **physics.med-ph**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05874v2)
- **Published**: 2019-08-16 07:45:38+00:00
- **Updated**: 2019-12-07 12:19:23+00:00
- **Authors**: Dan Nguyen, Rafe McBeth, Azar Sadeghnejad Barkousaraie, Gyanendra Bohara, Chenyang Shen, Xun Jia, Steve Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel domain specific loss, which is a differentiable loss function based on the dose volume histogram, and combine it with an adversarial loss for the training of deep neural networks to generate Pareto optimal dose distributions. The mean squared error (MSE) loss, dose volume histogram (DVH) loss, and adversarial (ADV) loss were used to train 4 instances of the neural network model: 1) MSE, 2) MSE+ADV, 3) MSE+DVH, and 4) MSE+DVH+ADV. 70 prostate patients were acquired, and the dose influence arrays were calculated for each patient. 1200 Pareto surface plans per patient were generated by pseudo-randomizing the tradeoff weights (84,000 plans total). We divided the data into 54 training, 6 validation, and 10 testing patients. Each model was trained for 100,000 iterations, with a batch size of 2. The prediction time of each model is 0.052 seconds. Quantitatively, the MSE+DVH+ADV model had the lowest prediction error of 0.038 (conformation), 0.026 (homogeneity), 0.298 (R50), 1.65% (D95), 2.14% (D98), 2.43% (D99). The MSE model had the worst prediction error of 0.134 (conformation), 0.041 (homogeneity), 0.520 (R50), 3.91% (D95), 4.33% (D98), 4.60% (D99). For both the mean dose PTV error and the max dose PTV, Body, Bladder and rectum error, the MSE+DVH+ADV outperformed all other models. All model's predictions have an average mean and max dose error less than 2.8% and 4.2%, respectively. Expert human domain specific knowledge can be the largest driver in the performance improvement, and adversarial learning can be used to further capture nuanced features. The real-time prediction capabilities allow for a physician to quickly navigate the tradeoff space, and produce a dose distribution as a tangible endpoint for the dosimetrist to use for planning. This can considerably reduce the treatment planning time, allowing for clinicians to focus their efforts on challenging cases.



### Zero-Shot Crowd Behavior Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.05877v1
- **DOI**: 10.1016/B978-0-12-809276-7.00018-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05877v1)
- **Published**: 2019-08-16 08:02:48+00:00
- **Updated**: 2019-08-16 08:02:48+00:00
- **Authors**: Xun Xu, Shaogang Gong, Timothy Hospedales
- **Comment**: Group and Crowd Behavior for Computer Vision 2017, Pages 341-369
- **Journal**: None
- **Summary**: Understanding crowd behavior in video is challenging for computer vision. There have been increasing attempts on modeling crowded scenes by introducing ever larger property ontologies (attributes) and annotating ever larger training datasets. However, in contrast to still images, manually annotating video attributes needs to consider spatiotemporal evolution which is inherently much harder and more costly. Critically, the most interesting crowd behaviors captured in surveillance videos (e.g., street fighting, flash mobs) are either rare, thus have few examples for model training, or unseen previously. Existing crowd analysis techniques are not readily scalable to recognize novel (unseen) crowd behaviors. To address this problem, we investigate and develop methods for recognizing visual crowd behavioral attributes without any training samples, i.e., zero-shot learning crowd behavior recognition. To that end, we relax the common assumption that each individual crowd video instance is only associated with a single crowd attribute. Instead, our model learns to jointly recognize multiple crowd behavioral attributes in each video instance by exploring multiattribute cooccurrence as contextual knowledge for optimizing individual crowd attribute recognition. Joint multilabel attribute prediction in zero-shot learning is inherently nontrivial because cooccurrence statistics does not exist for unseen attributes. To solve this problem, we learn to predict cross-attribute cooccurrence from both online text corpus and multilabel annotation of videos with known attributes. Our experiments show that this approach to modeling multiattribute context not only improves zero-shot crowd behavior recognition on the WWW crowd video dataset, but also generalizes to novel behavior (violence) detection cross-domain in the Violence Flow video dataset.



### GODS: Generalized One-class Discriminative Subspaces for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.05884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05884v1)
- **Published**: 2019-08-16 08:19:20+00:00
- **Updated**: 2019-08-16 08:19:20+00:00
- **Authors**: Jue Wang, Anoop Cherian
- **Comment**: Accepted by ICCV 2019, 8 pages
- **Journal**: None
- **Summary**: One-class learning is the classic problem of fitting a model to data for which annotations are available only for a single class. In this paper, we propose a novel objective for one-class learning. Our key idea is to use a pair of orthonormal frames -- as subspaces -- to "sandwich" the labeled data via optimizing for two objectives jointly: i) minimize the distance between the origins of the two subspaces, and ii) to maximize the margin between the hyperplanes and the data, either subspace demanding the data to be in its positive and negative orthant respectively. Our proposed objective however leads to a non-convex optimization problem, to which we resort to Riemannian optimization schemes and derive an efficient conjugate gradient scheme on the Stiefel manifold. To study the effectiveness of our scheme, we propose a new dataset~\emph{Dash-Cam-Pose}, consisting of clips with skeleton poses of humans seated in a car, the task being to classify the clips as normal or abnormal; the latter is when any human pose is out-of-position with regard to say an airbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as well as several other standard anomaly/novelty detection benchmarks demonstrate the benefits of our scheme, achieving state-of-the-art one-class accuracy.



### Multi-step Cascaded Networks for Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05887v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05887v3)
- **Published**: 2019-08-16 08:39:13+00:00
- **Updated**: 2019-09-25 11:03:46+00:00
- **Authors**: Xiangyu Li, Gongning Luo, Kuanquan Wang
- **Comment**: Paper for BraTS 2019 runs in conjunction with the MICCAI 2019
  conference
- **Journal**: None
- **Summary**: Automatic brain tumor segmentation method plays an extremely important role in the whole process of brain tumor diagnosis and treatment. In this paper, we propose a multi-step cascaded network which takes the hierarchical topology of the brain tumor substructures into consideration and segments the substructures from coarse to fine .During segmentation, the result of the former step is utilized as the prior information for the next step to guide the finer segmentation process. The whole network is trained in an end-to-end fashion. Besides, to alleviate the gradient vanishing issue and reduce overfitting, we added several auxiliary outputs as a kind of deep supervision for each step and introduced several data augmentation strategies, respectively, which proved to be quite efficient for brain tumor segmentation. Lastly, focal loss is utilized to solve the problem of remarkably imbalance of the tumor regions and background. Our model is tested on the BraTS 2019 validation dataset, the preliminary results of mean dice coefficients are 0.886, 0.813, 0.771 for the whole tumor, tumor core and enhancing tumor respectively. Code is available at https://github.com/JohnleeHIT/Brats2019



### Occlusion-shared and Feature-separated Network for Occlusion Relationship Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1908.05898v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05898v1)
- **Published**: 2019-08-16 09:09:50+00:00
- **Updated**: 2019-08-16 09:09:50+00:00
- **Authors**: Rui Lu, Feng Xue, Menghan Zhou, Anlong Ming, Yu Zhou
- **Comment**: Accepted by ICCV 2019. Code and pretrained model are available at
  https://github.com/buptlr/OFNet
- **Journal**: None
- **Summary**: Occlusion relationship reasoning demands closed contour to express the object, and orientation of each contour pixel to describe the order relationship between objects. Current CNN-based methods neglect two critical issues of the task: (1) simultaneous existence of the relevance and distinction for the two elements, i.e, occlusion edge and occlusion orientation; and (2) inadequate exploration to the orientation features. For the reasons above, we propose the Occlusion-shared and Feature-separated Network (OFNet). On one hand, considering the relevance between edge and orientation, two sub-networks are designed to share the occlusion cue. On the other hand, the whole network is split into two paths to learn the high-level semantic features separately. Moreover, a contextual feature for orientation prediction is extracted, which represents the bilateral cue of the foreground and background areas. The bilateral cue is then fused with the occlusion cue to precisely locate the object regions. Finally, a stripe convolution is designed to further aggregate features from surrounding scenes of the occlusion edge. The proposed OFNet remarkably advances the state-of-the-art approaches on PIOD and BSDS ownership dataset. The source code is available at https://github.com/buptlr/OFNet.



### Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network
- **Arxiv ID**: http://arxiv.org/abs/1908.05900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05900v2)
- **Published**: 2019-08-16 09:14:09+00:00
- **Updated**: 2020-08-02 03:38:04+00:00
- **Authors**: Wenhai Wang, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, Gang Yu, Chunhua Shen
- **Comment**: Accept by ICCV 2019
- **Journal**: None
- **Summary**: Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications.In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500.



### Context-Aware Emotion Recognition Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.05913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1908.05913v1)
- **Published**: 2019-08-16 09:59:15+00:00
- **Updated**: 2019-08-16 09:59:15+00:00
- **Authors**: Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, Kwanghoon Sohn
- **Comment**: International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to seperately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is more appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io.



### Empirical Bayesian Mixture Models for Medical Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1908.05926v1
- **DOI**: 10.1007/978-3-030-32778-1_1
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05926v1)
- **Published**: 2019-08-16 10:52:03+00:00
- **Updated**: 2019-08-16 10:52:03+00:00
- **Authors**: Mikael Brudfors, John Ashburner, Parashkev Nachev, Yael Balbastre
- **Comment**: Accepted to the Simulation and Synthesis in Medical Imaging (SASHIMI)
  workshop at MICCAI 2019
- **Journal**: None
- **Summary**: Automatically generating one medical imaging modality from another is known as medical image translation, and has numerous interesting applications. This paper presents an interpretable generative modelling approach to medical image translation. By allowing a common model for group-wise normalisation and segmentation of brain scans to handle missing data, the model allows for predicting entirely missing modalities from one, or a few, MR contrasts. Furthermore, the model can be trained on a fairly small number of subjects. The proposed model is validated on three clinically relevant scenarios. Results appear promising and show that a principled, probabilistic model of the relationship between multi-channel signal intensities can be used to infer missing modalities -- both MR contrasts and CT images.



### FSGAN: Subject Agnostic Face Swapping and Reenactment
- **Arxiv ID**: http://arxiv.org/abs/1908.05932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05932v1)
- **Published**: 2019-08-16 11:16:22+00:00
- **Updated**: 2019-08-16 11:16:22+00:00
- **Authors**: Yuval Nirkin, Yosi Keller, Tal Hassner
- **Comment**: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
- **Journal**: None
- **Summary**: We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.



### Gradient Weighted Superpixels for Interpretability in CNNs
- **Arxiv ID**: http://arxiv.org/abs/1908.08997v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08997v1)
- **Published**: 2019-08-16 12:02:25+00:00
- **Updated**: 2019-08-16 12:02:25+00:00
- **Authors**: Thomas Hartley, Kirill Sidorov, Christopher Willis, David Marshall
- **Comment**: Presented at BMVC 2019: Workshop on Interpretable and Explainable
  Machine Vision, Cardiff, UK
- **Journal**: None
- **Summary**: As Convolutional Neural Networks embed themselves into our everyday lives, the need for them to be interpretable increases. However, there is often a trade-off between methods that are efficient to compute but produce an explanation that is difficult to interpret, and those that are slow to compute but provide a more interpretable result. This is particularly challenging in problem spaces that require a large input volume, especially video which combines both spatial and temporal dimensions. In this work we introduce the idea of scoring superpixels through the use of gradient based pixel scoring techniques. We show qualitatively and quantitatively that this is able to approximate LIME, in a fraction of the time. We investigate our techniques using both image classification, and action recognition networks on large scale datasets (ImageNet and Kinetics-400 respectively).



### A Comparative Study of Filtering Approaches Applied to Color Archival Document Images
- **Arxiv ID**: http://arxiv.org/abs/1908.09007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.09007v1)
- **Published**: 2019-08-16 12:05:14+00:00
- **Updated**: 2019-08-16 12:05:14+00:00
- **Authors**: Walid Elhedda, Maroua Mehri, Mohamed Ali Mahjoub
- **Comment**: None
- **Journal**: None
- **Summary**: Current systems used by the Tunisian national archives for the automatic transcription of archival documents are hindered by many issues related to the performance of the optical character recognition (OCR) tools. Indeed, using a classical OCR system to transcribe and index ancient Arabic documents is not a straightforward task due to the idiosyncrasies of this category of documents, such as noise and degradation. Thus, applying an enhancement method or a denoising technique remains an essential prerequisite step to ease the archival document image analysis task. The state-of-the-art methods addressing the use of degraded document image enhancement and denoising are mainly based on applying filters. The most common filtering techniques applied to color images in the literature may be categorized into four approaches: scalar, marginal, vector and hybrid. To provide a set of comprehensive guidelines on the strengths and weaknesses of these filtering approaches, a thorough comparative study is proposed in this article. Numerical experiments are carried out in this study on color archival document images to show and quantify the performance of each assessed filtering approach.



### Multi-Domain Adaptation in Brain MRI through Paired Consistency and Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.05959v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05959v2)
- **Published**: 2019-08-16 13:06:18+00:00
- **Updated**: 2019-09-17 09:31:53+00:00
- **Authors**: Mauricio Orbes-Arteaga, Thomas Varsavsky, Carole H. Sudre, Zach Eaton-Rosen, Lewis J. Haddow, Lauge Sørensen, Mads Nielsen, Akshay Pai, Sébastien Ourselin, Marc Modat, Parashkev Nachev, M. Jorge Cardoso
- **Comment**: Accepted at 1st International Workshop on Domain Adaptation and
  Representation Transfer held at MICCAI 2019
- **Journal**: None
- **Summary**: Supervised learning algorithms trained on medical images will often fail to generalize across changes in acquisition parameters. Recent work in domain adaptation addresses this challenge and successfully leverages labeled data in a source domain to perform well on an unlabeled target domain. Inspired by recent work in semi-supervised learning we introduce a novel method to adapt from one source domain to $n$ target domains (as long as there is paired data covering all domains). Our multi-domain adaptation method utilises a consistency loss combined with adversarial learning. We provide results on white matter lesion hyperintensity segmentation from brain MRIs using the MICCAI 2017 challenge data as the source domain and two target domains. The proposed method significantly outperforms other domain adaptation baselines.



### Hyperparameter-Free Losses for Model-Based Monocular Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.09001v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.09001v1)
- **Published**: 2019-08-16 14:32:19+00:00
- **Updated**: 2019-08-16 14:32:19+00:00
- **Authors**: Eduard Ramon, Guillermo Ruiz, Thomas Batard, Xavier Giró-i-Nieto
- **Comment**: None
- **Journal**: None
- **Summary**: This work proposes novel hyperparameter-free losses for single view 3D reconstruction with morphable models (3DMM). We dispense with the hyperparameters used in other works by exploiting geometry, so that the shape of the object and the camera pose are jointly optimized in a sole term expression. This simplification reduces the optimization time and its complexity. Moreover, we propose a novel implicit regularization technique based on random virtual projections that does not require additional 2D or 3D annotations. Our experiments suggest that minimizing a shape reprojection error together with the proposed implicit regularization is especially suitable for applications that require precise alignment between geometry and image spaces, such as augmented reality. We evaluate our losses on a large scale dataset with 3D ground truth and publish our implementations to facilitate reproducibility and public benchmarking in this field.



### Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/1908.06752v1
- **DOI**: 10.1109/ICASSP.2019.8683318
- **Categories**: **cs.SD**, cs.CV, cs.LG, cs.MM, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1908.06752v1)
- **Published**: 2019-08-16 14:49:30+00:00
- **Updated**: 2019-08-16 14:49:30+00:00
- **Authors**: Aakanksha Rana, Cagri Ozcinar, Aljoscha Smolic
- **Comment**: ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)
- **Journal**: None
- **Summary**: Ambisonics i.e., a full-sphere surround sound, is quintessential with 360-degree visual content to provide a realistic virtual reality (VR) experience. While 360-degree visual content capture gained a tremendous boost recently, the estimation of corresponding spatial sound is still challenging due to the required sound-field microphones or information about the sound-source locations. In this paper, we introduce a novel problem of generating Ambisonics in 360-degree videos using the audio-visual cue. With this aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos is introduced with annotated sound-source locations. Secondly, a pipeline is designed for an automatic Ambisonic estimation problem. Benefiting from the deep learning-based audio-visual feature-embedding and prediction modules, our pipeline estimates the 3D sound-source locations and further use such locations to encode to the B-format. To benchmark our dataset and pipeline, we additionally propose evaluation criteria to investigate the performance using different 360-degree input representations. Our results demonstrate the efficacy of the proposed pipeline and open up a new area of research in 360-degree audio-visual analysis for future investigations.



### Regularizing CNN Transfer Learning with Randomised Regression
- **Arxiv ID**: http://arxiv.org/abs/1908.05997v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05997v2)
- **Published**: 2019-08-16 14:57:12+00:00
- **Updated**: 2020-04-28 09:50:27+00:00
- **Authors**: Yang Zhong, Atsuto Maki
- **Comment**: Accepted to CVPR 2020
- **Journal**: None
- **Summary**: This paper is about regularizing deep convolutional networks (CNNs) based on an adaptive framework for transfer learning with limited training data in the target domain. Recent advances of CNN regularization in this context are commonly due to the use of additional regularization objectives. They guide the training away from the target task using some forms of concrete tasks. Unlike those related approaches, we suggest that an objective without a concrete goal can still serve well as a regularized. In particular, we demonstrate Pseudo-task Regularization (PtR) which dynamically regularizes a network by simply attempting to regress image representations to pseudo-regression targets during fine-tuning. That is, a CNN is efficiently regularized without additional resources of data or prior domain expertise. In sum, the proposed PtR provides: a) an alternative for network regularization without dependence on the design of concrete regularization objectives or extra annotations; b) a dynamically adjusted and maintained strength of regularization effect by balancing the gradient norms between objectives on-line. Through numerous experiments, surprisingly, the improvements on classification accuracy by PtR are shown greater or on a par to the recent state-of-the-art methods.



### Visualization of Very Large High-Dimensional Data Sets as Minimum Spanning Trees
- **Arxiv ID**: http://arxiv.org/abs/1908.10410v3
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.DS, cs.IR, I.3.7, I.3.8, J.2, H.5.2, H.3.3, H.3.1, I.3.7; I.3.8; J.2; H.5.2; H.3.3; H.3.1
- **Links**: [PDF](http://arxiv.org/pdf/1908.10410v3)
- **Published**: 2019-08-16 15:14:19+00:00
- **Updated**: 2020-01-06 14:32:02+00:00
- **Authors**: Daniel Probst, Jean-Louis Reymond
- **Comment**: 33 pages, 14 figures, 1 table, supplementary information included
- **Journal**: None
- **Summary**: The chemical sciences are producing an unprecedented amount of large, high-dimensional data sets containing chemical structures and associated properties. However, there are currently no algorithms to visualize such data while preserving both global and local features with a sufficient level of detail to allow for human inspection and interpretation. Here, we propose a solution to this problem with a new data visualization method, TMAP, capable of representing data sets of up to millions of data points and arbitrary high dimensionality as a two-dimensional tree (http://tmap.gdb.tools). Visualizations based on TMAP are better suited than t-SNE or UMAP for the exploration and interpretation of large data sets due to their tree-like nature, increased local and global neighborhood and structure preservation, and the transparency of the methods the algorithm is based on. We apply TMAP to the most used chemistry data sets including databases of molecules such as ChEMBL, FDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet benchmark collection of data sets. We also show its broad applicability with further examples from biology, particle physics, and literature.



### SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1908.06022v6
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06022v6)
- **Published**: 2019-08-16 15:31:08+00:00
- **Updated**: 2021-08-14 13:37:59+00:00
- **Authors**: Xiangxiang Chu, Bo Zhang, Qingyuan Li, Ruijun Xu, Xudong Li
- **Comment**: ICCV Workshop 2021
- **Journal**: None
- **Summary**: To discover powerful yet compact models is an important goal of neural architecture search. Previous two-stage one-shot approaches are limited by search space with a fixed depth. It seems handy to include an additional skip connection in the search space to make depths variable. However, it creates a large range of perturbation during supernet training and it has difficulty giving a confident ranking for subnetworks. In this paper, we discover that skip connections bring about significant feature inconsistency compared with other operations, which potentially degrades the supernet performance. Based on this observation, we tackle the problem by imposing an equivariant learnable stabilizer to homogenize such disparities. Experiments show that our proposed stabilizer helps to improve the supernet's convergence as well as ranking performance. With an evolutionary search backend that incorporates the stabilized supernet as an evaluator, we derive a family of state-of-the-art architectures, the SCARLET series of several depths, especially SCARLET-A obtains 76.9% top-1 accuracy on ImageNet. Code is available at https://github.com/xiaomi-automl/ScarletNAS.



### Needles in Haystacks: On Classifying Tiny Objects in Large Images
- **Arxiv ID**: http://arxiv.org/abs/1908.06037v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06037v2)
- **Published**: 2019-08-16 15:42:55+00:00
- **Updated**: 2020-01-06 13:13:07+00:00
- **Authors**: Nick Pawlowski, Suvrat Bhooshan, Nicolas Ballas, Francesco Ciompi, Ben Glocker, Michal Drozdzal
- **Comment**: None
- **Journal**: None
- **Summary**: In some important computer vision domains, such as medical or hyperspectral imaging, we care about the classification of tiny objects in large images. However, most Convolutional Neural Networks (CNNs) for image classification were developed using biased datasets that contain large objects, in mostly central image positions. To assess whether classical CNN architectures work well for tiny object classification we build a comprehensive testbed containing two datasets: one derived from MNIST digits and one from histopathology images. This testbed allows controlled experiments to stress-test CNN architectures with a broad spectrum of signal-to-noise ratios. Our observations indicate that: (1) There exists a limit to signal-to-noise below which CNNs fail to generalize and that this limit is affected by dataset size - more data leading to better performances; however, the amount of training data required for the model to generalize scales rapidly with the inverse of the object-to-image ratio (2) in general, higher capacity models exhibit better generalization; (3) when knowing the approximate object sizes, adapting receptive field is beneficial; and (4) for very small signal-to-noise ratio the choice of global pooling operation affects optimization, whereas for relatively large signal-to-noise values, all tested global pooling operations exhibit similar performance.



### Robust Principal Component Analysis for Background Estimation of Particle Image Velocimetry Data
- **Arxiv ID**: http://arxiv.org/abs/1908.06047v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06047v1)
- **Published**: 2019-08-16 16:28:20+00:00
- **Updated**: 2019-08-16 16:28:20+00:00
- **Authors**: Ahmadreza Baghaie
- **Comment**: Presented in LISAT 2019
- **Journal**: None
- **Summary**: Particle Image Velocimetry (PIV) data processing procedures are adversely affected by light reflections and backgrounds as well as defects in the models and sticky particles that occlude the inner walls of the boundaries. In this paper, a novel approach is proposed for decomposition of the PIV data into background/foreground components, greatly reducing the effects of such artifacts. This is achieved by utilizing Robust Principal Component Analysis (RPCA) applied to the data matrix, generated by aggregating the vectorized PIV frames. It is assumed that the data matrix can be decomposed into two statistically different components, a low-rank component depicting the still background and a sparse component representing the moving particles within the imaged geometry. Formulating the assumptions as an optimization problem, Augmented Lagrange Multiplier (ALM) method is used for decomposing the data matrix into the low-rank and sparse components. Experiments and comparisons with the state-of-the-art using several PIV image sequences reveal the superiority of the proposed approach for background removal of PIV data.



### Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.06052v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06052v1)
- **Published**: 2019-08-16 16:39:20+00:00
- **Updated**: 2019-08-16 16:39:20+00:00
- **Authors**: Yu-Jhe Li, Yun-Chun Chen, Yen-Yu Lin, Xiaofei Du, Yu-Chiang Frank Wang
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.



### Survey on Deep Neural Networks in Speech and Vision Systems
- **Arxiv ID**: http://arxiv.org/abs/1908.07656v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, cs.SD, eess.AS, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.07656v2)
- **Published**: 2019-08-16 16:40:49+00:00
- **Updated**: 2019-12-01 03:30:37+00:00
- **Authors**: Mahbubul Alam, Manar D. Samad, Lasitha Vidyaratne, Alexander Glandon, Khan M. Iftekharuddin
- **Comment**: None
- **Journal**: None
- **Summary**: This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.



### Adversarial shape perturbations on 3D point clouds
- **Arxiv ID**: http://arxiv.org/abs/1908.06062v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06062v3)
- **Published**: 2019-08-16 17:19:34+00:00
- **Updated**: 2020-10-23 04:55:16+00:00
- **Authors**: Daniel Liu, Ronald Yu, Hao Su
- **Comment**: 18 pages, accepted to the 2020 ECCV workshop on Adversarial
  Robustness in the Real World, source code available at this https url:
  https://github.com/Daniel-Liu-c0deb0t/Adversarial-point-perturbations-on-3D-objects
- **Journal**: None
- **Summary**: The importance of training robust neural network grows as 3D data is increasingly utilized in deep learning for vision tasks in robotics, drone control, and autonomous driving. One commonly used 3D data type is 3D point clouds, which describe shape information. We examine the problem of creating robust models from the perspective of the attacker, which is necessary in understanding how 3D neural networks can be exploited. We explore two categories of attacks: distributional attacks that involve imperceptible perturbations to the distribution of points, and shape attacks that involve deforming the shape represented by a point cloud. We explore three possible shape attacks for attacking 3D point cloud classification and show that some of them are able to be effective even against preprocessing steps, like the previously proposed point-removal defenses.



### Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training
- **Arxiv ID**: http://arxiv.org/abs/1908.06066v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06066v3)
- **Published**: 2019-08-16 17:26:56+00:00
- **Updated**: 2019-12-02 10:15:38+00:00
- **Authors**: Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou
- **Comment**: accepted by AAAI-2020. arXiv admin note: text overlap with
  arXiv:1909.11740 by other authors
- **Journal**: None
- **Summary**: We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM and Unicoder, both visual and linguistic contents are fed into a multi-layer Transformer for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling (MLM), Masked Object Classification (MOC) and Visual-linguistic Matching (VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the cross-modal pre-training.



### Task-Assisted Domain Adaptation with Anchor Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.06079v3
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06079v3)
- **Published**: 2019-08-16 17:59:18+00:00
- **Updated**: 2020-11-10 01:46:18+00:00
- **Authors**: Zhizhong Li, Linjie Luo, Sergey Tulyakov, Qieyun Dai, Derek Hoiem
- **Comment**: In WACV 2021
- **Journal**: None
- **Summary**: Some tasks, such as surface normals or single-view depth estimation, require per-pixel ground truth that is difficult to obtain on real images but easy to obtain on synthetic. However, models learned on synthetic images often do not generalize well to real images due to the domain shift. Our key idea to improve domain adaptation is to introduce a separate anchor task (such as facial landmarks) whose annotations can be obtained at no cost or are already available on both synthetic and real datasets. To further leverage the implicit relationship between the anchor and main tasks, we apply our \freeze technique that learns the cross-task guidance on the source domain with the final network layers, and use it on the target domain. We evaluate our methods on surface normal estimation on two pairs of datasets (indoor scenes and faces) with two kinds of anchor tasks (semantic segmentation and facial landmarks). We show that blindly applying domain adaptation or training the auxiliary task on only one domain may hurt performance, while using anchor tasks on both domains is better behaved. Our \freeze technique outperforms competing approaches, reaching performance in facial images on par with a recently popular surface normal estimation method using shape from shading domain knowledge.



### RIO: 3D Object Instance Re-Localization in Changing Indoor Environments
- **Arxiv ID**: http://arxiv.org/abs/1908.06109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06109v1)
- **Published**: 2019-08-16 18:00:41+00:00
- **Updated**: 2019-08-16 18:00:41+00:00
- **Authors**: Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias Nießner
- **Comment**: ICCV 2019 (Oral) video https://youtu.be/367CeZtrEYM
- **Journal**: None
- **Summary**: In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%.



### Symmetric Cross Entropy for Robust Learning with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1908.06112v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06112v1)
- **Published**: 2019-08-16 18:01:32+00:00
- **Updated**: 2019-08-16 18:01:32+00:00
- **Authors**: Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, James Bailey
- **Comment**: ICCV2019
- **Journal**: None
- **Summary**: Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes ("easy" classes), but more surprisingly, it also suffers from significant under learning on some other classes ("hard" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of \textbf{Symmetric cross entropy Learning} (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.



### Large Scale Organization and Inference of an Imagery Dataset for Public Safety
- **Arxiv ID**: http://arxiv.org/abs/1908.09006v1
- **DOI**: 10.1109/HPEC.2019.8916437
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09006v1)
- **Published**: 2019-08-16 18:20:01+00:00
- **Updated**: 2019-08-16 18:20:01+00:00
- **Authors**: Jeffrey Liu, David Strohschein, Siddharth Samsi, Andrew Weinert
- **Comment**: Accepted for publication IEEE HPEC 2019
- **Journal**: None
- **Summary**: Video applications and analytics are routinely projected as a stressing and significant service of the Nationwide Public Safety Broadband Network. As part of a NIST PSCR funded effort, the New Jersey Office of Homeland Security and Preparedness and MIT Lincoln Laboratory have been developing a computer vision dataset of operational and representative public safety scenarios. The scale and scope of this dataset necessitates a hierarchical organization approach for efficient compute and storage. We overview architectural considerations using the Lincoln Laboratory Supercomputing Cluster as a test architecture. We then describe how we intelligently organized the dataset across LLSC and evaluated it with large scale imagery inference across terabytes of data.



### Multiple Light Source Dataset for Colour Research
- **Arxiv ID**: http://arxiv.org/abs/1908.06126v4
- **DOI**: 10.1117/12.2559491
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06126v4)
- **Published**: 2019-08-16 18:49:32+00:00
- **Updated**: 2019-10-27 10:42:04+00:00
- **Authors**: Anna Smagina, Egor Ershov, Anton Grigoryev
- **Comment**: None
- **Journal**: Proceedings Volume 11433, Twelfth International Conference on
  Machine Vision (ICMV 2019); 114332C
- **Summary**: We present a collection of 24 multiple object scenes each recorded under 18 multiple light source illumination scenarios. The illuminants are varying in dominant spectral colours, intensity and distance from the scene. We mainly address the realistic scenarios for evaluation of computational colour constancy algorithms, but also have aimed to make the data as general as possible for computational colour science and computer vision. Along with the images of the scenes, we provide spectral characteristics of the camera, light sources and the objects and include pixel-by-pixel ground truth annotation of uniformly coloured object surfaces thus making this useful for benchmarking colour-based image segmentation algorithms. The dataset is freely available at https://github.com/visillect/mls-dataset.



### Cascaded Parallel Filtering for Memory-Efficient Image-Based Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.06141v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06141v1)
- **Published**: 2019-08-16 19:39:17+00:00
- **Updated**: 2019-08-16 19:39:17+00:00
- **Authors**: Wentao Cheng, Weisi Lin, Kan Chen, Xinfeng Zhang
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner.



### Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.06965v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06965v2)
- **Published**: 2019-08-16 19:59:01+00:00
- **Updated**: 2019-08-29 16:24:54+00:00
- **Authors**: Md Mahfuzur Rahman Siddiquee, Zongwei Zhou, Nima Tajbakhsh, Ruibin Feng, Michael B. Gotway, Yoshua Bengio, Jianming Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN "virtually heal" anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN.



### TunaGAN: Interpretable GAN for Smart Editing
- **Arxiv ID**: http://arxiv.org/abs/1908.06163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06163v1)
- **Published**: 2019-08-16 20:57:57+00:00
- **Updated**: 2019-08-16 20:57:57+00:00
- **Authors**: Weiquan Mao, Beicheng Lou, Jiyao Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a tunable generative adversary network (TunaGAN) that uses an auxiliary network on top of existing generator networks (Style-GAN) to modify high-resolution face images according to user's high-level instructions, with good qualitative and quantitative performance. To optimize for feature disentanglement, we also investigate two different latent space that could be traversed for modification. The problem of mode collapse is characterized in detail for model robustness. This work could be easily extended to content-aware image editor based on other GANs and provide insight on mode collapse problems in more general settings.



### Detecting abnormalities in resting-state dynamics: An unsupervised learning approach
- **Arxiv ID**: http://arxiv.org/abs/1908.06168v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.06168v1)
- **Published**: 2019-08-16 21:03:08+00:00
- **Updated**: 2019-08-16 21:03:08+00:00
- **Authors**: Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R. Sabuncu
- **Comment**: 9 pages, 3 figures
- **Journal**: None
- **Summary**: Resting-state functional MRI (rs-fMRI) is a rich imaging modality that captures spontaneous brain activity patterns, revealing clues about the connectomic organization of the human brain. While many rs-fMRI studies have focused on static measures of functional connectivity, there has been a recent surge in examining the temporal patterns in these data. In this paper, we explore two strategies for capturing the normal variability in resting-state activity across a healthy population: (a) an autoencoder approach on the rs-fMRI sequence, and (b) a next frame prediction strategy. We show that both approaches can learn useful representations of rs-fMRI data and demonstrate their novel application for abnormality detection in the context of discriminating autism patients from healthy controls.



### Applying Adversarial Auto-encoder for Estimating Human Walking Gait Abnormality Index
- **Arxiv ID**: http://arxiv.org/abs/1908.06188v1
- **DOI**: 10.1007/s10044-019-00790-7
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06188v1)
- **Published**: 2019-08-16 21:52:04+00:00
- **Updated**: 2019-08-16 21:52:04+00:00
- **Authors**: Trong-Nguyen Nguyen, Jean Meunier
- **Comment**: None
- **Journal**: Pattern Analysis and Applications (2019) 1-12
- **Summary**: This paper proposes an approach that estimates human walking gait quality index using an adversarial auto-encoder (AAE), i.e. a combination of auto-encoder and generative adversarial network (GAN). Since most GAN-based models have been employed as data generators, our work introduces another perspective of their application. This method directly works on a sequence of 3D point clouds representing the walking postures of a subject. By fitting a cylinder onto each point cloud and feeding obtained histograms to an appropriate AAE, our system is able to provide different measures that may be used as gait quality indices. The combinations of such quantities are also investigated to obtain improved indicators. The ability of our method is demonstrated by experimenting on a large dataset of nearly 100 thousands point clouds and the results outperform related approaches that employ different input data types.



### Conv2Warp: An unsupervised deformable image registration with continuous convolution and warping
- **Arxiv ID**: http://arxiv.org/abs/1908.06194v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06194v1)
- **Published**: 2019-08-16 22:21:07+00:00
- **Updated**: 2019-08-16 22:21:07+00:00
- **Authors**: Sharib Ali, Jens Rittscher
- **Comment**: 8 pages (accepted at 10th International Workshop on Machine Learning
  in Medical Imaging, in conjunction with MICCAI2019)
- **Journal**: None
- **Summary**: Recent successes in deep learning based deformable image registration (DIR) methods have demonstrated that complex deformation can be learnt directly from data while reducing computation time when compared to traditional methods. However, the reliance on fully linear convolutional layers imposes a uniform sampling of pixel/voxel locations which ultimately limits their performance. To address this problem, we propose a novel approach of learning a continuous warp of the source image. Here, the required deformation vector fields are obtained from a concatenated linear and non-linear convolution layers and a learnable bicubic Catmull-Rom spline resampler. This allows to compute smooth deformation field and more accurate alignment compared to using only linear convolutions and linear resampling. In addition, the continuous warping technique penalizes disagreements that are due to topological changes. Our experiments demonstrate that this approach manages to capture large non-linear deformations and minimizes the propagation of interpolation errors. While improving accuracy the method is computationally efficient. We present comparative results on a range of public 4D CT lung (POPI) and brain datasets (CUMC12, MGH10).



### Human Gait Symmetry Assessment using a Depth Camera and Mirrors
- **Arxiv ID**: http://arxiv.org/abs/1908.07422v1
- **DOI**: 10.1016/j.compbiomed.2018.08.021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07422v1)
- **Published**: 2019-08-16 23:27:22+00:00
- **Updated**: 2019-08-16 23:27:22+00:00
- **Authors**: Trong-Nguyen Nguyen, Huu-Hung Huynh, Jean Meunier
- **Comment**: None
- **Journal**: Computers in Biology and Medicine 101 (2018) 174-183
- **Summary**: This paper proposes a reliable approach for human gait symmetry assessment using a depth camera and two mirrors. The input of our system is a sequence of 3D point clouds which are formed from a setup including a Time-of-Flight (ToF) depth camera and two mirrors. A cylindrical histogram is estimated for describing the posture in each point cloud. The sequence of such histograms is then separated into two sequences of sub-histograms representing two half-bodies. A cross-correlation technique is finally applied to provide values describing gait symmetry indices. The evaluation was performed on 9 different gait types to demonstrate the ability of our approach in assessing gait symmetry. A comparison between our system and related methods, that employ different input data types, is also provided.



