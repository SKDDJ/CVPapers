# Arxiv Papers in cs.CV on 2019-08-28
### O-MedAL: Online Active Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.10508v2
- **DOI**: 10.1002/widm.1353
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10508v2)
- **Published**: 2019-08-28 00:48:12+00:00
- **Updated**: 2020-07-27 20:53:28+00:00
- **Authors**: Asim Smailagic, Pedro Costa, Alex Gaudio, Kartik Khandelwal, Mostafa Mirshekari, Jonathon Fagert, Devesh Walawalkar, Susu Xu, Adrian Galdran, Pei Zhang, Aur√©lio Campilho, Hae Young Noh
- **Comment**: Code: https://github.com/adgaudio/o-medal ; Accepted and published by
  Wiley Journal of Pattern Recognition and Knowledge Discovery ; Journal URL:
  https://doi.org/10.1002/widm.1353
- **Journal**: Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery 10.4 (2020): e1353
- **Summary**: Active Learning methods create an optimized labeled training set from unlabeled data. We introduce a novel Online Active Deep Learning method for Medical Image Analysis. We extend our MedAL active learning framework to present new results in this paper. Our novel sampling method queries the unlabeled examples that maximize the average distance to all training set examples. Our online method enhances performance of its underlying baseline deep network. These novelties contribute significant performance improvements, including improving the model's underlying deep network accuracy by 6.30%, using only 25% of the labeled dataset to achieve baseline accuracy, reducing backpropagated images during training by as much as 67%, and demonstrating robustness to class imbalance in binary and multi-class tasks.



### A Coarse-to-Fine Multi-stream Hybrid Deraining Network for Single Image Deraining
- **Arxiv ID**: http://arxiv.org/abs/1908.10521v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10521v1)
- **Published**: 2019-08-28 02:05:36+00:00
- **Updated**: 2019-08-28 02:05:36+00:00
- **Authors**: Yanyan Wei, Zhao Zhang, Haijun Zhang, Richang Hong, Meng Wang
- **Comment**: Accepted by ICDM 2019 as a regular paper
- **Journal**: None
- **Summary**: Single image deraining task is still a very challenging task due to its ill-posed nature in reality. Recently, researchers have tried to fix this issue by training the CNN-based end-to-end models, but they still cannot extract the negative rain streaks from rainy images precisely, which usually leads to an over de-rained or under de-rained result. To handle this issue, this paper proposes a new coarse-to-fine single image deraining framework termed Multi-stream Hybrid Deraining Network (shortly, MH-DerainNet). To obtain the negative rain streaks during training process more accurately, we present a new module named dual path residual dense block, i.e., Residual path and Dense path. The Residual path is used to reuse com-mon features from the previous layers while the Dense path can explore new features. In addition, to concatenate different scaled features, we also apply the idea of multi-stream with shortcuts between cascaded dual path residual dense block based streams. To obtain more distinct derained images, we combine the SSIM loss and perceptual loss to preserve the per-pixel similarity as well as preserving the global structures so that the deraining result is more accurate. Extensive experi-ments on both synthetic and real rainy images demonstrate that our MH-DerainNet can deliver significant improvements over several recent state-of-the-art methods.



### Image Harmonization Dataset iHarmony4: HCOCO, HAdobe5k, HFlickr, and Hday2night
- **Arxiv ID**: http://arxiv.org/abs/1908.10526v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10526v4)
- **Published**: 2019-08-28 02:44:13+00:00
- **Updated**: 2020-03-20 12:36:30+00:00
- **Authors**: Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, Liqing Zhang
- **Comment**: Our full paper arXiv:1911.13239 "DoveNet: Deep Image Harmonization
  via Domain Verification" is accepted by CVPR2020
- **Journal**: None
- **Summary**: Image composition is an important operation in image processing, but the inconsistency between foreground and background significantly degrades the quality of composite image. Image harmonization, which aims to make the foreground compatible with the background, is a promising yet challenging task. However, the lack of high-quality public dataset for image harmonization, which significantly hinders the development of image harmonization techniques. Therefore, we contribute an image harmonization dataset iHarmony4 by generating synthesized composite images based on existing COCO (resp., Adobe5k, day2night) dataset, leading to our HCOCO (resp., HAdobe5k, Hday2night) sub-dataset. To enrich the diversity of our dataset, we also generate synthesized composite images based on our collected Flick images, leading to our HFlickr sub-dataset. The image harmonization dataset iHarmony4 is released at https://github.com/bcmi/Image_Harmonization_Datasets.



### Adversarial Representation Learning for Text-to-Image Matching
- **Arxiv ID**: http://arxiv.org/abs/1908.10534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10534v1)
- **Published**: 2019-08-28 03:25:13+00:00
- **Updated**: 2019-08-28 03:25:13+00:00
- **Authors**: Nikolaos Sarafianos, Xiang Xu, Ioannis A. Kakadiaris
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy.



### Push for Center Learning via Orthogonalization and Subspace Masking for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.10535v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10535v2)
- **Published**: 2019-08-28 03:32:57+00:00
- **Updated**: 2019-12-11 05:56:37+00:00
- **Authors**: Weinong Wang, Wenjie Pei, Qiong Cao, Shu Liu, Yu-Wing Tai
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification aims to identify whether pairs of images belong to the same person or not. This problem is challenging due to large differences in camera views, lighting and background. One of the mainstream in learning CNN features is to design loss functions which reinforce both the class separation and intra-class compactness. In this paper, we propose a novel Orthogonal Center Learning method with Subspace Masking for person re-identification. We make the following contributions: (i) we develop a center learning module to learn the class centers by simultaneously reducing the intra-class differences and inter-class correlations by orthogonalization; (ii) we introduce a subspace masking mechanism to enhance the generalization of the learned class centers; and (iii) we devise to integrate the average pooling and max pooling in a regularizing manner that fully exploits their powers. Extensive experiments show that our proposed method consistently outperforms the state-of-the-art methods on the large-scale ReID datasets including Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17.



### Inception-inspired LSTM for Next-frame Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1909.05622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05622v2)
- **Published**: 2019-08-28 03:49:07+00:00
- **Updated**: 2020-04-24 17:06:36+00:00
- **Authors**: Matin Hosseini, Anthony S. Maida, Majid Hosseini, Gottumukkala Raju
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of video frame prediction has received much interest due to its relevance to many computer vision applications such as autonomous vehicles or robotics. Supervised methods for video frame prediction rely on labeled data, which may not always be available. In this paper, we provide a novel unsupervised deep-learning method called Inception-based LSTM for video frame prediction. The general idea of inception networks is to implement wider networks instead of deeper networks. This network design was shown to improve the performance of image classification. The proposed method is evaluated on both Inception-v1 and Inception-v2 structures. The proposed Inception LSTM methods are compared with convolutional LSTM when applied using PredNet predictive coding framework for both the KITTI and KTH data sets. We observed that the Inception based LSTM outperforms the convolutional LSTM. Also, Inception LSTM has better prediction performance compared to Inception v2 LSTM. However, Inception v2 LSTM has a lower computational cost compared to Inception LSTM.



### SPair-71k: A Large-scale Benchmark for Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/1908.10543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10543v1)
- **Published**: 2019-08-28 04:16:52+00:00
- **Updated**: 2019-08-28 04:16:52+00:00
- **Authors**: Juhong Min, Jongmin Lee, Jean Ponce, Minsu Cho
- **Comment**: Extension of ICCV 2019 paper, Hyperpixel Flow: Semantic
  Correspondence with Multi-layer Neural Features. arXiv admin note: text
  overlap with arXiv:1908.06537
- **Journal**: None
- **Summary**: Establishing visual correspondences under large intra-class variations, which is often referred to as semantic correspondence or semantic matching, remains a challenging problem in computer vision. Despite its significance, however, most of the datasets for semantic correspondence are limited to a small amount of image pairs with similar viewpoints and scales. In this paper, we present a new large-scale benchmark dataset of semantically paired images, SPair-71k, which contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations. We believe this dataset will provide a reliable testbed to study the problem of semantic correspondence and will help to advance research in this area. We provide the results of recent methods on our new dataset as baselines for further research. Our benchmark is available online at http://cvlab.postech.ac.kr/research/SPair-71k/.



### Fingerspelling recognition in the wild with iterative visual attention
- **Arxiv ID**: http://arxiv.org/abs/1908.10546v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.10546v1)
- **Published**: 2019-08-28 04:52:32+00:00
- **Updated**: 2019-08-28 04:52:32+00:00
- **Authors**: Bowen Shi, Aurora Martinez Del Rio, Jonathan Keane, Diane Brentari, Greg Shakhnarovich, Karen Livescu
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It outperforms prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.



### A Global-Local Emebdding Module for Fashion Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.10548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10548v1)
- **Published**: 2019-08-28 04:57:12+00:00
- **Updated**: 2019-08-28 04:57:12+00:00
- **Authors**: Sumin Lee, Sungchan Oh, Chanho Jung, Changick Kim
- **Comment**: Accepted to ICCV 2019 Workshop: Computer Vision for Fashion, Art and
  Design
- **Journal**: None
- **Summary**: Detecting fashion landmarks is a fundamental technique for visual clothing analysis. Due to the large variation and non-rigid deformation of clothes, localizing fashion landmarks suffers from large spatial variances across poses, scales, and styles. Therefore, understanding contextual knowledge of clothes is required for accurate landmark detection. To that end, in this paper, we propose a fashion landmark detection network with a global-local embedding module. The global-local embedding module is based on a non-local operation for capturing long-range dependencies and a subsequent convolution operation for adopting local neighborhood relations. With this processing, the network can consider both global and local contextual knowledge for a clothing image. We demonstrate that our proposed method has an excellent ability to learn advanced deep feature representations for fashion landmark detection. Experimental results on two benchmark datasets show that the proposed network outperforms the state-of-the-art methods. Our code is available at https://github.com/shumming/GLE_FLD.



### Heterogeneous Domain Adaptation via Soft Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1908.10552v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10552v1)
- **Published**: 2019-08-28 05:25:12+00:00
- **Updated**: 2019-08-28 05:25:12+00:00
- **Authors**: Yuan Yao, Yu Zhang, Xutao Li, Yunming Ye
- **Comment**: Accepted by ACM Multimedia (ACM MM) 2019
- **Journal**: None
- **Summary**: Heterogeneous domain adaptation (HDA) aims to facilitate the learning task in a target domain by borrowing knowledge from a heterogeneous source domain. In this paper, we propose a Soft Transfer Network (STN), which jointly learns a domain-shared classifier and a domain-invariant subspace in an end-to-end manner, for addressing the HDA problem. The proposed STN not only aligns the discriminative directions of domains but also matches both the marginal and conditional distributions across domains. To circumvent negative transfer, STN aligns the conditional distributions by using the soft-label strategy of unlabeled target data, which prevents the hard assignment of each unlabeled target data to only one category that may be incorrect. Further, STN introduces an adaptive coefficient to gradually increase the importance of the soft-labels since they will become more and more accurate as the number of iterations increases. We perform experiments on the transfer tasks of image-to-image, text-to-image, and text-to-text. Experimental results testify that the STN significantly outperforms several state-of-the-art approaches.



### Unsupervised Scale-consistent Depth and Ego-motion Learning from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1908.10553v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10553v2)
- **Published**: 2019-08-28 05:25:46+00:00
- **Updated**: 2019-10-03 00:34:14+00:00
- **Authors**: Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, Ian Reid
- **Comment**: Accepted to NeurIPS 2019. Code is available at
  https://github.com/JiawangBian/SC-SfMLearner-Release
- **Journal**: None
- **Summary**: Recent work has shown that CNN-based depth and ego-motion estimators can be learned using unlabelled monocular videos. However, the performance is limited by unidentified moving objects that violate the underlying static scene assumption in geometric image reconstruction. More significantly, due to lack of proper constraints, networks output scale-inconsistent results over different samples, i.e., the ego-motion network cannot provide full camera trajectories over a long video sequence because of the per-frame scale ambiguity. This paper tackles these challenges by proposing a geometry consistency loss for scale-consistent predictions and an induced self-discovered mask for handling moving objects and occlusions. Since we do not leverage multi-task learning like recent works, our framework is much simpler and more efficient. Comprehensive evaluation results demonstrate that our depth estimator achieves the state-of-the-art performance on the KITTI dataset. Moreover, we show that our ego-motion network is able to predict a globally scale-consistent camera trajectory for long video sequences, and the resulting visual odometry accuracy is competitive with the recent model that is trained using stereo videos. To the best of our knowledge, this is the first work to show that deep networks trained using unlabelled monocular videos can predict globally scale-consistent camera trajectories over a long video sequence.



### CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.10555v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10555v1)
- **Published**: 2019-08-28 05:32:07+00:00
- **Updated**: 2019-08-28 05:32:07+00:00
- **Authors**: Gang Xu, Zhigang Song, Zhuo Sun, Calvin Ku, Zhe Yang, Cancheng Liu, Shuhao Wang, Jianpeng Ma, Wei Xu
- **Comment**: 10 pages, 9 figures, accepted by ICCV 2019
- **Journal**: None
- **Summary**: Histopathology image analysis plays a critical role in cancer diagnosis and treatment. To automatically segment the cancerous regions, fully supervised segmentation algorithms require labor-intensive and time-consuming labeling at the pixel level. In this research, we propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. CAMEL achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the automatic labeling methodology may benefit future weakly supervised learning studies for histopathology image analysis.



### Online Sensor Hallucination via Knowledge Distillation for Multimodal Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.10559v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10559v1)
- **Published**: 2019-08-28 05:55:09+00:00
- **Updated**: 2019-08-28 05:55:09+00:00
- **Authors**: Saurabh Kumar, Biplab Banerjee, Subhasis Chaudhuri
- **Comment**: Preprint: Manuscript under revision
- **Journal**: None
- **Summary**: We deal with the problem of information fusion driven satellite image/scene classification and propose a generic hallucination architecture considering that all the available sensor information are present during training while some of the image modalities may be absent while testing. It is well-known that different sensors are capable of capturing complementary information for a given geographical area and a classification module incorporating information from all the sources are expected to produce an improved performance as compared to considering only a subset of the modalities. However, the classical classifier systems inherently require all the features used to train the module to be present for the test instances as well, which may not always be possible for typical remote sensing applications (say, disaster management). As a remedy, we provide a robust solution in terms of a hallucination module that can approximate the missing modalities from the available ones during the decision-making stage. In order to ensure better knowledge transfer during modality hallucination, we explicitly incorporate concepts of knowledge distillation for the purpose of exploring the privileged (side) information in our framework and subsequently introduce an intuitive modular training approach. The proposed network is evaluated extensively on a large-scale corpus of PAN-MS image pairs (scene recognition) as well as on a benchmark hyperspectral image dataset (image classification) where we follow different experimental scenarios and find that the proposed hallucination based module indeed is capable of capturing the multi-source information, albeit the explicit absence of some of the sensor information, and aid in improved scene characterization.



### Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding
- **Arxiv ID**: http://arxiv.org/abs/1908.10568v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10568v1)
- **Published**: 2019-08-28 06:49:54+00:00
- **Updated**: 2019-08-28 06:49:54+00:00
- **Authors**: Xuejing Liu, Liang Li, Shuhui Wang, Zheng-Jun Zha, Dechao Meng, Qingming Huang
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Weakly supervised referring expression grounding aims at localizing the referential object in an image according to the linguistic query, where the mapping between the referential object and query is unknown in the training stage. To address this problem, we propose a novel end-to-end adaptive reconstruction network (ARN). It builds the correspondence between image region proposal and query in an adaptive manner: adaptive grounding and collaborative reconstruction. Specifically, we first extract the subject, location and context features to represent the proposals and the query respectively. Then, we design the adaptive grounding module to compute the matching score between each proposal and query by a hierarchical attention model. Finally, based on attention score and proposal features, we reconstruct the input query with a collaborative loss of language reconstruction loss, adaptive reconstruction loss, and attribute classification loss. This adaptive mechanism helps our model to alleviate the variance of different referring expressions. Experiments on four large-scale datasets show ARN outperforms existing state-of-the-art methods by a large margin. Qualitative results demonstrate that the proposed ARN can better handle the situation where multiple objects of a particular category situated together.



### Detecting Parking Spaces in a Parcel using Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1909.05624v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.05624v2)
- **Published**: 2019-08-28 07:00:13+00:00
- **Updated**: 2020-01-30 13:37:07+00:00
- **Authors**: Murugesan Vadivel, SelvaKumar Murugan, Suriyadeepan Ramamoorthy, Vaidheeswaran Archana, Malaikannan Sankarasubbu
- **Comment**: None
- **Journal**: None
- **Summary**: Remote Sensing Images from satellites have been used in various domains for detecting and understanding structures on the ground surface. In this work, satellite images were used for localizing parking spaces and vehicles in parking lots for a given parcel using an RCNN based Neural Network Architectures. Parcel shapefiles and raster images from USGS image archive were used for developing images for both training and testing. Feature Pyramid based Mask RCNN yields average class accuracy of 97.56% for both parking spaces and vehicles



### BioFaceNet: Deep Biophysical Face Image Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1908.10578v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10578v2)
- **Published**: 2019-08-28 07:28:48+00:00
- **Updated**: 2019-09-13 16:32:12+00:00
- **Authors**: Sarah Alotaibi, William Smith
- **Comment**: Accepted to BMVC2019
- **Journal**: None
- **Summary**: In this paper we present BioFaceNet, a deep CNN that learns to decompose a single face image into biophysical parameters maps, diffuse and specular shading maps as well as estimating the spectral power distribution of the scene illuminant and the spectral sensitivity of the camera. The network comprises a fully convolutional encoder for estimating the spatial maps with a fully connected branch for estimating the vector quantities. The network is trained using a self-supervised appearance loss computed via a model-based decoder. The task is highly underconstrained so we impose a number of model-based priors. Skin spectral reflectance is restricted to a biophysical model, we impose a statistical prior on camera spectral sensitivities, a physical constraint on illumination spectra, a sparsity prior on specular reflections and direct supervision on diffuse shading using a rough shape proxy. We show convincing qualitative results on in-the-wild data and introduce a benchmark for quantitative evaluation on this new task.



### Attention-based Fusion for Outfit Recommendation
- **Arxiv ID**: http://arxiv.org/abs/1908.10585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1908.10585v1)
- **Published**: 2019-08-28 07:41:53+00:00
- **Updated**: 2019-08-28 07:41:53+00:00
- **Authors**: Katrien Laenen, Marie-Francine Moens
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: This paper describes an attention-based fusion method for outfit recommendation which fuses the information in the product image and description to capture the most important, fine-grained product features into the item representation. We experiment with different kinds of attention mechanisms and demonstrate that the attention-based fusion improves item understanding. We outperform state-of-the-art outfit recommendation results on three benchmark datasets.



### Self-supervised blur detection from synthetically blurred scenes
- **Arxiv ID**: http://arxiv.org/abs/1908.10638v1
- **DOI**: 10.1016/j.imavis.2019.08.008
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10638v1)
- **Published**: 2019-08-28 10:58:55+00:00
- **Updated**: 2019-08-28 10:58:55+00:00
- **Authors**: Aitor Alvarez-Gila, Adrian Galdran, Estibaliz Garrote, Joost van de Weijer
- **Comment**: Image and Vision Computing
- **Journal**: None
- **Summary**: Blur detection aims at segmenting the blurred areas of a given image. Recent deep learning-based methods approach this problem by learning an end-to-end mapping between the blurred input and a binary mask representing the localization of its blurred areas. Nevertheless, the effectiveness of such deep models is limited due to the scarcity of datasets annotated in terms of blur segmentation, as blur annotation is labour intensive. In this work, we bypass the need for such annotated datasets for end-to-end learning, and instead rely on object proposals and a model for blur generation in order to produce a dataset of synthetically blurred images. This allows us to perform self-supervised learning over the generated image and ground truth blur mask pairs using CNNs, defining a framework that can be employed in purely self-supervised, weakly supervised or semi-supervised configurations. Interestingly, experimental results of such setups over the largest blur segmentation datasets available show that this approach achieves state of the art results in blur segmentation, even without ever observing any real blurred image.



### CASIA-SURF: A Large-scale Multi-modal Benchmark for Face Anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1908.10654v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10654v2)
- **Published**: 2019-08-28 11:40:51+00:00
- **Updated**: 2020-02-04 17:58:24+00:00
- **Authors**: Shifeng Zhang, Ajian Liu, Jun Wan, Yanyan Liang, Guogong Guo, Sergio Escalera, Hugo Jair Escalante, Stan Z. Li
- **Comment**: Accepted by TBIOM; Journal extension of our previous conference
  paper: arXiv:1812.00408
- **Journal**: None
- **Summary**: Face anti-spoofing is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face anti-spoofing benchmark datasets in recent years. However, existing face anti-spoofing benchmarks have limited number of subjects ($\le\negmedspace170$) and modalities ($\leq\negmedspace2$), which hinder the further development of the academic community. To facilitate face anti-spoofing research, we introduce a large-scale multi-modal dataset, namely CASIA-SURF, which is the largest publicly available dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of $1,000$ subjects with $21,000$ videos and each sample has $3$ modalities (i.e., RGB, Depth and IR). We also provide comprehensive evaluation metrics, diverse evaluation protocols, training/validation/testing subsets and a measurement tool, developing a new benchmark for face anti-spoofing. Moreover, we present a novel multi-modal multi-scale fusion method as a strong baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modality across different scales. Extensive experiments have been conducted on the proposed dataset to verify its significance and generalization capability. The dataset is available at https://sites.google.com/qq.com/face-anti-spoofing/welcome/challengecvpr2019?authuser=0



### Explainable Video Action Reasoning via Prior Knowledge and State Transitions
- **Arxiv ID**: http://arxiv.org/abs/1908.10700v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10700v1)
- **Published**: 2019-08-28 13:04:28+00:00
- **Updated**: 2019-08-28 13:04:28+00:00
- **Authors**: Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Human action analysis and understanding in videos is an important and challenging task. Although substantial progress has been made in past years, the explainability of existing methods is still limited. In this work, we propose a novel action reasoning framework that uses prior knowledge to explain semantic-level observations of video state changes. Our method takes advantage of both classical reasoning and modern deep learning approaches. Specifically, prior knowledge is defined as the information of a target video domain, including a set of objects, attributes and relationships in the target video domain, as well as relevant actions defined by the temporal attribute and relationship changes (i.e. state transitions). Given a video sequence, we first generate a scene graph on each frame to represent concerned objects, attributes and relationships. Then those scene graphs are linked by tracking objects across frames to form a spatio-temporal graph (also called video graph), which represents semantic-level video states. Finally, by sequentially examining each state transition in the video graph, our method can detect and explain how those actions are executed with prior knowledge, just like the logical manner of thinking by humans. Compared to previous works, the action reasoning results of our method can be explained by both logical rules and semantic-level observations of video content changes. Besides, the proposed method can be used to detect multiple concurrent actions with detailed information, such as who (particular objects), when (time), where (object locations) and how (what kind of changes). Experiments on a re-annotated dataset CAD-120 show the effectiveness of our method.



### Unsupervised Domain Adaptation for Cross-sensor Pore Detection in High-resolution Fingerprint Images
- **Arxiv ID**: http://arxiv.org/abs/1908.10701v2
- **DOI**: 10.1109/JSEN.2021.3128316
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10701v2)
- **Published**: 2019-08-28 13:05:03+00:00
- **Updated**: 2020-01-28 07:07:44+00:00
- **Authors**: Vijay Anand, Vivek Kanhangad
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: With the emergence of high-resolution fingerprint sensors, there has been a lot of focus on level-3 fingerprint features, especially the pores, for the next generation automated fingerprint recognition systems (AFRS). Following the success of deep learning in various computer vision tasks, researchers have developed learning-based approaches for detection of pores in high-resolution fingerprint images. Generally, learning-based approaches provide better performance than handcrafted feature-based approaches. However, domain adaptability of the existing learning-based pore detection methods has never been studied. In this paper, we study this aspect and propose an approach for pore detection in cross-sensor scenarios. For this purpose, we have generated an in-house 1000 dpi fingerprint dataset with ground truth pore coordinates (referred to as IITI-HRFP-GT), and evaluated the performance of the existing learning-based pore detection approaches. The core of the proposed approach for detection of pores in cross-sensor scenarios is DeepDomainPore, which is a residual learning-based convolutional neural network(CNN) trained for pore detection. The domain adaptability in DeepDomainPore is achieved by embedding a gradient reversal layer between the CNN and a domain classifier network. The proposed approach achieves state-of-the-art performance in a cross-sensor scenario involving public high-resolution fingerprint datasets with 88.12% true detection rate and 83.82% F-score.



### Fast Video Object Segmentation via Mask Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1908.10717v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10717v1)
- **Published**: 2019-08-28 13:31:34+00:00
- **Updated**: 2019-08-28 13:31:34+00:00
- **Authors**: Tao Zhuo, Zhiyong Cheng, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: Accuracy and processing speed are two important factors that affect the use of video object segmentation (VOS) in real applications. With the advanced techniques of deep neural networks, the accuracy has been significantly improved, however, the speed is still far below the real-time needs because of the complicated network design, such as the requirement of the first frame fine-tuning step. To overcome this limitation, we propose a novel mask transfer network (MTN), which can greatly boost the processing speed of VOS and also achieve a reasonable accuracy. The basic idea of MTN is to transfer the reference mask to the target frame via an efficient global pixel matching strategy. The global pixel matching between the reference frame and the target frame is to ensure good matching results. To enhance the matching speed, we perform the matching on a downsampled feature map with 1/32 of the original frame size. At the same time, to preserve the detailed mask information in such a small feature map, a mask network is designed to encode the annotated mask information with 512 channels. Finally, an efficient feature warping method is used to transfer the encoded reference mask to the target frame. Based on this design, our method avoids the fine-tuning step on the first frame and does not rely on the temporal cues and particular object categories. Therefore, it runs very fast and can be conveniently trained only with images, as well as being robust to unseen objects. Experiments on the DAVIS datasets demonstrate that MTN can achieve a speed of 37 fps, and also shows a competitive accuracy in comparison to the state-of-the-art methods.



### Facial age estimation by deep residual decision making
- **Arxiv ID**: http://arxiv.org/abs/1908.10737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10737v1)
- **Published**: 2019-08-28 14:12:04+00:00
- **Updated**: 2019-08-28 14:12:04+00:00
- **Authors**: Shichao Li, Kwang-Ting Cheng
- **Comment**: Following-up work for visualizing deep neural decision forest for
  facial age estimation
- **Journal**: None
- **Summary**: Residual representation learning simplifies the optimization problem of learning complex functions and has been widely used by traditional convolutional neural networks. However, it has not been applied to deep neural decision forest (NDF). In this paper we incorporate residual learning into NDF and the resulting model achieves state-of-the-art level accuracy on three public age estimation benchmarks while requiring less memory and computation. We further employ gradient-based technique to visualize the decision-making process of NDF and understand how it is influenced by facial image inputs. The code and pre-trained models will be available at https://github.com/Nicholasli1995/VisualizingNDF.



### SMART tracking: Simultaneous anatomical imaging and real-time passive device tracking for MR-guided interventions
- **Arxiv ID**: http://arxiv.org/abs/1908.10769v1
- **DOI**: 10.1016/j.ejmp.2019.07.019
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10769v1)
- **Published**: 2019-08-28 15:14:59+00:00
- **Updated**: 2019-08-28 15:14:59+00:00
- **Authors**: Frank Zijlstra, Max A. Viergever, Peter R. Seevinck
- **Comment**: None
- **Journal**: Physica Medica 64 (2019): 252-260
- **Summary**: Purpose: This study demonstrates a proof of concept of a method for simultaneous anatomical imaging and real-time (SMART) passive device tracking for MR-guided interventions.   Methods: Phase Correlation template matching was combined with a fast undersampled radial multi-echo acquisition using the white marker phenomenon after the first echo. In this way, the first echo provides anatomical contrast, whereas the other echoes provide white marker contrast to allow accurate device localization using fast simulations and template matching. This approach was tested on tracking of five 0.5 mm steel markers in an agarose phantom and on insertion of an MRI-compatible 20 Gauge titanium needle in ex vivo porcine tissue. The locations of the steel markers were quantitatively compared to the marker locations as found on a CT scan of the same phantom.   Results: The average pairwise error between the MRI and CT locations was 0.30 mm for tracking of stationary steel spheres and 0.29 mm during motion. Qualitative evaluation of the tracking of needle insertions showed that tracked positions were stable throughout needle insertion and retraction.   Conclusions: The proposed SMART tracking method provided accurate passive tracking of devices at high framerates, inclusion of real-time anatomical scanning, and the capability of automatic slice positioning. Furthermore, the method does not require specialized hardware and could therefore be applied to track any rigid metal device that causes appreciable magnetic field distortions.



### Image Captioning with Sparse Recurrent Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1908.10797v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10797v2)
- **Published**: 2019-08-28 15:53:13+00:00
- **Updated**: 2019-10-28 15:51:13+00:00
- **Authors**: Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah
- **Comment**: Corrected Eq 11, updated Table 5
- **Journal**: None
- **Summary**: Recurrent Neural Network (RNN) has been widely used to tackle a wide variety of language generation problems and are capable of attaining state-of-the-art (SOTA) performance. However despite its impressive results, the large number of parameters in the RNN model makes deployment to mobile and embedded devices infeasible. Driven by this problem, many works have proposed a number of pruning methods to reduce the sizes of the RNN model. In this work, we propose an end-to-end pruning method for image captioning models equipped with visual attention. Our proposed method is able to achieve sparsity levels up to 97.5% without significant performance loss relative to the baseline (~ 2% loss at 40x compression after fine-tuning). Our method is also simple to use and tune, facilitating faster development times for neural network practitioners. We perform extensive experiments on the popular MS-COCO dataset in order to empirically validate the efficacy of our proposed method.



### Self-supervised Recurrent Neural Network for 4D Abdominal and In-utero MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/1908.10842v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10842v1)
- **Published**: 2019-08-28 17:24:26+00:00
- **Updated**: 2019-08-28 17:24:26+00:00
- **Authors**: Tong Zhang, Laurence H. Jackson, Alena Uus, James R. Clough, Lisa Story, Mary A. Rutherford, Joseph V. Hajnal, Maria Deprez
- **Comment**: Accepted by MICCAI 2019 workshop on Machine Learning for Medical
  Image Reconstruction
- **Journal**: None
- **Summary**: Accurately estimating and correcting the motion artifacts are crucial for 3D image reconstruction of the abdominal and in-utero magnetic resonance imaging (MRI). The state-of-art methods are based on slice-to-volume registration (SVR) where multiple 2D image stacks are acquired in three orthogonal orientations. In this work, we present a novel reconstruction pipeline that only needs one orientation of 2D MRI scans and can reconstruct the full high-resolution image without masking or registration steps. The framework consists of two main stages: the respiratory motion estimation using a self-supervised recurrent neural network, which learns the respiratory signals that are naturally embedded in the asymmetry relationship of the neighborhood slices and cluster them according to a respiratory state. Then, we train a 3D deconvolutional network for super-resolution (SR) reconstruction of the sparsely selected 2D images using integrated reconstruction and total variation loss. We evaluate the classification accuracy on 5 simulated images and compare our results with the SVR method in adult abdominal and in-utero MRI scans. The results show that the proposed pipeline can accurately estimate the respiratory state and reconstruct 4D SR volumes with better or similar performance to the 3D SVR pipeline with less than 20\% sparsely selected slices. The method has great potential to transform the 4D abdominal and in-utero MRI in clinical practice.



### Transfer Learning from Partial Annotations for Whole Brain Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.10851v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10851v1)
- **Published**: 2019-08-28 17:46:56+00:00
- **Updated**: 2019-08-28 17:46:56+00:00
- **Authors**: Chengliang Dai, Yuanhan Mo, Elsa Angelini, Yike Guo, Wenjia Bai
- **Comment**: None
- **Journal**: None
- **Summary**: Brain MR image segmentation is a key task in neuroimaging studies. It is commonly conducted using standard computational tools, such as FSL, SPM, multi-atlas segmentation etc, which are often registration-based and suffer from expensive computation cost. Recently, there is an increased interest using deep neural networks for brain image segmentation, which have demonstrated advantages in both speed and performance. However, neural networks-based approaches normally require a large amount of manual annotations for optimising the massive amount of network parameters. For 3D networks used in volumetric image segmentation, this has become a particular challenge, as a 3D network consists of many more parameters compared to its 2D counterpart. Manual annotation of 3D brain images is extremely time-consuming and requires extensive involvement of trained experts. To address the challenge with limited manual annotations, here we propose a novel multi-task learning framework for brain image segmentation, which utilises a large amount of automatically generated partial annotations together with a small set of manually created full annotations for network training. Our method yields a high performance comparable to state-of-the-art methods for whole brain segmentation.



### Out the Window: A Crowd-Sourced Dataset for Activity Classification in Security Video
- **Arxiv ID**: http://arxiv.org/abs/1908.10899v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10899v2)
- **Published**: 2019-08-28 18:28:12+00:00
- **Updated**: 2019-09-15 19:21:43+00:00
- **Authors**: Gregory Castanon, Nathan Shnidman, Tim Anderson, Jeffrey Byrne
- **Comment**: None
- **Journal**: None
- **Summary**: The Out the Window (OTW) dataset is a crowdsourced activity dataset containing 5,668 instances of 17 activities from the NIST Activities in Extended Video (ActEV) challenge. These videos are crowdsourced from workers on the Amazon Mechanical Turk using a novel scenario acting strategy, which collects multiple instances of natural activities per scenario. Turkers are instructed to lean their mobile device against an upper story window overlooking an outdoor space, walk outside to perform a scenario involving people, vehicles and objects, and finally upload the video to us for annotation. Performance evaluation for activity classification on VIRAT Ground 2.0 shows that the OTW dataset provides an 8.3% improvement in mean classification accuracy, and a 12.5% improvement on the most challenging activities involving people with vehicles.



### DFPENet-geology: A Deep Learning Framework for High Precision Recognition and Segmentation of Co-seismic Landslides
- **Arxiv ID**: http://arxiv.org/abs/1908.10907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10907v2)
- **Published**: 2019-08-28 19:07:40+00:00
- **Updated**: 2020-02-13 14:42:33+00:00
- **Authors**: Qingsong Xu, Chaojun Ouyang, Tianhai Jiang, Xuanmei Fan, Duoxiang Cheng
- **Comment**: 1. There are some problems in the method and results, and there is a
  lot of room for improvement. Overall, the proposed DFPENet has a high
  redundancy by combining the Attention Gate Mechanism and Gate Convolution
  Networks, and we need to further improve and refine the results. 2. For our
  own research, we need experts to provide comments on my work whether negative
  or positive
- **Journal**: None
- **Summary**: The following lists two main reasons for withdrawal for the public. 1. There are some problems in the method and results, and there is a lot of room for improvement. In terms of method, "Pre-trained Datasets (PD)" represents selecting a small amount from the online test set, which easily causes the model to overfit the online test set and could not obtain robust performance. More importantly, the proposed DFPENet has a high redundancy by combining the Attention Gate Mechanism and Gate Convolution Networks, and we need to revisit the section of geological feature fusion, in terms of results, we need to further improve and refine. 2. arXiv is an open-access repository of electronic preprints without peer reviews. However, for our own research, we need experts to provide comments on my work whether negative or positive. I then would use their comments to significantly improve this manuscript. Therefore, we finally decided to withdraw this manuscript in arXiv, and we will update to arXiv with the final accepted manuscript to facilitate more researchers to use our proposed comprehensive and general scheme to recognize and segment seismic landslides more efficiently.



### ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients
- **Arxiv ID**: http://arxiv.org/abs/1909.02068v5
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.02068v5)
- **Published**: 2019-08-28 19:29:41+00:00
- **Updated**: 2021-07-14 20:22:04+00:00
- **Authors**: Ran Xu, Rakesh Kumar, Pengcheng Wang, Peter Bai, Ganga Meghanath, Somali Chaterji, Subrata Mitra, Saurabh Bagchi
- **Comment**: This paper has been accepted to appear in ACM Transactions on Sensor
  Networks in 2021
- **Journal**: None
- **Summary**: Videos take a lot of time to transport over the network, hence running analytics on the live video on embedded or mobile devices has become an important system driver. Considering that such devices, e.g., surveillance cameras or AR/VR gadgets, are resource constrained, creating lightweight deep neural networks (DNNs) for embedded devices is crucial. None of the current approximation techniques for object classification DNNs can adapt to changing runtime conditions, e.g., changes in resource availability on the device, the content characteristics, or requirements from the user. In this paper, we introduce ApproxNet, a video object classification system for embedded or mobile clients. It enables novel dynamic approximation techniques to achieve desired inference latency and accuracy trade-off under changing runtime conditions. It achieves this by enabling two approximation knobs within a single DNN model, rather than creating and maintaining an ensemble of models (e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at runtime to these changes, provides low and stable latency for the image and video frame classification problems, and show the improvement in accuracy and latency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17], NestDNN [MobiCom-18], and MSDNet [ICLR-18].



### A Possible Reason for why Data-Driven Beats Theory-Driven Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1908.10933v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10933v2)
- **Published**: 2019-08-28 20:19:41+00:00
- **Updated**: 2019-09-06 22:08:39+00:00
- **Authors**: John K. Tsotsos, Iuliia Kotseruba, Alexander Andreopoulos, Yulong Wu
- **Comment**: 8 pages, 5 figures
- **Journal**: None
- **Summary**: Why do some continue to wonder about the success and dominance of deep learning methods in computer vision and AI? Is it not enough that these methods provide practical solutions to many problems? Well no, it is not enough, at least for those who feel there should be a science that underpins all of this and that we should have a clear understanding of how this success was achieved. Here, this paper proposes that the dominance we are witnessing would not have been possible by the methods of deep learning alone: the tacit change has been the evolution of empirical practice in computer vision and AI over the past decades. We demonstrate this by examining the distribution of sensor settings in vision datasets and performance of both classic and deep learning algorithms under various camera settings. This reveals a strong mismatch between optimal performance ranges of classical theory-driven algorithms and sensor setting distributions in the common vision datasets, while data-driven models were trained for those datasets. The head-to-head comparisons between data-driven and theory-driven models were therefore unknowingly biased against the theory-driven models.



### Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1908.10937v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10937v1)
- **Published**: 2019-08-28 20:45:25+00:00
- **Updated**: 2019-08-28 20:45:25+00:00
- **Authors**: Vishwanath A Sindagi, Vishal M. Patel
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets.



### A Multiple Source Hourglass Deep Network for Multi-Focus Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1908.10945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10945v1)
- **Published**: 2019-08-28 21:01:24+00:00
- **Updated**: 2019-08-28 21:01:24+00:00
- **Authors**: Fidel Alejandro Guerrero Pe√±a, Pedro Diamel Marrero Fern√°ndez, Tsang Ing Ren, Germano Crispim Vasconcelos, Alexandre Cunha
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Focus Image Fusion seeks to improve the quality of an acquired burst of images with different focus planes. For solving the task, an activity level measurement and a fusion rule are typically established to select and fuse the most relevant information from the sources. However, the design of this kind of method by hand is really hard and sometimes restricted to solution spaces where the optimal all-in-focus images are not contained. Then, we propose here two fast and straightforward approaches for image fusion based on deep neural networks. Our solution uses a multiple source Hourglass architecture trained in an end-to-end fashion. Models are data-driven and can be easily generalized for other kinds of fusion problems. A segmentation approach is used for recognition of the focus map, while the weighted average rule is used for fusion. We designed a training loss function for our regression-based fusion function, which allows the network to learn both the activity level measurement and the fusion rule. Experimental results show our approach has comparable results to the state-of-the-art methods with a 60X increase of computational efficiency for 520X520 resolution images.



