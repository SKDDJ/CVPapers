# Arxiv Papers in cs.CV on 2019-08-12
### Dynamic Region Division for Adaptive Learning Pedestrian Counting
- **Arxiv ID**: http://arxiv.org/abs/1908.03978v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03978v1)
- **Published**: 2019-08-12 01:17:50+00:00
- **Updated**: 2019-08-12 01:17:50+00:00
- **Authors**: Gaoqi He, Zhenwei Ma, Binhao Huang, Bin Sheng, Yubo Yuan
- **Comment**: accepted by IEEE International Conference on Multimedia and Expo
  (ICME) 2019
- **Journal**: None
- **Summary**: Accurate pedestrian counting algorithm is critical to eliminate insecurity in the congested public scenes. However, counting pedestrians in crowded scenes often suffer from severe perspective distortion. In this paper, basing on the straight-line double region pedestrian counting method, we propose a dynamic region division algorithm to keep the completeness of counting objects. Utilizing the object bounding boxes obtained by YoloV3 and expectation division line of the scene, the boundary for nearby region and distant one is generated under the premise of retaining whole head. Ulteriorly, appropriate learning models are applied to count pedestrians in each obtained region. In the distant region, a novel inception dilated convolutional neural network is proposed to solve the problem of choosing dilation rate. In the nearby region, YoloV3 is used for detecting the pedestrian in multi-scale. Accordingly, the total number of pedestrians in each frame is obtained by fusing the result in nearby and distant regions. A typical subway pedestrian video dataset is chosen to conduct experiment in this paper. The result demonstrate that proposed algorithm is superior to existing machine learning based methods in general performance.



### Visual and Semantic Prototypes-Jointly Guided CNN for Generalized Zero-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.03983v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03983v2)
- **Published**: 2019-08-12 02:29:16+00:00
- **Updated**: 2019-08-14 12:58:23+00:00
- **Authors**: Chuanxing Geng, Lue Tao, Songcan Chen
- **Comment**: None
- **Journal**: None
- **Summary**: In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSR's assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework.



### Instance Enhancement Batch Normalization: an Adaptive Regulator of Batch Noise
- **Arxiv ID**: http://arxiv.org/abs/1908.04008v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04008v2)
- **Published**: 2019-08-12 05:42:09+00:00
- **Updated**: 2019-09-18 02:52:32+00:00
- **Authors**: Senwei Liang, Zhongzhan Huang, Mingfu Liang, Haizhao Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Batch Normalization (BN)(Ioffe and Szegedy 2015) normalizes the features of an input image via statistics of a batch of images and hence BN will bring the noise to the gradient of the training loss. Previous works indicate that the noise is important for the optimization and generalization of deep neural networks, but too much noise will harm the performance of networks. In our paper, we offer a new point of view that self-attention mechanism can help to regulate the noise by enhancing instance-specific information to obtain a better regularization effect. Therefore, we propose an attention-based BN called Instance Enhancement Batch Normalization (IEBN) that recalibrates the information of each channel by a simple linear transformation. IEBN has a good capacity of regulating noise and stabilizing network training to improve generalization even in the presence of two kinds of noise attacks during training. Finally, IEBN outperforms BN with only a light parameter increment in image classification tasks for different network structures and benchmark datasets.



### Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking
- **Arxiv ID**: http://arxiv.org/abs/1908.04011v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04011v2)
- **Published**: 2019-08-12 05:52:44+00:00
- **Updated**: 2020-07-29 14:11:32+00:00
- **Authors**: Tan Wang, Xing Xu, Yang Yang, Alan Hanjalic, Heng Tao Shen, Jingkuan Song
- **Comment**: ACM Multimedia 2019 Oral
- **Journal**: None
- **Summary**: A major challenge in matching images and text is that they have intrinsically different data distributions and feature representations. Most existing approaches are based either on embedding or classification, the first one mapping image and text instances into a common embedding space for distance measuring, and the second one regarding image-text matching as a binary classification problem. Neither of these approaches can, however, balance the matching accuracy and model complexity well. We propose a novel framework that achieves remarkable matching performance with acceptable model complexity. Specifically, in the training stage, we propose a novel Multi-modal Tensor Fusion Network (MTFN) to explicitly learn an accurate image-text similarity function with rank-based tensor fusion rather than seeking a common embedding space for each image-text instance. Then, during testing, we deploy a generic Cross-modal Re-ranking (RR) scheme for refinement without requiring additional training procedure. Extensive experiments on two datasets demonstrate that our MTFN-RR consistently achieves the state-of-the-art matching performance with much less time complexity. The implementation code is available at https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code.



### Multi-Frame Content Integration with a Spatio-Temporal Attention Mechanism for Person Video Motion Transfer
- **Arxiv ID**: http://arxiv.org/abs/1908.04013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04013v1)
- **Published**: 2019-08-12 06:01:20+00:00
- **Updated**: 2019-08-12 06:01:20+00:00
- **Authors**: Kun Cheng, Hao-Zhi Huang, Chun Yuan, Lingyiqing Zhou, Wei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Existing person video generation methods either lack the flexibility in controlling both the appearance and motion, or fail to preserve detailed appearance and temporal consistency. In this paper, we tackle the problem of motion transfer for generating person videos, which provides controls on both the appearance and the motion. Specifically, we transfer the motion of one person in a target video to another person in a source video, while preserving the appearance of the source person. Besides only relying on one source frame as the existing state-of-the-art methods, our proposed method integrates information from multiple source frames based on a spatio-temporal attention mechanism to preserve rich appearance details. In addition to a spatial discriminator employed for encouraging the frame-level fidelity, a multi-range temporal discriminator is adopted to enforce the generated video to resemble temporal dynamics of a real video in various time ranges. A challenging real-world dataset, which contains about 500 dancing video clips with complex and unpredictable motions, is collected for the training and testing. Extensive experiments show that the proposed method can produce more photo-realistic and temporally consistent person videos than previous methods. As our method decomposes the syntheses of the foreground and background into two branches, a flexible background substitution application can also be achieved.



### Douglas-Quaid -- Open Source Image Matching Library
- **Arxiv ID**: http://arxiv.org/abs/1908.04014v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04014v1)
- **Published**: 2019-08-12 06:02:31+00:00
- **Updated**: 2019-08-12 06:02:31+00:00
- **Authors**: Vincent Falconieri
- **Comment**: None
- **Journal**: None
- **Summary**: Security analysts need to classify, search and correlate numerous images. Automatic classification tools improve the efficiency of such tasks. However, no open-source and turnkey library was found able to reach this goal. The present paper introduces an Open-Source modular library for the specific cases of visual correlation and Image Matching named Douglas-Quaid. The design of the library, chosen tradeoffs, encountered challenges, envisioned solutions as well as quality and speed results are presented in this paper. We also explore researches directions and future potential developments of the library. Our claim is that even partial automation of screenshots classification would reduce the burden on security teams and that Douglas-Quaid is a step forward in this direction.



### Variational Autoencoded Regression: High Dimensional Regression of Visual Data on Complex Manifold
- **Arxiv ID**: http://arxiv.org/abs/1908.04015v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04015v1)
- **Published**: 2019-08-12 06:06:59+00:00
- **Updated**: 2019-08-12 06:06:59+00:00
- **Authors**: YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris, Jin Young Choi
- **Comment**: Published in CVPR 2017
- **Journal**: None
- **Summary**: This paper proposes a new high dimensional regression method by merging Gaussian process regression into a variational autoencoder framework. In contrast to other regression methods, the proposed method focuses on the case where output responses are on a complex high dimensional manifold, such as images. Our contributions are summarized as follows: (i) A new regression method estimating high dimensional image responses, which is not handled by existing regression algorithms, is proposed. (ii) The proposed regression method introduces a strategy to learn the latent space as well as the encoder and decoder so that the result of the regressed response in the latent space coincide with the corresponding response in the data space. (iii) The proposed regression is embedded into a generative model, and the whole procedure is developed by the variational autoencoder framework. We demonstrate the robustness and effectiveness of our method through a number of experiments on various visual data regression problems.



### An overlapping-free leaf segmentation method for plant point clouds
- **Arxiv ID**: http://arxiv.org/abs/1908.04018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04018v1)
- **Published**: 2019-08-12 06:18:00+00:00
- **Updated**: 2019-08-12 06:18:00+00:00
- **Authors**: Dawei Li, Yan Cao, Guoliang Shi, Xin Cai, Yang Chen, Sifan Wang, Siyuan Yan
- **Comment**: 24 Pages, 18 Figures, 7 Tables. Intends to submit to an open-access
  journal
- **Journal**: None
- **Summary**: Automatic leaf segmentation, as well as identification and classification methods that built upon it, are able to provide immediate monitoring for plant growth status to guarantee the output. Although 3D plant point clouds contain abundant phenotypic features, plant leaves are usually distributed in clusters and are sometimes seriously overlapped in the canopy. Therefore, it is still a big challenge to automatically segment each individual leaf from a highly crowded plant canopy in 3D for plant phenotyping purposes. In this work, we propose an overlapping-free individual leaf segmentation method for plant point clouds using the 3D filtering and facet region growing. In order to separate leaves with different overlapping situations, we develop a new 3D joint filtering operator, which integrates a Radius-based Outlier Filter (RBOF) and a Surface Boundary Filter (SBF) to help to separate occluded leaves. By introducing the facet over-segmentation and facet-based region growing, the noise in segmentation is suppressed and labeled leaf centers can expand to their whole leaves, respectively. Our method can work on point clouds generated from three types of 3D imaging platforms, and also suitable for different kinds of plant species. In experiments, it obtains a point-level cover rate of 97% for Epipremnum aureum, 99% for Monstera deliciosa, 99% for Calathea makoyana, and 87% for Hedera nepalensis sample plants. At the leaf level, our method reaches an average Recall at 100.00%, a Precision at 99.33%, and an average F-measure at 99.66%, respectively. The proposed method can also facilitate the automatic traits estimation of each single leaf (such as the leaf area, length, and width), which has potential to become a highly effective tool for plant research and agricultural engineering.



### Self-supervised Data Bootstrapping for Deep Optical Character Recognition of Identity Documents
- **Arxiv ID**: http://arxiv.org/abs/1908.04027v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04027v1)
- **Published**: 2019-08-12 07:02:24+00:00
- **Updated**: 2019-08-12 07:02:24+00:00
- **Authors**: Oliver Mothes, Joachim Denzler
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: The essential task of verifying person identities at airports and national borders is very time consuming. To accelerate it, optical character recognition for identity documents (IDs) using dictionaries is not appropriate due to high variability of the text content in IDs, e.g., individual street names or surnames. Additionally, no properties of the used fonts in IDs are known. Therefore, we propose an iterative self-supervised bootstrapping approach using a smart strategy to mine real character data from IDs. In combination with synthetically generated character data, the real data is used to train efficient convolutional neural networks for character classification serving a practical runtime as well as a high accuracy. On a dataset with 74 character classes, we achieve an average class-wise accuracy of 99.4 %. In contrast, if we would apply a classifier trained only using synthetic data, the accuracy is reduced to 58.1 %. Finally, we show that our whole proposed pipeline outperforms an established open-source framework



### Is it Raining Outside? Detection of Rainfall using General-Purpose Surveillance Cameras
- **Arxiv ID**: http://arxiv.org/abs/1908.04034v2
- **DOI**: 10.5281/zenodo.4715681
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04034v2)
- **Published**: 2019-08-12 07:32:25+00:00
- **Updated**: 2021-09-03 12:11:00+00:00
- **Authors**: Joakim Bruslund Haurum, Chris H. Bahnsen, Thomas B. Moeslund
- **Comment**: 10 pages, 7 figures, CVPR2019 V4AS workshop. Updated to include
  Zenodo data repository reference
- **Journal**: None
- **Summary**: In integrated surveillance systems based on visual cameras, the mitigation of adverse weather conditions is an active research topic. Within this field, rain removal algorithms have been developed that artificially remove rain streaks from images or video. In order to deploy such rain removal algorithms in a surveillance setting, one must detect if rain is present in the scene. In this paper, we design a system for the detection of rainfall by the use of surveillance cameras. We reimplement the former state-of-the-art method for rain detection and compare it against a modern CNN-based method by utilizing 3D convolutions. The two methods are evaluated on our new AAU Visual Rain Dataset (VIRADA) that consists of 215 hours of general-purpose surveillance video from two traffic crossings. The results show that the proposed 3D CNN outperforms the previous state-of-the-art method by a large margin on all metrics, for both of the traffic crossings. Finally, it is shown that the choice of region-of-interest has a large influence on performance when trying to generalize the investigated methods. The AAU VIRADA dataset and our implementation of the two rain detection algorithms are publicly available at https://bitbucket.org/aauvap/aau-virada.



### Who, Where, and What to Wear? Extracting Fashion Knowledge from Social Media
- **Arxiv ID**: http://arxiv.org/abs/1908.08985v1
- **DOI**: 10.1145/3343031.3350889
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1908.08985v1)
- **Published**: 2019-08-12 08:05:59+00:00
- **Updated**: 2019-08-12 08:05:59+00:00
- **Authors**: Yunshan Ma, Xun Yang, Lizi Liao, Yixin Cao, Tat-Seng Chua
- **Comment**: 9 pages, 8 figures, ACMMM 2019
- **Journal**: None
- **Summary**: Fashion knowledge helps people to dress properly and addresses not only physiological needs of users, but also the demands of social activities and conventions. It usually involves three mutually related aspects of: occasion, person and clothing. However, there are few works focusing on extracting such knowledge, which will greatly benefit many downstream applications, such as fashion recommendation. In this paper, we propose a novel method to automatically harvest fashion knowledge from social media. We unify three tasks of occasion, person and clothing discovery from multiple modalities of images, texts and metadata. For person detection and analysis, we use the off-the-shelf tools due to their flexibility and satisfactory performance. For clothing recognition and occasion prediction, we unify the two tasks by using a contextualized fashion concept learning module, which captures the dependencies and correlations among different fashion concepts. To alleviate the heavy burden of human annotations, we introduce a weak label modeling module which can effectively exploit machine-labeled data, a complementary of clean data. In experiments, we contribute a benchmark dataset and conduct extensive experiments from both quantitative and qualitative perspectives. The results demonstrate the effectiveness of our model in fashion concept prediction, and the usefulness of extracted knowledge with comprehensive analysis.



### SHREWD: Semantic Hierarchy-based Relational Embeddings for Weakly-supervised Deep Hashing
- **Arxiv ID**: http://arxiv.org/abs/1908.05602v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.AI, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1908.05602v1)
- **Published**: 2019-08-12 08:24:40+00:00
- **Updated**: 2019-08-12 08:24:40+00:00
- **Authors**: Heikki Arponen, Tom E Bishop
- **Comment**: 4 pages, Published in ICLR LLD Workshop
- **Journal**: None
- **Summary**: Using class labels to represent class similarity is a typical approach to training deep hashing systems for retrieval; samples from the same or different classes take binary 1 or 0 similarity values. This similarity does not model the full rich knowledge of semantic relations that may be present between data points. In this work we build upon the idea of using semantic hierarchies to form distance metrics between all available sample labels; for example cat to dog has a smaller distance than cat to guitar. We combine this type of semantic distance into a loss function to promote similar distances between the deep neural network embeddings. We also introduce an empirical Kullback-Leibler divergence loss term to promote binarization and uniformity of the embeddings. We test the resulting SHREWD method and demonstrate improvements in hierarchical retrieval scores using compact, binary hash codes instead of real valued ones, and show that in a weakly supervised hashing setting we are able to learn competitively without explicitly relying on class labels, but instead on similarities between labels.



### Mix & Match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency
- **Arxiv ID**: http://arxiv.org/abs/1908.08986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08986v1)
- **Published**: 2019-08-12 08:27:49+00:00
- **Updated**: 2019-08-12 08:27:49+00:00
- **Authors**: Elad Hoffer, Berry Weinstein, Itay Hubara, Tal Ben-Nun, Torsten Hoefler, Daniel Soudry
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) are commonly trained using a fixed spatial image size predetermined for a given model. Although trained on images of aspecific size, it is well established that CNNs can be used to evaluate a wide range of image sizes at test time, by adjusting the size of intermediate feature maps. In this work, we describe and evaluate a novel mixed-size training regime that mixes several image sizes at training time. We demonstrate that models trained using our method are more resilient to image size changes and generalize well even on small images. This allows faster inference by using smaller images attest time. For instance, we receive a 76.43% top-1 accuracy using ResNet50 with an image size of 160, which matches the accuracy of the baseline model with 2x fewer computations. Furthermore, for a given image size used at test time, we show this method can be exploited either to accelerate training or the final test accuracy. For example, we are able to reach a 79.27% accuracy with a model evaluated at a 288 spatial size for a relative improvement of 14% over the baseline.



### Semi-Supervised Video Salient Object Detection Using Pseudo-Labels
- **Arxiv ID**: http://arxiv.org/abs/1908.04051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04051v2)
- **Published**: 2019-08-12 08:32:48+00:00
- **Updated**: 2019-11-29 02:03:15+00:00
- **Authors**: Pengxiang Yan, Guanbin Li, Yuan Xie, Zhen Li, Chuan Wang, Tianshui Chen, Liang Lin
- **Comment**: ICCV2019, code is available at
  https://github.com/Kinpzz/RCRNet-Pytorch
- **Journal**: None
- **Summary**: Deep learning-based video salient object detection has recently achieved great success with its performance significantly outperforming any other unsupervised methods. However, existing data-driven approaches heavily rely on a large quantity of pixel-wise annotated video frames to deliver such promising results. In this paper, we address the semi-supervised video salient object detection task using pseudo-labels. Specifically, we present an effective video saliency detector that consists of a spatial refinement network and a spatiotemporal module. Based on the same refinement network and motion information in terms of optical flow, we further propose a novel method for generating pixel-level pseudo-labels from sparsely annotated frames. By utilizing the generated pseudo-labels together with a part of manual annotations, our video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement, thus producing accurate saliency maps. Experimental results demonstrate that our proposed semi-supervised method even greatly outperforms all the state-of-the-art fully supervised methods across three public benchmarks of VOS, DAVIS, and FBMS.



### Sentence Specified Dynamic Video Thumbnail Generation
- **Arxiv ID**: http://arxiv.org/abs/1908.04052v2
- **DOI**: 10.1145/3343031.3350985
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04052v2)
- **Published**: 2019-08-12 08:35:37+00:00
- **Updated**: 2019-10-16 09:47:43+00:00
- **Authors**: Yitian Yuan, Lin Ma, Wenwu Zhu
- **Comment**: None
- **Journal**: ACM Multimedia 2019
- **Summary**: With the tremendous growth of videos over the Internet, video thumbnails, providing video content previews, are becoming increasingly crucial to influencing users' online searching experiences. Conventional video thumbnails are generated once purely based on the visual characteristics of videos, and then displayed as requested. Hence, such video thumbnails, without considering the users' searching intentions, cannot provide a meaningful snapshot of the video contents that users concern. In this paper, we define a distinctively new task, namely sentence specified dynamic video thumbnail generation, where the generated thumbnails not only provide a concise preview of the original video contents but also dynamically relate to the users' searching intentions with semantic correspondences to the users' query sentences. To tackle such a challenging task, we propose a novel graph convolved video thumbnail pointer (GTP). Specifically, GTP leverages a sentence specified video graph convolutional network to model both the sentence-video semantic interaction and the internal video relationships incorporated with the sentence information, based on which a temporal conditioned pointer network is then introduced to sequentially generate the sentence specified video thumbnails. Moreover, we annotate a new dataset based on ActivityNet Captions for the proposed new task, which consists of 10,000+ video-sentence pairs with each accompanied by an annotated sentence specified video thumbnail. We demonstrate that our proposed GTP outperforms several baseline methods on the created dataset, and thus believe that our initial results along with the release of the new dataset will inspire further research on sentence specified dynamic video thumbnail generation. Dataset and code are available at https://github.com/yytzsy/GTP.



### Explicit Shape Encoding for Real-Time Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.04067v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04067v1)
- **Published**: 2019-08-12 09:47:03+00:00
- **Updated**: 2019-08-12 09:47:03+00:00
- **Authors**: Wenqiang Xu, Haiyang Wang, Fubo Qi, Cewu Lu
- **Comment**: to appear in ICCV2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named \textbf{ESE-Seg}. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP$^r$@0.5 while 7 times faster.



### Multimodal Unified Attention Networks for Vision-and-Language Interactions
- **Arxiv ID**: http://arxiv.org/abs/1908.04107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04107v2)
- **Published**: 2019-08-12 12:12:17+00:00
- **Updated**: 2019-08-19 06:14:06+00:00
- **Authors**: Zhou Yu, Yuhao Cui, Jun Yu, Dacheng Tao, Qi Tian
- **Comment**: 11 pages, 7 figures
- **Journal**: None
- **Summary**: Learning an effective attention mechanism for multimodal data is important in many vision-and-language tasks that require a synergic understanding of both the visual and textual contents. Existing state-of-the-art approaches use co-attention models to associate each visual object (e.g., image region) with each textual object (e.g., query word). Despite the success of these co-attention models, they only model inter-modal interactions while neglecting intra-modal interactions. Here we propose a general `unified attention' model that simultaneously captures the intra- and inter-modal interactions of multimodal features and outputs their corresponding attended representations. By stacking such unified attention blocks in depth, we obtain the deep Multimodal Unified Attention Network (MUAN), which can seamlessly be applied to the visual question answering (VQA) and visual grounding tasks. We evaluate our MUAN models on two VQA datasets and three visual grounding datasets, and the results show that MUAN achieves top-level performance on both tasks without bells and whistles.



### Enhanced 3D convolutional networks for crowd counting
- **Arxiv ID**: http://arxiv.org/abs/1908.04121v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04121v1)
- **Published**: 2019-08-12 12:39:28+00:00
- **Updated**: 2019-08-12 12:39:28+00:00
- **Authors**: Zhikang Zou, Huiliang Shao, Xiaoye Qu, Wei Wei, Pan Zhou
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: Recently, convolutional neural networks (CNNs) are the leading defacto method for crowd counting. However, when dealing with video datasets, CNN-based methods still process each video frame independently, thus ignoring the powerful temporal information between consecutive frames. In this work, we propose a novel architecture termed as "temporal channel-aware" (TCA) block, which achieves the capability of exploiting the temporal interdependencies among video sequences. Specifically, we incorporate 3D convolution kernels to encode local spatio-temporal features. Furthermore, the global contextual information is encoded into modulation weights which adaptively recalibrate channel-aware feature responses. With the local and global context combined, the proposed block enhances the discriminative ability of the feature representations and contributes to more precise results in diverse scenes. By stacking TCA blocks together, we obtain the deep trainable architecture called enhanced 3D convolutional networks (E3D). The experiments on three benchmark datasets show that the proposed method delivers state-of-the-art performance. To verify the generality, an extended experiment is conducted on a vehicle dataset TRANCOS and our approach beats previous methods by large margins.



### Automated retinal vessel segmentation based on morphological preprocessing and 2D-Gabor wavelets
- **Arxiv ID**: http://arxiv.org/abs/1908.04123v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04123v1)
- **Published**: 2019-08-12 12:44:05+00:00
- **Updated**: 2019-08-12 12:44:05+00:00
- **Authors**: Kundan Kumar, Debashisa Samal, Suraj
- **Comment**: 12 pages, 5 figures, conference
- **Journal**: None
- **Summary**: Automated segmentation of vascular map in retinal images endeavors a potential benefit in diagnostic procedure of different ocular diseases. In this paper, we suggest a new unsupervised retinal blood vessel segmentation approach using top-hat transformation, contrast-limited adaptive histogram equalization (CLAHE), and 2-D Gabor wavelet filters. Initially, retinal image is preprocessed using top-hat morphological transformation followed by CLAHE to enhance only the blood vessel pixels in the presence of exudates, optic disc, and fovea. Then, multiscale 2-D Gabor wavelet filters are applied on preprocessed image for better representation of thick and thin blood vessels located at different orientations. The efficacy of the presented algorithm is assessed on publicly available DRIVE database with manually labeled images. On DRIVE database, we achieve an average accuracy of 94.32% with a small standard deviation of 0.004. In comparison with major algorithms, our algorithm produces better performance concerning the accuracy, sensitivity, and kappa agreement.



### Improving Robustness of Deep Learning Based Knee MRI Segmentation: Mixup and Adversarial Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1908.04126v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04126v3)
- **Published**: 2019-08-12 12:50:59+00:00
- **Updated**: 2019-10-27 16:20:46+00:00
- **Authors**: Egor Panfilov, Aleksei Tiulpin, Stefan Klein, Miika T. Nieminen, Simo Saarakkala
- **Comment**: None
- **Journal**: None
- **Summary**: Degeneration of articular cartilage (AC) is actively studied in knee osteoarthritis (OA) research via magnetic resonance imaging (MRI). Segmentation of AC tissues from MRI data is an essential step in quantification of their damage. Deep learning (DL) based methods have shown potential in this realm and are the current state-of-the-art, however, their robustness to heterogeneity of MRI acquisition settings remains an open problem. In this study, we investigated two modern regularization techniques -- mixup and adversarial unsupervised domain adaptation (UDA) -- to improve the robustness of DL-based knee cartilage segmentation to new MRI acquisition settings. Our validation setup included two datasets produced by different MRI scanners and using distinct data acquisition protocols. We assessed the robustness of automatic segmentation by comparing mixup and UDA approaches to a strong baseline method at different OA severity stages and, additionally, in relation to anatomical locations. Our results showed that for moderate changes in knee MRI data acquisition settings both approaches may provide notable improvements in the robustness, which are consistent for all stages of the disease and affect the clinically important areas of the knee joint. However, mixup may be considered as a recommended approach, since it is more computationally efficient and does not require additional data from the target acquisition setup.



### Jointly Aligning Millions of Images with Deep Penalised Reconstruction Congealing
- **Arxiv ID**: http://arxiv.org/abs/1908.04130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04130v2)
- **Published**: 2019-08-12 12:55:31+00:00
- **Updated**: 2019-10-14 10:24:31+00:00
- **Authors**: Roberto Annunziata, Christos Sagonas, Jacques Cali
- **Comment**: International Conference on Computer Vision 2019 (ICCV 2019), Seoul,
  Korea
- **Journal**: None
- **Summary**: Extrapolating fine-grained pixel-level correspondences in a fully unsupervised manner from a large set of misaligned images can benefit several computer vision and graphics problems, e.g. co-segmentation, super-resolution, image edit propagation, structure-from-motion, and 3D reconstruction. Several joint image alignment and congealing techniques have been proposed to tackle this problem, but robustness to initialisation, ability to scale to large datasets, and alignment accuracy seem to hamper their wide applicability. To overcome these limitations, we propose an unsupervised joint alignment method leveraging a densely fused spatial transformer network to estimate the warping parameters for each image and a low-capacity auto-encoder whose reconstruction error is used as an auxiliary measure of joint alignment. Experimental results on digits from multiple versions of MNIST (i.e., original, perturbed, affNIST and infiMNIST) and faces from LFW, show that our approach is capable of aligning millions of images with high accuracy and robustness to different levels and types of perturbation. Moreover, qualitative and quantitative results suggest that the proposed method outperforms state-of-the-art approaches both in terms of alignment quality and robustness to initialisation.



### LIP: Local Importance-based Pooling
- **Arxiv ID**: http://arxiv.org/abs/1908.04156v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04156v3)
- **Published**: 2019-08-12 14:02:53+00:00
- **Updated**: 2019-08-27 08:09:44+00:00
- **Authors**: Ziteng Gao, Limin Wang, Gangshan Wu
- **Comment**: ICCV 2019 camera ready version. Code: https://github.com/sebgao/LIP
- **Journal**: None
- **Summary**: Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for discriminative tasks, there is a possibility that these layers lose the discriminative details due to improper pooling strategies, which could hinder the learning process and eventually result in suboptimal models. In this paper, we present a unified framework over the existing downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a local importance view. In this framework, we analyze the issues of these widely-used pooling layers and figure out the criteria for designing an effective downsampling layer. According to this analysis, we propose a conceptually simple, general, and effective pooling layer based on local importance modeling, termed as {\em Local Importance-based Pooling} (LIP). LIP can automatically enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. Experiment results show that LIP consistently yields notable gains with different depths and different architectures on ImageNet classification. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent improvement ($\ge 1.4\%$) over the vanilla ResNets, and especially achieve the current state-of-the-art performance in detecting small objects under the single-scale testing scheme.



### Decision Trees for Complexity Reduction in Video Compression
- **Arxiv ID**: http://arxiv.org/abs/1908.04168v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04168v1)
- **Published**: 2019-08-12 14:27:16+00:00
- **Updated**: 2019-08-12 14:27:16+00:00
- **Authors**: Natasha Westland, André Seixas Dias, Marta Mrak
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a method for complexity reduction in practical video encoders using multiple decision tree classifiers. The method is demonstrated for the fast implementation of the 'High Efficiency Video Coding' (HEVC) standard, chosen because of its high bit rate reduction capability but large complexity overhead. Optimal partitioning of each video frame into coding units (CUs) is the main source of complexity as a vast number of combinations are tested. The decision tree models were trained to identify when the CU testing process, a time-consuming Lagrangian optimisation, can be skipped i.e a high probability that the CU can remain whole. A novel approach to finding the simplest and most effective decision tree model called 'manual pruning' is described. Implementing the skip criteria reduced the average encoding time by 42.1% for a Bj{\o}ntegaard Delta rate detriment of 0.7%, for 17 standard test sequences in a range of resolutions and quantisation parameters.



### Domain-Specific Embedding Network for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.04174v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04174v1)
- **Published**: 2019-08-12 14:32:50+00:00
- **Updated**: 2019-08-12 14:32:50+00:00
- **Authors**: Shaobo Min, Hantao Yao, Hongtao Xie, Zheng-Jun Zha, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) seeks to recognize a sample from either seen or unseen domain by projecting the image data and semantic labels into a joint embedding space. However, most existing methods directly adapt a well-trained projection from one domain to another, thereby ignoring the serious bias problem caused by domain differences. To address this issue, we propose a novel Domain-Specific Embedding Network (DSEN) that can apply specific projections to different domains for unbiased embedding, as well as several domain constraints. In contrast to previous methods, the DSEN decomposes the domain-shared projection function into one domain-invariant and two domain-specific sub-functions to explore the similarities and differences between two domains. To prevent the two specific projections from breaking the semantic relationship, a semantic reconstruction constraint is proposed by applying the same decoder function to them in a cycle consistency way. Furthermore, a domain division constraint is developed to directly penalize the margin between real and pseudo image features in respective seen and unseen domains, which can enlarge the inter-domain difference of visual features. Extensive experiments on four public benchmarks demonstrate the effectiveness of DSEN with an average of $9.2\%$ improvement in terms of harmonic mean. The code is available in \url{https://github.com/mboboGO/DSEN-for-GZSL}.



### Left Ventricle Quantification Using Direct Regression with Segmentation Regularization and Ensembles of Pretrained 2D and 3D CNNs
- **Arxiv ID**: http://arxiv.org/abs/1908.04181v1
- **DOI**: 10.1007/978-3-030-39074-7_39
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04181v1)
- **Published**: 2019-08-12 14:46:00+00:00
- **Updated**: 2019-08-12 14:46:00+00:00
- **Authors**: Nils Gessert, Alexander Schlaefer
- **Comment**: Accepted at the MICCAI Workshop STACOM 2019
- **Journal**: None
- **Summary**: Cardiac left ventricle (LV) quantification provides a tool for diagnosing cardiac diseases. Automatic calculation of all relevant LV indices from cardiac MR images is an intricate task due to large variations among patients and deformation during the cardiac cycle. Typical methods are based on segmentation of the myocardium or direct regression from MR images. To consider cardiac motion and deformation, recurrent neural networks and spatio-temporal convolutional neural networks (CNNs) have been proposed. We study an approach combining state-of-the-art models and emphasizing transfer learning to account for the small dataset provided for the LVQuan19 challenge. We compare 2D spatial and 3D spatio-temporal CNNs for LV indices regression and cardiac phase classification. To incorporate segmentation information, we propose an architecture-independent segmentation-based regularization. To improve the robustness further, we employ a search scheme that identifies the optimal ensemble from a set of architecture variants. Evaluating on the LVQuan19 Challenge training dataset with 5-fold cross-validation, we achieve mean absolute errors of 111 +- 76mm^2, 1.84 +- 0.9mm and 1.22 +- 0.6mm for area, dimension and regional wall thickness regression, respectively. The error rate for cardiac phase classification is 6.7%.



### Towards Deep Learning-Based EEG Electrode Detection Using Automatically Generated Labels
- **Arxiv ID**: http://arxiv.org/abs/1908.04186v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04186v1)
- **Published**: 2019-08-12 15:03:55+00:00
- **Updated**: 2019-08-12 15:03:55+00:00
- **Authors**: Nils Gessert, Martin Gromniak, Marcel Bengs, Lars Matthäus, Alexander Schlaefer
- **Comment**: Accepted at the CURAC 2019 Conference
- **Journal**: None
- **Summary**: Electroencephalography (EEG) allows for source measurement of electrical brain activity. Particularly for inverse localization, the electrode positions on the scalp need to be known. Often, systems such as optical digitizing scanners are used for accurate localization with a stylus. However, the approach is time-consuming as each electrode needs to be scanned manually and the scanning systems are expensive. We propose using an RGBD camera to directly track electrodes in the images using deep learning methods. Studying and evaluating deep learning methods requires large amounts of labeled data. To overcome the time-consuming data annotation, we generate a large number of ground-truth labels using a robotic setup. We demonstrate that deep learning-based electrode detection is feasible with a mean absolute error of 5.69 +- 6.1mm and that our annotation scheme provides a useful environment for studying deep learning methods for electrode detection.



### Deep Learning-Based Quantification of Pulmonary Hemosiderophages in Cytology Slides
- **Arxiv ID**: http://arxiv.org/abs/1908.04767v1
- **DOI**: 10.1038/s41598-020-65958-2
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04767v1)
- **Published**: 2019-08-12 15:16:30+00:00
- **Updated**: 2019-08-12 15:16:30+00:00
- **Authors**: Christian Marzahl, Marc Aubreville, Christof A. Bertram, Jason Stayt, Anne-Katherine Jasensky, Florian Bartenschlager, Marco Fragoso-Garcia, Ann K. Barton, Svenja Elsemann, Samir Jabari, Jens Krauth, Prathmesh Madhu, Jörn Voigt, Jenny Hill, Robert Klopfleisch, Andreas Maier
- **Comment**: None
- **Journal**: Sci Rep 10, 9795 (2020)
- **Summary**: Purpose: Exercise-induced pulmonary hemorrhage (EIPH) is a common syndrome in sport horses with negative impact on performance. Cytology of bronchoalveolar lavage fluid by use of a scoring system is considered the most sensitive diagnostic method. Macrophages are classified depending on the degree of cytoplasmic hemosiderin content. The current gold standard is manual grading, which is however monotonous and time-consuming. Methods: We evaluated state-of-the-art deep learning-based methods for single cell macrophage classification and compared them against the performance of nine cytology experts and evaluated inter- and intra-observer variability. Additionally, we evaluated object detection methods on a novel data set of 17 completely annotated cytology whole slide images (WSI) containing 78,047 hemosiderophages. Resultsf: Our deep learning-based approach reached a concordance of 0.85, partially exceeding human expert concordance (0.68 to 0.86, $\mu$=0.73, $\sigma$ =0.04). Intra-observer variability was high (0.68 to 0.88) and inter-observer concordance was moderate (Fleiss kappa = 0.67). Our object detection approach has a mean average precision of 0.66 over the five classes from the whole slide gigapixel image and a computation time of below two minutes. Conclusion: To mitigate the high inter- and intra-rater variability, we propose our automated object detection pipeline, enabling accurate, reproducible and quick EIPH scoring in WSI.



### Deep Tone Mapping Operator for High Dynamic Range Images
- **Arxiv ID**: http://arxiv.org/abs/1908.04197v1
- **DOI**: 10.1109/TIP.2019.2936649
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.04197v1)
- **Published**: 2019-08-12 15:30:55+00:00
- **Updated**: 2019-08-12 15:30:55+00:00
- **Authors**: Aakanksha Rana, Praveer Singh, Giuseppe Valenzise, Frederic Dufaux, Nikos Komodakis, Aljosa Smolic
- **Comment**: None
- **Journal**: None
- **Summary**: A computationally fast tone mapping operator (TMO) that can quickly adapt to a wide spectrum of high dynamic range (HDR) content is quintessential for visualization on varied low dynamic range (LDR) output devices such as movie screens or standard displays. Existing TMOs can successfully tone-map only a limited number of HDR content and require an extensive parameter tuning to yield the best subjective-quality tone-mapped output. In this paper, we address this problem by proposing a fast, parameter-free and scene-adaptable deep tone mapping operator (DeepTMO) that yields a high-resolution and high-subjective quality tone mapped output. Based on conditional generative adversarial network (cGAN), DeepTMO not only learns to adapt to vast scenic-content (e.g., outdoor, indoor, human, structures, etc.) but also tackles the HDR related scene-specific challenges such as contrast and brightness, while preserving the fine-grained details. We explore 4 possible combinations of Generator-Discriminator architectural designs to specifically address some prominent issues in HDR related deep-learning frameworks like blurring, tiling patterns and saturation artifacts. By exploring different influences of scales, loss-functions and normalization layers under a cGAN setting, we conclude with adopting a multi-scale model for our task. To further leverage on the large-scale availability of unlabeled HDR data, we train our network by generating targets using an objective HDR quality metric, namely Tone Mapping Image Quality Index (TMQI). We demonstrate results both quantitatively and qualitatively, and showcase that our DeepTMO generates high-resolution, high-quality output images over a large spectrum of real-world scenes. Finally, we evaluate the perceived quality of our results by conducting a pair-wise subjective study which confirms the versatility of our method.



### Super-resolution of Omnidirectional Images Using Adversarial Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.04297v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04297v1)
- **Published**: 2019-08-12 16:05:59+00:00
- **Updated**: 2019-08-12 16:05:59+00:00
- **Authors**: Cagri Ozcinar, Aakanksha Rana, Aljosa Smolic
- **Comment**: None
- **Journal**: None
- **Summary**: An omnidirectional image (ODI) enables viewers to look in every direction from a fixed point through a head-mounted display providing an immersive experience compared to that of a standard image. Designing immersive virtual reality systems with ODIs is challenging as they require high resolution content. In this paper, we study super-resolution for ODIs and propose an improved generative adversarial network based model which is optimized to handle the artifacts obtained in the spherical observational space. Specifically, we propose to use a fast PatchGAN discriminator, as it needs fewer parameters and improves the super-resolution at a fine scale. We also explore the generative models with adversarial learning by introducing a spherical-content specific loss function, called 360-SS. To train and test the performance of our proposed model we prepare a dataset of 4500 ODIs. Our results demonstrate the efficacy of the proposed method and identify new challenges in ODI super-resolution for future investigations.



### Atlas: A Dataset and Benchmark for E-commerce Clothing Product Categorization
- **Arxiv ID**: http://arxiv.org/abs/1908.08984v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08984v1)
- **Published**: 2019-08-12 16:46:00+00:00
- **Updated**: 2019-08-12 16:46:00+00:00
- **Authors**: Venkatesh Umaashankar, Girish Shanmugam S, Aditi Prakash
- **Comment**: preprint
- **Journal**: None
- **Summary**: In E-commerce, it is a common practice to organize the product catalog using product taxonomy. This enables the buyer to easily locate the item they are looking for and also to explore various items available under a category. Product taxonomy is a tree structure with 3 or more levels of depth and several leaf nodes. Product categorization is a large scale classification task that assigns a category path to a particular product. Research in this area is restricted by the unavailability of good real-world datasets and the variations in taxonomy due to the absence of a standard across the different e-commerce stores. In this paper, we introduce a high-quality product taxonomy dataset focusing on clothing products which contain 186,150 images under clothing category with 3 levels and 52 leaf nodes in the taxonomy. We explain the methodology used to collect and label this dataset. Further, we establish the benchmark by comparing image classification and Attention based Sequence models for predicting the category path. Our benchmark model reaches a micro f-score of 0.92 on the test set. The dataset, code and pre-trained models are publicly available at \url{https://github.com/vumaasha/atlas}. We invite the community to improve upon these baselines.



### Automated Brain Tumour Segmentation Using Deep Fully Residual Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.04250v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04250v3)
- **Published**: 2019-08-12 16:58:50+00:00
- **Updated**: 2020-05-12 12:05:50+00:00
- **Authors**: Indrajit Mazumdar
- **Comment**: None
- **Journal**: None
- **Summary**: Automated brain tumour segmentation has the potential of making a massive improvement in disease diagnosis, surgery, monitoring and surveillance. However, this task is extremely challenging. Here, we describe our automated segmentation method using 2D CNNs that are based on U-Net. To deal with class imbalance effectively, we have used a weighted Dice loss function. We found that increasing the depth of the 'U' shape beyond a certain level results in a decrease in performance, so it is essential to choose an optimum depth. We also found that 3D contextual information cannot be captured by a single 2D network that is trained with patches extracted from multiple views whereas an ensemble of three 2D networks trained in multiple views can effectively capture the information and deliver much better performance. We obtained Dice scores of 0.79 for enhancing tumour, 0.90 for whole tumour, and 0.82 for tumour core on the BraTS 2018 validation set. Our method using 2D network consumes very less time and memory, and is much simpler and easier to implement compared to the state-of-the-art methods that used 3D networks; still, it manages to achieve comparable performance to those methods.



### Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.04321v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04321v1)
- **Published**: 2019-08-12 18:12:20+00:00
- **Updated**: 2019-08-12 18:12:20+00:00
- **Authors**: Royston Rodrigues, Neha Bhargava, Rajbabu Velmurugan, Subhasis Chaudhuri
- **Comment**: Project Page :
  https://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction/ ,
  This paper is under review in a conference
- **Journal**: None
- **Summary**: A classical approach to abnormal activity detection is to learn a representation for normal activities from the training data and then use this learned representation to detect abnormal activities while testing. Typically, the methods based on this approach operate at a fixed timescale - either a single time-instant (eg. frame-based) or a constant time duration (eg. video-clip based). But human abnormal activities can take place at different timescales. For example, jumping is a short term anomaly and loitering is a long term anomaly in a surveillance scenario. A single and pre-defined timescale is not enough to capture the wide range of anomalies occurring with different time duration. In this paper, we propose a multi-timescale model to capture the temporal dynamics at different timescales. In particular, the proposed model makes future and past predictions at different timescales for a given input pose trajectory. The model is multi-layered where intermediate layers are responsible to generate predictions corresponding to different timescales. These predictions are combined to detect abnormal activities. In addition, we also introduce an abnormal activity data-set for research use that contains 4,83,566 annotated frames. Data-set will be made available at https://rodrigues-royston.github.io/Multi-timescale_Trajectory_Prediction/ Our experiments show that the proposed model can capture the anomalies of different time duration and outperforms existing methods.



### Feature Partitioning for Efficient Multi-Task Architectures
- **Arxiv ID**: http://arxiv.org/abs/1908.04339v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04339v1)
- **Published**: 2019-08-12 19:06:32+00:00
- **Updated**: 2019-08-12 19:06:32+00:00
- **Authors**: Alejandro Newell, Lu Jiang, Chong Wang, Li-Jia Li, Jia Deng
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-task learning holds the promise of less data, parameters, and time than training of separate models. We propose a method to automatically search over multi-task architectures while taking resource constraints into consideration. We propose a search space that compactly represents different parameter sharing strategies. This provides more effective coverage and sampling of the space of multi-task architectures. We also present a method for quick evaluation of different architectures by using feature distillation. Together these contributions allow us to quickly optimize for efficient multi-task models. We benchmark on Visual Decathlon, demonstrating that we can automatically search for and identify multi-task architectures that effectively make trade-offs between task resource requirements while achieving a high level of final performance.



### Why Does a Visual Question Have Different Answers?
- **Arxiv ID**: http://arxiv.org/abs/1908.04342v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04342v2)
- **Published**: 2019-08-12 19:19:48+00:00
- **Updated**: 2019-08-14 18:55:01+00:00
- **Authors**: Nilavra Bhattacharya, Qing Li, Danna Gurari
- **Comment**: None
- **Journal**: The IEEE International Conference on Computer Vision (ICCV) 2019
- **Summary**: Visual question answering is the task of returning the answer to a question about an image. A challenge is that different people often provide different answers to the same visual question. To our knowledge, this is the first work that aims to understand why. We propose a taxonomy of nine plausible reasons, and create two labelled datasets consisting of ~45,000 visual questions indicating which reasons led to answer differences. We then propose a novel problem of predicting directly from a visual question which reasons will cause answer differences as well as a novel algorithm for this purpose. Experiments demonstrate the advantage of our approach over several related baselines on two diverse datasets. We publicly share the datasets and code at https://vizwiz.org.



### Adversarial Neural Pruning with Latent Vulnerability Suppression
- **Arxiv ID**: http://arxiv.org/abs/1908.04355v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04355v4)
- **Published**: 2019-08-12 19:33:58+00:00
- **Updated**: 2020-07-02 13:47:36+00:00
- **Authors**: Divyam Madaan, Jinwoo Shin, Sung Ju Hwang
- **Comment**: Accepted to ICML 2020. Code available at
  https://github.com/divyam3897/ANP_VS
- **Journal**: None
- **Summary**: Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.



### MULAN: Multitask Universal Lesion Analysis Network for Joint Lesion Detection, Tagging, and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.04373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04373v1)
- **Published**: 2019-08-12 20:40:12+00:00
- **Updated**: 2019-08-12 20:40:12+00:00
- **Authors**: Ke Yan, Youbao Tang, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, Ronald M. Summers
- **Comment**: MICCAI 2019, including appendix. code:
  https://github.com/rsummers11/CADLab/tree/master/MULAN_universal_lesion_analysis
- **Journal**: None
- **Summary**: When reading medical images such as a computed tomography (CT) scan, radiologists generally search across the image to find lesions, characterize and measure them, and then describe them in the radiological report. To automate this process, we propose a multitask universal lesion analysis network (MULAN) for joint detection, tagging, and segmentation of lesions in a variety of body parts, which greatly extends existing work of single-task lesion analysis on specific body parts. MULAN is based on an improved Mask R-CNN framework with three head branches and a 3D feature fusion strategy. It achieves the state-of-the-art accuracy in the detection and tagging tasks on the DeepLesion dataset, which contains 32K lesions in the whole body. We also analyze the relationship between the three tasks and show that tag predictions can improve detection accuracy via a score refinement layer.



### Point-Based Multi-View Stereo Network
- **Arxiv ID**: http://arxiv.org/abs/1908.04422v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04422v1)
- **Published**: 2019-08-12 22:21:52+00:00
- **Updated**: 2019-08-12 22:21:52+00:00
- **Authors**: Rui Chen, Songfang Han, Jing Xu, Hao Su
- **Comment**: Accepted as ICCV 2019 oral presentation
- **Journal**: None
- **Summary**: We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet .



### Learning Target-oriented Dual Attention for Robust RGB-T Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.04441v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04441v1)
- **Published**: 2019-08-12 23:54:35+00:00
- **Updated**: 2019-08-12 23:54:35+00:00
- **Authors**: Rui Yang, Yabin Zhu, Xiao Wang, Chenglong Li, Jin Tang
- **Comment**: Accepted by IEEE ICIP 2019
- **Journal**: None
- **Summary**: RGB-Thermal object tracking attempt to locate target object using complementary visual and thermal infrared data. Existing RGB-T trackers fuse different modalities by robust feature representation learning or adaptive modal weighting. However, how to integrate dual attention mechanism for visual tracking is still a subject that has not been studied yet. In this paper, we propose two visual attention mechanisms for robust RGB-T object tracking. Specifically, the local attention is implemented by exploiting the common visual attention of RGB and thermal data to train deep classifiers. We also introduce the global attention, which is a multi-modal target-driven attention estimation network. It can provide global proposals for the classifier together with local proposals extracted from previous tracking result. Extensive experiments on two RGB-T benchmark datasets validated the effectiveness of our proposed algorithm.



