# Arxiv Papers in cs.CV on 2019-08-13
### Few Labeled Atlases are Necessary for Deep-Learning-Based Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.04466v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04466v4)
- **Published**: 2019-08-13 02:39:52+00:00
- **Updated**: 2020-01-13 05:38:22+00:00
- **Authors**: Hyeon Woo Lee, Mert R. Sabuncu, Adrian V. Dalca
- **Comment**: Accepted as extended abstract to Machine Learning for Health (ML4H)
  at NeurIPS 2019
- **Journal**: None
- **Summary**: We tackle biomedical image segmentation in the scenario of only a few labeled brain MR images. This is an important and challenging task in medical applications, where manual annotations are time-consuming. Current multi-atlas based segmentation methods use image registration to warp segments from labeled images onto a new scan. In a different paradigm, supervised learning-based segmentation strategies have gained popularity. These method consistently use relatively large sets of labeled training data, and their behavior in the regime of a few labeled biomedical images has not been thoroughly evaluated. In this work, we provide two important results for segmentation in the scenario where few labeled images are available. First, we propose a straightforward implementation of efficient semi-supervised learning-based registration method, which we showcase in a multi-atlas segmentation framework. Second, through an extensive empirical study, we evaluate the performance of a supervised segmentation approach, where the training images are augmented via random deformations. Surprisingly, we find that in both paradigms, accurate segmentation is generally possible even in the context of few labeled images.



### Collaborative Multi-agent Learning for MR Knee Articular Cartilage Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.04469v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1908.04469v1)
- **Published**: 2019-08-13 02:58:17+00:00
- **Updated**: 2019-08-13 02:58:17+00:00
- **Authors**: Chaowei Tan, Zhennan Yan, Shaoting Zhang, Kang Li, Dimitris N. Metaxas
- **Comment**: None
- **Journal**: None
- **Summary**: The 3D morphology and quantitative assessment of knee articular cartilages (i.e., femoral, tibial, and patellar cartilage) in magnetic resonance (MR) imaging is of great importance for knee radiographic osteoarthritis (OA) diagnostic decision making. However, effective and efficient delineation of all the knee articular cartilages in large-sized and high-resolution 3D MR knee data is still an open challenge. In this paper, we propose a novel framework to solve the MR knee cartilage segmentation task. The key contribution is the adversarial learning based collaborative multi-agent segmentation network. In the proposed network, we use three parallel segmentation agents to label cartilages in their respective region of interest (ROI), and then fuse the three cartilages by a novel ROI-fusion layer. The collaborative learning is driven by an adversarial sub-network. The ROI-fusion layer not only fuses the individual cartilages from multiple agents, but also backpropagates the training loss from the adversarial sub-network to each agent to enable joint learning of shape and spatial constraints. Extensive evaluations are conducted on a dataset including hundreds of MR knee volumes with diverse populations, and the proposed method shows superior performance.



### Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.04501v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04501v1)
- **Published**: 2019-08-13 06:04:25+00:00
- **Updated**: 2019-08-13 06:04:25+00:00
- **Authors**: Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.



### Boosted GAN with Semantically Interpretable Information for Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1908.04503v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.04503v1)
- **Published**: 2019-08-13 06:05:24+00:00
- **Updated**: 2019-08-13 06:05:24+00:00
- **Authors**: Ang Li, Jianzhong Qi, Rui Zhang, Ramamohanarao Kotagiri
- **Comment**: None
- **Journal**: None
- **Summary**: Image inpainting aims at restoring missing region of corrupted images, which has many applications such as image restoration and object removal. However, current GAN-based inpainting models fail to explicitly consider the semantic consistency between restored images and original images. Forexample, given a male image with image region of one eye missing, current models may restore it with a female eye. This is due to the ambiguity of GAN-based inpainting models: these models can generate many possible restorations given a missing region. To address this limitation, our key insight is that semantically interpretable information (such as attribute and segmentation information) of input images (with missing regions) can provide essential guidance for the inpainting process. Based on this insight, we propose a boosted GAN with semantically interpretable information for image inpainting that consists of an inpainting network and a discriminative network. The inpainting network utilizes two auxiliary pretrained networks to discover the attribute and segmentation information of input images and incorporates them into the inpainting process to provide explicit semantic-level guidance. The discriminative network adopts a multi-level design that can enforce regularizations not only on overall realness but also on attribute and segmentation consistency with the original images. Experimental results show that our proposed model can preserve consistency on both attribute and segmentation level, and significantly outperforms the state-of-the-art models.



### Interpolated Convolutional Networks for 3D Point Cloud Understanding
- **Arxiv ID**: http://arxiv.org/abs/1908.04512v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04512v1)
- **Published**: 2019-08-13 06:44:04+00:00
- **Updated**: 2019-08-13 06:44:04+00:00
- **Authors**: Jiageng Mao, Xiaogang Wang, Hongsheng Li
- **Comment**: ICCV2019 oral. Code will be released soon
- **Journal**: None
- **Summary**: Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.



### Three Branches: Detecting Actions With Richer Features
- **Arxiv ID**: http://arxiv.org/abs/1908.04519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04519v1)
- **Published**: 2019-08-13 07:23:44+00:00
- **Updated**: 2019-08-13 07:23:44+00:00
- **Authors**: Jin Xia, Jiajun Tang, Cewu Lu
- **Comment**: None
- **Journal**: None
- **Summary**: We present our three branch solutions for International Challenge on Activity Recognition at CVPR2019. This model seeks to fuse richer information of global video clip, short human attention and long-term human activity into a unified model. We have participated in two tasks: Task A, the Kinetics challenge and Task B, spatio-temporal action localization challenge. For Kinetics, we achieve 21.59% error rate. For the AVA challenge, our final model obtains 32.49% mAP on the test sets, which outperforms all submissions to the AVA challenge at CVPR 2018 for more than 10% mAP. As the future work, we will introduce human activity knowledge, which is a new dataset including key information of human activity.



### SDM-NET: Deep Generative Network for Structured Deformable Mesh
- **Arxiv ID**: http://arxiv.org/abs/1908.04520v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04520v2)
- **Published**: 2019-08-13 07:26:15+00:00
- **Updated**: 2019-09-03 09:08:28+00:00
- **Authors**: Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, Hao Zhang
- **Comment**: Conditionally Accepted to Siggraph Asia 2019
- **Journal**: None
- **Summary**: We introduce SDM-NET, a deep generative neural network which produces structured deformable meshes. Specifically, the network is trained to generate a spatial arrangement of closed, deformable mesh parts, which respect the global part structure of a shape collection, e.g., chairs, airplanes, etc. Our key observation is that while the overall structure of a 3D shape can be complex, the shape can usually be decomposed into a set of parts, each homeomorphic to a box, and the finer-scale geometry of the part can be recovered by deforming the box. The architecture of SDM-NET is that of a two-level variational autoencoder (VAE). At the part level, a PartVAE learns a deformable model of part geometries. At the structural level, we train a Structured Parts VAE (SP-VAE), which jointly learns the part structure of a shape collection and the part geometries, ensuring a coherence between global shape structure and surface details. Through extensive experiments and comparisons with the state-of-the-art deep generative models of shapes, we demonstrate the superiority of SDM-NET in generating meshes with visual quality, flexible topology, and meaningful structures, which benefit shape interpolation and other subsequently modeling tasks.



### Multimodal Emotion Recognition Using Deep Canonical Correlation Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.05349v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05349v1)
- **Published**: 2019-08-13 09:22:23+00:00
- **Updated**: 2019-08-13 09:22:23+00:00
- **Authors**: Wei Liu, Jie-Lin Qiu, Wei-Long Zheng, Bao-Liang Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal signals are more powerful than unimodal data for emotion recognition since they can represent emotions more comprehensively. In this paper, we introduce deep canonical correlation analysis (DCCA) to multimodal emotion recognition. The basic idea behind DCCA is to transform each modality separately and coordinate different modalities into a hyperspace by using specified canonical correlation analysis constraints. We evaluate the performance of DCCA on five multimodal datasets: the SEED, SEED-IV, SEED-V, DEAP, and DREAMER datasets. Our experimental results demonstrate that DCCA achieves state-of-the-art recognition accuracy rates on all five datasets: 94.58% on the SEED dataset, 87.45% on the SEED-IV dataset, 84.33% and 85.62% for two binary classification tasks and 88.51% for a four-category classification task on the DEAP dataset, 83.08% on the SEED-V dataset, and 88.99%, 90.57%, and 90.67% for three binary classification tasks on the DREAMER dataset. We also compare the noise robustness of DCCA with that of existing methods when adding various amounts of noise to the SEED-V dataset. The experimental results indicate that DCCA has greater robustness. By visualizing feature distributions with t-SNE and calculating the mutual information between different modalities before and after using DCCA, we find that the features transformed by DCCA from different modalities are more homogeneous and discriminative across emotions.



### AIBench: An Industry Standard Internet Service AI Benchmark Suite
- **Arxiv ID**: http://arxiv.org/abs/1908.08998v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IR, cs.PF, cs.SE
- **Links**: [PDF](http://arxiv.org/pdf/1908.08998v2)
- **Published**: 2019-08-13 10:15:39+00:00
- **Updated**: 2019-10-23 14:39:47+00:00
- **Authors**: Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, Hainan Ye
- **Comment**: 24 pages
- **Journal**: None
- **Summary**: Today's Internet Services are undergoing fundamental changes and shifting to an intelligent computing era where AI is widely employed to augment services. In this context, many innovative AI algorithms, systems, and architectures are proposed, and thus the importance of benchmarking and evaluating them rises. However, modern Internet services adopt a microservice-based architecture and consist of various modules. The diversity of these modules and complexity of execution paths, the massive scale and complex hierarchy of datacenter infrastructure, the confidential issues of data sets and workloads pose great challenges to benchmarking. In this paper, we present the first industry-standard Internet service AI benchmark suite---AIBench with seventeen industry partners, including several top Internet service providers. AIBench provides a highly extensible, configurable, and flexible benchmark framework that contains loosely coupled modules. We identify sixteen prominent AI problem domains like learning to rank, each of which forms an AI component benchmark, from three most important Internet service domains: search engine, social network, and e-commerce, which is by far the most comprehensive AI benchmarking effort. On the basis of the AIBench framework, abstracting the real-world data sets and workloads from one of the top e-commerce providers, we design and implement the first end-to-end Internet service AI benchmark, which contains the primary modules in the critical paths of an industry scale application and is scalable to deploy on different cluster scales. The specifications, source code, and performance numbers are publicly available from the benchmark council web site http://www.benchcouncil.org/AIBench/index.html.



### Incorporating Task-Specific Structural Knowledge into CNNs for Brain Midline Shift Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.04568v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04568v3)
- **Published**: 2019-08-13 10:17:58+00:00
- **Updated**: 2019-12-14 13:46:19+00:00
- **Authors**: Maxim Pisov, Mikhail Goncharov, Nadezhda Kurochkina, Sergey Morozov, Victor Gombolevskiy, Valeria Chernina, Anton Vladzymyrskyy, Ksenia Zamyatina, Anna Chesnokova, Igor Pronin, Michael Shifrin, Mikhail Belyaev
- **Comment**: None
- **Journal**: None
- **Summary**: Midline shift (MLS) is a well-established factor used for outcome prediction in traumatic brain injury, stroke and brain tumors. The importance of automatic estimation of MLS was recently highlighted by ACR Data Science Institute. In this paper we introduce a novel deep learning based approach for the problem of MLS detection, which exploits task-specific structural knowledge. We evaluate our method on a large dataset containing heterogeneous images with significant MLS and show that its mean error approaches the inter-expert variability. Finally, we show the robustness of our approach by validating it on an external dataset, acquired during routine clinical practice.



### Is This The Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.04598v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04598v2)
- **Published**: 2019-08-13 12:12:05+00:00
- **Updated**: 2019-09-02 09:45:42+00:00
- **Authors**: Hajime Taira, Ignacio Rocco, Jiri Sedlar, Masatoshi Okutomi, Josef Sivic, Tomas Pajdla, Torsten Sattler, Akihiko Torii
- **Comment**: None
- **Journal**: None
- **Summary**: Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there is considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.



### Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data
- **Arxiv ID**: http://arxiv.org/abs/1908.04616v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04616v2)
- **Published**: 2019-08-13 12:56:03+00:00
- **Updated**: 2019-08-19 13:18:02+00:00
- **Authors**: Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung
- **Comment**: ICCV 2019 Oral
- **Journal**: None
- **Summary**: Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (~92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.



### Construction of efficient detectors for character information recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.04634v1
- **DOI**: None
- **Categories**: **cs.CV**, 94A08, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1908.04634v1)
- **Published**: 2019-08-13 13:39:17+00:00
- **Updated**: 2019-08-13 13:39:17+00:00
- **Authors**: A. A. Telnykh, I. V. Nuidel, Yu. R. Samorodova
- **Comment**: 14 pages, 7 figures, 1 table
- **Journal**: None
- **Summary**: We have developed and tested in numerical experiments a universal approach to searching objects of a given type in captured video images (for example, people's faces, vehicles, special characters, numbers and letters, etc.). The novelty and versatility of this approach consists in a unique combination of the well-known methods ranging from creating detectors to making decisions independent of the type of recognition objects. The efficiencies of various types of basic features used for image coding, including the Haar features, the LBP features, and the modified Census transformation are compared. A combination of the modified methods is used for constructing 11 types of detectors of the number of railway carriages and for recognizing digits from zero to nine. The efficiency of the constructed detectors is studied.



### Matrix Nets: A New Deep Architecture for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.04646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04646v2)
- **Published**: 2019-08-13 13:54:31+00:00
- **Updated**: 2019-08-14 04:11:13+00:00
- **Authors**: Abdullah Rashwan, Agastya Kalra, Pascal Poupart
- **Comment**: Short paper, stay tuned for the full paper!
- **Journal**: None
- **Summary**: We present Matrix Nets (xNets), a new deep architecture for object detection. xNets map objects with different sizes and aspect ratios into layers where the sizes and the aspect ratios of the objects within their layers are nearly uniform. Hence, xNets provide a scale and aspect ratio aware architecture. We leverage xNets to enhance key-points based object detection. Our architecture achieves mAP of 47.8 on MS COCO, which is higher than any other single-shot detector while using half the number of parameters and training 3x faster than the next best architecture.



### Detecting semantic anomalies
- **Arxiv ID**: http://arxiv.org/abs/1908.04388v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04388v3)
- **Published**: 2019-08-13 14:56:40+00:00
- **Updated**: 2019-11-21 20:48:53+00:00
- **Authors**: Faruk Ahmed, Aaron Courville
- **Comment**: Preprint for AAAI '20 publication
- **Journal**: None
- **Summary**: We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that out-distributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.



### Generalizing Deep Whole Brain Segmentation for Pediatric and Post-Contrast MRI with Augmented Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.04702v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04702v1)
- **Published**: 2019-08-13 15:27:19+00:00
- **Updated**: 2019-08-13 15:27:19+00:00
- **Authors**: Camilo Bermudez, Justin Blaber, Samuel W. Remedios, Jess E. Reynolds, Catherine Lebel, Maureen McHugo, Stephan Heckers, Yuankai Huo, Bennett A. Landman
- **Comment**: None
- **Journal**: None
- **Summary**: Generalizability is an important problem in deep neural networks, especially in the context of the variability of data acquisition in clinical magnetic resonance imaging (MRI). Recently, the Spatially Localized Atlas Network Tiles (SLANT) approach has been shown to effectively segment whole brain non-contrast T1w MRI with 132 volumetric labels. Enhancing generalizability of SLANT would enable broader application of volumetric assessment in multi-site studies. Transfer learning (TL) is commonly used to update the neural network weights for local factors; yet, it is commonly recognized to risk degradation of performance on the original validation/test cohorts. Here, we explore TL by data augmentation to address these concerns in the context of adapting SLANT to anatomical variation and scanning protocol. We consider two datasets: First, we optimize for age with 30 T1w MRI of young children with manually corrected volumetric labels, and accuracy of automated segmentation defined relative to the manually provided truth. Second, we optimize for acquisition with 36 paired datasets of pre- and post-contrast clinically acquired T1w MRI, and accuracy of the post-contrast segmentations assessed relative to the pre-contrast automated assessment. For both studies, we augment the original TL step of SLANT with either only the new data or with both original and new data. Over baseline SLANT, both approaches yielded significantly improved performance (signed rank tests; pediatric: 0.89 vs. 0.82 DSC, p<0.001; contrast: 0.80 vs 0.76, p<0.001). The performance on the original test set decreased with the new-data only transfer learning approach, so data augmentation was superior to strict transfer learning.



### MEx: Multi-modal Exercises Dataset for Human Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.08992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08992v1)
- **Published**: 2019-08-13 16:09:53+00:00
- **Updated**: 2019-08-13 16:09:53+00:00
- **Authors**: Anjana Wijekoon, Nirmalie Wiratunga, Kay Cooper
- **Comment**: None
- **Journal**: None
- **Summary**: MEx: Multi-modal Exercises Dataset is a multi-sensor, multi-modal dataset, implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion algorithms. Collection of this dataset was inspired by the need for recognising and evaluating quality of exercise performance to support patients with Musculoskeletal Disorders(MSD). We select 7 exercises regularly recommended for MSD patients by physiotherapists and collected data with four sensors a pressure mat, a depth camera and two accelerometers. The dataset contains three data modalities; numerical time-series data, video data and pressure sensor data posing interesting research challenges when reasoning for HAR and Exercise Quality Assessment. This paper presents our evaluation of the dataset on number of standard classification algorithms for the HAR task by comparing different feature representation algorithms for each sensor. These results set a reference performance for each individual sensor that expose their strengths and weaknesses for the future tasks. In addition we visualise pressure mat data to explore the potential of the sensor to capture exercise performance quality. With the recent advancement in multi-modal fusion, we also believe MEx is a suitable dataset to benchmark not only HAR algorithms, but also fusion algorithms of heterogeneous data types in multiple application domains.



### Learning elementary structures for 3D shape generation and matching
- **Arxiv ID**: http://arxiv.org/abs/1908.04725v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.04725v2)
- **Published**: 2019-08-13 16:36:51+00:00
- **Updated**: 2019-08-14 17:30:02+00:00
- **Authors**: Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.



### Predicting 3D Human Dynamics from Video
- **Arxiv ID**: http://arxiv.org/abs/1908.04781v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04781v2)
- **Published**: 2019-08-13 17:58:36+00:00
- **Updated**: 2019-08-20 17:58:41+00:00
- **Authors**: Jason Y. Zhang, Panna Felsen, Angjoo Kanazawa, Jitendra Malik
- **Comment**: To Appear in ICCV 2019. (v2: Updated "3D Pose from Video" in Related
  Work.)
- **Journal**: None
- **Summary**: Given a video of a person in action, we can easily guess the 3D future motion of the person. In this work, we present perhaps the first approach for predicting a future 3D mesh model sequence of a person from past video input. We do this for periodic motions such as walking and also actions like bowling and squatting seen in sports or workout videos. While there has been a surge of future prediction problems in computer vision, most approaches predict 3D future from 3D past or 2D future from 2D past inputs. In this work, we focus on the problem of predicting 3D future motion from past image sequences, which has a plethora of practical applications in autonomous systems that must operate safely around people from visual inputs. Inspired by the success of autoregressive models in language modeling tasks, we learn an intermediate latent space on which we predict the future. This effectively facilitates autoregressive predictions when the input differs from the output domain. Our approach can be trained on video sequences obtained in-the-wild without 3D ground truth labels. The project website with videos can be found at https://jasonyzhang.com/phd.



### SP-NET: One Shot Fingerprint Singular-Point Detector
- **Arxiv ID**: http://arxiv.org/abs/1908.04842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04842v1)
- **Published**: 2019-08-13 19:51:58+00:00
- **Updated**: 2019-08-13 19:51:58+00:00
- **Authors**: Geetika Arora, Ranjeet Ranjan Jha, Akash Agrawal, Kamlesh Tiwari, Aditya Nigam
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Singular points of a fingerprint image are special locations having high curvature properties. They can play a pivotal role in fingerprint normalization and reliable feature extraction. Accurate and efficient extraction of a singular point plays a major role in successful fingerprint recognition and indexing. In this paper, a novel deep learning based architecture is proposed for one shot (end-to-end) singular point detection from an input fingerprint image. The model consists of a Macro-Localization Network and a Micro-Regression Network along with three stacked hourglass as a bottleneck. The proposed model has been tested on three databases viz. FVC2002 DB1_A, FVC2002 DB2_A and FPL30K and has been found to achieve true detection rate of 98.75%, 97.5% and 92.72% respectively, which is better than any other state-of-the-art technique.



