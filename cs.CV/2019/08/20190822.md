# Arxiv Papers in cs.CV on 2019-08-22
### Multiple instance dense connected convolution neural network for aerial image scene classification
- **Arxiv ID**: http://arxiv.org/abs/1908.08156v1
- **DOI**: 10.1109/TIP.2020.2975718
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08156v1)
- **Published**: 2019-08-22 00:59:47+00:00
- **Updated**: 2019-08-22 00:59:47+00:00
- **Authors**: Qi Bi, Kun Qin, Zhili Li, Han Zhang, Kai Xu
- **Comment**: 5 pages,3 figures, a conference paper accepted by IEEE ICIP 2019
- **Journal**: None
- **Summary**: With the development of deep learning, many state-of-the-art natural image scene classification methods have demonstrated impressive performance. While the current convolution neural network tends to extract global features and global semantic information in a scene, the geo-spatial objects can be located at anywhere in an aerial image scene and their spatial arrangement tends to be more complicated. One possible solution is to preserve more local semantic information and enhance feature propagation. In this paper, an end to end multiple instance dense connected convolution neural network (MIDCCNN) is proposed for aerial image scene classification. First, a 23 layer dense connected convolution neural network (DCCNN) is built and served as a backbone to extract convolution features. It is capable of preserving middle and low level convolution features. Then, an attention based multiple instance pooling is proposed to highlight the local semantics in an aerial image scene. Finally, we minimize the loss between the bag-level predictions and the ground truth labels so that the whole framework can be trained directly. Experiments on three aerial image datasets demonstrate that our proposed methods can outperform current baselines by a large margin.



### Globally optimal registration of noisy point clouds
- **Arxiv ID**: http://arxiv.org/abs/1908.08162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08162v1)
- **Published**: 2019-08-22 01:24:09+00:00
- **Updated**: 2019-08-22 01:24:09+00:00
- **Authors**: Rangaprasad Arun Srivatsan, Tejas Zodage, Howie Choset
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Registration of 3D point clouds is a fundamental task in several applications of robotics and computer vision. While registration methods such as iterative closest point and variants are very popular, they are only locally optimal. There has been some recent work on globally optimal registration, but they perform poorly in the presence of noise in the measurements. In this work we develop a mixed integer programming-based approach for globally optimal registration that explicitly considers uncertainty in its optimization, and hence produces more accurate estimates. Furthermore, from a practical implementation perspective we develop a multi-step optimization that combines fast local methods with our accurate global formulation. Through extensive simulation and real world experiments we demonstrate improved performance over state-of-the-art methods for various level of noise and outliers in the data as well as for partial geometric overlap.



### Building change detection based on multi-scale filtering and grid partition
- **Arxiv ID**: http://arxiv.org/abs/1908.08164v1
- **DOI**: 10.1109/PRRS.2018.8486194
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08164v1)
- **Published**: 2019-08-22 01:38:47+00:00
- **Updated**: 2019-08-22 01:38:47+00:00
- **Authors**: Qi Bi, Kun Qin, Han Zhang, Wenjun Han, Zhili Li, Kai Xu
- **Comment**: 8 pages, 6 figures, conference paper
- **Journal**: 10th IAPR Workshop on Pattern Recognition in Remote Sensing
  (PRRS),2018,1-6
- **Summary**: Building change detection is of great significance in high resolution remote sensing applications. Multi-index learning, one of the state-of-the-art building change detection methods, still has drawbacks like incapability to find change types directly and heavy computation consumption of MBI. In this paper, a two-stage building change detection method is proposed to address these problems. In the first stage, a multi-scale filtering building index (MFBI) is calculated to detect building areas in each temporal with fast speed and moderate accuracy. In the second stage, images and the corresponding building maps are partitioned into grids. In each grid, the ratio of building areas in time T2 and time T1 is calculated. Each grid is classified into one of the three change patterns, i.e., significantly increase, significantly decrease and approximately unchanged. Exhaustive experiments indicate that the proposed method can detect building change types directly and outperform the current multi-index learning method.



### Multi-Stream Single Shot Spatial-Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.08178v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08178v1)
- **Published**: 2019-08-22 03:13:22+00:00
- **Updated**: 2019-08-22 03:13:22+00:00
- **Authors**: Pengfei Zhang, Yu Cao, Benyuan Liu
- **Comment**: None
- **Journal**: 26th IEEE International Conference on Image Processing (ICIP 2019)
- **Summary**: We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (1) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (2) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.



### Pro-Cam SSfM: Projector-Camera System for Structure and Spectral Reflectance from Motion
- **Arxiv ID**: http://arxiv.org/abs/1908.08185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08185v1)
- **Published**: 2019-08-22 03:35:48+00:00
- **Updated**: 2019-08-22 03:35:48+00:00
- **Authors**: Chunyu Li, Yusuke Monno, Hironori Hidaka, Masatoshi Okutomi
- **Comment**: Accepted by ICCV 2019. Project homepage:
  http://www.ok.sc.e.titech.ac.jp/res/PCSSfM/
- **Journal**: None
- **Summary**: In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. In our system, we use a standard RGB camera and leverage an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first reconstruct the 3D points while estimating the poses of the camera and the projector, which are alternately moved around the object, by combining multi-view structured light and structure-from-motion (SfM) techniques. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation model considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on several real objects demonstrate that our system can precisely acquire a dense 3D model with the full spectral reflectance property using off-the-shelf devices.



### An Image Fusion Scheme for Single-Shot High Dynamic Range Imaging with Spatially Varying Exposures
- **Arxiv ID**: http://arxiv.org/abs/1908.08195v1
- **DOI**: 10.1587/transfun.E102.A.1856
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08195v1)
- **Published**: 2019-08-22 04:21:10+00:00
- **Updated**: 2019-08-22 04:21:10+00:00
- **Authors**: Chihiro Go, Yuma Kinoshita, Sayaka Shiota, Hitoshi Kiya
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel multi-exposure image fusion (MEF) scheme for single-shot high dynamic range imaging with spatially varying exposures (SVE). Single-shot imaging with SVE enables us not only to produce images without color saturation regions from a single-shot image, but also to avoid ghost artifacts in the producing ones. However, the number of exposures is generally limited to two, and moreover it is difficult to decide the optimum exposure values before the photographing. In the proposed scheme, a scene segmentation method is applied to input multi-exposure images, and then the luminance of the input images is adjusted according to both of the number of scenes and the relationship between exposure values and pixel values. The proposed method with the luminance adjustment allows us to improve the above two issues. In this paper, we focus on dual-ISO imaging as one of single-shot imaging. In an experiment, the proposed scheme is demonstrated to be effective for single-shot high dynamic range imaging with SVE, compared with conventional MEF schemes with exposure compensation.



### Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes
- **Arxiv ID**: http://arxiv.org/abs/1908.08207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08207v1)
- **Published**: 2019-08-22 05:31:53+00:00
- **Updated**: 2019-08-22 05:31:53+00:00
- **Authors**: Minghui Liao, Pengyuan Lyu, Minghang He, Cong Yao, Wenhao Wu, Xiang Bai
- **Comment**: Accepted by TPAMI. An extension of the conference version. arXiv
  admin note: text overlap with arXiv:1807.02242
- **Journal**: None
- **Summary**: Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.



### BIM-assisted object recognition for the on-site autonomous robotic assembly of discrete structures
- **Arxiv ID**: http://arxiv.org/abs/1908.08209v1
- **DOI**: 10.1007/s41693-019-00021-9
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08209v1)
- **Published**: 2019-08-22 05:44:08+00:00
- **Updated**: 2019-08-22 05:44:08+00:00
- **Authors**: Mohamed Dawod, Sean Hanna
- **Comment**: None
- **Journal**: Construction Robotics, 2019
- **Summary**: Robots-operating autonomous assembly applications in an unstructured environment require precise methods to locate the building components on site. However, the current available object detection systems are not well-optimised for construction applications, due to the tedious setups incorporated for referencing an object to a system and inability to cope with the elements imperfections. In this paper, we propose a flexible object pose estimation framework to enable robots to autonomously handle building components on-site with an error tolerance to build a specific design target without the need to sort or label them. We implemented an object recognition approach that uses the virtual representation model of all the objects found in a BIM model to autonomously search for the best-matched objects in a scene. The design layout is used to guide the robot to grasp and manipulate the found elements to build the desired structure. We verify our proposed framework by testing it in an automatic discrete wall assembly workflow. Although the precision is not as expected, we analyse the possible reasons that might cause this imprecision, which paves the path for future improvements.



### 3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.08216v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08216v2)
- **Published**: 2019-08-22 06:20:38+00:00
- **Updated**: 2019-11-18 12:28:41+00:00
- **Authors**: Sanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan, Ling Shao
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net.



### NL-LinkNet: Toward Lighter but More Accurate Road Extraction with Non-Local Operations
- **Arxiv ID**: http://arxiv.org/abs/1908.08223v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08223v3)
- **Published**: 2019-08-22 06:56:57+00:00
- **Updated**: 2020-11-11 10:25:37+00:00
- **Authors**: Yooseung Wang, Junghoon Seo, Taegyun Jeon
- **Comment**: IEEE Geoscience and Remote Sensing Letters (2020, to appear)
- **Journal**: None
- **Summary**: Road extraction from very high resolution satellite (VHR) images is one of the most important topics in the field of remote sensing. In this paper, we propose an efficient Non-Local LinkNet with non-local blocks that can grasp relations between global features. This enables each spatial feature point to refer to all other contextual information and results in more accurate road segmentation. In detail, our single model without any post-processing like CRF refinement, performed better than any other published state-of-the-art ensemble model in the official DeepGlobe Challenge. Moreover, our NL-LinkNet beat the D-LinkNet, the winner of the DeepGlobe challenge, with 43 \% less parameters, less giga floating-point operations per seconds (GFLOPs) and shorter training convergence time. We also present empirical analyses on the proper usages of non-local blocks for the baseline model.



### Progressive Face Super-Resolution via Attention to Facial Landmark
- **Arxiv ID**: http://arxiv.org/abs/1908.08239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08239v1)
- **Published**: 2019-08-22 07:50:41+00:00
- **Updated**: 2019-08-22 07:50:41+00:00
- **Authors**: Deokyun Kim, Minseon Kim, Gihyun Kwon, Dae-Shik Kim
- **Comment**: BMVC 2019 Accepted
- **Journal**: None
- **Summary**: Face Super-Resolution (SR) is a subfield of the SR domain that specifically targets the reconstruction of face images. The main challenge of face SR is to restore essential facial features without distortion. We propose a novel face SR method that generates photo-realistic 8x super-resolved face images with fully retained facial details. To that end, we adopt a progressive training method, which allows stable training by splitting the network into successive steps, each producing output with a progressively higher resolution. We also propose a novel facial attention loss and apply it at each step to focus on restoring facial attributes in greater details by multiplying the pixel difference and heatmap values. Lastly, we propose a compressed version of the state-of-the-art face alignment network (FAN) for landmark heatmap extraction. With the proposed FAN, we can extract the heatmaps suitable for face SR and also reduce the overall training time. Experimental results verify that our method outperforms state-of-the-art methods in both qualitative and quantitative measurements, especially in perceptual quality.



### Uncertainty-Guided Domain Alignment for Layer Segmentation in OCT Images
- **Arxiv ID**: http://arxiv.org/abs/1908.08242v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08242v2)
- **Published**: 2019-08-22 08:07:11+00:00
- **Updated**: 2019-08-30 01:13:17+00:00
- **Authors**: Jiexiang Wang, Cheng Bian, Meng Li, Xin Yang, Kai Ma, Wenao Ma, Jin Yuan, Xinghao Ding, Yefeng Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic and accurate segmentation for retinal and choroidal layers of Optical Coherence Tomography (OCT) is crucial for detection of various ocular diseases. However, because of the variations in different equipments, OCT data obtained from different manufacturers might encounter appearance discrepancy, which could lead to performance fluctuation to a deep neural network. In this paper, we propose an uncertainty-guided domain alignment method to aim at alleviating this problem to transfer discriminative knowledge across distinct domains. We disign a novel uncertainty-guided cross-entropy loss for boosting the performance over areas with high uncertainty. An uncertainty-guided curriculum transfer strategy is developed for the self-training (ST), which regards uncertainty as efficient and effective guidance to optimize the learning process in target domain. Adversarial learning with feature recalibration module (FRM) is applied to transfer informative knowledge from the domain feature spaces adaptively. The experiments on two OCT datasets show that the proposed methods can obtain significant segmentation improvements compared with the baseline models.



### Object detection on aerial imagery using CenterNet
- **Arxiv ID**: http://arxiv.org/abs/1908.08244v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.08244v1)
- **Published**: 2019-08-22 08:10:23+00:00
- **Updated**: 2019-08-22 08:10:23+00:00
- **Authors**: Dheeraj Reddy Pailla, Varghese Kollerathu, Sai Saketh Chennamsetty
- **Comment**: 6 pages, 6 tables, 2 figures
- **Journal**: None
- **Summary**: Detection and classification of objects in aerial imagery have several applications like urban planning, crop surveillance, and traffic surveillance. However, due to the lower resolution of the objects and the effect of noise in aerial images, extracting distinguishing features for the objects is a challenge. We evaluate CenterNet, a state of the art method for real-time 2D object detection, on the VisDrone2019 dataset. We evaluate the performance of the model with different backbone networks in conjunction with varying resolutions during training and testing.



### Optimal input configuration of dynamic contrast enhanced MRI in convolutional neural networks for liver segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.08251v1
- **DOI**: 10.1117/12.2506770
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08251v1)
- **Published**: 2019-08-22 08:27:28+00:00
- **Updated**: 2019-08-22 08:27:28+00:00
- **Authors**: Mariëlle J. A. Jansen, Hugo J. Kuijf, Josien P. W. Pluim
- **Comment**: Submitted to SPIE Medical Imaging 2019
- **Journal**: None
- **Summary**: Most MRI liver segmentation methods use a structural 3D scan as input, such as a T1 or T2 weighted scan. Segmentation performance may be improved by utilizing both structural and functional information, as contained in dynamic contrast enhanced (DCE) MR series. Dynamic information can be incorporated in a segmentation method based on convolutional neural networks in a number of ways. In this study, the optimal input configuration of DCE MR images for convolutional neural networks (CNNs) is studied. The performance of three different input configurations for CNNs is studied for a liver segmentation task. The three configurations are I) one phase image of the DCE-MR series as input image; II) the separate phases of the DCE-MR as input images; and III) the separate phases of the DCE-MR as channels of one input image. The three input configurations are fed into a dilated fully convolutional network and into a small U-net. The CNNs were trained using 19 annotated DCE-MR series and tested on another 19 annotated DCE-MR series. The performance of the three input configurations for both networks is evaluated against manual annotations. The results show that both neural networks perform better when the separate phases of the DCE-MR series are used as channels of an input image in comparison to one phase as input image or the separate phases as input images. No significant difference between the performances of the two network architectures was found for the separate phases as channels of an input image.



### Motion correction of dynamic contrast enhanced MRI of the liver
- **Arxiv ID**: http://arxiv.org/abs/1908.08254v1
- **DOI**: 10.1117/12.2253842
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08254v1)
- **Published**: 2019-08-22 08:39:51+00:00
- **Updated**: 2019-08-22 08:39:51+00:00
- **Authors**: Mariëlle J. A. Jansen, Wouter B. Veldhuis, Maarten S. van Leeuwen, Josien P. W. Pluim
- **Comment**: None
- **Journal**: None
- **Summary**: Motion correction of dynamic contrast enhanced magnetic resonance images (DCE-MRI) is a challenging task, due to changes in image appearance. In this study a groupwise registration, using a principle component analysis (PCA) based metric,1 is evaluated for clinical DCE MRI of the liver. The groupwise registration transforms the images to a common space, rather than to a reference volume as conventional pairwise methods do, and computes the similarity metric on all volumes simultaneously. This groupwise registration method is compared to a pairwise approach using a mutual information metric. Clinical DCE MRI of the abdomen of eight patients were included. Per patient one lesion in the liver was manually segmented in all temporal images (N=16). The registered images were compared for accuracy, spatial and temporal smoothness after transformation, and lesion volume change. Compared to a pairwise method or no registration, groupwise registration provided better alignment. In our recently started clinical study groupwise registered clinical DCE MRI of the abdomen of nine patients were scored by three radiologists. Groupwise registration increased the assessed quality of alignment. The gain in reading time for the radiologist was estimated to vary from no difference to almost a minute. A slight increase in reader confidence was also observed. Registration had no added value for images with little motion. In conclusion, the groupwise registration of DCE MR images results in better alignment than achieved by pairwise registration, which is beneficial for clinical assessment.



### Contour Detection in Cassini ISS images based on Hierarchical Extreme Learning Machine and Dense Conditional Random Field
- **Arxiv ID**: http://arxiv.org/abs/1908.08279v1
- **DOI**: 10.1088/1674-4527/20/1/11
- **Categories**: **astro-ph.IM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08279v1)
- **Published**: 2019-08-22 09:33:30+00:00
- **Updated**: 2019-08-22 09:33:30+00:00
- **Authors**: Xiqi Yang, Qingfeng Zhang, Zhan Li
- **Comment**: None
- **Journal**: None
- **Summary**: In Cassini ISS (Imaging Science Subsystem) images, contour detection is often performed on disk-resolved object to accurately locate their center. Thus, the contour detection is a key problem. Traditional edge detection methods, such as Canny and Roberts, often extract the contour with too much interior details and noise. Although the deep convolutional neural network has been applied successfully in many image tasks, such as classification and object detection, it needs more time and computer resources. In the paper, a contour detection algorithm based on H-ELM (Hierarchical Extreme Learning Machine) and DenseCRF (Dense Conditional Random Field) is proposed for Cassini ISS images. The experimental results show that this algorithm's performance is better than both traditional machine learning methods such as SVM, ELM and even deep convolutional neural network. And the extracted contour is closer to the actual contour. Moreover, it can be trained and tested quickly on the general configuration of PC, so can be applied to contour detection for Cassini ISS images.



### Efficient Capon-Based Approach Exploiting Temporal Windowing For Electric Network Frequency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.08813v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08813v2)
- **Published**: 2019-08-22 09:57:23+00:00
- **Updated**: 2019-10-16 08:12:42+00:00
- **Authors**: Georgios Karantaidis, Constantine Kotropoulos
- **Comment**: 6 pages, 1 figure, IEEE International Workshop on Machine Learning
  For Signal Processing (MLSP) 2019
- **Journal**: None
- **Summary**: Electric Network Frequency (ENF) fluctuations constitute a powerful tool in multimedia forensics. An efficient approach for ENF estimation is introduced with temporal windowing based on the filter-bank Capon spectral estimator. A type of Gohberg-Semencul factorization of the model covariance matrix is used due to the Toeplitz structure of the covariance matrix. Moreover, this approach uses, for the first time in the field of ENF, a temporal window, not necessarily the rectangular one, at the stage preceding spectral estimation. Krylov matrices are employed for fast implementation of matrix inversions. The proposed approach outperforms the state-of-the-art methods in ENF estimation, when a short time window of $1$ second is employed in power recordings. In speech recordings, the proposed approach yields highly accurate results with respect to both time complexity and accuracy. Moreover, the impact of different temporal windows is studied. The results show that even the most trivial methods for ENF estimation, such as the Short-Time Fourier Transform, can provide better results than the most recent state-of-the-art methods, when a temporal window is employed. The correlation coefficient is used to measure the ENF estimation accuracy.



### Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.08289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08289v1)
- **Published**: 2019-08-22 10:03:30+00:00
- **Updated**: 2019-08-22 10:03:30+00:00
- **Authors**: Jiahao Lin, Gim Hee Lee
- **Comment**: 13 pages, 5 figures. Accepted in BMVC 2019
- **Journal**: None
- **Summary**: Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing state-of-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input. In this paper, we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence. More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed from matrix factorization approaches such as Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. We demonstrate the effectiveness of our framework on long sequences by achieving state-of-the-art performances on multiple benchmark datasets. Our source code is available at: https://github.com/jiahaoLjh/trajectory-pose-3d.



### EGNet:Edge Guidance Network for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.08297v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08297v1)
- **Published**: 2019-08-22 10:36:36+00:00
- **Updated**: 2019-08-22 10:36:36+00:00
- **Authors**: Jia-Xing Zhao, Jiangjiang Liu, Den-Ping Fan, Yang Cao, Jufeng Yang, Ming-Ming Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: Fully convolutional neural networks (FCNs) have shown their advantages in the salient object detection task. However, most existing FCNs-based methods still suffer from coarse object boundaries. In this paper, to solve this problem, we focus on the complementarity between salient edge information and salient object information. Accordingly, we present an edge guidance network (EGNet) for salient object detection with three steps to simultaneously model these two kinds of complementary information in a single network. In the first step, we extract the salient object features by a progressive fusion way. In the second step, we integrate the local edge information and global location information to obtain the salient edge features. Finally, to sufficiently leverage these complementary features, we couple the same salient edge features with salient object features at various resolutions. Benefiting from the rich edge information and location information in salient edge features, the fused features can help locate salient objects, especially their boundaries more accurately. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on six widely used datasets without any pre-processing and post-processing. The source code is available at http: //mmcheng.net/egnet/.



### Image Colorization By Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.08307v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08307v1)
- **Published**: 2019-08-22 11:03:14+00:00
- **Updated**: 2019-08-22 11:03:14+00:00
- **Authors**: Gökhan Özbulak
- **Comment**: Accepted to New Trends in Image Restoration and Enhancement(NTIRE)
  Workshop at CVPR 2019
- **Journal**: None
- **Summary**: In this paper, a simple topology of Capsule Network (CapsNet) is investigated for the problem of image colorization. The generative and segmentation capabilities of the original CapsNet topology, which is proposed for image classification problem, is leveraged for the colorization of the images by modifying the network as follows:1) The original CapsNet model is adapted to map the grayscale input to the output in the CIE Lab colorspace, 2) The feature detector part of the model is updated by using deeper feature layers inherited from VGG-19 pre-trained model with weights in order to transfer low-level image representation capability to this model, 3) The margin loss function is modified as Mean Squared Error (MSE) loss to minimize the image-to-imagemapping. The resulting CapsNet model is named as Colorizer Capsule Network (ColorCapsNet).The performance of the ColorCapsNet is evaluated on the DIV2K dataset and promising results are obtained to investigate Capsule Networks further for image colorization problem.



### Deep Green Function Convolution for Improving Saliency in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.08331v2
- **DOI**: 10.1007/s00371-020-01795-8
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08331v2)
- **Published**: 2019-08-22 12:14:26+00:00
- **Updated**: 2019-11-15 04:07:46+00:00
- **Authors**: Dominique Beaini, Sofiane Achiche, Alexandre Duperré, Maxime Raison
- **Comment**: 15 pages, 11 figures
- **Journal**: None
- **Summary**: Current saliency methods require to learn large scale regional features using small convolutional kernels, which is not possible with a simple feed-forward network. Some methods solve this problem by using segmentation into superpixels while others downscale the image through the network and rescale it back to its original size. The objective of this paper is to show that saliency convolutional neural networks (CNN) can be improved by using a Green's function convolution (GFC) to extrapolate edges features into salient regions. The GFC acts as a gradient integrator, allowing to produce saliency features by filling thin edges directly inside the CNN. Hence, we propose the gradient integration and sum (GIS) layer that combines the edges features with the saliency features. Using the HED and DSS architecture, we demonstrated that adding a GIS layer near the network's output allows to reduce the sensitivity to the parameter initialization, to reduce the overfitting and to improve the repeatability of the training. By simply adding a GIS layer to the state-of-the-art DSS model, there is an absolute increase of 1.6% for the F-measure on the DUT-OMRON dataset, with only 10ms of additional computation time. The GIS layer further allows the network to perform significantly better in the case of highly noisy images or low-brightness images. In fact, we observed an F-measure improvement of 5.2% when noise was added to the dataset and 2.8% when the brightness was reduced. Since the GIS layer is model agnostic, it can be implemented into different fully convolutional networks. A major contribution of the current work is the first implementation of Green's function convolution inside a neural network, which allows the network to operate in the feature domain and in the gradient domain at the same time, thus improving the regional representation via edge filling.



### Indoor Depth Completion with Boundary Consistency and Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/1908.08344v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08344v3)
- **Published**: 2019-08-22 12:58:43+00:00
- **Updated**: 2022-06-09 01:13:50+00:00
- **Authors**: Yu-Kai Huang, Tsung-Han Wu, Yueh-Cheng Liu, Winston H. Hsu
- **Comment**: Accepted by ICCVW (RLQ) 2019. The code is available at
  https://github.com/tsunghan-wu/Depth-Completion
- **Journal**: None
- **Summary**: Depth estimation features are helpful for 3D recognition. Commodity-grade depth cameras are able to capture depth and color image in real-time. However, glossy, transparent or distant surface cannot be scanned properly by the sensor. As a result, enhancement and restoration from sensing depth is an important task. Depth completion aims at filling the holes that sensors fail to detect, which is still a complex task for machine to learn. Traditional hand-tuned methods have reached their limits, while neural network based methods tend to copy and interpolate the output from surrounding depth values. This leads to blurred boundaries, and structures of the depth map are lost. Consequently, our main work is to design an end-to-end network improving completion depth maps while maintaining edge clarity. We utilize self-attention mechanism, previously used in image inpainting fields, to extract more useful information in each layer of convolution so that the complete depth map is enhanced. In addition, we propose boundary consistency concept to enhance the depth map quality and structure. Experimental results validate the effectiveness of our self-attention and boundary consistency schema, which outperforms previous state-of-the-art depth completion work on Matterport3D dataset. Our code is publicly available at https://github.com/tsunghan-wu/Depth-Completion.



### Saliency Methods for Explaining Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1908.08413v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08413v4)
- **Published**: 2019-08-22 14:44:02+00:00
- **Updated**: 2019-10-21 11:36:36+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: Human-Centric Machine Learning, NeurIPS 2019 Workshop
- **Summary**: The classification decisions of neural networks can be misled by small imperceptible perturbations. This work aims to explain the misled classifications using saliency methods. The idea behind saliency methods is to explain the classification decisions of neural networks by creating so-called saliency maps. Unfortunately, a number of recent publications have shown that many of the proposed saliency methods do not provide insightful explanations. A prominent example is Guided Backpropagation (GuidedBP), which simply performs (partial) image recovery. However, our numerical analysis shows the saliency maps created by GuidedBP do indeed contain class-discriminative information. We propose a simple and efficient way to enhance the saliency maps. The proposed enhanced GuidedBP shows the state-of-the-art performance to explain adversary classifications.



### Noise Flow: Noise Modeling with Conditional Normalizing Flows
- **Arxiv ID**: http://arxiv.org/abs/1908.08453v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08453v1)
- **Published**: 2019-08-22 15:30:32+00:00
- **Updated**: 2019-08-22 15:30:32+00:00
- **Authors**: Abdelrahman Abdelhamed, Marcus A. Brubaker, Michael S. Brown
- **Comment**: None
- **Journal**: None
- **Summary**: Modeling and synthesizing image noise is an important aspect in many computer vision applications. The long-standing additive white Gaussian and heteroscedastic (signal-dependent) noise models widely used in the literature provide only a coarse approximation of real sensor noise. This paper introduces Noise Flow, a powerful and accurate noise model based on recent normalizing flow architectures. Noise Flow combines well-established basic parametric noise models (e.g., signal-dependent noise) with the flexibility and expressiveness of normalizing flow networks. The result is a single, comprehensive, compact noise model containing fewer than 2500 parameters yet able to represent multiple cameras and gain factors. Noise Flow dramatically outperforms existing noise models, with 0.42 nats/pixel improvement over the camera-calibrated noise level functions, which translates to 52% improvement in the likelihood of sampled noise. Noise Flow represents the first serious attempt to go beyond simple parametric models to one that leverages the power of deep learning and data-driven noise distributions.



### EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.08498v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08498v1)
- **Published**: 2019-08-22 17:07:04+00:00
- **Updated**: 2019-08-22 17:07:04+00:00
- **Authors**: Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen
- **Comment**: Accepted for presentation at ICCV 2019
- **Journal**: None
- **Summary**: We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multi-modal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -- RGB, Flow and Audio -- and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality and fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities.   We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.



### Predicting Animation Skeletons for 3D Articulated Models via Volumetric Nets
- **Arxiv ID**: http://arxiv.org/abs/1908.08506v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.08506v1)
- **Published**: 2019-08-22 17:26:46+00:00
- **Updated**: 2019-08-22 17:26:46+00:00
- **Authors**: Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Karan Singh
- **Comment**: 3DV 2019
- **Journal**: None
- **Summary**: We present a learning method for predicting animation skeletons for input 3D models of articulated characters. In contrast to previous approaches that fit pre-defined skeleton templates or predict fixed sets of joints, our method produces an animation skeleton tailored for the structure and geometry of the input 3D model. Our architecture is based on a stack of hourglass modules trained on a large dataset of 3D rigged characters mined from the web. It operates on the volumetric representation of the input 3D shapes augmented with geometric shape features that provide additional cues for joint and bone locations. Our method also enables intuitive user control of the level-of-detail for the output skeleton. Our evaluation demonstrates that our approach predicts animation skeletons that are much more similar to the ones created by humans compared to several alternatives and baselines.



### Adversarial-Based Knowledge Distillation for Multi-Model Ensemble and Noisy Data Refinement
- **Arxiv ID**: http://arxiv.org/abs/1908.08520v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08520v1)
- **Published**: 2019-08-22 17:51:16+00:00
- **Updated**: 2019-08-22 17:51:16+00:00
- **Authors**: Zhiqiang Shen, Zhankui He, Wanyun Cui, Jiahui Yu, Yutong Zheng, Chenchen Zhu, Marios Savvides
- **Comment**: This is an extended version of our previous conference paper
  arXiv:1812.02425
- **Journal**: None
- **Summary**: Generic Image recognition is a fundamental and fairly important visual problem in computer vision. One of the major challenges of this task lies in the fact that single image usually has multiple objects inside while the labels are still one-hot, another one is noisy and sometimes missing labels when annotated by humans. In this paper, we focus on tackling these challenges accompanying with two different image recognition problems: multi-model ensemble and noisy data recognition with a unified framework. As is well-known, usually the best performing deep neural models are ensembles of multiple base-level networks, as it can mitigate the variation or noise containing in the dataset. Unfortunately, the space required to store these many networks, and the time required to execute them at runtime, prohibit their use in applications where test sets are large (e.g., ImageNet). In this paper, we present a method for compressing large, complex trained ensembles into a single network, where the knowledge from a variety of trained deep neural networks (DNNs) is distilled and transferred to a single DNN. In order to distill diverse knowledge from different trained (teacher) models, we propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models, and to promote the discriminator network to distinguish teacher vs. student features simultaneously. Extensive experiments on CIFAR-10/100, SVHN, ImageNet and iMaterialist Challenge Dataset demonstrate the effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL achieves top-1/5 21.79%/5.99% val error, which outperforms the original model by 2.06%/1.14%. On iMaterialist Challenge Dataset, our MEAL obtains a remarkable improvement of top-3 1.15% (official evaluation metric) on a strong baseline model of ResNet-101.



### Compositional Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1908.08522v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08522v1)
- **Published**: 2019-08-22 17:55:58+00:00
- **Updated**: 2019-08-22 17:55:58+00:00
- **Authors**: Yufei Ye, Maneesh Singh, Abhinav Gupta, Shubham Tulsiani
- **Comment**: accepted to ICCV19
- **Journal**: None
- **Summary**: We present an approach for pixel-level future prediction given an input image of a scene. We observe that a scene is comprised of distinct entities that undergo motion and present an approach that operationalizes this insight. We implicitly predict future states of independent entities while reasoning about their interactions, and compose future video frames using these predicted states. We overcome the inherent multi-modality of the task using a global trajectory-level latent random variable, and show that this allows us to sample diverse and plausible futures. We empirically validate our approach against alternate representations and ways of incorporating multi-modality. We examine two datasets, one comprising of stacked objects that may fall, and the other containing videos of humans performing activities in a gym, and show that our approach allows realistic stochastic video prediction across these diverse settings. See https://judyye.github.io/CVP/ for video predictions.



### ViCo: Word Embeddings from Visual Co-occurrences
- **Arxiv ID**: http://arxiv.org/abs/1908.08527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.08527v1)
- **Published**: 2019-08-22 17:58:52+00:00
- **Updated**: 2019-08-22 17:58:52+00:00
- **Authors**: Tanmay Gupta, Alexander Schwing, Derek Hoiem
- **Comment**: Accepted to ICCV 2019. Project Page: http://tanmaygupta.info/vico/
- **Journal**: None
- **Summary**: We propose to learn word embeddings from visual co-occurrences. Two words co-occur visually if both words apply to the same image or image region. Specifically, we extract four types of visual co-occurrences between object and attribute words from large-scale, textually-annotated visual databases like VisualGenome and ImageNet. We then train a multi-task log-bilinear model that compactly encodes word "meanings" represented by each co-occurrence type into a single visual word-vector. Through unsupervised clustering, supervised partitioning, and a zero-shot-like generalization analysis we show that our word embeddings complement text-only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone. We further evaluate our embeddings on five downstream applications, four of which are vision-language tasks. Augmenting GloVe with our embeddings yields gains on all tasks. We also find that random embeddings perform comparably to learned embeddings on all supervised vision-language tasks, contrary to conventional wisdom.



### Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.08529v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08529v1)
- **Published**: 2019-08-22 17:59:08+00:00
- **Updated**: 2019-08-22 17:59:08+00:00
- **Authors**: Jyoti Aneja, Harsh Agrawal, Dhruv Batra, Alexander Schwing
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Diverse and accurate vision+language modeling is an important goal to retain creative freedom and maintain user engagement. However, adequately capturing the intricacies of diversity in language models is challenging. Recent works commonly resort to latent variable models augmented with more or less supervision from object detectors or part-of-speech tags. Common to all those methods is the fact that the latent variable either only initializes the sentence generation process or is identical across the steps of generation. Both methods offer no fine-grained control. To address this concern, we propose Seq-CVAE which learns a latent space for every word position. We encourage this temporal latent space to capture the 'intention' about how to complete the sentence by mimicking a representation which summarizes the future. We illustrate the efficacy of the proposed approach to anticipate the sentence continuation on the challenging MSCOCO dataset, significantly improving diversity metrics compared to baselines while performing on par w.r.t sentence quality.



### VL-BERT: Pre-training of Generic Visual-Linguistic Representations
- **Arxiv ID**: http://arxiv.org/abs/1908.08530v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08530v4)
- **Published**: 2019-08-22 17:59:30+00:00
- **Updated**: 2020-02-18 02:59:17+00:00
- **Authors**: Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai
- **Comment**: Accepted by ICLR 2020
- **Journal**: None
- **Summary**: We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \url{https://github.com/jackroos/VL-BERT}.



### Feedbackward Decoding for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.08584v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08584v1)
- **Published**: 2019-08-22 20:29:05+00:00
- **Updated**: 2019-08-22 20:29:05+00:00
- **Authors**: Beinan Wang, John Glossner, Daniel Iancu, Georgi N. Gaydadjiev
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel approach for semantic segmentation that uses an encoder in the reverse direction to decode. Many semantic segmentation networks adopt a feedforward encoder-decoder architecture. Typically, an input is first downsampled by the encoder to extract high-level semantic features and continues to be fed forward through the decoder module to recover low-level spatial clues. Our method works in an alternative direction that lets information flow backward from the last layer of the encoder towards the first. The encoder performs encoding in the forward pass and the same network performs decoding in the backward pass. Therefore, the encoder itself is also the decoder. Compared to conventional encoder-decoder architectures, ours doesn't require additional layers for decoding and further reuses the encoder weights thereby reducing the total number of parameters required for processing. We show by using only the 13 convolutional layers from VGG-16 plus one tiny classification layer, our model significantly outperforms other frequently cited models that are also adapted from VGG-16. On the Cityscapes semantic segmentation benchmark, our model uses 50.0% less parameters than SegNet and achieves an 18.1% higher "IoU class" score; it uses 28.3% less parameters than DeepLab LargeFOV and the achieved "IoU class" score is 3.9% higher; it uses 89.1% fewer parameters than FCN-8s and the achieved "IoU class" score is 3.1% higher. Our code will be publicly available on Github later.



### A joint 3D UNet-Graph Neural Network-based method for Airway Segmentation from chest CTs
- **Arxiv ID**: http://arxiv.org/abs/1908.08588v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08588v1)
- **Published**: 2019-08-22 20:32:01+00:00
- **Updated**: 2019-08-22 20:32:01+00:00
- **Authors**: Antonio Garcia-Uceda Juarez, Raghavendra Selvan, Zaigham Saghir, Marleen de Bruijne
- **Comment**: None
- **Journal**: None
- **Summary**: We present an end-to-end deep learning segmentation method by combining a 3D UNet architecture with a graph neural network (GNN) model. In this approach, the convolutional layers at the deepest level of the UNet are replaced by a GNN-based module with a series of graph convolutions. The dense feature maps at this level are transformed into a graph input to the GNN module. The incorporation of graph convolutions in the UNet provides nodes in the graph with information that is based on node connectivity, in addition to the local features learnt through the downsampled paths. This information can help improve segmentation decisions. By stacking several graph convolution layers, the nodes can access higher order neighbourhood information without substantial increase in computational expense. We propose two types of node connectivity in the graph adjacency: i) one predefined and based on a regular node neighbourhood, and ii) one dynamically computed during training and using the nearest neighbour nodes in the feature space. We have applied this method to the task of segmenting the airway tree from chest CT scans. Experiments have been performed on 32 CTs from the Danish Lung Cancer Screening Trial dataset. We evaluate the performance of the UNet-GNN models with two types of graph adjacency and compare it with the baseline UNet.



### Learning Similarity Conditions Without Explicit Supervision
- **Arxiv ID**: http://arxiv.org/abs/1908.08589v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08589v1)
- **Published**: 2019-08-22 20:35:04+00:00
- **Updated**: 2019-08-22 20:35:04+00:00
- **Authors**: Reuben Tan, Mariya I. Vasileva, Kate Saenko, Bryan A. Plummer
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories.



### Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective
- **Arxiv ID**: http://arxiv.org/abs/1908.08597v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.CY, cs.GR, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1908.08597v1)
- **Published**: 2019-08-22 21:05:17+00:00
- **Updated**: 2019-08-22 21:05:17+00:00
- **Authors**: Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudrealt, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa Verhoef, Christian Vogler, Meredith Ringel Morris
- **Comment**: None
- **Journal**: None
- **Summary**: Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.



