# Arxiv Papers in cs.CV on 2019-08-10
### Deep ensemble network with explicit complementary model for accuracy-balanced classification
- **Arxiv ID**: http://arxiv.org/abs/1908.03671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03671v1)
- **Published**: 2019-08-10 02:34:42+00:00
- **Updated**: 2019-08-10 02:34:42+00:00
- **Authors**: Dohyun Kim, Kyeorye Lee, Jiyeon Kim, Junseok Kwon, Joongheon Kim
- **Comment**: None
- **Journal**: None
- **Summary**: The average accuracy is one of major evaluation metrics for classification systems, while the accuracy deviation is another important performance metric used to evaluate various deep neural networks. In this paper, we present a new ensemble-like fast deep neural network, Harmony, that can reduce the accuracy deviation among categories without degrading overall average accuracy. Harmony consists of three sub-models, namely, Target model, Complementary model, and Conductor model. In Harmony, an object is classified by using either Target model or Complementary model. Target model is a conventional classification network for general categories, while Complementary model is a classification network especially for weak categories that are inaccurately classified by Target model. Conductor model is used to select one of two models. Experimental results demonstrate that Harmony accurately classifies categories, while it reduces the accuracy deviation among the categories.



### Recent Advances in Deep Learning for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1908.03673v1)
- **Published**: 2019-08-10 02:54:17+00:00
- **Updated**: 2019-08-10 02:54:17+00:00
- **Authors**: Xiongwei Wu, Doyen Sahoo, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to find objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classification, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications & benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning. Keywords: Object Detection, Deep Learning, Deep Convolutional Neural Networks



### Unconstrained Foreground Object Search
- **Arxiv ID**: http://arxiv.org/abs/1908.03675v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03675v1)
- **Published**: 2019-08-10 02:57:04+00:00
- **Updated**: 2019-08-10 02:57:04+00:00
- **Authors**: Yinan Zhao, Brian Price, Scott Cohen, Danna Gurari
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines.



### Distance Map Loss Penalty Term for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.03679v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03679v1)
- **Published**: 2019-08-10 03:37:18+00:00
- **Updated**: 2019-08-10 03:37:18+00:00
- **Authors**: Francesco Caliva, Claudia Iriondo, Alejandro Morales Martinez, Sharmila Majumdar, Valentina Pedoia
- **Comment**: Medical Imaging with Deep Learning (MIDL2019) Conference
  [arXiv:1907.08612], Extended Abstract
- **Journal**: None
- **Summary**: Convolutional neural networks for semantic segmentation suffer from low performance at object boundaries. In medical imaging, accurate representation of tissue surfaces and volumes is important for tracking of disease biomarkers such as tissue morphology and shape features. In this work, we propose a novel distance map derived loss penalty term for semantic segmentation. We propose to use distance maps, derived from ground truth masks, to create a penalty term, guiding the network's focus towards hard-to-segment boundary regions. We investigate the effects of this penalizing factor against cross-entropy, Dice, and focal loss, among others, evaluating performance on a 3D MRI bone segmentation task from the publicly available Osteoarthritis Initiative dataset. We observe a significant improvement in the quality of segmentation, with better shape preservation at bone boundaries and areas affected by partial volume. We ultimately aim to use our loss penalty term to improve the extraction of shape biomarkers and derive metrics to quantitatively evaluate the preservation of shape.



### Natural-Logarithm-Rectified Activation Function in Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.03682v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03682v2)
- **Published**: 2019-08-10 03:51:36+00:00
- **Updated**: 2019-08-25 02:24:49+00:00
- **Authors**: Yang Liu, Jianpeng Zhang, Chao Gao, Jinghua Qu, Lixin Ji
- **Comment**: None
- **Journal**: None
- **Summary**: Activation functions play a key role in providing remarkable performance in deep neural networks, and the rectified linear unit (ReLU) is one of the most widely used activation functions. Various new activation functions and improvements on ReLU have been proposed, but each carry performance drawbacks. In this paper, we propose an improved activation function, which we name the natural-logarithm-rectified linear unit (NLReLU). This activation function uses the parametric natural logarithmic transform to improve ReLU and is simply defined as. NLReLU not only retains the sparse activation characteristic of ReLU, but it also alleviates the "dying ReLU" and vanishing gradient problems to some extent. It also reduces the bias shift effect and heteroscedasticity of neuron data distributions among network layers in order to accelerate the learning process. The proposed method was verified across ten convolutional neural networks with different depths for two essential datasets. Experiments illustrate that convolutional neural networks with NLReLU exhibit higher accuracy than those with ReLU, and that NLReLU is comparable to other well-known activation functions. NLReLU provides 0.16% and 2.04% higher classification accuracy on average compared to ReLU when used in shallow convolutional neural networks with the MNIST and CIFAR-10 datasets, respectively. The average accuracy of deep convolutional neural networks with NLReLU is 1.35% higher on average with the CIFAR-10 dataset.



### Bayesian Loss for Crowd Count Estimation with Point Supervision
- **Arxiv ID**: http://arxiv.org/abs/1908.03684v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03684v1)
- **Published**: 2019-08-10 04:01:36+00:00
- **Updated**: 2019-08-10 04:01:36+00:00
- **Authors**: Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong
- **Comment**: Accepted by ICCV 2019 as an oral presentation
- **Journal**: None
- **Summary**: In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a "ground truth" density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a "ground-truth" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose \emph{Bayesian loss}, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset. The source code is available at \url{https://github.com/ZhihengCV/Baysian-Crowd-Counting}.



### User independent Emotion Recognition with Residual Signal-Image Network
- **Arxiv ID**: http://arxiv.org/abs/1908.03692v2
- **DOI**: 10.1109/ICIP.2019.8803627
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03692v2)
- **Published**: 2019-08-10 05:18:21+00:00
- **Updated**: 2020-08-03 01:28:18+00:00
- **Authors**: Guanghao Yin, Shouqian Sun, Hui Zhang, Dian Yu, Chao Li, Kejun Zhang, Ning Zou
- **Comment**: None
- **Journal**: ICIP2019
- **Summary**: User independent emotion recognition with large scale physiological signals is a tough problem. There exist many advanced methods but they are conducted under relatively small datasets with dozens of subjects. Here, we propose Res-SIN, a novel end-to-end framework using Electrodermal Activity(EDA) signal images to classify human emotion. We first apply convex optimization-based EDA (cvxEDA) to decompose signals and mine the static and dynamic emotion changes. Then, we transform decomposed signals to images so that they can be effectively processed by CNN frameworks. The Res-SIN combines individual emotion features and external emotion benchmarks to accelerate convergence. We evaluate our approach on the PMEmo dataset, the currently largest emotional dataset containing music and EDA signals. To the best of author's knowledge, our method is the first attempt to classify large scale subject-independent emotion with 7962 pieces of EDA signals from 457 subjects. Experimental results demonstrate the reliability of our model and the binary classification accuracy of 73.65% and 73.43% on arousal and valence dimension can be used as a baseline.



### Semi-Supervised Multi-Task Learning With Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/1908.03693v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03693v2)
- **Published**: 2019-08-10 05:28:41+00:00
- **Updated**: 2019-08-26 06:42:29+00:00
- **Authors**: Abdullah-Al-Zubaer Imran, Demetri Terzopoulos
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2019)
- **Journal**: None
- **Summary**: Discriminative models that require full supervision are inefficacious in the medical imaging domain when large labeled datasets are unavailable. By contrast, generative modeling---i.e., learning data generation and classification---facilitates semi-supervised training with limited labeled data. Moreover, generative modeling can be advantageous in accomplishing multiple objectives for better generalization. We propose a novel multi-task learning model for jointly learning a classifier and a segmentor, from chest X-ray images, through semi-supervised learning. In addition, we propose a new loss function that combines absolute KL divergence with Tversky loss (KLTV) to yield faster convergence and better segmentation performance. Based on our experimental results using a novel segmentation model, an Adversarial Pyramid Progressive Attention U-Net (APPAU-Net), we hypothesize that KLTV can be more effective for generalizing multi-tasking models while being competitive in segmentation-only tasks.



### Multi-modality Latent Interaction Network for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1908.04289v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.SD, eess.AS, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04289v1)
- **Published**: 2019-08-10 05:57:01+00:00
- **Updated**: 2019-08-10 05:57:01+00:00
- **Authors**: Peng Gao, Haoxuan You, Zhanpeng Zhang, Xiaogang Wang, Hongsheng Li
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for Visual Question Answering (VQA). However, we argue that existing methods mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. From humans' perspective, answering a visual question requires understanding the summarizations of visual and language information. In this paper, we proposed the Multi-modality Latent Interaction module (MLI) to tackle this problem. The proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. The cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. Such MLI modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public VQA benchmarks, VQA v2.0 and TDIUC . In addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model BERT.



### Channel Decomposition into Painting Actions
- **Arxiv ID**: http://arxiv.org/abs/1908.04694v4
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04694v4)
- **Published**: 2019-08-10 06:58:55+00:00
- **Updated**: 2019-11-12 06:24:11+00:00
- **Authors**: Shih-Chieh Su
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents a method to decompose a convolutional layer of the deep neural network into painting actions. To behave like the human painter, these actions are driven by the cost simulating the hand movement, the paint color change, the stroke shape and the stroking style. To help planning, the Mask R-CNN is applied to detect the object areas and decide the painting order. The proposed painting system introduces a variety of extensions in artistic styles, based on the chosen parameters. Further experiments are performed to evaluate the channel penetration and the channel sensitivity on the strokes.



### Boundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Verification
- **Arxiv ID**: http://arxiv.org/abs/1908.03701v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03701v1)
- **Published**: 2019-08-10 07:15:39+00:00
- **Updated**: 2019-08-10 07:15:39+00:00
- **Authors**: Changhong Fu, Ziyuan Huang, Yiming Li, Ran Duan, Peng Lu
- **Comment**: IROS 2019 accepted, 8 pages, 9 figures
- **Journal**: None
- **Summary**: Due to implicitly introduced periodic shifting of limited searching area, visual object tracking using correlation filters often has to confront undesired boundary effect. As boundary effect severely degrade the quality of object model, it has made it a challenging task for unmanned aerial vehicles (UAV) to perform robust and accurate object following. Traditional hand-crafted features are also not precise and robust enough to describe the object in the viewing point of UAV. In this work, a novel tracker with online enhanced background learning is specifically proposed to tackle boundary effects. Real background samples are densely extracted to learn as well as update correlation filters. Spatial penalization is introduced to offset the noise introduced by exceedingly more background information so that a more accurate appearance model can be established. Meanwhile, convolutional features are extracted to provide a more comprehensive representation of the object. In order to mitigate changes of objects' appearances, multi-frame technique is applied to learn an ideal response map and verify the generated one in each frame. Exhaustive experiments were conducted on 100 challenging UAV image sequences and the proposed tracker has achieved state-of-the-art performance.



### Exploiting temporal consistency for real-time video depth estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.03706v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03706v1)
- **Published**: 2019-08-10 07:47:36+00:00
- **Updated**: 2019-08-10 07:47:36+00:00
- **Authors**: Haokui Zhang, Chunhua Shen, Ying Li, Yuanzhouhan Cao, Yu Liu, Youliang Yan
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision 2019
- **Journal**: None
- **Summary**: Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at: https://tinyurl.com/STCLSTM



### SCAR: Spatial-/Channel-wise Attention Regression Networks for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1908.03716v1
- **DOI**: 10.1016/j.neucom.2019.08.018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03716v1)
- **Published**: 2019-08-10 09:20:18+00:00
- **Updated**: 2019-08-10 09:20:18+00:00
- **Authors**: Junyu Gao, Qi Wang, Yuan Yuan
- **Comment**: accepted by Neurocomputing
- **Journal**: None
- **Summary**: Recently, crowd counting is a hot topic in crowd analysis. Many CNN-based counting algorithms attain good performance. However, these methods only focus on the local appearance features of crowd scenes but ignore the large-range pixel-wise contextual and crowd attention information. To remedy the above problems, in this paper, we introduce the Spatial-/Channel-wise Attention Models into the traditional Regression CNN to estimate the density map, which is named as "SCAR". It consists of two modules, namely Spatial-wise Attention Model (SAM) and Channel-wise Attention Model (CAM). The former can encode the pixel-wise context of the entire image to more accurately predict density maps at the pixel level. The latter attempts to extract more discriminative features among different channels, which aids model to pay attention to the head region, the core of crowd scenes. Intuitively, CAM alleviates the mistaken estimation for background regions. Finally, two types of attention information and traditional CNN's feature maps are integrated by a concatenation operation. Furthermore, the extensive experiments are conducted on four popular datasets, Shanghai Tech Part A/B, GCC, and UCF_CC_50 Dataset. The results show that the proposed method achieves state-of-the-art results.



### Automatic acute ischemic stroke lesion segmentation using semi-supervised learning
- **Arxiv ID**: http://arxiv.org/abs/1908.03735v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03735v3)
- **Published**: 2019-08-10 11:48:02+00:00
- **Updated**: 2020-09-20 13:20:00+00:00
- **Authors**: Bin Zhao, Shuxue Ding, Hong Wu, Guohua Liu, Chen Cao, Song Jin, Zhiyang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Ischemic stroke is a common disease in the elderly population, which can cause long-term disability and even death. However, the time window for treatment of ischemic stroke in its acute stage is very short. To fast localize and quantitively evaluate the acute ischemic stroke (AIS) lesions, many deep-learning-based lesion segmentation methods have been proposed in the literature, where a deep convolutional neural network (CNN) was trained on hundreds of fully labeled subjects with accurate annotations of AIS lesions. Despite that high segmentation accuracy can be achieved, the accurate labels should be annotated by experienced clinicians, and it is therefore very time-consuming to obtain a large number of fully labeled subjects. In this paper, we propose a semi-supervised method to automatically segment AIS lesions in diffusion weighted images and apparent diffusion coefficient maps. By using a large number of weakly labeled subjects and a small number of fully labeled subjects, our proposed method is able to accurately detect and segment the AIS lesions. In particular, our proposed method consists of three parts: 1) a double-path classification net (DPC-Net) trained in a weakly-supervised way is used to detect the suspicious regions of AIS lesions; 2) a pixel-level K-Means clustering algorithm is used to identify the hyperintensive regions on the DWIs; and 3) a region-growing algorithm combines the outputs of the DPC-Net and the K-Means to obtain the final precise lesion segmentation. In our experiment, we use 460 weakly labeled subjects and 15 fully labeled subjects to train and fine-tune the proposed method. By evaluating on a clinical dataset with 150 fully labeled subjects, our proposed method achieves a mean dice coefficient of 0.642, and a lesion-wise F1 score of 0.822.



### Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations
- **Arxiv ID**: http://arxiv.org/abs/1908.04680v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04680v3)
- **Published**: 2019-08-10 11:48:55+00:00
- **Updated**: 2021-06-04 00:26:53+00:00
- **Authors**: Bohan Zhuang, Jing Liu, Mingkui Tan, Lingqiao Liu, Ian Reid, Chunhua Shen
- **Comment**: Accepted to IEEE T. Pattern Analysis and Machine Intelligence
  (TPAMI). Extended version of arXiv:1711.00205 (CVPR 2018)
- **Journal**: None
- **Summary**: This paper tackles the problem of training a deep convolutional neural network of both low-bitwidth weights and activations. Optimizing a low-precision network is very challenging due to the non-differentiability of the quantizer, which may result in substantial accuracy loss. To address this, we propose three practical approaches, including (i) progressive quantization; (ii) stochastic precision; and (iii) joint knowledge distillation to improve the network training. First, for progressive quantization, we propose two schemes to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and subsequently quantize activations. This is in contrast to the traditional methods which optimize them simultaneously. Furthermore, we propose a second progressive quantization scheme which gradually decreases the bit-width from high-precision to low-precision during training. Second, to alleviate the excessive training burden due to the multi-round training stages, we further propose a one-stage stochastic precision strategy to randomly sample and quantize sub-networks while keeping other parts in full-precision. Finally, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training and significantly improves the performance of the low-precision network. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet) show the effectiveness of the proposed methods.



### Lightweight and Scalable Particle Tracking and Motion Clustering of 3D Cell Trajectories
- **Arxiv ID**: http://arxiv.org/abs/1908.03775v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1908.03775v3)
- **Published**: 2019-08-10 15:43:49+00:00
- **Updated**: 2021-01-12 14:24:05+00:00
- **Authors**: Mojtaba S. Fazli, Rachel V. Stadler, BahaaEddin Alaila, Stephen A. Vella, Silvia N. J. Moreno, Gary E. Ward, Shannon Quinn
- **Comment**: Accepted to 2019 IEEE International Conference on Data Science and
  Advanced Analytics
- **Journal**: None
- **Summary**: Tracking cell particles in 3D microscopy videos is a challenging task but is of great significance for modeling the motion of cells. Proper characterization of the cell's shape, evolution, and their movement over time is crucial to understanding and modeling the mechanobiology of cell migration in many diseases. One in particular, toxoplasmosis is the disease caused by the parasite Toxoplasma gondii. Roughly, one-third of the world's population tests positive for T. gondii. Its virulence is linked to its lytic cycle, predicated on its motility and ability to enter and exit nucleated cells; therefore, studies elucidating its motility patterns are critical to the eventual development of therapeutic strategies. Here, we present a computational framework for fast and scalable detection, tracking, and identification of T. gondii motion phenotypes in 3D videos, in a completely unsupervised fashion. Our pipeline consists of several different modules including preprocessing, sparsification, cell detection, cell tracking, trajectories extraction, parametrization of the trajectories; and finally, a clustering step. Additionally, we identified the computational bottlenecks, and developed a lightweight and highly scalable pipeline through a combination of task distribution and parallelism. Our results prove both the accuracy and performance of our method.



### Object-Aware Instance Labeling for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03792v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03792v1)
- **Published**: 2019-08-10 17:48:19+00:00
- **Updated**: 2019-08-10 17:48:19+00:00
- **Authors**: Satoshi Kosugi, Toshihiko Yamasaki, Kiyoharu Aizawa
- **Comment**: Accepted to ICCV 2019 (oral)
- **Journal**: None
- **Summary**: Weakly supervised object detection (WSOD), where a detector is trained with only image-level annotations, is attracting more and more attention. As a method to obtain a well-performing detector, the detector and the instance labels are updated iteratively. In this study, for more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. Instead of simply labeling the top-scoring region and its highly overlapping regions as positive and others as negative, we propose more effective instance labeling methods as follows. First, to solve the problem that regions covering only some parts of the object tend to be labeled as positive, we find regions covering the whole object focusing on the context classification loss. Second, considering the situation where the other objects contained in the image can be labeled as negative, we impose a spatial restriction on regions labeled as negative. Using these instance labeling methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results compared with other state-of-the-art approaches.



### Deep Dexterous Grasping of Novel Objects from a Single View
- **Arxiv ID**: http://arxiv.org/abs/1908.04293v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04293v1)
- **Published**: 2019-08-10 18:31:35+00:00
- **Updated**: 2019-08-10 18:31:35+00:00
- **Authors**: Umit Rusen Aktas, Chao Zhao, Marek Kopicki, Ales Leonardis, Jeremy L. Wyatt
- **Comment**: Submitted for IEEE Transactions on Robotics (T-RO). 14 pages
- **Journal**: None
- **Summary**: Dexterous grasping of a novel object given a single view is an open problem. This paper makes several contributions to its solution. First, we present a simulator for generating and testing dexterous grasps. Second we present a data set, generated by this simulator, of 2.4 million simulated dexterous grasps of variations of 294 base objects drawn from 20 categories. Third, we present a basic architecture for generation and evaluation of dexterous grasps that may be trained in a supervised manner. Fourth, we present three different evaluative architectures, employing ResNet-50 or VGG16 as their visual backbone. Fifth, we train, and evaluate seventeen variants of generative-evaluative architectures on this simulated data set, showing improvement from 69.53% grasp success rate to 90.49%. Finally, we present a real robot implementation and evaluate the four most promising variants, executing 196 real robot grasps in total. We show that our best architectural variant achieves a grasp success rate of 87.8% on real novel objects seen from a single view, improving on a baseline of 57.1%.



### Conditional Generative Adversarial Networks for Data Augmentation and Adaptation in Remotely Sensed Imagery
- **Arxiv ID**: http://arxiv.org/abs/1908.03809v1
- **DOI**: 10.1117/12.2529586
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03809v1)
- **Published**: 2019-08-10 21:26:54+00:00
- **Updated**: 2019-08-10 21:26:54+00:00
- **Authors**: Jonathan Howe, Kyle Pula, Aaron A. Reite
- **Comment**: None
- **Journal**: None
- **Summary**: The difficulty in obtaining labeled data relevant to a given task is among the most common and well-known practical obstacles to applying deep learning techniques to new or even slightly modified domains. The data volumes required by the current generation of supervised learning algorithms typically far exceed what a human needs to learn and complete a given task. We investigate ways to expand a given labeled corpus of remote sensed imagery into a larger corpus using Generative Adversarial Networks (GANs). We then measure how these additional synthetic data affect supervised machine learning performance on an object detection task.   Our data driven strategy is to train GANs to (1) generate synthetic segmentation masks and (2) generate plausible synthetic remote sensing imagery corresponding to these segmentation masks. Run sequentially, these GANs allow the generation of synthetic remote sensing imagery complete with segmentation labels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling Contest - Potsdam, with a follow on vehicle detection task. We find that in scenarios with limited training data, augmenting the available data with such synthetically generated data can improve detector performance.



### Attentive Deep Regression Networks for Real-Time Visual Face Tracking in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1908.03812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03812v1)
- **Published**: 2019-08-10 21:40:25+00:00
- **Updated**: 2019-08-10 21:40:25+00:00
- **Authors**: Safa Alver, Ugur Halici
- **Comment**: None
- **Journal**: None
- **Summary**: Visual face tracking is one of the most important tasks in video surveillance systems. However, due to the variations in pose, scale, expression, and illumination it is considered to be a difficult task. Recent studies show that deep learning methods have a significant potential in object tracking tasks and adaptive feature selection methods can boost their performance. Motivated by these, we propose an end-to-end attentive deep learning based tracker, that is build on top of the state-of-the-art GOTURN tracker, for the task of real-time visual face tracking in video surveillance. Our method outperforms the state-of-the-art GOTURN and IVT trackers by very large margins and it achieves speeds that are very far beyond the requirements of real-time tracking. Additionally, to overcome the scarce data problem in visual face tracking, we also provide bounding box annotations for the G1 and G2 sets of ChokePoint dataset and make it suitable for further studies in face tracking under surveillance conditions.



### DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better
- **Arxiv ID**: http://arxiv.org/abs/1908.03826v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03826v1)
- **Published**: 2019-08-10 23:28:09+00:00
- **Updated**: 2019-08-10 23:28:09+00:00
- **Authors**: Orest Kupyn, Tetiana Martyniuk, Junru Wu, Zhangyang Wang
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-v2, which considerably boosts state-of-the-art deblurring efficiency, quality, and flexibility. DeblurGAN-v2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-v2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g., Inception-ResNet-v2) can lead to solid state-of-the-art deblurring. Meanwhile, with light-weight backbones (e.g., MobileNet and its variants), DeblurGAN-v2 reaches 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-v2 obtains very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. Besides, we show the architecture to be effective for general image restoration tasks too. Our codes, models and data are available at: https://github.com/KupynOrest/DeblurGANv2



