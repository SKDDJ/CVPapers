# Arxiv Papers in cs.CV on 2019-08-09
### Multi Scale Supervised 3D U-Net for Kidney and Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.03204v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03204v2)
- **Published**: 2019-08-09 02:41:55+00:00
- **Updated**: 2019-08-13 15:46:23+00:00
- **Authors**: Wenshuai Zhao, Zengfeng Zeng
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: U-Net has achieved huge success in various medical image segmentation challenges. Kinds of new architectures with bells and whistles might succeed in certain dataset when employed with optimal hyper-parameter, but their generalization always can't be guaranteed. Here, we focused on the basic U-Net architecture and proposed a multi scale supervised 3D U-Net for the segmentation task in KiTS19 challenge. To enhance the performance, our work can be summarized as three folds: first, we used multi scale supervision in the decoder pathway, which could encourage the network to predict right results from the deep layers; second, with the aim to alleviate the bad effect from the sample imbalance of kidney and tumor, we adopted exponential logarithmic loss; third, a connected-component based post processing method was designed to remove the obviously wrong voxels. In the published KiTS19 training dataset (totally 210 patients), we divided 42 patients to be test dataset and finally obtained DICE scores of 0.969 and 0.805 for the kidney and tumor respectively. In the challenge, we finally achieved the 7th place among 106 teams with the Composite Dice of 0.8961, namely 0.9741 for kidney and 0.8181 for tumor.



### Question-Agnostic Attention for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1908.03289v2
- **DOI**: 10.1109/ICPR48806.2021.9413330
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03289v2)
- **Published**: 2019-08-09 03:03:23+00:00
- **Updated**: 2020-09-06 03:52:27+00:00
- **Authors**: Moshiur R Farazi, Salman H Khan, Nick Barnes
- **Comment**: To appear in the proceedings of International Conference on Pattern
  Recognition (ICPR) 2020
- **Journal**: None
- **Summary**: Visual Question Answering (VQA) models employ attention mechanisms to discover image locations that are most relevant for answering a specific question. For this purpose, several multimodal fusion strategies have been proposed, ranging from relatively simple operations (e.g., linear sum) to more complex ones (e.g., Block). The resulting multimodal representations define an intermediate feature space for capturing the interplay between visual and semantic features, that is helpful in selectively focusing on image content. In this paper, we propose a question-agnostic attention mechanism that is complementary to the existing question-dependent attention mechanisms. Our proposed model parses object instances to obtain an `object map' and applies this map on the visual features to generate Question-Agnostic Attention (QAA) features. In contrast to question-dependent attention approaches that are learned end-to-end, the proposed QAA does not involve question-specific training, and can be easily included in almost any existing VQA model as a generic light-weight pre-processing step, thereby adding minimal computation overhead for training. Further, when used in complement with the question-dependent attention, the QAA allows the model to focus on the regions containing objects that might have been overlooked by the learned attention representation. Through extensive evaluation on VQAv1, VQAv2 and TDIUC datasets, we show that incorporating complementary QAA allows state-of-the-art VQA models to perform better, and provides significant boost to simplistic VQA models, enabling them to performance on par with highly sophisticated fusion strategies.



### PosNeg-Balanced Anchors with Aligned Features for Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03295v1)
- **Published**: 2019-08-09 03:29:15+00:00
- **Updated**: 2019-08-09 03:29:15+00:00
- **Authors**: Qiankun Tang, Shice Liu, Jie Li, Yu Hu
- **Comment**: Submitted to a conference, under review
- **Journal**: None
- **Summary**: We introduce a novel single-shot object detector to ease the imbalance of foreground-background class by suppressing the easy negatives while increasing the positives. To achieve this, we propose an Anchor Promotion Module (APM) which predicts the probability of each anchor as positive and adjusts their initial locations and shapes to promote both the quality and quantity of positive anchors. In addition, we design an efficient Feature Alignment Module (FAM) to extract aligned features for fitting the promoted anchors with the help of both the location and shape transformation information from the APM. We assemble the two proposed modules to the backbone of VGG-16 and ResNet-101 network with an encoder-decoder architecture. Extensive experiments on MS COCO well demonstrate our model performs competitively with alternative methods (40.0\% mAP on \textit{test-dev} set) and runs faster (28.6 \textit{fps}).



### Deep Density-aware Count Regressor
- **Arxiv ID**: http://arxiv.org/abs/1908.03314v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03314v3)
- **Published**: 2019-08-09 04:44:13+00:00
- **Updated**: 2020-08-02 02:02:05+00:00
- **Authors**: Zhuojun Chen, Junhao Cheng, Yuchen Yuan, Dongping Liao, Yizhou Li, Jiancheng Lv
- **Comment**: None
- **Journal**: None
- **Summary**: We seek to improve crowd counting as we perceive limits of currently prevalent density map estimation approach on both prediction accuracy and time efficiency. We leverage multilevel pixelation of density map as it helps improve SNR of training data and therefore, reduce prediction error. To achieve a better model, we introduce multilayer gradient fusion for training a density-aware global count regressor. More specifically, on training stage, a backbone network receives gradients from multiple branches to learn the density information, whereas those branches are to be detached to accelerate inference. By taking advantages of such method, our model improves benchmark results on public datasets and exhibits itself to be a new solution to crowd counting problems in practice.



### Graph Embedding Using Infomax for ASD Classification and Brain Functional Difference Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.04769v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04769v2)
- **Published**: 2019-08-09 05:25:46+00:00
- **Updated**: 2019-08-14 00:22:03+00:00
- **Authors**: Xiaoxiao Li, Nicha C. Dvornek, Juntang Zhuang, Pamela Ventola, James Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Significant progress has been made using fMRI to characterize the brain changes that occur in ASD, a complex neuro-developmental disorder. However, due to the high dimensionality and low signal-to-noise ratio of fMRI, embedding informative and robust brain regional fMRI representations for both graph-level classification and region-level functional difference detection tasks between ASD and healthy control (HC) groups is difficult. Here, we model the whole brain fMRI as a graph, which preserves geometrical and temporal information and use a Graph Neural Network (GNN) to learn from the graph-structured fMRI data. We investigate the potential of including mutual information (MI) loss (Infomax), which is an unsupervised term encouraging large MI of each nodal representation and its corresponding graph-level summarized representation to learn a better graph embedding. Specifically, this work developed a pipeline including a GNN encoder, a classifier and a discriminator, which forces the encoded nodal representations to both benefit classification and reveal the common nodal patterns in a graph. We simultaneously optimize graph-level classification loss and Infomax. We demonstrated that Infomax graph embedding improves classification performance as a regularization term. Furthermore, we found separable nodal representations of ASD and HC groups in prefrontal cortex, cingulate cortex, visual regions, and other social, emotional and execution related brain regions. In contrast with GNN with classification loss only, the proposed pipeline can facilitate training more robust ASD classification models. Moreover, the separable nodal representations can detect the functional differences between the two groups and contribute to revealing new ASD biomarkers.



### Convex hull algorithms based on some variational models
- **Arxiv ID**: http://arxiv.org/abs/1908.03323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03323v1)
- **Published**: 2019-08-09 05:58:18+00:00
- **Updated**: 2019-08-09 05:58:18+00:00
- **Authors**: Lingfeng Li, Shousheng Luo, Xue-Cheng Tai, Jiang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Seeking the convex hull of an object is a very fundamental problem arising from various tasks. In this work, we propose two variational convex hull models using level set representation for 2-dimensional data. The first one is an exact model, which can get the convex hull of one or multiple objects. In this model, the convex hull is characterized by the zero sublevel-set of a convex level set function, which is non-positive at every given point. By minimizing the area of the zero sublevel-set, we can find the desired convex hull. The second one is intended to get convex hull of objects with outliers. Instead of requiring all the given points are included, this model penalizes the distance from each given point to the zero sublevel-set. Literature methods are not able to handle outliers. For the solution of these models, we develop efficient numerical schemes using alternating direction method of multipliers. Numerical examples are given to demonstrate the advantages of the proposed methods.



### Hard-Mining Loss based Convolutional Neural Network for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.09747v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.09747v2)
- **Published**: 2019-08-09 06:55:45+00:00
- **Updated**: 2020-12-23 00:49:00+00:00
- **Authors**: Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey
- **Comment**: Accepted in Fifth IAPR International Conference on Computer Vision
  and Image Processing (CVIP), 2020
- **Journal**: None
- **Summary**: Face Recognition is one of the prominent problems in the computer vision domain. Witnessing advances in deep learning, significant work has been observed in face recognition, which touched upon various parts of the recognition framework like Convolutional Neural Network (CNN), Layers, Loss functions, etc. Various loss functions such as Cross-Entropy, Angular-Softmax and ArcFace have been introduced to learn the weights of network for face recognition. However, these loss functions do not give high priority to the hard samples as compared to the easy samples. Moreover, their learning process is biased due to a number of easy examples compared to hard examples. In this paper, we address this issue by considering hard examples with more priority. In order to do so, We propose a Hard-Mining loss by increasing the loss for harder examples and decreasing the loss for easy examples. The proposed concept is generic and can be used with any existing loss function. We test the Hard-Mining loss with different losses such as Cross-Entropy, Angular-Softmax and ArcFace. The proposed Hard-Mining loss is tested over widely used Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) datasets. The training is performed over CASIA-WebFace and MS-Celeb-1M datasets. We use the residual network (i.e., ResNet18) for the experimental analysis. The experimental results suggest that the performance of existing loss functions is boosted when used in the framework of the proposed Hard-Mining loss.



### Recognizing Part Attributes with Insufficient Data
- **Arxiv ID**: http://arxiv.org/abs/1908.03335v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03335v2)
- **Published**: 2019-08-09 06:56:02+00:00
- **Updated**: 2019-08-13 03:32:19+00:00
- **Authors**: Xiangyun Zhao, Yi Yang, Feng Zhou, Xiao Tan, Yuchen Yuan, Yingze Bao, Ying Wu
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Recognizing attributes of objects and their parts is important to many computer vision applications. Although great progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotation which is more expensive to obtain. To solve the data insufficiency problem and get rid of dependence on the part annotation, we introduce a novel Concept Sharing Network (CSN) for part attribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination of part location and appearance pattern) that has insufficient or zero training data, by learning the part location and appearance pattern respectively from the training data that usually mix them in a single label. Extensive experiments on CUB-200-2011 [51], CelebA [35] and a newly proposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot part attribute recognition. The code will be made available at https://github.com/Zhaoxiangyun/Concept-Sharing-Network.



### Hyper Vision Net: Kidney Tumor Segmentation Using Coordinate Convolutional Layer and Attention Unit
- **Arxiv ID**: http://arxiv.org/abs/1908.03339v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03339v1)
- **Published**: 2019-08-09 07:05:34+00:00
- **Updated**: 2019-08-09 07:05:34+00:00
- **Authors**: D. Sabarinathan, M. Parisa Beham, S. M. Md. Mansoor Roomi
- **Comment**: 9 pages, 3 figures, KiTs19 challenge
- **Journal**: None
- **Summary**: KiTs19 challenge paves the way to haste the improvement of solid kidney tumor semantic segmentation methodologies. Accurate segmentation of kidney tumor in computer tomography (CT) images is a challenging task due to the non-uniform motion, similar appearance and various shape. Inspired by this fact, in this manuscript, we present a novel kidney tumor segmentation method using deep learning network termed as Hyper vision Net model. All the existing U-net models are using a modified version of U-net to segment the kidney tumor region. In the proposed architecture, we introduced supervision layers in the decoder part, and it refines even minimal regions in the output. A dataset consists of real arterial phase abdominal CT scans of 300 patients, including 45964 images has been provided from KiTs19 for training and validation of the proposed model. Compared with the state-of-the-art segmentation methods, the results demonstrate the superiority of our approach on training dice value score of 0.9552 and 0.9633 in tumor region and kidney region, respectively.



### Enhancing Flood Impact Analysis using Interactive Retrieval of Social Media Images
- **Arxiv ID**: http://arxiv.org/abs/1908.03361v1
- **DOI**: 10.5445/KSP/1000087327/06
- **Categories**: **cs.IR**, cs.CV, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03361v1)
- **Published**: 2019-08-09 08:29:57+00:00
- **Updated**: 2019-08-09 08:29:57+00:00
- **Authors**: Björn Barz, Kai Schröter, Moritz Münch, Bin Yang, Andrea Unger, Doris Dransch, Joachim Denzler
- **Comment**: None
- **Journal**: Archives of Data Science, Series A, 5.1, 2018
- **Summary**: The analysis of natural disasters such as floods in a timely manner often suffers from limited data due to a coarse distribution of sensors or sensor failures. This limitation could be alleviated by leveraging information contained in images of the event posted on social media platforms, so-called "Volunteered Geographic Information (VGI)". To save the analyst from the need to inspect all images posted online manually, we propose to use content-based image retrieval with the possibility of relevance feedback for retrieving only relevant images of the event to be analyzed. To evaluate this approach, we introduce a new dataset of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution). We compare several image features and relevance feedback methods on that dataset, mixed with 97,085 distractor images, and are able to improve the precision among the top 100 retrieval results from 55% with the baseline retrieval to 87% after 5 rounds of feedback.



### Enforcing Perceptual Consistency on Generative Adversarial Networks by Using the Normalised Laplacian Pyramid Distance
- **Arxiv ID**: http://arxiv.org/abs/1908.04347v2
- **DOI**: 10.7557/18.5124
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04347v2)
- **Published**: 2019-08-09 08:33:51+00:00
- **Updated**: 2020-11-17 10:48:29+00:00
- **Authors**: Alexander Hepburn, Valero Laparra, Ryan McConville, Raul Santos-Rodriguez
- **Comment**: None
- **Journal**: Proceedings of the Northern Lights Deep Learning Workshop. Vol. 1.
  2020
- **Summary**: In recent years there has been a growing interest in image generation through deep learning. While an important part of the evaluation of the generated images usually involves visual inspection, the inclusion of human perception as a factor in the training process is often overlooked. In this paper we propose an alternative perceptual regulariser for image-to-image translation using conditional generative adversarial networks (cGANs). To do so automatically (avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance (NLPD) to measure the perceptual similarity between the generated image and the original image. The NLPD is based on the principle of normalising the value of coefficients with respect to a local estimate of mean energy at different scales and has already been successfully tested in different experiments involving human perception. We compare this regulariser with the originally proposed L1 distance and note that when using NLPD the generated images contain more realistic values for both local and global contrast. We found that using NLPD as a regulariser improves image segmentation accuracy on generated images as well as improving two no-reference image quality metrics.



### Deep Learning based Wearable Assistive System for Visually Impaired People
- **Arxiv ID**: http://arxiv.org/abs/1908.03364v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03364v1)
- **Published**: 2019-08-09 08:41:06+00:00
- **Updated**: 2019-08-09 08:41:06+00:00
- **Authors**: Yimin Lin, Kai Wang, Wanxin Yi, Shiguo Lian
- **Comment**: Accepted by ICCV/ACVR2019
- **Journal**: None
- **Summary**: In this paper, we propose a deep learning based assistive system to improve the environment perception experience of visually impaired (VI). The system is composed of a wearable terminal equipped with an RGBD camera and an earphone, a powerful processor mainly for deep learning inferences and a smart phone for touch-based interaction. A data-driven learning approach is proposed to predict safe and reliable walkable instructions using RGBD data and the established semantic map. This map is also used to help VI understand their 3D surrounding objects and layout through well-designed touchscreen interactions. The quantitative and qualitative experimental results show that our learning based obstacle avoidance approach achieves excellent results in both indoor and outdoor datasets with low-lying obstacles. Meanwhile, user studies have also been carried out in various scenarios and showed the improvement of VI's environment perception experience with our system.



### An Update on Machine Learning in Neuro-oncology Diagnostics
- **Arxiv ID**: http://arxiv.org/abs/1910.08157v1
- **DOI**: 10.1007/978-3-030-11723-8_4
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08157v1)
- **Published**: 2019-08-09 08:57:51+00:00
- **Updated**: 2019-08-09 08:57:51+00:00
- **Authors**: Thomas Booth
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1910.07440
- **Journal**: None
- **Summary**: Imaging biomarkers in neuro-oncology are used for diagnosis, prognosis and treatment response monitoring. Magnetic resonance imaging is typically used throughout the patient pathway because routine structural imaging provides detailed anatomical and pathological information and advanced techniques provide additional physiological detail. Following image feature extraction, machine learning allows accurate classification in a variety of scenarios. Machine learning also enables image feature extraction de novo although the low prevalence of brain tumours makes such approaches challenging. Much research is applied to determining molecular profiles, histological tumour grade and prognosis at the time that patients first present with a brain tumour. Following treatment, differentiating a treatment response from a post-treatment related effect is clinically important and also an area of study. Most of the evidence is low level having been obtained retrospectively and in single centres.



### Video Face Clustering with Unknown Number of Clusters
- **Arxiv ID**: http://arxiv.org/abs/1908.03381v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03381v2)
- **Published**: 2019-08-09 09:19:51+00:00
- **Updated**: 2019-08-20 09:37:57+00:00
- **Authors**: Makarand Tapaswi, Marc T. Law, Sanja Fidler
- **Comment**: Accepted to ICCV 2019, code and data at
  https://github.com/makarandtapaswi/BallClustering_ICCV2019
- **Journal**: None
- **Summary**: Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded.   To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.



### Distinguishing Individual Red Pandas from Their Faces
- **Arxiv ID**: http://arxiv.org/abs/1908.03391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03391v1)
- **Published**: 2019-08-09 10:01:28+00:00
- **Updated**: 2019-08-09 10:01:28+00:00
- **Authors**: Qi He, Qijun Zhao, Ning Liu, Peng Chen, Zhihe Zhang, Rong Hou
- **Comment**: Accepted by the 2nd Chinese Conference on Pattern Recognition and
  Computer Vision (PRCV 2019)
- **Journal**: None
- **Summary**: Individual identification is essential to animal behavior and ecology research and is of significant importance for protecting endangered species. Red pandas, among the world's rarest animals, are currently identified mainly by visual inspection and microelectronic chips, which are costly and inefficient. Motivated by recent advancement in computer-vision-based animal identification, in this paper, we propose an automatic framework for identifying individual red pandas based on their face images. We implement the framework by exploring well-established deep learning models with necessary adaptation for effectively dealing with red panda images. Based on a database of red panda images constructed by ourselves, we evaluate the effectiveness of the proposed automatic individual red panda identification method. The evaluation results show the promising potential of automatically recognizing individual red pandas from their faces. We are going to release our database and model in the public domain to promote the research on automatic animal identification and particularly on the technique for protecting red pandas.



### Transferable Representation Learning in Vision-and-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1908.03409v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.03409v2)
- **Published**: 2019-08-09 10:58:01+00:00
- **Updated**: 2019-08-12 22:00:55+00:00
- **Authors**: Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.



### Repetitive Reprediction Deep Decipher for Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.04345v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04345v2)
- **Published**: 2019-08-09 11:57:16+00:00
- **Updated**: 2019-11-27 01:59:50+00:00
- **Authors**: Guo-Hua Wang, Jianxin Wu
- **Comment**: Accepted by AAAI 2020
- **Journal**: None
- **Summary**: Most recent semi-supervised deep learning (deep SSL) methods used a similar paradigm: use network predictions to update pseudo-labels and use pseudo-labels to update network parameters iteratively. However, they lack theoretical support and cannot explain why predictions are good candidates for pseudo-labels. In this paper, we propose a principled end-to-end framework named deep decipher (D2) for SSL. Within the D2 framework, we prove that pseudo-labels are related to network predictions by an exponential link function, which gives a theoretical support for using predictions as pseudo-labels. Furthermore, we demonstrate that updating pseudo-labels by network predictions will make them uncertain. To mitigate this problem, we propose a training strategy called repetitive reprediction (R2). Finally, the proposed R2-D2 method is tested on the large-scale ImageNet dataset and outperforms state-of-the-art methods by 5 percentage points.



### A Fast and Precise Method for Large-Scale Land-Use Mapping Based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.03438v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03438v1)
- **Published**: 2019-08-09 12:41:06+00:00
- **Updated**: 2019-08-09 12:41:06+00:00
- **Authors**: Xuan Yang, Zhengchao Chen, Baipeng Li, Dailiang Peng, Pan Chen, Bing Zhang
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2019
- **Journal**: None
- **Summary**: The land-use map is an important data that can reflect the use and transformation of human land, and can provide valuable reference for land-use planning. For the traditional image classification method, producing a high spatial resolution (HSR), land-use map in large-scale is a big project that requires a lot of human labor, time, and financial expenditure. The rise of the deep learning technique provides a new solution to the problems above. This paper proposes a fast and precise method that can achieve large-scale land-use classification based on deep convolutional neural network (DCNN). In this paper, we optimize the data tiling method and the structure of DCNN for the multi-channel data and the splicing edge effect, which are unique to remote sensing deep learning, and improve the accuracy of land-use classification. We apply our improved methods in the Guangdong Province of China using GF-1 images, and achieve the land-use classification accuracy of 81.52%. It takes only 13 hours to complete the work, which will take several months for human labor.



### Relation-Aware Pyramid Network (RapNet) for temporal action proposal
- **Arxiv ID**: http://arxiv.org/abs/1908.03448v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03448v1)
- **Published**: 2019-08-09 13:20:32+00:00
- **Updated**: 2019-08-09 13:20:32+00:00
- **Authors**: Jialin Gao, Zhixiang Shi, Jiani Li, Yufeng Yuan, Jiwei Li, Xi Zhou
- **Comment**: Submission to temporal action proposal task in ActivityNet Challenge
  2019
- **Journal**: None
- **Summary**: In this technical report, we describe our solution to temporal action proposal (task 1) in ActivityNet Challenge 2019. First, we fine-tune a ResNet-50-C3D CNN on ActivityNet v1.3 based on Kinetics pretrained model to extract snippet-level video representations and then we design a Relation-Aware Pyramid Network (RapNet) to generate temporal multiscale proposals with confidence score. After that, we employ a two-stage snippet-level boundary adjustment scheme to re-rank the order of generated proposals. Ensemble methods are also been used to improve the performance of our solution, which helps us achieve 2nd place.



### Bias and variance reduction and denoising for CTF Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.03454v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03454v3)
- **Published**: 2019-08-09 13:32:41+00:00
- **Updated**: 2020-01-28 16:36:04+00:00
- **Authors**: Ayelet Heimowitz, Joakim Andén, Amit Singer
- **Comment**: None
- **Journal**: None
- **Summary**: When using an electron microscope for imaging of particles embedded in vitreous ice, the objective lens will inevitably corrupt the projection images. This corruption manifests as a band-pass filter on the micrograph. In addition, it causes the phase of several frequency bands to be flipped and distorts frequency bands. As a precursor to compensating for this distortion, the corrupting point spread function, which is termed the contrast transfer function (CTF) in reciprocal space, must be estimated. In this paper, we will present a novel method for CTF estimation. Our method is based on the multi-taper method for power spectral density estimation, which aims to reduce the bias and variance of the estimator. Furthermore, we use known properties of the CTF and of the background of the power spectrum to increase the accuracy of our estimation. We will show that the resulting estimates capture the zero-crossings of the CTF in the low-mid frequency range.



### The Channel Attention based Context Encoder Network for Inner Limiting Membrane Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.04413v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1908.04413v1)
- **Published**: 2019-08-09 13:48:50+00:00
- **Updated**: 2019-08-09 13:48:50+00:00
- **Authors**: Hao Qiu, Zaiwang Gu, Lei Mou, Xiaoqian Mao, Liyang Fang, Yitian Zhao, Jiang Liu, Jun Cheng
- **Comment**: This paper has been accepted by the miccai workshop (OMIA-6)
- **Journal**: None
- **Summary**: The optic disc segmentation is an important step for retinal image-based disease diagnosis such as glaucoma. The inner limiting membrane (ILM) is the first boundary in the OCT, which can help to extract the retinal pigment epithelium (RPE) through gradient edge information to locate the boundary of the optic disc. Thus, the ILM layer segmentation is of great importance for optic disc localization. In this paper, we build a new optic disc centered dataset from 20 volunteers and manually annotated the ILM boundary in each OCT scan as ground-truth. We also propose a channel attention based context encoder network modified from the CE-Net to segment the optic disc. It mainly contains three phases: the encoder module, the channel attention based context encoder module, and the decoder module. Finally, we demonstrate that our proposed method achieves state-of-the-art disc segmentation performance on our dataset mentioned above.



### Zero-Shot Feature Selection via Transferring Supervised Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1908.03464v3
- **DOI**: 10.4018/IJDWM.2021040101
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03464v3)
- **Published**: 2019-08-09 14:09:40+00:00
- **Updated**: 2021-07-14 12:57:06+00:00
- **Authors**: Zheng Wang, Qiao Wang, Tingzhang Zhao, Xiaojun Ye
- **Comment**: Published in IJDWM21
- **Journal**: None
- **Summary**: Feature selection, an effective technique for dimensionality reduction, plays an important role in many machine learning systems. Supervised knowledge can significantly improve the performance. However, faced with the rapid growth of newly emerging concepts, existing supervised methods might easily suffer from the scarcity and validity of labeled data for training. In this paper, the authors study the problem of zero-shot feature selection (i.e., building a feature selection model that generalizes well to "unseen" concepts with limited training data of "seen" concepts). Specifically, they adopt class-semantic descriptions (i.e., attributes) as supervision for feature selection, so as to utilize the supervised knowledge transferred from the seen concepts. For more reliable discriminative features, they further propose the center-characteristic loss which encourages the selected features to capture the central characteristics of seen concepts. Extensive experiments conducted on various real-world datasets demonstrate the effectiveness of the method.



### Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1908.03477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03477v1)
- **Published**: 2019-08-09 14:41:06+00:00
- **Updated**: 2019-08-09 14:41:06+00:00
- **Authors**: Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen
- **Comment**: Accepted for presentation at ICCV. Project Page:
  https://mwray.github.io/FGAR
- **Journal**: None
- **Summary**: We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities.   We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.



### Synthetic Elastography using B-mode Ultrasound through a Deep Fully-Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1908.03573v2
- **DOI**: 10.1109/TUFFC.2020.2983099
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03573v2)
- **Published**: 2019-08-09 14:57:07+00:00
- **Updated**: 2020-04-04 10:55:01+00:00
- **Authors**: R. R. Wildeboer, R. J. G. van Sloun, C. K. Mannaerts, P. H. Moraes, G. Salomon, M. C. Chammas, H. Wijkstra, M. Mischi
- **Comment**: (c) 2020 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency
  Control, 2020
- **Summary**: Shear-wave elastography (SWE) permits local estimation of tissue elasticity, an important imaging marker in biomedicine. This recently-developed, advanced technique assesses the speed of a laterally-travelling shear wave after an acoustic radiation force "push" to estimate local Young's moduli in an operator-independent fashion. In this work, we show how synthetic SWE (sSWE) images can be generated based on conventional B-mode imaging through deep learning. Using side-by-side-view B-mode/SWE images collected in 50 patients with prostate cancer, we show that sSWE images with a pixel-wise mean absolute error of 4.5+/-0.96 kPa with regard to the original SWE can be generated. Visualization of high-level feature levels through t-Distributed Stochastic Neighbor Embedding reveals substantial overlap between data from two different scanners. Qualitatively, we examined the use of the sSWE methodology for B-mode images obtained with a scanner without SWE functionality. We also examined the use of this type of network in elasticity imaging in the thyroid. Limitations of the technique reside in the fact that networks have to be retrained for different organs, and that the method requires standardization of the imaging settings and procedure. Future research will be aimed at development of sSWE as an elasticity-related tissue typing strategy that is solely based on B-mode ultrasound acquisition, and the examination of its clinical utility.



### Bayesian Inference for Large Scale Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.03491v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03491v1)
- **Published**: 2019-08-09 15:15:56+00:00
- **Updated**: 2019-08-09 15:15:56+00:00
- **Authors**: Jonathan Heek, Nal Kalchbrenner
- **Comment**: None
- **Journal**: None
- **Summary**: Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters. Despite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind optimization methods for large scale deep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. We use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline.



### Neural Image Compression and Explanation
- **Arxiv ID**: http://arxiv.org/abs/1908.08988v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08988v2)
- **Published**: 2019-08-09 15:39:20+00:00
- **Updated**: 2020-12-08 03:01:50+00:00
- **Authors**: Xiang Li, Shihao Ji
- **Comment**: Published as a journal paper at IEEE Access 2020
- **Journal**: None
- **Summary**: Explaining the prediction of deep neural networks (DNNs) and semantic image compression are two active research areas of deep learning with a numerous of applications in decision-critical systems, such as surveillance cameras, drones and self-driving cars, where interpretable decision is critical and storage/network bandwidth is limited. In this paper, we propose a novel end-to-end Neural Image Compression and Explanation (NICE) framework that learns to (1) explain the predictions of convolutional neural networks (CNNs), and (2) subsequently compress the input images for efficient storage or transmission. Specifically, NICE generates a sparse mask over an input image by attaching a stochastic binary gate to each pixel of the image, whose parameters are learned through the interaction with the CNN classifier to be explained. The generated mask is able to capture the saliency of each pixel measured by its influence to the final prediction of CNN; it can also be used to produce a mixed-resolution image, where important pixels maintain their original high resolution and insignificant background pixels are subsampled to a low resolution. The produced images achieve a high compression rate (e.g., about 0.6x of original image file size), while retaining a similar classification accuracy. Extensive experiments across multiple image classification benchmarks demonstrate the superior performance of NICE compared to the state-of-the-art methods in terms of explanation quality and semantic image compression rate. Our code is available at: https://github.com/lxuniverse/NICE.



### Human Perceptual Evaluations for Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1908.04187v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04187v1)
- **Published**: 2019-08-09 16:53:43+00:00
- **Updated**: 2019-08-09 16:53:43+00:00
- **Authors**: Yash Patel, Srikar Appalaraju, R. Manmatha
- **Comment**: arXiv admin note: text overlap with arXiv:1907.08310
- **Journal**: None
- **Summary**: Recently, there has been much interest in deep learning techniques to do image compression and there have been claims that several of these produce better results than engineered compression schemes (such as JPEG, JPEG2000 or BPG). A standard way of comparing image compression schemes today is to use perceptual similarity metrics such as PSNR or MS-SSIM (multi-scale structural similarity). This has led to some deep learning techniques which directly optimize for MS-SSIM by choosing it as a loss function. While this leads to a higher MS-SSIM for such techniques, we demonstrate using user studies that the resulting improvement may be misleading. Deep learning techniques for image compression with a higher MS-SSIM may actually be perceptually worse than engineered compression schemes with a lower MS-SSIM.



### VisualBERT: A Simple and Performant Baseline for Vision and Language
- **Arxiv ID**: http://arxiv.org/abs/1908.03557v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03557v1)
- **Published**: 2019-08-09 17:57:13+00:00
- **Updated**: 2019-08-09 17:57:13+00:00
- **Authors**: Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang
- **Comment**: Work in Progress
- **Journal**: None
- **Summary**: We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.



### Breast Ultrasound Computer-Aided Diagnosis Using Structure-Aware Triplet Path Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.09825v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09825v2)
- **Published**: 2019-08-09 20:36:40+00:00
- **Updated**: 2020-06-14 23:32:10+00:00
- **Authors**: Erlei Zhang, Zi Yang, Stephen Seiler, Mingli Chen, Weiguo Lu, Xuejun Gu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.01076
- **Journal**: None
- **Summary**: Breast ultrasound (US) is an effective imaging modality for breast cancer detec-tion and diagnosis. The structural characteristics of breast lesion play an im-portant role in Computer-Aided Diagnosis (CAD). In this paper, a novel struc-ture-aware triplet path networks (SATPN) was designed to integrate classifica-tion and two image reconstruction tasks to achieve accurate diagnosis on US im-ages with small training dataset. Specifically, we enhance clinically-approved breast lesion structure characteristics though converting original breast US imag-es to BIRADS-oriented feature maps (BFMs) with a distance-transformation coupled Gaussian filter. Then, the converted BFMs were used as the inputs of SATPN, which performed lesion classification task and two unsupervised stacked convolutional Auto-Encoder (SCAE) networks for benign and malignant image reconstruction tasks, independently. We trained the SATPN with an alter-native learning strategy by balancing image reconstruction error and classification label prediction error. At the test stage, the lesion label was determined by the weighted voting with reconstruction error and label prediction error. We com-pared the performance of the SATPN with TPN using original image as input and our previous developed semi-supervised deep learning methods using BFMs as inputs. Experimental results on two breast US datasets showed that SATPN ranked the best among the three networks, with classification accuracy around 93.5%. These findings indicated that SATPN is promising for effective breast US lesion CAD using small datasets.



### A Mask-RCNN Baseline for Probabilistic Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03621v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03621v2)
- **Published**: 2019-08-09 20:45:46+00:00
- **Updated**: 2019-10-14 15:45:38+00:00
- **Authors**: Phil Ammirato, Alexander C. Berg
- **Comment**: 2nd place in 1st PODC at CVPR 2019
- **Journal**: None
- **Summary**: The Probabilistic Object Detection Challenge evaluates object detection methods using a new evaluation measure, Probability-based Detection Quality (PDQ), on a new synthetic image dataset. We present our submission to the challenge, a fine-tuned version of Mask-RCNN with some additional post-processing. Our method, submitted under username pammirato, is currently second on the leaderboard with a score of 21.432, while also achieving the highest spatial quality and average overall quality of detections. We hope this method can provide some insight into how detectors designed for mean average precision (mAP) evaluation behave under PDQ, as well as a strong baseline for future work.



### Learning morphological operators for skin detection
- **Arxiv ID**: http://arxiv.org/abs/1908.03630v2
- **DOI**: 10.33969/AIS.2019.11004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03630v2)
- **Published**: 2019-08-09 21:05:11+00:00
- **Updated**: 2019-09-23 22:01:10+00:00
- **Authors**: Alessandra Lumini, Loris Nanni, Alice Codogno, Filippo Berno
- **Comment**: None
- **Journal**: Journal of Artificial Intelligence and Systems (2019)
- **Summary**: In this work we propose a novel post processing approach for skin detectors based on trained morphological operators. The first step, consisting in skin segmentation is performed according to an existing skin detection approach is performed for skin segmentation, then a second step is carried out consisting in the application of a set of morphological operators to refine the resulting mask. Extensive experimental evaluation performed considering two different detection approaches (one based on deep learning and a handcrafted one) carried on 10 different datasets confirms the quality of the proposed method.



### DSIC: Deep Stereo Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1908.03631v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03631v1)
- **Published**: 2019-08-09 21:10:23+00:00
- **Updated**: 2019-08-09 21:10:23+00:00
- **Authors**: Jerry Liu, Shenlong Wang, Raquel Urtasun
- **Comment**: Accepted at International Conference on Computer Vision 2019
- **Journal**: None
- **Summary**: In this paper we tackle the problem of stereo image compression, and leverage the fact that the two images have overlapping fields of view to further compress the representations. Our approach leverages state-of-the-art single-image compression autoencoders and enhances the compression with novel parametric skip functions to feed fully differentiable, disparity-warped features at all levels to the encoder/decoder of the second image. Moreover, we model the probabilistic dependence between the image codes using a conditional entropy model. Our experiments show an impressive 30 - 50% reduction in the second image bitrate at low bitrates compared to deep single-image compression, and a 10 - 20% reduction at higher bitrates.



### Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1908.03636v2
- **DOI**: 10.1109/WACV45572.2020.9093435
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03636v2)
- **Published**: 2019-08-09 21:22:29+00:00
- **Updated**: 2020-06-15 13:45:29+00:00
- **Authors**: Martin Weigert, Uwe Schmidt, Robert Haase, Ko Sugawara, Gene Myers
- **Comment**: Conference paper at WACV 2020
- **Journal**: None
- **Summary**: Accurate detection and segmentation of cell nuclei in volumetric (3D) fluorescence microscopy datasets is an important step in many biomedical research projects. Although many automated methods for these tasks exist, they often struggle for images with low signal-to-noise ratios and/or dense packing of nuclei. It was recently shown for 2D microscopy images that these issues can be alleviated by training a neural network to directly predict a suitable shape representation (star-convex polygon) for cell nuclei. In this paper, we adopt and extend this approach to 3D volumes by using star-convex polyhedra to represent cell nuclei and similar shapes. To that end, we overcome the challenges of 1) finding parameter-efficient star-convex polyhedra representations that can faithfully describe cell nuclei shapes, 2) adapting to anisotropic voxel sizes often found in fluorescence microscopy datasets, and 3) efficiently computing intersections between pairs of star-convex polyhedra (required for non-maximum suppression). Although our approach is quite general, since star-convex polyhedra include common shapes like bounding boxes and spheres as special cases, our focus is on accurate detection and segmentation of cell nuclei. Finally, we demonstrate on two challenging datasets that our approach (StarDist-3D) leads to superior results when compared to classical and deep learning based methods.



### A Distraction Score for Watermarks
- **Arxiv ID**: http://arxiv.org/abs/1908.03651v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03651v1)
- **Published**: 2019-08-09 22:44:14+00:00
- **Updated**: 2019-08-09 22:44:14+00:00
- **Authors**: Aurelia Guy, Sema Berkiten
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we propose a novel technique to quantify how distracting watermarks are on an image. We begin with watermark detection using a two-tower CNN model composed of a binary classification task and a semantic segmentation prediction. With this model, we demonstrate significant improvement in image precision while maintaining per-pixel accuracy, especially for our real-world dataset with sparse positive examples. We fit a nonlinear function to represent detected watermarks by a single score correlated with human perception based on their size, location, and visual obstructiveness. Finally, we validate our method in an image ranking setup, which is the main application of our watermark scoring algorithm.



