# Arxiv Papers in cs.CV on 2019-08-19
### HumanMeshNet: Polygonal Mesh Recovery of Humans
- **Arxiv ID**: http://arxiv.org/abs/1908.06544v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06544v1)
- **Published**: 2019-08-19 00:27:24+00:00
- **Updated**: 2019-08-19 00:27:24+00:00
- **Authors**: Abbhinav Venkat, Chaitanya Patel, Yudhik Agrawal, Avinash Sharma
- **Comment**: to appear in ICCV-W, 2019. Project:
  https://github.com/yudhik11/HumanMeshNet
- **Journal**: None
- **Summary**: 3D Human Body Reconstruction from a monocular image is an important problem in computer vision with applications in virtual and augmented reality platforms, animation industry, en-commerce domain, etc. While several of the existing works formulate it as a volumetric or parametric learning with complex and indirect reliance on re-projections of the mesh, we would like to focus on implicitly learning the mesh representation. To that end, we propose a novel model, HumanMeshNet, that regresses a template mesh's vertices, as well as receives a regularization by the 3D skeletal locations in a multi-branch, multi-task setup. The image to mesh vertex regression is further regularized by the neighborhood constraint imposed by mesh topology ensuring smooth surface reconstruction. The proposed paradigm can theoretically learn local surface deformations induced by body shape variations and can therefore learn high-resolution meshes going ahead. We show comparable performance with SoA (in terms of surface and joint error) with far lesser computational complexity, modeling cost and therefore real-time reconstructions on three publicly available datasets. We also show the generalizability of the proposed paradigm for a similar task of predicting hand mesh models. Given these initial results, we would like to exploit the mesh topology in an explicit manner going ahead.



### Weakly-supervised Action Localization with Background Modeling
- **Arxiv ID**: http://arxiv.org/abs/1908.06552v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06552v1)
- **Published**: 2019-08-19 01:33:14+00:00
- **Updated**: 2019-08-19 01:33:14+00:00
- **Authors**: Phuc Xuan Nguyen, Deva Ramanan, Charless C. Fowlkes
- **Comment**: To appear at ICCV 2019
- **Journal**: None
- **Summary**: We describe a latent approach that learns to detect actions in long sequences given training videos with only whole-video class labels. Our approach makes use of two innovations to attention-modeling in weakly-supervised learning. First, and most notably, our framework uses an attention model to extract both foreground and background frames whose appearance is explicitly modeled. Most prior works ignore the background, but we show that modeling it allows our system to learn a richer notion of actions and their temporal extents. Second, we combine bottom-up, class-agnostic attention modules with top-down, class-specific activation maps, using the latter as form of self-supervision for the former. Doing so allows our model to learn a more accurate model of attention without explicit temporal supervision. These modifications lead to 10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed weaklysupervised system outperforms recent state-of-the-arts by at least 4.3% AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used to aggressively scale-up learning to in-the-wild, uncurated Instagram videos. The addition of these videos significantly improves localization performance of our weakly-supervised model



### Adversarial Defense by Suppressing High-frequency Components
- **Arxiv ID**: http://arxiv.org/abs/1908.06566v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06566v3)
- **Published**: 2019-08-19 02:43:35+00:00
- **Updated**: 2019-09-03 09:00:46+00:00
- **Authors**: Zhendong Zhang, Cheolkon Jung, Xiaolong Liang
- **Comment**: 3 pages. This paper is a technical report of the 5th place solution
  in the IJCAI-2019 Alibaba Adversarial AI Challenge. This paper has been
  accepted by the corresponding workshop
- **Journal**: None
- **Summary**: Recent works show that deep neural networks trained on image classification dataset bias towards textures. Those models are easily fooled by applying small high-frequency perturbations to clean images. In this paper, we learn robust image classification models by removing high-frequency components. Specifically, we develop a differentiable high-frequency suppression module based on discrete Fourier transform (DFT). Combining with adversarial training, we won the 5th place in the IJCAI-2019 Alibaba Adversarial AI Challenge. Our code is available online.



### A Co-analysis Framework for Exploring Multivariate Scientific Data
- **Arxiv ID**: http://arxiv.org/abs/1908.06576v1
- **DOI**: 10.1016/j.visinf.2018.12.005
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.06576v1)
- **Published**: 2019-08-19 03:31:48+00:00
- **Updated**: 2019-08-19 03:31:48+00:00
- **Authors**: Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin
- **Comment**: 31 pages, 7 figures
- **Journal**: Visual Informatics Volume 2, Issue 4, December 2018, Pages 254-263
- **Summary**: In complex multivariate data sets, different features usually include diverse associations with different variables, and different variables are associated within different regions. Therefore, exploring the associations between variables and voxels locally becomes necessary to better understand the underlying phenomena. In this paper, we propose a co-analysis framework based on biclusters, which are two subsets of variables and voxels with close scalar-value relationships, to guide the process of visually exploring multivariate data. We first automatically extract all meaningful biclusters, each of which only contains voxels with a similar scalar-value pattern over a subset of variables. These biclusters are organized according to their variable sets, and biclusters in each variable set are further grouped by a similarity metric to reduce redundancy and support diversity during visual exploration. Biclusters are visually represented in coordinated views to facilitate interactive exploration of multivariate data based on the similarity between biclusters and the correlation of scalar values with different variables. Experiments on several representative multivariate scientific data sets demonstrate the effectiveness of our framework in exploring local relationships among variables, biclusters and scalar values in the data.



### How far should self-driving cars see? Effect of observation range on vehicle self-localization
- **Arxiv ID**: http://arxiv.org/abs/1908.06588v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06588v1)
- **Published**: 2019-08-19 04:32:44+00:00
- **Updated**: 2019-08-19 04:32:44+00:00
- **Authors**: Mahdi Javanmardi, Ehsan Javanmardi, Shunsuke Kamijo
- **Comment**: 6 pages, 11 figures, IEEE International Conference on Intelligent
  Transportation Systems 2019
- **Journal**: None
- **Summary**: Accuracy and time efficiency are two essential requirements for the self-localization of autonomous vehicles. While the observation range considered for simultaneous localization and mapping (SLAM) has a significant effect on both accuracy and computation time, its effect is not well investigated in the literature. In this paper, we will answer the question: How far should a driverless car observe during self-localization? We introduce a framework to dynamically define the observation range for localization to meet the accuracy requirement for autonomous driving, while keeping the computation time low. To model the effect of scanning range on the localization accuracy for every point on the map, several map factors were employed. The capability of the proposed framework was verified using field data, demonstrating that it is able to improve the average matching time from 142.2 ms to 39.3 ms while keeping the localization accuracy around 8.1 cm.



### Seq-SG2SL: Inferring Semantic Layout from Scene Graph Through Sequence to Sequence Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.06592v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06592v1)
- **Published**: 2019-08-19 04:47:26+00:00
- **Updated**: 2019-08-19 04:47:26+00:00
- **Authors**: Boren Li, Boyu Zhuang, Mingyang Li, Jian Gu
- **Comment**: This paper will appear at ICCV 2019
- **Journal**: None
- **Summary**: Generating semantic layout from scene graph is a crucial intermediate task connecting text to image. We present a conceptually simple, flexible and general framework using sequence to sequence (seq-to-seq) learning for this task. The framework, called Seq-SG2SL, derives sequence proxies for the two modality and a Transformer-based seq-to-seq model learns to transduce one into the other. A scene graph is decomposed into a sequence of semantic fragments (SF), one for each relationship. A semantic layout is represented as the consequence from a series of brick-action code segments (BACS), dictating the position and scale of each object bounding box in the layout. Viewing the two building blocks, SF and BACS, as corresponding terms in two different vocabularies, a seq-to-seq model is fittingly used to translate. A new metric, semantic layout evaluation understudy (SLEU), is devised to evaluate the task of semantic layout prediction inspired by BLEU. SLEU defines relationships within a layout as unigrams and looks at the spatial distribution for n-grams. Unlike the binary precision of BLEU, SLEU allows for some tolerances spatially through thresholding the Jaccard Index and is consequently more adapted to the task. Experimental results on the challenging Visual Genome dataset show improvement over a non-sequential approach based on graph convolution.



### Video synthesis of human upper body with realistic face
- **Arxiv ID**: http://arxiv.org/abs/1908.06607v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06607v3)
- **Published**: 2019-08-19 06:30:23+00:00
- **Updated**: 2019-09-12 08:33:03+00:00
- **Authors**: Zhaoxiang Liu, Huan Hu, Zipeng Wang, Kai Wang, Jinqiang Bai, Shiguo Lian
- **Comment**: 3 pages, 4 figures,Accepted by ISMAR 2019
- **Journal**: None
- **Summary**: This paper presents a generative adversarial learning-based human upper body video synthesis approach to generate an upper body video of target person that is consistent with the body motion, face expression, and pose of the person in source video. We use upper body keypoints, facial action units and poses as intermediate representations between source video and target video. Instead of directly transferring the source video to the target video, we firstly map the source person's facial action units and poses into the target person's facial landmarks, then combine the normalized upper body keypoints and generated facial landmarks with spatio-temporal smoothing to generate the corresponding target video's image. Experimental results demonstrated the effectiveness of our method.



### SPA-GAN: Spatial Attention GAN for Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1908.06616v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06616v3)
- **Published**: 2019-08-19 07:09:05+00:00
- **Updated**: 2020-12-30 22:46:05+00:00
- **Authors**: Hajar Emami, Majid Moradi Aliabadi, Ming Dong, Ratna Babu Chinnam
- **Comment**: IEEE Transactions on Multimedia, Digital Object Identifier:
  10.1109/TMM.2020.2975961
- **Journal**: None
- **Summary**: Image-to-image translation is to learn a mapping between images from a source domain and images from a target domain. In this paper, we introduce the attention mechanism directly to the generative adversarial network (GAN) architecture and propose a novel spatial attention GAN model (SPA-GAN) for image-to-image translation tasks. SPA-GAN computes the attention in its discriminator and use it to help the generator focus more on the most discriminative regions between the source and target domains, leading to more realistic output images. We also find it helpful to introduce an additional feature map loss in SPA-GAN training to preserve domain specific features during translation. Compared with existing attention-guided GAN models, SPA-GAN is a lightweight model that does not need additional attention networks or supervision. Qualitative and quantitative comparison against state-of-the-art methods on benchmark datasets demonstrates the superior performance of SPA-GAN.



### Cross-modal Zero-shot Hashing
- **Arxiv ID**: http://arxiv.org/abs/1908.07388v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07388v1)
- **Published**: 2019-08-19 07:14:41+00:00
- **Updated**: 2019-08-19 07:14:41+00:00
- **Authors**: Xuanwu Liu, Zhao Li, Jun Wang, Guoxian Yu, Carlotta Domeniconi, Xiangliang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing has been widely studied for big data retrieval due to its low storage cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing model that is trained using only samples from seen categories, but can generalize well to samples of unseen categories. ZSH generally uses category attributes to seek a semantic embedding space to transfer knowledge from seen categories to unseen ones. As a result, it may perform poorly when labeled data are insufficient. ZSH methods are mainly designed for single-modality data, which prevents their application to the widely spread multi-modal data. On the other hand, existing cross-modal hashing solutions assume that all the modalities share the same category labels, while in practice the labels of different data modalities may be different. To address these issues, we propose a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively leverage unlabeled and labeled multi-modality data with different label spaces. CZHash first quantifies the composite similarity between instances using label and feature information. It then defines an objective function to achieve deep feature learning compatible with the composite similarity preserving, category attribute space learning, and hashing coding function learning. CZHash further introduces an alternative optimization procedure to jointly optimize these learning objectives. Experiments on benchmark multi-modal datasets show that CZHash significantly outperforms related representative hashing approaches both on effectiveness and adaptability.



### IRNet: Instance Relation Network for Overlapping Cervical Cell Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.06623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06623v1)
- **Published**: 2019-08-19 07:34:02+00:00
- **Updated**: 2019-08-19 07:34:02+00:00
- **Authors**: Yanning Zhou, Hao Chen, Jiaqi Xu, Qi Dou, Pheng-Ann Heng
- **Comment**: Accepted by the 22nd International Conference on Medical Image
  Computing and Computer Assisted Intervention
- **Journal**: None
- **Summary**: Cell instance segmentation in Pap smear image remains challenging due to the wide existence of occlusion among translucent cytoplasm in cell clumps. Conventional methods heavily rely on accurate nuclei detection results and are easily disturbed by miscellaneous objects. In this paper, we propose a novel Instance Relation Network (IRNet) for robust overlapping cell segmentation by exploring instance relation interaction. Specifically, we propose the Instance Relation Module to construct the cell association matrix for transferring information among individual cell-instance features. With the collaboration of different instances, the augmented features gain benefits from contextual information and improve semantic consistency. Meanwhile, we proposed a sparsity constrained Duplicate Removal Module to eliminate the misalignment between classification and localization accuracy for candidates selection. The largest cervical Pap smear (CPS) dataset with more than 8000 cell annotations in Pap smear image was constructed for comprehensive evaluation. Our method outperforms other methods by a large margin, demonstrating the effectiveness of exploring instance relation.



### Multi Target Tracking by Learning from Generalized Graph Differences
- **Arxiv ID**: http://arxiv.org/abs/1908.06646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06646v1)
- **Published**: 2019-08-19 08:57:46+00:00
- **Updated**: 2019-08-19 08:57:46+00:00
- **Authors**: Håkan Ardö, Mikael Nilsson
- **Comment**: None
- **Journal**: None
- **Summary**: Formulating the multi object tracking problem as a network flow optimization problem is a popular choice. In this paper an efficient way of learning the weights of such a network is presented. It separates the problem into one embedding of feasible solutions into a one dimensional feature space and one optimization problem. The embedding can be learned using standard SGD type optimization without relying on an additional optimizations within each step. Training data is produced by performing small perturbations of ground truth tracks and representing them using generalized graph differences, which is an efficient way introduced to represent the difference between two graphs. The proposed method is evaluated on DukeMTMCT with competitive results.



### RANet: Ranking Attention Network for Fast Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.06647v4
- **DOI**: 10.1109/ICCV.2019.00408
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06647v4)
- **Published**: 2019-08-19 08:58:49+00:00
- **Updated**: 2019-09-08 04:01:27+00:00
- **Authors**: Ziqin Wang, Jun Xu, Li Liu, Fan Zhu, Ling Shao
- **Comment**: Accepted by ICCV 2019. 10 pages, 7 figures, 6 tables. The
  supplementary file can be found at
  https://csjunxu.github.io/paper/2019ICCV/RANet_supp.pdf ; Code is available
  at https://github.com/Storife/RANet
- **Journal**: None
- **Summary**: Despite online learning (OL) techniques have boosted the performance of semi-supervised video object segmentation (VOS) methods, the huge time costs of OL greatly restrict their practicality. Matching based and propagation based methods run at a faster speed by avoiding OL techniques. However, they are limited by sub-optimal accuracy, due to mismatching and drifting problems. In this paper, we develop a real-time yet very accurate Ranking Attention Network (RANet) for VOS. Specifically, to integrate the insights of matching based and propagation based methods, we employ an encoder-decoder framework to learn pixel-level similarity and segmentation in an end-to-end manner. To better utilize the similarity maps, we propose a novel ranking attention module, which automatically ranks and selects these maps for fine-grained VOS performance. Experiments on DAVIS-16 and DAVIS-17 datasets show that our RANet achieves the best speed-accuracy trade-off, e.g., with 33 milliseconds per frame and J&F=85.5% on DAVIS-16. With OL, our RANet reaches J&F=87.1% on DAVIS-16, exceeding state-of-the-art VOS methods. The code can be found at https://github.com/Storife/RANet.



### Graph-Based Object Classification for Neuromorphic Vision Sensing
- **Arxiv ID**: http://arxiv.org/abs/1908.06648v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06648v1)
- **Published**: 2019-08-19 08:59:21+00:00
- **Updated**: 2019-08-19 08:59:21+00:00
- **Authors**: Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze, Yiannis Andreopoulos
- **Comment**: 13 pages, 4 figures, ICCV 2019
- **Journal**: None
- **Summary**: Neuromorphic vision sensing (NVS)\ devices represent visual information as sequences of asynchronous discrete events (a.k.a., ``spikes'') in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, object classification with NVS streams cannot leverage on state-of-the-art convolutional neural networks (CNNs), since NVS does not produce frame representations. To circumvent this mismatch between sensing and processing with CNNs, we propose a compact graph representation for NVS. We couple this with novel residual graph CNN architectures and show that, when trained on spatio-temporal NVS data for object classification, such residual graph CNNs preserve the spatial and temporal coherence of spike events, while requiring less computation and memory. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we present and make available a 100k dataset of NVS recordings of the American sign language letters, acquired with an iniLabs DAVIS240c device under real-world conditions.



### Development of a Robotic System for Automatic Wheel Removal and Fitting
- **Arxiv ID**: http://arxiv.org/abs/1908.09009v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09009v1)
- **Published**: 2019-08-19 09:24:59+00:00
- **Updated**: 2019-08-19 09:24:59+00:00
- **Authors**: Gideon Gbenga Oladipupo
- **Comment**: 8 pages, 17 figures and 3 tables
- **Journal**: None
- **Summary**: This paper discusses the image processing and computer vision algorithms for real time detection and tracking of a sample wheel of a vehicle. During the manual tyre changing process, spinal and other muscular injuries are common and even more serious injuries have been recorded when occasionally, tyres fail (burst) during this process. It, therefore, follows that the introduction of a robotic system to take over this process would be a welcome development. This work discusses various useful applicable algorithms, Circular Hough Transform (CHT) as well as Continuously adaptive mean shift (Camshift) and provides some of the software solutions which can be deployed with a robotic mechanical arm to make the task of tyre changing faster, safer and more efficient. Image acquisition and software to accurately detect and classify specific objects of interest were implemented successfully, outcomes were discussed and areas for further studies suggested.



### C-RPNs: Promoting Object Detection in real world via a Cascade Structure of Region Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.06665v1
- **DOI**: 10.1016/j.neucom.2019.08.016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06665v1)
- **Published**: 2019-08-19 09:37:08+00:00
- **Updated**: 2019-08-19 09:37:08+00:00
- **Authors**: Dongming Yang, YueXian Zou, Jian Zhang, Ge Li
- **Comment**: 28 pages,10 figures
- **Journal**: None
- **Summary**: Recently, significant progresses have been made in object detection on common benchmarks (i.e., Pascal VOC). However, object detection in real world is still challenging due to the serious data imbalance. Images in real world are dominated by easy samples like the wide range of background and some easily recognizable objects, for example. Although two-stage detectors like Faster R-CNN achieved big successes in object detection due to the strategy of extracting region proposals by region proposal network, they show their poor adaption in real-world object detection as a result of without considering mining hard samples during extracting region proposals. To address this issue, we propose a Cascade framework of Region Proposal Networks, referred to as C-RPNs. The essence of C-RPNs is adopting multiple stages to mine hard samples while extracting region proposals and learn stronger classifiers. Meanwhile, a feature chain and a score chain are proposed to help learning more discriminative representations for proposals. Moreover, a loss function of cascade stages is designed to train cascade classifiers through backpropagation. Our proposed method has been evaluated on Pascal VOC and several challenging datasets like BSBDV 2017, CityPersons, etc. Our method achieves competitive results compared with the current state-of-the-arts and all-sided improvements in error analysis, validating its efficacy for detection in real world.



### Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.06673v1
- **DOI**: 10.1016/j.isprsjprs.2020.02.004
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06673v1)
- **Published**: 2019-08-19 09:57:24+00:00
- **Updated**: 2019-08-19 09:57:24+00:00
- **Authors**: Congcong Wen, Lina Yang, Ling Peng, Xiang Li, Tianhe Chi
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 2020, 162:
  50-62
- **Summary**: Point cloud classification plays an important role in a wide range of airborne light detection and ranging (LiDAR) applications, such as topographic mapping, forest monitoring, power line detection, and road detection. However, due to the sensor noise, high redundancy, incompleteness, and complexity of airborne LiDAR systems, point cloud classification is challenging. In this paper, we proposed a directionally constrained fully convolutional neural network (D-FCN) that can take the original 3D coordinates and LiDAR intensity as input; thus, it can directly apply to unstructured 3D point clouds for semantic labeling. Specifically, we first introduce a novel directionally constrained point convolution (D-Conv) module to extract locally representative features of 3D point sets from the projected 2D receptive fields. To make full use of the orientation information of neighborhood points, the proposed D-Conv module performs convolution in an orientation-aware manner by using a directionally constrained nearest neighborhood search. Then, we designed a multiscale fully convolutional neural network with downsampling and upsampling blocks to enable multiscale point feature learning. The proposed D-FCN model can therefore process input point cloud with arbitrary sizes and directly predict the semantic labels for all the input points in an end-to-end manner. Without involving additional geometry features as input, the proposed method has demonstrated superior performance on the International Society for Photogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark dataset. The results show that our model has achieved a new state-of-the-art level of performance with an average F1 score of 70.7%, and it has improved the performance by a large margin on categories with a small number of points (such as powerline, car, and facade).



### A unified representation network for segmentation with missing modalities
- **Arxiv ID**: http://arxiv.org/abs/1908.06683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06683v1)
- **Published**: 2019-08-19 10:31:59+00:00
- **Updated**: 2019-08-19 10:31:59+00:00
- **Authors**: Kenneth Lau, Jonas Adler, Jens Sjölund
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last few years machine learning has demonstrated groundbreaking results in many areas of medical image analysis, including segmentation. A key assumption, however, is that the train- and test distributions match. We study a realistic scenario where this assumption is clearly violated, namely segmentation with missing input modalities. We describe two neural network approaches that can handle a variable number of input modalities. The first is modality dropout: a simple but surprisingly effective modification of the training. The second is the unified representation network: a network architecture that maps a variable number of input modalities into a unified representation that can be used for downstream tasks such as segmentation. We demonstrate that modality dropout makes a standard segmentation network reasonably robust to missing modalities, but that the same network works even better if trained on the unified representation.



### In defense of OSVOS
- **Arxiv ID**: http://arxiv.org/abs/1908.06692v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06692v2)
- **Published**: 2019-08-19 11:07:17+00:00
- **Updated**: 2019-08-20 01:24:35+00:00
- **Authors**: Yu Liu, Yutong Dai, Anh-Dzung Doan, Lingqiao Liu, Ian Reid
- **Comment**: None
- **Journal**: None
- **Summary**: As a milestone for video object segmentation, one-shot video object segmentation (OSVOS) has achieved a large margin compared to the conventional optical-flow based methods regarding to the segmentation accuracy. Its excellent performance mainly benefit from the three-step training mechanism, that are: (1) acquiring object features on the base dataset (i.e. ImageNet), (2) training the parent network on the training set of the target dataset (i.e. DAVIS-2016) to be capable of differentiating the object of interest from the background. (3) online fine-tuning the interested object on the first frame of the target test set to overfit its appearance, then the model can be utilized to segment the same object in the rest frames of that video. In this paper, we argue that for the step (2), OSVOS has the limitation to 'overemphasize' the generic semantic object information while 'dilute' the instance cues of the object(s), which largely block the whole training process. Through adding a common module, video loss, which we formulate with various forms of constraints (including weighted BCE loss, high-dimensional triplet loss, as well as a novel mixed instance-aware video loss), to train the parent network in the step (2), the network is then better prepared for the step (3), i.e. online fine-tuning on the target instance. Through extensive experiments using different network structures as the backbone, we show that the proposed video loss module can improve the segmentation performance significantly, compared to that of OSVOS. Meanwhile, since video loss is a common module, it can be generalized to other fine-tuning based methods and similar vision tasks such as depth estimation and saliency detection.



### Adaptative Inference Cost With Convolutional Neural Mixture Models
- **Arxiv ID**: http://arxiv.org/abs/1908.06694v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06694v1)
- **Published**: 2019-08-19 11:10:50+00:00
- **Updated**: 2019-08-19 11:10:50+00:00
- **Authors**: Adria Ruiz, Jakob Verbeek
- **Comment**: None
- **Journal**: International Conference on Computer Vision (ICCV), 2019
- **Summary**: Despite the outstanding performance of convolutional neural networks (CNNs) for many vision tasks, the required computational cost during inference is problematic when resources are limited. In this context, we propose Convolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a large number of CNNs that can be jointly trained and evaluated in an efficient manner. Within the proposed framework, we present different mechanisms to prune subsets of CNNs from the mixture, allowing to easily adapt the computational cost required for inference. Image classification and semantic segmentation experiments show that our method achieve excellent accuracy-compute trade-offs. Moreover, unlike most of previous approaches, a single CNMM provides a large range of operating points along this trade-off, without any re-training.



### Floor-SP: Inverse CAD for Floorplans by Sequential Room-wise Shortest Path
- **Arxiv ID**: http://arxiv.org/abs/1908.06702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06702v1)
- **Published**: 2019-08-19 11:20:58+00:00
- **Updated**: 2019-08-19 11:20:58+00:00
- **Authors**: Jiacheng Chen, Chen Liu, Jiaye Wu, Yasutaka Furukawa
- **Comment**: 10 pages, 9 figures, accepted to ICCV 2019
- **Journal**: None
- **Summary**: This paper proposes a new approach for automated floorplan reconstruction from RGBD scans, a major milestone in indoor mapping research. The approach, dubbed Floor-SP, formulates a novel optimization problem, where room-wise coordinate descent sequentially solves dynamic programming to optimize the floorplan graph structure. The objective function consists of data terms guided by deep neural networks, consistency terms encouraging adjacent rooms to share corners and walls, and the model complexity term. The approach does not require corner/edge detection with thresholds, unlike most other methods. We have evaluated our system on production-quality RGBD scans of 527 apartments or houses, including many units with non-Manhattan structures. Qualitative and quantitative evaluations demonstrate a significant performance boost over the current state-of-the-art. Please refer to our project website http://jcchen.me/floor-sp/ for code and data.



### Some Aspects of Geometric Computer Vision for Analysing Dynamical Scenes focusing Automotive Applications
- **Arxiv ID**: http://arxiv.org/abs/1908.06726v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06726v1)
- **Published**: 2019-08-19 12:12:57+00:00
- **Updated**: 2019-08-19 12:12:57+00:00
- **Authors**: Volker Willert, Martin Buczko
- **Comment**: None
- **Journal**: None
- **Summary**: This draft summarizes some basics about geometric computer vision needed to implement efficient computer vision algorithms for applications that use measurements from at least one digital camera mounted on a moving platform with a special focus on automotive applications processing image streams taken from cameras mounted on a car. Our intention is twofold: On the one hand, we would like to introduce well-known basic geometric relations in a compact way that can also be found in lecture books about geometric computer vision like [1, 2]. On the other hand, we would like to share some experience about subtleties that should be taken into account in order to set up quite simple but robust and fast vision algorithms that are able to run in real time. We added a conglomeration of literature, we found to be relevant when implementing basic algorithms like optical flow, visual odometry and structure from motion. The reader should get some feeling about how the estimates of these algorithms are interrelated, which parts of the algorithms are critical in terms of robustness and what kind of additional assumptions can be useful to constrain the solution space of the underlying usually non-convex optimization problems.



### A Kings Ransom for Encryption: Ransomware Classification using Augmented One-Shot Learning and Bayesian Approximation
- **Arxiv ID**: http://arxiv.org/abs/1908.06750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06750v1)
- **Published**: 2019-08-19 12:38:38+00:00
- **Updated**: 2019-08-19 12:38:38+00:00
- **Authors**: Amir Atapour-Abarghouei, Stephen Bonner, Andrew Stephen McGough
- **Comment**: Submitted to 2019 IEEE International Conference on Big Data
- **Journal**: None
- **Summary**: Newly emerging variants of ransomware pose an ever-growing threat to computer systems governing every aspect of modern life through the handling and analysis of big data. While various recent security-based approaches have focused on detecting and classifying ransomware at the network or system level, easy-to-use post-infection ransomware classification for the lay user has not been attempted before. In this paper, we investigate the possibility of classifying the ransomware a system is infected with simply based on a screenshot of the splash screen or the ransom note captured using a consumer camera commonly found in any modern mobile device. To train and evaluate our system, we create a sample dataset of the splash screens of 50 well-known ransomware variants. In our dataset, only a single training image is available per ransomware. Instead of creating a large training dataset of ransomware screenshots, we simulate screenshot capture conditions via carefully designed data augmentation techniques, enabling simple and efficient one-shot learning. Moreover, using model uncertainty obtained via Bayesian approximation, we ensure special input cases such as unrelated non-ransomware images and previously-unseen ransomware variants are correctly identified for special handling and not mis-classified. Extensive experimental evaluation demonstrates the efficacy of our work, with accuracy levels of up to 93.6% for ransomware classification.



### Data Consistent Artifact Reduction for Limited Angle Tomography with Deep Learning Prior
- **Arxiv ID**: http://arxiv.org/abs/1908.06792v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06792v2)
- **Published**: 2019-08-19 13:28:35+00:00
- **Updated**: 2019-08-28 08:47:31+00:00
- **Authors**: Yixing Huang, Alexander Preuhs, Guenter Lauritsch, Michael Manhart, Xiaolin Huang, Andreas Maier
- **Comment**: Accepted by MICCAI MLMIR workshop
- **Journal**: None
- **Summary**: Robustness of deep learning methods for limited angle tomography is challenged by two major factors: a) due to insufficient training data the network may not generalize well to unseen data; b) deep learning methods are sensitive to noise. Thus, generating reconstructed images directly from a neural network appears inadequate. We propose to constrain the reconstructed images to be consistent with the measured projection data, while the unmeasured information is complemented by learning based methods. For this purpose, a data consistent artifact reduction (DCAR) method is introduced: First, a prior image is generated from an initial limited angle reconstruction via deep learning as a substitute for missing information. Afterwards, a conventional iterative reconstruction algorithm is applied, integrating the data consistency in the measured angular range and the prior information in the missing angular range. This ensures data integrity in the measured area, while inaccuracies incorporated by the deep learning prior lie only in areas where no information is acquired. The proposed DCAR method achieves significant image quality improvement: for 120-degree cone-beam limited angle tomography more than 10% RMSE reduction in noise-free case and more than 24% RMSE reduction in noisy case compared with a state-of-the-art U-Net based method.



### GLAMpoints: Greedily Learned Accurate Match points
- **Arxiv ID**: http://arxiv.org/abs/1908.06812v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06812v3)
- **Published**: 2019-08-19 14:06:27+00:00
- **Updated**: 2020-06-15 17:34:03+00:00
- **Authors**: Prune Truong, Stefanos Apostolopoulos, Agata Mosinska, Samuel Stucky, Carlos Ciller, Sandro De Zanet
- **Comment**: None
- **Journal**: In the IEEE International Conference on Computer Vision (ICCV),
  2019, pp. 10732-10741
- **Summary**: We introduce a novel CNN-based feature point detector - GLAMpoints - learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images. Our method can also be extended to other domains, such as natural images. Training code and model weights are available at https://github.com/PruneTruong/GLAMpoints_pytorch.



### Fully Automated Image De-fencing using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.06837v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06837v1)
- **Published**: 2019-08-19 14:44:24+00:00
- **Updated**: 2019-08-19 14:44:24+00:00
- **Authors**: Divyanshu Gupta, Shorya Jain, Utkarsh Tripathi, Pratik Chattopadhyay, Lipo Wang
- **Comment**: 10 pages, 5 figures
- **Journal**: None
- **Summary**: Image de-fencing is one of the important aspects of recreational photography in which the objective is to remove the fence texture present in an image and generate an aesthetically pleasing version of the same image without the fence texture. In this paper, we aim to develop an automated and effective technique for fence removal and image reconstruction using conditional Generative Adversarial Networks (cGANs). These networks have been successfully applied in several domains of Computer Vision focusing on image generation and rendering. Our initial approach is based on a two-stage architecture involving two cGANs that generate the fence mask and the inpainted image, respectively. Training of these networks is carried out independently and, during evaluation, the input image is passed through the two generators in succession to obtain the de-fenced image. The results obtained from this approach are satisfactory, but the response time is long since the image has to pass through two sets of convolution layers. To reduce the response time, we propose a second approach involving only a single cGAN architecture that is trained using the ground-truth of fenced de-fenced image pairs along with the edge map of the fenced image produced by the Canny Filter. Incorporation of the edge map helps the network to precisely detect the edges present in the input image, and also imparts it an ability to carry out high quality de-fencing in an efficient manner, even in the presence of a fewer number of layers as compared to the two-stage network. Qualitative and quantitative experimental results reported in the manuscript reveal that the de-fenced images generated by the single-stage de-fencing network have similar visual quality to those produced by the two-stage network. Comparative performance analysis also emphasizes the effectiveness of our approach over state-of-the-art image de-fencing techniques.



### SDIT: Scalable and Diverse Cross-domain Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1908.06881v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06881v1)
- **Published**: 2019-08-19 15:33:44+00:00
- **Updated**: 2019-08-19 15:33:44+00:00
- **Authors**: Yaxing Wang, Abel Gonzalez-Garcia, Joost van de Weijer, Luis Herranz
- **Comment**: ACM-MM2019 camera ready
- **Journal**: None
- **Summary**: Recently, image-to-image translation research has witnessed remarkable progress. Although current approaches successfully generate diverse outputs or perform scalable image transfer, these properties have not been combined into a single method. To address this limitation, we propose SDIT: Scalable and Diverse image-to-image translation. These properties are combined into a single generator. The diversity is determined by a latent variable which is randomly sampled from a normal distribution. The scalability is obtained by conditioning the network on the domain attributes. Additionally, we also exploit an attention mechanism that permits the generator to focus on the domain-specific attribute. We empirically demonstrate the performance of the proposed method on face mapping and other datasets beyond faces.



### Cross-Enhancement Transform Two-Stream 3D ConvNets for Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.08916v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08916v2)
- **Published**: 2019-08-19 15:48:35+00:00
- **Updated**: 2019-10-22 08:17:33+00:00
- **Authors**: Dong Cao, Lisha Xu, Dongdong Zhang
- **Comment**: Accepted for publication in AIIPCC 2019
- **Journal**: None
- **Summary**: Action recognition is an important research topic in computer vision. It is the basic work for visual understanding and has been applied in many fields. Since human actions can vary in different environments, it is difficult to infer actions in completely different states with a same structural model. For this case, we propose a Cross-Enhancement Transform Two-Stream 3D ConvNets algorithm, which considers the action distribution characteristics on the specific dataset. As a teaching model, stream with better performance in both streams is expected to assist in training another stream. In this way, the enhanced-trained stream and teacher stream are combined to infer actions. We implement experiments on the video datasets UCF-101, HMDB-51, and Kinetics-400, and the results confirm the effectiveness of our algorithm.



### Genetic Algorithms for the Optimization of Diffusion Parameters in Content-Based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1908.06896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06896v1)
- **Published**: 2019-08-19 15:54:55+00:00
- **Updated**: 2019-08-19 15:54:55+00:00
- **Authors**: Federico Magliani, Laura Sani, Stefano Cagnoni, Andrea Prati
- **Comment**: None
- **Journal**: None
- **Summary**: Several computer vision and artificial intelligence projects are nowadays exploiting the manifold data distribution using, e.g., the diffusion process. This approach has produced dramatic improvements on the final performance thanks to the application of such algorithms to the kNN graph. Unfortunately, this recent technique needs a manual configuration of several parameters, thus it is not straightforward to find the best configuration for each dataset. Moreover, the brute-force approach is computationally very demanding when used to optimally set the parameters of the diffusion approach. We propose to use genetic algorithms to find the optimal setting of all the diffusion parameters with respect to retrieval performance for each different dataset. Our approach is faster than others used as references (brute-force, random-search and PSO). A comparison with these methods has been made on three public image datasets: Oxford5k, Paris6k and Oxford105k.



### Multi-Garment Net: Learning to Dress 3D People from Images
- **Arxiv ID**: http://arxiv.org/abs/1908.06903v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06903v2)
- **Published**: 2019-08-19 16:05:26+00:00
- **Updated**: 2019-08-20 07:31:46+00:00
- **Authors**: Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, Gerard Pons-Moll
- **Comment**: International Conference in Computer Vision (ICCV), 2019
- **Journal**: None
- **Summary**: We present Multi-Garment Network (MGN), a method to predict body shape and clothing, layered on top of the SMPL model from a few frames (1-8) of a video. Several experiments demonstrate that this representation allows higher level of control when compared to single mesh or voxel representations of shape. Our model allows to predict garment geometry, relate it to the body shape, and transfer it to new body shapes and poses. To train MGN, we leverage a digital wardrobe containing 712 digital garments in correspondence, obtained with a novel method to register a set of clothing templates to a dataset of real 3D scans of people in different clothing and poses. Garments from the digital wardrobe, or predicted by MGN, can be used to dress any body shape in arbitrary poses. We will make publicly available the digital wardrobe, the MGN model, and code to dress SMPL with the garments.



### Algorithm Selection for Image Quality Assessment
- **Arxiv ID**: http://arxiv.org/abs/1908.06911v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/1908.06911v1)
- **Published**: 2019-08-19 16:15:22+00:00
- **Updated**: 2019-08-19 16:15:22+00:00
- **Authors**: Markus Wagner, Hanhe Lin, Shujun Li, Dietmar Saupe
- **Comment**: Presented at the Seventh Workshop on COnfiguration and SElection of
  ALgorithms (COSEAL), Potsdam, Germany, August 26--27, 2019
- **Journal**: None
- **Summary**: Subjective perceptual image quality can be assessed in lab studies by human observers. Objective image quality assessment (IQA) refers to algorithms for estimation of the mean subjective quality ratings. Many such methods have been proposed, both for blind IQA in which no original reference image is available as well as for the full-reference case. We compared 8 state-of-the-art algorithms for blind IQA and showed that an oracle, able to predict the best performing method for any given input image, yields a hybrid method that could outperform even the best single existing method by a large margin. In this contribution we address the research question whether established methods to learn such an oracle can improve blind IQA. We applied AutoFolio, a state-of-the-art system that trains an algorithm selector to choose a well-performing algorithm for a given instance. We also trained deep neural networks to predict the best method. Our results did not give a positive answer, algorithm selection did not yield a significant improvement over the single best method. Looking into the results in depth, we observed that the noise in images may have played a role in why our trained classifiers could not predict the oracle. This motivates the consideration of noisiness in IQA methods, a property that has so far not been observed and that opens up several interesting new research questions and applications.



### Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.06912v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06912v1)
- **Published**: 2019-08-19 16:20:39+00:00
- **Updated**: 2019-08-19 16:20:39+00:00
- **Authors**: Zongwei Zhou, Vatsal Sodha, Md Mahfuzur Rahman Siddiquee, Ruibin Feng, Nima Tajbakhsh, Michael B. Gotway, Jianming Liang
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI)
- **Journal**: None
- **Summary**: Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.



### A Blind Multiscale Spatial Regularization Framework for Kernel-based Spectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1908.06925v3
- **DOI**: 10.1109/LGRS.2018.2878394
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.06925v3)
- **Published**: 2019-08-19 16:52:14+00:00
- **Updated**: 2020-03-02 19:31:46+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard
- **Comment**: None
- **Journal**: None
- **Summary**: Introducing spatial prior information in hyperspectral imaging (HSI) analysis has led to an overall improvement of the performance of many HSI methods applied for denoising, classification, and unmixing. Extending such methodologies to nonlinear settings is not always straightforward, specially for unmixing problems where the consideration of spatial relationships between neighboring pixels might comprise intricate interactions between their fractional abundances and nonlinear contributions. In this paper, we consider a multiscale regularization strategy for nonlinear spectral unmixing with kernels. The proposed methodology splits the unmixing problem into two sub-problems at two different spatial scales: a coarse scale containing low-dimensional structures, and the original fine scale. The coarse spatial domain is defined using superpixels that result from a multiscale transformation. Spectral unmixing is then formulated as the solution of quadratically constrained optimization problems, which are solved efficiently by exploring their strong duality and a reformulation of their dual cost functions in the form of root-finding problems. Furthermore, we employ a theory-based statistical framework to devise a consistent strategy to estimate all required parameters, including both the regularization parameters of the algorithm and the number of superpixels of the transformation, resulting in a truly blind (from the parameters setting perspective) unmixing method. Experimental results attest the superior performance of the proposed method when comparing with other, state-of-the-art, related strategies.



### Deep Active Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.06933v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06933v4)
- **Published**: 2019-08-19 17:12:00+00:00
- **Updated**: 2020-08-30 16:31:13+00:00
- **Authors**: Ali Hatamizadeh, Assaf Hoogi, Debleena Sengupta, Wuyue Lu, Brian Wilcox, Daniel Rubin, Demetri Terzopoulos
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2019). Link to
  source code added
- **Journal**: MLMI 2019
- **Summary**: Lesion segmentation is an important problem in computer-assisted diagnosis that remains challenging due to the prevalence of low contrast, irregular boundaries that are unamenable to shape priors. We introduce Deep Active Lesion Segmentation (DALS), a fully automated segmentation framework for that leverages the powerful nonlinear feature extraction abilities of fully Convolutional Neural Networks (CNNs) and the precise boundary delineation abilities of Active Contour Models (ACMs). Our DALS framework benefits from an improved level-set ACM formulation with a per-pixel-parameterized energy functional and a novel multiscale encoder-decoder CNN that learns an initialization probability map along with parameter maps for the ACM. We evaluate our lesion segmentation model on a new Multiorgan Lesion Segmentation (MLS) dataset that contains images of various organs, including brain, liver, and lung, across different imaging modalities---MR and CT. Our results demonstrate favorable performance compared to competing methods, especially for small training datasets. Source code : $\text{https://github.com/ahatamiz/dals}$



### Representing text as abstract images enables image classifiers to also simultaneously classify text
- **Arxiv ID**: http://arxiv.org/abs/1908.07846v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07846v3)
- **Published**: 2019-08-19 17:28:29+00:00
- **Updated**: 2020-02-06 07:28:03+00:00
- **Authors**: Stephen M. Petrie, T'Mir D. Julius
- **Comment**: Minor changes in order to submit paper to a different conference
  (e.g. made minor changes to writing in several places and added extra data to
  Table 3 in order to make it clearer)
- **Journal**: None
- **Summary**: We introduce a novel method for converting text data into abstract image representations, which allows image-based processing techniques (e.g. image classification networks) to be applied to text-based comparison problems. We apply the technique to entity disambiguation of inventor names in US patents. The method involves converting text from each pairwise comparison between two inventor name records into a 2D RGB (stacked) image representation. We then train an image classification neural network to discriminate between such pairwise comparison images, and use the trained network to label each pair of records as either matched (same inventor) or non-matched (different inventors), obtaining highly accurate results. Our new text-to-image representation method could also be used more broadly for other NLP comparison problems, such as disambiguation of academic publications, or for problems that require simultaneous classification of both text and image datasets.



### Attention on Attention for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.06954v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06954v2)
- **Published**: 2019-08-19 17:44:14+00:00
- **Updated**: 2019-08-21 17:58:38+00:00
- **Authors**: Lun Huang, Wenmin Wang, Jie Chen, Xiao-Yong Wei
- **Comment**: Accepted to ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.



### Dynamic Graph Message Passing Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.06955v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.06955v5)
- **Published**: 2019-08-19 17:46:34+00:00
- **Updated**: 2022-09-15 03:42:47+00:00
- **Authors**: Li Zhang, Dan Xu, Anurag Arnab, Philip H. S. Torr
- **Comment**: CVPR 2020 Oral
- **Journal**: None
- **Summary**: Modelling long-range dependencies is critical for scene understanding tasks in computer vision. Although CNNs have excelled in many vision tasks, they are still limited in capturing long-range structured relationships as they typically consist of layers of local kernels. A fully-connected graph is beneficial for such modelling, however, its computational overhead is prohibitive. We propose a dynamic graph message passing network, that significantly reduces the computational complexity compared to related works modelling a fully-connected graph. This is achieved by adaptively sampling nodes in the graph, conditioned on the input, for message passing. Based on the sampled nodes, we dynamically predict node-dependent filter weights and the affinity matrix for propagating information between them. Using this model, we show significant improvements with respect to strong, state-of-the-art baselines on three different tasks and backbone architectures. Our approach also outperforms fully-connected graphs while using substantially fewer floating-point operations and parameters. The project website is http://www.robots.ox.ac.uk/~lz/dgmn/



### Joint Embedding of 3D Scan and CAD Objects
- **Arxiv ID**: http://arxiv.org/abs/1908.06989v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06989v1)
- **Published**: 2019-08-19 18:00:03+00:00
- **Updated**: 2019-08-19 18:00:03+00:00
- **Authors**: Manuel Dahnert, Angela Dai, Leonidas Guibas, Matthias Nießner
- **Comment**: None
- **Journal**: None
- **Summary**: 3D scan geometry and CAD models often contain complementary information towards understanding environments, which could be leveraged through establishing a mapping between the two domains. However, this is a challenging task due to strong, lower-level differences between scan and CAD geometry. We propose a novel approach to learn a joint embedding space between scan and CAD geometry, where semantically similar objects from both domains lie close together. To achieve this, we introduce a new 3D CNN-based approach to learn a joint embedding space representing object similarities across these domains. To learn a shared space where scan objects and CAD models can interlace, we propose a stacked hourglass approach to separate foreground and background from a scan object, and transform it to a complete, CAD-like representation to produce a shared embedding space. This embedding space can then be used for CAD model retrieval; to further enable this task, we introduce a new dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. Our learned joint embedding outperforms current state of the art for CAD model retrieval by 12% in instance retrieval accuracy.



### Boundless: Generative Adversarial Networks for Image Extension
- **Arxiv ID**: http://arxiv.org/abs/1908.07007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07007v1)
- **Published**: 2019-08-19 18:15:35+00:00
- **Updated**: 2019-08-19 18:15:35+00:00
- **Authors**: Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, William T. Freeman
- **Comment**: None
- **Journal**: None
- **Summary**: Image extension models have broad applications in image editing, computational photography and computer graphics. While image inpainting has been extensively studied in the literature, it is challenging to directly apply the state-of-the-art inpainting methods to image extension as they tend to generate blurry or repetitive pixels with inconsistent semantics. We introduce semantic conditioning to the discriminator of a generative adversarial network (GAN), and achieve strong results on image extension with coherent semantics and visually pleasing colors and textures. We also show promising results in extreme extensions, such as panorama generation.



### DirectPET: Full Size Neural Network PET Reconstruction from Sinogram Data
- **Arxiv ID**: http://arxiv.org/abs/1908.07516v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1908.07516v4)
- **Published**: 2019-08-19 19:03:13+00:00
- **Updated**: 2020-02-11 20:22:53+00:00
- **Authors**: William Whiteley, Wing K. Luk, Jens Gregor
- **Comment**: Submitted to the Journal of Medical Imaging
- **Journal**: None
- **Summary**: Purpose: Neural network image reconstruction directly from measurement data is a relatively new field of research, that until now has been limited to producing small single-slice images (e.g., 1x128x128). This paper proposes a novel and more efficient network design for Positron Emission Tomography called DirectPET which is capable of reconstructing multi-slice image volumes (i.e., 16x400x400) from sinograms.   Approach: Large-scale direct neural network reconstruction is accomplished by addressing the associated memory space challenge through the introduction of a specially designed Radon inversion layer. Using patient data, we compare the proposed method to the benchmark Ordered Subsets Expectation Maximization (OSEM) algorithm using signal-to-noise ratio, bias, mean absolute error and structural similarity measures. In addition, line profiles and full-width half-maximum measurements are provided for a sample of lesions.   Results: DirectPET is shown capable of producing images that are quantitatively and qualitatively similar to the OSEM target images in a fraction of the time. We also report on an experiment where DirectPET is trained to map low count raw data to normal count target images demonstrating the method's ability to maintain image quality under a low dose scenario.   Conclusion: The ability of DirectPET to quickly reconstruct high-quality, multi-slice image volumes suggests potential clinical viability of the method. However, design parameters and performance boundaries need to be fully established before adoption can be considered.



### UprightNet: Geometry-Aware Camera Orientation Estimation from Single Images
- **Arxiv ID**: http://arxiv.org/abs/1908.07070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07070v1)
- **Published**: 2019-08-19 21:07:16+00:00
- **Updated**: 2019-08-19 21:07:16+00:00
- **Authors**: Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisenmann, Eli Shechtman, Noah Snavely
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce UprightNet, a learning-based approach for estimating 2DoF camera orientation from a single RGB image of an indoor scene. Unlike recent methods that leverage deep learning to perform black-box regression from image to orientation parameters, we propose an end-to-end framework that incorporates explicit geometric reasoning. In particular, we design a network that predicts two representations of scene geometry, in both the local camera and global reference coordinate systems, and solves for the camera orientation as the rotation that best aligns these two predictions via a differentiable least squares module. This network can be trained end-to-end, and can be supervised with both ground truth camera poses and intermediate representations of surface geometry. We evaluate UprightNet on the single-image camera orientation task on synthetic and real datasets, and show significant improvements over prior state-of-the-art approaches.



### BoxNet: A Deep Learning Method for 2D Bounding Box Estimation from Bird's-Eye View Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/1908.07085v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07085v1)
- **Published**: 2019-08-19 21:50:41+00:00
- **Updated**: 2019-08-19 21:50:41+00:00
- **Authors**: Ehsan Nezhadarya, Yang Liu, Bingbing Liu
- **Comment**: None
- **Journal**: None
- **Summary**: We present a learning-based method to estimate the object bounding box from its 2D bird's-eye view (BEV) LiDAR points. Our method, entitled BoxNet, exploits a simple deep neural network that can efficiently handle unordered points. The method takes as input the 2D coordinates of all the points and the output is a vector consisting of both the box pose (position and orientation in LiDAR coordinate system) and its size (width and length). In order to deal with the angle discontinuity problem, we propose to estimate the double-angle sinusoidal values rather than the angle itself. We also predict the center relative to the point cloud mean to boost the performance of estimating the location of the box. The proposed method does not rely on the ordering of points as in many existing approaches, and can accurately predict the actual size of the bounding box based on the prior information that is obtained from the training data. BoxNet is validated using the KITTI 3D object dataset, with significant improvement compared with the state-of-the-art non-learning based methods



### Human uncertainty makes classification more robust
- **Arxiv ID**: http://arxiv.org/abs/1908.07086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07086v1)
- **Published**: 2019-08-19 22:08:22+00:00
- **Updated**: 2019-08-19 22:08:22+00:00
- **Authors**: Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, Olga Russakovsky
- **Comment**: In Proceedings of the 2019 IEEE International Conference on Computer
  Vision (ICCV)
- **Journal**: None
- **Summary**: The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.



### Unpaired Image-to-Speech Synthesis with Multimodal Information Bottleneck
- **Arxiv ID**: http://arxiv.org/abs/1908.07094v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07094v1)
- **Published**: 2019-08-19 22:52:59+00:00
- **Updated**: 2019-08-19 22:52:59+00:00
- **Authors**: Shuang Ma, Daniel McDuff, Yale Song
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem "skip-modal generation" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.



### Autonomous, Monocular, Vision-Based Snake Robot Navigation and Traversal of Cluttered Environments using Rectilinear Gait Motion
- **Arxiv ID**: http://arxiv.org/abs/1908.07101v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07101v1)
- **Published**: 2019-08-19 23:06:42+00:00
- **Updated**: 2019-08-19 23:06:42+00:00
- **Authors**: Alexander H. Chang, Shiyu Feng, Yipu Zhao, Justin S. Smith, Patricio A. Vela
- **Comment**: None
- **Journal**: None
- **Summary**: Rectilinear forms of snake-like robotic locomotion are anticipated to be an advantage in obstacle-strewn scenarios characterizing urban disaster zones, subterranean collapses, and other natural environments. The elongated, laterally-narrow footprint associated with these motion strategies is well-suited to traversal of confined spaces and narrow pathways. Navigation and path planning in the absence of global sensing, however, remains a pivotal challenge to be addressed prior to practical deployment of these robotic mechanisms. Several challenges related to visual processing and localization need to be resolved to to enable navigation. As a first pass in this direction, we equip a wireless, monocular color camera to the head of a robotic snake. Visiual odometry and mapping from ORB-SLAM permits self-localization in planar, obstacle-strewn environments. Ground plane traversability segmentation in conjunction with perception-space collision detection permits path planning for navigation. A previously presented dynamical reduction of rectilinear snake locomotion to a non-holonomic kinematic vehicle informs both SLAM and planning. The simplified motion model is then applied to track planned trajectories through an obstacle configuration. This navigational framework enables a snake-like robotic platform to autonomously navigate and traverse unknown scenarios with only monocular vision.



