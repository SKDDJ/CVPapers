# Arxiv Papers in cs.CV on 2019-08-23
### Shadow Removal via Shadow Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1908.08628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08628v1)
- **Published**: 2019-08-23 00:51:32+00:00
- **Updated**: 2019-08-23 00:51:32+00:00
- **Authors**: Hieu Le, Dimitris Samaras
- **Comment**: ICCV 19 Poster. Higher resolution version is available in the project
  homepage: www3.cs.stonybrook.edu/~cvl/projects/SID/index.html
- **Journal**: None
- **Summary**: We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects on the images. We train and test our framework on the most challenging shadow removal dataset (ISTD). Compared to the state-of-the-art method, our model achieves a 40% error reduction in terms of root mean square error (RMSE) for the shadow area, reducing RMSE from 13.3 to 7.9. Moreover, we create an augmented ISTD dataset based on an image decomposition system by modifying the shadow parameters to generate new synthetic shadow images. Training our model on this new augmented ISTD dataset further lowers the RMSE on the shadow area to 7.4.



### Image based cellular contractile force evaluation with small-world network inspired CNN: SW-UNet
- **Arxiv ID**: http://arxiv.org/abs/1908.08631v1
- **DOI**: None
- **Categories**: **physics.bio-ph**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08631v1)
- **Published**: 2019-08-23 01:47:06+00:00
- **Updated**: 2019-08-23 01:47:06+00:00
- **Authors**: Li Honghan, Daiki Matsunaga, Tsubasa S. Matsui, Hiroki Aosaki, Shinji Deguchi
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an image-based cellular contractile force evaluation method using a machine learning technique. We use a special substrate that exhibits wrinkles when cells grab the substrate and contract, and the wrinkles can be used to visualize the force magnitude and direction. In order to extract wrinkles from the microscope images, we develop a new CNN (convolutional neural network) architecture SW-UNet (small-world U-Net), which is a CNN that reflects the concept of the small-world network. The SW-UNet shows better performance in wrinkle segmentation task compared to other methods: the error (Euclidean distance) of SW-UNet is 4.9 times smaller than 2D-FFT (fast Fourier transform) based segmentation approach, and is 2.9 times smaller than U-Net. As a demonstration, we compare the contractile force of U2OS (human osteosarcoma) cells and show that cells with a mutation in the KRAS oncogne show larger force compared to the wild-type cells. Our new machine learning based algorithm provides us an efficient, automated and accurate method to evaluate the cell contractile force.



### MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.08652v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08652v1)
- **Published**: 2019-08-23 03:30:53+00:00
- **Updated**: 2019-08-23 03:30:53+00:00
- **Authors**: Abhay Kumar, Nishant Jain, Suraj Tripathi, Chirag Singh, Kamal Krishna
- **Comment**: 5 pages, 3 figures, Accepted in IEEE AVSS 2019
- **Journal**: None
- **Summary**: We propose a Multi-Task Learning (MTL) paradigm based deep neural network architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and count estimation. Crowd count estimation is challenging due to the non-uniform scale variations and the arbitrary perspective of an individual image. The proposed model has two related tasks, with Crowd Density Estimation as the main task and Crowd-Count Group Classification as the auxiliary task. The auxiliary task helps in capturing the relevant scale-related information to improve the performance of the main task. The main task model comprises two blocks: VGG-16 front-end for feature extraction and a dilated Convolutional Neural Network for density map generation. The auxiliary task model shares the same front-end as the main task, followed by a CNN classifier. Our proposed network achieves 5.8% and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on ShanghaiTech dataset without using any data augmentation. Our model also outperforms with 10.5% lower MAE on UCF_CC_50 dataset.



### A BLSTM Network for Printed Bengali OCR System with High Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1908.08674v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08674v1)
- **Published**: 2019-08-23 05:45:27+00:00
- **Updated**: 2019-08-23 05:45:27+00:00
- **Authors**: Debabrata Paul, Bidyut Baran Chaudhuri
- **Comment**: 6 pages, 6 figures, This OCR system is available online at
  https://banglaocr.nltr.org
- **Journal**: None
- **Summary**: This paper presents a printed Bengali and English text OCR system developed by us using a single hidden BLSTM-CTC architecture having 128 units. Here, we did not use any peephole connection and dropout in the BLSTM, which helped us in getting better accuracy. This architecture was trained by 47,720 text lines that include English words also. When tested over 20 different Bengali fonts, it has produced character level accuracy of 99.32% and word level accuracy of 96.65%. A good Indic multi script OCR system is also developed by Google. It sometimes recognizes a character of Bengali into the same character of a non-Bengali script, especially Assamese, which has no distinction from Bengali, except for a few characters. For example, Bengali character for 'RA' is sometimes recognized as that of Assamese, mainly in conjunct consonant forms. Our OCR is free from such errors. This OCR system is available online at https://banglaocr.nltr.org



### Mish: A Self Regularized Non-Monotonic Activation Function
- **Arxiv ID**: http://arxiv.org/abs/1908.08681v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08681v3)
- **Published**: 2019-08-23 06:22:06+00:00
- **Updated**: 2020-08-13 05:42:12+00:00
- **Authors**: Diganta Misra
- **Comment**: Accepted to BMVC 2020
- **Journal**: None
- **Summary**: We propose $\textit{Mish}$, a novel self-regularized non-monotonic activation function which can be mathematically defined as: $f(x)=x\tanh(softplus(x))$. As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision ($AP_{50}^{val}$) by 2.1$\%$ in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by $\approx$1$\%$ while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks. Code is publicly available at https://github.com/digantamisra98/Mish.



### Crowd Counting with Deep Structured Scale Integration Network
- **Arxiv ID**: http://arxiv.org/abs/1908.08692v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08692v1)
- **Published**: 2019-08-23 06:59:36+00:00
- **Updated**: 2019-08-23 06:59:36+00:00
- **Authors**: Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang Lin
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. In this module, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. Specifically, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.



### ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths
- **Arxiv ID**: http://arxiv.org/abs/1909.04148v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04148v1)
- **Published**: 2019-08-23 07:03:48+00:00
- **Updated**: 2019-08-23 07:03:48+00:00
- **Authors**: Yanhao Zhu, Zhineng Chen, Shuai Zhao, Hongtao Xie, Wenming Guo, Yongdong Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays U-net-like FCNs predominate various biomedical image segmentation applications and attain promising performance, largely due to their elegant architectures, e.g., symmetric contracting and expansive paths as well as lateral skip-connections. It remains a research direction to devise novel architectures to further benefit the segmentation. In this paper, we develop an ACE-net that aims to enhance the feature representation and utilization by augmenting the contracting and expansive paths. In particular, we augment the paths by the recently proposed advanced techniques including ASPP, dense connection and deep supervision mechanisms, and novel connections such as directly connecting the raw image to the expansive side. With these augmentations, ACE-net can utilize features from multiple sources, scales and reception fields to segment while still maintains a relative simple architecture. Experiments on two typical biomedical segmentation tasks validate its effectiveness, where highly competitive results are obtained in both tasks while ACE-net still runs fast at inference.



### Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1908.08704v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.08704v1)
- **Published**: 2019-08-23 07:53:35+00:00
- **Updated**: 2019-08-23 07:53:35+00:00
- **Authors**: Shunkai Li, Fei Xue, Xin Wang, Zike Yan, Hongbin Zha
- **Comment**: Accept to ICCV 2019
- **Journal**: None
- **Summary**: We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.



### AdvHat: Real-world adversarial attack on ArcFace Face ID system
- **Arxiv ID**: http://arxiv.org/abs/1908.08705v1
- **DOI**: 10.1109/ICPR48806.2021.9412236
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08705v1)
- **Published**: 2019-08-23 07:55:42+00:00
- **Updated**: 2019-08-23 07:55:42+00:00
- **Authors**: Stepan Komkov, Aleksandr Petiushko
- **Comment**: None
- **Journal**: 2020 25th International Conference on Pattern Recognition (ICPR)
- **Summary**: In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.



### Onion-Peel Networks for Deep Video Completion
- **Arxiv ID**: http://arxiv.org/abs/1908.08718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08718v1)
- **Published**: 2019-08-23 08:51:41+00:00
- **Updated**: 2019-08-23 08:51:41+00:00
- **Authors**: Seoung Wug Oh, Sungho Lee, Joon-Young Lee, Seon Joo Kim
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We propose the onion-peel networks for video completion. Given a set of reference images and a target image with holes, our network fills the hole by referring the contents in the reference images. Our onion-peel network progressively fills the hole from the hole boundary enabling it to exploit richer contextual information for the missing regions every step. Given a sufficient number of recurrences, even a large hole can be inpainted successfully. To attend to the missing information visible in the reference images, we propose an asymmetric attention block that computes similarities between the hole boundary pixels in the target and the non-hole pixels in the references in a non-local manner. With our attention block, our network can have an unlimited spatial-temporal window size and fill the holes with globally coherent contents. In addition, our framework is applicable to the image completion guided by the reference images without any modification, which is difficult to do with the previous methods. We validate that our method produces visually pleasing image and video inpainting results in realistic test cases.



### Automatic Rodent Brain MRI Lesion Segmentation with Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.08746v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08746v1)
- **Published**: 2019-08-23 10:15:37+00:00
- **Updated**: 2019-08-23 10:15:37+00:00
- **Authors**: Juan Miguel Valverde, Artem Shatillo, Riccardo de Feo, Olli Gröhn, Alejandra Sierra, Jussi Tohka
- **Comment**: Accepted to Machine Learning in Medical Imaging (MLMI 2019)
- **Journal**: None
- **Summary**: Manual segmentation of rodent brain lesions from magnetic resonance images (MRIs) is an arduous, time-consuming and subjective task that is highly important in pre-clinical research. Several automatic methods have been developed for different human brain MRI segmentation, but little research has targeted automatic rodent lesion segmentation. The existing tools for performing automatic lesion segmentation in rodents are constrained by strict assumptions about the data. Deep learning has been successfully used for medical image segmentation. However, there has not been any deep learning approach specifically designed for tackling rodent brain lesion segmentation. In this work, we propose a novel Fully Convolutional Network (FCN), RatLesNet, for the aforementioned task. Our dataset consists of 131 T2-weighted rat brain scans from 4 different studies in which ischemic stroke was induced by transient middle cerebral artery occlusion. We compare our method with two other 3D FCNs originally developed for anatomical segmentation (VoxResNet and 3D-U-Net) with 5-fold cross-validation on a single study and a generalization test, where the training was done on a single study and testing on three remaining studies. The labels generated by our method were quantitatively and qualitatively better than the predictions of the compared methods. The average Dice coefficient achieved in the 5-fold cross-validation experiment with the proposed approach was 0.88, between 3.7% and 38% higher than the compared architectures. The presented architecture also outperformed the other FCNs at generalizing on different studies, achieving the average Dice coefficient of 0.79.



### Self-reinforcing Unsupervised Matching
- **Arxiv ID**: http://arxiv.org/abs/1909.04138v1
- **DOI**: 10.1109/TPAMI.2021.3061945
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04138v1)
- **Published**: 2019-08-23 10:43:43+00:00
- **Updated**: 2019-08-23 10:43:43+00:00
- **Authors**: Jiang Lu, Lei Li, Changshui Zhang
- **Comment**: 15 pages
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence,
  vol. 44, no. 8, AUGUST 2022, pp.4404-4418
- **Summary**: Remarkable gains in deep learning usually rely on tremendous supervised data. Ensuring the modality diversity for one object in training set is critical for the generalization of cutting-edge deep models, but it burdens human with heavy manual labor on data collection and annotation. In addition, some rare or unexpected modalities are new for the current model, causing reduced performance under such emerging modalities. Inspired by the achievements in speech recognition, psychology and behavioristics, we present a practical solution, self-reinforcing unsupervised matching (SUM), to annotate the images with 2D structure-preserving property in an emerging modality by cross-modality matching. This approach requires no any supervision in emerging modality and only one template in seen modality, providing a possible route towards continual learning.



### KLDivNet: An unsupervised neural network for multi-modality image registration
- **Arxiv ID**: http://arxiv.org/abs/1908.08767v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08767v2)
- **Published**: 2019-08-23 11:48:50+00:00
- **Updated**: 2020-02-05 06:27:45+00:00
- **Authors**: Yechong Huang, Tao Song, Jiahang Xu, Yinan Chen, Xiahai Zhuang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modality image registration is one of the most underlined processes in medical image analysis. Recently, convolutional neural networks (CNNs) have shown significant potential in deformable registration. However, the lack of voxel-wise ground truth challenges the training of CNNs for an accurate registration. In this work, we propose a cross-modality similarity metric, based on the KL-divergence of image variables, and implement an efficient estimation method using a CNN. This estimation network, referred to as KLDivNet, can be trained unsupervisedly. We then embed the KLDivNet into a registration network to achieve the unsupervised deformable registration for multi-modality images. We employed three datasets, i.e., AAL Brain, LiTS Liver and Hospital Liver, with both the intra- and inter-modality image registration tasks for validation. Results showed that our similarity metric was effective, and the proposed registration network delivered superior performance compared to the state-of-the-art methods.



### Multi-Spectral Visual Odometry without Explicit Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1908.08814v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08814v1)
- **Published**: 2019-08-23 13:19:05+00:00
- **Updated**: 2019-08-23 13:19:05+00:00
- **Authors**: Weichen Dai, Yu Zhang, Donglei Sun, Naira Hovakimyan, Ping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-spectral sensors consisting of a standard (visible-light) camera and a long-wave infrared camera can simultaneously provide both visible and thermal images. Since thermal images are independent from environmental illumination, they can help to overcome certain limitations of standard cameras under complicated illumination conditions. However, due to the difference in the information source of the two types of cameras, their images usually share very low texture similarity. Hence, traditional texture-based feature matching methods cannot be directly applied to obtain stereo correspondences. To tackle this problem, a multi-spectral visual odometry method without explicit stereo matching is proposed in this paper. Bundle adjustment of multi-view stereo is performed on the visible and the thermal images using direct image alignment. Scale drift can be avoided by additional temporal observations of map points with the fixed-baseline stereo. Experimental results indicate that the proposed method can provide accurate visual odometry results with recovered metric scale. Moreover, the proposed method can also provide a metric 3D reconstruction in semi-dense density with multi-spectral information, which is not available from existing multi-spectral methods.



### Spooky effect in optimal OSPA estimation and how GOSPA solves it
- **Arxiv ID**: http://arxiv.org/abs/1908.08815v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08815v1)
- **Published**: 2019-08-23 13:21:42+00:00
- **Updated**: 2019-08-23 13:21:42+00:00
- **Authors**: Ángel F. García-Fernández, Lennart Svensson
- **Comment**: This paper received the third best paper award at the 22nd
  International Conference on Information Fusion, Ottawa, Canada, 2019. Matlab
  code of the GOSPA metric can be found in https://github.com/abusajana/GOSPA .
  Additional information on MTT can be found in the online course
  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw
- **Journal**: Proceedings of the 22nd International Conference on Information
  Fusion, 2019
- **Summary**: In this paper, we show the spooky effect at a distance that arises in optimal estimation of multiple targets with the optimal sub-pattern assignment (OSPA) metric. This effect refers to the fact that if we have several independent potential targets at distant locations, a change in the probability of existence of one of them can completely change the optimal estimation of the rest of the potential targets. As opposed to OSPA, the generalised OSPA (GOSPA) metric ($\alpha=2$) penalises localisation errors for properly detected targets, false targets and missed targets. As a consequence, optimal GOSPA estimation aims to lower the number of false and missed targets, as well as the localisation error for properly detected targets, and avoids the spooky effect.



### Gaussian implementation of the multi-Bernoulli mixture filter
- **Arxiv ID**: http://arxiv.org/abs/1908.08819v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1908.08819v1)
- **Published**: 2019-08-23 13:31:49+00:00
- **Updated**: 2019-08-23 13:31:49+00:00
- **Authors**: Ángel F. García-Fernández, Yuxuan Xia, Karl Granström, Lennart Svensson, Jason L. Williams
- **Comment**: Matlab code of the MBM and PMBM filters is provided in
  https://github.com/Agarciafernandez/MTT . Additional information on MTT
  including PMBM and MBM filters can be found in the online course
  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw
- **Journal**: Proceedings of the 22nd International Conference on Information
  Fusion, 2019
- **Summary**: This paper presents the Gaussian implementation of the multi-Bernoulli mixture (MBM) filter. The MBM filter provides the filtering (multi-target) density for the standard dynamic and radar measurement models when the birth model is multi-Bernoulli or multi-Bernoulli mixture. Under linear/Gaussian models, the single target densities of the MBM mixture admit Gaussian closed-form expressions. Murty's algorithm is used to select the global hypotheses with highest weights. The MBM filter is compared with other algorithms in the literature via numerical simulations.



### DRFN: Deep Recurrent Fusion Network for Single-Image Super-Resolution with Large Factors
- **Arxiv ID**: http://arxiv.org/abs/1908.08837v1
- **DOI**: 10.1109/TMM.2018.2863602
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08837v1)
- **Published**: 2019-08-23 14:13:21+00:00
- **Updated**: 2019-08-23 14:13:21+00:00
- **Authors**: Xin Yang, Haiyang Mei, Jiqing Zhang, Ke Xu, Baocai Yin, Qiang Zhang, Xiaopeng Wei
- **Comment**: None
- **Journal**: IEEE Transactions on Multimedia ( Volume: 21 , Issue: 2 , Feb.
  2019 ) 328 - 337
- **Summary**: Recently, single-image super-resolution has made great progress owing to the development of deep convolutional neural networks (CNNs). The vast majority of CNN-based models use a pre-defined upsampling operator, such as bicubic interpolation, to upscale input low-resolution images to the desired size and learn non-linear mapping between the interpolated image and ground truth high-resolution (HR) image. However, interpolation processing can lead to visual artifacts as details are over-smoothed, particularly when the super-resolution factor is high. In this paper, we propose a Deep Recurrent Fusion Network (DRFN), which utilizes transposed convolution instead of bicubic interpolation for upsampling and integrates different-level features extracted from recurrent residual blocks to reconstruct the final HR images. We adopt a deep recurrence learning strategy and thus have a larger receptive field, which is conducive to reconstructing an image more accurately. Furthermore, we show that the multi-level fusion structure is suitable for dealing with image super-resolution problems. Extensive benchmark evaluations demonstrate that the proposed DRFN performs better than most current deep learning methods in terms of accuracy and visual effects, especially for large-scale images, while using fewer parameters.



### Feature Learning to Automatically Assess Radiographic Knee Osteoarthritis Severity
- **Arxiv ID**: http://arxiv.org/abs/1908.08840v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08840v1)
- **Published**: 2019-08-23 14:23:35+00:00
- **Updated**: 2019-08-23 14:23:35+00:00
- **Authors**: Joseph Antony, Kevin McGuinness, Kieran Moran, Noel E O' Connor
- **Comment**: Book Chapter preprint :: Deep Learners and Deep Learner Descriptors
  for Medical Applications
- **Journal**: None
- **Summary**: This chapter presents the investigations and the results of feature learning using convolutional neural networks to automatically assess knee osteoarthritis (OA) severity and the associated clinical and diagnostic features of knee OA from X-ray images. Also, this chapter demonstrates that feature learning in a supervised manner is more effective than using conventional handcrafted features for automatic detection of knee joints and fine-grained knee OA image classification. In the general machine learning approach to automatically assess knee OA severity, the first step is to localize the region of interest that is to detect and extract the knee joint regions from the radiographs, and the next step is to classify the localized knee joints based on a radiographic classification scheme such as Kellgren and Lawrence grades. First, the existing approaches for detecting (or localizing) the knee joint regions based on handcrafted features are reviewed and outlined. Next, three new approaches are introduced: 1) to automatically detect the knee joint region using a fully convolutional network, 2) to automatically assess the radiographic knee OA using CNNs trained from scratch for classification and regression of knee joint images to predict KL grades in ordinal and continuous scales, and 3) to quantify the knee OA severity optimizing a weighted ratio of two loss functions: categorical cross entropy and mean-squared error using multi-objective convolutional learning and ordinal regression. Two public datasets: the OAI and the MOST are used to evaluate the approaches with promising results that outperform existing approaches. In summary, this work primarily contributes to the field of automated methods for localization (automatic detection) and quantification (image classification) of radiographic knee OA.



### Cephalometric Landmark Detection by AttentiveFeature Pyramid Fusion and Regression-Voting
- **Arxiv ID**: http://arxiv.org/abs/1908.08841v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08841v1)
- **Published**: 2019-08-23 14:34:44+00:00
- **Updated**: 2019-08-23 14:34:44+00:00
- **Authors**: Runnan Chen, Yuexin Ma, Nenglun Chen, Daniel Lee, Wenping Wang
- **Comment**: Early accepted by International Conference on Medical image computing
  and computer-assisted intervention (MICCAI 2019)
- **Journal**: None
- **Summary**: Marking anatomical landmarks in cephalometric radiography is a critical operation in cephalometric analysis. Automatically and accurately locating these landmarks is a challenging issue because different landmarks require different levels of resolution and semantics. Based on this observation, we propose a novel attentive feature pyramid fusion module (AFPF) to explicitly shape high-resolution and semantically enhanced fusion features to achieve significantly higher accuracy than existing deep learning-based methods. We also combine heat maps and offset maps to perform pixel-wise regression-voting to improve detection accuracy. By incorporating the AFPF and regression-voting, we develop an end-to-end deep learning framework that improves detection accuracy by 7%~11% for all the evaluation metrics over the state-of-the-art method. We present ablation studies to give more insights into different components of our method and demonstrate its generalization capability and stability for unseen data from diverse devices.



### Generating High-Resolution Fashion Model Images Wearing Custom Outfits
- **Arxiv ID**: http://arxiv.org/abs/1908.08847v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08847v1)
- **Published**: 2019-08-23 14:46:41+00:00
- **Updated**: 2019-08-23 14:46:41+00:00
- **Authors**: Gökhan Yildirim, Nikolay Jetchev, Roland Vollgraf, Urs Bergmann
- **Comment**: Accepted to the International Conference on Computer Vision, ICCV
  2019, Workshop on Computer Vision for Fashion, Art and Design
- **Journal**: None
- **Summary**: Visualizing an outfit is an essential part of shopping for clothes. Due to the combinatorial aspect of combining fashion articles, the available images are limited to a pre-determined set of outfits. In this paper, we broaden these visualizations by generating high-resolution images of fashion models wearing a custom outfit under an input body pose. We show that our approach can not only transfer the style and the pose of one generated outfit to another, but also create realistic images of human bodies and garments.



### Linking Points With Labels in 3D: A Review of Point Cloud Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.08854v3
- **DOI**: 10.1109/MGRS.2019.2937630
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08854v3)
- **Published**: 2019-08-23 14:55:11+00:00
- **Updated**: 2020-06-27 00:24:30+00:00
- **Authors**: Yuxing Xie, Jiaojiao Tian, Xiao Xiang Zhu
- **Comment**: The title of published version was modified to "Linking Points With
  Labels in 3D: A Review of Point Cloud Semantic Segmentation". To read its
  final version please go to IEEE Geoscience and Remote Sensing Magazine on
  IEEE XPlore: https://ieeexplore.ieee.org/document/9028090
- **Journal**: None
- **Summary**: 3D Point Cloud Semantic Segmentation (PCSS) is attracting increasing interest, due to its applicability in remote sensing, computer vision and robotics, and due to the new possibilities offered by deep learning techniques. In order to provide a needed up-to-date review of recent developments in PCSS, this article summarizes existing studies on this topic. Firstly, we outline the acquisition and evolution of the 3D point cloud from the perspective of remote sensing and computer vision, as well as the published benchmarks for PCSS studies. Then, traditional and advanced techniques used for Point Cloud Segmentation (PCS) and PCSS are reviewed and compared. Finally, important issues and open questions in PCSS studies are discussed.



### Assessing Knee OA Severity with CNN attention-based end-to-end architectures
- **Arxiv ID**: http://arxiv.org/abs/1908.08856v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08856v1)
- **Published**: 2019-08-23 14:59:52+00:00
- **Updated**: 2019-08-23 14:59:52+00:00
- **Authors**: Marc Górriz, Joseph Antony, Kevin McGuinness, Xavier Giró-i-Nieto, Noel E. O'Connor
- **Comment**: Proceedings of the 2nd International Conference on Medical Imaging
  with Deep Learning
- **Journal**: Proceedings of The 2nd International Conference on Medical Imaging
  with Deep Learning, PMLR 102:197-214, 2019
- **Summary**: This work proposes a novel end-to-end convolutional neural network (CNN) architecture to automatically quantify the severity of knee osteoarthritis (OA) using X-Ray images, which incorporates trainable attention modules acting as unsupervised fine-grained detectors of the region of interest (ROI). The proposed attention modules can be applied at different levels and scales across any CNN pipeline helping the network to learn relevant attention patterns over the most informative parts of the image at different resolutions. We test the proposed attention mechanism on existing state-of-the-art CNN architectures as our base models, achieving promising results on the benchmark knee OA datasets from the osteoarthritis initiative (OAI) and multicenter osteoarthritis study (MOST). All code from our experiments will be publicly available on the github repository: https://github.com/marc-gorriz/KneeOA-CNNAttention



### Topology-preserving augmentation for CNN-based segmentation of congenital heart defects from 3D paediatric CMR
- **Arxiv ID**: http://arxiv.org/abs/1908.08870v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08870v1)
- **Published**: 2019-08-23 15:34:09+00:00
- **Updated**: 2019-08-23 15:34:09+00:00
- **Authors**: Nick Byrne, James R. Clough, Isra Valverde, Giovanni Montana, Andrew P. King
- **Comment**: To be published at MICCAI PIPPI 2019
- **Journal**: None
- **Summary**: Patient-specific 3D printing of congenital heart anatomy demands an accurate segmentation of the thin tissue interfaces which characterise these diagnoses. Even when a label set has a high spatial overlap with the ground truth, inaccurate delineation of these interfaces can result in topological errors. These compromise the clinical utility of such models due to the anomalous appearance of defects. CNNs have achieved state-of-the-art performance in segmentation tasks. Whilst data augmentation has often played an important role, we show that conventional image resampling schemes used therein can introduce topological changes in the ground truth labelling of augmented samples. We present a novel pipeline to correct for these changes, using a fast-marching algorithm to enforce the topology of the ground truth labels within their augmented representations. In so doing, we invoke the idea of cardiac contiguous topology to describe an arbitrary combination of congenital heart defects and develop an associated, clinically meaningful metric to measure the topological correctness of segmentations. In a series of five-fold cross-validations, we demonstrate the performance gain produced by this pipeline and the relevance of topological considerations to the segmentation of congenital heart defects. We speculate as to the applicability of this approach to any segmentation task involving morphologically complex targets.



### Predicting knee osteoarthritis severity: comparative modeling based on patient's data and plain X-ray images
- **Arxiv ID**: http://arxiv.org/abs/1908.08873v1
- **DOI**: 10.1038/s41598-019-42215-9
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08873v1)
- **Published**: 2019-08-23 15:38:59+00:00
- **Updated**: 2019-08-23 15:38:59+00:00
- **Authors**: Jaynal Abedin, Joseph Antony, Kevin McGuinness, Kieran Moran, Noel E O'Connor, Dietrich Rebholz-Schuhmann, John Newell
- **Comment**: Published in Nature Scientific Reports, 2019
- **Journal**: Scientific reports 9, no. 1 (2019): 5761
- **Summary**: Knee osteoarthritis (KOA) is a disease that impairs knee function and causes pain. A radiologist reviews knee X-ray images and grades the severity level of the impairments according to the Kellgren and Lawrence grading scheme; a five-point ordinal scale (0--4). In this study, we used Elastic Net (EN) and Random Forests (RF) to build predictive models using patient assessment data (i.e. signs and symptoms of both knees and medication use) and a convolution neural network (CNN) trained using X-ray images only. Linear mixed effect models (LMM) were used to model the within subject correlation between the two knees. The root mean squared error for the CNN, EN, and RF models was 0.77, 0.97, and 0.94 respectively. The LMM shows similar overall prediction accuracy as the EN regression but correctly accounted for the hierarchical structure of the data resulting in more reliable inference. Useful explanatory variables were identified that could be used for patient monitoring before X-ray imaging. Our analyses suggest that the models trained for predicting the KOA severity levels achieve comparable results when modeling X-ray images and patient data. The subjectivity in the KL grade is still a primary concern.



### Trajectory Prediction by Coupling Scene-LSTM with Human Movement LSTM
- **Arxiv ID**: http://arxiv.org/abs/1908.08908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08908v1)
- **Published**: 2019-08-23 17:31:59+00:00
- **Updated**: 2019-08-23 17:31:59+00:00
- **Authors**: Manh Huynh, Gita Alaghband
- **Comment**: To appear in ISVC 2019
- **Journal**: None
- **Summary**: We develop a novel human trajectory prediction system that incorporates the scene information (Scene-LSTM) as well as individual pedestrian movement (Pedestrian-LSTM) trained simultaneously within static crowded scenes. We superimpose a two-level grid structure (grid cells and subgrids) on the scene to encode spatial granularity plus common human movements. The Scene-LSTM captures the commonly traveled paths that can be used to significantly influence the accuracy of human trajectory prediction in local areas (i.e. grid cells). We further design scene data filters, consisting of a hard filter and a soft filter, to select the relevant scene information in a local region when necessary and combine it with Pedestrian-LSTM for forecasting a pedestrian's future locations. The experimental results on several publicly available datasets demonstrate that our method outperforms related works and can produce more accurate predicted trajectories in different scene contexts.



### Region Tracking in an Image Sequence: Preventing Driver Inattention
- **Arxiv ID**: http://arxiv.org/abs/1908.08914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08914v1)
- **Published**: 2019-08-23 17:38:27+00:00
- **Updated**: 2019-08-23 17:38:27+00:00
- **Authors**: Matthew Kowal, Gillian Sandison, Len Yabuki-Soh, Raner la Bastide
- **Comment**: None
- **Journal**: None
- **Summary**: Driver inattention is a large problem on the roads around the world. The objective of this project was to develop an eye tracking algorithm with sufficient computational efficiency and accuracy, to successfully realize when the driver was looking away from the road for an extended period. The method of tracking involved the minimization of a functional, using the gradient descent and level set methods. The algorithm was then discretized and implemented using C and MATLAB. Multiple synthetic images, grey-scale and colour images were tested using the final design, with a desired region coverage of 82%. Further work is needed to decrease the computation time, increase the robustness of the algorithm, develop a small device capable of running the algorithm, as well as physically implement this device into various vehicles.



### Learning Filter Basis for Convolutional Neural Network Compression
- **Arxiv ID**: http://arxiv.org/abs/1908.08932v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08932v2)
- **Published**: 2019-08-23 17:55:26+00:00
- **Updated**: 2019-12-23 18:55:28+00:00
- **Authors**: Yawei Li, Shuhang Gu, Luc Van Gool, Radu Timofte
- **Comment**: Code is available at
  https://github.com/ofsoundof/learning_filter_basis. ICCV 2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) based solutions have achieved state-of-the-art performances for many computer vision tasks, including classification and super-resolution of images. Usually the success of these methods comes with a cost of millions of parameters due to stacking deep convolutional layers. Moreover, quite a large number of filters are also used for a single convolutional layer, which exaggerates the parameter burden of current methods. Thus, in this paper, we try to reduce the number of parameters of CNNs by learning a basis of the filters in convolutional layers. For the forward pass, the learned basis is used to approximate the original filters and then used as parameters for the convolutional layers. We validate our proposed solution for multiple CNN architectures on image classification and image super-resolution benchmarks and compare favorably to the existing state-of-the-art in terms of reduction of parameters and preservation of accuracy.



### Pareto-optimal data compression for binary classification tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.08961v2
- **DOI**: 10.3390/e22010007
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08961v2)
- **Published**: 2019-08-23 18:00:40+00:00
- **Updated**: 2020-01-15 18:43:57+00:00
- **Authors**: Max Tegmark, Tailin Wu
- **Comment**: Replaced to match version published in Entropy. 17 pages, 9 figs;
  improved discussion, comparison with Blahut-Arimoto method
- **Journal**: Entropy (2020), 22, 7
- **Summary**: The goal of lossy data compression is to reduce the storage cost of a data set $X$ while retaining as much information as possible about something ($Y$) that you care about. For example, what aspects of an image $X$ contain the most information about whether it depicts a cat? Mathematically, this corresponds to finding a mapping $X\to Z\equiv f(X)$ that maximizes the mutual information $I(Z,Y)$ while the entropy $H(Z)$ is kept below some fixed threshold. We present a method for mapping out the Pareto frontier for classification tasks, reflecting the tradeoff between retained entropy and class information. We first show how a random variable $X$ (an image, say) drawn from a class $Y\in\{1,...,n\}$ can be distilled into a vector $W=f(X)\in \mathbb{R}^{n-1}$ losslessly, so that $I(W,Y)=I(X,Y)$; for example, for a binary classification task of cats and dogs, each image $X$ is mapped into a single real number $W$ retaining all information that helps distinguish cats from dogs. For the $n=2$ case of binary classification, we then show how $W$ can be further compressed into a discrete variable $Z=g_\beta(W)\in\{1,...,m_\beta\}$ by binning $W$ into $m_\beta$ bins, in such a way that varying the parameter $\beta$ sweeps out the full Pareto frontier, solving a generalization of the Discrete Information Bottleneck (DIB) problem. We argue that the most interesting points on this frontier are "corners" maximizing $I(Z,Y)$ for a fixed number of bins $m=2,3...$ which can be conveniently be found without multiobjective optimization. We apply this method to the CIFAR-10, MNIST and Fashion-MNIST datasets, illustrating how it can be interpreted as an information-theoretically optimal image clustering algorithm.



### No Fear of the Dark: Image Retrieval under Varying Illumination Conditions
- **Arxiv ID**: http://arxiv.org/abs/1908.08999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08999v1)
- **Published**: 2019-08-23 19:30:37+00:00
- **Updated**: 2019-08-23 19:30:37+00:00
- **Authors**: Tomas Jenicek, Ondřej Chum
- **Comment**: None
- **Journal**: None
- **Summary**: Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets.



### Generic Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1908.09031v1
- **DOI**: 10.1109/TITS.2019.2930310
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09031v1)
- **Published**: 2019-08-23 20:34:53+00:00
- **Updated**: 2019-08-23 20:34:53+00:00
- **Authors**: Jiachen Li, Wei Zhan, Yeping Hu, Masayoshi Tomizuka
- **Comment**: IEEE Transactions on Intelligent Transportation Systems
- **Journal**: None
- **Summary**: Accurately tracking and predicting behaviors of surrounding objects are key prerequisites for intelligent systems such as autonomous vehicles to achieve safe and high-quality decision making and motion planning. However, there still remain challenges for multi-target tracking due to object number fluctuation and occlusion. To overcome these challenges, we propose a constrained mixture sequential Monte Carlo (CMSMC) method in which a mixture representation is incorporated in the estimated posterior distribution to maintain multi-modality. Multiple targets can be tracked simultaneously within a unified framework without explicit data association between observations and tracking targets. The framework can incorporate an arbitrary prediction model as the implicit proposal distribution of the CMSMC method. An example in this paper is a learning-based model for hierarchical time-series prediction, which consists of a behavior recognition module and a state evolution module. Both modules in the proposed model are generic and flexible so as to be applied to a class of time-series prediction problems where behaviors can be separated into different levels. Finally, the proposed framework is applied to a numerical case study as well as a task of on-road vehicle tracking, behavior recognition, and prediction in highway scenarios. Instead of only focusing on forecasting trajectory of a single entity, we jointly predict continuous motions for interactive entities simultaneously. The proposed approaches are evaluated from multiple aspects, which demonstrate great potential for intelligent vehicular systems and traffic surveillance systems.



### LiDARTag: A Real-Time Fiducial Tag System for Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1908.10349v3
- **DOI**: 10.1109/LRA.2021.3070302
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10349v3)
- **Published**: 2019-08-23 22:10:39+00:00
- **Updated**: 2021-02-13 22:33:19+00:00
- **Authors**: Jiunn-Kai Huang, Shoutian Wang, Maani Ghaffari, Jessy W. Grizzle
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters, 31 March 2021
- **Summary**: Image-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This paper introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer indoor dataset and the Honda H3D outdoor dataset. All implementations are coded in C++ and are available at: https://github.com/UMich-BipedLab/LiDARTag.



