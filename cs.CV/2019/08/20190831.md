# Arxiv Papers in cs.CV on 2019-08-31
### Energy Clustering for Unsupervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.00112v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00112v3)
- **Published**: 2019-08-31 02:52:28+00:00
- **Updated**: 2020-04-22 07:29:14+00:00
- **Authors**: Kaiwei Zeng
- **Comment**: Accepted by Image and Vision Computing
- **Journal**: None
- **Summary**: Due to the high cost of data annotation in supervised learning for person re-identification (Re-ID) methods, unsupervised learning becomes more attractive in the real world. The Bottom-up Clustering (BUC) approach based on hierarchical clustering serves as one promising unsupervised clustering method. One key factor of BUC is the distance measurement strategy. Ideally, the distance measurement should consider both inter-cluster and intra-cluster distance of all samples. However, BUC uses the minimum distance, only considers a pair of the nearest sample between two clusters and ignores the diversity of other samples in clusters. To solve this problem, we propose to use the energy distance to evaluate both the inter-cluster and intra-cluster distance in hierarchical clustering(E-cluster), and use the sum of squares of deviations(SSD) as a regularization term to further balance the diversity and similarity of energy distance evaluation. We evaluate our method on large scale re-ID datasets, including Market-1501, DukeMTMC-reID and MARS. Extensive experiments show that our method obtains significant improvements over the state-of-the-art unsupervised methods, and even better than some transfer learning methods.



### Towards Learning Affine-Invariant Representations via Data-Efficient CNNs
- **Arxiv ID**: http://arxiv.org/abs/1909.00114v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00114v1)
- **Published**: 2019-08-31 03:11:59+00:00
- **Updated**: 2019-08-31 03:11:59+00:00
- **Authors**: Xenju Xu, Guanghui Wang, Alan Sullivan, Ziming Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we propose integrating a priori knowledge into both design and training of convolutional neural networks (CNNs) to learn object representations that are invariant to affine transformations (i.e., translation, scale, rotation). Accordingly we propose a novel multi-scale maxout CNN and train it end-to-end with a novel rotation-invariant regularizer. This regularizer aims to enforce the weights in each 2D spatial filter to approximate circular patterns. In this way, we manage to handle affine transformations in training using convolution, multi-scale maxout, and circular filters. Empirically we demonstrate that such knowledge can significantly improve the data-efficiency as well as generalization and robustness of learned models. For instance, on the Traffic Sign data set and trained with only 10 images per class, our method can achieve 84.15% that outperforms the state-of-the-art by 29.80% in terms of test accuracy.



### From perception to control: an autonomous driving system for a formula student driverless car
- **Arxiv ID**: http://arxiv.org/abs/1909.00119v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00119v1)
- **Published**: 2019-08-31 03:56:47+00:00
- **Updated**: 2019-08-31 03:56:47+00:00
- **Authors**: Tairan Chen, Zirui Li, Yiting He, Zewen Xu, Zhe Yan, Huiqian Li
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces the autonomous system of the "Smart Shark II" which won the Formula Student Autonomous China (FSAC) Competition in 2018. In this competition, an autonomous racecar is required to complete autonomously two laps of unknown track. In this paper, the author presents the self-driving software structure of this racecar which ensure high vehicle speed and safety. The key components ensure a stable driving of the racecar, LiDAR-based and Vision-based cone detection provide a redundant perception; the EKF-based localization offers high accuracy and high frequency state estimation; perception results are accumulated in time and space by occupancy grid map. After getting the trajectory, a model predictive control algorithm is used to optimize in both longitudinal and lateral control of the racecar. Finally, the performance of an experiment based on real-world data is shown.



### A Semantics-Assisted Video Captioning Model Trained with Scheduled Sampling
- **Arxiv ID**: http://arxiv.org/abs/1909.00121v3
- **DOI**: 10.3389/frobt.2020.475767
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1909.00121v3)
- **Published**: 2019-08-31 04:01:38+00:00
- **Updated**: 2020-07-24 08:01:29+00:00
- **Authors**: Haoran Chen, Ke Lin, Alexander Maye, Jianming Li, Xiaolin Hu
- **Comment**: 11 pages
- **Journal**: Front. Robot. AI 7:475767 (2020)
- **Summary**: Given the features of a video, recurrent neural networks can be used to automatically generate a caption for the video. Existing methods for video captioning have at least three limitations. First, semantic information has been widely applied to boost the performance of video captioning models, but existing networks often fail to provide meaningful semantic features. Second, the Teacher Forcing algorithm is often utilized to optimize video captioning models, but during training and inference, different strategies are applied to guide word generation, leading to poor performance. Third, current video captioning models are prone to generate relatively short captions that express video contents inappropriately. Toward resolving these three problems, we suggest three corresponding improvements. First of all, we propose a metric to compare the quality of semantic features, and utilize appropriate features as input for a semantic detection network (SDN) with adequate complexity in order to generate meaningful semantic features for videos. Then, we apply a scheduled sampling strategy that gradually transfers the training phase from a teacher-guided manner toward a more self-teaching manner. Finally, the ordinary logarithm probability loss function is leveraged by sentence length so that the inclination of generating short sentences is alleviated. Our model achieves better results than previous models on the YouTube2Text dataset and is competitive with the previous best model on the MSR-VTT dataset.



### HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking
- **Arxiv ID**: http://arxiv.org/abs/1909.00122v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.00122v2)
- **Published**: 2019-08-31 04:02:16+00:00
- **Updated**: 2019-09-07 08:33:25+00:00
- **Authors**: Shen Yan, Biyi Fang, Faen Zhang, Yu Zheng, Xiao Zeng, Hui Xu, Mi Zhang
- **Comment**: 9 pages, 6 figures, 6 tables. Nominated for ICCV 2019 Neural
  Architects Workshop Best Paper Award
- **Journal**: None
- **Summary**: The use of automatic methods, often referred to as Neural Architecture Search (NAS), in designing neural network architectures has recently drawn considerable attention. In this work, we present an efficient NAS approach, named HM- NAS, that generalizes existing weight sharing based NAS approaches. Existing weight sharing based NAS approaches still adopt hand-designed heuristics to generate architecture candidates. As a consequence, the space of architecture candidates is constrained in a subset of all possible architectures, making the architecture search results sub-optimal. HM-NAS addresses this limitation via two innovations. First, HM-NAS incorporates a multi-level architecture encoding scheme to enable searching for more flexible network architectures. Second, it discards the hand-designed heuristics and incorporates a hierarchical masking scheme that automatically learns and determines the optimal architecture. Compared to state-of-the-art weight sharing based approaches, HM-NAS is able to achieve better architecture search performance and competitive model evaluation accuracy. Without the constraint imposed by the hand-designed heuristics, our searched networks contain more flexible and meaningful architectures that existing weight sharing based NAS approaches are not able to discover.



### Detecting floodwater on roadways from image data with handcrafted features and deep transfer learning
- **Arxiv ID**: http://arxiv.org/abs/1909.00125v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00125v1)
- **Published**: 2019-08-31 04:35:56+00:00
- **Updated**: 2019-08-31 04:35:56+00:00
- **Authors**: Cem Sazara, Mecit Cetin, Khan M. Iftekharuddin
- **Comment**: Accepted at IEEE-ITSC 2019: The 22nd IEEE International Conference on
  Intelligent Transportation Systems, Auckland, NZ, October 27-30, 2019
- **Journal**: None
- **Summary**: Detecting roadway segments inundated due to floodwater has important applications for vehicle routing and traffic management decisions. This paper proposes a set of algorithms to automatically detect floodwater that may be present in an image captured by mobile phones or other types of optical cameras. For this purpose, image classification and flood area segmentation methods are developed. For the classification task, we used Local Binary Patterns (LBP), Histogram of Oriented Gradients (HOG) and pre-trained deep neural network (VGG-16) as feature extractors and trained logistic regression, k-nearest neighbors, and decision tree classifiers on the extracted features. Pre-trained VGG-16 network with logistic regression classifier outperformed all other methods. For the flood area segmentation task, we investigated superpixel based methods and Fully Convolutional Neural Network (FCN). Similar to the classification task, we trained logistic regression and k-nearest neighbors classifiers on the superpixel areas and compared that with an end-to-end trained FCN. Conditional Random Fields (CRF) method was applied after both segmentation methods to post-process coarse segmentation results. FCN offered the highest scores in all metrics; it was followed by superpixel-based logistic regression and then superpixel-based KNN.



### Deep Coarse-to-fine Dense Light Field Reconstruction with Flexible Sampling and Geometry-aware Fusion
- **Arxiv ID**: http://arxiv.org/abs/1909.01341v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.01341v3)
- **Published**: 2019-08-31 05:16:21+00:00
- **Updated**: 2020-09-26 10:33:15+00:00
- **Authors**: Jing Jin, Junhui Hou, Jie Chen, Huanqiang Zeng, Sam Kwong, Jingyi Yu
- **Comment**: 17 pages, 11 figures, 10 tables
- **Journal**: None
- **Summary**: A densely-sampled light field (LF) is highly desirable in various applications, such as 3-D reconstruction, post-capture refocusing and virtual reality. However, it is costly to acquire such data. Although many computational methods have been proposed to reconstruct a densely-sampled LF from a sparsely-sampled one, they still suffer from either low reconstruction quality, low computational efficiency, or the restriction on the regularity of the sampling pattern. To this end, we propose a novel learning-based method, which accepts sparsely-sampled LFs with irregular structures, and produces densely-sampled LFs with arbitrary angular resolution accurately and efficiently. We also propose a simple yet effective method for optimizing the sampling pattern. Our proposed method, an end-to-end trainable network, reconstructs a densely-sampled LF in a coarse-to-fine manner. Specifically, the coarse sub-aperture image (SAI) synthesis module first explores the scene geometry from an unstructured sparsely-sampled LF and leverages it to independently synthesize novel SAIs, in which a confidence-based blending strategy is proposed to fuse the information from different input SAIs, giving an intermediate densely-sampled LF. Then, the efficient LF refinement module learns the angular relationship within the intermediate result to recover the LF parallax structure. Comprehensive experimental evaluations demonstrate the superiority of our method on both real-world and synthetic LF images when compared with state-of-the-art methods. In addition, we illustrate the benefits and advantages of the proposed approach when applied in various LF-based applications, including image-based rendering and depth estimation enhancement.



### Object Detection in Optical Remote Sensing Images: A Survey and A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1909.00133v2
- **DOI**: 10.1016/j.isprsjprs.2019.11.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00133v2)
- **Published**: 2019-08-31 05:36:15+00:00
- **Updated**: 2019-09-22 02:13:30+00:00
- **Authors**: Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, Junwei Han
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, 159: 296-307,
  2020
- **Summary**: Substantial efforts have been devoted more recently to presenting various methods for object detection in optical remote sensing images. However, the current survey of datasets and deep learning based methods for object detection in optical remote sensing images is not adequate. Moreover, most of the existing datasets have some shortcomings, for example, the numbers of images and object categories are small scale, and the image diversity and variations are insufficient. These limitations greatly affect the development of deep learning based object detection methods. In the paper, we provide a comprehensive review of the recent deep learning based object detection progress in both the computer vision and earth observation communities. Then, we propose a large-scale, publicly available benchmark for object DetectIon in Optical Remote sensing images, which we name as DIOR. The dataset contains 23463 images and 192472 instances, covering 20 object classes. The proposed DIOR dataset 1) is large-scale on the object categories, on the object instance number, and on the total image number; 2) has a large range of object size variations, not only in terms of spatial resolutions, but also in the aspect of inter- and intra-class size variability across objects; 3) holds big variations as the images are obtained with different imaging conditions, weathers, seasons, and image quality; and 4) has high inter-class similarity and intra-class diversity. The proposed benchmark can help the researchers to develop and validate their data-driven methods. Finally, we evaluate several state-of-the-art approaches on our DIOR dataset to establish a baseline for future research.



### Scraping Social Media Photos Posted in Kenya and Elsewhere to Detect and Analyze Food Types
- **Arxiv ID**: http://arxiv.org/abs/1909.00134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00134v1)
- **Published**: 2019-08-31 05:37:13+00:00
- **Updated**: 2019-08-31 05:37:13+00:00
- **Authors**: Kaihong Wang, Mona Jalal, Sankara Jefferson, Yi Zheng, Elaine O. Nsoesie, Margrit Betke
- **Comment**: Another version of the paper was submitted to the ACM International
  Conference on Multimedia (ACMMM2019)
- **Journal**: None
- **Summary**: Monitoring population-level changes in diet could be useful for education and for implementing interventions to improve health. Research has shown that data from social media sources can be used for monitoring dietary behavior. We propose a scrape-by-location methodology to create food image datasets from Instagram posts. We used it to collect 3.56 million images over a period of 20 days in March 2019. We also propose a scrape-by-keywords methodology and used it to scrape ~30,000 images and their captions of 38 Kenyan food types. We publish two datasets of 104,000 and 8,174 image/caption pairs, respectively. With the first dataset, Kenya104K, we train a Kenyan Food Classifier, called KenyanFC, to distinguish Kenyan food from non-food images posted in Kenya. We used the second dataset, KenyanFood13, to train a classifier KenyanFTR, short for Kenyan Food Type Recognizer, to recognize 13 popular food types in Kenya. The KenyanFTR is a multimodal deep neural network that can identify 13 types of Kenyan foods using both images and their corresponding captions. Experiments show that the average top-1 accuracy of KenyanFC is 99% over 10,400 tested Instagram images and of KenyanFTR is 81% over 8,174 tested data points. Ablation studies show that three of the 13 food types are particularly difficult to categorize based on image content only and that adding analysis of captions to the image analysis yields a classifier that is 9 percent points more accurate than a classifier that relies only on images. Our food trend analysis revealed that cakes and roasted meats were the most popular foods in photographs on Instagram in Kenya in March 2019.



### Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1909.00166v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00166v1)
- **Published**: 2019-08-31 08:29:31+00:00
- **Updated**: 2019-08-31 08:29:31+00:00
- **Authors**: Reza Azad, Maryam Asadi-Aghbolaghi, Mahmood Fathy, Sergio Escalera
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, deep learning-based networks have achieved state-of-the-art performance in medical image segmentation. Among the existing networks, U-Net has been successfully applied on medical image segmentation. In this paper, we propose an extension of U-Net, Bi-directional ConvLSTM U-Net with Densely connected convolutions (BCDU-Net), for medical image segmentation, in which we take full advantages of U-Net, bi-directional ConvLSTM (BConvLSTM) and the mechanism of dense convolutions. Instead of a simple concatenation in the skip connection of U-Net, we employ BConvLSTM to combine the feature maps extracted from the corresponding encoding path and the previous decoding up-convolutional layer in a non-linear way. To strengthen feature propagation and encourage feature reuse, we use densely connected convolutions in the last convolutional layer of the encoding path. Finally, we can accelerate the convergence speed of the proposed network by employing batch normalization (BN). The proposed model is evaluated on three datasets of: retinal blood vessel segmentation, skin lesion segmentation, and lung nodule segmentation, achieving state-of-the-art performance.



### Imbalance Problems in Object Detection: A Review
- **Arxiv ID**: http://arxiv.org/abs/1909.00169v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00169v3)
- **Published**: 2019-08-31 08:55:28+00:00
- **Updated**: 2020-03-11 17:54:54+00:00
- **Authors**: Kemal Oksuz, Baris Can Cam, Sinan Kalkan, Emre Akbas
- **Comment**: Accepted to IEEE TPAMI; currently in press
- **Journal**: None
- **Summary**: In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: https://github.com/kemaloksuz/ObjectDetectionImbalance .



### Boundary-Aware Feature Propagation for Scene Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1909.00179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00179v1)
- **Published**: 2019-08-31 09:56:09+00:00
- **Updated**: 2019-08-31 09:56:09+00:00
- **Authors**: Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, Gang Wang
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes.



### Scale Calibrated Training: Improving Generalization of Deep Networks via Scale-Specific Normalization
- **Arxiv ID**: http://arxiv.org/abs/1909.00182v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00182v2)
- **Published**: 2019-08-31 10:01:37+00:00
- **Updated**: 2020-09-07 15:09:01+00:00
- **Authors**: Zhuoran Yu, Aojun Zhou, Yukun Ma, Yudian Li, Xiaohan Zhang, Ping Luo
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Standard convolutional neural networks(CNNs) require consistent image resolutions in both training and testing phase. However, in practice, testing with smaller image sizes is necessary for fast inference. We show that trivially evaluating low-resolution images on networks trained with high-resolution images results in a catastrophic accuracy drop in standard CNN architectures. We propose a novel training regime called Scale calibrated Training(SCT) which allows networks to learn from various scales of input simultaneously. By taking advantages of SCT, single network can provide decent accuracy at test time in response to multiple test scales. In our analysis, we surprisingly find that vanilla batch normalization can lead to sub-optimal performance in SCT. Therefore, a novel normalization scheme called Scale-Specific Batch Normalization is equipped to SCT in replacement of batch normalization. Experiment results show that SCT improves accuracy of single Resnet-50 on ImageNet by 1.7% and 11.5% accuracy when testing on image sizes of 224 and 128 respectively.



### Joint Segmentation and Landmark Localization of Fetal Femur in Ultrasound Volumes
- **Arxiv ID**: http://arxiv.org/abs/1909.00186v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00186v1)
- **Published**: 2019-08-31 10:37:56+00:00
- **Updated**: 2019-08-31 10:37:56+00:00
- **Authors**: Xu Wang, Xin Yang, Haoran Dou, Shengli Li, Pheng-Ann Heng, Dong Ni
- **Comment**: Accepted by IEEE-EMBS International Conference on Biomedical and
  Health Informatics (BHI), 2019
- **Journal**: None
- **Summary**: Volumetric ultrasound has great potentials in promoting prenatal examinations. Automated solutions are highly desired to efficiently and effectively analyze the massive volumes. Segmentation and landmark localization are two key techniques in making the quantitative evaluation of prenatal ultrasound volumes available in clinic. However, both tasks are non-trivial when considering the poor image quality, boundary ambiguity and anatomical variations in volumetric ultrasound. In this paper, we propose an effective framework for simultaneous segmentation and landmark localization in prenatal ultrasound volumes. The proposed framework has two branches where informative cues of segmentation and landmark localization can be propagated bidirectionally to benefit both tasks. As landmark localization tends to suffer from false positives, we propose a distance based loss to suppress the noise and thus enhance the localization map and in turn the segmentation. Finally, we further leverage an adversarial module to emphasize the correspondence between segmentation and landmark localization. Extensively validated on a volumetric ultrasound dataset of fetal femur, our proposed framework proves to be a promising solution to facilitate the interpretation of prenatal ultrasound volumes.



### Push for Quantization: Deep Fisher Hashing
- **Arxiv ID**: http://arxiv.org/abs/1909.00206v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00206v1)
- **Published**: 2019-08-31 12:18:43+00:00
- **Updated**: 2019-08-31 12:18:43+00:00
- **Authors**: Yunqiang Li, Wenjie Pei, Yufei zha, Jan van Gemert
- **Comment**: BMVC 2019
- **Journal**: None
- **Summary**: Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus beneficial because they map high-dimensional data to compact binary codes that are efficient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher's Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art.



### Robust BGA Void Detection Using Multi Directional Scan Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1909.00211v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1909.00211v1)
- **Published**: 2019-08-31 13:09:55+00:00
- **Updated**: 2019-08-31 13:09:55+00:00
- **Authors**: Vikas Ahuja, Vijay Kumar Neeluru
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1907.04222
- **Journal**: None
- **Summary**: The life time of electronic circuits board are impacted by the voids present in soldering balls. The quality inspection of solder balls by detecting and measuring the void is important to improve the board yield issues in electronic circuits. In general, the inspection is carried out manually, based on 2D or 3D X-ray images. For high quality inspection, it is difficult to detect and measure voids accurately with high repeatability through the manual inspection and it is time consuming process. In need of high quality and fast inspection, various approaches were proposed for void detection. But, lacks in robustness in dealing with various challenges like vias, reflections from the plating or vias, inconsistent lighting, noise, void-like artefacts, various void shapes, low resolution images and scalability to various devices. Robust BGA void detection becomes quite difficult problem, especially if the image size is very small (say, around 40x40) and with low contrast between void and the BGA background (say around 7 intensity levels on a scale of 255). In this work, we propose novel approach for void detection based on the multi directional scanning. The proposed approach is able to segment the voids for low resolution images and can be easily scaled to various electronic manufacturing products.



### UPI-Net: Semantic Contour Detection in Placental Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1909.00229v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00229v2)
- **Published**: 2019-08-31 15:39:12+00:00
- **Updated**: 2019-09-13 16:27:51+00:00
- **Authors**: Huan Qi, Sally Collins, J. Alison Noble
- **Comment**: 9 pages, 8 figures, accepted at Visual Recognition for Medical Images
  (VRMI), ICCV 2019
- **Journal**: None
- **Summary**: Semantic contour detection is a challenging problem that is often met in medical imaging, of which placental image analysis is a particular example. In this paper, we investigate utero-placental interface (UPI) detection in 2D placental ultrasound images by formulating it as a semantic contour detection problem. As opposed to natural images, placental ultrasound images contain specific anatomical structures thus have unique geometry. We argue it would be beneficial for UPI detectors to incorporate global context modelling in order to reduce unwanted false positive UPI predictions. Our approach, namely UPI-Net, aims to capture long-range dependencies in placenta geometry through lightweight global context modelling and effective multi-scale feature aggregation. We perform a subject-level 10-fold nested cross-validation on a placental ultrasound database (4,871 images with labelled UPI from 49 scans). Experimental results demonstrate that, without introducing considerable computational overhead, UPI-Net yields the highest performance in terms of standard contour detection metrics, compared to other competitive benchmarks.



### WSLLN: Weakly Supervised Natural Language Localization Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.00239v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00239v1)
- **Published**: 2019-08-31 16:30:28+00:00
- **Updated**: 2019-08-31 16:30:28+00:00
- **Authors**: Mingfei Gao, Larry S. Davis, Richard Socher, Caiming Xiong
- **Comment**: accepted by EMNLP2019
- **Journal**: None
- **Summary**: We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance.



### Integrating Data and Image Domain Deep Learning for Limited Angle Tomography using Consensus Equilibrium
- **Arxiv ID**: http://arxiv.org/abs/1909.00240v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00240v1)
- **Published**: 2019-08-31 16:46:12+00:00
- **Updated**: 2019-08-31 16:46:12+00:00
- **Authors**: Muhammad Usman Ghani, W. Clem Karl
- **Comment**: Accepted for publication in proceedings of IEEE ICCV Workshop on
  Learning for Computational Imaging (ICCVW-LCI)
- **Journal**: None
- **Summary**: Computed Tomography (CT) is a non-invasive imaging modality with applications ranging from healthcare to security. It reconstructs cross-sectional images of an object using a collection of projection data collected at different angles. Conventional methods, such as FBP, require that the projection data be uniformly acquired over the complete angular range. In some applications, it is not possible to acquire such data. Security is one such domain where non-rotational scanning configurations are being developed which violate the complete data assumption. Conventional methods produce images from such data that are filled with artifacts. The recent success of deep learning (DL) methods has inspired researchers to post-process these artifact laden images using deep neural networks (DNNs). This approach has seen limited success on real CT problems. Another approach has been to pre-process the incomplete data using DNNs aiming to avoid the creation of artifacts altogether. Due to imperfections in the learning process, this approach can still leave perceptible residual artifacts. In this work, we aim to combine the power of deep learning in both the data and image domains through a two-step process based on the consensus equilibrium (CE) framework. Specifically, we use conditional generative adversarial networks (cGANs) in both the data and the image domain for enhanced performance and efficient computation and combine them through a consensus process. We demonstrate the effectiveness of our approach on a real security CT dataset for a challenging 90 degree limited-angle problem. The same framework can be applied to other limited data problems arising in applications such as electron microscopy, non-destructive evaluation, and medical imaging.



### Gland Segmentation in Histopathology Images Using Deep Networks and Handcrafted Features
- **Arxiv ID**: http://arxiv.org/abs/1909.00270v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00270v1)
- **Published**: 2019-08-31 19:42:12+00:00
- **Updated**: 2019-08-31 19:42:12+00:00
- **Authors**: Safiyeh Rezaei, Ali Emami, Hamidreza Zarrabi, Shima Rafiei, Kayvan Najarian, Nader Karimi, Shadrokh Samavi, S. M. Reza Soroushmehr
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathology images contain essential information for medical diagnosis and prognosis of cancerous disease. Segmentation of glands in histopathology images is a primary step for analysis and diagnosis of an unhealthy patient. Due to the widespread application and the great success of deep neural networks in intelligent medical diagnosis and histopathology, we propose a modified version of LinkNet for gland segmentation and recognition of malignant cases. We show that using specific handcrafted features such as invariant local binary pattern drastically improves the system performance. The experimental results demonstrate the competency of the proposed system against state-of-the-art methods. We achieved the best results in testing on section B images of the Warwick-QU dataset and obtained comparable results on section A images.



### Fetal Ultrasound Image Segmentation for Measuring Biometric Parameters Using Multi-Task Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1909.00273v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00273v1)
- **Published**: 2019-08-31 19:43:31+00:00
- **Updated**: 2019-08-31 19:43:31+00:00
- **Authors**: Zahra Sobhaninia, Shima Rafiei, Ali Emami, Nader Karimi, Kayvan Najarian, Shadrokh Samavi, S. M. Reza Soroushmehr
- **Comment**: None
- **Journal**: None
- **Summary**: Ultrasound imaging is a standard examination during pregnancy that can be used for measuring specific biometric parameters towards prenatal diagnosis and estimating gestational age. Fetal head circumference (HC) is one of the significant factors to determine the fetus growth and health. In this paper, a multi-task deep convolutional neural network is proposed for automatic segmentation and estimation of HC ellipse by minimizing a compound cost function composed of segmentation dice score and MSE of ellipse parameters. Experimental results on fetus ultrasound dataset in different trimesters of pregnancy show that the segmentation results and the extracted HC match well with the radiologist annotations. The obtained dice scores of the fetal head segmentation and the accuracy of HC evaluations are comparable to the state-of-the-art.



### Automatic Detection of Bowel Disease with Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1909.00276v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1909.00276v1)
- **Published**: 2019-08-31 19:51:23+00:00
- **Updated**: 2019-08-31 19:51:23+00:00
- **Authors**: Robert Holland, Uday Patel, Phillip Lung, Elisa Chotzoglou, Bernhard Kainz
- **Comment**: Accepted to PRIME-MICCAI 2019
- **Journal**: None
- **Summary**: Crohn's disease, one of two inflammatory bowel diseases (IBD), affects 200,000 people in the UK alone, or roughly one in every 500. We explore the feasibility of deep learning algorithms for identification of terminal ileal Crohn's disease in Magnetic Resonance Enterography images on a small dataset. We show that they provide comparable performance to the current clinical standard, the MaRIA score, while requiring only a fraction of the preparation and inference time. Moreover, bowels are subject to high variation between individuals due to the complex and free-moving anatomy. Thus we also explore the effect of difficulty of the classification at hand on performance. Finally, we employ soft attention mechanisms to amplify salient local features and add interpretability.



### SSSDET: Simple Short and Shallow Network for Resource Efficient Vehicle Detection in Aerial Scenes
- **Arxiv ID**: http://arxiv.org/abs/1909.00292v1
- **DOI**: 10.1109/ICIP.2019.8803262
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.00292v1)
- **Published**: 2019-08-31 22:00:07+00:00
- **Updated**: 2019-08-31 22:00:07+00:00
- **Authors**: Murari Mandal, Manal Shah, Prashant Meena, Santosh Kumar Vipparthi
- **Comment**: International Conference on Image Processing (ICIP) 2019, Taipei,
  Taiwan
- **Journal**: None
- **Summary**: Detection of small-sized targets is of paramount importance in many aerial vision-based applications. The commonly deployed low cost unmanned aerial vehicles (UAVs) for aerial scene analysis are highly resource constrained in nature. In this paper we propose a simple short and shallow network (SSSDet) to robustly detect and classify small-sized vehicles in aerial scenes. The proposed SSSDet is up to 4x faster, requires 4.4x less FLOPs, has 30x less parameters, requires 31x less memory space and provides better accuracy in comparison to existing state-of-the-art detectors. Thus, it is more suitable for hardware implementation in real-time applications. We also created a new airborne image dataset (ABD) by annotating 1396 new objects in 79 aerial images for our experiments. The effectiveness of the proposed method is validated on the existing VEDAI, DLR-3K, DOTA and Combined dataset. The SSSDet outperforms state-of-the-art detectors in term of accuracy, speed, compute and memory efficiency.



### Second-order Non-local Attention Networks for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1909.00295v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.00295v1)
- **Published**: 2019-08-31 22:50:42+00:00
- **Updated**: 2019-08-31 22:50:42+00:00
- **Authors**: Bryan, Xia, Yuan Gong, Yizhe Zhang, Christian Poellabauer
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID.



