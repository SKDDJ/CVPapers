# Arxiv Papers in cs.CV on 2019-08-20
### Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses
- **Arxiv ID**: http://arxiv.org/abs/1908.07116v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.07116v1)
- **Published**: 2019-08-20 00:29:23+00:00
- **Updated**: 2019-08-20 00:29:23+00:00
- **Authors**: Xiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, Peter Chin
- **Comment**: Published as Conference Paper @ IJCAI 2019
- **Journal**: None
- **Summary**: Despite achieving remarkable success in various domains, recent studies have uncovered the vulnerability of deep neural networks to adversarial perturbations, creating concerns on model generalizability and new threats such as prediction-evasive misclassification or stealthy reprogramming. Among different defense proposals, stochastic network defenses such as random neuron activation pruning or random perturbation to layer inputs are shown to be promising for attack mitigation. However, one critical drawback of current defenses is that the robustness enhancement is at the cost of noticeable performance degradation on legitimate data, e.g., large drop in test accuracy. This paper is motivated by pursuing for a better trade-off between adversarial robustness and test accuracy for stochastic network defenses. We propose Defense Efficiency Score (DES), a comprehensive metric that measures the gain in unsuccessful attack attempts at the cost of drop in test accuracy of any defense. To achieve a better DES, we propose hierarchical random switching (HRS), which protects neural networks through a novel randomization scheme. A HRS-protected model contains several blocks of randomly switching channels to prevent adversaries from exploiting fixed model structures and parameters for their malicious purposes. Extensive experiments show that HRS is superior in defending against state-of-the-art white-box and adaptive adversarial misclassification attacks. We also demonstrate the effectiveness of HRS in defending adversarial reprogramming, which is the first defense against adversarial programs. Moreover, in most settings the average DES of HRS is at least 5X higher than current stochastic network defenses, validating its significantly improved robustness-accuracy trade-off.



### 360-Degree Textures of People in Clothing from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1908.07117v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.07117v1)
- **Published**: 2019-08-20 00:42:40+00:00
- **Updated**: 2019-08-20 00:42:40+00:00
- **Authors**: Verica Lazova, Eldar Insafutdinov, Gerard Pons-Moll
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we predict a full 3D avatar of a person from a single image. We infer texture and geometry in the UV-space of the SMPL model using an image-to-image translation method. Given partial texture and segmentation layout maps derived from the input view, our model predicts the complete segmentation map, the complete texture map, and a displacement map. The predicted maps can be applied to the SMPL model in order to naturally generalize to novel poses, shapes, and even new clothing. In order to learn our model in a common UV-space, we non-rigidly register the SMPL model to thousands of 3D scans, effectively encoding textures and geometries as images in correspondence. This turns a difficult 3D inference task into a simpler image-to-image translation one. Results on rendered scans of people and images from the DeepFashion dataset demonstrate that our method can reconstruct plausible 3D avatars from a single image. We further use our model to digitally change pose, shape, swap garments between people and edit clothing. To encourage research in this direction we will make the source code available for research purpose.



### Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation
- **Arxiv ID**: http://arxiv.org/abs/1908.07121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07121v1)
- **Published**: 2019-08-20 01:13:26+00:00
- **Updated**: 2019-08-20 01:13:26+00:00
- **Authors**: Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, Mingli Song
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: A massive number of well-trained deep networks have been released by developers online. These networks may focus on different tasks and in many cases are optimized for different datasets. In this paper, we study how to exploit such heterogeneous pre-trained networks, known as teachers, so as to train a customized student network that tackles a set of selective tasks defined by the user. We assume no human annotations are available, and each teacher may be either single- or multi-task. To this end, we introduce a dual-step strategy that first extracts the task-specific knowledge from the heterogeneous teachers sharing the same sub-task, and then amalgamates the extracted knowledge to build the student network. To facilitate the training, we employ a selective learning scheme where, for each unlabelled sample, the student learns adaptively from only the teacher with the least prediction ambiguity. We evaluate the proposed approach on several datasets and experimental results demonstrate that the student, learned by such adaptive knowledge amalgamation, achieves performances even better than those of the teachers.



### Zero-Shot Grounding of Objects from Natural Language Queries
- **Arxiv ID**: http://arxiv.org/abs/1908.07129v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.07129v1)
- **Published**: 2019-08-20 02:07:14+00:00
- **Updated**: 2019-08-20 02:07:14+00:00
- **Authors**: Arka Sadhu, Kan Chen, Ram Nevatia
- **Comment**: ICCV19 oral, camera-ready version
- **Journal**: None
- **Summary**: A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, "unseen" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual "seen" settings and performs significantly better than baseline in the zero-shot setting.



### An End-to-end Video Text Detector with Online Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.07135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07135v1)
- **Published**: 2019-08-20 02:53:29+00:00
- **Updated**: 2019-08-20 02:53:29+00:00
- **Authors**: Hongyuan Yu, Chengquan Zhang, Xuan Li, Junyu Han, Errui Ding, Liang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Video text detection is considered as one of the most difficult tasks in document analysis due to the following two challenges: 1) the difficulties caused by video scenes, i.e., motion blur, illumination changes, and occlusion; 2) the properties of text including variants of fonts, languages, orientations, and shapes. Most existing methods attempt to enhance the performance of video text detection by cooperating with video text tracking, but treat these two tasks separately. In this work, we propose an end-to-end video text detection model with online tracking to address these two challenges. Specifically, in the detection branch, we adopt ConvLSTM to capture spatial structure information and motion memory. In the tracking branch, we convert the tracking problem to text instance association, and an appearance-geometry descriptor with memory mechanism is proposed to generate robust representation of text instances. By integrating these two branches into one trainable framework, they can promote each other and the computational cost is significantly reduced. Experiments on existing video text benchmarks including ICDAR2013 Video, Minetto and YVT demonstrate that the proposed method significantly outperforms state-of-the-art methods. Our method improves F-score by about 2 on all datasets and it can run realtime with 24.36 fps on TITAN Xp.



### StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible
- **Arxiv ID**: http://arxiv.org/abs/1908.07144v1
- **DOI**: 10.1145/3332165.3347873
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07144v1)
- **Published**: 2019-08-20 03:23:35+00:00
- **Updated**: 2019-08-20 03:23:35+00:00
- **Authors**: Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu, Jeffrey P. Bigham
- **Comment**: ACM UIST 2019
- **Journal**: None
- **Summary**: Blind people frequently encounter inaccessible dynamic touchscreens in their everyday lives that are difficult, frustrating, and often impossible to use independently. Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-flight entertainment systems. Interacting with dynamic touchscreens is difficult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen. To solve these problems, we introduce StateLens - a three-part reverse engineering solution that makes existing dynamic touchscreens accessible. First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline. Second, using the state diagrams, StateLens automatically generates conversational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface. Finally, a set of 3D-printed accessories enable blind people to explore capacitive touchscreens without the risk of triggering accidental touches on the interface. Our technical evaluation shows that StateLens can accurately reconstruct interfaces from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.



### Endotracheal Tube Detection and Segmentation in Chest Radiographs using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1908.07170v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07170v1)
- **Published**: 2019-08-20 05:32:22+00:00
- **Updated**: 2019-08-20 05:32:22+00:00
- **Authors**: Maayan Frid-Adar, Rula Amer, Hayit Greenspan
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Chest radiographs are frequently used to verify the correct intubation of patients in the emergency room. Fast and accurate identification and localization of the endotracheal (ET) tube is critical for the patient. In this study we propose a novel automated deep learning scheme for accurate detection and segmentation of the ET tubes. Development of automatic systems using deep learning networks for classification and segmentation require large annotated data which is not always available. Here we present an approach for synthesizing ET tubes in real X-ray images. We suggest a method for training the network, first with synthetic data and then with real X-ray images in a fine-tuning phase, which allows the network to train on thousands of cases without annotating any data. The proposed method was tested on 477 real chest radiographs from a public dataset and reached AUC of 0.99 in classifying the presence vs. absence of the ET tube, along with outputting high quality ET tube segmentation maps.



### Human Mesh Recovery from Monocular Images via a Skeleton-disentangled Representation
- **Arxiv ID**: http://arxiv.org/abs/1908.07172v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07172v2)
- **Published**: 2019-08-20 05:35:58+00:00
- **Updated**: 2019-09-17 10:35:56+00:00
- **Authors**: Sun Yu, Ye Yun, Liu Wu, Gao Wenpeng, Fu YiLi, Mei Tao
- **Comment**: Accepted by ICCV2019. The code is released at
  https://github.com/Arthur151/DSD-SATN
- **Journal**: IEEE International Conference on Computer Vision, ICCV, 2019
- **Summary**: We describe an end-to-end method for recovering 3D human body mesh from single images and monocular videos. Different from the existing methods try to obtain all the complex 3D pose, shape, and camera parameters from one coupling feature, we propose a skeleton-disentangling based framework, which divides this task into multi-level spatial and temporal granularity in a decoupling manner. In spatial, we propose an effective and pluggable "disentangling the skeleton from the details" (DSD) module. It reduces the complexity and decouples the skeleton, which lays a good foundation for temporal modeling. In temporal, the self-attention based temporal convolution network is proposed to efficiently exploit the short and long-term temporal cues. Furthermore, an unsupervised adversarial training strategy, temporal shuffles and order recovery, is designed to promote the learning of motion dynamics. The proposed method outperforms the state-of-the-art 3D human mesh recovery methods by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also achieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning. Especially, ablation studies demonstrate that skeleton-disentangled representation is crucial for better temporal modeling and generalization.



### Ranking Viscous Finger Simulations to an Acquired Ground Truth with Topology-aware Matchings
- **Arxiv ID**: http://arxiv.org/abs/1908.07841v1
- **DOI**: None
- **Categories**: **physics.geo-ph**, cs.CG, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07841v1)
- **Published**: 2019-08-20 05:37:59+00:00
- **Updated**: 2019-08-20 05:37:59+00:00
- **Authors**: Maxime Soler, Martin Petitfrere, Gilles Darche, Melanie Plainchault, Bruno Conche, Julien Tierny
- **Comment**: None
- **Journal**: None
- **Summary**: This application paper presents a novel framework based on topological data analysis for the automatic evaluation and ranking of viscous finger simulation runs in an ensemble with respect to a reference acquisition. Individual fingers in a given time-step are associated with critical point pairs in the distance field to the injection point, forming persistence diagrams. Different metrics, based on optimal transport, for comparing time-varying persistence diagrams in this specific applicative case are introduced. We evaluate the relevance of the rankings obtained with these metrics, both qualitatively thanks to a lightweight web visual interface, and quantitatively by studying the deviation from a reference ranking suggested by experts. Extensive experiments show the quantitative superiority of our approach compared to traditional alternatives. Our web interface allows experts to conveniently explore the produced rankings. We show a complete viscous fingering case study demonstrating the utility of our approach in the context of porous media fluid flow, where our framework can be used to automatically discard physically-irrelevant simulation runs from the ensemble and rank the most plausible ones. We document an in-situ implementation to lighten I/O and performance constraints arising in the context of parametric studies.



### Make a Face: Towards Arbitrary High Fidelity Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1908.07191v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07191v1)
- **Published**: 2019-08-20 06:53:55+00:00
- **Updated**: 2019-08-20 06:53:55+00:00
- **Authors**: Shengju Qian, Kwan-Yee Lin, Wayne Wu, Yangxiaokang Liu, Quan Wang, Fumin Shen, Chen Qian, Ran He
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Recent studies have shown remarkable success in face manipulation task with the advance of GANs and VAEs paradigms, but the outputs are sometimes limited to low-resolution and lack of diversity.   In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a novel approach that can arbitrarily manipulate high-resolution face images using a simple yet effective model and only weak supervision of reconstruction and KL divergence losses. First, a novel additive Gaussian Mixture assumption is introduced with an unsupervised clustering mechanism in the structural latent space, which endows better disentanglement and boosts multi-modal representation with external memory. Second, to improve the perceptual quality of synthesized results, two simple strategies in architecture design are further tailored and discussed on the behavior of Human Visual System (HVS) for the first time, allowing for fine control over the model complexity and sample quality. Human opinion studies and new state-of-the-art Inception Score (IS) / Frechet Inception Distance (FID) demonstrate the superiority of our approach over existing algorithms, advancing both the fidelity and extremity of face manipulation task.



### Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images "In the Wild"
- **Arxiv ID**: http://arxiv.org/abs/1908.07201v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07201v2)
- **Published**: 2019-08-20 07:45:57+00:00
- **Updated**: 2019-09-17 13:20:59+00:00
- **Authors**: Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, Michael J. Black
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other. To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. Learning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision. Moreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. We show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. Code and data are available at https://github.com/silviazuffi/smalst.



### SROBB: Targeted Perceptual Loss for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1908.07222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07222v1)
- **Published**: 2019-08-20 08:39:48+00:00
- **Updated**: 2019-08-20 08:39:48+00:00
- **Authors**: Mohammad Saeed Rad, Behzad Bozorgtabar, Urs-Viktor Marti, Max Basler, Hazim Kemal Ekenel, Jean-Philippe Thiran
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: By benefiting from perceptual losses, recent studies have improved significantly the performance of the super-resolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objective functions generate near-photorealistic results, their capability is limited, since they estimate the reconstruction error for an entire image in the same way, without considering any semantic information. In this paper, we propose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based decoder with a targeted objective function that penalizes images at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed approach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and results of extensive user studies.



### Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention
- **Arxiv ID**: http://arxiv.org/abs/1908.07236v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07236v2)
- **Published**: 2019-08-20 09:22:54+00:00
- **Updated**: 2020-03-12 10:02:31+00:00
- **Authors**: Cristian Rodriguez-Opazo, Edison Marrese-Taylor, Fatemeh Sadat Saleh, Hongdong Li, Stephen Gould
- **Comment**: Winter Conference on Applications of Computer Vision 2020
- **Journal**: None
- **Summary**: This paper studies the problem of temporal moment localization in a long untrimmed video using natural language as the query. Given an untrimmed video and a sentence as the query, the goal is to determine the starting, and the ending, of the relevant visual moment in the video, that corresponds to the query sentence. While previous works have tackled this task by a propose-and-rank approach, we introduce a more efficient, end-to-end trainable, and {\em proposal-free approach} that relies on three key components: a dynamic filter to transfer language information to the visual domain, a new loss function to guide our model to attend the most relevant parts of the video, and soft labels to model annotation uncertainty. We evaluate our method on two benchmark datasets, Charades-STA and ActivityNet-Captions. Experimental results show that our approach outperforms state-of-the-art methods on both datasets.



### n-MeRCI: A new Metric to Evaluate the Correlation Between Predictive Uncertainty and True Error
- **Arxiv ID**: http://arxiv.org/abs/1908.07253v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07253v1)
- **Published**: 2019-08-20 09:51:08+00:00
- **Updated**: 2019-08-20 09:51:08+00:00
- **Authors**: Michel Moukari, Loïc Simon, Sylvaine Picard, Frédéric Jurie
- **Comment**: None
- **Journal**: IEEE/RJS International Conference on Intelligent Robots and
  Systems (IROS), In press
- **Summary**: As deep learning applications are becoming more and more pervasive in robotics, the question of evaluating the reliability of inferences becomes a central question in the robotics community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches adapted to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, these approaches are solely evaluated in terms of raw performance of the network prediction, while the quality of their estimated uncertainty is not assessed. Evaluating such uncertainty prediction quality is especially important in robotics, as actions shall depend on the confidence in perceived information. In this context, the main contribution of this article is to propose a novel metric that is adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep neural networks. To experimentally validate this metric, we evaluate it on a toy dataset and then apply it to the task of monocular depth estimation.



### A Neural Virtual Anchor Synthesizer based on Seq2Seq and GAN Models
- **Arxiv ID**: http://arxiv.org/abs/1908.07262v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07262v2)
- **Published**: 2019-08-20 10:27:31+00:00
- **Updated**: 2019-09-12 04:38:58+00:00
- **Authors**: Zipeng Wang, Zhaoxiang Liu, Zezhou Chen, Huan Hu, Shiguo Lian
- **Comment**: Accepted to ISMAR 2019
- **Journal**: None
- **Summary**: This paper presents a novel framework to generate realistic face video of an anchor, who is reading certain news. This task is also known as Virtual Anchor. Given some paragraphs of words, we first utilize a pretrained Word2Vec model to embed each word into a vector; then we utilize a Seq2Seq-based model to translate these word embeddings into action units and head poses of the target anchor; these action units and head poses will be concatenated with facial landmarks as well as the former $n$ synthesized frames, and the concatenation serves as input of a Pix2PixHD-based model to synthesize realistic facial images for the virtual anchor. The experimental results demonstrate our framework is feasible for the synthesis of virtual anchor.



### Deep High-Resolution Representation Learning for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.07919v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07919v2)
- **Published**: 2019-08-20 10:47:46+00:00
- **Updated**: 2020-03-13 13:38:30+00:00
- **Authors**: Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao
- **Comment**: To appear in TPAMI. State-of-the-art performance on human pose
  estimation, semantic segmentation, object detection, instance segmentation,
  and face alignment. Full version of arXiv:1904.04514. (arXiv admin note: text
  overlap with arXiv:1904.04514)
- **Journal**: None
- **Summary**: High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\url{https://github.com/HRNet}}.



### RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes
- **Arxiv ID**: http://arxiv.org/abs/1908.07269v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07269v1)
- **Published**: 2019-08-20 10:54:34+00:00
- **Updated**: 2019-08-20 10:54:34+00:00
- **Authors**: Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y. Chang, Shih-Wei Liao
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Multi-domain image-to-image translation has gained increasing attention recently. Previous methods take an image and some target attributes as inputs and generate an output image with the desired attributes. However, such methods have two limitations. First, these methods assume binary-valued attributes and thus cannot yield satisfactory results for fine-grained control. Second, these methods require specifying the entire set of target attributes, even if most of the attributes would not be changed. To address these limitations, we propose RelGAN, a new method for multi-domain image-to-image translation. The key idea is to use relative attributes, which describes the desired change on selected attributes. Our method is capable of modifying images by changing particular attributes of interest in a continuous manner while preserving the other attributes. Experimental results demonstrate both the quantitative and qualitative effectiveness of our method on the tasks of facial attribute transfer and interpolation.



### Towards High-Resolution Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.07274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07274v1)
- **Published**: 2019-08-20 11:24:02+00:00
- **Updated**: 2019-08-20 11:24:02+00:00
- **Authors**: Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, Huchuan Lu
- **Comment**: Accepted by ICCV2019. The HRSOD dataset is available at
  https://github.com/yi94code/HRSOD
- **Journal**: None
- **Summary**: Deep neural network based methods have made a significant breakthrough in salient object detection. However, they are typically limited to input images with low resolutions ($400\times400$ pixels or less). Little effort has been made to train deep neural networks to directly handle salient object detection in very high-resolution images. This paper pushes forward high-resolution saliency detection, and contributes a new dataset, named High-Resolution Salient Object Detection (HRSOD). To our best knowledge, HRSOD is the first high-resolution saliency detection dataset to date. As another contribution, we also propose a novel approach, which incorporates both global semantic information and local high-resolution details, to address this challenging task. More specifically, our approach consists of a Global Semantic Network (GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network (GLFN). GSN extracts the global semantic information based on down-sampled entire image. Guided by the results of GSN, LRN focuses on some local regions and progressively produces high-resolution predictions. GLFN is further proposed to enforce spatial consistency and boost performance. Experiments illustrate that our method outperforms existing state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable or even better performance than them on widely-used saliency benchmarks. The HRSOD dataset is available at https://github.com/yi94code/HRSOD.



### Non-negative Sparse and Collaborative Representation for Pattern Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.07956v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07956v3)
- **Published**: 2019-08-20 12:54:24+00:00
- **Updated**: 2022-05-12 01:35:23+00:00
- **Authors**: Jun Xu, Zhou Xu, Wangpeng An, Haoqian Wang, David Zhang
- **Comment**: This paper is not maintained anymore, we will not publish it anywhere
- **Journal**: None
- **Summary**: Sparse representation (SR) and collaborative representation (CR) have been successfully applied in many pattern classification tasks such as face recognition. In this paper, we propose a novel Non-negative Sparse and Collaborative Representation (NSCR) for pattern classification. The NSCR representation of each test sample is obtained by seeking a non-negative sparse and collaborative representation vector that represents the test sample as a linear combination of training samples. We observe that the non-negativity can make the SR and CR more discriminative and effective for pattern classification. Based on the proposed NSCR, we propose a NSCR based classifier for pattern classification. Extensive experiments on benchmark datasets demonstrate that the proposed NSCR based classifier outperforms the previous SR or CR based approach, as well as state-of-the-art deep approaches, on diverse challenging pattern classification tasks.



### Instance Scale Normalization for image understanding
- **Arxiv ID**: http://arxiv.org/abs/1908.07323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07323v2)
- **Published**: 2019-08-20 13:12:33+00:00
- **Updated**: 2020-06-10 01:42:50+00:00
- **Authors**: Zewen He, He Huang, Yudong Wu, Guan Huang, Wensheng Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Scale variation remains a challenging problem for object detection. Common paradigms usually adopt multiscale training & testing (image pyramid) or FPN (feature pyramid network) to process objects in a wide scale range. However, multi-scale methods aggravate more variations of scale that even deep convolution neural networks with FPN cannot handle well. In this work, we propose an innovative paradigm called Instance Scale Normalization (ISN) to resolve the above problem. ISN compresses the scale space of objects into a consistent range (ISN range), in both training and testing phases. This reassures the problem of scale variation fundamentally and reduces the difficulty of network optimization. Experiments show that ISN surpasses multi-scale counterpart significantly for object detection, instance segmentation, and multi-task human pose estimation, on several architectures. On COCO test-dev, our single model based on ISN achieves 46.5 mAP with a ResNet-101 backbone, which is among the state-of-the-art (SOTA) candidates for object detection.



### Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.07325v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07325v1)
- **Published**: 2019-08-20 13:19:28+00:00
- **Updated**: 2019-08-20 13:19:28+00:00
- **Authors**: Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, Liang Lin
- **Comment**: accepted by ICCV 2019
- **Journal**: None
- **Summary**: Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.



### Unsupervised Multi-modal Style Transfer for Cardiac MR Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.07344v3
- **DOI**: 10.1007/978-3-030-39074-7_22
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07344v3)
- **Published**: 2019-08-20 13:47:42+00:00
- **Updated**: 2019-11-09 15:32:08+00:00
- **Authors**: Chen Chen, Cheng Ouyang, Giacomo Tarroni, Jo Schlemper, Huaqi Qiu, Wenjia Bai, Daniel Rueckert
- **Comment**: STACOM 2019 camera-ready. Winner of Multi-sequence Cardiac MR
  Segmentation Challenge (MS-CMRSeg 2019) https://zmiclab.github.io/mscmrseg19/
- **Journal**: None
- **Summary**: In this work, we present a fully automatic method to segment cardiac structures from late-gadolinium enhanced (LGE) images without using labelled LGE data for training, but instead by transferring the anatomical knowledge and features learned on annotated balanced steady-state free precession (bSSFP) images, which are easier to acquire. Our framework mainly consists of two neural networks: a multi-modal image translation network for style transfer and a cascaded segmentation network for image segmentation. The multi-modal image translation network generates realistic and diverse synthetic LGE images conditioned on a single annotated bSSFP image, forming a synthetic LGE training set. This set is then utilized to fine-tune the segmentation network pre-trained on labelled bSSFP images, achieving the goal of unsupervised LGE image segmentation. In particular, the proposed cascaded segmentation network is able to produce accurate segmentation by taking both shape prior and image appearance into account, achieving an average Dice score of 0.92 for the left ventricle, 0.83 for the myocardium, and 0.88 for the right ventricle on the test set.



### A Novel method for IDC Prediction in Breast Cancer Histopathology images using Deep Residual Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.07362v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07362v2)
- **Published**: 2019-08-20 14:01:14+00:00
- **Updated**: 2019-11-11 07:48:32+00:00
- **Authors**: Chandra Churh Chatterjee, Gopal Krishna
- **Comment**: Accepted at 2nd International Conference on Intelligent Communication
  and Computational Techniques,2019
- **Journal**: None
- **Summary**: Invasive ductal carcinoma (IDC), which is also sometimes known as the infiltrating ductal carcinoma, is the most regular form of breast cancer. It accounts for about 80% of all breast cancers. According to the American Cancer Society, more than 180,000 women in the United States are diagnosed with invasive breast cancer each year. The survival rate associated with this form of cancer is about 77% to 93% depending on the stage at which they are being diagnosed. The invasiveness and the frequency of the occurrence of these disease makes it one of the difficult cancers to be diagnosed. Our proposed methodology involves diagnosing the invasive ductal carcinoma with a deep residual convolution network to classify the IDC affected histopathological images from the normal images. The dataset for the purpose used is a benchmark dataset known as the Breast Histopathology Images. The microscopic RGB images are converted into a seven channel image matrix, which is then fed to the network. The proposed model produces a 99.29% accurate approach towards the prediction of IDC in the histopathology images with an AUROC score of 0.9996. Classification ability of the model is tested using standard performance metrics.



### Blind Image Deconvolution using Pretrained Generative Priors
- **Arxiv ID**: http://arxiv.org/abs/1908.07404v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07404v1)
- **Published**: 2019-08-20 14:49:45+00:00
- **Updated**: 2019-08-20 14:49:45+00:00
- **Authors**: Muhammad Asim, Fahad Shamshad, Ali Ahmed
- **Comment**: Accepted in BMVC 2019. Extended version of this paper can be found at
  arXiv:1802.04073
- **Journal**: None
- **Summary**: This paper proposes a novel approach to regularize the ill-posed blind image deconvolution (blind image deblurring) problem using deep generative networks. We employ two separate deep generative models - one trained to produce sharp images while the other trained to generate blur kernels from lower dimensional parameters. To deblur, we propose an alternating gradient descent scheme operating in the latent lower-dimensional space of each of the pretrained generative models. Our experiments show excellent deblurring results even under large blurs and heavy noise. To improve the performance on rich image datasets not well learned by the generative networks, we present a modification of the proposed scheme that governs the deblurring process under both generative and classical priors.



### ViSiL: Fine-grained Spatio-Temporal Video Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.07410v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1908.07410v1)
- **Published**: 2019-08-20 15:06:24+00:00
- **Updated**: 2019-08-20 15:06:24+00:00
- **Authors**: Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis Kompatsiaris
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we introduce ViSiL, a Video Similarity Learning architecture that considers fine-grained Spatio-Temporal relations between pairs of videos -- such relations are typically lost in previous video retrieval approaches that embed the whole frame or even the whole video into a vector descriptor before the similarity estimation. By contrast, our Convolutional Neural Network (CNN)-based approach is trained to calculate video-to-video similarity from refined frame-to-frame similarity matrices, so as to consider both intra- and inter-frame relations. In the proposed method, pairwise frame similarity is estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on regional CNN frame features - this avoids feature aggregation before the similarity calculation between frames. Subsequently, the similarity matrix between all video frames is fed to a four-layer CNN, and then summarized using Chamfer Similarity (CS) into a video-to-video similarity score -- this avoids feature aggregation before the similarity calculation between videos and captures the temporal similarity patterns between matching frame sequences. We train the proposed network using a triplet loss scheme and evaluate it on five public benchmark datasets on four different video retrieval problems where we demonstrate large improvements in comparison to the state of the art. The implementation of ViSiL is publicly available.



### Human activity recognition from skeleton poses
- **Arxiv ID**: http://arxiv.org/abs/1908.08928v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08928v1)
- **Published**: 2019-08-20 15:12:19+00:00
- **Updated**: 2019-08-20 15:12:19+00:00
- **Authors**: Frederico Belmonte Klein, Angelo Cangelosi
- **Comment**: 8 pages, 1 figure
- **Journal**: None
- **Summary**: Human Action Recognition is an important task of Human Robot Interaction as cooperation between robots and humans requires that artificial agents recognise complex cues from the environment. A promising approach is using trained classifiers to recognise human actions through sequences of skeleton poses extracted from images or RGB-D data from a sensor. However, with many different data-sets focused on slightly different sets of actions and different algorithms it is not clear which strategy produces highest accuracy for indoor activities performed in a home environment. This work discussed, tested and compared classic algorithms, namely, support vector machines and k-nearest neighbours, to 2 similar hierarchical neural gas approaches, the growing when required neural gas and the growing neural gas.



### Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control
- **Arxiv ID**: http://arxiv.org/abs/1908.07423v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07423v2)
- **Published**: 2019-08-20 15:14:54+00:00
- **Updated**: 2020-12-16 11:33:08+00:00
- **Authors**: Yu-Wei Chao, Jimei Yang, Weifeng Chen, Jia Deng
- **Comment**: Accepted to AAAI 2021
- **Journal**: None
- **Summary**: Recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis, through imitating motion capture data via deep reinforcement learning. However, results have mostly been demonstrated on imitating a single distinct motion pattern, and do not generalize to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To bridge this gap, we focus on one class of interactive tasks -- sitting onto a chair. We propose a hierarchical reinforcement learning framework which relies on a collection of subtask controllers trained to imitate simple, reusable mocap motions, and a meta controller trained to execute the subtasks properly to complete the main task. We experimentally demonstrate the strength of our approach over different non-hierarchical and hierarchical baselines. We also show that our approach can be applied to motion prediction given an image input. A supplementary video can be found at https://youtu.be/3CeN0OGz2cA.



### DefSLAM: Tracking and Mapping of Deforming Scenes from Monocular Sequences
- **Arxiv ID**: http://arxiv.org/abs/1908.08918v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08918v2)
- **Published**: 2019-08-20 15:27:47+00:00
- **Updated**: 2020-08-25 17:54:25+00:00
- **Authors**: Jose Lamarca, Shaifali Parashar, Adrien Bartoli, J. M. M. Montiel
- **Comment**: Experiments results: https://www.youtube.com/watch?v=6mmhD2_t6Gs ;
  More Results:
  https://www.youtube.com/playlist?list=PLKBuKNhAV30SlKGJ9eaMlAExdWRypUy-K
- **Journal**: None
- **Summary**: Monocular SLAM algorithms perform robustly when observing rigid scenes, however, they fail when the observed scene deforms, for example, in medical endoscopy applications. We present DefSLAM, the first monocular SLAM capable of operating in deforming scenes in real-time. Our approach intertwines Shape-from-Template (SfT) and Non-Rigid Structure-from-Motion (NRSfM) techniques to deal with the exploratory sequences typical of SLAM. A deformation tracking thread recovers the pose of the camera and the deformation of the observed map, at frame rate, by means of SfT processing a template that models the scene shape-at-rest. A deformation mapping thread runs in parallel with the tracking to update the template, at keyframe rate, by means of an isometric NRSfM processing a batch of full perspective keyframes. In our experiments, DefSLAM processes close-up sequences of deforming scenes, both in a laboratory controlled experiment and in medical endoscopy sequences, producing accurate 3D models of the scene with respect to the moving camera.



### Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.07433v1
- **DOI**: 10.1109/ICCV.2019.00776
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07433v1)
- **Published**: 2019-08-20 15:34:13+00:00
- **Updated**: 2019-08-20 15:34:13+00:00
- **Authors**: Kiru Park, Timothy Patten, Markus Vincze
- **Comment**: Accepted at ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.



### Multi-Modal Recognition of Worker Activity for Human-Centered Intelligent Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/1908.07519v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1908.07519v1)
- **Published**: 2019-08-20 15:46:07+00:00
- **Updated**: 2019-08-20 15:46:07+00:00
- **Authors**: Wenjin Tao, Ming C. Leu, Zhaozheng Yin
- **Comment**: 17 pages, 8 figures, 6 tables
- **Journal**: None
- **Summary**: In a human-centered intelligent manufacturing system, sensing and understanding of the worker's activity are the primary tasks. In this paper, we propose a novel multi-modal approach for worker activity recognition by leveraging information from different sensors and in different modalities. Specifically, a smart armband and a visual camera are applied to capture Inertial Measurement Unit (IMU) signals and videos, respectively. For the IMU signals, we design two novel feature transform mechanisms, in both frequency and spatial domains, to assemble the captured IMU signals as images, which allow using convolutional neural networks to learn the most discriminative features. Along with the above two modalities, we propose two other modalities for the video data, at the video frame and video clip levels, respectively. Each of the four modalities returns a probability distribution on activity prediction. Then, these probability distributions are fused to output the worker activity classification result. A worker activity dataset of 6 activities is established, which at present contains 6 common activities in assembly tasks, i.e., grab a tool/part, hammer a nail, use a power-screwdriver, rest arms, turn a screwdriver, and use a wrench. The developed multi-modal approach is evaluated on this dataset and achieves recognition accuracies as high as 97% and 100% in the leave-one-out and half-half experiments, respectively.



### Playing magic tricks to deep neural networks untangles human deception
- **Arxiv ID**: http://arxiv.org/abs/1908.07446v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07446v1)
- **Published**: 2019-08-20 15:50:11+00:00
- **Updated**: 2019-08-20 15:50:11+00:00
- **Authors**: Regina Zaghi-Lara, Miguel Ángel Gea, Jordi Camí, Luis M. Martínez, Alex Gomez-Marin
- **Comment**: None
- **Journal**: None
- **Summary**: Magic is the art of producing in the spectator an illusion of impossibility. Although the scientific study of magic is in its infancy, the advent of recent tracking algorithms based on deep learning allow now to quantify the skills of the magician in naturalistic conditions at unprecedented resolution and robustness. In this study, we deconstructed stage magic into purely motor maneuvers and trained an artificial neural network (DeepLabCut) to follow coins as a professional magician made them appear and disappear in a series of tricks. Rather than using AI as a mere tracking tool, we conceived it as an "artificial spectator". When the coins were not visible, the algorithm was trained to infer their location as a human spectator would (i.e. in the left fist). This created situations where the human was fooled while AI (as seen by a human) was not, and vice versa. Magic from the perspective of the machine reveals our own cognitive biases.



### Probabilistic Reconstruction Networks for 3D Shape Inference from a Single Image
- **Arxiv ID**: http://arxiv.org/abs/1908.07475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07475v1)
- **Published**: 2019-08-20 16:28:10+00:00
- **Updated**: 2019-08-20 16:28:10+00:00
- **Authors**: Roman Klokov, Jakob Verbeek, Edmond Boyer
- **Comment**: Accepted as Oral at BMVC'19
- **Journal**: None
- **Summary**: We study end-to-end learning strategies for 3D shape inference from images, in particular from a single image. Several approaches in this direction have been investigated that explore different shape representations and suitable learning architectures. We focus instead on the underlying probabilistic mechanisms involved and contribute a more principled probabilistic inference-based reconstruction framework, which we coin Probabilistic Reconstruction Networks. This framework expresses image conditioned 3D shape inference through a family of latent variable models, and naturally decouples the choice of shape representations from the inference itself. Moreover, it suggests different options for the image conditioning and allows training in two regimes, using either Monte Carlo or variational approximation of the marginal likelihood. Using our Probabilistic Reconstruction Networks we obtain single image 3D reconstruction results that set a new state of the art on the ShapeNet dataset in terms of the intersection over union and earth mover's distance evaluation metrics. Interestingly, we obtain these results using a basic voxel grid representation, improving over recent work based on finer point cloud or mesh based representations.



### LXMERT: Learning Cross-Modality Encoder Representations from Transformers
- **Arxiv ID**: http://arxiv.org/abs/1908.07490v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07490v3)
- **Published**: 2019-08-20 17:05:18+00:00
- **Updated**: 2019-12-03 19:30:19+00:00
- **Authors**: Hao Tan, Mohit Bansal
- **Comment**: EMNLP 2019 (14 pages; with new attention visualizations)
- **Journal**: None
- **Summary**: Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert



### Image Synthesis From Reconfigurable Layout and Style
- **Arxiv ID**: http://arxiv.org/abs/1908.07500v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.07500v1)
- **Published**: 2019-08-20 17:22:31+00:00
- **Updated**: 2019-08-20 17:22:31+00:00
- **Authors**: Wei Sun, Tianfu Wu
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long-standing problem to learn generative models that are capable of synthesizing realistic and sharp images from reconfigurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at \url{https://github.com/iVMCL/LostGANs}.



### Resolving 3D Human Pose Ambiguities with 3D Scene Constraints
- **Arxiv ID**: http://arxiv.org/abs/1908.06963v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06963v1)
- **Published**: 2019-08-20 17:23:00+00:00
- **Updated**: 2019-08-20 17:23:00+00:00
- **Authors**: Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, Michael J. Black
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.



### Phrase Localization Without Paired Training Examples
- **Arxiv ID**: http://arxiv.org/abs/1908.07553v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.07553v1)
- **Published**: 2019-08-20 18:07:37+00:00
- **Updated**: 2019-08-20 18:07:37+00:00
- **Authors**: Josiah Wang, Lucia Specia
- **Comment**: Accepted for oral presentation at the IEEE/CVF International
  Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: Localizing phrases in images is an important part of image understanding and can be useful in many applications that require mappings between textual and visual information. Existing work attempts to learn these mappings from examples of phrase-image region correspondences (strong supervision) or from phrase-image pairs (weak supervision). We postulate that such paired annotations are unnecessary, and propose the first method for the phrase localization problem where neither training procedure nor paired, task-specific data is required. Our method is simple but effective: we use off-the-shelf approaches to detect objects, scenes and colours in images, and explore different approaches to measure semantic similarity between the categories of detected visual elements and words in phrases. Experiments on two well-known phrase localization datasets show that this approach surpasses all weakly supervised methods by a large margin and performs very competitively to strongly supervised methods, and can thus be considered a strong baseline to the task. The non-paired nature of our method makes it applicable to any domain and where no paired phrase localization annotation is available.



### More unlabelled data or label more data? A study on semi-supervised laparoscopic image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.08035v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08035v1)
- **Published**: 2019-08-20 20:54:58+00:00
- **Updated**: 2019-08-20 20:54:58+00:00
- **Authors**: Yunguan Fu, Maria R. Robu, Bongjin Koo, Crispin Schneider, Stijn van Laarhoven, Danail Stoyanov, Brian Davidson, Matthew J. Clarkson, Yipeng Hu
- **Comment**: Accepted to MICCAI MIL3ID 2019
- **Journal**: None
- **Summary**: Improving a semi-supervised image segmentation task has the option of adding more unlabelled images, labelling the unlabelled images or combining both, as neither image acquisition nor expert labelling can be considered trivial in most clinical applications. With a laparoscopic liver image segmentation application, we investigate the performance impact by altering the quantities of labelled and unlabelled training data, using a semi-supervised segmentation algorithm based on the mean teacher learning paradigm. We first report a significantly higher segmentation accuracy, compared with supervised learning. Interestingly, this comparison reveals that the training strategy adopted in the semi-supervised algorithm is also responsible for this observed improvement, in addition to the added unlabelled data. We then compare different combinations of labelled and unlabelled data set sizes for training semi-supervised segmentation networks, to provide a quantitative example of the practically useful trade-off between the two data planning strategies in this surgical guidance application.



### 2D moment invariants from the point of view of the classical invariant theory
- **Arxiv ID**: http://arxiv.org/abs/1908.08927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08927v2)
- **Published**: 2019-08-20 20:55:00+00:00
- **Updated**: 2019-09-01 20:39:39+00:00
- **Authors**: Leonid Bedratyuk
- **Comment**: 20 pages
- **Journal**: None
- **Summary**: Invariants allow to classify images up to the action of a group of transformations. In this paper we introduce notions of the algebras of simultaneous polynomial and rational 2D moment invariants and prove that they are isomorphic to the algebras of joint polynomial and rational $SO(2)$-invariants of binary forms. Also, to simplify the calculating of invariants we pass from an action of Lie group $SO(2)$ to an action of its Lie algebra $\mathfrak{so}_2$. This allow us to reduce the problem to standard problems of the classical invariant theory.



### Joint Motion Estimation and Segmentation from Undersampled Cardiac MR Image
- **Arxiv ID**: http://arxiv.org/abs/1908.07623v1
- **DOI**: 10.1007/978-3-030-00129-2_7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07623v1)
- **Published**: 2019-08-20 21:54:01+00:00
- **Updated**: 2019-08-20 21:54:01+00:00
- **Authors**: Chen Qin, Wenjia Bai, Jo Schlemper, Steffen E. Petersen, Stefan K. Piechnik, Stefan Neubauer, Daniel Rueckert
- **Comment**: This work is published at MLMIR 2018: Machine Learning for Medical
  Image Reconstruction
- **Journal**: None
- **Summary**: Accelerating the acquisition of magnetic resonance imaging (MRI) is a challenging problem, and many works have been proposed to reconstruct images from undersampled k-space data. However, if the main purpose is to extract certain quantitative measures from the images, perfect reconstructions may not always be necessary as long as the images enable the means of extracting the clinically relevant measures. In this paper, we work on jointly predicting cardiac motion estimation and segmentation directly from undersampled data, which are two important steps in quantitatively assessing cardiac function and diagnosing cardiovascular diseases. In particular, a unified model consisting of both motion estimation branch and segmentation branch is learned by optimising the two tasks simultaneously. Additional corresponding fully-sampled images are incorporated into the network as a parallel sub-network to enhance and guide the learning during the training process. Experimental results using cardiac MR images from 220 subjects show that the proposed model is robust to undersampled data and is capable of predicting results that are close to that from fully-sampled ones, while bypassing the usual image reconstruction stage.



### Action recognition with spatial-temporal discriminative filter banks
- **Arxiv ID**: http://arxiv.org/abs/1908.07625v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07625v1)
- **Published**: 2019-08-20 21:57:32+00:00
- **Updated**: 2019-08-20 21:57:32+00:00
- **Authors**: Brais Martinez, Davide Modolo, Yuanjun Xiong, Joseph Tighe
- **Comment**: ICCV 2019 Accepted Paper
- **Journal**: None
- **Summary**: Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or they explore different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we show that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.



### P2L: Predicting Transfer Learning for Images and Semantic Relations
- **Arxiv ID**: http://arxiv.org/abs/1908.07630v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.07630v2)
- **Published**: 2019-08-20 22:09:40+00:00
- **Updated**: 2020-10-15 20:08:59+00:00
- **Authors**: Bishwaranjan Bhattacharjee, John R. Kender, Matthew Hill, Parijat Dube, Siyu Huo, Michael R. Glass, Brian Belgodere, Sharath Pankanti, Noel Codella, Patrick Watson
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: Transfer learning enhances learning across tasks, by leveraging previously learned representations -- if they are properly chosen. We describe an efficient method to accurately estimate the appropriateness of a previously trained model for use in a new learning task. We use this measure, which we call "Predict To Learn" ("P2L"), in the two very different domains of images and semantic relations, where it predicts, from a set of "source" models, the one model most likely to produce effective transfer for training a given "target" model. We validate our approach thoroughly, by assembling a collection of candidate source models, then fine-tuning each candidate to perform each of a collection of target tasks, and finally measuring how well transfer has been enhanced. Across 95 tasks within multiple domains (images classification and semantic relations), the P2L approach was able to select the best transfer learning model on average, while the heuristic of choosing model trained with the largest data set selected the best model in only 55 cases. These results suggest that P2L captures important information in common between source and target tasks, and that this shared informational structure contributes to successful transfer learning more than simple data size.



### On Object Symmetries and 6D Pose Estimation from Images
- **Arxiv ID**: http://arxiv.org/abs/1908.07640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07640v1)
- **Published**: 2019-08-20 22:44:24+00:00
- **Updated**: 2019-08-20 22:44:24+00:00
- **Authors**: Giorgia Pitteri, Michaël Ramamonjisoa, Slobodan Ilic, Vincent Lepetit
- **Comment**: International Conference on 3D Vision
- **Journal**: None
- **Summary**: Objects with symmetries are common in our daily life and in industrial contexts, but are often ignored in the recent literature on 6D pose estimation from images. In this paper, we study in an analytical way the link between the symmetries of a 3D object and its appearance in images. We explain why symmetrical objects can be a challenge when training machine learning algorithms that aim at estimating their 6D pose from images. We propose an efficient and simple solution that relies on the normalization of the pose rotation. Our approach is general and can be used with any 6D pose estimation algorithm. Moreover, our method is also beneficial for objects that are 'almost symmetrical', i.e. objects for which only a detail breaks the symmetry. We validate our approach within a Faster-RCNN framework on a synthetic dataset made with objects from the T-Less dataset, which exhibit various types of symmetries, as well as real sequences from T-Less.



### Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.08926v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.08926v1)
- **Published**: 2019-08-20 23:26:04+00:00
- **Updated**: 2019-08-20 23:26:04+00:00
- **Authors**: Bichen Wu
- **Comment**: Ph.D. dissertation of Bichen Wu
- **Journal**: None
- **Summary**: The success of deep neural networks (DNNs) is attributable to three factors: increased compute capacity, more complex models, and more data. These factors, however, are not always present, especially for edge applications such as autonomous driving, augmented reality, and internet-of-things. Training DNNs requires a large amount of data, which is difficult to obtain. Edge devices such as mobile phones have limited compute capacity, and therefore, require specialized and efficient DNNs. However, due to the enormous design space and prohibitive training costs, designing efficient DNNs for different target devices is challenging. So the question is, with limited data, compute capacity, and model complexity, can we still successfully apply deep neural networks?   This dissertation focuses on the above problems and improving the efficiency of deep neural networks at four levels. Model efficiency: we designed neural networks for various computer vision tasks and achieved more than 10x faster speed and lower energy. Data efficiency: we developed an advanced tool that enables 6.2x faster annotation of a LiDAR point cloud. We also leveraged domain adaptation to utilize simulated data, bypassing the need for real data. Hardware efficiency: we co-designed neural networks and hardware accelerators and achieved 11.6x faster inference. Design efficiency: the process of finding the optimal neural networks is time-consuming. Our automated neural architecture search algorithms discovered, using 421x lower computational cost than previous search methods, models with state-of-the-art accuracy and efficiency.



### Sparse Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1908.08930v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.08930v1)
- **Published**: 2019-08-20 23:35:41+00:00
- **Updated**: 2019-08-20 23:35:41+00:00
- **Authors**: Shahin Mahdizadehaghdam, Ashkan Panahi, Hamid Krim
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: We propose a new approach to Generative Adversarial Networks (GANs) to achieve an improved performance with additional robustness to its so-called and well recognized mode collapse. We first proceed by mapping the desired data onto a frame-based space for a sparse representation to lift any limitation of small support features prior to learning the structure. To that end we start by dividing an image into multiple patches and modifying the role of the generative network from producing an entire image, at once, to creating a sparse representation vector for each image patch. We synthesize an entire image by multiplying generated sparse representations to a pre-trained dictionary and assembling the resulting patches. This approach restricts the output of the generator to a particular structure, obtained by imposing a Union of Subspaces (UoS) model to the original training data, leading to more realistic images, while maintaining a desired diversity. To further regularize GANs in generating high-quality images and to avoid the notorious mode-collapse problem, we introduce a third player in GANs, called reconstructor. This player utilizes an auto-encoding scheme to ensure that first, the input-output relation in the generator is injective and second each real image corresponds to some input noise. We present a number of experiments, where the proposed algorithm shows a remarkably higher inception score compared to the equivalent conventional GANs.



### Saccader: Improving Accuracy of Hard Attention Models for Vision
- **Arxiv ID**: http://arxiv.org/abs/1908.07644v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.07644v3)
- **Published**: 2019-08-20 23:40:21+00:00
- **Updated**: 2019-12-07 00:34:57+00:00
- **Authors**: Gamaleldin F. Elsayed, Simon Kornblith, Quoc V. Le
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Journal**: None
- **Summary**: Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textit{hard attention}, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving $75\%$ top-1 and $91\%$ top-5 while attending to less than one-third of the image.



### Communal Domain Learning for Registration in Drifted Image Spaces
- **Arxiv ID**: http://arxiv.org/abs/1908.07646v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.07646v1)
- **Published**: 2019-08-20 23:50:56+00:00
- **Updated**: 2019-08-20 23:50:56+00:00
- **Authors**: Awais Mansoor, Marius George Linguraru
- **Comment**: MLMI-2019
- **Journal**: None
- **Summary**: Designing a registration framework for images that do not share the same probability distribution is a major challenge in modern image analytics yet trivial task for the human visual system (HVS). Discrepancies in probability distributions, also known as \emph{drifts}, can occur due to various reasons including, but not limited to differences in sequences and modalities (e.g., MRI T1-T2 and MRI-CT registration), or acquisition settings (e.g., multisite, inter-subject, or intra-subject registrations). The popular assumption about the working of HVS is that it exploits a communal feature subspace exists between the registering images or fields-of-view that encompasses key drift-invariant features. Mimicking the approach that is potentially adopted by the HVS, herein, we present a representation learning technique of this invariant communal subspace that is shared by registering domains. The proposed communal domain learning (CDL) framework uses a set of hierarchical nonlinear transforms to learn the communal subspace that minimizes the probability differences and maximizes the amount of shared information between the registering domains. Similarity metric and parameter optimization calculations for registration are subsequently performed in the drift-minimized learned communal subspace. This generic registration framework is applied to register multisequence (MR: T1, T2) and multimodal (MR, CT) images. Results demonstrated generic applicability, consistent performance, and statistically significant improvement for both multi-sequence and multi-modal data using the proposed approach ($p$-value$<0.001$; Wilcoxon rank sum test) over baseline methods.



