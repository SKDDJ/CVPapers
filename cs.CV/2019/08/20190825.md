# Arxiv Papers in cs.CV on 2019-08-25
### Multi-Channel Neural Network for Assessing Neonatal Pain from Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.09254v1
- **DOI**: 10.1109/SMC.2019.8914537
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09254v1)
- **Published**: 2019-08-25 05:51:54+00:00
- **Updated**: 2019-08-25 05:51:54+00:00
- **Authors**: Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho, Yu Sun
- **Comment**: Accepted to IEEE SMC 2019
- **Journal**: None
- **Summary**: Neonates do not have the ability to either articulate pain or communicate it non-verbally by pointing. The current clinical standard for assessing neonatal pain is intermittent and highly subjective. This discontinuity and subjectivity can lead to inconsistent assessment, and therefore, inadequate treatment. In this paper, we propose a multi-channel deep learning framework for assessing neonatal pain from videos. The proposed framework integrates information from two pain indicators or channels, namely facial expression and body movement, using convolutional neural network (CNN). It also integrates temporal information using a recurrent neural network (LSTM). The experimental results prove the efficiency and superiority of the proposed temporal and multi-channel framework as compared to existing similar methods.



### Recon-GLGAN: A Global-Local context based Generative Adversarial Network for MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.09262v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09262v1)
- **Published**: 2019-08-25 07:07:35+00:00
- **Updated**: 2019-08-25 07:07:35+00:00
- **Authors**: Balamurali Murugesan, Vijaya Raghavan S, Kaushik Sarveswaran, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Accepted at MLMIR-MICCAIW 2019
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) is one of the best medical imaging modalities as it offers excellent spatial resolution and soft-tissue contrast. But, the usage of MRI is limited by its slow acquisition time, which makes it expensive and causes patient discomfort. In order to accelerate the acquisition, multiple deep learning networks have been proposed. Recently, Generative Adversarial Networks (GANs) have shown promising results in MRI reconstruction. The drawback with the proposed GAN based methods is it does not incorporate the prior information about the end goal which could help in better reconstruction. For instance, in the case of cardiac MRI, the physician would be interested in the heart region which is of diagnostic relevance while excluding the peripheral regions. In this work, we show that incorporating prior information about a region of interest in the model would offer better performance. Thereby, we propose a novel GAN based architecture, Reconstruction Global-Local GAN (Recon-GLGAN) for MRI reconstruction. The proposed model contains a generator and a context discriminator which incorporates global and local contextual information from images. Our model offers significant performance improvement over the baseline models. Our experiments show that the concept of a context discriminator can be extended to existing GAN based reconstruction models to offer better performance. We also demonstrate that the reconstructions from the proposed method give segmentation results similar to fully sampled images.



### Texture and Structure Two-view Classification of Images
- **Arxiv ID**: http://arxiv.org/abs/1908.09264v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09264v1)
- **Published**: 2019-08-25 07:13:25+00:00
- **Updated**: 2019-08-25 07:13:25+00:00
- **Authors**: Samah Khawaled, Michael Zibulevsky, Yehoshua Y. Zeevi
- **Comment**: None
- **Journal**: None
- **Summary**: Textural and structural features can be regraded as "two-view" feature sets. Inspired by the recent progress in multi-view learning, we propose a novel two-view classification method that models each feature set and optimizes the process of merging these views efficiently. Examples of implementation of this approach in classification of real-world data are presented, with special emphasis on medical images. We firstly decompose fully-textured images into two layers of representation, corresponding to natural stochastic textures (NST) and structural layer, respectively. The structural, edge-and-curve-type, information is mostly represented by the local spatial phase, whereas, the pure NST has random phase and is characterized by Gaussianity and self-similarity. Therefore, the NST is modeled by the 2D self-similar process, fractional Brownian motion (fBm). The Hurst parameter, characteristic of fBm, specifies the roughness or irregularity of the texture. This leads us to its estimation and implementation along other features extracted from the structure layer, to build the "two-view" features sets used in our classification scheme. A shallow neural net (NN) is exploited to execute the process of merging these feature sets, in a straightforward and efficient manner.



### Principal Component Analysis Using Structural Similarity Index for Images
- **Arxiv ID**: http://arxiv.org/abs/1908.09287v1
- **DOI**: 10.1007/978-3-030-27202-9_7
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09287v1)
- **Published**: 2019-08-25 09:18:03+00:00
- **Updated**: 2019-08-25 09:18:03+00:00
- **Authors**: Benyamin Ghojogh, Fakhri Karray, Mark Crowley
- **Comment**: Paper for the methods named "Image Structural Component Analysis
  (ISCA)" and "Kernel Image Structural Component Analysis (Kernel ISCA)"
- **Journal**: International Conference on Image Analysis and Recognition,
  Springer, pp. 77-88, 2019
- **Summary**: Despite the advances of deep learning in specific tasks using images, the principled assessment of image fidelity and similarity is still a critical ability to develop. As it has been shown that Mean Squared Error (MSE) is insufficient for this task, other measures have been developed with one of the most effective being Structural Similarity Index (SSIM). Such measures can be used for subspace learning but existing methods in machine learning, such as Principal Component Analysis (PCA), are based on Euclidean distance or MSE and thus cannot properly capture the structural features of images. In this paper, we define an image structure subspace which discriminates different types of image distortions. We propose Image Structural Component Analysis (ISCA) and also kernel ISCA by using SSIM, rather than Euclidean distance, in the formulation of PCA. This paper provides a bridge between image quality assessment and manifold learning opening a broad new area for future research.



### Locally Linear Image Structural Embedding for Image Structure Manifold Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.09288v1
- **DOI**: 10.1007/978-3-030-27202-9_11
- **Categories**: **stat.ML**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09288v1)
- **Published**: 2019-08-25 09:32:45+00:00
- **Updated**: 2019-08-25 09:32:45+00:00
- **Authors**: Benyamin Ghojogh, Fakhri Karray, Mark Crowley
- **Comment**: This is the paper for the methods named "Locally Linear Image
  Structural Embedding (LLISE)" and "Kernel Locally Linear Image Structural
  Embedding (Kernel LLISE)"
- **Journal**: International Conference on Image Analysis and Recognition,
  Springer, pp. 126-138, 2019
- **Summary**: Most of existing manifold learning methods rely on Mean Squared Error (MSE) or $\ell_2$ norm. However, for the problem of image quality assessment, these are not promising measure. In this paper, we introduce the concept of an image structure manifold which captures image structure features and discriminates image distortions. We propose a new manifold learning method, Locally Linear Image Structural Embedding (LLISE), and kernel LLISE for learning this manifold. The LLISE is inspired by Locally Linear Embedding (LLE) but uses SSIM rather than MSE. This paper builds a bridge between manifold learning and image fidelity assessment and it can open a new area for future investigations.



### Adversarial Convolutional Networks with Weak Domain-Transfer for Multi-Sequence Cardiac MR Images Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.09298v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09298v2)
- **Published**: 2019-08-25 10:43:55+00:00
- **Updated**: 2019-08-28 14:37:23+00:00
- **Authors**: Jingkun Chen, Hongwei Li, Jianguo Zhang, Bjoern Menze
- **Comment**: 9 pages, 4 figures, conference
- **Journal**: None
- **Summary**: Analysis and modeling of the ventricles and myocardium are important in the diagnostic and treatment of heart diseases. Manual delineation of those tissues in cardiac MR (CMR) scans is laborious and time-consuming. The ambiguity of the boundaries makes the segmentation task rather challenging. Furthermore, the annotations on some modalities such as Late Gadolinium Enhancement (LGE) MRI, are often not available. We propose an end-to-end segmentation framework based on convolutional neural network (CNN) and adversarial learning. A dilated residual U-shape network is used as a segmentor to generate the prediction mask; meanwhile, a CNN is utilized as a discriminator model to judge the segmentation quality. To leverage the available annotations across modalities per patient, a new loss function named weak domain-transfer loss is introduced to the pipeline. The proposed model is evaluated on the public dataset released by the challenge organizer in MICCAI 2019, which consists of 45 sets of multi-sequence CMR images. We demonstrate that the proposed adversarial pipeline outperforms baseline deep-learning methods.



### A Comparison of CNN and Classic Features for Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1908.09300v1
- **DOI**: 10.1109/CBMI.2019.8877470
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09300v1)
- **Published**: 2019-08-25 11:20:33+00:00
- **Updated**: 2019-08-25 11:20:33+00:00
- **Authors**: Umut Özaydın, Theodoros Georgiou, Michael Lew
- **Comment**: 5 pages, 3 figures, 3 tables, CBMI 2019
- **Journal**: None
- **Summary**: Feature detectors and descriptors have been successfully used for various computer vision tasks, such as video object tracking and content-based image retrieval. Many methods use image gradients in different stages of the detection-description pipeline to describe local image structures. Recently, some, or all, of these stages have been replaced by convolutional neural networks (CNNs), in order to increase their performance. A detector is defined as a selection problem, which makes it more challenging to implement as a CNN. They are therefore generally defined as regressors, converting input images to score maps and keypoints can be selected with non-maximum suppression. This paper discusses and compares several recent methods that use CNNs for keypoint detection. Experiments are performed both on the CNN based approaches, as well as a selection of conventional methods. In addition to qualitative measures defined on keypoints and descriptors, the bag-of-words (BoW) model is used to implement an image retrieval application, in order to determine how the methods perform in practice. The results show that each type of features are best in different contexts.



### Towards Unsupervised Image Captioning with Shared Multimodal Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1908.09317v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09317v1)
- **Published**: 2019-08-25 12:56:41+00:00
- **Updated**: 2019-08-25 12:56:41+00:00
- **Authors**: Iro Laina, Christian Rupprecht, Nassir Navab
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work.



### advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns
- **Arxiv ID**: http://arxiv.org/abs/1908.09327v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09327v3)
- **Published**: 2019-08-25 13:33:35+00:00
- **Updated**: 2019-10-25 13:52:10+00:00
- **Authors**: Zhibo Wang, Siyan Zheng, Mengkai Song, Qian Wang, Alireza Rahimpour, Hairong Qi
- **Comment**: 10 pages, 6 figures, Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Person re-identification (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks. In this paper, we take the first attempt to implement robust physical-world attacks against deep re-ID. We propose a novel attack algorithm, called advPattern, for generating adversarial patterns on clothes, which learns the variations of image pairs across cameras to pull closer the image features from the same camera, while pushing features from different cameras farther. By wearing our crafted "invisible cloak", an adversary can evade person search, or impersonate a target person to fool deep re-ID models in physical world. We evaluate the effectiveness of our transformable patterns on adversaries'clothes with Market1501 and our established PRCS dataset. The experimental results show that the rank-1 accuracy of re-ID models for matching the adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore, the adversary can impersonate a target person with 47.1% rank-1 accuracy and 67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID systems are vulnerable to our physical attacks.



### Learning adaptively from the unknown for few-example video person re-ID
- **Arxiv ID**: http://arxiv.org/abs/1908.09340v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09340v1)
- **Published**: 2019-08-25 14:41:08+00:00
- **Updated**: 2019-08-25 14:41:08+00:00
- **Authors**: Jian Han
- **Comment**: This is a draft and there are many places to be revised
- **Journal**: None
- **Summary**: This paper mainly studies one-example and few-example video person re-identification. A multi-branch network PAM that jointly learns local and global features is proposed. PAM has high accuracy, few parameters and converges fast, which is suitable for few-example person re-identification. We iteratively estimates labels for unlabeled samples, incorporates them into training sets, and trains a more robust network. We propose the static relative distance sampling(SRD) strategy based on the relative distance between classes. For the problem that SRD can not use all unlabeled samples, we propose adaptive relative distance sampling (ARD) strategy. For one-example setting, We get 89.78\%, 56.13\% rank-1 accuracy on PRID2011 and iLIDS-VID respectively, and 85.16\%, 45.36\% mAP on DukeMTMC and MARS respectively, which exceeds the previous methods by large margin.



### Dedge-AGMNet:an effective stereo matching network optimized by depth edge auxiliary task
- **Arxiv ID**: http://arxiv.org/abs/1908.09346v4
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09346v4)
- **Published**: 2019-08-25 15:39:33+00:00
- **Updated**: 2020-03-24 04:38:07+00:00
- **Authors**: Weida Yang, Xindong Ai, Zuliu Yang, Yong Xu, Yong Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: To improve the performance in ill-posed regions, this paper proposes an atrous granular multi-scale network based on depth edge subnetwork(Dedge-AGMNet). According to a general fact, the depth edge is the binary semantic edge of instance-sensitive. This paper innovatively generates the depth edge ground-truth by mining the semantic and instance dataset simultaneously. To incorporate the depth edge cues efficiently, our network employs the hard parameter sharing mechanism for the stereo matching branch and depth edge branch. The network modifies SPP to Dedge-SPP, which fuses the depth edge features to the disparity estimation network. The granular convolution is extracted and extends to 3D architecture. Then we design the AGM module to build a more suitable structure. This module could capture the multi-scale receptive field with fewer parameters. Integrating the ranks of different stereo datasets, our network outperforms other stereo matching networks and advances state-of-the-art performances on the Sceneflow, KITTI 2012 and KITTI 2015 benchmark datasets.



### A Three-Feature Model to Predict Colour Change Blindness
- **Arxiv ID**: http://arxiv.org/abs/1909.04147v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.04147v1)
- **Published**: 2019-08-25 23:20:18+00:00
- **Updated**: 2019-08-25 23:20:18+00:00
- **Authors**: Steven Le Moan, Marius Pedersen
- **Comment**: None
- **Journal**: None
- **Summary**: Change blindness is a striking shortcoming of our visual system which is exploited in the popular "Spot the difference" game. It makes us unable to notice large visual changes happening right before our eyes and illustrates the fact that we see much less than we think we do. We introduce a fully automated model to predict colour change blindness in cartoon images based on two low-level image features and observer experience. Using linear regression with only three parameters, the predictions of the proposed model correlate significantly with measured detection times. We also demonstrate the efficacy of the model to classify stimuli in terms of difficulty.



### Machine and Deep Learning for Crowd Analytics
- **Arxiv ID**: http://arxiv.org/abs/1909.04150v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.04150v1)
- **Published**: 2019-08-25 23:43:39+00:00
- **Updated**: 2019-08-25 23:43:39+00:00
- **Authors**: Muhammad Siraj
- **Comment**: None
- **Journal**: None
- **Summary**: In high population cities, the gatherings of large crowds in public places and public areas accelerate or jeopardize people safety and transportation, which is a key challenge to the researchers. Although much research has been carried out on crowd analytics, many of existing methods are problem-specific, i.e., methods learned from a specific scene cannot be properly adopted to other videos. Therefore, this presents weakness and the discovery of these researches, since additional training samples have to be found from diverse videos. This paper will investigate diverse scene crowd analytics with traditional and deep learning models. We will also consider pros and cons of these approaches. However, once general deep methods are investigated from large datasets, they can be consider to investigate different crowd videos and images. Therefore, it would be able to cope with the problem including to not limited to crowd density estimation, crowd people counting, and crowd event recognition. Deep learning models and approaches are required to have large datasets for training and testing. Many datasets are collected taking into account many different and various problems related to building crowd datasets, including manual annotations and increasing diversity of videos and images. In this paper, we will also propose many models of deep neural networks and training approaches to learn the feature modeling for crowd analytics.



