# Arxiv Papers in cs.CV on 2019-08-27
### Curved Text Detection in Natural Scene Images with Semi- and Weakly-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.09990v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09990v1)
- **Published**: 2019-08-27 02:21:49+00:00
- **Updated**: 2019-08-27 02:21:49+00:00
- **Authors**: Xugong Qin, Yu Zhou, Dongbao Yang, Weiping Wang
- **Comment**: Accepted by ICDAR 2019
- **Journal**: None
- **Summary**: Detecting curved text in the wild is very challenging. Recently, most state-of-the-art methods are segmentation based and require pixel-level annotations. We propose a novel scheme to train an accurate text detector using only a small amount of pixel-level annotated data and a large amount of data annotated with rectangles or even unlabeled data. A baseline model is first obtained by training with the pixel-level annotated data and then used to annotate unlabeled or weakly labeled data. A novel strategy which utilizes ground-truth bounding boxes to generate pseudo mask annotations is proposed in weakly-supervised learning. Experimental results on CTW1500 and Total-Text demonstrate that our method can substantially reduce the requirement of pixel-level annotated data. Our method can also generalize well across two datasets. The performance of the proposed method is comparable with the state-of-the-art methods with only 10% pixel-level annotated data and 90% rectangle-level weakly annotated data.



### Deep Learning-Based Strategy for Macromolecules Classification with Imbalanced Data from Cellular Electron Cryotomography
- **Arxiv ID**: http://arxiv.org/abs/1908.09993v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1908.09993v1)
- **Published**: 2019-08-27 02:37:42+00:00
- **Updated**: 2019-08-27 02:37:42+00:00
- **Authors**: Ziqian Luo, Xiangrui Zeng, Zhipeng Bao, Min Xu
- **Comment**: 13 pages. arXiv admin note: text overlap with arXiv:1710.09412,
  arXiv:1710.05381, arXiv:1708.02002 by other authors
- **Journal**: 2019 International Joint Conference on Neural Networks (IJCNN)
- **Summary**: Deep learning model trained by imbalanced data may not work satisfactorily since it could be determined by major classes and thus may ignore the classes with small amount of data. In this paper, we apply deep learning based imbalanced data classification for the first time to cellular macromolecular complexes captured by Cryo-electron tomography (Cryo-ET). We adopt a range of strategies to cope with imbalanced data, including data sampling, bagging, boosting, Genetic Programming based method and. Particularly, inspired from Inception 3D network, we propose a multi-path CNN model combining focal loss and mixup on the Cryo-ET dataset to expand the dataset, where each path had its best performance corresponding to each type of data and let the network learn the combinations of the paths to improve the classification performance. In addition, extensive experiments have been conducted to show our proposed method is flexible enough to cope with different number of classes by adjusting the number of paths in our multi-path model. To our knowledge, this work is the first application of deep learning methods of dealing with imbalanced data to the internal tissue classification of cell macromolecular complexes, which opened up a new path for cell classification in the field of computational biology.



### Temporal Reasoning Graph for Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.09995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09995v1)
- **Published**: 2019-08-27 02:39:52+00:00
- **Updated**: 2019-08-27 02:39:52+00:00
- **Authors**: Jingran Zhang, Fumin Shen, Xing Xu, Heng Tao Shen
- **Comment**: 14pages, 8figures
- **Journal**: None
- **Summary**: Despite great success has been achieved in activity analysis, it still has many challenges. Most existing work in activity recognition pay more attention to design efficient architecture or video sampling strategy. However, due to the property of fine-grained action and long term structure in video, activity recognition is expected to reason temporal relation between video sequences. In this paper, we propose an efficient temporal reasoning graph (TRG) to simultaneously capture the appearance features and temporal relation between video sequences at multiple time scales. Specifically, we construct learnable temporal relation graphs to explore temporal relation on the multi-scale range. Additionally, to facilitate multi-scale temporal relation extraction, we design a multi-head temporal adjacent matrix to represent multi-kinds of temporal relations. Eventually, a multi-head temporal relation aggregator is proposed to extract the semantic meaning of those features convolving through the graphs. Extensive experiments are performed on widely-used large-scale datasets, such as Something-Something and Charades, and the results show that our model can achieve state-of-the-art performance. Further analysis shows that temporal relation reasoning with our TRG can extract discriminative features for activity recognition.



### Distorted Representation Space Characterization Through Backpropagated Gradients
- **Arxiv ID**: http://arxiv.org/abs/1908.09998v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09998v1)
- **Published**: 2019-08-27 02:58:43+00:00
- **Updated**: 2019-08-27 02:58:43+00:00
- **Authors**: Gukyeong Kwon, Mohit Prabhushankar, Dogancan Temel, Ghassan AlRegib
- **Comment**: 5 pages, 5 figures, 2 tables, ICIP 2019
- **Journal**: None
- **Summary**: In this paper, we utilize weight gradients from backpropagation to characterize the representation space learned by deep learning algorithms. We demonstrate the utility of such gradients in applications including perceptual image quality assessment and out-of-distribution classification. The applications are chosen to validate the effectiveness of gradients as features when the test image distribution is distorted from the train image distribution. In both applications, the proposed gradient based features outperform activation features. In image quality assessment, the proposed approach is compared with other state of the art approaches and is generally the top performing method on TID 2013 and MULTI-LIVE databases in terms of accuracy, consistency, linearity, and monotonic behavior. Finally, we analyze the effect of regularization on gradients using CURE-TSR dataset for out-of-distribution classification.



### A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image
- **Arxiv ID**: http://arxiv.org/abs/1908.09999v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09999v1)
- **Published**: 2019-08-27 02:58:47+00:00
- **Updated**: 2019-08-27 02:58:47+00:00
- **Authors**: Fu Xiong, Boshen Zhang, Yang Xiao, Zhiguo Cao, Taidong Yu, Joey Tianyi Zhou, Junsong Yuan
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet-50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.



### Learning Reinforced Attentional Representation for End-to-End Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.10009v3
- **DOI**: 10.1016/j.ins.2019.12.084
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10009v3)
- **Published**: 2019-08-27 03:55:17+00:00
- **Updated**: 2020-01-02 01:07:09+00:00
- **Authors**: Peng Gao, Qiquan Zhang, Fei Wang, Liyi Xiao, Hamido Fujita, Yan Zhang
- **Comment**: Accepted by Information Sciences
- **Journal**: None
- **Summary**: Although numerous recent tracking approaches have made tremendous advances in the last decade, achieving high-performance visual tracking remains a challenge. In this paper, we propose an end-to-end network model to learn reinforced attentional representation for accurate target object discrimination and localization. We utilize a novel hierarchical attentional module with long short-term memory and multi-layer perceptrons to leverage both inter- and intra-frame attention to effectively facilitate visual pattern emphasis. Moreover, we incorporate a contextual attentional correlation filter into the backbone network to make our model trainable in an end-to-end fashion. Our proposed approach not only takes full advantage of informative geometries and semantics but also updates correlation filters online without fine-tuning the backbone network to enable the adaptation of variations in the target object's appearance. Extensive experiments conducted on several popular benchmark datasets demonstrate that our proposed approach is effective and computationally efficient.



### Unsupervised Deep Feature Transfer for Low Resolution Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.10012v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10012v2)
- **Published**: 2019-08-27 04:06:02+00:00
- **Updated**: 2019-10-19 19:56:27+00:00
- **Authors**: Yuanwei Wu, Ziming Zhang, Guanghui Wang
- **Comment**: 4 pages, accepted to ICCV19 Workshop and Challenge on Real-World
  Recognition from Low-Quality Images and Videos
- **Journal**: None
- **Summary**: In this paper, we propose a simple while effective unsupervised deep feature transfer algorithm for low resolution image classification. No fine-tuning on convenet filters is required in our method. We use pre-trained convenet to extract features for both high- and low-resolution images, and then feed them into a two-layer feature transfer network for knowledge transfer. A SVM classifier is learned directly using these transferred low resolution features. Our network can be embedded into the state-of-the-art deep neural networks as a plug-in feature enhancement module. It preserves data structures in feature space for high resolution images, and transfers the distinguishing features from a well-structured source domain (high resolution features space) to a not well-organized target domain (low resolution features space). Extensive experiments on VOC2007 test set show that the proposed method achieves significant improvements over the baseline of using feature extraction.



### Dual Directed Capsule Network for Very Low Resolution Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.10027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10027v1)
- **Published**: 2019-08-27 04:42:57+00:00
- **Updated**: 2019-08-27 04:42:57+00:00
- **Authors**: Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa
- **Comment**: Accepted in the International Conference on Computer Vision (ICCV)
  2019
- **Journal**: None
- **Summary**: Very low resolution (VLR) image recognition corresponds to classifying images with resolution 16x16 or less. Though it has widespread applicability when objects are captured at a very large stand-off distance (e.g. surveillance scenario) or from wide angle mobile cameras, it has received limited attention. This research presents a novel Dual Directed Capsule Network model, termed as DirectCapsNet, for addressing VLR digit and face recognition. The proposed architecture utilizes a combination of capsule and convolutional layers for learning an effective VLR recognition model. The architecture also incorporates two novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed targeted reconstruction loss, in order to overcome the challenges of limited information content in VLR images. The proposed losses use high resolution images as auxiliary data during training to "direct" discriminative feature learning. Multiple experiments for VLR digit classification and VLR face recognition are performed along with comparisons with state-of-the-art algorithms. The proposed DirectCapsNet consistently showcases state-of-the-art results; for example, on the UCCS face database, it shows over 95\% face recognition accuracy when 16x16 images are matched with 80x80 images.



### Attention-based Dropout Layer for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.10028v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10028v1)
- **Published**: 2019-08-27 04:48:16+00:00
- **Updated**: 2019-08-27 04:48:16+00:00
- **Authors**: Junsuk Choe, Hyunjung Shim
- **Comment**: CVPR 2019 (Oral)
- **Journal**: None
- **Summary**: Weakly Supervised Object Localization (WSOL) techniques learn the object location only using image-level labels, without location annotations. A common limitation for these techniques is that they cover only the most discriminative part of the object, not the entire object. To address this problem, we propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to process the feature maps of the model. The proposed method is composed of two key components: 1) hiding the most discriminative part from the model for capturing the integral extent of object, and 2) highlighting the informative region for improving the recognition power of the model. Based on extensive experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL, achieving a new state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that the proposed method is much more efficient in terms of both parameter and computation overheads than existing techniques.



### Global-Local Temporal Representations For Video Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1908.10049v1
- **DOI**: 10.1109/TIP.2020.2972108
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10049v1)
- **Published**: 2019-08-27 06:57:03+00:00
- **Updated**: 2019-08-27 06:57:03+00:00
- **Authors**: Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, Shiliang Zhang
- **Comment**: None
- **Journal**: ICCV2019
- **Summary**: This paper proposes the Global-Local Temporal Representation (GLTR) to exploit the multi-scale temporal cues in video sequences for video person Re-Identification (ReID). GLTR is constructed by first modeling the short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. Specifically, the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestrian. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences. The short and long-term temporal cues are aggregated as the final GLTR by a simple single-stream CNN. GLTR shows substantial superiority to existing features learned with body part cues or metric learning on four widely-used video ReID datasets. For instance, it achieves Rank-1 Accuracy of 87.02% on MARS dataset without re-ranking, better than current state-of-the art.



### Visual Question Answering using Deep Learning: A Survey and Performance Analysis
- **Arxiv ID**: http://arxiv.org/abs/1909.01860v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1909.01860v2)
- **Published**: 2019-08-27 07:03:03+00:00
- **Updated**: 2020-12-23 01:11:29+00:00
- **Authors**: Yash Srivastava, Vaishnav Murali, Shiv Ram Dubey, Snehasis Mukherjee
- **Comment**: Accepted in Fifth IAPR International Conference on Computer Vision
  and Image Processing (CVIP), 2020
- **Journal**: None
- **Summary**: The Visual Question Answering (VQA) task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA datasets. At the end, we present and discuss some of the results computed by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge 2017 winner model. We also provide the detailed analysis along with the challenges and future research directions.



### MetaMixUp: Learning Adaptive Interpolation Policy of MixUp with Meta-Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.10059v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10059v1)
- **Published**: 2019-08-27 07:26:35+00:00
- **Updated**: 2019-08-27 07:26:35+00:00
- **Authors**: Zhijun Mai, Guosheng Hu, Dexiong Chen, Fumin Shen, Heng Tao Shen
- **Comment**: None
- **Journal**: None
- **Summary**: MixUp is an effective data augmentation method to regularize deep neural networks via random linear interpolations between pairs of samples and their labels. It plays an important role in model regularization, semi-supervised learning and domain adaption. However, despite its empirical success, its deficiency of randomly mixing samples has poorly been studied. Since deep networks are capable of memorizing the entire dataset, the corrupted samples generated by vanilla MixUp with a badly chosen interpolation policy will degrade the performance of networks. To overcome the underfitting by corrupted samples, inspired by Meta-learning (learning to learn), we propose a novel technique of learning to mixup in this work, namely, MetaMixUp. Unlike the vanilla MixUp that samples interpolation policy from a predefined distribution, this paper introduces a meta-learning based online optimization approach to dynamically learn the interpolation policy in a data-adaptive way. The validation set performance via meta-learning captures the underfitting issue, which provides more information to refine interpolation policy. Furthermore, we adapt our method for pseudo-label based semisupervised learning (SSL) along with a refined pseudo-labeling strategy. In our experiments, our method achieves better performance than vanilla MixUp and its variants under supervised learning configuration. In particular, extensive experiments show that our MetaMixUp adapted SSL greatly outperforms MixUp and many state-of-the-art methods on CIFAR-10 and SVHN benchmarks under SSL configuration.



### Controllable Video Captioning with POS Sequence Guidance Based on Gated Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1908.10072v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10072v1)
- **Published**: 2019-08-27 08:22:54+00:00
- **Updated**: 2019-08-27 08:22:54+00:00
- **Authors**: Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Jingwen Wang, Wei Liu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: In this paper, we propose to guide the video caption generation with Part-of-Speech (POS) information, based on a gated fusion of multiple representations of input videos. We construct a novel gated fusion network, with one particularly designed cross-gating (CG) block, to effectively encode and fuse different types of representations, e.g., the motion and content features of an input video. One POS sequence generator relies on this fused representation to predict the global syntactic structure, which is thereafter leveraged to guide the video captioning generation and control the syntax of the generated sentence. Specifically, a gating strategy is proposed to dynamically and adaptively incorporate the global syntactic POS information into the decoder for generating each word. Experimental results on two benchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed model can well exploit complementary information from multiple representations, resulting in improved performances. Moreover, the generated global POS information can well capture the global syntactic structure of the sentence, and thus be exploited to control the syntactic structure of the description. Such POS information not only boosts the video captioning performance but also improves the diversity of the generated captions. Our code is at: https://github.com/vsislab/Controllable_XGating.



### HRGE-Net: Hierarchical Relational Graph Embedding Network for Multi-view 3D Shape Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.10098v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10098v1)
- **Published**: 2019-08-27 09:23:16+00:00
- **Updated**: 2019-08-27 09:23:16+00:00
- **Authors**: Xin Wei, Ruixuan Yu, Jian Sun
- **Comment**: None
- **Journal**: None
- **Summary**: View-based approach that recognizes 3D shape through its projected 2D images achieved state-of-the-art performance for 3D shape recognition. One essential challenge for view-based approach is how to aggregate the multi-view features extracted from 2D images to be a global 3D shape descriptor. In this work, we propose a novel feature aggregation network by fully investigating the relations among views. We construct a relational graph with multi-view images as nodes, and design relational graph embedding by modeling pairwise and neighboring relations among views. By gradually coarsening the graph, we build a hierarchical relational graph embedding network (HRGE-Net) to aggregate the multi-view features to be a global shape descriptor. Extensive experiments show that HRGE-Net achieves stateof-the-art performance for 3D shape classification and retrieval on benchmark datasets.



### Synthetic patches, real images: screening for centrosome aberrations in EM images of human cancer cells
- **Arxiv ID**: http://arxiv.org/abs/1908.10109v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10109v1)
- **Published**: 2019-08-27 09:48:02+00:00
- **Updated**: 2019-08-27 09:48:02+00:00
- **Authors**: Artem Lukoyanov, Isabella Haberbosch, Constantin Pape, Alwin Kraemer, Yannick Schwab, Anna Kreshuk
- **Comment**: Accepted at MICCAI 2019
- **Journal**: None
- **Summary**: Recent advances in high-throughput electron microscopy imaging enable detailed study of centrosome aberrations in cancer cells. While the image acquisition in such pipelines is automated, manual detection of centrioles is still necessary to select cells for re-imaging at higher magnification. In this contribution we propose an algorithm which performs this step automatically and with high accuracy. From the image labels produced by human experts and a 3D model of a centriole we construct an additional training set with patch-level labels. A two-level DenseNet is trained on the hybrid training data with synthetic patches and real images, achieving much better results on real patient data than training only at the image-level. The code can be found at https://github.com/kreshuklab/centriole_detection.



### Cooperative Cross-Stream Network for Discriminative Action Representation
- **Arxiv ID**: http://arxiv.org/abs/1908.10136v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10136v1)
- **Published**: 2019-08-27 11:23:34+00:00
- **Updated**: 2019-08-27 11:23:34+00:00
- **Authors**: Jingran Zhang, Fumin Shen, Xing Xu, Heng Tao Shen
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: Spatial and temporal stream model has gained great success in video action recognition. Most existing works pay more attention to designing effective features fusion methods, which train the two-stream model in a separate way. However, it's hard to ensure discriminability and explore complementary information between different streams in existing works. In this work, we propose a novel cooperative cross-stream network that investigates the conjoint information in multiple different modalities. The jointly spatial and temporal stream networks feature extraction is accomplished by an end-to-end learning manner. It extracts this complementary information of different modality from a connection block, which aims at exploring correlations of different stream features. Furthermore, different from the conventional ConvNet that learns the deep separable features with only one cross-entropy loss, our proposed model enhances the discriminative power of the deeply learned features and reduces the undesired modality discrepancy by jointly optimizing a modality ranking constraint and a cross-entropy loss for both homogeneous and heterogeneous modalities. The modality ranking constraint constitutes intra-modality discriminative embedding and inter-modality triplet constraint, and it reduces both the intra-modality and cross-modality feature variations. Experiments on three benchmark datasets demonstrate that by cooperating appearance and motion feature extraction, our method can achieve state-of-the-art or competitive performance compared with existing results.



### Enabling Hyper-Personalisation: Automated Ad Creative Generation and Ranking for Fashion e-Commerce
- **Arxiv ID**: http://arxiv.org/abs/1908.10139v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10139v1)
- **Published**: 2019-08-27 11:28:37+00:00
- **Updated**: 2019-08-27 11:28:37+00:00
- **Authors**: Sreekanth Vempati, Korah T Malayil, Sruthi V, Sandeep R
- **Comment**: Workshop on Recommender Systems in Fashion, 13th ACM Conference on
  Recommender Systems, 2019
- **Journal**: None
- **Summary**: Homepage is the first touch point in the customer's journey and is one of the prominent channels of revenue for many e-commerce companies. A user's attention is mostly captured by homepage banner images (also called Ads/Creatives). The set of banners shown and their design, influence the customer's interest and plays a key role in optimizing the click through rates of the banners. Presently, massive and repetitive effort is put in, to manually create aesthetically pleasing banner images. Due to the large amount of time and effort involved in this process, only a small set of banners are made live at any point. This reduces the number of banners created as well as the degree of personalization that can be achieved. This paper thus presents a method to generate creatives automatically on a large scale in a short duration. The availability of diverse banners generated helps in improving personalization as they can cater to the taste of larger audience. The focus of our paper is on generating wide variety of homepage banners that can be made as an input for user level personalization engine. Following are the main contributions of this paper: 1) We introduce and explain the need for large scale banner generation for e-commerce 2) We present on how we utilize existing deep learning based detectors which can automatically annotate the required objects/tags from the image. 3) We also propose a Genetic Algorithm based method to generate an optimal banner layout for the given image content, input components and other design constraints. 4) Further, to aid the process of picking the right set of banners, we designed a ranking method and evaluated multiple models. All our experiments have been performed on data from Myntra (http://www.myntra.com), one of the top fashion e-commerce players in India.



### Mobile Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.10155v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10155v1)
- **Published**: 2019-08-27 12:16:55+00:00
- **Updated**: 2019-08-27 12:16:55+00:00
- **Authors**: Yuqi Huo, Xiaoli Xu, Yao Lu, Yulei Niu, Zhiwu Lu, Ji-Rong Wen
- **Comment**: None
- **Journal**: None
- **Summary**: Video action recognition, which is topical in computer vision and video analysis, aims to allocate a short video clip to a pre-defined category such as brushing hair or climbing stairs. Recent works focus on action recognition with deep neural networks that achieve state-of-the-art results in need of high-performance platforms. Despite the fast development of mobile computing, video action recognition on mobile devices has not been fully discussed. In this paper, we focus on the novel mobile video action recognition task, where only the computational capabilities of mobile devices are accessible. Instead of raw videos with huge storage, we choose to extract multiple modalities (including I-frames, motion vectors, and residuals) directly from compressed videos. By employing MobileNetV2 as backbone, we propose a novel Temporal Trilinear Pooling (TTP) module to fuse the multiple modalities for mobile video action recognition. In addition to motion vectors, we also provide a temporal fusion method to explicitly induce the temporal context. The efficiency test on a mobile device indicates that our model can perform mobile video action recognition at about 40FPS. The comparative results on two benchmarks show that our model outperforms existing action recognition methods in model size and time consuming, but with competitive accuracy.



### Fingerprint Presentation Attack Detection Based on Local Features Encoding for Unknown Attacks
- **Arxiv ID**: http://arxiv.org/abs/1908.10163v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10163v1)
- **Published**: 2019-08-27 12:44:37+00:00
- **Updated**: 2019-08-27 12:44:37+00:00
- **Authors**: Lázaro J. González-Soler, Marta Gomez-Barrero, Leonardo Chang, Airel Pérez-Suárez, Christoph Busch
- **Comment**: 14 pages
- **Journal**: None
- **Summary**: Fingerprint-based biometric systems have experienced a large development in the last years. Despite their many advantages, they are still vulnerable to presentation attacks (PAs). Therefore, the task of determining whether a sample stems from a live subject (i.e., bona fide) or from an artificial replica is a mandatory issue which has received a lot of attention recently. Nowadays, when the materials for the fabrication of the Presentation Attack Instruments (PAIs) have been used to train the PA Detection (PAD) methods, the PAIs can be successfully identified. However, current PAD methods still face difficulties detecting PAIs built from unknown materials or captured using other sensors. Based on that fact, we propose a new PAD technique based on three image representation approaches combining local and global information of the fingerprint. By transforming these representations into a common feature space, we can correctly discriminate bona fide from attack presentations in the aforementioned scenarios. The experimental evaluation of our proposal over the LivDet 2011 to 2015 databases, yielded error rates outperforming the top state-of-the-art results by up to 50\% in the most challenging scenarios. In addition, the best configuration achieved the best results in the LivDet 2019 competition (overall accuracy of 96.17\%).



### Key Protected Classification for Collaborative Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.10172v2
- **DOI**: 10.1016/j.patcog.2020.107327
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10172v2)
- **Published**: 2019-08-27 13:00:30+00:00
- **Updated**: 2020-04-22 09:31:45+00:00
- **Authors**: Mert Bülent Sarıyıldız, Ramazan Gökberk Cinbiş, Erman Ayday
- **Comment**: Accepted to Pattern Recognition
- **Journal**: None
- **Summary**: Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code is available at https://github.com/mbsariyildiz/key-protected-classification.



### Segmentation Mask Guided End-to-End Person Search
- **Arxiv ID**: http://arxiv.org/abs/1908.10179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10179v1)
- **Published**: 2019-08-27 13:16:00+00:00
- **Updated**: 2019-08-27 13:16:00+00:00
- **Authors**: Dingyuan Zheng, Jimin Xiao, Kaizhu Huang, Yao Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Person search aims to search for a target person among multiple images recorded by multiple surveillance cameras, which faces various challenges from both pedestrian detection and person re-identification. Besides the large intra-class variations owing to various illumination conditions, occlusions and varying poses, background clutters in the detected pedestrian bounding boxes further deteriorate the extracted features for each person, making them less discriminative. To tackle these problems, we develop a novel approach which guides the network with segmentation masks so that discriminative features can be learned invariant to the background clutters. We demonstrate that joint optimization of pedestrian detection, person re-identification and pedestrian segmentation enables to produce more discriminative features for pedestrian, and consequently leads to better person search performance. Extensive experiments on benchmark dataset CUHK-SYSU, show that our proposed model achieves the state-of-the-art performance with 86.3% mAP and 86.5 top-1 accuracy respectively.



### Large Scale Landmark Recognition via Deep Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.10192v3
- **DOI**: 10.1145/3357384.3357956
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10192v3)
- **Published**: 2019-08-27 13:37:49+00:00
- **Updated**: 2019-08-29 09:04:17+00:00
- **Authors**: Andrei Boiarov, Eduard Tyantov
- **Comment**: Accepted at CIKM 2019
- **Journal**: None
- **Summary**: This paper presents a novel approach for landmark recognition in images that we've successfully deployed at Mail ru. This method enables us to recognize famous places, buildings, monuments, and other landmarks in user photos. The main challenge lies in the fact that it's very complicated to give a precise definition of what is and what is not a landmark. Some buildings, statues and natural objects are landmarks; others are not. There's also no database with a fairly large number of landmarks to train a recognition model. A key feature of using landmark recognition in a production environment is that the number of photos containing landmarks is extremely small. This is why the model should have a very low false positive rate as well as high recognition accuracy.   We propose a metric learning-based approach that successfully deals with existing challenges and efficiently handles a large number of landmarks. Our method uses a deep neural network and requires a single pass inference that makes it fast to use in production. We also describe an algorithm for cleaning landmarks database which is essential for training a metric learning model. We provide an in-depth description of basic components of our method like neural network architecture, the learning strategy, and the features of our metric learning approach. We show the results of proposed solutions in tests that emulate the distribution of photos with and without landmarks from a user collection. We compare our method with others during these tests. The described system has been deployed as a part of a photo recognition solution at Cloud Mail ru, which is the photo sharing and storage service at Mail ru Group.



### 3D Convolutional Neural Networks Image Registration Based on Efficient Supervised Learning from Artificial Deformations
- **Arxiv ID**: http://arxiv.org/abs/1908.10235v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10235v1)
- **Published**: 2019-08-27 14:34:45+00:00
- **Updated**: 2019-08-27 14:34:45+00:00
- **Authors**: Hessam Sokooti, Bob de Vos, Floris Berendsen, Mohsen Ghafoorian, Sahar Yousefi, Boudewijn P. F. Lelieveldt, Ivana Isgum, Marius Staring
- **Comment**: TMI
- **Journal**: None
- **Summary**: We propose a supervised nonrigid image registration method, trained using artificial displacement vector fields (DVF), for which we propose and compare three network architectures. The artificial DVFs allow training in a fully supervised and voxel-wise dense manner, but without the cost usually associated with the creation of densely labeled data. We propose a scheme to artificially generate DVFs, and for chest CT registration augment these with simulated respiratory motion. The proposed architectures are embedded in a multi-stage approach, to increase the capture range of the proposed networks in order to more accurately predict larger displacements. The proposed method, RegNet, is evaluated on multiple databases of chest CT scans and achieved a target registration error of 2.32 $\pm$ 5.33 mm and 1.86 $\pm$ 2.12 mm on SPREAD and DIR-Lab-4DCT studies, respectively. The average inference time of RegNet with two stages is about 2.2 s.



### Large-Scale Historical Watermark Recognition: dataset and a new consistency-based approach
- **Arxiv ID**: http://arxiv.org/abs/1908.10254v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10254v1)
- **Published**: 2019-08-27 15:00:32+00:00
- **Updated**: 2019-08-27 15:00:32+00:00
- **Authors**: Xi Shen, Ilaria Pastrolin, Oumayma Bounou, Spyros Gidaris, Marc Smith, Olivier Poncet, Mathieu Aubry
- **Comment**: None
- **Journal**: None
- **Summary**: Historical watermark recognition is a highly practical, yet unsolved challenge for archivists and historians. With a large number of well-defined classes, cluttered and noisy samples, different types of representations, both subtle differences between classes and high intra-class variation, historical watermarks are also challenging for pattern recognition. In this paper, overcoming the difficulty of data collection, we present a large public dataset with more than 6k new photographs, allowing for the first time to tackle at scale the scenarios of practical interest for scholars: one-shot instance recognition and cross-domain one-shot instance recognition amongst more than 16k fine-grained classes. We demonstrate that this new dataset is large enough to train modern deep learning approaches, and show that standard methods can be improved considerably by using mid-level deep features. More precisely, we design both a matching score and a feature fine-tuning strategy based on filtering local matches using spatial consistency. This consistency-based approach provides important performance boost compared to strong baselines. Our model achieves 55% top-1 accuracy on our very challenging 16,753-class one-shot cross-domain recognition task, each class described by a single drawing from the classic Briquet catalog. In addition to watermark classification, we show our approach provides promising results on fine-grained sketch-based image retrieval.



### Few-shot Learning with Deep Triplet Networks for Brain Imaging Modality Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.10266v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10266v1)
- **Published**: 2019-08-27 15:19:24+00:00
- **Updated**: 2019-08-27 15:19:24+00:00
- **Authors**: Santi Puch, Irina Sánchez, Matt Rowe
- **Comment**: Medical Image Learning with Less Labels and Imperfect Data, MICCAI
  2019 workshop
- **Journal**: None
- **Summary**: Image modality recognition is essential for efficient imaging workflows in current clinical environments, where multiple imaging modalities are used to better comprehend complex diseases. Emerging biomarkers from novel, rare modalities are being developed to aid in such understanding, however the availability of these images is often limited. This scenario raises the necessity of recognising new imaging modalities without them being collected and annotated in large amounts. In this work, we present a few-shot learning model for limited training examples based on Deep Triplet Networks. We show that the proposed model is more accurate in distinguishing different modalities than a traditional Convolutional Neural Network classifier when limited samples are available. Furthermore, we evaluate the performance of both classifiers when presented with noisy samples and provide an initial inspection of how the proposed model can incorporate measures of uncertainty to be more robust against out-of-sample examples.



### DRD-Net: Detail-recovery Image Deraining via Context Aggregation Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.10267v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10267v2)
- **Published**: 2019-08-27 15:22:09+00:00
- **Updated**: 2019-08-28 12:01:53+00:00
- **Authors**: Sen Deng, Mingqiang Wei, Jun Wang, Luming Liang, Haoran Xie, Meng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Image deraining is a fundamental, yet not well-solved problem in computer vision and graphics. The traditional image deraining approaches commonly behave ineffectively in medium and heavy rain removal, while the learning-based ones lead to image degradations such as the loss of image details, halo artifacts and/or color distortion. Unlike existing image deraining approaches that lack the detail-recovery mechanism, we propose an end-to-end detail-recovery image deraining network (termed a DRD-Net) for single images. We for the first time introduce two sub-networks with a comprehensive loss function which synergize to derain and recover the lost details caused by deraining. We have three key contributions. First, we present a rain residual network to remove rain streaks from the rainy images, which combines the squeeze-and-excitation (SE) operation with residual blocks to make full advantage of spatial contextual information. Second, we design a new connection style block, named structure detail context aggregation block (SDCAB), which aggregates context feature information and has a large reception field. Third, benefiting from the SDCAB, we construct a detail repair network to encourage the lost details to return for eliminating image degradations. We have validated our approach on four recognized datasets (three synthetic and one real-world). Both quantitative and qualitative comparisons show that our approach outperforms the state-of-the-art deraining methods in terms of the deraining robustness and detail accuracy. The source code has been available for public evaluation and use on GitHub.



### Global Planar Convolutions for improved context aggregation in Brain Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.10281v1
- **DOI**: 10.1007/978-3-030-11726-9_35
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10281v1)
- **Published**: 2019-08-27 15:38:50+00:00
- **Updated**: 2019-08-27 15:38:50+00:00
- **Authors**: Santi Puch, Irina Sánchez, Aura Hernández, Gemma Piella, Vesna Prchkovska
- **Comment**: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain
  Injuries. BrainLes 2018
- **Journal**: None
- **Summary**: In this work, we introduce the Global Planar Convolution module as a building-block for fully-convolutional networks that aggregates global information and, therefore, enhances the context perception capabilities of segmentation networks in the context of brain tumor segmentation. We implement two baseline architectures (3D UNet and a residual version of 3D UNet, ResUNet) and present a novel architecture based on these two architectures, ContextNet, that includes the proposed Global Planar Convolution module. We show that the addition of such module eliminates the need of building networks with several representation levels, which tend to be over-parametrized and to showcase slow rates of convergence. Furthermore, we provide a visual demonstration of the behavior of GPC modules via visualization of intermediate representations. We finally participate in the 2018 edition of the BraTS challenge with our best performing models, that are based on ContextNet, and report the evaluation scores on the validation and the test sets of the challenge.



### Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts
- **Arxiv ID**: http://arxiv.org/abs/1908.10285v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10285v1)
- **Published**: 2019-08-27 15:44:17+00:00
- **Updated**: 2019-08-27 15:44:17+00:00
- **Authors**: Sandro Pezzelle, Raquel Fernández
- **Comment**: Accepted at EMNLP-IJCNLP 2019
- **Journal**: None
- **Summary**: This work aims at modeling how the meaning of gradable adjectives of size (`big', `small') can be learned from visually-grounded contexts. Inspired by cognitive and linguistic evidence showing that the use of these expressions relies on setting a threshold that is dependent on a specific context, we investigate the ability of multi-modal models in assessing whether an object is `big' or `small' in a given visual scene. In contrast with the standard computational approach that simplistically treats gradable adjectives as `fixed' attributes, we pose the problem as relational: to be successful, a model has to consider the full visual context. By means of four main tasks, we show that state-of-the-art models (but not a relatively strong baseline) can learn the function subtending the meaning of size adjectives, though their performance is found to decrease while moving from simple to more complex tasks. Crucially, models fail in developing abstract representations of gradable adjectives that can be used compositionally.



### Physiological and Affective Computing through Thermal Imaging: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1908.10307v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10307v1)
- **Published**: 2019-08-27 16:30:51+00:00
- **Updated**: 2019-08-27 16:30:51+00:00
- **Authors**: Youngjun Cho, Nadia Bianchi-Berthouze
- **Comment**: None
- **Journal**: None
- **Summary**: Thermal imaging-based physiological and affective computing is an emerging research area enabling technologies to monitor our bodily functions and understand psychological and affective needs in a contactless manner. However, up to recently, research has been mainly carried out in very controlled lab settings. As small size and even low-cost versions of thermal video cameras have started to appear on the market, mobile thermal imaging is opening its door to ubiquitous and real-world applications. Here we review the literature on the use of thermal imaging to track changes in physiological cues relevant to affective computing and the technological requirements set so far. In doing so, we aim to establish computational and methodological pipelines from thermal images of the human skin to affective states and outline the research opportunities and challenges to be tackled to make ubiquitous real-life thermal imaging-based affect monitoring a possibility.



### Physics-Based Rendering for Improving Robustness to Rain
- **Arxiv ID**: http://arxiv.org/abs/1908.10335v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10335v1)
- **Published**: 2019-08-27 17:13:46+00:00
- **Updated**: 2019-08-27 17:13:46+00:00
- **Authors**: Shirsendu Sukanta Halder, Jean-François Lalonde, Raoul de Charette
- **Comment**: ICCV 2019. Supplementary pdf / videos available on project page
- **Journal**: None
- **Summary**: To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.



### Intra-Camera Supervised Person Re-Identification: A New Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1908.10344v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10344v1)
- **Published**: 2019-08-27 17:33:48+00:00
- **Updated**: 2019-08-27 17:33:48+00:00
- **Authors**: Xiangping Zhu, Xiatian Zhu, Minxian Li, Vittorio Murino, Shaogang Gong
- **Comment**: 9 pages, 3 figures, accepted by ICCV Workshop on Real-World
  Recognition from Low-Quality Images and Videos, 2019
- **Journal**: None
- **Summary**: Existing person re-identification (re-id) methods rely mostly on a large set of inter-camera identity labelled training data, requiring a tedious data collection and annotation process therefore leading to poor scalability in practical re-id applications. To overcome this fundamental limitation, we consider person re-identification without inter-camera identity association but only with identity labels independently annotated within each individual camera-view. This eliminates the most time-consuming and tedious inter-camera identity labelling process in order to significantly reduce the amount of human efforts required during annotation. It hence gives rise to a more scalable and more feasible learning scenario, which we call Intra-Camera Supervised (ICS) person re-id. Under this ICS setting with weaker label supervision, we formulate a Multi-Task Multi-Label (MTML) deep learning method. Given no inter-camera association, MTML is specially designed for self-discovering the inter-camera identity correspondence. This is achieved by inter-camera multi-label learning under a joint multi-task inference framework. In addition, MTML can also efficiently learn the discriminative re-id feature representations by fully using the available identity labels within each camera-view. Extensive experiments demonstrate the performance superiority of our MTML model over the state-of-the-art alternative methods on three large-scale person re-id datasets in the proposed intra-camera supervised learning setting.



### HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.10357v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10357v3)
- **Published**: 2019-08-27 17:54:08+00:00
- **Updated**: 2020-03-12 16:13:53+00:00
- **Authors**: Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, Lei Zhang
- **Comment**: CVPR 2020
- **Journal**: None
- **Summary**: Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.



### Unsupervised Domain-Adaptive Person Re-identification Based on Attributes
- **Arxiv ID**: http://arxiv.org/abs/1908.10359v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10359v1)
- **Published**: 2019-08-27 17:56:05+00:00
- **Updated**: 2019-08-27 17:56:05+00:00
- **Authors**: Xiangping Zhu, Pietro Morerio, Vittorio Murino
- **Comment**: 5 pages, accepted by ICIP2019
- **Journal**: None
- **Summary**: Pedestrian attributes, e.g., hair length, clothes type and color, locally describe the semantic appearance of a person. Training person re-identification (ReID) algorithms under the supervision of such attributes have proven to be effective in extracting local features which are important for ReID. Unlike person identity, attributes are consistent across different domains (or datasets). However, most of ReID datasets lack attribute annotations. On the other hand, there are several datasets labeled with sufficient attributes for the case of pedestrian attribute recognition. Exploiting such data for ReID purpose can be a way to alleviate the shortage of attribute annotations in ReID case. In this work, an unsupervised domain adaptive ReID feature learning framework is proposed to make full use of attribute annotations. We propose to transfer attribute-related features from their original domain to the ReID one: to this end, we introduce an adversarial discriminative domain adaptation method in order to learn domain invariant features for encoding semantic attributes. Experiments on three large-scale datasets validate the effectiveness of the proposed ReID framework.



### An Effective and Efficient Method for Detecting Hands in Egocentric Videos for Rehabilitation Applications
- **Arxiv ID**: http://arxiv.org/abs/1908.10406v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; J.3
- **Links**: [PDF](http://arxiv.org/pdf/1908.10406v1)
- **Published**: 2019-08-27 18:47:13+00:00
- **Updated**: 2019-08-27 18:47:13+00:00
- **Authors**: Ryan J. Visée, Jirapat Likitlersuang, José Zariffa
- **Comment**: 7 pages, 3 figures, 5 tables
- **Journal**: None
- **Summary**: Objective: Individuals with spinal cord injury (SCI) report upper limb function as their top recovery priority. To accurately represent the true impact of new interventions on patient function and independence, evaluation should occur in a natural setting. Wearable cameras can be used to monitor hand function at home, using computer vision to automatically analyze the resulting videos (egocentric video). A key step in this process, hand detection, is difficult to do robustly and reliably, hindering deployment of a complete monitoring system in the home and community. We propose an accurate and efficient hand detection method that uses a simple combination of existing detection and tracking algorithms. Methods: Detection, tracking, and combination methods were evaluated on a new hand detection dataset, consisting of 167,622 frames of egocentric videos collected on 17 individuals with SCI performing activities of daily living in a home simulation laboratory. Results: The F1-scores for the best detector and tracker alone (SSD and Median Flow) were 0.90$\pm$0.07 and 0.42$\pm$0.18, respectively. The best combination method, in which a detector was used to initialize and reset a tracker, resulted in an F1-score of 0.87$\pm$0.07 while being two times faster than the fastest detector alone. Conclusion: The combination of the fastest detector and best tracker improved the accuracy over online trackers while improving the speed of detectors. Significance: The method proposed here, in combination with wearable cameras, will help clinicians directly measure hand function in a patient's daily life at home, enabling independence after SCI.



### Complex Deep Learning Models for Denoising of Human Heart ECG signals
- **Arxiv ID**: http://arxiv.org/abs/1908.10417v3
- **DOI**: 10.5281/zenodo.3904247
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10417v3)
- **Published**: 2019-08-27 19:14:32+00:00
- **Updated**: 2020-06-23 10:14:02+00:00
- **Authors**: Corneliu Arsene
- **Comment**: 51 pages, 23 figures
- **Journal**: EUSIPCO.2019 (Pages 11- 18)
- **Summary**: Effective and powerful methods for denoising real electrocardiogram (ECG) signals are important for wearable sensors and devices. Deep Learning (DL) models have been used extensively in image processing and other domains with great success but only very recently have been used in processing ECG signals. This paper presents several DL models namely Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), Restricted Boltzmann Machine (RBM) together with the more conventional filtering methods (low pass filtering, high pass filtering, Notch filtering) and the standard wavelet-based technique for denoising EEG signals. These methods are trained, tested and evaluated on different synthetic and real ECG datasets taken from the MIT PhysioNet database and for different simulation conditions (i.e. various lengths of the ECG signals, single or multiple records). The results show the CNN model is a performant model that can be used for off-line denoising ECG applications where it is satisfactory to train on a clean part of an ECG signal from an ECG record, and then to test on the same ECG signal, which would have some high level of noise added to it. However, for real-time applications or near-real time applications, this task becomes more cumbersome, as the clean part of an ECG signal is very probable to be very limited in size. Therefore the solution put forth in this work is to train a CNN model on 1 second ECG noisy artificial multiple heartbeat data (i.e. ECG at effort), which was generated in a first instance based on few sequences of real signal heartbeat ECG data (i.e. ECG at rest). Afterwards it would be possible to use the trained CNN model in real life situations to denoise the ECG signal.



### Improving Visual Feature Extraction in Glacial Environments
- **Arxiv ID**: http://arxiv.org/abs/1908.10425v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.10425v2)
- **Published**: 2019-08-27 19:29:34+00:00
- **Updated**: 2019-11-30 03:18:30+00:00
- **Authors**: Steven D. Morad, Jeremy Nash, Shoya Higa, Russell Smith, Aaron Parness, Kobus Barnard
- **Comment**: None
- **Journal**: None
- **Summary**: Glacial science could benefit tremendously from autonomous robots, but previous glacial robots have had perception issues in these colorless and featureless environments, specifically with visual feature extraction. This translates to failures in visual odometry and visual navigation. Glaciologists use near-infrared imagery to reveal the underlying heterogeneous spatial structure of snow and ice, and we theorize that this hidden near-infrared structure could produce more and higher quality features than available in visible light. We took a custom camera rig to Igloo Cave at Mt. St. Helens to test our theory. The camera rig contains two identical machine vision cameras, one which was outfitted with multiple filters to see only near-infrared light. We extracted features from short video clips taken inside Igloo Cave at Mt. St. Helens, using three popular feature extractors (FAST, SIFT, and SURF). We quantified the number of features and their quality for visual navigation by comparing the resulting orientation estimates to ground truth. Our main contribution is the use of NIR longpass filters to improve the quantity and quality of visual features in icy terrain, irrespective of the feature extractor used.



### Embracing Imperfect Datasets: A Review of Deep Learning Solutions for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.10454v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10454v2)
- **Published**: 2019-08-27 20:25:52+00:00
- **Updated**: 2020-02-12 02:11:18+00:00
- **Authors**: Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey Chiang, Zhihao Wu, Xiaowei Ding
- **Comment**: Accepted for publication in the journal of Medical Image Analysis
- **Journal**: None
- **Summary**: The medical imaging literature has witnessed remarkable progress in high-performing segmentation models based on convolutional neural networks. Despite the new performance highs, the recent advanced segmentation models still require large, representative, and high quality annotated datasets. However, rarely do we have a perfect training dataset, particularly in the field of medical imaging, where data and annotations are both expensive to acquire. Recently, a large body of research has studied the problem of medical image segmentation with imperfect datasets, tackling two major dataset limitations: scarce annotations where only limited annotated data is available for training, and weak annotations where the training data has only sparse annotations, noisy annotations, or image-level annotations. In this article, we provide a detailed review of the solutions above, summarizing both the technical novelties and empirical results. We further compare the benefits and requirements of the surveyed methodologies and provide our recommended solutions. We hope this survey article increases the community awareness of the techniques that are available to handle imperfect medical image segmentation datasets.



### Self-Supervised Representation Learning via Neighborhood-Relational Encoding
- **Arxiv ID**: http://arxiv.org/abs/1908.10455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10455v1)
- **Published**: 2019-08-27 20:26:01+00:00
- **Updated**: 2019-08-27 20:26:01+00:00
- **Authors**: Mohammad Sabokrou, Mohammad Khalooei, Ehsan Adeli
- **Comment**: Accepted in International Conference on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach.



### Adversarial regression training for visualizing the progression of chronic obstructive pulmonary disease with chest x-rays
- **Arxiv ID**: http://arxiv.org/abs/1908.10468v1
- **DOI**: 10.1007/978-3-030-32226-7_76
- **Categories**: **eess.IV**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1908.10468v1)
- **Published**: 2019-08-27 21:14:12+00:00
- **Updated**: 2019-08-27 21:14:12+00:00
- **Authors**: Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga Tasdizen
- **Comment**: Accepted for MICCAI 2019
- **Journal**: International Conference on Medical Image Computing and
  Computer-Assisted Intervention. Springer, Cham, 2019. p. 685-693
- **Summary**: Knowledge of what spatial elements of medical images deep learning methods use as evidence is important for model interpretability, trustiness, and validation. There is a lack of such techniques for models in regression tasks. We propose a method, called visualization for regression with a generative adversarial network (VR-GAN), for formulating adversarial training specifically for datasets containing regression target values characterizing disease severity. We use a conditional generative adversarial network where the generator attempts to learn to shift the output of a regressor through creating disease effect maps that are added to the original images. Meanwhile, the regressor is trained to predict the original regression value for the modified images. A model trained with this technique learns to provide visualization for how the image would appear at different stages of the disease. We analyze our method in a dataset of chest x-rays associated with pulmonary function tests, used for diagnosing chronic obstructive pulmonary disease (COPD). For validation, we compute the difference of two registered x-rays of the same patient at different time points and correlate it to the generated disease effect map. The proposed method outperforms a technique based on classification and provides realistic-looking images, making modifications to images following what radiologists usually observe for this disease. Implementation code is available at https://github.com/ricbl/vrgan.



### Exploiting Global Camera Network Constraints for Unsupervised Video Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.10486v3
- **DOI**: 10.1109/TCSVT.2020.3043444
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10486v3)
- **Published**: 2019-08-27 22:35:43+00:00
- **Updated**: 2020-12-12 04:47:42+00:00
- **Authors**: Xueping Wang, Rameswar Panda, Min Liu, Yaonan Wang, Amit K Roy-Chowdhury
- **Comment**: This paper has been accepted to IEEE Transactions on Circuits and
  Systems for Video Technology (T-CSVT)
- **Journal**: None
- **Summary**: Many unsupervised approaches have been proposed recently for the video-based re-identification problem since annotations of samples across cameras are time-consuming. However, higher-order relationships across the entire camera network are ignored by these methods, leading to contradictory outputs when matching results from different camera pairs are combined. In this paper, we address the problem of unsupervised video-based re-identification by proposing a consistent cross-view matching (CCM) framework, in which global camera network constraints are exploited to guarantee the matched pairs are with consistency. Specifically, we first propose to utilize the first neighbor of each sample to discover relations among samples and find the groups in each camera. Additionally, a cross-view matching strategy followed by global camera network constraints is proposed to explore the matching relationships across the entire camera network. Finally, we learn metric models for camera pairs progressively by alternatively mining consistent cross-view matching pairs and updating metric models using these obtained matches. Rigorous experiments on two widely-used benchmarks for video re-identification demonstrate the superiority of the proposed method over current state-of-the-art unsupervised methods; for example, on the MARS dataset, our method achieves an improvement of 4.2\% over unsupervised methods, and even 2.5\% over one-shot supervision-based methods for rank-1 accuracy.



### Domain-Agnostic Learning with Anatomy-Consistent Embedding for Cross-Modality Liver Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.10489v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.10489v1)
- **Published**: 2019-08-27 22:44:41+00:00
- **Updated**: 2019-08-27 22:44:41+00:00
- **Authors**: Junlin Yang, Nicha C. Dvornek, Fan Zhang, Juntang Zhuang, Julius Chapiro, MingDe Lin, James S. Duncan
- **Comment**: None
- **Journal**: None
- **Summary**: Domain Adaptation (DA) has the potential to greatly help the generalization of deep learning models. However, the current literature usually assumes to transfer the knowledge from the source domain to a specific known target domain. Domain Agnostic Learning (DAL) proposes a new task of transferring knowledge from the source domain to data from multiple heterogeneous target domains. In this work, we propose the Domain-Agnostic Learning framework with Anatomy-Consistent Embedding (DALACE) that works on both domain-transfer and task-transfer to learn a disentangled representation, aiming to not only be invariant to different modalities but also preserve anatomical structures for the DA and DAL tasks in cross-modality liver segmentation. We validated and compared our model with state-of-the-art methods, including CycleGAN, Task Driven Generative Adversarial Network (TD-GAN), and Domain Adaptation via Disentangled Representations (DADR). For the DA task, our DALACE model outperformed CycleGAN, TD-GAN ,and DADR with DSC of 0.847 compared to 0.721, 0.793 and 0.806. For the DAL task, our model improved the performance with DSC of 0.794 from 0.522, 0.719 and 0.742 by CycleGAN, TD-GAN, and DADR. Further, we visualized the success of disentanglement, which added human interpretability of the learned meaningful representations. Through ablation analysis, we specifically showed the concrete benefits of disentanglement for downstream tasks and the role of supervision for better disentangled representation with segmentation consistency to be invariant to domains with the proposed Domain-Agnostic Module (DAM) and to preserve anatomical information with the proposed Anatomy-Preserving Module (APM).



### Detection of Backdoors in Trained Classifiers Without Access to the Training Set
- **Arxiv ID**: http://arxiv.org/abs/1908.10498v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.10498v3)
- **Published**: 2019-08-27 23:51:43+00:00
- **Updated**: 2020-08-19 15:52:35+00:00
- **Authors**: Zhen Xiang, David J. Miller, George Kesidis
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, a special type of data poisoning (DP) attack targeting Deep Neural Network (DNN) classifiers, known as a backdoor, was proposed. These attacks do not seek to degrade classification accuracy, but rather to have the classifier learn to classify to a target class whenever the backdoor pattern is present in a test example. Launching backdoor attacks does not require knowledge of the classifier or its training process - it only needs the ability to poison the training set with (a sufficient number of) exemplars containing a sufficiently strong backdoor pattern (labeled with the target class). Here we address post-training detection of backdoor attacks in DNN image classifiers, seldom considered in existing works, wherein the defender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean examples from the classification domain. This is an important scenario because a trained classifier may be the basis of e.g. a phone app that will be shared with many users. Detecting backdoors post-training may thus reveal a widespread attack. We propose a purely unsupervised anomaly detection (AD) defense against imperceptible backdoor attacks that: i) detects whether the trained DNN has been backdoor-attacked; ii) infers the source and target classes involved in a detected attack; iii) we even demonstrate it is possible to accurately estimate the backdoor pattern. We test our AD approach, in comparison with alternative defenses, for several backdoor patterns, data sets, and attack settings and demonstrate its favorability. Our defense essentially requires setting a single hyperparameter (the detection threshold), which can e.g. be chosen to fix the system's false positive rate.



