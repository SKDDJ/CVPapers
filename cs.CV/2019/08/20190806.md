# Arxiv Papers in cs.CV on 2019-08-06
### Attract or Distract: Exploit the Margin of Open Set
- **Arxiv ID**: http://arxiv.org/abs/1908.01925v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01925v2)
- **Published**: 2019-08-06 01:36:01+00:00
- **Updated**: 2019-08-10 05:27:13+00:00
- **Authors**: Qianyu Feng, Guoliang Kang, Hehe Fan, Yi Yang
- **Comment**: Presented at ICCV 2019
- **Journal**: None
- **Summary**: Open set domain adaptation aims to diminish the domain shift across domains, with partially shared classes. There exist unknown target samples out of the knowledge of source domain. Compared to the close set setting, how to separate the unknown (unshared) class from the known (shared) ones plays a key role. Whereas, previous methods did not emphasize the semantic structure of the open set data, which may introduce bias into the domain alignment and confuse the classifier around the decision boundary. In this paper, we exploit the semantic structure of open set data from two aspects: 1) Semantic Categorical Alignment, which aims to achieve good separability of target known classes by categorically aligning the centroid of target with the source. 2)Semantic Contrastive Mapping, which aims to push the unknown class away from the decision boundary. Empirically, we demonstrate that our method performs favourably against the state-of-the-art methods on representative benchmarks, e.g. Digit datasets and Office-31 datasets.



### Cascaded Revision Network for Novel Object Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.02726v1
- **DOI**: 10.1109/TCSVT.2020.2965966
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02726v1)
- **Published**: 2019-08-06 01:36:31+00:00
- **Updated**: 2019-08-06 01:36:31+00:00
- **Authors**: Qianyu Feng, Yu Wu, Hehe Fan, Chenggang Yan, Yi Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Image captioning, a challenging task where the machine automatically describes an image by sentences, has drawn significant attention in recent years. Despite the remarkable improvements of recent approaches, however, these methods are built upon a large set of training image-sentence pairs. The expensive labor efforts hence limit the captioning model to describe the wider world. In this paper, we present a novel network structure, Cascaded Revision Network, which aims at relieving the problem by equipping the model with out-of-domain knowledge. CRN first tries its best to describe an image using the existing vocabulary from in-domain knowledge. Due to the lack of out-of-domain knowledge, the caption may be inaccurate or include ambiguous words for the image with unknown (novel) objects. We propose to re-edit the primary captioning sentence by a series of cascaded operations. We introduce a perplexity predictor to find out which words are most likely to be inaccurate given the input image. Thereafter, we utilize external knowledge from a pre-trained object detection model and select more accurate words from detection results by the visual matching module. In the last step, we design a semantic matching module to ensure that the novel object is fit in the right position. By this novel cascaded captioning-revising mechanism, CRN can accurately describe images with unseen objects. We validate the proposed method with state-of-the-art performance on the held-out MSCOCO dataset as well as scale to ImageNet, demonstrating the effectiveness of this method.



### OD-GCN: Object Detection Boosted by Knowledge GCN
- **Arxiv ID**: http://arxiv.org/abs/1908.04385v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1908.04385v3)
- **Published**: 2019-08-06 02:23:29+00:00
- **Updated**: 2019-11-11 03:27:23+00:00
- **Authors**: Zheng Liu, Zidong Jiang, Wei Feng, Hui Feng
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Classical CNN based object detection methods only extract the objects' image features, but do not consider the high-level relationship among objects in context. In this article, the graph convolutional networks (GCN) is integrated into the object detection framework to exploit the benefit of category relationship among objects, which is able to provide extra confidence for any pre-trained object detection model in our framework. In experiments, we test several popular base detection models on COCO dataset. The results show promising improvement on mAP by 1-5pp. In addition, visualized analysis reveals the benchmark improvement is quite reasonable in human's opinion.



### Logic could be learned from images
- **Arxiv ID**: http://arxiv.org/abs/1908.01931v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01931v2)
- **Published**: 2019-08-06 02:24:31+00:00
- **Updated**: 2021-06-29 15:22:57+00:00
- **Authors**: Qian Guo, Yuhua Qian, Xinyan Liang, Yanhong She, Deyu Li, Jiye Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Logic reasoning is a significant ability of human intelligence and also an important task in artificial intelligence. The existing logic reasoning methods, quite often, need to design some reasoning patterns beforehand. This has led to an interesting question: can logic reasoning patterns be directly learned from given data? The problem is termed as a data concept logic. In this study, a learning logic task from images, called a LiLi task, first is proposed. This task is to learn and reason the logic relation from images, without presetting any reasoning patterns. As a preliminary exploration, we design six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition, Subtraction and Multiplication), in which each image is embedded with a n-digit number. It is worth noting that a learning model beforehand does not know the meaning of the n-digit numbers embedded in images and the relation between the input images and the output image. In order to tackle the task, in this work we use many typical neural network models and produce fruitful results. However, these models have the poor performances on the difficult logic task. For furthermore addressing this task, a novel network framework called a divide and conquer model by adding some label information is designed, achieving a high testing accuracy.



### Restoration of Non-rigidly Distorted Underwater Images using a Combination of Compressive Sensing and Local Polynomial Image Representations
- **Arxiv ID**: http://arxiv.org/abs/1908.01940v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01940v1)
- **Published**: 2019-08-06 03:33:50+00:00
- **Updated**: 2019-08-06 03:33:50+00:00
- **Authors**: Jerin Geo James, Pranay Agrawal, Ajit Rajwade
- **Comment**: Accepted in ICCV 2019 for oral presentation
- **Journal**: ICCV 2019
- **Summary**: Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage.



### Local Supports Global: Deep Camera Relocalization with Sequence Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1908.04391v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.04391v1)
- **Published**: 2019-08-06 03:49:52+00:00
- **Updated**: 2019-08-06 03:49:52+00:00
- **Authors**: Fei Xue, Xin Wang, Zike Yan, Qiuyuan Wang, Junqiu Wang, Hongbin Zha
- **Comment**: Accept to ICCV 2019
- **Journal**: None
- **Summary**: We propose to leverage the local information in image sequences to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure.



### Multiple Riemannian Manifold-valued Descriptors based Image Set Classification with Multi-Kernel Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01950v1)
- **Published**: 2019-08-06 04:21:31+00:00
- **Updated**: 2019-08-06 04:21:31+00:00
- **Authors**: Rui Wang, XiaoJun Wu, Josef Kittler
- **Comment**: 15 pages, 9 figures
- **Journal**: None
- **Summary**: The importance of wild video based image set recognition is becoming monotonically increasing. However, the contents of these collected videos are often complicated, and how to efficiently perform set modeling and feature extraction is a big challenge for set-based classification algorithms. In recent years, some proposed image set classification methods have made a considerable advance by modeling the original image set with covariance matrix, linear subspace, or Gaussian distribution. As a matter of fact, most of them just adopt a single geometric model to describe each given image set, which may lose some other useful information for classification. To tackle this problem, we propose a novel algorithm to model each image set from a multi-geometric perspective. Specifically, the covariance matrix, linear subspace, and Gaussian distribution are applied for set representation simultaneously. In order to fuse these multiple heterogeneous Riemannian manifoldvalued features, the well-equipped Riemannian kernel functions are first utilized to map them into high dimensional Hilbert spaces. Then, a multi-kernel metric learning framework is devised to embed the learned hybrid kernels into a lower dimensional common subspace for classification. We conduct experiments on four widely used datasets corresponding to four different classification tasks: video-based face recognition, set-based object categorization, video-based emotion recognition, and dynamic scene classification, to evaluate the classification performance of the proposed algorithm. Extensive experimental results justify its superiority over the state-of-the-art.



### Symmetry-constrained Rectification Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.01957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01957v1)
- **Published**: 2019-08-06 05:00:27+00:00
- **Updated**: 2019-08-06 05:00:27+00:00
- **Authors**: MingKun Yang, Yushuo Guan, Minghui Liao, Xin He, Kaigui Bian, Song Bai, Cong Yao, Xiang Bai
- **Comment**: The paper was accepted to ICCV2019
- **Journal**: None
- **Summary**: Reading text in the wild is a very challenging task due to the diversity of text instances and the complexity of natural scenes. Recently, the community has paid increasing attention to the problem of recognizing text instances with irregular shapes. One intuitive and effective way to handle this problem is to rectify irregular text to a canonical form before recognition. However, these methods might struggle when dealing with highly curved or distorted text instances. To tackle this issue, we propose in this paper a Symmetry-constrained Rectification Network (ScRN) based on local attributes of text instances, such as center line, scale and orientation. Such constraints with an accurate description of text shape enable ScRN to generate better rectification results than existing methods and thus lead to higher recognition accuracy. Our method achieves state-of-the-art performance on text with both regular and irregular shapes. Specifically, the system outperforms existing algorithms by a large margin on datasets that contain quite a proportion of irregular text instances, e.g., ICDAR 2015, SVT-Perspective and CUTE80.



### View N-gram Network for 3D Object Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1908.01958v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01958v2)
- **Published**: 2019-08-06 05:03:53+00:00
- **Updated**: 2019-08-15 14:30:34+00:00
- **Authors**: Xinwei He, Tengteng Huang, Song Bai, Xiang Bai
- **Comment**: The paper was accepted to ICCV 2019
- **Journal**: None
- **Summary**: How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.



### Real-Time Global Illumination Decomposition of Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.01961v2
- **DOI**: 10.1145/3374753
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01961v2)
- **Published**: 2019-08-06 05:23:45+00:00
- **Updated**: 2021-06-10 21:28:52+00:00
- **Authors**: Abhimitra Meka, Mohammad Shafiei, Michael Zollhoefer, Christian Richardt, Christian Theobalt
- **Comment**: None
- **Journal**: ACM Transactions on Graphics, 2021, 40(3), 22:1-16
- **Summary**: We propose the first approach for the decomposition of a monocular color video into direct and indirect illumination components in real time. We retrieve, in separate layers, the contribution made to the scene appearance by the scene reflectance, the light sources and the reflections from various coherent scene regions to one another. Existing techniques that invert global light transport require image capture under multiplexed controlled lighting, or only enable the decomposition of a single image at slow off-line frame rates. In contrast, our approach works for regular videos and produces temporally coherent decomposition layers at real-time frame rates. At the core of our approach are several sparsity priors that enable the estimation of the per-pixel direct and indirect illumination layers based on a small set of jointly estimated base reflectance colors. The resulting variational decomposition problem uses a new formulation based on sparse and dense sets of non-linear equations that we solve efficiently using a novel alternating data-parallel optimization strategy. We evaluate our approach qualitatively and quantitatively, and show improvements over the state of the art in this field, in both quality and runtime. In addition, we demonstrate various real-time appearance editing applications for videos with consistent illumination.



### REAPS: Towards Better Recognition of Fine-grained Images by Region Attending and Part Sequencing
- **Arxiv ID**: http://arxiv.org/abs/1908.01962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01962v1)
- **Published**: 2019-08-06 05:34:59+00:00
- **Updated**: 2019-08-06 05:34:59+00:00
- **Authors**: Peng Zhang, Xinyu Zhu, Zhanzhan Cheng, Shuigeng Zhou, Yi Niu
- **Comment**: PRCV 2019
- **Journal**: None
- **Summary**: Fine-grained image recognition has been a hot research topic in computer vision due to its various applications. The-state-of-the-art is the part/region-based approaches that first localize discriminative parts/regions, and then learn their fine-grained features. However, these approaches have some inherent drawbacks: 1) the discriminative feature representation of an object is prone to be disturbed by complicated background; 2) it is unreasonable and inflexible to fix the number of salient parts, because the intended parts may be unavailable under certain circumstances due to occlusion or incompleteness, and 3) the spatial correlation among different salient parts has not been thoroughly exploited (if not completely neglected). To overcome these drawbacks, in this paper we propose a new, simple yet robust method by building part sequence model on the attended object region. Concretely, we first try to alleviate the background effect by using a region attention mechanism to generate the attended region from the original image. Then, instead of localizing different salient parts and extracting their features separately, we learn the part representation implicitly by applying a mapping function on the serialized features of the object. Finally, we combine the region attending network and the part sequence learning network into a unified framework that can be trained end-to-end with only image-level labels. Our extensive experiments on three fine-grained benchmarks show that the proposed method achieves the state of the art performance.



### Contour Loss: Boundary-Aware Learning for Salient Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.01975v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D19
- **Links**: [PDF](http://arxiv.org/pdf/1908.01975v1)
- **Published**: 2019-08-06 06:23:50+00:00
- **Updated**: 2019-08-06 06:23:50+00:00
- **Authors**: Zixuan Chen, Huajun Zhou, Xiaohua Xie, Jianhuang Lai
- **Comment**: 12 pages, 7 figures
- **Journal**: None
- **Summary**: We present a learning model that makes full use of boundary information for salient object segmentation. Specifically, we come up with a novel loss function, i.e., Contour Loss, which leverages object contours to guide models to perceive salient object boundaries. Such a boundary-aware network can learn boundary-wise distinctions between salient objects and background, hence effectively facilitating the saliency detection. Yet the Contour Loss emphasizes on the local saliency. We further propose the hierarchical global attention module (HGAM), which forces the model hierarchically attend to global contexts, thus captures the global visual saliency. Comprehensive experiments on six benchmark datasets show that our method achieves superior performance over state-of-the-art ones. Moreover, our model has a real-time speed of 26 fps on a TITAN X GPU.



### Semi-supervised Skin Detection by Network with Mutual Guidance
- **Arxiv ID**: http://arxiv.org/abs/1908.01977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01977v1)
- **Published**: 2019-08-06 06:29:50+00:00
- **Updated**: 2019-08-06 06:29:50+00:00
- **Authors**: Yi He, Jiayuan Shi, Chuan Wang, Haibin Huang, Jiaming Liu, Guanbin Li, Risheng Liu, Jue Wang
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: In this paper we present a new data-driven method for robust skin detection from a single human portrait image. Unlike previous methods, we incorporate human body as a weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose a dual-task neural network for joint detection of skin and body via a semi-supervised learning strategy. The dual-task network contains a shared encoder but two decoders for skin and body separately. For each decoder, its output also serves as a guidance for its counterpart, making both decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection.



### Multi-view Deep Subspace Clustering Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.01978v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.01978v1)
- **Published**: 2019-08-06 06:44:43+00:00
- **Updated**: 2019-08-06 06:44:43+00:00
- **Authors**: Pengfei Zhu, Binyuan Hui, Changqing Zhang, Dawei Du, Longyin Wen, Qinghua Hu
- **Comment**: Submitted to the IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: Multi-view subspace clustering aims to discover the inherent structure by fusing multi-view complementary information. Most existing methods first extract multiple types of hand-crafted features and then learn a joint affinity matrix for clustering. The disadvantage lies in two aspects: 1) Multi-view relations are not embedded into feature learning. 2) The end-to-end learning manner of deep learning is not well used in multi-view clustering. To address the above issues, we propose a novel multi-view deep subspace clustering network (MvDSCN) by learning a multi-view self-representation matrix in an end-to-end manner. MvDSCN consists of two sub-networks, i.e., diversity network (Dnet) and universality network (Unet). A latent space is built upon deep convolutional auto-encoders and a self-representation matrix is learned in the latent space using a fully connected layer. Dnet learns view-specific self-representation matrices while Unet learns a common self-representation matrix for all views. To exploit the complementarity of multi-view representations, Hilbert Schmidt Independence Criterion (HSIC) is introduced as a diversity regularization, which can capture the non-linear and high-order inter-view relations. As different views share the same label space, the self-representation matrices of each view are aligned to the common one by a universality regularization. Experiments on both multi-feature and multi-modality learning validate the superiority of the proposed multi-view subspace clustering model.



### Learning Cross-Modal Deep Representations for Multi-Modal MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.01997v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01997v1)
- **Published**: 2019-08-06 07:42:44+00:00
- **Updated**: 2019-08-06 07:42:44+00:00
- **Authors**: Cheng Li, Hui Sun, Zaiyi Liu, Meiyun Wang, Hairong Zheng, Shanshan Wang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Multi-modal magnetic resonance imaging (MRI) is essential in clinics for comprehensive diagnosis and surgical planning. Nevertheless, the segmentation of multi-modal MR images tends to be time-consuming and challenging. Convolutional neural network (CNN)-based multi-modal MR image analysis commonly proceeds with multiple down-sampling streams fused at one or several layers. Although inspiring performance has been achieved, the feature fusion is usually conducted through simple summation or concatenation without optimization. In this work, we propose a supervised image fusion method to selectively fuse the useful information from different modalities and suppress the respective noise signals. Specifically, an attention block is introduced as guidance for the information selection. From the different modalities, one modality that contributes most to the results is selected as the master modality, which supervises the information selection of the other assistant modalities. The effectiveness of the proposed method is confirmed through breast mass segmentation in MR images of two modalities and better segmentation results are achieved compared to the state-of-the-art methods.



### Few-Shot Object Detection with Attention-RPN and Multi-Relation Detector
- **Arxiv ID**: http://arxiv.org/abs/1908.01998v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01998v4)
- **Published**: 2019-08-06 07:43:35+00:00
- **Updated**: 2020-05-10 03:09:53+00:00
- **Authors**: Qi Fan, Wei Zhuo, Chi-Keung Tang, Yu-Wing Tai
- **Comment**: CVPR2020 Camera Ready. (Fix Figure 3 and Table 5. More implementation
  details in the supplementary material.)
- **Journal**: None
- **Summary**: Conventional methods for object detection typically require a substantial amount of training data and preparing such high-quality training data is very labor-intensive. In this paper, we propose a novel few-shot object detection network that aims at detecting objects of unseen categories with only a few annotated examples. Central to our method are our Attention-RPN, Multi-Relation Detector and Contrastive Training strategy, which exploit the similarity between the few shot support set and query set to detect novel objects while suppressing false detection in the background. To train our network, we contribute a new dataset that contains 1000 categories of various objects with high-quality annotations. To the best of our knowledge, this is one of the first datasets specifically designed for few-shot object detection. Once our few-shot network is trained, it can detect objects of unseen categories without further training or fine-tuning. Our method is general and has a wide range of potential applications. We produce a new state-of-the-art performance on different datasets in the few-shot setting. The dataset link is https://github.com/fanq15/Few-Shot-Object-Detection-Dataset.



### Generalised Zero-Shot Learning with a Classifier Ensemble over Multi-Modal Embedding Spaces
- **Arxiv ID**: http://arxiv.org/abs/1908.02013v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02013v1)
- **Published**: 2019-08-06 08:35:13+00:00
- **Updated**: 2019-08-06 08:35:13+00:00
- **Authors**: Rafael Felix, Ben Harwood, Michele Sasdelli, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: Generalised zero-shot learning (GZSL) methods aim to classify previously seen and unseen visual classes by leveraging the semantic information of those classes. In the context of GZSL, semantic information is non-visual data such as a text description of both seen and unseen classes. Previous GZSL methods have utilised transformations between visual and semantic embedding spaces, as well as the learning of joint spaces that include both visual and semantic information. In either case, classification is then performed on a single learned space. We argue that each embedding space contains complementary information for the GZSL problem. By using just a visual, semantic or joint space some of this information will invariably be lost. In this paper, we demonstrate the advantages of our new GZSL method that combines the classification of visual, semantic and joint spaces. Most importantly, this ensembling allows for more information from the source domains to be seen during classification. An additional contribution of our work is the application of a calibration procedure for each classifier in the ensemble. This calibration mitigates the problem of model selection when combining the classifiers. Lastly, our proposed method achieves state-of-the-art results on the CUB, AWA1 and AWA2 benchmark data sets and provides competitive performance on the SUN data set.



### Consensus Maximization Tree Search Revisited
- **Arxiv ID**: http://arxiv.org/abs/1908.02021v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02021v3)
- **Published**: 2019-08-06 08:54:04+00:00
- **Updated**: 2019-08-25 12:12:19+00:00
- **Authors**: Zhipeng Cai, Tat-Jun Chin, Vladlen Koltun
- **Comment**: Accepted as oral presentation to ICCV'19
- **Journal**: None
- **Summary**: Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach.



### Full-Stack Filters to Build Minimum Viable CNNs
- **Arxiv ID**: http://arxiv.org/abs/1908.02023v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02023v1)
- **Published**: 2019-08-06 08:55:47+00:00
- **Updated**: 2019-08-06 08:55:47+00:00
- **Authors**: Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Dacheng Tao, Chang Xu
- **Comment**: Tech report
- **Journal**: None
- **Summary**: Deep convolutional neural networks (CNNs) are usually over-parameterized, which cannot be easily deployed on edge devices such as mobile phones and smart cameras. Existing works used to decrease the number or size of requested convolution filters for a minimum viable CNN on edge devices. In contrast, this paper introduces filters that are full-stack and can be used to generate many more sub-filters. Weights of these sub-filters are inherited from full-stack filters with the help of different binary masks. Orthogonal constraints are applied over binary masks to decrease their correlation and promote the diversity of generated sub-filters. To preserve the same volume of output feature maps, we can naturally reduce the number of established filters by only maintaining a few full-stack filters and a set of binary masks. We also conduct theoretical analysis on the memory cost and an efficient implementation is introduced for the convolution of the proposed filters. Experiments on several benchmark datasets and CNN models demonstrate that the proposed method is able to construct minimum viable convolution networks of comparable performance.



### Model-based Convolutional De-Aliasing Network Learning for Parallel MR Imaging
- **Arxiv ID**: http://arxiv.org/abs/1908.02054v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02054v1)
- **Published**: 2019-08-06 10:20:56+00:00
- **Updated**: 2019-08-06 10:20:56+00:00
- **Authors**: Yanxia Chen, Taohui Xiao, Cheng Li, Qiegen Liu, Shanshan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Parallel imaging has been an essential technique to accelerate MR imaging. Nevertheless, the acceleration rate is still limited due to the ill-condition and challenges associated with the undersampled reconstruction. In this paper, we propose a model-based convolutional de-aliasing network with adaptive parameter learning to achieve accurate reconstruction from multi-coil undersampled k-space data. Three main contributions have been made: a de-aliasing reconstruction model was proposed to accelerate parallel MR imaging with deep learning exploring both spatial redundancy and multi-coil correlations; a split Bregman iteration algorithm was developed to solve the model efficiently; and unlike most existing parallel imaging methods which rely on the accuracy of the estimated multi-coil sensitivity, the proposed method can perform parallel reconstruction from undersampled data without explicit sensitivity calculation. Evaluations were conducted on \emph{in vivo} brain dataset with a variety of undersampling patterns and different acceleration factors. Our results demonstrated that this method could achieve superior performance in both quantitative and qualitative analysis, compared to three state-of-the-art methods.



### Fast Fourier Color Constancy and Grayness Index for ISPA Illumination Estimation Challenge
- **Arxiv ID**: http://arxiv.org/abs/1908.02076v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02076v2)
- **Published**: 2019-08-06 11:03:41+00:00
- **Updated**: 2019-09-17 11:53:40+00:00
- **Authors**: Yanlin Qian, Ke Chen, Huanglin Yu
- **Comment**: The 3-page challenge report for the Illumination Estimation
  Challenge, in the Int'l Workshop on Color Vision, affiliated to the 11th
  Int'l Symposium on Image and Signal Processing and Analysis (ISPA2019,
  Dubrovnik, Croatia). Second version
- **Journal**: None
- **Summary**: We briefly introduce two submissions to the Illumination Estimation Challenge, in the Int'l Workshop on Color Vision, affiliated to the 11th Int'l Symposium on Image and Signal Processing and Analysis. The Fourier-transform-based submission is ranked 3rd, and the statistical Gray-pixel-based one ranked 6th.



### BCN20000: Dermoscopic Lesions in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1908.02288v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02288v2)
- **Published**: 2019-08-06 11:16:07+00:00
- **Updated**: 2019-08-30 09:42:42+00:00
- **Authors**: Marc Combalia, Noel C. F. Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C. Halpern, Susana Puig, Josep Malvehy
- **Comment**: Abstract for BCN20000
- **Journal**: None
- **Summary**: This article summarizes the BCN20000 dataset, composed of 19424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Cl\'inic in Barcelona. With this dataset, we aim to study the problem of unconstrained classification of dermoscopic images of skin cancer, including lesions found in hard-to-diagnose locations (nails and mucosa), large lesions which do not fit in the aperture of the dermoscopy device, and hypo-pigmented lesions. The BCN20000 will be provided to the participants of the ISIC Challenge 2019, where they will be asked to train algorithms to classify dermoscopic images of skin cancer automatically.



### AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.02095v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02095v1)
- **Published**: 2019-08-06 12:06:12+00:00
- **Updated**: 2019-08-06 12:06:12+00:00
- **Authors**: Gozde Nur Gunesli, Cenk Sokmensuer, Cigdem Gunduz-Demir
- **Comment**: This work has been submitted to the IEEE for possible publication
- **Journal**: None
- **Summary**: Dense prediction models are widely used for image segmentation. One important challenge is to sufficiently train these models to yield good generalizations for hard-to-learn pixels. A typical group of such hard-to-learn pixels are boundaries between instances. Many studies have proposed to give specific attention to learning the boundary pixels. They include designing multi-task networks with an additional task of boundary prediction and increasing the weights of boundary pixels' predictions in the loss function. Such strategies require defining what to attend beforehand and incorporating this defined attention to the learning model. However, there may exist other groups of hard-to-learn pixels and manually defining and incorporating the appropriate attention for each group may not be feasible. In order to provide a more attainable and scalable solution, this paper proposes AttentionBoost, which is a new multi-attention learning model based on adaptive boosting. AttentionBoost designs a multi-stage network and introduces a new loss adjustment mechanism for a dense prediction model to adaptively learn what to attend at each stage directly on image data without necessitating any prior definition about what to attend. This mechanism modulates the attention of each stage to correct the mistakes of previous stages, by adjusting the loss weight of each pixel prediction separately with respect to how accurate the previous stages are on this pixel. This mechanism enables AttentionBoost to learn different attentions for different pixels at the same stage, according to difficulty of learning these pixels, as well as multiple attentions for the same pixel at different stages, according to confidence of these stages on their predictions for this pixel. Using gland segmentation as a showcase application, our experiments demonstrate that AttentionBoost improves the results of its counterparts.



### SkrGAN: Sketching-rendering Unconditional Generative Adversarial Networks for Medical Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1908.04346v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.04346v1)
- **Published**: 2019-08-06 12:14:56+00:00
- **Updated**: 2019-08-06 12:14:56+00:00
- **Authors**: Tianyang Zhang, Huazhu Fu, Yitian Zhao, Jun Cheng, Mengjie Guo, Zaiwang Gu, Bing Yang, Yuting Xiao, Shenghua Gao, Jiang Liu
- **Comment**: Accepted to MICCAI 2019
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have the capability of synthesizing images, which have been successfully applied to medical image synthesis tasks. However, most of existing methods merely consider the global contextual information and ignore the fine foreground structures, e.g., vessel, skeleton, which may contain diagnostic indicators for medical image analysis. Inspired by human painting procedure, which is composed of stroking and color rendering steps, we propose a Sketching-rendering Unconditional Generative Adversarial Network (SkrGAN) to introduce a sketch prior constraint to guide the medical image generation. In our SkrGAN, a sketch guidance module is utilized to generate a high quality structural sketch from random noise, then a color render mapping is used to embed the sketch-based representations and resemble the background appearances. Experimental results show that the proposed SkrGAN achieves the state-of-the-art results in synthesizing images for various image modalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). In addition, we also show that the performances of medical image segmentation method have been improved by using our synthesized images as data augmentation.



### Exploiting Channel Similarity for Accelerating Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.02620v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.02620v1)
- **Published**: 2019-08-06 12:44:30+00:00
- **Updated**: 2019-08-06 12:44:30+00:00
- **Authors**: Yunxiang Zhang, Chenglong Zhao, Bingbing Ni, Jian Zhang, Haoran Deng
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: To address the limitations of existing magnitude-based pruning algorithms in cases where model weights or activations are of large and similar magnitude, we propose a novel perspective to discover parameter redundancy among channels and accelerate deep CNNs via channel pruning. Precisely, we argue that channels revealing similar feature information have functional overlap and that most channels within each such similarity group can be removed without compromising model's representational power. After deriving an effective metric for evaluating channel similarity through probabilistic modeling, we introduce a pruning algorithm via hierarchical clustering of channels. In particular, the proposed algorithm does not rely on sparsity training techniques or complex data-driven optimization and can be directly applied to pre-trained models. Extensive experiments on benchmark datasets strongly demonstrate the superior acceleration performance of our approach over prior arts. On ImageNet, our pruned ResNet-50 with 30% FLOPs reduced outperforms the baseline model.



### Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.02116v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02116v3)
- **Published**: 2019-08-06 12:57:24+00:00
- **Updated**: 2019-08-13 17:09:26+00:00
- **Authors**: Xuanyi Dong, Yi Yang
- **Comment**: This paper was accepted to IEEE ICCV 2019. Model codes are publicly
  available on GitHub: https://github.com/D-X-Y/landmark-detection
- **Journal**: None
- **Summary**: Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data to become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.



### City-Scale Road Extraction from Satellite Imagery v2: Road Speeds and Travel Times
- **Arxiv ID**: http://arxiv.org/abs/1908.09715v3
- **DOI**: 10.1109/WACV45572.2020.9093593
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09715v3)
- **Published**: 2019-08-06 12:59:28+00:00
- **Updated**: 2020-05-29 20:32:08+00:00
- **Authors**: Adam Van Etten
- **Comment**: In Proceedings WACV 2020. 8 pages, 11 figures, 5 appendices. arXiv
  admin note: text overlap with arXiv:1904.09901
- **Journal**: None
- **Summary**: Automated road network extraction from remote sensing imagery remains a significant challenge despite its importance in a broad array of applications. To this end, we explore road network extraction at scale with inference of semantic features of the graph, identifying speed limits and route travel times for each roadway. We call this approach City-Scale Road Extraction from Satellite Imagery v2 (CRESIv2), Including estimates for travel time permits true optimal routing (rather than just the shortest geographic distance), which is not possible with existing remote sensing imagery based methods. We evaluate our method using two sources of labels (OpenStreetMap, and those from the SpaceNet dataset), and find that models both trained and tested on SpaceNet labels outperform OpenStreetMap labels by greater than 60%. We quantify the performance of our algorithm with the Average Path Length Similarity (APLS) and map topology (TOPO) graph-theoretic metrics over a diverse test area covering four cities in the SpaceNet dataset. For a traditional edge weight of geometric distance, we find an aggregate of 5% improvement over existing methods for SpaceNet data. We also test our algorithm on Google satellite imagery with OpenStreetMap labels, and find a 23% improvement over previous work. Metric scores decrease by only 4% on large graphs when using travel time rather than geometric distance for edge weights, indicating that optimizing routing for travel time is feasible with this approach.



### Explaining Convolutional Neural Networks using Softmax Gradient Layer-wise Relevance Propagation
- **Arxiv ID**: http://arxiv.org/abs/1908.04351v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1908.04351v3)
- **Published**: 2019-08-06 13:05:04+00:00
- **Updated**: 2019-11-07 07:48:43+00:00
- **Authors**: Brian Kenji Iwana, Ryohei Kuroki, Seiichi Uchida
- **Comment**: Published at ICCV 2019 Workshops
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNN) have become state-of-the-art in the field of image classification. However, not everything is understood about their inner representations. This paper tackles the interpretability and explainability of the predictions of CNNs for multi-class classification problems. Specifically, we propose a novel visualization method of pixel-wise input attribution called Softmax-Gradient Layer-wise Relevance Propagation (SGLRP). The proposed model is a class discriminate extension to Deep Taylor Decomposition (DTD) using the gradient of softmax to back propagate the relevance of the output probability to the input image. Through qualitative and quantitative analysis, we demonstrate that SGLRP can successfully localize and attribute the regions on input images which contribute to a target object's classification. We show that the proposed method excels at discriminating the target objects class from the other possible objects in the images. We confirm that SGLRP performs better than existing Layer-wise Relevance Propagation (LRP) based methods and can help in the understanding of the decision process of CNNs.



### Addressing Data Bias Problems for Chest X-ray Image Report Generation
- **Arxiv ID**: http://arxiv.org/abs/1908.02123v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02123v1)
- **Published**: 2019-08-06 13:18:07+00:00
- **Updated**: 2019-08-06 13:18:07+00:00
- **Authors**: Philipp Harzig, Yan-Ying Chen, Francine Chen, Rainer Lienhart
- **Comment**: Oral at BMVC 2019
- **Journal**: None
- **Summary**: Automatic medical report generation from chest X-ray images is one possibility for assisting doctors to reduce their workload. However, the different patterns and data distribution of normal and abnormal cases can bias machine learning models. Previous attempts did not focus on isolating the generation of the abnormal and normal sentences in order to increase the variability of generated paragraphs. To address this, we propose to separate abnormal and normal sentence generation by using two different word LSTMs in a hierarchical LSTM model. We conduct an analysis on the distinctiveness of generated sentences compared to the BLEU score, which increases when less distinct reports are generated. We hope our findings will help to encourage the development of new metrics to better verify methods of automatic medical report generation.



### Semi-Supervised Adversarial Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.02126v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02126v1)
- **Published**: 2019-08-06 13:19:24+00:00
- **Updated**: 2019-08-06 13:19:24+00:00
- **Authors**: Rongrong Ji, Ke Li, Yan Wang, Xiaoshuai Sun, Feng Guo, Xiaowei Guo, Yongjian Wu, Feiyue Huang, Jiebo Luo
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of monocular depth estimation when only a limited number of training image-depth pairs are available. To achieve a high regression accuracy, the state-of-the-art estimation methods rely on CNNs trained with a large number of image-depth pairs, which are prohibitively costly or even infeasible to acquire. Aiming to break the curse of such expensive data collections, we propose a semi-supervised adversarial learning framework that only utilizes a small number of image-depth pairs in conjunction with a large number of easily-available monocular images to achieve high performance. In particular, we use one generator to regress the depth and two discriminators to evaluate the predicted depth , i.e., one inspects the image-depth pair while the other inspects the depth channel alone. These two discriminators provide their feedbacks to the generator as the loss to generate more realistic and accurate depth predictions. Experiments show that the proposed approach can (1) improve most state-of-the-art models on the NYUD v2 dataset by effectively leveraging additional unlabeled data sources; (2) reach state-of-the-art accuracy when the training set is small, e.g., on the Make3D dataset; (3) adapt well to an unseen new dataset (Make3D in our case) after training on an annotated dataset (KITTI in our case).



### Aligning Linguistic Words and Visual Semantic Units for Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.02127v1
- **DOI**: 10.1145/3343031.3350943
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.MM, I.5.4; I.4.9; I.4.10; I.2.7
- **Links**: [PDF](http://arxiv.org/pdf/1908.02127v1)
- **Published**: 2019-08-06 13:19:24+00:00
- **Updated**: 2019-08-06 13:19:24+00:00
- **Authors**: Longteng Guo, Jing Liu, Jinhui Tang, Jiangwei Li, Wei Luo, Hanqing Lu
- **Comment**: 8 pages, 5 figures. Accepted by ACM MM 2019
- **Journal**: None
- **Summary**: Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.



### Deep Self-Learning From Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1908.02160v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02160v2)
- **Published**: 2019-08-06 13:43:58+00:00
- **Updated**: 2019-08-20 08:37:35+00:00
- **Authors**: Jiangfan Han, Ping Luo, Xiaogang Wang
- **Comment**: Accepted by IEEE International Conference on Computer Vision (ICCV)
  2019
- **Journal**: None
- **Summary**: ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Clothing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings.



### Semiparametric Wavelet-based JPEG IV Estimator for endogenously truncated data
- **Arxiv ID**: http://arxiv.org/abs/1908.02166v1
- **DOI**: 10.1109/ACCESS.2019.2929571
- **Categories**: **stat.ME**, cs.CV, cs.LG, econ.EM, stat.ML, I.2.6; E.4.0; G.1.2; I.4.2; I.4.4; I.4.5; I.5.4; G.1.3
- **Links**: [PDF](http://arxiv.org/pdf/1908.02166v1)
- **Published**: 2019-08-06 13:54:52+00:00
- **Updated**: 2019-08-06 13:54:52+00:00
- **Authors**: Nir Billfeld, Moshe Kim
- **Comment**: 18 pages
- **Journal**: IEEE Access, 7, 99602-99621 (2019)
- **Summary**: A new and an enriched JPEG algorithm is provided for identifying redundancies in a sequence of irregular noisy data points which also accommodates a reference-free criterion function. Our main contribution is by formulating analytically (instead of approximating) the inverse of the transpose of JPEGwavelet transform without involving matrices which are computationally cumbersome. The algorithm is suitable for the widely-spread situations where the original data distribution is unobservable such as in cases where there is deficient representation of the entire population in the training data (in machine learning) and thus the covariate shift assumption is violated. The proposed estimator corrects for both biases, the one generated by endogenous truncation and the one generated by endogenous covariates. Results from utilizing 2,000,000 different distribution functions verify the applicability and high accuracy of our procedure to cases in which the disturbances are neither jointly nor marginally normally distributed.



### An attempt at beating the 3D U-Net
- **Arxiv ID**: http://arxiv.org/abs/1908.02182v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02182v2)
- **Published**: 2019-08-06 14:28:17+00:00
- **Updated**: 2019-10-04 11:03:40+00:00
- **Authors**: Fabian Isensee, Klaus H. Maier-Hein
- **Comment**: None
- **Journal**: None
- **Summary**: The U-Net is arguably the most successful segmentation architecture in the medical domain. Here we apply a 3D U-Net to the 2019 Kidney and Kidney Tumor Segmentation Challenge and attempt to improve upon it by augmenting it with residual and pre-activation residual blocks. Cross-validation results on the training cases suggest only very minor, barely measurable improvements. Due to marginally higher dice scores, the residual 3D U-Net is chosen for test set prediction. With a Composite Dice score of 91.23 on the test set, our method outperformed all 105 competing teams and won the KiTS2019 challenge by a small margin.



### Neural Blind Deconvolution Using Deep Priors
- **Arxiv ID**: http://arxiv.org/abs/1908.02197v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02197v2)
- **Published**: 2019-08-06 15:03:44+00:00
- **Updated**: 2020-03-18 04:40:36+00:00
- **Authors**: Dongwei Ren, Kai Zhang, Qilong Wang, Qinghua Hu, Wangmeng Zuo
- **Comment**: Accepted to CVPR 2020. The source code is available at
  https://github.com/csdwren/SelfDeblur, and the supplementary file is at
  https://csdwren.github.io/papers/SelfDeblur_supp.pdf
- **Journal**: None
- **Summary**: Blind deconvolution is a classical yet challenging low-level vision problem with many real-world applications. Traditional maximum a posterior (MAP) based methods rely heavily on fixed and handcrafted priors that certainly are insufficient in characterizing clean images and blur kernels, and usually adopt specially designed alternating minimization to avoid trivial solution. In contrast, existing deep motion deblurring networks learn from massive training images the mapping to clean image or blur kernel, but are limited in handling various complex and large size blur kernels. To connect MAP and deep models, we in this paper present two generative networks for respectively modeling the deep priors of clean image and blur kernel, and propose an unconstrained neural optimization solution to blind deconvolution. In particular, we adopt an asymmetric Autoencoder with skip connections for generating latent clean image, and a fully-connected network (FCN) for generating blur kernel. Moreover, the SoftMax nonlinearity is applied to the output layer of FCN to meet the non-negative and equality constraints. The process of neural optimization can be explained as a kind of "zero-shot" self-supervised learning of the generative networks, and thus our proposed method is dubbed SelfDeblur. Experimental results show that our SelfDeblur can achieve notable quantitative gains as well as more visually plausible deblurring results in comparison to state-of-the-art blind deconvolution methods on benchmark datasets and real-world blurry images. The source code is available at https://github.com/csdwren/SelfDeblur



### MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1908.02199v1
- **DOI**: 10.1145/3343031.3350887
- **Categories**: **cs.CV**, cs.LG, 2010
- **Links**: [PDF](http://arxiv.org/pdf/1908.02199v1)
- **Published**: 2019-08-06 15:06:21+00:00
- **Updated**: 2019-08-06 15:06:21+00:00
- **Authors**: Chen Ma, Chenxu Zhao, Hailin Shi, Li Chen, Junhai Yong, Dan Zeng
- **Comment**: 10 pages, 2 figures, accepted as the conference paper of Proceedings
  of the 27th ACM International Conference on Multimedia (MM'19)
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) are vulnerable to adversarial attack which is maliciously implemented by adding human-imperceptible perturbation to images and thus leads to incorrect prediction. Existing studies have proposed various methods to detect the new adversarial attacks. However, new attack methods keep evolving constantly and yield new adversarial examples to bypass the existing detectors. It needs to collect tens of thousands samples to train detectors, while the new attacks evolve much more frequently than the high-cost data collection. Thus, this situation leads the newly evolved attack samples to remain in small scales. To solve such few-shot problem with the evolving attack, we propose a meta-learning based robust detection method to detect new adversarial attacks with limited examples. Specifically, the learning consists of a double-network framework: a task-dedicated network and a master network which alternatively learn the detection capability for either seen attack or a new attack. To validate the effectiveness of our approach, we construct the benchmarks with few-shot-fashion protocols based on three conventional datasets, i.e. CIFAR-10, MNIST and Fashion-MNIST. Comprehensive experiments are conducted on them to verify the superiority of our approach with respect to the traditional adversarial attack detection methods.



### Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.02231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02231v2)
- **Published**: 2019-08-06 16:11:48+00:00
- **Updated**: 2019-08-07 13:24:10+00:00
- **Authors**: Ziyuan Huang, Changhong Fu, Yiming Li, Fuling Lin, Peng Lu
- **Comment**: iccv 2019 accepted, 10 pages, 8 figures
- **Journal**: None
- **Summary**: Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.



### Deep Learning for Detecting Building Defects Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.04392v1
- **DOI**: 10.20944/preprints201908.0068.v1
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.ML, 68T0
- **Links**: [PDF](http://arxiv.org/pdf/1908.04392v1)
- **Published**: 2019-08-06 16:21:10+00:00
- **Updated**: 2019-08-06 16:21:10+00:00
- **Authors**: Husein Perez, Joseph H. M. Tah, Amir Mosavi
- **Comment**: 29 pages, 11 figures
- **Journal**: None
- **Summary**: Clients are increasingly looking for fast and effective means to quickly and frequently survey and communicate the condition of their buildings so that essential repairs and maintenance work can be done in a proactive and timely manner before it becomes too dangerous and expensive. Traditional methods for this type of work commonly comprise of engaging building surveyors to undertake a condition assessment which involves a lengthy site inspection to produce a systematic recording of the physical condition of the building elements, including cost estimates of immediate and projected long-term costs of renewal, repair and maintenance of the building. Current asset condition assessment procedures are extensively time consuming, laborious, and expensive and pose health and safety threats to surveyors, particularly at height and roof levels which are difficult to access. This paper aims at evaluating the application of convolutional neural networks (CNN) towards an automated detection and localisation of key building defects, e.g., mould, deterioration, and stain, from images. The proposed model is based on pre-trained CNN classifier of VGG-16 (later compaired with ResNet-50, and Inception models), with class activation mapping (CAM) for object localisation. The challenges and limitations of the model in real-life applications have been identified. The proposed model has proven to be robust and able to accurately detect and localise building defects. The approach is being developed with the potential to scale-up and further advance to support automated detection of defects and deterioration of buildings in real-time using mobile devices and drones.



### An Unsupervised, Iterative N-Dimensional Point-Set Registration Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1908.04384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04384v1)
- **Published**: 2019-08-06 17:03:53+00:00
- **Updated**: 2019-08-06 17:03:53+00:00
- **Authors**: A. Pasha Hosseinbor, R. Zhdanov, A. Ushveridze
- **Comment**: arXiv admin note: text overlap with arXiv:1702.01870
- **Journal**: None
- **Summary**: An unsupervised, iterative point-set registration algorithm for an unlabeled (i.e. correspondence between points is unknown) N-dimensional Euclidean point-cloud is proposed. It is based on linear least squares, and considers all possible point pairings and iteratively aligns the two sets until the number of point pairs does not exceed the maximum number of allowable one-to-one pairings.



### ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.02265v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.02265v1)
- **Published**: 2019-08-06 17:33:52+00:00
- **Updated**: 2019-08-06 17:33:52+00:00
- **Authors**: Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.



### Relative Afferent Pupillary Defect Screening through Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.02300v1
- **DOI**: 10.1109/JBHI.2019.2933773
- **Categories**: **cs.CV**, eess.IV, eess.SP, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1908.02300v1)
- **Published**: 2019-08-06 18:03:47+00:00
- **Updated**: 2019-08-06 18:03:47+00:00
- **Authors**: Dogancan Temel, Melvin J. Mathew, Ghassan AlRegib, Yousuf M. Khalifa
- **Comment**: 8 pages, 7 figures, 4 tables. IEEE Journal of Biomedical and Health
  Informatics, 2019
- **Journal**: None
- **Summary**: Abnormalities in pupillary light reflex can indicate optic nerve disorders that may lead to permanent visual loss if not diagnosed in an early stage. In this study, we focus on relative afferent pupillary defect (RAPD), which is based on the difference between the reactions of the eyes when they are exposed to light stimuli. Incumbent RAPD assessment methods are based on subjective practices that can lead to unreliable measurements. To eliminate subjectivity and obtain reliable measurements, we introduced an automated framework to detect RAPD. For validation, we conducted a clinical study with lab-on-a-headset, which can perform automated light reflex test. In addition to benchmarking handcrafted algorithms, we proposed a transfer learning-based approach that transformed a deep learning-based generic object recognition algorithm into a pupil detector. Based on the conducted experiments, proposed algorithm RAPDNet can achieve a sensitivity and a specificity of 90.6% over 64 test cases in a balanced set, which corresponds to an AUC of 0.929 in ROC analysis. According to our benchmark with three handcrafted algorithms and nine performance metrics, RAPDNet outperforms all other algorithms in every performance category.



### A fast multi-object tracking system using an object detector ensemble
- **Arxiv ID**: http://arxiv.org/abs/1908.04349v1
- **DOI**: 10.1109/ColCACI.2019.8781972
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04349v1)
- **Published**: 2019-08-06 20:23:31+00:00
- **Updated**: 2019-08-06 20:23:31+00:00
- **Authors**: Richard Cobos, Jefferson Hernandez, Andres G. Abad
- **Comment**: 5 pages, 4 figures, 1 table, published in 2019 IEEE Colombian
  Conference on Applications in Computational Intelligence (ColCACI)
- **Journal**: None
- **Summary**: Multiple-Object Tracking (MOT) is of crucial importance for applications such as retail video analytics and video surveillance. Object detectors are often the computational bottleneck of modern MOT systems, limiting their use for real-time applications. In this paper, we address this issue by leveraging on an ensemble of detectors, each running every f frames. We measured the performance of our system in the MOT16 benchmark. The proposed model surpassed other online entries of the MOT16 challenge in speed, while maintaining an acceptable accuracy.



### Estimating sex and age for forensic applications using machine learning based on facial measurements from frontal cephalometric landmarks
- **Arxiv ID**: http://arxiv.org/abs/1908.02353v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02353v1)
- **Published**: 2019-08-06 20:33:11+00:00
- **Updated**: 2019-08-06 20:33:11+00:00
- **Authors**: Lucas F. Porto, Laise N. Correia Lima, Ademir Franco, Donald M. Pianto, Carlos Eduardo Machado Palhares, Donald M. Pianto, Flavio de Barros Vidal
- **Comment**: 17 pages, 17 figures
- **Journal**: None
- **Summary**: Facial analysis permits many investigations some of the most important of which are craniofacial identification, facial recognition, and age and sex estimation. In forensics, photo-anthropometry describes the study of facial growth and allows the identification of patterns in facial skull development by using a group of cephalometric landmarks to estimate anthropological information. In several areas, automation of manual procedures has achieved advantages over and similar measurement confidence as a forensic expert. This manuscript presents an approach using photo-anthropometric indexes, generated from frontal faces cephalometric landmarks, to create an artificial neural network classifier that allows the estimation of anthropological information, in this specific case age and sex. The work is focused on four tasks: i) sex estimation over ages from 5 to 22 years old, evaluating the interference of age on sex estimation; ii) age estimation from photo-anthropometric indexes for four age intervals (1 year, 2 years, 4 years and 5 years); iii) age group estimation for thresholds of over 14 and over 18 years old; and; iv) the provision of a new data set, available for academic purposes only, with a large and complete set of facial photo-anthropometric points marked and checked by forensic experts, measured from over 18,000 faces of individuals from Brazil over the last 4 years. The proposed classifier obtained significant results, using this new data set, for the sex estimation of individuals over 14 years old, achieving accuracy values greater than 0.85 by the F_1 measure. For age estimation, the accuracy results are 0.72 for measure with an age interval of 5 years. For the age group estimation, the measures of accuracy are greater than 0.93 and 0.83 for thresholds of 14 and 18 years, respectively.



### Bag of Negatives for Siamese Architectures
- **Arxiv ID**: http://arxiv.org/abs/1908.02391v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02391v1)
- **Published**: 2019-08-06 22:44:34+00:00
- **Updated**: 2019-08-06 22:44:34+00:00
- **Authors**: Bojana Gajic, Ariel Amato, Ramon Baldrich, Carlo Gatta
- **Comment**: accepted for BMVC2019
- **Journal**: None
- **Summary**: Training a Siamese architecture for re-identification with a large number of identities is a challenging task due to the difficulty of finding relevant negative samples efficiently. In this work we present Bag of Negatives (BoN), a method for accelerated and improved training of Siamese networks that scales well on datasets with a very large number of identities. BoN is an efficient and loss-independent method, able to select a bag of high quality negatives, based on a novel online hashing strategy.



