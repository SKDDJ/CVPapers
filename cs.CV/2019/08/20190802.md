# Arxiv Papers in cs.CV on 2019-08-02
### Recognizing Image Objects by Relational Analysis Using Heterogeneous Superpixels and Deep Convolutional Features
- **Arxiv ID**: http://arxiv.org/abs/1908.00669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00669v1)
- **Published**: 2019-08-02 00:40:27+00:00
- **Updated**: 2019-08-02 00:40:27+00:00
- **Authors**: Alex Yang, Charlie T. Veal, Derek T. Anderson, Grant J. Scott
- **Comment**: None
- **Journal**: None
- **Summary**: Superpixel-based methodologies have become increasingly popular in computer vision, especially when the computation is too expensive in time or memory to perform with a large number of pixels or features. However, rarely is superpixel segmentation examined within the context of deep convolutional neural network architectures. This paper presents a novel neural architecture that exploits the superpixel feature space. The visual feature space is organized using superpixels to provide the neural network with a substructure of the images. As the superpixels associate the visual feature space with parts of the objects in an image, the visual feature space is transformed into a structured vector representation per superpixel. It is shown that it is feasible to learn superpixel features using capsules and it is potentially beneficial to perform image analysis in such a structured manner. This novel deep learning architecture is examined in the context of an image classification task, highlighting explicit interpretability (explainability) of the network's decision making. The results are compared against a baseline deep neural model, as well as among superpixel capsule networks with a variety of hyperparameter settings.



### Indices Matter: Learning to Index for Deep Image Matting
- **Arxiv ID**: http://arxiv.org/abs/1908.00672v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00672v1)
- **Published**: 2019-08-02 01:10:42+00:00
- **Updated**: 2019-08-02 01:10:42+00:00
- **Authors**: Hao Lu, Yutong Dai, Chunhua Shen, Songcen Xu
- **Comment**: Accepted to Proc. Int. Conf. Computer Vision 2019
- **Journal**: None
- **Summary**: We show that existing upsampling operators can be unified with the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can recover boundary details much better than other upsampling operators such as bilinear interpolation. By looking at the indices as a function of the feature map, we introduce the concept of learning to index, and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the pooling and upsampling operators, without the need of supervision. At the core of this framework is a flexible network module, termed IndexNet, which dynamically predicts indices given an input. Due to its flexibility, IndexNet can be used as a plug-in applying to any off-the-shelf convolutional networks that have coupled downsampling and upsampling stages.   We demonstrate the effectiveness of IndexNet on the task of natural image matting where the quality of learned indices can be visually observed from predicted alpha mattes. Results on the Composition-1k matting dataset show that our model built on MobileNetv2 exhibits at least $16.1\%$ improvement over the seminal VGG-16 based deep matting baseline, with less training data and lower model capacity. Code and models has been made available at: https://tinyurl.com/IndexNetV1



### Attention Guided Low-light Image Enhancement with a Large Scale Low-light Simulation Dataset
- **Arxiv ID**: http://arxiv.org/abs/1908.00682v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00682v3)
- **Published**: 2019-08-02 02:28:00+00:00
- **Updated**: 2020-03-15 03:09:03+00:00
- **Authors**: Feifan Lv, Yu Li, Feng Lu
- **Comment**: 18 pages, 16 figures, 5 tables, supplementary materials and project
  page: http://phi-ai.org/project/AgLLNet/default.htm
- **Journal**: None
- **Summary**: Low-light image enhancement is challenging in that it needs to consider not only brightness recovery but also complex issues like color distortion and noise, which usually hide in the dark. Simply adjusting the brightness of a low-light image will inevitably amplify those artifacts. To address this difficult problem, this paper proposes a novel end-to-end attention-guided method based on multi-branch convolutional neural network. To this end, we first construct a synthetic dataset with carefully designed low-light simulation strategies. The dataset is much larger and more diverse than existing ones. With the new dataset for training, our method learns two attention maps to guide the brightness enhancement and denoising tasks respectively. The first attention map distinguishes underexposed regions from well lit regions, and the second attention map distinguishes noises from real textures. With their guidance, the proposed multi-branch decomposition-and-fusion enhancement network works in an input adaptive way. Moreover, a reinforcement-net further enhances color and contrast of the output image. Extensive experiments on multiple datasets demonstrate that our method can produce high fidelity enhancement results for low-light images and outperforms the current state-of-the-art methods by a large margin both quantitatively and visually.



### Real Time Visual Tracking using Spatial-Aware Temporal Aggregation Network
- **Arxiv ID**: http://arxiv.org/abs/1908.00692v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00692v1)
- **Published**: 2019-08-02 03:22:58+00:00
- **Updated**: 2019-08-02 03:22:58+00:00
- **Authors**: Tao Hu, Lichao Huang, Xianming Liu, Han Shen
- **Comment**: None
- **Journal**: None
- **Summary**: More powerful feature representations derived from deep neural networks benefit visual tracking algorithms widely. However, the lack of exploitation on temporal information prevents tracking algorithms from adapting to appearances changing or resisting to drift. This paper proposes a correlation filter based tracking method which aggregates historical features in a spatial-aligned and scale-aware paradigm. The features of historical frames are sampled and aggregated to search frame according to a pixel-level alignment module based on deformable convolutions. In addition, we also use a feature pyramid structure to handle motion estimation at different scales, and address the different demands on feature granularity between tracking losses and deformation offset learning. By this design, the tracker, named as Spatial-Aware Temporal Aggregation network (SATA), is able to assemble appearances and motion contexts of various scales in a time period, resulting in better performance compared to a single static image. Our tracker achieves leading performance in OTB2013, OTB2015, VOT2015, VOT2016 and LaSOT, and operates at a real-time speed of 26 FPS, which indicates our method is effective and practical. Our code will be made publicly available at \href{https://github.com/ecart18/SATA}{https://github.com/ecart18/SATA}.



### Greedy AutoAugment
- **Arxiv ID**: http://arxiv.org/abs/1908.00704v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.00704v2)
- **Published**: 2019-08-02 05:28:03+00:00
- **Updated**: 2020-10-07 01:34:48+00:00
- **Authors**: Alireza Naghizadeh, Mohammadsajad Abavisani, Dimitris N. Metaxas
- **Comment**: Pattern Recognition Letters (2020)
- **Journal**: None
- **Summary**: A major problem in data augmentation is to ensure that the generated new samples cover the search space. This is a challenging problem and requires exploration for data augmentation policies to ensure their effectiveness in covering the search space. In this paper, we propose Greedy AutoAugment as a highly efficient search algorithm to find the best augmentation policies. We use a greedy approach to reduce the exponential growth of the number of possible trials to linear growth. The Greedy Search also helps us to lead the search towards the sub-policies with better results, which eventually helps to increase the accuracy. The proposed method can be used as a reliable addition to the current artifitial neural networks. Our experiments on four datasets (Tiny ImageNet, CIFAR-10, CIFAR-100, and SVHN) show that Greedy AutoAugment provides better accuracy, while using 360 times fewer computational resources.



### AdvGAN++ : Harnessing latent layers for adversary generation
- **Arxiv ID**: http://arxiv.org/abs/1908.00706v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00706v2)
- **Published**: 2019-08-02 05:37:03+00:00
- **Updated**: 2019-12-23 19:31:19+00:00
- **Authors**: Puneet Mangla, Surgan Jandial, Sakshi Varshney, Vineeth N Balasubramanian
- **Comment**: Accepted at Neural Architects Workshop, ICCV 2019
- **Journal**: None
- **Summary**: Adversarial examples are fabricated examples, indistinguishable from the original image that mislead neural networks and drastically lower their performance. Recently proposed AdvGAN, a GAN based approach, takes input image as a prior for generating adversaries to target a model. In this work, we show how latent features can serve as better priors than input images for adversary generation by proposing AdvGAN++, a version of AdvGAN that achieves higher attack rates than AdvGAN and at the same time generates perceptually realistic images on MNIST and CIFAR-10 datasets.



### Scale Matters: Temporal Scale Aggregation Network for Precise Action Localization in Untrimmed Videos
- **Arxiv ID**: http://arxiv.org/abs/1908.00707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00707v1)
- **Published**: 2019-08-02 05:49:37+00:00
- **Updated**: 2019-08-02 05:49:37+00:00
- **Authors**: Guoqiang Gong, Liangfeng Zheng, Kun Bai, Yadong Mu
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action localization is a recently-emerging task, aiming to localize video segments from untrimmed videos that contain specific actions. Despite the remarkable recent progress, most two-stage action localization methods still suffer from imprecise temporal boundaries of action proposals. This work proposes a novel integrated temporal scale aggregation network (TSA-Net). Our main insight is that ensembling convolution filters with different dilation rates can effectively enlarge the receptive field with low computational cost, which inspires us to devise multi-dilation temporal convolution (MDC) block. Furthermore, to tackle video action instances with different durations, TSA-Net consists of multiple branches of sub-networks. Each of them adopts stacked MDC blocks with different dilation parameters, accomplishing a temporal receptive field specially optimized for specific-duration actions. We follow the formulation of boundary point detection, novelly detecting three kinds of critical points (ie, starting / mid-point / ending) and pairing them for proposal generation. Comprehensive evaluations are conducted on two challenging video benchmarks, THUMOS14 and ActivityNet-1.3. Our proposed TSA-Net demonstrates clear and consistent better performances and re-calibrates new state-of-the-art on both benchmarks. For example, our new record on THUMOS14 is 46.9% while the previous best is 42.8% under mAP@0.5.



### AutoML: A Survey of the State-of-the-Art
- **Arxiv ID**: http://arxiv.org/abs/1908.00709v6
- **DOI**: 10.1016/j.knosys.2020.106622
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.00709v6)
- **Published**: 2019-08-02 05:56:13+00:00
- **Updated**: 2021-04-16 03:38:23+00:00
- **Authors**: Xin He, Kaiyong Zhao, Xiaowen Chu
- **Comment**: automated machine learning (AutoML), published in journal of
  Knowledge-Based Systems
- **Journal**: Knowledge-Based Systems, Volume 212, 5 January 2021, 106622
- **Summary**: Deep learning (DL) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering the applications of DL to more areas. Automated machine learning (AutoML) becomes a promising solution to build a DL system without human assistance, and a growing number of researchers focus on AutoML. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce AutoML methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS). We focus more on NAS, as it is currently very hot sub-topic of AutoML. We summarize the performance of the representative NAS algorithms on the CIFAR-10 and ImageNet datasets and further discuss several worthy studying directions of NAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing AutoML methods for future research.



### Monitoring of people entering and exiting private areas using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1908.00716v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00716v2)
- **Published**: 2019-08-02 06:33:06+00:00
- **Updated**: 2019-08-28 08:12:44+00:00
- **Authors**: Vinay Kumar V, P Nagabhushan
- **Comment**: 4 pages
- **Journal**: None
- **Summary**: Entry-Exit surveillance is a novel research problem that addresses security concerns when people attain absolute privacy in camera forbidden areas such as toilets and changing rooms that are basic amenities to the humans in public places such as Shopping malls, Airports, Bus and Rail stations. The objective is, if not inside these camera forbidden areas, from outside, the individuals are to be monitored to analyze the time spent by them inside and also the suspecting transformations in their appearances if any. In this paper, firstly, a pseudo-annotated dataset of a laboratory observation of people entering and exiting the camera forbidden area captured using two cameras in contrast to the state-of-the-art single-camera based EnEx dataset is presented. Conventionally the proposed dataset is named \textbf{\textit{EnEx2}}. Next, a spatial transition based event detection to determine the entry or exit of individuals is presented with standard results by evaluating the proposed model using the proposed dataset and the publicly available standard video surveillance datasets that are hypothesized to Entry-Exit surveillance scenarios. The proposed dataset is expected to enkindle active research in Entry-Exit Surveillance domain.



### L2G Auto-encoder: Understanding Point Clouds by Local-to-Global Reconstruction with Hierarchical Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/1908.00720v1
- **DOI**: 10.1145/3343031.3350960
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00720v1)
- **Published**: 2019-08-02 06:50:59+00:00
- **Updated**: 2019-08-02 06:50:59+00:00
- **Authors**: Xinhai Liu, Zhizhong Han, Xin Wen, Yu-Shen Liu, Matthias Zwicker
- **Comment**: None
- **Journal**: None
- **Summary**: Auto-encoder is an important architecture to understand point clouds in an encoding and decoding procedure of self reconstruction. Current auto-encoder mainly focuses on the learning of global structure by global shape reconstruction, while ignoring the learning of local structures. To resolve this issue, we propose Local-to-Global auto-encoder (L2G-AE) to simultaneously learn the local and global structure of point clouds by local to global reconstruction. Specifically, L2G-AE employs an encoder to encode the geometry information of multiple scales in a local region at the same time. In addition, we introduce a novel hierarchical self-attention mechanism to highlight the important points, scales and regions at different levels in the information aggregation of the encoder. Simultaneously, L2G-AE employs a recurrent neural network (RNN) as decoder to reconstruct a sequence of scales in a local region, based on which the global point cloud is incrementally reconstructed. Our outperforming results in shape classification, retrieval and upsampling show that L2G-AE can understand point clouds better than state-of-the-art methods.



### Learning to combine primitive skills: A step towards versatile robotic manipulation
- **Arxiv ID**: http://arxiv.org/abs/1908.00722v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.00722v3)
- **Published**: 2019-08-02 07:04:17+00:00
- **Updated**: 2020-06-20 14:26:45+00:00
- **Authors**: Robin Strudel, Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Josef Sivic, Cordelia Schmid
- **Comment**: ICRA 2020. See the project webpage at
  https://www.di.ens.fr/willow/research/rlbc/
- **Journal**: IEEE ROBOTICS AND AUTOMATION LETTERS, JULY 2020. 4637-4643
- **Summary**: Manipulation tasks such as preparing a meal or assembling furniture remain highly challenging for robotics and vision. Traditional task and motion planning (TAMP) methods can solve complex tasks but require full state observability and are not adapted to dynamic scene changes. Recent learning methods can operate directly on visual inputs but typically require many demonstrations and/or task-specific reward engineering. In this work we aim to overcome previous limitations and propose a reinforcement learning (RL) approach to task planning that learns to combine primitive skills. First, compared to previous learning methods, our approach requires neither intermediate rewards nor complete task demonstrations during training. Second, we demonstrate the versatility of our vision-based task planning in challenging settings with temporary occlusions and dynamic scene changes. Third, we propose an efficient training of basic skills from few synthetic demonstrations by exploring recent CNN architectures and data augmentation. Notably, while all of our policies are learned on visual inputs in simulated environments, we demonstrate the successful transfer and high success rates when applying such policies to manipulation tasks on a real UR5 robotic arm.



### Y-Net: A Hybrid Deep Learning Reconstruction Framework for Photoacoustic Imaging in vivo
- **Arxiv ID**: http://arxiv.org/abs/1908.00975v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.00975v1)
- **Published**: 2019-08-02 07:27:17+00:00
- **Updated**: 2019-08-02 07:27:17+00:00
- **Authors**: Hengrong Lan, Daohuai Jiang, Changchun Yang, Fei Gao
- **Comment**: submitted the journal version
- **Journal**: None
- **Summary**: Photoacoustic imaging (PAI) is an emerging non-invasive imaging modality combining the advantages of deep ultrasound penetration and high optical contrast. Image reconstruction is an essential topic in PAI, which is unfortunately an ill-posed problem due to the complex and unknown optical/acoustic parameters in tissue. Conventional algorithms used in PAI (e.g., delay-and-sum) provide a fast solution while many artifacts remain, especially for linear array probe with limited-view issue. Convolutional neural network (CNN) has shown state-of-the-art results in computer vision, and more and more work based on CNN has been studied in medical image processing recently. In this paper, we present a non-iterative scheme filling the gap between existing direct-processing and post-processing methods, and propose a new framework Y-Net: a CNN architecture to reconstruct the PA image by optimizing both raw data and beamformed images once. The network connected two encoders with one decoder path, which optimally utilizes more information from raw data and beamformed image. The results of the test set showed good performance compared with conventional reconstruction algorithms and other deep learning methods. Our method is also validated with experiments both in-vitro and in vivo, which still performs better than other existing methods. The proposed Y-Net architecture also has high potential in medical image reconstruction for other imaging modalities beyond PAI.



### Road Context-aware Intrusion Detection System for Autonomous Cars
- **Arxiv ID**: http://arxiv.org/abs/1908.00732v1
- **DOI**: 10.1007/978-3-030-41579-2_8
- **Categories**: **cs.CR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00732v1)
- **Published**: 2019-08-02 07:48:31+00:00
- **Updated**: 2019-08-02 07:48:31+00:00
- **Authors**: Jingxuan Jiang, Chundong Wang, Sudipta Chattopadhyay, Wei Zhang
- **Comment**: This manuscript presents an intrusion detection system that makes use
  of road context for autonomous cars
- **Journal**: None
- **Summary**: Security is of primary importance to vehicles. The viability of performing remote intrusions onto the in-vehicle network has been manifested. In regard to unmanned autonomous cars, limited work has been done to detect intrusions for them while existing intrusion detection systems (IDSs) embrace limitations against strong adversaries. In this paper, we consider the very nature of autonomous car and leverage the road context to build a novel IDS, named Road context-aware IDS (RAIDS). When a computer-controlled car is driving through continuous roads, road contexts and genuine frames transmitted on the car's in-vehicle network should resemble a regular and intelligible pattern. RAIDS hence employs a lightweight machine learning model to extract road contexts from sensory information (e.g., camera images and distance sensor values) that are used to generate control signals for maneuvering the car. With such ongoing road context, RAIDS validates corresponding frames observed on the in-vehicle network. Anomalous frames that substantially deviate from road context will be discerned as intrusions. We have implemented a prototype of RAIDS with neural networks, and conducted experiments on a Raspberry Pi with extensive datasets and meaningful intrusion cases. Evaluations show that RAIDS significantly outperforms state-of-the-art IDS without using road context by up to 99.9% accuracy and short response time.



### Learning Variations in Human Motion via Mix-and-Match Perturbation
- **Arxiv ID**: http://arxiv.org/abs/1908.00733v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.00733v2)
- **Published**: 2019-08-02 07:48:48+00:00
- **Updated**: 2020-02-24 22:03:12+00:00
- **Authors**: Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould, Amirhossein Habibian
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is a stochastic process: Given an observed sequence of poses, multiple future motions are plausible. Existing approaches to modeling this stochasticity typically combine a random noise vector with information about the previous poses. This combination, however, is done in a deterministic manner, which gives the network the flexibility to learn to ignore the random noise. In this paper, we introduce an approach to stochastically combine the root of variations with previous pose information, which forces the model to take the noise into account. We exploit this idea for motion prediction by incorporating it into a recurrent encoder-decoder network with a conditional variational autoencoder block that learns to exploit the perturbations. Our experiments demonstrate that our model yields high-quality pose sequences that are much more diverse than those from state-of-the-art stochastic motion prediction techniques.



### Integrating Spatial Configuration into Heatmap Regression Based CNNs for Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.00748v1
- **DOI**: 10.1016/j.media.2019.03.007
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00748v1)
- **Published**: 2019-08-02 08:17:50+00:00
- **Updated**: 2019-08-02 08:17:50+00:00
- **Authors**: Christian Payer, Darko Štern, Horst Bischof, Martin Urschler
- **Comment**: MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: In many medical image analysis applications, often only a limited amount of training data is available, which makes training of convolutional neural networks (CNNs) challenging. In this work on anatomical landmark localization, we propose a CNN architecture that learns to split the localization task into two simpler sub-problems, reducing the need for large training datasets. Our fully convolutional SpatialConfiguration-Net (SCN) dedicates one component to locally accurate but ambiguous candidate predictions, while the other component improves robustness to ambiguities by incorporating the spatial configuration of landmarks. In our experimental evaluation, we show that the proposed SCN outperforms related methods in terms of landmark localization error on size-limited datasets.



### Network with Sub-Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.00763v2
- **DOI**: 10.2991/jrnal.k.201215.006
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00763v2)
- **Published**: 2019-08-02 09:04:28+00:00
- **Updated**: 2019-12-03 04:41:02+00:00
- **Authors**: Ninnart Fuengfusin, Hakaru Tamukoh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce network with sub-networks, a neural network which its weight layers could be detached into sub-neural networks during inference. To develop weights and biases which could be inserted in both base and sub-neural networks, firstly, the parameters are copied from sub-model to base-model. Each model is forward-propagated separately. Gradients from a pair of networks are averaged and, used to update both networks. Our base model achieves the test-accuracy which is comparable to the regularly trained models, while the model maintains the ability to detach weight layers.



### An amplified-target loss approach for photoreceptor layer segmentation in pathological OCT scans
- **Arxiv ID**: http://arxiv.org/abs/1908.00764v2
- **DOI**: 10.1007/978-3-030-32956-3_4
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00764v2)
- **Published**: 2019-08-02 09:05:37+00:00
- **Updated**: 2019-10-21 13:38:28+00:00
- **Authors**: José Ignacio Orlando, Anna Breger, Hrvoje Bogunović, Sophie Riedl, Bianca S. Gerendas, Martin Ehler, Ursula Schmidt-Erfurth
- **Comment**: Accepted for publication at MICCAI-OMIA 2019
- **Journal**: None
- **Summary**: Segmenting anatomical structures such as the photoreceptor layer in retinal optical coherence tomography (OCT) scans is challenging in pathological scenarios. Supervised deep learning models trained with standard loss functions are usually able to characterize only the most common disease appeareance from a training set, resulting in suboptimal performance and poor generalization when dealing with unseen lesions. In this paper we propose to overcome this limitation by means of an augmented target loss function framework. We introduce a novel amplified-target loss that explicitly penalizes errors within the central area of the input images, based on the observation that most of the challenging disease appeareance is usually located in this area. We experimentally validated our approach using a data set with OCT scans of patients with macular diseases. We observe increased performance compared to the models that use only the standard losses. Our proposed loss function strongly supports the segmentation model to better distinguish photoreceptors in highly pathological scenarios.



### DAWN: Dual Augmented Memory Network for Unsupervised Video Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1908.00777v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00777v2)
- **Published**: 2019-08-02 09:52:57+00:00
- **Updated**: 2019-08-08 15:53:13+00:00
- **Authors**: Zhenmei Shi, Haoyang Fang, Yu-Wing Tai, Chi-Keung Tang
- **Comment**: Zhenmei and Haoyang have equal contribution
- **Journal**: None
- **Summary**: Psychological studies have found that human visual tracking system involves learning, memory, and planning. Despite recent successes, not many works have focused on memory and planning in deep learning based tracking. We are thus interested in memory augmented network, where an external memory remembers the evolving appearance of the target (foreground) object without backpropagation for updating weights. Our Dual Augmented Memory Network (DAWN) is unique in remembering both target and background, and using an improved attention LSTM memory to guide the focus on memorized features. DAWN is effective in unsupervised tracking in handling total occlusion, severe motion blur, abrupt changes in target appearance, multiple object instances, and similar foreground and background features. We present extensive quantitative and qualitative experimental comparison with state-of-the-art methods including top contenders in recent VOT challenges. Notably, despite the straightforward implementation, DAWN is ranked third in both VOT2016 and VOT2017 challenges with excellent success rate among all VOT fast trackers running at fps > 10 in unsupervised tracking in both challenges. We propose DAWN-RPN, where we simply augment our memory and attention LSTM modules to the state-of-the-art SiamRPN, and report immediate performance gain, thus demonstrating DAWN can work well with and directly benefit other models to handle difficult cases as well.



### A Structural Graph-Based Method for MRI Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.00778v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00778v1)
- **Published**: 2019-08-02 09:53:18+00:00
- **Updated**: 2019-08-02 09:53:18+00:00
- **Authors**: Larissa de O. Penteado, Mateus Riva, Roberto M. Cesar Jr
- **Comment**: Published in the Workshop of Works In Progress of the SIBGRAPI 2018
- **Journal**: None
- **Summary**: The importance of imaging exams, such as Magnetic Resonance Imaging (MRI), for the diagnostic and follow-up of pediatric pathologies and the assessment of anatomical structures' development has been increasingly highlighted in recent times. Manual analysis of MRIs is time-consuming, subjective, and requires significant expertise. To mitigate this, automatic techniques are necessary. Most techniques focus on adult subjects, while pediatric MRI has specific challenges such as the ongoing anatomical and histological changes related to normal development of the organs, reduced signal-to-noise ratio due to the smaller bodies, motion artifacts and cooperation issues, especially in long exams, which can in many cases preclude common analysis methods developed for use in adults. Therefore, the development of a robust technique to aid in pediatric MRI analysis is necessary. This paper presents the current development of a new method based on the learning and matching of structural relational graphs (SRGs). The experiments were performed on liver MRI sequences of one patient from ICr-HC-FMUSP, and preliminary results showcased the viability of the project. Future experiments are expected to culminate with an application for pediatric liver substructure and brain tumor segmentation.



### Deformable Medical Image Registration Using a Randomly-Initialized CNN as Regularization Prior
- **Arxiv ID**: http://arxiv.org/abs/1908.00788v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00788v1)
- **Published**: 2019-08-02 10:19:44+00:00
- **Updated**: 2019-08-02 10:19:44+00:00
- **Authors**: Max-Heinrich Laves, Sontje Ihler, Tobias Ortmaier
- **Comment**: Accepted at MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: We present deformable unsupervised medical image registration using a randomly-initialized deep convolutional neural network (CNN) as regularization prior. Conventional registration methods predict a transformation by minimizing dissimilarities between an image pair. The minimization is usually regularized with manually engineered priors, which limits the potential of the registration. By learning transformation priors from a large dataset, CNNs have achieved great success in deformable registration. However, learned methods are restricted to domain-specific data and the required amounts of medical data are difficult to obtain. Our approach uses the idea of deep image priors to combine convolutional networks with conventional registration methods based on manually engineered priors. The proposed method is applied to brain MRI scans. We show that our approach registers image pairs with state-of-the-art accuracy by providing dense, pixel-wise correspondence maps. It does not rely on prior training and is therefore not limited to a specific image domain.



### Uncertainty Quantification in Computer-Aided Diagnosis: Make Your Model say "I don't know" for Ambiguous Cases
- **Arxiv ID**: http://arxiv.org/abs/1908.00792v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00792v1)
- **Published**: 2019-08-02 10:31:02+00:00
- **Updated**: 2019-08-02 10:31:02+00:00
- **Authors**: Max-Heinrich Laves, Sontje Ihler, Tobias Ortmaier
- **Comment**: Accepted at MIDL 2019 [arXiv:1907.08612]
- **Journal**: None
- **Summary**: We evaluate two different methods for the integration of prediction uncertainty into diagnostic image classifiers to increase patient safety in deep learning. In the first method, Monte Carlo sampling is applied with dropout at test time to get a posterior distribution of the class labels (Bayesian ResNet). The second method extends ResNet to a probabilistic approach by predicting the parameters of the posterior distribution and sampling the final result from it (Variational ResNet).The variance of the posterior is used as metric for uncertainty.Both methods are trained on a data set of optical coherence tomography scans showing four different retinal conditions. Our results shown that cases in which the classifier predicts incorrectly correlate with a higher uncertainty. Mean uncertainty of incorrectly diagnosed cases was between 4.6 and 8.1 times higher than mean uncertainty of correctly diagnosed cases. Modeling of the prediction uncertainty in computer-aided diagnosis with deep learning yields more reliable results and is anticipated to increase patient safety.



### Space-adaptive anisotropic bivariate Laplacian regularization for image restoration
- **Arxiv ID**: http://arxiv.org/abs/1908.00801v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1908.00801v1)
- **Published**: 2019-08-02 11:01:19+00:00
- **Updated**: 2019-08-02 11:01:19+00:00
- **Authors**: Luca Calatroni, Alessandro Lanza, Monica Pragliola, Fiorella Sgallari
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present a new regularization term for variational image restoration which can be regarded as a space-variant anisotropic extension of the classical isotropic Total Variation (TV) regularizer. The proposed regularizer comes from the statistical assumption that the gradients of the target image distribute locally according to space-variant bivariate Laplacian distributions. The highly flexible variational structure of the corresponding regularizer encodes several free parameters which hold the potential for faithfully modelling the local geometry in the image and describing local orientation preferences. For an automatic estimation of such parameters, we design a robust maximum likelihood approach and report results on its reliability on synthetic data and natural images. A minimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) is presented for the efficient numerical solution of the proposed variational model. Some experimental results are reported which demonstrate the high-quality of restorations achievable by the proposed model, in particular with respect to classical Total Variation regularization.



### Learning Lightweight Lane Detection CNNs by Self Attention Distillation
- **Arxiv ID**: http://arxiv.org/abs/1908.00821v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00821v1)
- **Published**: 2019-08-02 12:13:34+00:00
- **Updated**: 2019-08-02 12:13:34+00:00
- **Authors**: Yuenan Hou, Zheng Ma, Chunxiao Liu, Chen Change Loy
- **Comment**: 9 pages, 8 figures; This paper is accepted by ICCV 2019; Our code is
  available at https://github.com/cardwing/Codes-for-Lane-Detection
- **Journal**: None
- **Summary**: Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing topdown and layer-wise attention distillation within the network itself. SAD can be easily incorporated in any feedforward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks. Our code is available at https://github.com/cardwing/Codes-for-Lane-Detection.



### High Accuracy Tumor Diagnoses and Benchmarking of Hematoxylin and Eosin Stained Prostate Core Biopsy Images Generated by Explainable Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.01593v1
- **DOI**: 10.1001/jamanetworkopen.2020.5111
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01593v1)
- **Published**: 2019-08-02 13:25:07+00:00
- **Updated**: 2019-08-02 13:25:07+00:00
- **Authors**: Aman Rana, Alarice Lowe, Marie Lithgow, Katharine Horback, Tyler Janovitz, Annacarolina Da Silva, Harrison Tsai, Vignesh Shanmugam, Hyung-Jin Yoon, Pratik Shah
- **Comment**: None
- **Journal**: JAMA Network. 2020;3(5):e205111
- **Summary**: Histopathological diagnoses of tumors in tissue biopsy after Hematoxylin and Eosin (H&E) staining is the gold standard for oncology care. H&E staining is slow and uses dyes, reagents and precious tissue samples that cannot be reused. Thousands of native nonstained RGB Whole Slide Image (RWSI) patches of prostate core tissue biopsies were registered with their H&E stained versions. Conditional Generative Adversarial Neural Networks (cGANs) that automate conversion of native nonstained RWSI to computational H&E stained images were then trained. High similarities between computational and H&E dye stained images with Structural Similarity Index (SSIM) 0.902, Pearsons Correlation Coefficient (CC) 0.962 and Peak Signal to Noise Ratio (PSNR) 22.821 dB were calculated. A second cGAN performed accurate computational destaining of H&E dye stained images back to their native nonstained form with SSIM 0.9, CC 0.963 and PSNR 25.646 dB. A single-blind study computed more than 95% pixel-by-pixel overlap between prostate tumor annotations on computationally stained images, provided by five-board certified MD pathologists, with those on H&E dye stained counterparts. We report the first visualization and explanation of neural network kernel activation maps during H&E staining and destaining of RGB images by cGANs. High similarities between kernel activation maps of computational and H&E stained images (Mean-Squared Errors <0.0005) provide additional mathematical and mechanistic validation of the staining system. Our neural network framework thus is automated, explainable and performs high precision H&E staining and destaining of low cost native RGB images, and is computer vision and physician authenticated for rapid and accurate tumor diagnoses.



### Learning the Model Update for Siamese Trackers
- **Arxiv ID**: http://arxiv.org/abs/1908.00855v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00855v2)
- **Published**: 2019-08-02 13:40:43+00:00
- **Updated**: 2019-09-06 14:35:51+00:00
- **Authors**: Lichao Zhang, Abel Gonzalez-Garcia, Joost van de Weijer, Martin Danelljan, Fahad Shahbaz Khan
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Siamese approaches address the visual tracking problem by extracting an appearance template from the current frame, which is used to localize the target in the next frame. In general, this template is linearly combined with the accumulated template from the previous frame, resulting in an exponential decay of information over time. While such an approach to updating has led to improved results, its simplicity limits the potential gain likely to be obtained by learning to update. Therefore, we propose to replace the handcrafted update function with a method which learns to update. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame aims to estimate the optimal template for the next frame. The UpdateNet is compact and can easily be integrated into existing Siamese trackers. We demonstrate the generality of the proposed approach by applying it to two Siamese trackers, SiamFC and DaSiamRPN. Extensive experiments on VOT2016, VOT2018, LaSOT, and TrackingNet datasets demonstrate that our UpdateNet effectively predicts the new target template, outperforming the standard linear update. On the large-scale TrackingNet dataset, our UpdateNet improves the results of DaSiamRPN with an absolute gain of 3.9% in terms of success score.



### Distilling Knowledge From a Deep Pose Regressor Network
- **Arxiv ID**: http://arxiv.org/abs/1908.00858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.00858v1)
- **Published**: 2019-08-02 13:48:31+00:00
- **Updated**: 2019-08-02 13:48:31+00:00
- **Authors**: Muhamad Risqi U. Saputra, Pedro P. B. de Gusmao, Yasin Almalioglu, Andrew Markham, Niki Trigoni
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on "dark knowledge" for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time.



### Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.00862v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00862v2)
- **Published**: 2019-08-02 13:58:26+00:00
- **Updated**: 2021-07-09 14:35:59+00:00
- **Authors**: Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi, Xin Geng, Yang Gao
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: In person re-identification (Re-ID), supervised methods usually need a large amount of expensive label information, while unsupervised ones are still unable to deliver satisfactory identification performance. In this paper, we introduce a novel person Re-ID task called unsupervised cross-camera person Re-ID, which only needs the within-camera (intra-camera) label information but not cross-camera (inter-camera) labels which are more expensive to obtain. In real-world applications, the intra-camera label information can be easily captured by tracking algorithms or few manual annotations. In this situation, the main challenge becomes the distribution discrepancy across different camera views, caused by the various body pose, occlusion, image resolution, illumination conditions, and background noises in different cameras. To address this situation, we propose a novel Adversarial Camera Alignment Network (ACAN) for unsupervised cross-camera person Re-ID. It consists of the camera-alignment task and the supervised within-camera learning task. To achieve the camera alignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images of different cameras into a shared subspace. Particularly, we investigate two different schemes, including the existing GRL (i.e., gradient reversal layer) scheme and the proposed scheme called "other camera equiprobability" (OCE), to conduct the multi-camera adversarial task. Based on this shared subspace, we then leverage the within-camera labels to train the network. Extensive experiments on five large-scale datasets demonstrate the superiority of ACAN over multiple state-of-the-art unsupervised methods that take advantage of labeled source domains and generated images by GAN-based models. In particular, we verify that the proposed multi-camera adversarial task does contribute to the significant improvement.



### An Evaluation of Action Recognition Models on EPIC-Kitchens
- **Arxiv ID**: http://arxiv.org/abs/1908.00867v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00867v1)
- **Published**: 2019-08-02 14:07:07+00:00
- **Updated**: 2019-08-02 14:07:07+00:00
- **Authors**: Will Price, Dima Damen
- **Comment**: 6 pages, 3 figures, 3 tables. Models released at
  https://github.com/epic-kitchens/action-models
- **Journal**: None
- **Summary**: We benchmark contemporary action recognition models (TSN, TRN, and TSM) on the recently introduced EPIC-Kitchens dataset and release pretrained models on GitHub (https://github.com/epic-kitchens/action-models) for others to build upon. In contrast to popular action recognition datasets like Kinetics, Something-Something, UCF101, and HMDB51, EPIC-Kitchens is shot from an egocentric perspective and captures daily actions in-situ. In this report, we aim to understand how well these models can tackle the challenges present in this dataset, such as its long tail class distribution, unseen environment test set, and multiple tasks (verb, noun and, action classification). We discuss the models' shortcomings and avenues for future research.



### Exact and fast inversion of the approximate discrete Radon transform from partial data
- **Arxiv ID**: http://arxiv.org/abs/1908.00887v3
- **DOI**: 10.1016/j.aml.2019.106159
- **Categories**: **math.NA**, cs.CC, cs.CV, cs.NA, 44A12, 65R10, 65F05, 65Q30
- **Links**: [PDF](http://arxiv.org/pdf/1908.00887v3)
- **Published**: 2019-08-02 14:41:35+00:00
- **Updated**: 2020-05-18 05:29:43+00:00
- **Authors**: Donsub Rim
- **Comment**: 4 pages, 1 figure
- **Journal**: Appl. Math. Lett. 102 106159 (2020)
- **Summary**: We give an exact inversion formula for the approximate discrete Radon transform introduced in [Brady, SIAM J. Comput., 27(1), 107--119] that is of cost $O(N \log N)$ for a square 2D image with $N$ pixels and requires only partial data.



### Pothole Detection Based on Disparity Transformation and Road Surface Modeling
- **Arxiv ID**: http://arxiv.org/abs/1908.00894v3
- **DOI**: 10.1109/TIP.2019.2933750
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00894v3)
- **Published**: 2019-08-02 14:55:48+00:00
- **Updated**: 2022-05-22 03:56:39+00:00
- **Authors**: Rui Fan, Umar Ozgunalp, Brett Hosking, Ming Liu, Ioannis Pitas
- **Comment**: 12 pages, 15 figures, IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Pothole detection is one of the most important tasks for road maintenance. Computer vision approaches are generally based on either 2D road image analysis or 3D road surface modeling. However, these two categories are always used independently. Furthermore, the pothole detection accuracy is still far from satisfactory. Therefore, in this paper, we present a robust pothole detection algorithm that is both accurate and computationally efficient. A dense disparity map is first transformed to better distinguish between damaged and undamaged road areas. To achieve greater disparity transformation efficiency, golden section search and dynamic programming are utilized to estimate the transformation parameters. Otsu's thresholding method is then used to extract potential undamaged road areas from the transformed disparity map. The disparities in the extracted areas are modeled by a quadratic surface using least squares fitting. To improve disparity map modeling robustness, the surface normal is also integrated into the surface modeling process. Furthermore, random sample consensus is utilized to reduce the effects caused by outliers. By comparing the difference between the actual and modeled disparity maps, the potholes can be detected accurately. Finally, the point clouds of the detected potholes are extracted from the reconstructed 3D road surface. The experimental results show that the successful detection accuracy of the proposed system is around 98.7% and the overall pixel-level accuracy is approximately 99.6%.



### Effects of Illumination on the Categorization of Shiny Materials
- **Arxiv ID**: http://arxiv.org/abs/1908.00902v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1908.00902v2)
- **Published**: 2019-08-02 15:19:54+00:00
- **Updated**: 2019-10-21 16:22:27+00:00
- **Authors**: J. Farley Norman, James T. Todd, Flip Phillips
- **Comment**: v2, 20 pages, 15 figures, 26 references
- **Journal**: None
- **Summary**: The present research was designed to examine how patterns of illumination influence the perceptual categorization of metal, shiny black, and shiny white materials. The stimuli depicted three possible objects that were illuminated by five possible HDRI light maps, which varied in their overall distributions of illuminant directions and intensities. The surfaces included a low roughness chrome material, a shiny black material, and a shiny white material with both diffuse and specular components. Observers rated each stimulus by adjusting four sliders to indicate their confidence that the depicted material was metal, shiny black, shiny white or something else, and these adjustments were constrained so that the sum of all four settings was always 100%. The results revealed that the metal and shiny black categories are easily confused. For example, metal materials with low intensity light maps or a narrow range of illuminant directions are often judged as shiny black, whereas shiny black materials with high intensity light maps or a wide range of illuminant directions are often judged as metal. A spherical harmonic analysis was performed on the different light maps in an effort to quantitatively predict how they would bias observers' judgments of metal and shiny black surfaces.



### Prediction and Description of Near-Future Activities in Video
- **Arxiv ID**: http://arxiv.org/abs/1908.00943v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00943v5)
- **Published**: 2019-08-02 16:25:59+00:00
- **Updated**: 2021-05-27 02:46:23+00:00
- **Authors**: Tahmida Mahmud, Mohammad Billah, Mahmudul Hasan, Amit K. Roy-Chowdhury
- **Comment**: 15 pages, 4 figures, 14 tables
- **Journal**: None
- **Summary**: Most of the existing works on human activity analysis focus on recognition or early recognition of the activity labels from complete or partial observations. Similarly, almost all of the existing video captioning approaches focus on the observed events in videos. Predicting the labels and the captions of future activities where no frames of the predicted activities have been observed is a challenging problem, with important applications that require anticipatory response. In this work, we propose a system that can infer the labels and the captions of a sequence of future activities. Our proposed network for label prediction of a future activity sequence has three branches where the first branch takes visual features from the objects present in the scene, the second branch takes observed sequential activity features, and the third branch captures the last observed activity features. The predicted labels and the observed scene context are then mapped to meaningful captions using a sequence-to-sequence learning-based method. Experiments on four challenging activity analysis datasets and a video description dataset demonstrate that our label prediction approach achieves comparable performance with the state-of-the-arts and our captioning framework outperform the state-of-the-arts.



### Learning to Train with Synthetic Humans
- **Arxiv ID**: http://arxiv.org/abs/1908.00967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.00967v1)
- **Published**: 2019-08-02 17:49:30+00:00
- **Updated**: 2019-08-02 17:49:30+00:00
- **Authors**: David T. Hoffmann, Dimitrios Tzionas, Micheal J. Black, Siyu Tang
- **Comment**: In German Conference on Pattern Recognition (GCPR)
- **Journal**: None
- **Summary**: Neural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans and a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. Using the augmented dataset, without considering synthetic humans in the loss, leads to the best results. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that the student-teacher framework outperforms normal training on the purely synthetic dataset.



### Cycle In Cycle Generative Adversarial Networks for Keypoint-Guided Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1908.00999v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.00999v3)
- **Published**: 2019-08-02 18:21:28+00:00
- **Updated**: 2020-04-16 00:53:39+00:00
- **Authors**: Hao Tang, Dan Xu, Gaowen Liu, Wei Wang, Nicu Sebe, Yan Yan
- **Comment**: 9 pages, 8 figures, accepted to ACM MM 2019
- **Journal**: ACM MM 2019
- **Summary**: In this work, we propose a novel Cycle In Cycle Generative Adversarial Network (C$^2$GAN) for the task of keypoint-guided image generation. The proposed C$^2$GAN is a cross-modal framework exploring a joint exploitation of the keypoint and the image data in an interactive manner. C$^2$GAN contains two different types of generators, i.e., keypoint-oriented generator and image-oriented generator. Both of them are mutually connected in an end-to-end learnable fashion and explicitly form three cycled sub-networks, i.e., one image generation cycle and two keypoint generation cycles. Each cycle not only aims at reconstructing the input domain, and also produces useful output involving in the generation of another cycle. By so doing, the cycles constrain each other implicitly, which provides complementary information from the two different modalities and brings extra supervision across cycles, thus facilitating more robust optimization of the whole network. Extensive experimental results on two publicly available datasets, i.e., Radboud Faces and Market-1501, demonstrate that our approach is effective to generate more photo-realistic images compared with state-of-the-art models.



### Adaloss: Adaptive Loss Function for Landmark Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.01070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01070v1)
- **Published**: 2019-08-02 21:18:50+00:00
- **Updated**: 2019-08-02 21:18:50+00:00
- **Authors**: Brian Teixeira, Birgi Tamersoy, Vivek Singh, Ankur Kapoor
- **Comment**: None
- **Journal**: None
- **Summary**: Landmark localization is a challenging problem in computer vision with a multitude of applications. Recent deep learning based methods have shown improved results by regressing likelihood maps instead of regressing the coordinates directly. However, setting the precision of these regression targets during the training is a cumbersome process since it creates a trade-off between trainability vs localization accuracy. Using precise targets introduces a significant sampling bias and hence makes the training more difficult, whereas using imprecise targets results in inaccurate landmark detectors. In this paper, we introduce "Adaloss", an objective function that adapts itself during the training by updating the target precision based on the training statistics. This approach does not require setting problem-specific parameters and shows improved stability in training and better localization accuracy during inference. We demonstrate the effectiveness of our proposed method in three different applications of landmark localization: 1) the challenging task of precisely detecting catheter tips in medical X-ray images, 2) localizing surgical instruments in endoscopic images, and 3) localizing facial features on in-the-wild images where we show state-of-the-art results on the 300-W benchmark dataset.



### Toward Understanding Catastrophic Forgetting in Continual Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01091v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01091v1)
- **Published**: 2019-08-02 23:30:35+00:00
- **Updated**: 2019-08-02 23:30:35+00:00
- **Authors**: Cuong V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, Stefano Soatto
- **Comment**: None
- **Journal**: None
- **Summary**: We study the relationship between catastrophic forgetting and properties of task sequences. In particular, given a sequence of tasks, we would like to understand which properties of this sequence influence the error rates of continual learning algorithms trained on the sequence. To this end, we propose a new procedure that makes use of recent developments in task space modeling as well as correlation analysis to specify and analyze the properties we are interested in. As an application, we apply our procedure to study two properties of a task sequence: (1) total complexity and (2) sequential heterogeneity. We show that error rates are strongly and positively correlated to a task sequence's total complexity for some state-of-the-art algorithms. We also show that, surprisingly, the error rates have no or even negative correlations in some cases to sequential heterogeneity. Our findings suggest directions for improving continual learning benchmarks and methods.



