# Arxiv Papers in cs.CV on 2019-08-08
### EdgeNet: Semantic Scene Completion from a Single RGB-D Image
- **Arxiv ID**: http://arxiv.org/abs/1908.02893v2
- **DOI**: 10.1109/ICPR48806.2021.9413252
- **Categories**: **cs.CV**, I.4.6; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1908.02893v2)
- **Published**: 2019-08-08 01:00:11+00:00
- **Updated**: 2020-09-06 20:17:18+00:00
- **Authors**: Aloisio Dourado, Teofilo Emidio de Campos, Hansung Kim, Adrian Hilton
- **Comment**: 10 pages, 5 figures Accepted at ICPR 2020
- **Journal**: None
- **Summary**: Semantic scene completion is the task of predicting a complete 3D representation of volumetric occupancy with corresponding semantic labels for a scene from a single point of view. Previous works on Semantic Scene Completion from RGB-D data used either only depth or depth with colour by projecting the 2D image into the 3D volume resulting in a sparse data representation. In this work, we present a new strategy to encode colour information in 3D space using edge detection and flipped truncated signed distance. We also present EdgeNet, a new end-to-end neural network architecture capable of handling features generated from the fusion of depth and edge information. Experimental results show improvement of 6.9% over the state-of-the-art result on real data, for end-to-end approaches.



### iCassava 2019 Fine-Grained Visual Categorization Challenge
- **Arxiv ID**: http://arxiv.org/abs/1908.02900v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02900v2)
- **Published**: 2019-08-08 01:43:24+00:00
- **Updated**: 2019-12-24 00:21:51+00:00
- **Authors**: Ernest Mwebaze, Timnit Gebru, Andrea Frome, Solomon Nsumba, Jeremy Tusubira
- **Comment**: Kaggle competition website:
  https://www.kaggle.com/c/cassava-disease/overview, CVPR fine-grained visual
  categorization website: https://sites.google.com/view/fgvc6
- **Journal**: None
- **Summary**: Viral diseases are major sources of poor yields for cassava, the 2nd largest provider of carbohydrates in Africa.At least 80% of small-holder farmer households in Sub-Saharan Africa grow cassava. Since many of these farmers have smart phones, they can easily obtain photos of dis-eased and healthy cassava leaves in their farms, allowing the opportunity to use computer vision techniques to monitor the disease type and severity and increase yields. How-ever, annotating these images is extremely difficult as ex-perts who are able to distinguish between highly similar dis-eases need to be employed. We provide a dataset of labeled and unlabeled cassava leaves and formulate a Kaggle challenge to encourage participants to improve the performance of their algorithms using semi-supervised approaches. This paper describes our dataset and challenge which is part of the Fine-Grained Visual Categorization workshop at CVPR2019.



### Image Captioning using Facial Expression and Attention
- **Arxiv ID**: http://arxiv.org/abs/1908.02923v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.02923v3)
- **Published**: 2019-08-08 04:07:39+00:00
- **Updated**: 2020-04-15 02:01:07+00:00
- **Authors**: Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris
- **Comment**: None
- **Journal**: None
- **Summary**: Benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. For the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer's view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. This paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. The models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. We compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. The evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.



### Bayesian Feature Pyramid Networks for Automatic Multi-Label Segmentation of Chest X-rays and Assessment of Cardio-Thoratic Ratio
- **Arxiv ID**: http://arxiv.org/abs/1908.02924v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02924v1)
- **Published**: 2019-08-08 04:16:25+00:00
- **Updated**: 2019-08-08 04:16:25+00:00
- **Authors**: Roman Solovyev, Iaroslav Melekhov, Timo Lesonen, Elias Vaattovaara, Osmo Tervonen, Aleksei Tiulpin
- **Comment**: Roman Solovyev and Iaroslav Melekhov contributed equally. Timo
  Lesonen and Elias Vaattovaara contributed equally
- **Journal**: None
- **Summary**: Cardiothoratic ratio (CTR) estimated from chest radiographs is a marker indicative of cardiomegaly, the presence of which is in the criteria for heart failure diagnosis. Existing methods for automatic assessment of CTR are driven by Deep Learning-based segmentation. However, these techniques produce only point estimates of CTR but clinical decision making typically assumes the uncertainty. In this paper, we propose a novel method for chest X-ray segmentation and CTR assessment in an automatic manner. In contrast to the previous art, we, for the first time, propose to estimate CTR with uncertainty bounds. Our method is based on Deep Convolutional Neural Network with Feature Pyramid Network (FPN) decoder. We propose two modifications of FPN: replace the batch normalization with instance normalization and inject the dropout which allows to obtain the Monte-Carlo estimates of the segmentation maps at test time. Finally, using the predicted segmentation mask samples, we estimate CTR with uncertainty. In our experiments we demonstrate that the proposed method generalizes well to three different test sets. Finally, we make the annotations produced by two radiologists for all our datasets publicly available.



### Towards Generating Stylized Image Captions via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1908.02943v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.02943v1)
- **Published**: 2019-08-08 06:25:38+00:00
- **Updated**: 2019-08-08 06:25:38+00:00
- **Authors**: Omid Mohamad Nezami, Mark Dras, Stephen Wan, Cecile Paris, Len Hamey
- **Comment**: None
- **Journal**: None
- **Summary**: While most image captioning aims to generate objective descriptions of images, the last few years have seen work on generating visually grounded image captions which have a specific style (e.g., incorporating positive or negative sentiment). However, because the stylistic component is typically the last part of training, current models usually pay more attention to the style at the expense of accurate content description. In addition, there is a lack of variability in terms of the stylistic aspects. To address these issues, we propose an image captioning model called ATTEND-GAN which has two core components: first, an attention-based caption generator to strongly correlate different parts of an image with different parts of a caption; and second, an adversarial training mechanism to assist the caption generator to add diverse stylistic components to the generated captions. Because of these components, ATTEND-GAN can generate correlated captions as well as more human-like variability of stylistic patterns. Our system outperforms the state-of-the-art as well as a collection of our baseline models. A linguistic analysis of the generated captions demonstrates that captions generated using ATTEND-GAN have a wider range of stylistic adjectives and adjective-noun pairs.



### Progressive Relation Learning for Group Activity Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.02948v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02948v2)
- **Published**: 2019-08-08 06:50:42+00:00
- **Updated**: 2020-03-03 09:05:03+00:00
- **Authors**: Guyue Hu, Bo Cui, Yuan He, Shan Yu
- **Comment**: 8 pages; Accepted to CVPR2020; Supplementary Materials will appear on
  site <CVF Open Access>;
- **Journal**: None
- **Summary**: Group activities usually involve spatiotemporal dynamics among many interactive individuals, while only a few participants at several key frames essentially define the activity. Therefore, effectively modeling the group-relevant and suppressing the irrelevant actions (and interactions) are vital for group activity recognition. In this paper, we propose a novel method based on deep reinforcement learning to progressively refine the low-level features and high-level relations of group activities. Firstly, we construct a semantic relation graph (SRG) to explicitly model the relations among persons. Then, two agents adopting policy according to two Markov decision processes are applied to progressively refine the SRG. Specifically, one feature-distilling (FD) agent in the discrete action space refines the low-level spatio-temporal features by distilling the most informative frames. Another relation-gating (RG) agent in continuous action space adjusts the high-level semantic graph to pay more attention to group-relevant relations. The SRG, FD agent, and RG agent are optimized alternately to mutually boost the performance of each other. Extensive experiments on two widely used benchmarks demonstrate the effectiveness and superiority of the proposed approach.



### Semi Supervised Phrase Localization in a Bidirectional Caption-Image Retrieval Framework
- **Arxiv ID**: http://arxiv.org/abs/1908.02950v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.02950v1)
- **Published**: 2019-08-08 07:01:26+00:00
- **Updated**: 2019-08-08 07:01:26+00:00
- **Authors**: Deepan Das, Noor Mohammed Ghouse, Shashank Verma, Yin Li
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel deep neural network architecture that links visual regions to corresponding textual segments including phrases and words. To accomplish this task, our architecture makes use of the rich semantic information available in a joint embedding space of multi-modal data. From this joint embedding space, we extract the associative localization maps that develop naturally, without explicitly providing supervision during training for the localization task. The joint space is learned using a bidirectional ranking objective that is optimized using a $N$-Pair loss formulation. This training mechanism demonstrates the idea that localization information is learned inherently while optimizing a Bidirectional Retrieval objective. The model's retrieval and localization performance is evaluated on MSCOCO and Flickr30K Entities datasets. This architecture outperforms the state of the art results in the semi-supervised phrase localization setting.



### CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense
- **Arxiv ID**: http://arxiv.org/abs/1908.02962v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1908.02962v3)
- **Published**: 2019-08-08 08:07:35+00:00
- **Updated**: 2021-10-27 02:22:47+00:00
- **Authors**: Difei Gao, Ruiping Wang, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Alternatively inferring on the visual facts and commonsense is fundamental for an advanced VQA system. This ability requires models to go beyond the literal understanding of commonsense. The system should not just treat objects as the entrance to query background knowledge, but fully ground commonsense to the visual world and imagine the possible relationships between objects, e.g., "fork, can lift, food". To comprehensively evaluate such abilities, we propose a VQA benchmark, CRIC, which introduces new types of questions about Compositional Reasoning on vIsion and Commonsense, and an evaluation metric integrating the correctness of answering and commonsense grounding. To collect such questions and rich additional annotations to support the metric, we also propose an automatic algorithm to generate question samples from the scene graph associated with the images and the relevant knowledge graph. We further analyze several representative types of VQA models on the CRIC dataset. Experimental results show that grounding the commonsense to the image region and joint reasoning on vision and commonsense are still challenging for current approaches. The dataset is available at https://cricvqa.github.io.



### Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.02983v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02983v5)
- **Published**: 2019-08-08 09:17:54+00:00
- **Updated**: 2020-06-29 08:18:31+00:00
- **Authors**: Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, Kevin McGuinness
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised learning, i.e. jointly learning from labeled and unlabeled samples, is an active research topic due to its key role on relaxing human supervision. In the context of image classification, recent advances to learn from unlabeled samples are mainly focused on consistency regularization methods that encourage invariant predictions for different perturbations of unlabeled samples. We, conversely, propose to learn from unlabeled data by generating soft pseudo-labels using the network predictions. We show that a naive pseudo-labeling overfits to incorrect pseudo-labels due to the so-called confirmation bias and demonstrate that mixup augmentation and setting a minimum number of labeled samples per mini-batch are effective regularization techniques for reducing it. The proposed approach achieves state-of-the-art results in CIFAR-10/100, SVHN, and Mini-ImageNet despite being much simpler than other methods. These results demonstrate that pseudo-labeling alone can outperform consistency regularization methods, while the opposite was supposed in previous work. Source code is available at https://git.io/fjQsC.



### Fast Point R-CNN
- **Arxiv ID**: http://arxiv.org/abs/1908.02990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02990v2)
- **Published**: 2019-08-08 09:52:09+00:00
- **Updated**: 2019-08-16 03:05:50+00:00
- **Authors**: Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia
- **Comment**: None
- **Journal**: None
- **Summary**: We present a unified, efficient and effective framework for point-cloud based 3D object detection. Our two-stage approach utilizes both voxel representation and raw point cloud data to exploit respective advantages. The first stage network, with voxel representation as input, only consists of light convolutional operations, producing a small number of high-quality initial predictions. Coordinate and indexed convolutional feature of each point in initial prediction are effectively fused with the attention mechanism, preserving both accurate localization and context information. The second stage works on interior points with their fused feature for further refining the prediction. Our method is evaluated on KITTI dataset, in terms of both 3D and Bird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS detection rate.



### Manifold Modeling in Embedded Space: A Perspective for Interpreting Deep Image Prior
- **Arxiv ID**: http://arxiv.org/abs/1908.02995v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.02995v2)
- **Published**: 2019-08-08 10:05:09+00:00
- **Updated**: 2020-01-21 08:14:35+00:00
- **Authors**: Tatsuya Yokota, Hidekata Hontani, Qibin Zhao, Andrzej Cichocki
- **Comment**: None
- **Journal**: None
- **Summary**: Deep image prior (DIP), which utilizes a deep convolutional network (ConvNet) structure itself as an image prior, has attracted attentions in computer vision and machine learning communities. It empirically shows the effectiveness of ConvNet structure for various image restoration applications. However, why the DIP works so well is still unknown, and why convolution operation is useful for image reconstruction or enhancement is not very clear. In this study, we tackle these questions. The proposed approach is dividing the convolution into ``delay-embedding'' and ``transformation (\ie encoder-decoder)'', and proposing a simple, but essential, image/tensor modeling method which is closely related to dynamical systems and self-similarity. The proposed method named as manifold modeling in embedded space (MMES) is implemented by using a novel denoising-auto-encoder in combination with multi-way delay-embedding transform. In spite of its simplicity, the image/tensor completion, super-resolution, deconvolution, and denoising results of MMES are quite similar even competitive to DIP in our extensive experiments, and these results would help us for reinterpreting/characterizing the DIP from a perspective of ``low-dimensional patch-manifold prior''.



### Constrained domain adaptation for Image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.02996v2
- **DOI**: 10.1109/TMI.2021.3067688
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.02996v2)
- **Published**: 2019-08-08 10:07:39+00:00
- **Updated**: 2021-04-11 20:37:07+00:00
- **Authors**: Mathilde Bateson, Jose Dolz, Hoel Kervadec, Hervé Lombaert, Ismail Ben Ayed
- **Comment**: Published in IEEE Transactions on Medical Imaging. First version in
  MICCAI 2019
- **Journal**: IEEE IEEE Transactions on Medical Imaging 2021
- **Summary**: We propose to adapt segmentation networks with a constrained formulation, which embeds domain-invariant prior knowledge about the segmentation regions. Such knowledge may take the form of simple anatomical information, e.g., structure size or shape, estimated from source samples or known a priori. Our method imposes domain-invariant inequality constraints on the network outputs of unlabeled target samples. It implicitly matches prediction statistics between target and source domains with permitted uncertainty of prior knowledge. We address our constrained problem with a differentiable penalty, fully suited for standard stochastic gradient descent approaches, removing the need for computationally expensive Lagrangian optimization with dual projections. Unlike current two-step adversarial training, our formulation is based on a single loss in a single network, which simplifies adaptation by avoiding extra adversarial steps, while improving convergence and quality of training.   The comparison of our approach with state-of-the-art adversarial methods reveals substantially better performance on the challenging task of adapting spine segmentation across different MRI modalities. Our results also show a robustness to imprecision of size priors, approaching the accuracy of a fully supervised model trained directly in a target domain.Our method can be readily used for various constraints and segmentation problems.



### Learning Vision-based Flight in Drone Swarms by Imitation
- **Arxiv ID**: http://arxiv.org/abs/1908.02999v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1908.02999v1)
- **Published**: 2019-08-08 10:19:48+00:00
- **Updated**: 2019-08-08 10:19:48+00:00
- **Authors**: Fabian Schilling, Julien Lecoeur, Fabrizio Schiano, Dario Floreano
- **Comment**: 8 pages, 8 figures, accepted for publication in the IEEE Robotics and
  Automation Letters (RA-L) on July 28, 2019. arXiv admin note: substantial
  text overlap with arXiv:1809.00543
- **Journal**: None
- **Summary**: Decentralized drone swarms deployed today either rely on sharing of positions among agents or detecting swarm members with the help of visual markers. This work proposes an entirely visual approach to coordinate markerless drone swarms based on imitation learning. Each agent is controlled by a small and efficient convolutional neural network that takes raw omnidirectional images as inputs and predicts 3D velocity commands that match those computed by a flocking algorithm. We start training in simulation and propose a simple yet effective unsupervised domain adaptation approach to transfer the learned controller to the real world. We further train the controller with data collected in our motion capture hall. We show that the convolutional neural network trained on the visual inputs of the drone can learn not only robust inter-agent collision avoidance but also cohesion of the swarm in a sample-efficient manner. The neural controller effectively learns to localize other agents in the visual input, which we show by visualizing the regions with the most influence on the motion of an agent. We remove the dependence on sharing positions among swarm members by taking only local visual information into account for control. Our work can therefore be seen as the first step towards a fully decentralized, vision-based swarm without the need for communication or visual markers.



### Feature selection of neural networks is skewed towards the less abstract cue
- **Arxiv ID**: http://arxiv.org/abs/1908.03000v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03000v1)
- **Published**: 2019-08-08 10:24:37+00:00
- **Updated**: 2019-08-08 10:24:37+00:00
- **Authors**: Marcell Wolnitza, Babette Dellen
- **Comment**: 8 pages, 5 figures, 8 tables
- **Journal**: None
- **Summary**: Artificial neural networks (ANNs) have become an important tool for image classification with many applications in research and industry. However, it remains largely unknown how relevant image features are selected and how data properties affect this process. In particular, we are interested whether the abstraction level of image cues correlating with class membership influences feature selection. We perform experiments with binary images that contain a combination of cues, representing two different levels of abstractions: one is a pattern drawn from a random distribution where class membership correlates with the statistics of the pattern, the other a combination of symbol-like entities, where the symbolic code correlates with class membership. When the network is trained with data in which both cues are equally significant, we observe that the cues at the lower abstraction level, i.e., the pattern, is learned, while the symbolic information is largely ignored, even in networks with many layers. Symbol-like entities are only learned if the importance of low-level cues is reduced compared to the high-level ones. These findings raise important questions about the relevance of features that are learned by deep ANNs and how learning could be shifted towards symbolic features.



### Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras
- **Arxiv ID**: http://arxiv.org/abs/1908.03030v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03030v2)
- **Published**: 2019-08-08 12:11:24+00:00
- **Updated**: 2020-09-07 13:07:28+00:00
- **Authors**: Andrew Gilbert, Matthew Trumble, Adrian Hilton, John Collomosse
- **Comment**: None
- **Journal**: None
- **Summary**: We aim to simultaneously estimate the 3D articulated pose and high fidelity volumetric occupancy of human performance, from multiple viewpoint video (MVV) with as few as two views. We use a multi-channel symmetric 3D convolutional encoder-decoder with a dual loss to enforce the learning of a latent embedding that enables inference of skeletal joint positions and a volumetric reconstruction of the performance. The inference is regularised via a prior learned over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions, and show this to generalise well across unseen subjects and actions. We demonstrate improved reconstruction accuracy and lower pose estimation error relative to prior work on two MVV performance capture datasets: Human 3.6M and TotalCapture.



### Editing Text in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1908.03047v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03047v1)
- **Published**: 2019-08-08 12:59:18+00:00
- **Updated**: 2019-08-08 12:59:18+00:00
- **Authors**: Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jingtuo Liu, Errui Ding, Xiang Bai
- **Comment**: accepted by ACM MM 2019
- **Journal**: None
- **Summary**: In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image. Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc.



### What goes around comes around: Cycle-Consistency-based Short-Term Motion Prediction for Anomaly Detection using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.03055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03055v1)
- **Published**: 2019-08-08 13:20:31+00:00
- **Updated**: 2019-08-08 13:20:31+00:00
- **Authors**: Thomas Golda, Nils Murzyn, Chengchao Qu, Kristian Kroschel
- **Comment**: Accepted for DLAM workshop at AVSS 2019
- **Journal**: None
- **Summary**: Anomaly detection plays in many fields of research, along with the strongly related task of outlier detection, a very important role. Especially within the context of the automated analysis of video material recorded by surveillance cameras, abnormal situations can be of very different nature. For this purpose this work investigates Generative-Adversarial-Network-based methods (GAN) for anomaly detection related to surveillance applications. The focus is on the usage of static camera setups, since this kind of camera is one of the most often used and belongs to the lower price segment. In order to address this task, multiple subtasks are evaluated, including the influence of existing optical flow methods for the incorporation of short-term temporal information, different forms of network setups and losses for GANs, and the use of morphological operations for further performance improvement. With these extension we achieved up to 2.4% better results. Furthermore, the final method reduced the anomaly detection error for GAN-based methods by about 42.8%.



### Sim-to-Real Learning for Casualty Detection from Ground Projected Point Cloud Data
- **Arxiv ID**: http://arxiv.org/abs/1908.03057v2
- **DOI**: 10.1109/IROS40897.2019.8967642
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.03057v2)
- **Published**: 2019-08-08 13:25:00+00:00
- **Updated**: 2019-08-09 15:37:31+00:00
- **Authors**: Roni Permana Saputra, Nemanja Rakicevic, Petar Kormushev
- **Comment**: 10 pages, 10 figures, accepted to the IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS), 2019
- **Journal**: 2019 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Macau, China, 2019, pp. 3918-3925
- **Summary**: This paper addresses the problem of human body detection---particularly a human body lying on the ground (a.k.a. casualty)---using point cloud data. This ability to detect a casualty is one of the most important features of mobile rescue robots, in order for them to be able to operate autonomously. We propose a deep-learning-based casualty detection method using a deep convolutional neural network (CNN). This network is trained to be able to detect a casualty using a point-cloud data input. In the method we propose, the point cloud input is pre-processed to generate a depth image-like ground-projected heightmap. This heightmap is generated based on the projected distance of each point onto the detected ground plane within the point cloud data. The generated heightmap -- in image form -- is then used as an input for the CNN to detect a human body lying on the ground. To train the neural network, we propose a novel sim-to-real approach, in which the network model is trained using synthetic data obtained in simulation and then tested on real sensor data. To make the model transferable to real data implementations, during the training we adopt specific data augmentation strategies with the synthetic training data. The experimental results show that data augmentation introduced during the training process is essential for improving the performance of the trained model on real data. More specifically, the results demonstrate that the data augmentations on raw point-cloud data have contributed to a considerable improvement of the trained model performance.



### ExtremeC3Net: Extreme Lightweight Portrait Segmentation Networks using Advanced C3-modules
- **Arxiv ID**: http://arxiv.org/abs/1908.03093v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03093v3)
- **Published**: 2019-08-08 14:31:18+00:00
- **Updated**: 2019-12-09 16:26:26+00:00
- **Authors**: Hyojin Park, Lars Lowe Sjösund, YoungJoon Yoo, Jihwan Bang, Nojun Kwak
- **Comment**: https://github.com/HYOJINPARK/ExtPortraitSeg
- **Journal**: None
- **Summary**: Designing a lightweight and robust portrait segmentation algorithm is an important task for a wide range of face applications. However, the problem has been considered as a subset of the object segmentation problem. bviously, portrait segmentation has its unique requirements. First, because the portrait segmentation is performed in the middle of a whole process of many realworld applications, it requires extremely lightweight models. Second, there has not been any public datasets in this domain that contain a sufficient number of images with unbiased statistics. To solve the problems, we introduce a new extremely lightweight portrait segmentation model consisting of a two-branched architecture based on the concentrated-comprehensive convolutions block. Our method reduces the number of parameters from 2.1M to 37.7K (around 98.2% reduction), while maintaining the accuracy within a 1% margin from the state-of-the-art portrait segmentation method. In our qualitative and quantitative analysis on the EG1800 dataset, we show that our method outperforms various existing lightweight segmentation models. Second, we propose a simple method to create additional portrait segmentation data which can improve accuracy on the EG1800 dataset. Also, we analyze the bias in public datasets by additionally annotating race, gender, and age on our own. The augmented dataset, the additional annotations and code are available in https://github.com/HYOJINPARK/ExtPortraitSeg .



### Enhancing self-supervised monocular depth estimation with traditional visual odometry
- **Arxiv ID**: http://arxiv.org/abs/1908.03127v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03127v2)
- **Published**: 2019-08-08 15:39:34+00:00
- **Updated**: 2019-08-12 19:28:18+00:00
- **Authors**: Lorenzo Andraghetti, Panteleimon Myriokefalitakis, Pier Luigi Dovesi, Belen Luque, Matteo Poggi, Alessandro Pieropan, Stefano Mattoccia
- **Comment**: Accepted to 3DV 2019
- **Journal**: None
- **Summary**: Estimating depth from a single image represents an attractive alternative to more traditional approaches leveraging multiple cameras. In this field, deep learning yielded outstanding results at the cost of needing large amounts of data labeled with precise depth measurements for training. An issue softened by self-supervised approaches leveraging monocular sequences or stereo pairs in place of expensive ground truth depth annotations. This paper enables to further improve monocular depth estimation by integrating into existing self-supervised networks a geometrical prior. Specifically, we propose a sparsity-invariant autoencoder able to process the output of conventional visual odometry algorithms working in synergy with depth-from-mono networks. Experimental results on the KITTI dataset show that by exploiting the geometrical prior, our proposal: i) outperforms existing approaches in the literature and ii) couples well with both compact and complex depth-from-mono architectures, allowing for its deployment on high-end GPUs as well as on embedded devices (e.g., NVIDIA Jetson TX2).



### Defending Against Adversarial Iris Examples Using Wavelet Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1908.03176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.03176v1)
- **Published**: 2019-08-08 17:08:25+00:00
- **Updated**: 2019-08-08 17:08:25+00:00
- **Authors**: Sobhan Soleymani, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: The Tenth IEEE International Conference on Biometrics: Theory,
  Applications, and Systems (BTAS 2019)
- **Journal**: None
- **Summary**: Deep neural networks have presented impressive performance in biometric applications. However, their performance is highly at risk when facing carefully crafted input samples known as adversarial examples. In this paper, we present three defense strategies to detect adversarial iris examples. These defense strategies are based on wavelet domain denoising of the input examples by investigating each wavelet sub-band and removing the sub-bands that are most affected by the adversary. The first proposed defense strategy reconstructs multiple denoised versions of the input example through manipulating the mid- and high-frequency components of the wavelet domain representation of the input example and makes a decision upon the classification result of the majority of the denoised examples. The second and third proposed defense strategies aim to denoise each wavelet domain sub-band and determine the sub-bands that are most likely affected by the adversary using the reconstruction error computed for each sub-band. We test the performance of the proposed defense strategies against several attack scenarios and compare the results with five state of the art defense strategies.



### Moviescope: Large-scale Analysis of Movies using Multiple Modalities
- **Arxiv ID**: http://arxiv.org/abs/1908.03180v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03180v1)
- **Published**: 2019-08-08 17:20:24+00:00
- **Updated**: 2019-08-08 17:20:24+00:00
- **Authors**: Paola Cascante-Bonilla, Kalpathy Sitaraman, Mengjia Luo, Vicente Ordonez
- **Comment**: None
- **Journal**: None
- **Summary**: Film media is a rich form of artistic expression. Unlike photography, and short videos, movies contain a storyline that is deliberately complex and intricate in order to engage its audience. In this paper we present a large scale study comparing the effectiveness of visual, audio, text, and metadata-based features for predicting high-level information about movies such as their genre or estimated budget. We demonstrate the usefulness of content-based methods in this domain in contrast to human-based and metadata-based predictions in the era of deep learning. Additionally, we provide a comprehensive study of temporal feature aggregation methods for representing video and text and find that simple pooling operations are effective in this domain. We also show to what extent different modalities are complementary to each other. To this end, we also introduce Moviescope, a new large-scale dataset of 5,000 movies with corresponding movie trailers (video + audio), movie posters (images), movie plots (text), and metadata.



### Dynamic Scale Inference by Entropy Minimization
- **Arxiv ID**: http://arxiv.org/abs/1908.03182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.03182v1)
- **Published**: 2019-08-08 17:21:20+00:00
- **Updated**: 2019-08-08 17:21:20+00:00
- **Authors**: Dequan Wang, Evan Shelhamer, Bruno Olshausen, Trevor Darrell
- **Comment**: None
- **Journal**: None
- **Summary**: Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.



### LVIS: A Dataset for Large Vocabulary Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.03195v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03195v2)
- **Published**: 2019-08-08 17:57:01+00:00
- **Updated**: 2019-09-15 15:10:45+00:00
- **Authors**: Agrim Gupta, Piotr Dollár, Ross Girshick
- **Comment**: Extension of the CVPR'19 paper describing release v0.5, the LVIS
  Challenge, and baseline results
- **Journal**: None
- **Summary**: Progress on object detection is enabled by datasets that focus the research community's attention on open challenges. This process led us from simple images to complex scenes and from bounding boxes to segmentation masks. In this work, we introduce LVIS (pronounced `el-vis'): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect ~2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples. Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge. LVIS is available at http://www.lvisdataset.org.



### Sparse Coding of Shape Trajectories for Facial Expression and Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.03231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03231v1)
- **Published**: 2019-08-08 18:16:37+00:00
- **Updated**: 2019-08-08 18:16:37+00:00
- **Authors**: Amor Ben Tanfous, Hassen Drira, Boulbaba Ben Amor
- **Comment**: 14 pages, 5 figures
- **Journal**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2019
- **Summary**: The detection and tracking of human landmarks in video streams has gained in reliability partly due to the availability of affordable RGB-D sensors. The analysis of such time-varying geometric data is playing an important role in the automatic human behavior understanding. However, suitable shape representations as well as their temporal evolution, termed trajectories, often lie to nonlinear manifolds. This puts an additional constraint (i.e., nonlinearity) in using conventional Machine Learning techniques. As a solution, this paper accommodates the well-known Sparse Coding and Dictionary Learning approach to study time-varying shapes on the Kendall shape spaces of 2D and 3D landmarks. We illustrate effective coding of 3D skeletal sequences for action recognition and 2D facial landmark sequences for macro- and micro-expression recognition. To overcome the inherent nonlinearity of the shape spaces, intrinsic and extrinsic solutions were explored. As main results, shape trajectories give rise to more discriminative time-series with suitable computational properties, including sparsity and vector space structure. Extensive experiments conducted on commonly-used datasets demonstrate the competitiveness of the proposed approaches with respect to state-of-the-art.



### Image-based marker tracking and registration for intraoperative 3D image-guided interventions using augmented reality
- **Arxiv ID**: http://arxiv.org/abs/1908.03237v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03237v1)
- **Published**: 2019-08-08 18:57:34+00:00
- **Updated**: 2019-08-08 18:57:34+00:00
- **Authors**: Andong Cao, Ali Dhanaliwala, Jianbo Shi, Terence Gade, Brian Park
- **Comment**: None
- **Journal**: None
- **Summary**: Augmented reality has the potential to improve operating room workflow by allowing physicians to "see" inside a patient through the projection of imaging directly onto the surgical field. For this to be useful the acquired imaging must be quickly and accurately registered with patient and the registration must be maintained. Here we describe a method for projecting a CT scan with Microsoft Hololens and then aligning that projection to a set of fiduciary markers. Radio-opaque stickers with unique QR-codes are placed on an object prior to acquiring a CT scan. The location of the markers in the CT scan are extracted and the CT scan is converted into a 3D surface object. The 3D object is then projected using the Hololens onto a table on which the same markers are placed. We designed an algorithm that aligns the markers on the 3D object with the markers on the table. To extract the markers and convert the CT into a 3D object took less than 5 seconds. To align three markers, it took $0.9 \pm 0.2$ seconds to achieve an accuracy of $5 \pm 2$ mm. These findings show that it is feasible to use a combined radio-opaque optical marker, placed on a patient prior to a CT scan, to subsequently align the acquired CT scan with the patient.



### WhiteNNer-Blind Image Denoising via Noise Whiteness Priors
- **Arxiv ID**: http://arxiv.org/abs/1908.03238v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03238v2)
- **Published**: 2019-08-08 19:00:17+00:00
- **Updated**: 2019-09-19 19:02:22+00:00
- **Authors**: Saeed Izadi, Zahra Mirikharaji, Mengliu Zhao, Ghassan Hamarneh
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: The accuracy of medical imaging-based diagnostics is directly impacted by the quality of the collected images. A passive approach to improve image quality is one that lags behind improvements in imaging hardware, awaiting better sensor technology of acquisition devices. An alternative, active strategy is to utilize prior knowledge of the imaging system to directly post-process and improve the acquired images. Traditionally, priors about the image properties are taken into account to restrict the solution space. However, few techniques exploit the prior about the noise properties. In this paper, we propose a neural network-based model for disentangling the signal and noise components of an input noisy image, without the need for any ground truth training data. We design a unified loss function that encodes priors about signal as well as noise estimate in the form of regularization terms. Specifically, by using total variation and piecewise constancy priors along with noise whiteness priors such as auto-correlation and stationary losses, our network learns to decouple an input noisy image into the underlying signal and noise components. We compare our proposed method to Noise2Noise and Noise2Self, as well as non-local mean and BM3D, on three public confocal laser endomicroscopy datasets. Experimental results demonstrate the superiority of our network compared to state-of-the-art in terms of PSNR and SSIM.



### GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1908.03245v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03245v1)
- **Published**: 2019-08-08 19:34:36+00:00
- **Updated**: 2019-08-08 19:34:36+00:00
- **Authors**: Xiaohong Liu, Yongrui Ma, Zhihao Shi, Jun Chen
- **Comment**: 10 pages, accepted in ICCV 2019, project page:
  https://proteus1991.github.io/GridDehazeNet/
- **Journal**: None
- **Summary**: We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.



### Efficient Inference of CNNs via Channel Pruning
- **Arxiv ID**: http://arxiv.org/abs/1908.03266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.03266v1)
- **Published**: 2019-08-08 20:57:27+00:00
- **Updated**: 2019-08-08 20:57:27+00:00
- **Authors**: Boyu Zhang, Azadeh Davoodi, Yu Hen Hu
- **Comment**: None
- **Journal**: None
- **Summary**: The deployment of Convolutional Neural Networks (CNNs) on resource constrained platforms such as mobile devices and embedded systems has been greatly hindered by their high implementation cost, and thus motivated a lot research interest in compressing and accelerating trained CNN models. Among various techniques proposed in literature, structured pruning, especially channel pruning, has gain a lot focus due to 1) its superior performance in memory, computation, and energy reduction; and 2) it is friendly to existing hardware and software libraries. In this paper, we investigate the intermediate results of convolutional layers and present a novel pivoted QR factorization based channel pruning technique that can prune any specified number of input channels of any layer. We also explore more pruning opportunities in ResNet-like architectures by applying two tweaks to our technique. Experiment results on VGG-16 and ResNet-50 models with ImageNet ILSVRC 2012 dataset are very impressive with 4.29X and 2.84X computation reduction while only sacrificing about 1.40\% top-5 accuracy. Compared to many prior works, the pruned models produced by our technique require up to 47.7\% less computation while still achieve higher accuracies.



### Apache Spark Accelerated Deep Learning Inference for Large Scale Satellite Image Analytics
- **Arxiv ID**: http://arxiv.org/abs/1908.04383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04383v1)
- **Published**: 2019-08-08 21:36:41+00:00
- **Updated**: 2019-08-08 21:36:41+00:00
- **Authors**: Dalton Lunga, Jonathan Gerrand, Hsiuhan Lexie Yang, Christopher Layton, Robert Stewart
- **Comment**: None
- **Journal**: None
- **Summary**: The shear volumes of data generated from earth observation and remote sensing technologies continue to make major impact; leaping key geospatial applications into the dual data and compute intensive era. As a consequence, this rapid advancement poses new computational and data processing challenges. We implement a novel remote sensing data flow (RESFlow) for advanced machine learning and computing with massive amounts of remotely sensed imagery. The core contribution is partitioning massive amount of data based on the spectral and semantic characteristics for distributed imagery analysis. RESFlow takes advantage of both a unified analytics engine for large-scale data processing and the availability of modern computing hardware to harness the acceleration of deep learning inference on expansive remote sensing imagery. The framework incorporates a strategy to optimize resource utilization across multiple executors assigned to a single worker. We showcase its deployment across computationally and data-intensive on pixel-level labeling workloads. The pipeline invokes deep learning inference at three stages; during deep feature extraction, deep metric mapping, and deep semantic segmentation. The tasks impose compute intensive and GPU resource sharing challenges motivating for a parallelized pipeline for all execution steps. By taking advantage of Apache Spark, Nvidia DGX1, and DGX2 computing platforms, we demonstrate unprecedented compute speed-ups for deep learning inference on pixel labeling workloads; processing 21,028~Terrabytes of imagery data and delivering an output maps at area rate of 5.245sq.km/sec, amounting to 453,168 sq.km/day - reducing a 28 day workload to 21~hours.



### Deep Learning for Visual Recognition of Environmental Enteropathy and Celiac Disease
- **Arxiv ID**: http://arxiv.org/abs/1908.03272v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.03272v1)
- **Published**: 2019-08-08 21:44:30+00:00
- **Updated**: 2019-08-08 21:44:30+00:00
- **Authors**: Aman Shrivastava, Karan Kant, Saurav Sengupta, Sung-Jun Kang, Marium Khan, Asad Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Donald E. Brown, Sana Syed
- **Comment**: None
- **Journal**: None
- **Summary**: Physicians use biopsies to distinguish between different but histologically similar enteropathies. The range of syndromes and pathologies that could cause different gastrointestinal conditions makes this a difficult problem. Recently, deep learning has been used successfully in helping diagnose cancerous tissues in histopathological images. These successes motivated the research presented in this paper, which describes a deep learning approach that distinguishes between Celiac Disease (CD) and Environmental Enteropathy (EE) and normal tissue from digitized duodenal biopsies. Experimental results show accuracies of over 90% for this approach. We also look into interpreting the neural network model using Gradient-weighted Class Activation Mappings and filter activations on input images to understand the visual explanations for the decisions made by the model.



### Exploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization
- **Arxiv ID**: http://arxiv.org/abs/1908.03274v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.03274v1)
- **Published**: 2019-08-08 21:52:28+00:00
- **Updated**: 2019-08-08 21:52:28+00:00
- **Authors**: Wei-Chiu Ma, Ignacio Tartavull, Ioan Andrei Bârsan, Shenlong Wang, Min Bai, Gellert Mattyus, Namdar Homayounfar, Shrinidhi Kowshika Lakshmikanth, Andrei Pokrovsky, Raquel Urtasun
- **Comment**: 8 pages, 4 figures, 4 tables, 2019 IEEE/RSJ International Conference
  on Intelligent Robots and Systems (IROS 2019)
- **Journal**: None
- **Summary**: In this paper we propose a novel semantic localization algorithm that exploits multiple sensors and has precision on the order of a few centimeters. Our approach does not require detailed knowledge about the appearance of the world, and our maps require orders of magnitude less storage than maps utilized by traditional geometry- and LiDAR intensity-based localizers. This is important as self-driving cars need to operate in large environments. Towards this goal, we formulate the problem in a Bayesian filtering framework, and exploit lanes, traffic signs, as well as vehicle dynamics to localize robustly with respect to a sparse semantic map. We validate the effectiveness of our method on a new highway dataset consisting of 312km of roads. Our experiments show that the proposed approach is able to achieve 0.05m lateral accuracy and 1.12m longitudinal accuracy on average while taking up only 0.3% of the storage required by previous LiDAR intensity-based approaches.



