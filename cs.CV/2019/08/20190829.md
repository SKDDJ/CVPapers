# Arxiv Papers in cs.CV on 2019-08-29
### Real-time interactive magnetic resonance (MR) temperature imaging in both aqueous and adipose tissues using cascaded deep neural networks for MR-guided focused ultrasound surgery (MRgFUS)
- **Arxiv ID**: http://arxiv.org/abs/1908.10995v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.10995v1)
- **Published**: 2019-08-29 00:33:29+00:00
- **Updated**: 2019-08-29 00:33:29+00:00
- **Authors**: Jong-Min Kim, You-Jin Jeong, Han-Jae Chung, Chulhyun Lee, Chang-Hyun Oh
- **Comment**: 40 pages, 11 figures, 1 table
- **Journal**: None
- **Summary**: Purpose: To acquire the real-time interactive temperature map for aqueous and adipose tissue, the problems of long acquisition and processing time must be addressed. To overcome these major challenges, this paper proposes a cascaded convolutional neural network (CNN) framework and multi-echo gradient echo (meGRE) with a single reference variable flip angle (srVFA).   Methods: To optimize the echo times for each method, MR images are acquired using a meGRE sequence; meGRE images with two flip angles (FAs) and meGRE images with a single FA are acquired during the pretreatment and treatment stages, respectively. These images are then processed and reconstructed by a cascaded CNN, which consists of two CNNs. The first CNN (called DeepACCnet) performs HR complex MR image reconstruction from the LR MR image acquired during the treatment stage, which is improved by the HR magnitude MR image acquired during the pretreatment stage. The second CNN (called DeepPROCnet) copes with T1 mapping.   Results: Measurements of temperature and T1 changes obtained by meGRE combined with srVFA and cascaded CNNs were achieved in an agarose gel phantom, ex vivo porcine muscle, and ex vivo porcine muscle with fat layers (heating tests), and in vivo human prostate and brain (non-heating tests). In the heating test, the maximum differences between fiber-optic sensor and samples are less than 1 degree Celcius. In all cases, temperature mapping using the cascaded CNN achieved the best results in all cases. The acquisition and processing times for the proposed method are 0.8 s and 32 ms, respectively.   Conclusions: Real-time interactive HR MR temperature mapping for simultaneously measuring aqueous and adipose tissue is feasible by combining a cascaded CNN with meGRE and srVFA.



### Focus-Enhanced Scene Text Recognition with Deformable Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1908.10998v2
- **DOI**: 10.1109/ICCC47050.2019.9064428
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.10998v2)
- **Published**: 2019-08-29 00:54:02+00:00
- **Updated**: 2019-09-23 07:23:03+00:00
- **Authors**: Linjie Deng, Yanxiang Gong, Xinchen Lu, Xin Yi, Zheng Ma, Mei Xie
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, scene text recognition methods based on deep learning have sprung up in computer vision area. The existing methods achieved great performances, but the recognition of irregular text is still challenging due to the various shapes and distorted patterns. Consider that at the time of reading words in the real world, normally we will not rectify it in our mind but adjust our focus and visual fields. Similarly, through utilizing deformable convolutional layers whose geometric structures are adjustable, we present an enhanced recognition network without the steps of rectification to deal with irregular text in this work. A number of experiments have been applied, where the results on public benchmarks demonstrate the effectiveness of our proposed components and shows that our method has reached satisfactory performances. The code will be publicly available at https://github.com/Alpaca07/dtr soon.



### Metric-based Regularization and Temporal Ensemble for Multi-task Learning using Heterogeneous Unsupervised Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.11024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11024v1)
- **Published**: 2019-08-29 02:39:27+00:00
- **Updated**: 2019-08-29 02:39:27+00:00
- **Authors**: Dae Ha Kim, Seung Hyun Lee, Byung Cheol Song
- **Comment**: 11 pages. To Appear in the IEEE International Conference on Computer
  Vision Workshops (ICCVW) 2019
- **Journal**: None
- **Summary**: One of the ways to improve the performance of a target task is to learn the transfer of abundant knowledge of a pre-trained network. However, learning of the pre-trained network requires high computation capability and large-scale labeled dataset. To mitigate the burden of large-scale labeling, learning in un/self-supervised manner can be a solution. In addition, using unsupervised multi-task learning, a generalized feature representation can be learned. However, unsupervised multi-task learning can be biased to a specific task. To overcome this problem, we propose the metric-based regularization term and temporal task ensemble (TTE) for multi-task learning. Since these two techniques prevent the entire network from learning in a state deviated to a specific task, it is possible to learn a generalized feature representation that appropriately reflects the characteristics of each task without biasing. Experimental results for three target tasks such as classification, object detection and embedding clustering prove that the TTE-based multi-task framework is more effective than the state-of-the-art (SOTA) method in improving the performance of a target task.



### Deep Floor Plan Recognition Using a Multi-Task Network with Room-Boundary-Guided Attention
- **Arxiv ID**: http://arxiv.org/abs/1908.11025v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11025v1)
- **Published**: 2019-08-29 02:50:05+00:00
- **Updated**: 2019-08-29 02:50:05+00:00
- **Authors**: Zhiliang Zeng, Xianzhi Li, Ying Kin Yu, Chi-Wing Fu
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a new approach to recognize elements in floor plan layouts. Besides walls and rooms, we aim to recognize diverse floor plan elements, such as doors, windows and different types of rooms, in the floor layouts. To this end, we model a hierarchy of floor plan elements and design a deep multi-task neural network with two tasks: one to learn to predict room-boundary elements, and the other to predict rooms with types. More importantly, we formulate the room-boundary-guided attention mechanism in our spatial contextual module to carefully take room-boundary features into account to enhance the room-type predictions. Furthermore, we design a cross-and-within-task weighted loss to balance the multi-label tasks and prepare two new datasets for floor plan recognition. Experimental results demonstrate the superiority and effectiveness of our network over the state-of-the-art methods.



### Point2SpatialCapsule: Aggregating Features and Spatial Relationships of Local Regions on Point Clouds using Spatial-aware Capsules
- **Arxiv ID**: http://arxiv.org/abs/1908.11026v1
- **DOI**: 10.1109/TIP.2020.3019925
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11026v1)
- **Published**: 2019-08-29 02:54:31+00:00
- **Updated**: 2019-08-29 02:54:31+00:00
- **Authors**: Xin Wen, Zhizhong Han, Xinhai Liu, Yu-Shen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Learning discriminative shape representation directly on point clouds is still challenging in 3D shape analysis and understanding. Recent studies usually involve three steps: first splitting a point cloud into some local regions, then extracting corresponding feature of each local region, and finally aggregating all individual local region features into a global feature as shape representation using simple max pooling. However, such pooling-based feature aggregation methods do not adequately take the spatial relationships between local regions into account, which greatly limits the ability to learn discriminative shape representation. To address this issue, we propose a novel deep learning network, named Point2SpatialCapsule, for aggregating features and spatial relationships of local regions on point clouds, which aims to learn more discriminative shape representation. Compared with traditional max-pooling based feature aggregation networks, Point2SpatialCapsule can explicitly learn not only geometric features of local regions but also spatial relationships among them. It consists of two modules. To resolve the disorder problem of local regions, the first module, named geometric feature aggregation, is designed to aggregate the local region features into the learnable cluster centers, which explicitly encodes the spatial locations from the original 3D space. The second module, named spatial relationship aggregation, is proposed for further aggregating clustered features and the spatial relationships among them in the feature space using the spatial-aware capsules developed in this paper. Compared to the previous capsule network based methods, the feature routing on the spatial-aware capsules can learn more discriminative spatial relationships among local regions for point clouds, which establishes a direct mapping between log priors and the spatial locations through feature clusters.



### DWnet: Deep-Wide Network for 3D Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.11036v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11036v1)
- **Published**: 2019-08-29 03:50:43+00:00
- **Updated**: 2019-08-29 03:50:43+00:00
- **Authors**: Yonghao Dang, Fuxing Yang, Jianqin Yin
- **Comment**: None
- **Journal**: None
- **Summary**: We propose in this paper a deep-wide network (DWnet) which combines the deep structure with the broad learning system (BLS) to recognize actions. Compared with the deep structure, the novel model saves lots of testing time and almost achieves real-time testing. Furthermore, the DWnet can capture better features than broad learning system can. In terms of methodology, we use pruned hierarchical co-occurrence network (PruHCN) to learn local and global spatial-temporal features. To obtain sufficient global information, BLS is used to expand features extracted by PruHCN. Experiments on two common skeletal datasets demonstrate the advantage of the proposed model on testing time and the effectiveness of the novel model to recognize the action.



### 3D Face Pose and Animation Tracking via Eigen-Decomposition based Bayesian Approach
- **Arxiv ID**: http://arxiv.org/abs/1908.11039v1
- **DOI**: 10.1007/978-3-642-41914-0\_55
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11039v1)
- **Published**: 2019-08-29 03:59:51+00:00
- **Updated**: 2019-08-29 03:59:51+00:00
- **Authors**: Ngoc-Trung Tran, Fakhr-Eddine Ababsa, Maurice Charbit, Jacques Feldmar, Dijana Petrovska-Delacrétaz, Gérard Chollet
- **Comment**: None
- **Journal**: Advances in Visual Computing - 9th International Symposium, ISVC
  2013, Rethymnon, Crete, Greece, July 29-31, 2013. Proceedings, Part I, pages
  562--571
- **Summary**: This paper presents a new method to track both the face pose and the face animation with a monocular camera. The approach is based on the 3D face model CANDIDE and on the SIFT (Scale Invariant Feature Transform) descriptors, extracted around a few given landmarks (26 selected vertices of CANDIDE model) with a Bayesian approach. The training phase is performed on a synthetic database generated from the first video frame. At each current frame, the face pose and animation parameters are estimated via a Bayesian approach, with a Gaussian prior and a Gaussian likelihood function whose the mean and the covariance matrix eigenvalues are updated from the previous frame using eigen decomposition. Numerical results on pose estimation and landmark locations are reported using the Boston University Face Tracking (BUFT) database and Talking Face video. They show that our approach, compared to six other published algorithms, provides a very good compromise and presents a promising perspective due to the good results in terms of landmark localization.



### Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.11044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11044v1)
- **Published**: 2019-08-29 04:17:23+00:00
- **Updated**: 2019-08-29 04:17:23+00:00
- **Authors**: Xiangyu Xu, Enrique Dunn
- **Comment**: Accepted for oral presentation at ICCV 2019
- **Journal**: None
- **Summary**: We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the Spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclideanshape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association.



### PopEval: A Character-Level Approach to End-To-End Evaluation Compatible with Word-Level Benchmark Dataset
- **Arxiv ID**: http://arxiv.org/abs/1908.11060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11060v1)
- **Published**: 2019-08-29 05:38:37+00:00
- **Updated**: 2019-08-29 05:38:37+00:00
- **Authors**: Hong-Seok Lee, Youngmin Yoon, Pil-Hoon Jang, Chankyu Choi
- **Comment**: Accepted by ICDAR 2019
- **Journal**: None
- **Summary**: The most prevalent scope of interest for OCR applications used to be scanned documents, but it has now shifted towards the natural scene. Despite the change of times, the existing evaluation methods are still based on the old criteria suited better for the past interests. In this paper, we propose PopEval, a novel evaluation approach for the recent OCR interests. The new and past evaluation algorithms were compared through the results on various datasets and OCR models. Compared to the other evaluation methods, the proposed evaluation algorithm was closer to the human's qualitative evaluation than other existing methods. Although the evaluation algorithm was devised as a character-level approach, the comparative experiment revealed that PopEval is also compatible on existing benchmark datasets annotated at word-level. The proposed evaluation algorithm is not only applicable to current end-to-end tasks, but also suggests a new direction to redesign the evaluation concept for further OCR researches.



### StarNet: Targeted Computation for Object Detection in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1908.11069v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11069v3)
- **Published**: 2019-08-29 06:54:46+00:00
- **Updated**: 2019-12-02 22:15:26+00:00
- **Authors**: Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, Zhifeng Chen, Jonathon Shlens, Vijay Vasudevan
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting objects from LiDAR point clouds is an important component of self-driving car technology as LiDAR provides high resolution spatial information. Previous work on point-cloud 3D object detection has re-purposed convolutional approaches from traditional camera imagery. In this work, we present an object detection system called StarNet designed specifically to take advantage of the sparse and 3D nature of point cloud data. StarNet is entirely point-based, uses no global information, has data dependent anchors, and uses sampling instead of learned region proposals. We demonstrate how this design leads to competitive or superior performance on the large Waymo Open Dataset and the KITTI detection dataset, as compared to convolutional baselines. In particular, we show how our detector can outperform a competitive baseline on Pedestrian detection on the Waymo Open Dataset by more than 7 absolute mAP while being more computationally efficient. We show how our redesign---namely using only local information and using sampling instead of learned proposals---leads to a significantly more flexible and adaptable system: we demonstrate how we can vary the computational cost of a single trained StarNet without retraining, and how we can target proposals towards areas of interest with priors and heuristics. Finally, we show how our design allows for incorporating temporal context by using detections from previous frames to target computation of the detector, which leads to further improvements in performance without additional computational cost.



### DV3+HED+: A DCNNs-based Framework to Monitor Temporary Works and ESAs in Railway Construction Project Using VHR Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1908.11080v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11080v1)
- **Published**: 2019-08-29 07:46:32+00:00
- **Updated**: 2019-08-29 07:46:32+00:00
- **Authors**: Rui Guo, Ronghua Liu, Na Li, Wei Liu
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Current VHR(Very High Resolution) satellite images enable the detailed monitoring of the earth and can capture the ongoing works of railway construction. In this paper, we present an integrated framework applied to monitoring the railway construction in China, using QuickBird, GF-2 and Google Earth VHR satellite images. We also construct a novel DCNNs-based (Deep Convolutional Neural Networks) semantic segmentation network to label the temporary works such as borrow & spoil area, camp, beam yard and ESAs(Environmental Sensitive Areas) such as resident houses throughout the whole railway construction project using VHR satellite images. In addition, we employ HED edge detection sub-network to refine the boundary details and attention cross entropy loss function to fit the sample class disequilibrium problem. Our semantic segmentation network is trained on 572 VHR true color images, and tested on the 15 QuickBird true color images along Ruichang-Jiujiang railway during 2015-2017. The experiment results show that compared with the existing state-of-the-art approach, our approach has obvious improvements with an overall accuracy of more than 80%.



### Minimum Delay Object Detection From Video
- **Arxiv ID**: http://arxiv.org/abs/1908.11092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11092v1)
- **Published**: 2019-08-29 08:25:40+00:00
- **Updated**: 2019-08-29 08:25:40+00:00
- **Authors**: Dong Lao, Ganesh Sundaramoorthi
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We consider the problem of detecting objects, as they come into view, from videos in an online fashion. We provide the first real-time solution that is guaranteed to minimize the delay, i.e., the time between when the object comes in view and the declared detection time, subject to acceptable levels of detection accuracy. The method leverages modern CNN-based object detectors that operate on a single frame, to aggregate detection results over frames to provide reliable detection at a rate, specified by the user, in guaranteed minimal delay. To do this, we formulate the problem as a Quickest Detection problem, which provides the aforementioned guarantees. We derive our algorithms from this theory. We show in experiments, that with an overhead of just 50 fps, we can increase the number of correct detections and decrease the overall computational cost compared to running a modern single-frame detector.



### Texture Retrieval in the Wild through detection-based attributes
- **Arxiv ID**: http://arxiv.org/abs/1908.11111v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11111v4)
- **Published**: 2019-08-29 09:13:13+00:00
- **Updated**: 2019-10-04 07:56:46+00:00
- **Authors**: Christian Joppi, Marco Godi, Andrea Giachetti, Fabio Pellacini, Marco Cristani
- **Comment**: ICIAP - International Conference on Image Analysis and Processing
- **Journal**: None
- **Summary**: Capturing the essence of a textile image in a robust way is important to retrieve it in a large repository, especially if it has been acquired in the wild (by taking a photo of the textile of interest). In this paper we show that a texel-based representation fits well with this task. In particular, we refer to Texel-Att, a recent texel-based descriptor which has shown to capture fine grained variations of a texture, for retrieval purposes. After a brief explanation of Texel-Att, we will show in our experiments that this descriptor is robust to distortions resulting from acquisitions in the wild by setting up an experiment in which textures from the ElBa (an Element-Based texture dataset) are artificially distorted and then used to retrieve the original image. We compare our approach with existing descriptors using a simple ranking framework based on distance functions. Results show that even under extreme conditions (such as a down-sampling with a factor of 10), we perform better than alternative approaches.



### Improving Self-Supervised Single View Depth Estimation by Masking Occlusion
- **Arxiv ID**: http://arxiv.org/abs/1908.11112v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11112v1)
- **Published**: 2019-08-29 09:13:29+00:00
- **Updated**: 2019-08-29 09:13:29+00:00
- **Authors**: Maarten Schellevis
- **Comment**: None
- **Journal**: None
- **Summary**: Single view depth estimation models can be trained from video footage using a self-supervised end-to-end approach with view synthesis as the supervisory signal. This is achieved with a framework that predicts depth and camera motion, with a loss based on reconstructing a target video frame from temporally adjacent frames. In this context, occlusion relates to parts of a scene that can be observed in the target frame but not in a frame used for image reconstruction. Since the image reconstruction is based on sampling from the adjacent frame, and occluded areas by definition cannot be sampled, reconstructed occluded areas corrupt to the supervisory signal. In previous work arXiv:1806.01260 occlusion is handled based on reconstruction error; at each pixel location, only the reconstruction with the lowest error is included in the loss. The current study aims to determine whether performance improvements of depth estimation models can be gained by during training only ignoring those regions that are affected by occlusion.   In this work we introduce occlusion mask, a mask that during training can be used to specifically ignore regions that cannot be reconstructed due to occlusions. Occlusion mask is based entirely on predicted depth information. We introduce two novel loss formulations which incorporate the occlusion mask. The method and implementation of arXiv:1806.01260 serves as the foundation for our modifications as well as the baseline in our experiments. We demonstrate that (i) incorporating occlusion mask in the loss function improves the performance of single image depth prediction models on the KITTI benchmark. (ii) loss functions that select from reconstructions based on error are able to ignore some of the reprojection error caused by object motion.



### Texel-Att: Representing and Classifying Element-based Textures by Attributes
- **Arxiv ID**: http://arxiv.org/abs/1908.11127v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.11127v2)
- **Published**: 2019-08-29 09:50:44+00:00
- **Updated**: 2019-08-30 07:14:19+00:00
- **Authors**: Marco Godi, Christian Joppi, Andrea Giachetti, Fabio Pellacini, Marco Cristani
- **Comment**: Accepted as oral at BMVC 2019
- **Journal**: None
- **Summary**: Element-based textures are a kind of texture formed by nameable elements, the texels [1], distributed according to specific statistical distributions; it is of primary importance in many sectors, namely textile, fashion and interior design industry. State-of-theart texture descriptors fail to properly characterize element-based texture, so we present Texel-Att to fill this gap. Texel-Att is the first fine-grained, attribute-based representation and classification framework for element-based textures. It first individuates texels, characterizing them with individual attributes; subsequently, texels are grouped and characterized through layout attributes, which give the Texel-Att representation. Texels are detected by a Mask-RCNN, trained on a brand-new element-based texture dataset, ElBa, containing 30K texture images with 3M fully-annotated texels. Examples of individual and layout attributes are exhibited to give a glimpse on the level of achievable graininess. In the experiments, we present detection results to show that texels can be precisely individuated, even on textures "in the wild"; to this sake, we individuate the element-based classes of the Describable Texture Dataset (DTD), where almost 900K texels have been manually annotated, leading to the Element-based DTD (E-DTD). Subsequently, classification and ranking results demonstrate the expressivity of Texel-Att on ElBa and E-DTD, overcoming the alternative features and relative attributes, doubling the best performance in some cases; finally, we report interactive search results on ElBa and E-DTD: with Texel-Att on the E-DTD dataset we are able to individuate within 10 iterations the desired texture in the 90% of cases, against the 71% obtained with a combination of the finest existing attributes so far. Dataset and code is available at https://github.com/godimarcovr/Texel-Att



### Estimation of Body Mass Index from Photographs using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.11694v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11694v1)
- **Published**: 2019-08-29 10:33:26+00:00
- **Updated**: 2019-08-29 10:33:26+00:00
- **Authors**: Adam Pantanowitz, Emmanuel Cohen, Philippe Gradidge, Nigel Crowther, Vered Aharonson, Benjamin Rosman, David M Rubin
- **Comment**: 7 pages, 4 figures, preprint journal
- **Journal**: None
- **Summary**: Obesity is an important concern in public health, and Body Mass Index is one of the useful (and proliferant) measures. We use Convolutional Neural Networks to determine Body Mass Index from photographs in a study with 161 participants. Low data, a common problem in medicine, is addressed by reducing the information in the photographs by generating silhouette images. Results present with high correlation when tested on unseen data.



### DeepDistance: A Multi-task Deep Regression Model for Cell Detection in Inverted Microscopy Images
- **Arxiv ID**: http://arxiv.org/abs/1908.11211v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.11211v1)
- **Published**: 2019-08-29 13:21:25+00:00
- **Updated**: 2019-08-29 13:21:25+00:00
- **Authors**: Can Fahrettin Koyuncu, Gozde Nur Gunesli, Rengul Cetin-Atalay, Cigdem Gunduz-Demir
- **Comment**: Preprint submitted to Elsevier
- **Journal**: None
- **Summary**: This paper presents a new deep regression model, which we call DeepDistance, for cell detection in images acquired with inverted microscopy. This model considers cell detection as a task of finding most probable locations that suggest cell centers in an image. It represents this main task with a regression task of learning an inner distance metric. However, different than the previously reported regression based methods, the DeepDistance model proposes to approach its learning as a multi-task regression problem where multiple tasks are learned by using shared feature representations. To this end, it defines a secondary metric, normalized outer distance, to represent a different aspect of the problem and proposes to define its learning as complementary to the main cell detection task. In order to learn these two complementary tasks more effectively, the DeepDistance model designs a fully convolutional network (FCN) with a shared encoder path and end-to-end trains this FCN to concurrently learn the tasks in parallel. DeepDistance uses the inner distances estimated by this FCN in a detection algorithm to locate individual cells in a given image. For further performance improvement on the main task, this paper also presents an extended version of the DeepDistance model. This extended model includes an auxiliary classification task and learns it in parallel to the two regression tasks by sharing feature representations with them. Our experiments on three different human cell lines reveal that the proposed multi-task learning models, the DeepDistance model and its extended version, successfully identify cell locations, even for the cell line that was not used in training, and improve the results of the previous deep learning methods.



### Texture Underfitting for Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1908.11215v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11215v1)
- **Published**: 2019-08-29 13:33:14+00:00
- **Updated**: 2019-08-29 13:33:14+00:00
- **Authors**: Jan-Nico Zaech, Dengxin Dai, Martin Hahner, Luc Van Gool
- **Comment**: Accepted manuscript, IEEE Intelligent Transportation Systems
  Conference, 6 pages
- **Journal**: None
- **Summary**: Comprehensive semantic segmentation is one of the key components for robust scene understanding and a requirement to enable autonomous driving. Driven by large scale datasets, convolutional neural networks show impressive results on this task. However, a segmentation algorithm generalizing to various scenes and conditions would require an enormously diverse dataset, making the labour intensive data acquisition and labeling process prohibitively expensive. Under the assumption of structural similarities between segmentation maps, domain adaptation promises to resolve this challenge by transferring knowledge from existing, potentially simulated datasets to new environments where no supervision exists. While the performance of this approach is contingent on the concept that neural networks learn a high level understanding of scene structure, recent work suggests that neural networks are biased towards overfitting to texture instead of learning structural and shape information. Considering the ideas underlying semantic segmentation, we employ random image stylization to augment the training dataset and propose a training procedure that facilitates texture underfitting to improve the performance of domain adaptation. In experiments with supervised as well as unsupervised methods for the task of synthetic-to-real domain adaptation, we show that our approach outperforms conventional training methods.



### Great Ape Detection in Challenging Jungle Camera Trap Footage via Attention-Based Spatial and Temporal Feature Blending
- **Arxiv ID**: http://arxiv.org/abs/1908.11240v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11240v1)
- **Published**: 2019-08-29 14:02:59+00:00
- **Updated**: 2019-08-29 14:02:59+00:00
- **Authors**: Xinyu Yang, Majid Mirmehdi, Tilo Burghardt
- **Comment**: Accepted by ICCV workshop 2019
- **Journal**: None
- **Summary**: We propose the first multi-frame video object detection framework trained to detect great apes. It is applicable to challenging camera trap footage in complex jungle environments and extends a traditional feature pyramid architecture by adding self-attention driven feature blending in both the spatial as well as the temporal domain. We demonstrate that this extension can detect distinctive species appearance and motion signatures despite significant partial occlusion. We evaluate the framework using 500 camera trap videos of great apes from the Pan African Programme containing 180K frames, which we manually annotated with accurate per-frame animal bounding boxes. These clips contain significant partial occlusions, challenging lighting, dynamic backgrounds, and natural camouflage effects. We show that our approach performs highly robustly and significantly outperforms frame-based detectors. We also perform detailed ablation studies and validation on the full ILSVRC 2015 VID data corpus to demonstrate wider applicability at adequate performance levels. We conclude that the framework is ready to assist human camera trap inspection efforts. We publish code, weights, and ground truth annotations with this paper.



### Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics
- **Arxiv ID**: http://arxiv.org/abs/1908.11262v1
- **DOI**: 10.1109/TITS.2019.2931429
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1908.11262v1)
- **Published**: 2019-08-29 14:37:40+00:00
- **Updated**: 2019-08-29 14:37:40+00:00
- **Authors**: Dogancan Temel, Min-Hung Chen, Ghassan AlRegib
- **Comment**: 13 pages, 9 figures, 4 tables. IEEE Transactions on Intelligent
  Transportation Systems, 2019
- **Journal**: None
- **Summary**: Traffic signs are critical for maintaining the safety and efficiency of our roads. Therefore, we need to carefully assess the capabilities and limitations of automated traffic sign detection systems. Existing traffic sign datasets are limited in terms of type and severity of challenging conditions. Metadata corresponding to these conditions are unavailable and it is not possible to investigate the effect of a single factor because of simultaneous changes in numerous conditions. To overcome the shortcomings in existing datasets, we introduced the CURE-TSD-Real dataset, which is based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. We test the performance of two benchmark algorithms and show that severe conditions can result in an average performance degradation of 29% in precision and 68% in recall. We investigate the effect of challenging conditions through spectral analysis and show that challenging conditions can lead to distinct magnitude spectrum characteristics. Moreover, we show that mean magnitude spectrum of changes in video sequences under challenging conditions can be an indicator of detection performance. CURE-TSD-Real dataset is available online at https://github.com/olivesgatech/CURE-TSD.



### Automated Detecting and Placing Road Objects from Street-level Images
- **Arxiv ID**: http://arxiv.org/abs/1909.05621v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1909.05621v3)
- **Published**: 2019-08-29 14:45:21+00:00
- **Updated**: 2019-09-17 11:06:37+00:00
- **Authors**: Chaoquan Zhang, Hongchao Fan, Wanzhi Li, Bo Mao, Xuan Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Navigation services utilized by autonomous vehicles or ordinary users require the availability of detailed information about road-related objects and their geolocations, especially at road intersections. However, these road intersections are mainly represented as point elements without detailed information, or are even not available in current versions of crowdsourced mapping databases including OpenStreetMap(OSM). This study develops an approach to automatically detect road objects and place them to right location from street-level images. Our processing pipeline relies on two convolutional neural networks: the first segments the images, while the second detects and classifies the specific objects. Moreover, to locate the detected objects, we establish an attributed topological binary tree(ATBT) based on urban grammar for each image to depict the coherent relations of topologies, attributes and semantics of the road objects. Then the ATBT is further matched with map features on OSM to determine the right placed location. The proposed method has been applied to a case study in Berlin, Germany. We validate the effectiveness of our method on two object classes: traffic signs and traffic lights. Experimental results demonstrate that the proposed approach provides near-precise localization results in terms of completeness and positional accuracy. Among many potential applications, the output may be combined with other sources of data to guide autonomous vehicles



### Exploiting Temporality for Semi-Supervised Video Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.11309v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11309v1)
- **Published**: 2019-08-29 15:50:12+00:00
- **Updated**: 2019-08-29 15:50:12+00:00
- **Authors**: Radu Sibechi, Olaf Booij, Nora Baka, Peter Bloem
- **Comment**: Accepted as workshop paper at ICCV 2019
- **Journal**: None
- **Summary**: In recent years, there has been remarkable progress in supervised image segmentation. Video segmentation is less explored, despite the temporal dimension being highly informative. Semantic labels, e.g. that cannot be accurately detected in the current frame, may be inferred by incorporating information from previous frames. However, video segmentation is challenging due to the amount of data that needs to be processed and, more importantly, the cost involved in obtaining ground truth annotations for each frame. In this paper, we tackle the issue of label scarcity by using consecutive frames of a video, where only one frame is annotated. We propose a deep, end-to-end trainable model which leverages temporal information in order to make use of easy to acquire unlabeled data. Our network architecture relies on a novel interconnection of two components: a fully convolutional network to model spatial information and temporal units that are employed at intermediate levels of the convolutional network in order to propagate information through time. The main contribution of this work is the guidance of the temporal signal through the network. We show that only placing a temporal module between the encoder and decoder is suboptimal (baseline). Our extensive experiments on the CityScapes dataset indicate that the resulting model can leverage unlabeled temporal frames and significantly outperform both the frame-by-frame image segmentation and the baseline approach.



### Aesthetic Image Captioning From Weakly-Labelled Photographs
- **Arxiv ID**: http://arxiv.org/abs/1908.11310v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.11310v1)
- **Published**: 2019-08-29 15:50:28+00:00
- **Updated**: 2019-08-29 15:50:28+00:00
- **Authors**: Koustav Ghosal, Aakanksha Rana, Aljosa Smolic
- **Comment**: International Workshop on Cross-Modal Learning in Real World, ICCV
  2019
- **Journal**: None
- **Summary**: Aesthetic image captioning (AIC) refers to the multi-modal task of generating critical textual feedbacks for photographs. While in natural image captioning (NIC), deep models are trained in an end-to-end manner using large curated datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC. Towards this goal, we propose an automatic cleaning strategy to create a benchmarking AIC dataset, by exploiting the images and noisy comments easily available from photography websites. We propose a probabilistic caption-filtering method for cleaning the noisy web-data, and compile a large-scale, clean dataset "AVA-Captions", (230, 000 images with 5 captions per image). Additionally, by exploiting the latent associations between aesthetic attributes, we propose a strategy for training the convolutional neural network (CNN) based visual feature extractor, the first component of the AIC framework. The strategy is weakly supervised and can be effectively used to learn rich aesthetic representations, without requiring expensive ground-truth annotations. We finally show-case a thorough analysis of the proposed contributions using automatic metrics and subjective evaluations.



### Flexible Conditional Image Generation of Missing Data with Learned Mental Maps
- **Arxiv ID**: http://arxiv.org/abs/1908.11312v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11312v1)
- **Published**: 2019-08-29 15:53:40+00:00
- **Updated**: 2019-08-29 15:53:40+00:00
- **Authors**: Benjamin Hou, Athanasios Vlontzos, Amir Alansary, Daniel Rueckert, Bernhard Kainz
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world settings often do not allow acquisition of high-resolution volumetric images for accurate morphological assessment and diagnostic. In clinical practice it is frequently common to acquire only sparse data (e.g. individual slices) for initial diagnostic decision making. Thereby, physicians rely on their prior knowledge (or mental maps) of the human anatomy to extrapolate the underlying 3D information. Accurate mental maps require years of anatomy training, which in the first instance relies on normative learning, i.e. excluding pathology. In this paper, we leverage Bayesian Deep Learning and environment mapping to generate full volumetric anatomy representations from none to a small, sparse set of slices. We evaluate proof of concept implementations based on Generative Query Networks (GQN) and Conditional BRUNO using abdominal CT and brain MRI as well as in a clinical application involving sparse, motion-corrupted MR acquisition for fetal imaging. Our approach allows to reconstruct 3D volumes from 1 to 4 tomographic slices, with a SSIM of 0.7+ and cross-correlation of 0.8+ compared to the 3D ground truth.



### Variational Denoising Network: Toward Blind Noise Modeling and Removal
- **Arxiv ID**: http://arxiv.org/abs/1908.11314v4
- **DOI**: None
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1908.11314v4)
- **Published**: 2019-08-29 15:54:06+00:00
- **Updated**: 2020-07-11 08:48:57+00:00
- **Authors**: Zongsheng Yue, Hongwei Yong, Qian Zhao, Lei Zhang, Deyu Meng
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.



### 3D Anchor-Free Lesion Detector on Computed Tomography Scans
- **Arxiv ID**: http://arxiv.org/abs/1908.11324v1
- **DOI**: 10.1109/TransAI.2019.00016
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11324v1)
- **Published**: 2019-08-29 16:15:45+00:00
- **Updated**: 2019-08-29 16:15:45+00:00
- **Authors**: Ning Zhang, Dechun Wang, Xinzi Sun, Pengfei Zhang, Chenxi Zhang, Yu Cao, Benyuan Liu
- **Comment**: None
- **Journal**: TransAI 2019
- **Summary**: Lesions are injuries and abnormal tissues in the human body. Detecting lesions in 3D Computed Tomography (CT) scans can be time-consuming even for very experienced physicians and radiologists. In recent years, CNN based lesion detectors have demonstrated huge potentials. Most of current state-of-the-art lesion detectors employ anchors to enumerate all possible bounding boxes with respect to the dataset in process. This anchor mechanism greatly improves the detection performance while also constraining the generalization ability of detectors. In this paper, we propose an anchor-free lesion detector. The anchor mechanism is removed and lesions are formalized as single keypoints. By doing so, we witness a considerable performance gain in terms of both accuracy and inference speed compared with the anchor-based baseline



### Temporal Consistency Objectives Regularize the Learning of Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1908.11330v1
- **DOI**: 10.1007/978-3-030-33391-1_2
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11330v1)
- **Published**: 2019-08-29 16:23:50+00:00
- **Updated**: 2019-08-29 16:23:50+00:00
- **Authors**: Gabriele Valvano, Agisilaos Chartsias, Andrea Leo, Sotirios A. Tsaftaris
- **Comment**: 9 pages, 4 figures (1 .gif), 1 table
- **Journal**: Domain Adaptation and Representation Transfer and Medical Image
  Learning with Less Labels and Imperfect Data. DART 2019, MIL3ID 2019. Lecture
  Notes in Computer Science, vol 11795 pp 11-19. Springer, Cham
- **Summary**: There has been an increasing focus in learning interpretable feature representations, particularly in applications such as medical image analysis that require explainability, whilst relying less on annotated data (since annotations can be tedious and costly). Here we build on recent innovations in style-content representations to learn anatomy, imaging characteristics (appearance) and temporal correlations. By introducing a self-supervised objective of predicting future cardiac phases we improve disentanglement. We propose a temporal transformer architecture that given an image conditioned on phase difference, it predicts a future frame. This forces the anatomical decomposition to be consistent with the temporal cardiac contraction in cine MRI and to have semantic meaning with less need for annotations. We demonstrate that using this regularization, we achieve competitive results and improve semi-supervised segmentation, especially when very few labelled data are available. Specifically, we show Dice increase of up to 19\% and 7\% compared to supervised and semi-supervised approaches respectively on the ACDC dataset. Code is available at: https://github.com/gvalvano/sdtnet .



### A Robust Image Watermarking System Based on Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.11331v1
- **DOI**: 10.1109/TMM.2020.3006415
- **Categories**: **cs.MM**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.11331v1)
- **Published**: 2019-08-29 16:24:29+00:00
- **Updated**: 2019-08-29 16:24:29+00:00
- **Authors**: Xin Zhong, Frank Y. Shih
- **Comment**: None
- **Journal**: None
- **Summary**: Digital image watermarking is the process of embedding and extracting watermark covertly on a carrier image. Incorporating deep learning networks with image watermarking has attracted increasing attention during recent years. However, existing deep learning-based watermarking systems cannot achieve robustness, blindness, and automated embedding and extraction simultaneously. In this paper, a fully automated image watermarking system based on deep neural networks is proposed to generalize the image watermarking processes. An unsupervised deep learning structure and a novel loss computation are proposed to achieve high capacity and high robustness without any prior knowledge of possible attacks. Furthermore, a challenging application of watermark extraction from camera-captured images is provided to validate the practicality as well as the robustness of the proposed system. Experimental results show the superiority performance of the proposed system as comparing against several currently available techniques.



### Universal, transferable and targeted adversarial attacks
- **Arxiv ID**: http://arxiv.org/abs/1908.11332v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11332v4)
- **Published**: 2019-08-29 16:27:24+00:00
- **Updated**: 2022-06-13 04:15:43+00:00
- **Authors**: Junde Wu, Rao Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks have been found vulnerable re-cently. A kind of well-designed inputs, which called adver-sarial examples, can lead the networks to make incorrectpredictions. Depending on the different scenarios, goalsand capabilities, the difficulties of the attacks are different.For example, a targeted attack is more difficult than a non-targeted attack, a universal attack is more difficult than anon-universal attack, a transferable attack is more difficultthan a nontransferable one. The question is: Is there existan attack that can meet all these requirements? In this pa-per, we answer this question by producing a kind of attacksunder these conditions. We learn a universal mapping tomap the sources to the adversarial examples. These exam-ples can fool classification networks to classify all of theminto one targeted class, and also have strong transferability.Our code is released at: xxxxx.



### GeoStyle: Discovering Fashion Trends and Events
- **Arxiv ID**: http://arxiv.org/abs/1908.11412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11412v1)
- **Published**: 2019-08-29 18:26:33+00:00
- **Updated**: 2019-08-29 18:26:33+00:00
- **Authors**: Utkarsh Mall, Kevin Matzen, Bharath Hariharan, Noah Snavely, Kavita Bala
- **Comment**: Accepted in ICCV 2019
- **Journal**: None
- **Summary**: Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper, we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are >20% more accurate than the prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe.



### Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training
- **Arxiv ID**: http://arxiv.org/abs/1908.11415v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11415v2)
- **Published**: 2019-08-29 18:33:21+00:00
- **Updated**: 2019-09-09 19:09:42+00:00
- **Authors**: Zelun Wang, Jyh-Charn Liu
- **Comment**: 11 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper we propose a deep neural network model with an encoder-decoder architecture that translates images of math formulas into their LaTeX markup sequences. The encoder is a convolutional neural network (CNN) that transforms images into a group of feature maps. To better capture the spatial relationships of math symbols, the feature maps are augmented with 2D positional encoding before being unfolded into a vector. The decoder is a stacked bidirectional long short-term memory (LSTM) model integrated with the soft attention mechanism, which works as a language model to translate the encoder output into a sequence of LaTeX tokens. The neural network is trained in two steps. The first step is token-level training using the Maximum-Likelihood Estimation (MLE) as the objective function. At completion of the token-level training, the sequence-level training objective function is employed to optimize the overall model based on the policy gradient algorithm from reinforcement learning. Our design also overcomes the exposure bias problem by closing the feedback loop in the decoder during sequence-level training, i.e., feeding in the predicted token instead of the ground truth token at every time step. The model is trained and evaluated on the IM2LATEX-100K dataset and shows state-of-the-art performance on both sequence-based and image-based evaluation metrics.



### DeepBbox: Accelerating Precise Ground Truth Generation for Autonomous Driving Datasets
- **Arxiv ID**: http://arxiv.org/abs/1909.05620v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1909.05620v1)
- **Published**: 2019-08-29 20:04:35+00:00
- **Updated**: 2019-08-29 20:04:35+00:00
- **Authors**: Govind Rathore, Wan-Yi Lin, Ji Eun Kim
- **Comment**: accepted by ITSC 2019
- **Journal**: None
- **Summary**: Autonomous driving requires various computer vision algorithms, such as object detection and tracking.Precisely-labeled datasets (i.e., objects are fully contained in bounding boxes with only a few extra pixels) are preferred for training such algorithms, so that the algorithms can detect exact locations of the objects. However, it is very time-consuming and hence expensive to generate precise labels for image sequences at scale. In this paper, we propose DeepBbox, an algorithm that corrects loose object labels into right bounding boxes to reduce human annotation efforts. We use Cityscapes dataset to show annotation efficiency and accuracy improvement using DeepBbox. Experimental results show that, with DeepBbox,we can increase the number of object edges that are labeled automatically (within 1\% error) by 50% to reduce manual annotation time.



### CorNet: Generic 3D Corners for 6D Pose Estimation of New Objects without Retraining
- **Arxiv ID**: http://arxiv.org/abs/1908.11457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.11457v1)
- **Published**: 2019-08-29 21:17:09+00:00
- **Updated**: 2019-08-29 21:17:09+00:00
- **Authors**: Giorgia Pitteri, Slobodan Ilic, Vincent Lepetit
- **Comment**: None
- **Journal**: None
- **Summary**: We present a novel approach to the detection and 3D pose estimation of objects in color images. Its main contribution is that it does not require any training phases nor data for new objects, while state-of-the-art methods typically require hours of training time and hundreds of training registered images. Instead, our method relies only on the objects' geometries. Our method focuses on objects with prominent corners, which covers a large number of industrial objects. We first learn to detect object corners of various shapes in images and also to predict their 3D poses, by using training images of a small set of objects. To detect a new object in a given image, we first identify its corners from its CAD model; we also detect the corners visible in the image and predict their 3D poses. We then introduce a RANSAC-like algorithm that robustly and efficiently detects and estimates the object's 3D pose by matching its corners on the CAD model with their detected counterparts in the image. Because we also estimate the 3D poses of the corners in the image, detecting only 1 or 2 corners is sufficient to estimate the pose of the object, which makes the approach robust to occlusions. We finally rely on a final check that exploits the full 3D geometry of the objects, in case multiple objects have the same corner spatial arrangement. The advantages of our approach make it particularly attractive for industrial contexts, and we demonstrate our approach on the challenging T-LESS dataset.



### Potential Flow Generator with $L_2$ Optimal Transport Regularity for Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1908.11462v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.11462v1)
- **Published**: 2019-08-29 22:00:49+00:00
- **Updated**: 2019-08-29 22:00:49+00:00
- **Authors**: Liu Yang, George Em Karniadakis
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a potential flow generator with $L_2$ optimal transport regularity, which can be easily integrated into a wide range of generative models including different versions of GANs and flow-based models. We show the correctness and robustness of the potential flow generator in several 2D problems, and illustrate the concept of "proximity" due to the $L_2$ optimal transport regularity. Subsequently, we demonstrate the effectiveness of the potential flow generator in image translation tasks with unpaired training data from the MNIST dataset and the CelebA dataset.



### Kinematic Single Vehicle Trajectory Prediction Baselines and Applications with the NGSIM Dataset
- **Arxiv ID**: http://arxiv.org/abs/1908.11472v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, 68-04
- **Links**: [PDF](http://arxiv.org/pdf/1908.11472v4)
- **Published**: 2019-08-29 22:38:28+00:00
- **Updated**: 2020-10-28 12:44:17+00:00
- **Authors**: Jean Mercat, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois, Guillermo Pita Gil
- **Comment**: None
- **Journal**: None
- **Summary**: In the recent vehicle trajectory prediction literature, the most common baselines are briefly introduced without the necessary information to reproduce it. In this article we produce reproducible vehicle prediction results from simple models. For that purpose, the process is explicit, and the code is available. Those baseline models are a constant velocity model and a single-vehicle prediction model. They are applied on the NGSIM US-101 and I-80 datasets using only relative positions. Thus, the process can be reproduced with any database containing tracking of vehicle positions. The evaluation reports Root Mean Squared Error (RMSE), Final Displacement Error (FDE), Negative Log-Likelihood (NLL), and Miss Rate (MR). The NLL estimation needs a careful definition because several formulations that differ from the mathematical definition are used in other works. This article is meant to be used along with the published code to establish baselines for further work. An extension is proposed to replace the constant velocity assumption with a learned model using a recurrent neural network. This brings good improvements in accuracy and uncertainty estimation and opens possibilities for both complex and interpretable models.



