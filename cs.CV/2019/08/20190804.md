# Arxiv Papers in cs.CV on 2019-08-04
### Attentive Normalization
- **Arxiv ID**: http://arxiv.org/abs/1908.01259v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01259v3)
- **Published**: 2019-08-04 02:17:34+00:00
- **Updated**: 2021-03-25 17:16:13+00:00
- **Authors**: Xilai Li, Wei Sun, Tianfu Wu
- **Comment**: ECCV2020
- **Journal**: None
- **Summary**: In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous. % with significant performance improvement shown in a vast amount of tasks. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between the two schema and present Attentive Normalization (AN). Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weighted-sum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging channel-wise feature attention. In experiments, we test the proposed AN using four representative neural architectures in the ImageNet-1000 classification benchmark and the MS-COCO 2017 object detection and instance segmentation benchmark. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5\% and 2.7\%, and absolute increase up to 1.8\% and 2.2\% for bounding box and mask AP in MS-COCO respectively. We observe that the proposed AN provides a strong alternative to the widely used Squeeze-and-Excitation (SE) module. The source codes are publicly available at https://github.com/iVMCL/AOGNet-v2 (the ImageNet Classification Repo) and https://github.com/iVMCL/AttentiveNorm\_Detection (the MS-COCO Detection and Segmentation Repo).



### Action Recognition in Untrimmed Videos with Composite Self-Attention Two-Stream Framework
- **Arxiv ID**: http://arxiv.org/abs/1908.04353v2
- **DOI**: 10.1007/978-3-030-41299-9_3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04353v2)
- **Published**: 2019-08-04 02:44:37+00:00
- **Updated**: 2019-09-02 05:36:11+00:00
- **Authors**: Dong Cao, Lisha Xu, HaiBo Chen
- **Comment**: Accepted to ACPR 2019
- **Journal**: None
- **Summary**: With the rapid development of deep learning algorithms, action recognition in video has achieved many important research results. One issue in action recognition, Zero-Shot Action Recognition (ZSAR), has recently attracted considerable attention, which classify new categories without any positive examples. Another difficulty in action recognition is that untrimmed data may seriously affect model performance. We propose a composite two-stream framework with a pre-trained model. Our proposed framework includes a classifier branch and a composite feature branch. The graph network model is adopted in each of the two branches, which effectively improves the feature extraction and reasoning ability of the framework. In the composite feature branch, a 3-channel self-attention models are constructed to weight each frame in the video and give more attention to the key frames. Each self-attention models channel outputs a set of attention weights to focus on a particular aspect of the video, and a set of attention weights corresponds to a one-dimensional vector. The 3-channel self-attention models can evaluate key frames from multiple aspects, and the output sets of attention weight vectors form an attention matrix, which effectively enhances the attention of key frames with strong correlation of action. This model can implement action recognition under zero-shot conditions, and has good recognition performance for untrimmed video data. Experimental results on relevant data sets confirm the validity of our model.



### Robust Subspace Discovery by Block-diagonal Adaptive Locality-constrained Representation
- **Arxiv ID**: http://arxiv.org/abs/1908.01266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01266v1)
- **Published**: 2019-08-04 03:43:00+00:00
- **Updated**: 2019-08-04 03:43:00+00:00
- **Authors**: Zhao Zhang, Jiahuan Ren, Sheng Li, Richang Hong, Zhengjun Zha, Meng Wang
- **Comment**: accepted by ACM Multimedia 2019
- **Journal**: None
- **Summary**: We propose a novel and unsupervised representation learning model, i.e., Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR). rBDLR is able to recover multi-subspace structures and extract the adaptive locality-preserving salient features jointly. Leveraging on the Frobenius-norm based latent low-rank representation model, rBDLR jointly learns the coding coefficients and salient features, and improves the results by enhancing the robustness to outliers and errors in given data, preserving local information of salient features adaptively and ensuring the block-diagonal structures of the coefficients. To improve the robustness, we perform the latent representation and adaptive weighting in a recovered clean data space. To force the coefficients to be block-diagonal, we perform auto-weighting by minimizing the reconstruction error based on salient features, constrained using a block-diagonal regularizer. This ensures that a strict block-diagonal weight matrix can be obtained and salient features will possess the adaptive locality preserving ability. By minimizing the difference between the coefficient and weights matrices, we can obtain a block-diagonal coefficients matrix and it can also propagate and exchange useful information between salient features and coefficients. Extensive results demonstrate the superiority of rBDLR over other state-of-the-art methods.



### Automatic segmentation of kidney and liver tumors in CT images
- **Arxiv ID**: http://arxiv.org/abs/1908.01279v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01279v2)
- **Published**: 2019-08-04 06:31:02+00:00
- **Updated**: 2019-09-16 23:46:24+00:00
- **Authors**: Dina B. Efremova, Dmitry A. Konovalov, Thanongchai Siriapisith, Worapan Kusakunniran, Peter Haddawy
- **Comment**: Method description manuscript for our test predictions for the 2019
  Kidney Tumor Segmentation Challenge, https://kits19.grand-challenge.org/home/
- **Journal**: None
- **Summary**: Automatic segmentation of hepatic lesions in computed tomography (CT) images is a challenging task to perform due to heterogeneous, diffusive shape of tumors and complex background. To address the problem more and more researchers rely on assistance of deep convolutional neural networks (CNN) with 2D or 3D type architecture that have proven to be effective in a wide range of computer vision tasks, including medical image processing. In this technical report, we carry out research focused on more careful approach to the process of learning rather than on complex architecture of the CNN. We have chosen MICCAI 2017 LiTS dataset for training process and the public 3DIRCADb dataset for validation of our method. The proposed algorithm reached DICE score 78.8% on the 3DIRCADb dataset. The described method was then applied to the 2019 Kidney Tumor Segmentation (KiTS-2019) challenge, where our single submission achieved 96.38% for kidney and 67.38% for tumor Dice scores.



### Softmax Dissection: Towards Understanding Intra- and Inter-class Objective for Embedding Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.01281v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.01281v2)
- **Published**: 2019-08-04 06:50:20+00:00
- **Updated**: 2020-02-12 08:03:55+00:00
- **Authors**: Lanqing He, Zhongdao Wang, Yali Li, Shengjin Wang
- **Comment**: Accepted to AAAI-2020, Oral presentation
- **Journal**: None
- **Summary**: The softmax loss and its variants are widely used as objectives for embedding learning, especially in applications like face recognition. However, the intra- and inter-class objectives in the softmax loss are entangled, therefore a well-optimized inter-class objective leads to relaxation on the intra-class objective, and vice versa. In this paper, we propose to dissect the softmax loss into independent intra- and inter-class objective (D-Softmax). With D-Softmax as objective, we can have a clear understanding of both the intra- and inter-class objective, therefore it is straightforward to tune each part to the best state. Furthermore, we find the computation of the inter-class objective is redundant and propose two sampling-based variants of D-Softmax to reduce the computation cost. Training with regular-scale data, experiments in face verification show D-Softmax is favorably comparable to existing losses such as SphereFace and ArcFace. Training with massive-scale data, experiments show the fast variants of D-Softmax significantly accelerates the training process (such as 64x) with only a minor sacrifice in performance, outperforming existing acceleration methods of softmax in terms of both performance and efficiency.



### BCD-Net for Low-dose CT Reconstruction: Acceleration, Convergence, and Generalization
- **Arxiv ID**: http://arxiv.org/abs/1908.01287v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01287v1)
- **Published**: 2019-08-04 07:10:24+00:00
- **Updated**: 2019-08-04 07:10:24+00:00
- **Authors**: Il Yong Chun, Xuehang Zheng, Yong Long, Jeffrey A. Fessler
- **Comment**: Accepted to MICCAI 2019, and the authors indicated by asterisks (*)
  equally contributed to this work
- **Journal**: None
- **Summary**: Obtaining accurate and reliable images from low-dose computed tomography (CT) is challenging. Regression convolutional neural network (CNN) models that are learned from training data are increasingly gaining attention in low-dose CT reconstruction. This paper modifies the architecture of an iterative regression CNN, BCD-Net, for fast, stable, and accurate low-dose CT reconstruction, and presents the convergence property of the modified BCD-Net. Numerical results with phantom data show that applying faster numerical solvers to model-based image reconstruction (MBIR) modules of BCD-Net leads to faster and more accurate BCD-Net; BCD-Net significantly improves the reconstruction accuracy, compared to the state-of-the-art MBIR method using learned transforms; BCD-Net achieves better image quality, compared to a state-of-the-art iterative NN architecture, ADMM-Net. Numerical results with clinical data show that BCD-Net generalizes significantly better than a state-of-the-art deep (non-iterative) regression NN, FBPConvNet, that lacks MBIR modules.



### To Learn or Not to Learn: Visual Localization from Essential Matrices
- **Arxiv ID**: http://arxiv.org/abs/1908.01293v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01293v2)
- **Published**: 2019-08-04 08:23:44+00:00
- **Updated**: 2020-03-09 13:14:40+00:00
- **Authors**: Qunjie Zhou, Torsten Sattler, Marc Pollefeys, Laura Leal-Taixe
- **Comment**: Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Visual localization is the problem of estimating a camera within a scene and a key component in computer vision applications such as self-driving cars and Mixed Reality. State-of-the-art approaches for accurate visual localization use scene-specific representations, resulting in the overhead of constructing these models when applying the techniques to new scenes. Recently, deep learning-based approaches based on relative pose estimation have been proposed, carrying the promise of easily adapting to new scenes. However, it has been shown such approaches are currently significantly less accurate than state-of-the-art approaches. In this paper, we are interested in analyzing this behavior. To this end, we propose a novel framework for visual localization from relative poses. Using a classical feature-based approach within this framework, we show state-of-the-art performance. Replacing the classical approach with learned alternatives at various levels, we then identify the reasons for why deep learned approaches do not perform well. Based on our analysis, we make recommendations for future work.



### Building Deep, Equivariant Capsule Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.01300v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01300v3)
- **Published**: 2019-08-04 09:14:29+00:00
- **Updated**: 2019-09-26 04:26:10+00:00
- **Authors**: Sairaam Venkatraman, S. Balasubramanian, R. Raghunatha Sarma
- **Comment**: None
- **Journal**: None
- **Summary**: Capsule networks are constrained by the parameter-expensive nature of their layers, and the general lack of provable equivariance guarantees. We present a variation of capsule networks that aims to remedy this. We identify that learning all pair-wise part-whole relationships between capsules of successive layers is inefficient. Further, we also realise that the choice of prediction networks and the routing mechanism are both key to equivariance. Based on these, we propose an alternative framework for capsule networks that learns to projectively encode the manifold of pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. This is done using a trainable, equivariant function defined over a grid of group-transformations. Thus, the prediction-phase of routing involves projection into the SOV of a deeper capsule using the corresponding function. As a specific instantiation of this idea, and also in order to reap the benefits of increased parameter-sharing, we use type-homogeneous group-equivariant convolutions of shallower capsules in this phase. We also introduce an equivariant routing mechanism based on degree-centrality. We show that this particular instance of our general model is equivariant, and hence preserves the compositional representation of an input under transformations. We conduct several experiments on standard object-classification datasets that showcase the increased transformation-robustness, as well as general performance, of our model to several capsule baselines.



### Adversarial View-Consistent Learning for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.01301v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01301v1)
- **Published**: 2019-08-04 09:37:24+00:00
- **Updated**: 2019-08-04 09:37:24+00:00
- **Authors**: Yixuan Liu, Yuwang Wang, Shengjin Wang
- **Comment**: BMVC 2019 Spotlight
- **Journal**: None
- **Summary**: This paper addresses the problem of Monocular Depth Estimation (MDE). Existing approaches on MDE usually model it as a pixel-level regression problem, ignoring the underlying geometry property. We empirically find this may result in sub-optimal solution: while the predicted depth map presents small loss value in one specific view, it may exhibit large loss if viewed in different directions. In this paper, inspired by multi-view stereo (MVS), we propose an Adversarial View-Consistent Learning (AVCL) framework to force the estimated depth map to be all reasonable viewed from multiple views. To this end, we first design a differentiable depth map warping operation, which is end-to-end trainable, and then propose a pose generator to generate novel views for a given image in an adversarial manner. Collaborating with the differentiable depth map warping operation, the pose generator encourages the depth estimation network to learn from hard views, hence produce view-consistent depth maps . We evaluate our method on NYU Depth V2 dataset and the experimental results show promising performance gain upon state-of-the-art MDE approaches.



### Theme-Aware Aesthetic Distribution Prediction With Full-Resolution Photographs
- **Arxiv ID**: http://arxiv.org/abs/1908.01308v3
- **DOI**: 10.1109/TNNLS.2022.3151787
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01308v3)
- **Published**: 2019-08-04 10:03:38+00:00
- **Updated**: 2022-03-16 14:27:28+00:00
- **Authors**: Gengyun Jia, Peipei Li, Ran He
- **Comment**: Accepted by IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Aesthetic quality assessment (AQA) is a challenging task due to complex aesthetic factors. Currently, it is common to conduct AQA using deep neural networks that require fixed-size inputs. Existing methods mainly transform images by resizing, cropping, and padding or employ adaptive pooling to alternately capture the aesthetic features from fixed-size inputs. However, these transformations potentially damage aesthetic features. To address this issue, we propose a simple but effective method to accomplish full-resolution image AQA by combining image padding with region of image (RoM) pooling. Padding turns inputs into the same size. RoM pooling pools image features and discards extra padded features to eliminate the side effects of padding. In addition, the image aspect ratios are encoded and fused with visual features to remedy the shape information loss of RoM pooling. Furthermore, we observe that the same image may receive different aesthetic evaluations under different themes, which we call theme criterion bias. Hence, a theme-aware model that uses theme information to guide model predictions is proposed. Finally, we design an attention-based feature fusion module to effectively utilize both the shape and theme information. Extensive experiments prove the effectiveness of the proposed method over state-of-the-art methods.



### Fully Automatic Video Colorization with Self-Regularization and Diversity
- **Arxiv ID**: http://arxiv.org/abs/1908.01311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01311v1)
- **Published**: 2019-08-04 10:22:36+00:00
- **Updated**: 2019-08-04 10:22:36+00:00
- **Authors**: Chenyang Lei, Qifeng Chen
- **Comment**: Published at the Computer Vision and Pattern Recognition (CVPR), 2019
- **Journal**: None
- **Summary**: We present a fully automatic approach to video colorization with self-regularization and diversity. Our model contains a colorization network for video frame colorization and a refinement network for spatiotemporal color refinement. Without any labeled data, both networks can be trained with self-regularized losses defined in bilateral and temporal space. The bilateral loss enforces color consistency between neighboring pixels in a bilateral space and the temporal loss imposes constraints between corresponding pixels in two nearby frames. While video colorization is a multi-modal problem, our method uses a perceptual loss with diversity to differentiate various modes in the solution space. Perceptual experiments demonstrate that our approach outperforms state-of-the-art approaches on fully automatic video colorization. The results are shown in the supplementary video at https://youtu.be/Y15uv2jnK-4



### Low-Rank Pairwise Alignment Bilinear Network For Few-Shot Fine-Grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.01313v3
- **DOI**: 10.1109/TMM.2020.3001510
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01313v3)
- **Published**: 2019-08-04 10:32:31+00:00
- **Updated**: 2020-05-28 03:43:34+00:00
- **Authors**: Huaxi Huang, Junjie Zhang, Jian Zhang, Jingsong Xu, Qiang Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks have demonstrated advanced abilities on various visual classification tasks, which heavily rely on the large-scale training samples with annotated ground-truth. However, it is unrealistic always to require such annotation in real-world applications. Recently, Few-Shot learning (FS), as an attempt to address the shortage of training samples, has made significant progress in generic classification tasks. Nonetheless, it is still challenging for current FS models to distinguish the subtle differences between fine-grained categories given limited training data. To filling the classification gap, in this paper, we address the Few-Shot Fine-Grained (FSFG) classification problem, which focuses on tackling the fine-grained classification under the challenging few-shot learning setting. A novel low-rank pairwise bilinear pooling operation is proposed to capture the nuanced differences between the support and query images for learning an effective distance metric. Moreover, a feature alignment layer is designed to match the support image features with query ones before the comparison. We name the proposed model Low-Rank Pairwise Alignment Bilinear Network (LRPABN), which is trained in an end-to-end fashion. Comprehensive experimental results on four widely used fine-grained classification datasets demonstrate that our LRPABN model achieves the superior performances compared to state-of-the-art methods.



### MoGA: Searching Beyond MobileNetV3
- **Arxiv ID**: http://arxiv.org/abs/1908.01314v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01314v4)
- **Published**: 2019-08-04 10:40:04+00:00
- **Updated**: 2020-03-03 04:11:41+00:00
- **Authors**: Xiangxiang Chu, Bo Zhang, Ruijun Xu
- **Comment**: Accepted by ICASSP2020
- **Journal**: None
- **Summary**: The evolution of MobileNets has laid a solid foundation for neural network applications on mobile end. With the latest MobileNetV3, neural architecture search again claimed its supremacy in network design. Unfortunately, till today all mobile methods mainly focus on CPU latencies instead of GPU, the latter, however, is much preferred in practice for it has faster speed, lower overhead and less interference. Bearing the target hardware in mind, we propose the first Mobile GPU-Aware (MoGA) neural architecture search in order to be precisely tailored for real-world applications. Further, the ultimate objective to devise a mobile network lies in achieving better performance by maximizing the utilization of bounded resources. Urging higher capability while restraining time consumption is not reconcilable. We alleviate the tension by weighted evolution techniques. Moreover, we encourage increasing the number of parameters for higher representational power. With 200x fewer GPU days than MnasNet, we obtain a series of models that outperform MobileNetV3 under the similar latency constraints, i.e., MoGA-A achieves 75.9% top-1 accuracy on ImageNet, MoGA-B meets 75.5% which costs only 0.5 ms more on mobile GPU. MoGA-C best attests GPU-awareness by reaching 75.3% and being slower on CPU but faster on GPU.The models and test code is made available here https://github.com/xiaomi-automl/MoGA.



### Detection of the Group of Traffic Signs with Central Slice Theorem
- **Arxiv ID**: http://arxiv.org/abs/1908.04386v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04386v1)
- **Published**: 2019-08-04 11:31:26+00:00
- **Updated**: 2019-08-04 11:31:26+00:00
- **Authors**: Koba Natroshvili
- **Comment**: None
- **Journal**: None
- **Summary**: Our sensor system consists of a combination of Photonic Mixer Device - PMD and Mono optical cameras. Some traffic signs have stripes at 45{deg}. These traffic signs cancel different restrictions on the road. We detect this class of signs with Radon transformation. Here the Radon transformation is calculated using Central Slice Theorem. We approximate the slice of spectrum by the Discrete Cosine Transformation (DCT).



### ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal
- **Arxiv ID**: http://arxiv.org/abs/1908.01323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01323v1)
- **Published**: 2019-08-04 12:18:12+00:00
- **Updated**: 2019-08-04 12:18:12+00:00
- **Authors**: Bin Ding, Chengjiang Long, Ling Zhang, Chunxia Xiao
- **Comment**: The paper was accepted to the IEEE / CVF International Conference on
  Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: In this paper we propose an attentive recurrent generative adversarial network (ARGAN) to detect and remove shadows in an image. The generator consists of multiple progressive steps. At each step a shadow attention detector is firstly exploited to generate an attention map which specifies shadow regions in the input image.Given the attention map, a negative residual by a shadow remover encoder will recover a shadow-lighter or even a shadow-free image. A discriminator is designed to classify whether the output image in the last progressive step is real or fake. Moreover, ARGAN is suitable to be trained with a semi-supervised strategy to make full use of sufficient unsupervised data. The experiments on four public datasets have demonstrated that our ARGAN is robust to detect both simple and complex shadows and to produce more realistic shadow removal results. It outperforms the state-of-the-art methods, especially in detail of recovering shadow areas.



### SF-Net: Structured Feature Network for Continuous Sign Language Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.01341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01341v1)
- **Published**: 2019-08-04 13:34:41+00:00
- **Updated**: 2019-08-04 13:34:41+00:00
- **Authors**: Zhaoyang Yang, Zhenmei Shi, Xiaoyong Shen, Yu-Wing Tai
- **Comment**: 12 pages, 8 figures
- **Journal**: None
- **Summary**: Continuous sign language recognition (SLR) aims to translate a signing sequence into a sentence. It is very challenging as sign language is rich in vocabulary, while many among them contain similar gestures and motions. Moreover, it is weakly supervised as the alignment of signing glosses is not available. In this paper, we propose Structured Feature Network (SF-Net) to address these challenges by effectively learn multiple levels of semantic information in the data. The proposed SF-Net extracts features in a structured manner and gradually encodes information at the frame level, the gloss level and the sentence level into the feature representation. The proposed SF-Net can be trained end-to-end without the help of other models or pre-training. We tested the proposed SF-Net on two large scale public SLR datasets collected from different continuous SLR scenarios. Results show that the proposed SF-Net clearly outperforms previous sequence level supervision based methods in terms of both accuracy and adaptability.



### Improving IT Support by Enhancing Incident Management Process with Multi-modal Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.01351v1
- **DOI**: None
- **Categories**: **cs.IR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01351v1)
- **Published**: 2019-08-04 14:27:11+00:00
- **Updated**: 2019-08-04 14:27:11+00:00
- **Authors**: Atri Mandal, Shivali Agarwal, Nikhil Malhotra, Giriprasad Sridhara, Anupama Ray, Daivik Swarup
- **Comment**: None
- **Journal**: None
- **Summary**: IT support services industry is going through a major transformation with AI becoming commonplace. There has been a lot of effort in the direction of automation at every human touchpoint in the IT support processes. Incident management is one such process which has been a beacon process for AI based automation. The vision is to automate the process from the time an incident/ticket arrives till it is resolved and closed. While text is the primary mode of communicating the incidents, there has been a growing trend of using alternate modalities like image to communicate the problem. A large fraction of IT support tickets today contain attached image data in the form of screenshots, log messages, invoices and so on. These attachments help in better explanation of the problem which aids in faster resolution. Anybody who aspires to provide AI based IT support, it is essential to build systems which can handle multi-modal content. In this paper we present how incident management in IT support domain can be made much more effective using multi-modal analysis. The information extracted from different modalities are correlated to enrich the information in the ticket and used for better ticket routing and resolution. We evaluate our system using about 25000 real tickets containing attachments from selected problem areas. Our results demonstrate significant improvements in both routing and resolution with the use of multi-modal ticket analysis compared to only text based analysis.



### Unsupervised Learning of Depth and Deep Representation for Visual Odometry from Monocular Videos in a Metric Space
- **Arxiv ID**: http://arxiv.org/abs/1908.01367v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01367v1)
- **Published**: 2019-08-04 15:48:31+00:00
- **Updated**: 2019-08-04 15:48:31+00:00
- **Authors**: Xiaochuan Yin, Chengju Liu
- **Comment**: None
- **Journal**: None
- **Summary**: For ego-motion estimation, the feature representation of the scenes is crucial. Previous methods indicate that both the low-level and semantic feature-based methods can achieve promising results. Therefore, the incorporation of hierarchical feature representation may benefit from both methods. From this perspective, we propose a novel direct feature odometry framework, named DFO, for depth estimation and hierarchical feature representation learning from monocular videos. By exploiting the metric distance, our framework is able to learn the hierarchical feature representation without supervision. The pose is obtained with a coarse-to-fine approach from high-level to low-level features in enlarged feature maps. The pixel-level attention mask can be self-learned to provide the prior information. In contrast to the previous methods, our proposed method calculates the camera motion with a direct method rather than regressing the ego-motion from the pose network. With this approach, the consistency of the scale factor of translation can be constrained. Additionally, the proposed method is thus compatible with the traditional SLAM pipeline. Experiments on the KITTI dataset demonstrate the effectiveness of our method.



### Unsupervised Microvascular Image Segmentation Using an Active Contours Mimicking Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1908.01373v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01373v2)
- **Published**: 2019-08-04 16:36:58+00:00
- **Updated**: 2019-08-16 14:41:48+00:00
- **Authors**: Shir Gur, Lior Wolf, Lior Golgher, Pablo Blinder
- **Comment**: None
- **Journal**: None
- **Summary**: The task of blood vessel segmentation in microscopy images is crucial for many diagnostic and research applications. However, vessels can look vastly different, depending on the transient imaging conditions, and collecting data for supervised training is laborious. We present a novel deep learning method for unsupervised segmentation of blood vessels. The method is inspired by the field of active contours and we introduce a new loss term, which is based on the morphological Active Contours Without Edges (ACWE) optimization method. The role of the morphological operators is played by novel pooling layers that are incorporated to the network's architecture. We demonstrate the challenges that are faced by previous supervised learning solutions, when the imaging conditions shift. Our unsupervised method is able to outperform such previous methods in both the labeled dataset, and when applied to similar but different datasets. Our code, as well as efficient PyTorch reimplementations of the baseline methods VesselNN and DeepVess is available on GitHub - https://github.com/shirgur/UMIS.



### Image-Guided Depth Sampling and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1908.01379v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.01379v1)
- **Published**: 2019-08-04 17:55:47+00:00
- **Updated**: 2019-08-04 17:55:47+00:00
- **Authors**: Adam Wolff, Shachar Praisler, Ilya Tcenov, Guy Gilboa
- **Comment**: None
- **Journal**: None
- **Summary**: Depth acquisition, based on active illumination, is essential for autonomous and robotic navigation. LiDARs (Light Detection And Ranging) with mechanical, fixed, sampling templates are commonly used in today's autonomous vehicles. An emerging technology, based on solid-state depth sensors, with no mechanical parts, allows fast, adaptive, programmable scans.   In this paper, we investigate the topic of adaptive, image-driven, sampling and reconstruction strategies. First, we formulate a piece-wise linear depth model with several tolerance parameters and estimate its validity for indoor and outdoor scenes. Our model and experiments predict that, in the optimal case, about 20-60 piece-wise linear structures can approximate well a depth map. This translates to a depth-to-image sampling ratio of about 1/1200. We propose a simple, generic, sampling and reconstruction algorithm, based on super-pixels. We reach a sampling rate which is still far from the optimal case. However, our sampling improves grid and random sampling, consistently, for a wide variety of reconstruction methods. Moreover, our proposed reconstruction achieves state-of-the-art results, compared to image-guided depth completion algorithms, reducing the required sampling rate by a factor of 3-4. A single-pixel depth camera built in our lab illustrates the concept.



### Simultaneous Clustering and Optimization for Evolving Datasets
- **Arxiv ID**: http://arxiv.org/abs/1908.01384v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.01384v1)
- **Published**: 2019-08-04 18:45:42+00:00
- **Updated**: 2019-08-04 18:45:42+00:00
- **Authors**: Yawei Zhao, En Zhu, Xinwang Liu, Chang Tang, Deke Guo, Jianping Yin
- **Comment**: Accepted by IEEE Transactions on Knowledge and Data Engineering
  (TKDE)
- **Journal**: None
- **Summary**: Simultaneous clustering and optimization (SCO) has recently drawn much attention due to its wide range of practical applications. Many methods have been previously proposed to solve this problem and obtain the optimal model. However, when a dataset evolves over time, those existing methods have to update the model frequently to guarantee accuracy; such updating is computationally infeasible. In this paper, we propose a new formulation of SCO to handle evolving datasets. Specifically, we propose a new variant of the alternating direction method of multipliers (ADMM) to solve this problem efficiently. The guarantee of model accuracy is analyzed theoretically for two specific tasks: ridge regression and convex clustering. Extensive empirical studies confirm the effectiveness of our method.



### Deep Neural Network for Semantic-based Text Recognition in Images
- **Arxiv ID**: http://arxiv.org/abs/1908.01403v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.01403v3)
- **Published**: 2019-08-04 21:32:31+00:00
- **Updated**: 2019-12-09 19:46:11+00:00
- **Authors**: Yi Zheng, Qitong Wang, Margrit Betke
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art text spotting systems typically aim to detect isolated words or word-by-word text in images of natural scenes and ignore the semantic coherence within a region of text. However, when interpreted together, seemingly isolated words may be easier to recognize. On this basis, we propose a novel "semantic-based text recognition" (STR) deep learning model that reads text in images with the help of understanding context. STR consists of several modules. We introduce the Text Grouping and Arranging (TGA) algorithm to connect and order isolated text regions. A text-recognition network interprets isolated words. Benefiting from semantic information, a sequenceto-sequence network model efficiently corrects inaccurate and uncertain phrases produced earlier in the STR pipeline. We present experiments on two new distinct datasets that contain scanned catalog images of interior designs and photographs of protesters with hand-written signs, respectively. Our results show that our STR model outperforms a baseline method that uses state-of-the-art single-wordrecognition techniques on both datasets. STR yields a high accuracy rate of 90% on the catalog images and 71% on the more difficult protest images, suggesting its generality in recognizing text.



