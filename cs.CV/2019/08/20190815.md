# Arxiv Papers in cs.CV on 2019-08-15
### Progressive Cross-camera Soft-label Learning for Semi-supervised Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.05669v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05669v2)
- **Published**: 2019-08-15 00:19:02+00:00
- **Updated**: 2020-03-24 15:34:39+00:00
- **Authors**: Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)
- **Journal**: None
- **Summary**: In this paper, we focus on the semi-supervised person re-identification (Re-ID) case, which only has the intra-camera (within-camera) labels but not inter-camera (cross-camera) labels. In real-world applications, these intra-camera labels can be readily captured by tracking algorithms or few manual annotations, when compared with cross-camera labels. In this case, it is very difficult to explore the relationships between cross-camera persons in the training stage due to the lack of cross-camera label information. To deal with this issue, we propose a novel Progressive Cross-camera Soft-label Learning (PCSL) framework for the semi-supervised person Re-ID task, which can generate cross-camera soft-labels and utilize them to optimize the network. Concretely, we calculate an affinity matrix based on person-level features and adapt them to produce the similarities between cross-camera persons (i.e., cross-camera soft-labels). To exploit these soft-labels to train the network, we investigate the weighted cross-entropy loss and the weighted triplet loss from the classification and discrimination perspectives, respectively. Particularly, the proposed framework alternately generates progressive cross-camera soft-labels and gradually improves feature representations in the whole learning course. Extensive experiments on five large-scale benchmark datasets show that PCSL significantly outperforms the state-of-the-art unsupervised methods that employ labeled source domains or the images generated by the GAN-based models. Furthermore, the proposed method even has a competitive performance with respect to deep supervised Re-ID methods.



### SFSegNet: Parse Freehand Sketches using Deep Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.05389v1
- **DOI**: 10.1109/IJCNN.2019.8851974
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05389v1)
- **Published**: 2019-08-15 01:35:36+00:00
- **Updated**: 2019-08-15 01:35:36+00:00
- **Authors**: Junkun Jiang, Ruomei Wang, Shujin Lin, Fei Wang
- **Comment**: Accepted for the 2019 International Joint Conference on Neural
  Networks (IJCNN-19)
- **Journal**: None
- **Summary**: Parsing sketches via semantic segmentation is attractive but challenging, because (i) free-hand drawings are abstract with large variances in depicting objects due to different drawing styles and skills; (ii) distorting lines drawn on the touchpad make sketches more difficult to be recognized; (iii) the high-performance image segmentation via deep learning technologies needs enormous annotated sketch datasets during the training stage. In this paper, we propose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automatic free-hand sketch segmentation, labeling each sketch in a single object with multiple parts. SFSegNet has an end-to-end network process between the input sketches and the segmentation results, composed of 2 parts: (i) a modified deep Fully Convolutional Network(FCN) using a reweighting strategy to ignore background pixels and classify which part each pixel belongs to; (ii) affine transform encoders that attempt to canonicalize the shaking strokes. We train our network with the dataset that consists of 10,000 annotated sketches, to find an extensively applicable model to segment stokes semantically in one ground truth. Extensive experiments are carried out and segmentation results show that our method outperforms other state-of-the-art networks.



### From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer
- **Arxiv ID**: http://arxiv.org/abs/1908.06473v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.06473v1)
- **Published**: 2019-08-15 03:19:16+00:00
- **Updated**: 2019-08-15 03:19:16+00:00
- **Authors**: Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Zhiguo Cao, Chunhua Shen
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in $[0,+\infty)$ in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until sub-region counts are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and- Conquer Network (S-DCNet). S-DCNet only learns from a closed set but can generalize well to open-set scenarios via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTech Part B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has been made available at: https://github. com/xhp-hust-2018-2011/S-DCNet.



### Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards
- **Arxiv ID**: http://arxiv.org/abs/1908.05407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.05407v1)
- **Published**: 2019-08-15 03:50:18+00:00
- **Updated**: 2019-08-15 03:50:18+00:00
- **Authors**: Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin
- **Comment**: Accepted by ACMMM 2019
- **Journal**: None
- **Summary**: Generating image descriptions in different languages is essential to satisfy users worldwide. However, it is prohibitively expensive to collect large-scale paired image-caption dataset for every target language which is critical for training descent image captioning models. Previous works tackle the unpaired cross-lingual image captioning problem through a pivot language, which is with the help of paired image-caption data in the pivot language and pivot-to-target machine translation models. However, such language-pivoted approach suffers from inaccuracy brought by the pivot-to-target translation, including disfluency and visual irrelevancy errors. In this paper, we propose to generate cross-lingual image captions with self-supervised rewards in the reinforcement learning framework to alleviate these two types of errors. We employ self-supervision from mono-lingual corpus in the target language to provide fluency reward, and propose a multi-level visual semantic matching model to provide both sentence-level and concept-level visual relevancy rewards. We conduct extensive experiments for unpaired cross-lingual image captioning in both English and Chinese respectively on two widely used image caption corpora. The proposed approach achieves significant performance improvement over state-of-the-art methods.



### Multimodal Volume-Aware Detection and Segmentation for Brain Metastases Radiosurgery
- **Arxiv ID**: http://arxiv.org/abs/1908.05418v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05418v1)
- **Published**: 2019-08-15 04:48:53+00:00
- **Updated**: 2019-08-15 04:48:53+00:00
- **Authors**: Szu-Yeu Hu, Wei-Hung Weng, Shao-Lun Lu, Yueh-Hung Cheng, Furen Xiao, Feng-Ming Hsu, Jen-Tang Lu
- **Comment**: Accepted to 2019 MICCAI AIRT
- **Journal**: None
- **Summary**: Stereotactic radiosurgery (SRS), which delivers high doses of irradiation in a single or few shots to small targets, has been a standard of care for brain metastases. While very effective, SRS currently requires manually intensive delineation of tumors. In this work, we present a deep learning approach for automated detection and segmentation of brain metastases using multimodal imaging and ensemble neural networks. In order to address small and multiple brain metastases, we further propose a volume-aware Dice loss which optimizes model performance using the information of lesion size. This work surpasses current benchmark levels and demonstrates a reliable AI-assisted system for SRS treatment planning for multiple brain metastases.



### PS^2-Net: A Locally and Globally Aware Network for Point-Based Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05425v1
- **DOI**: 10.1007/978-3-030-68787-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05425v1)
- **Published**: 2019-08-15 05:35:27+00:00
- **Updated**: 2019-08-15 05:35:27+00:00
- **Authors**: Na Zhao, Tat-Seng Chua, Gim Hee Lee
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present the PS^2-Net -- a locally and globally aware deep learning framework for semantic segmentation on 3D scene-level point clouds. In order to deeply incorporate local structures and global context to support 3D scene segmentation, our network is built on four repeatedly stacked encoders, where each encoder has two basic components: EdgeConv that captures local structures and NetVLAD that models global context. Different from existing start-of-the-art methods for point-based scene semantic segmentation that either violate or do not achieve permutation invariance, our PS^2-Net is designed to be permutation invariant which is an essential property of any deep network used to process unordered point clouds. We further provide theoretical proof to guarantee the permutation invariance property of our network. We perform extensive experiments on two large-scale 3D indoor scene datasets and demonstrate that our PS2-Net is able to achieve state-of-the-art performances as compared to existing approaches.



### Learning Trajectory Dependencies for Human Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1908.05436v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05436v3)
- **Published**: 2019-08-15 06:36:32+00:00
- **Updated**: 2020-07-07 02:05:29+00:00
- **Authors**: Wei Mao, Miaomiao Liu, Mathieu Salzmann, Hongdong Li
- **Comment**: Accepted by ICCV2019(Oral)
- **Journal**: None
- **Summary**: Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction. In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionally-used pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https://github.com/wei-mao-2019/LearnTrajDep



### Bypass Enhancement RGB Stream Model for Pedestrian Action Recognition of Autonomous Vehicles
- **Arxiv ID**: http://arxiv.org/abs/1908.05674v2
- **DOI**: 10.1007/978-981-15-3651-9_2
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05674v2)
- **Published**: 2019-08-15 07:37:14+00:00
- **Updated**: 2019-09-02 07:02:31+00:00
- **Authors**: Dong Cao, Lisha Xu
- **Comment**: Accepted to ACPR 2019 - Workshop on Computer Vision for Modern
  Vehicles
- **Journal**: None
- **Summary**: Pedestrian action recognition and intention prediction is one of the core issues in the field of autonomous driving. In this research field, action recognition is one of the key technologies. A large number of scholars have done a lot of work to im-prove the accuracy of the algorithm for the task. However, there are relatively few studies and improvements in the computational complexity of algorithms and sys-tem real-time. In the autonomous driving application scenario, the real-time per-formance and ultra-low latency of the algorithm are extremely important evalua-tion indicators, which are directly related to the availability and safety of the au-tonomous driving system. To this end, we construct a bypass enhanced RGB flow model, which combines the previous two-branch algorithm to extract RGB feature information and optical flow feature information respectively. In the train-ing phase, the two branches are merged by distillation method, and the bypass enhancement is combined in the inference phase to ensure accuracy. The real-time behavior of the behavior recognition algorithm is significantly improved on the premise that the accuracy does not decrease. Experiments confirm the superiority and effectiveness of our algorithm.



### Accelerated CNN Training Through Gradient Approximation
- **Arxiv ID**: http://arxiv.org/abs/1908.05460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05460v1)
- **Published**: 2019-08-15 09:11:31+00:00
- **Updated**: 2019-08-15 09:11:31+00:00
- **Authors**: Ziheng Wang, Sree Harsha Nelaturu
- **Comment**: An abridged version was presented at EMC^2 : Workshop On Energy
  Efficient Machine Learning And Cognitive Computing For Embedded Applications
  at ISCA 2019
- **Journal**: None
- **Summary**: Training deep convolutional neural networks such as VGG and ResNet by gradient descent is an expensive exercise requiring specialized hardware such as GPUs. Recent works have examined the possibility of approximating the gradient computation while maintaining the same convergence properties. While promising, the approximations only work on relatively small datasets such as MNIST. They also fail to achieve real wall-clock speedups due to lack of efficient GPU implementations of the proposed approximation methods. In this work, we explore three alternative methods to approximate gradients, with an efficient GPU kernel implementation for one of them. We achieve wall-clock speedup with ResNet-20 and VGG-19 on the CIFAR-10 dataset upwards of 7%, with a minimal loss in validation accuracy.



### Automated Rib Fracture Detection of Postmortem Computed Tomography Images Using Machine Learning Techniques
- **Arxiv ID**: http://arxiv.org/abs/1908.05467v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05467v1)
- **Published**: 2019-08-15 09:37:42+00:00
- **Updated**: 2019-08-15 09:37:42+00:00
- **Authors**: Samuel Gunz, Svenja Erne, Eric J. Rawdon, Garyfalia Ampanozi, Till Sieberth, Raffael Affolter, Lars C. Ebert, Akos Dobay
- **Comment**: 12 pages, 5 figures
- **Journal**: None
- **Summary**: Imaging techniques is widely used for medical diagnostics. This leads in some cases to a real bottleneck when there is a lack of medical practitioners and the images have to be manually processed. In such a situation there is a need to reduce the amount of manual work by automating part of the analysis. In this article, we investigate the potential of a machine learning algorithm for medical image processing by computing a topological invariant classifier. First, we select retrospectively from our database of postmortem computed tomography images of rib fractures. The images are prepared by applying a rib unfolding tool that flattens the rib cage to form a two-dimensional projection. We compare the results of our analysis with two independent convolutional neural network models. In the case of the neural network model, we obtain an $F_1$ Score of 0.73. To access the performance of our classifier, we compute the relative proportion of images that were not shared between the two classes. We obtain a precision of 0.60 for the images with rib fractures.



### Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems
- **Arxiv ID**: http://arxiv.org/abs/1908.05480v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05480v1)
- **Published**: 2019-08-15 10:27:32+00:00
- **Updated**: 2019-08-15 10:27:32+00:00
- **Authors**: Anna Kuzina, Evgenii Egorov, Evgeny Burnaev
- **Comment**: 24 page, 6 figures, 6 tabels
- **Journal**: None
- **Summary**: Automatic segmentation methods based on deep learning have recently demonstrated state-of-the-art performance, outperforming the ordinary methods. Nevertheless, these methods are inapplicable for small datasets, which are very common in medical problems. To this end, we propose a knowledge transfer method between diseases via the Generative Bayesian Prior network. Our approach is compared to a pre-train approach and random initialization and obtains the best results in terms of Dice Similarity Coefficient metric for the small subsets of the Brain Tumor Segmentation 2018 database (BRATS2018).



### Improved Mix-up with KL-Entropy for Learning From Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1908.05488v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05488v2)
- **Published**: 2019-08-15 10:55:37+00:00
- **Updated**: 2019-08-16 14:36:56+00:00
- **Authors**: Qian Zhang, Feifei Lee, Ya-Gang Wang, Qiu Chen
- **Comment**: The research in this paper we think is not enough to publish, so we
  will continue to research it. Due to we need to add more experiments and
  change the whole structure of this paper, it will spend a lot of time. We
  want to publish our research after all the change have been done, so we apply
  to withdraw this version
- **Journal**: None
- **Summary**: Despite the deep neural networks (DNN) has achieved excellent performance in image classification researches, the training of DNNs needs a large of clean data with accurate annotations. The collect of a dataset is easy, but it is difficult to annotate the collecting data. On the websites, there exist a lot of image data which contains inaccurate annotations, but training on these datasets may make networks easier to over-fit the noisy labels and cause performance degradation. In this work, we propose an improved joint optimization framework, which mixed the mix-up entropy and Kullback-Leibler (KL) entropy as the loss function. The new loss function can give the better fine-tuning after the framework updates both the label annotations. We conduct experiments on CIFAR-10 dataset and Clothing1M dataset. The result shows the advantageous performance of our approach compared with other state-of-the-art methods.



### Deep learning for Plankton and Coral Classification
- **Arxiv ID**: http://arxiv.org/abs/1908.05489v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05489v2)
- **Published**: 2019-08-15 10:57:40+00:00
- **Updated**: 2019-12-10 00:16:38+00:00
- **Authors**: Alessandra Lumini, Loris Nanni, Gianluca Maguolo
- **Comment**: None
- **Journal**: None
- **Summary**: Oceans are the essential lifeblood of the Earth: they provide over 70% of the oxygen and over 97% of the water. Plankton and corals are two of the most fundamental components of ocean ecosystems, the former due to their function at many levels of the oceans food chain, the latter because they provide spawning and nursery grounds to many fish populations. Studying and monitoring plankton distribution and coral reefs is vital for environment protection. In the last years there has been a massive proliferation of digital imagery for the monitoring of underwater ecosystems and much research is concentrated on the automated recognition of plankton and corals. In this paper, we present a study about an automated system for monitoring of underwater ecosystems. The system here proposed is based on the fusion of different deep learning methods. We study how to create an ensemble based of different CNN models, fine tuned on several datasets with the aim of exploiting their diversity. The aim of our study is to experiment the possibility of fine-tuning pretrained CNN for underwater imagery analysis, the opportunity of using different datasets for pretraining models, the possibility to design an ensemble using the same architecture with small variations in the training procedure. The experimental results are very encouraging, our experiments performed on 5 well-knowns datasets (3 plankton and 2 coral datasets) show that the fusion of such different CNN models in a heterogeneous ensemble grants a substantial performance improvement with respect to other state-of-the-art approaches in all the tested problems. One of the main contributions of this work is a wide experimental evaluation of famous CNN architectures to report performance of both single CNN and ensemble of CNNs in different problems. Moreover, we show how to create an ensemble which improves the performance of the best single model.



### Foveated image processing for faster object detection and recognition in embedded systems using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1908.09000v1
- **DOI**: 10.1007/978-3-030-24741-6_17
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09000v1)
- **Published**: 2019-08-15 11:32:48+00:00
- **Updated**: 2019-08-15 11:32:48+00:00
- **Authors**: Uziel Jaramillo-Avila, Sean R. Anderson
- **Comment**: None
- **Journal**: Biomimetic and Biohybrid Systems (2019) 193--204
- **Summary**: Object detection and recognition algorithms using deep convolutional neural networks (CNNs) tend to be computationally intensive to implement. This presents a particular challenge for embedded systems, such as mobile robots, where the computational resources tend to be far less than for workstations. As an alternative to standard, uniformly sampled images, we propose the use of foveated image sampling here to reduce the size of images, which are faster to process in a CNN due to the reduced number of convolution operations. We evaluate object detection and recognition on the Microsoft COCO database, using foveated image sampling at different image sizes, ranging from 416x416 to 96x96 pixels, on an embedded GPU -- an NVIDIA Jetson TX2 with 256 CUDA cores. The results show that it is possible to achieve a 4x speed-up in frame rates, from 3.59 FPS to 15.24 FPS, using 416x416 and 128x128 pixel images respectively. For foveated sampling, this image size reduction led to just a small decrease in recall performance in the foveal region, to 92.0% of the baseline performance with full-sized images, compared to a significant decrease to 50.1% of baseline recall performance in uniformly sampled images, demonstrating the advantage of foveated sampling.



### A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning
- **Arxiv ID**: http://arxiv.org/abs/1908.05498v1
- **DOI**: 10.1145/3343031.3350988
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05498v1)
- **Published**: 2019-08-15 11:36:52+00:00
- **Updated**: 2019-08-15 11:36:52+00:00
- **Authors**: Pengfei Wang, Chengquan Zhang, Fei Qi, Zuming Huang, Mengyi En, Junyu Han, Jingtuo Liu, Errui Ding, Guangming Shi
- **Comment**: 9 pages, 6 figures, 7 tables, To appear in ACM Multimedia 2019
- **Journal**: In Proceedings of the 27th ACM International Conference on
  Multimedia (MM '19), October 21-25, 2019, Nice, France
- **Summary**: Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.



### To complete or to estimate, that is the question: A Multi-Task Approach to Depth Completion and Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.05540v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05540v1)
- **Published**: 2019-08-15 13:50:50+00:00
- **Updated**: 2019-08-15 13:50:50+00:00
- **Authors**: Amir Atapour-Abarghouei, Toby P. Breckon
- **Comment**: International Conference on 3D Vision (3DV) 2019
- **Journal**: None
- **Summary**: Robust three-dimensional scene understanding is now an ever-growing area of research highly relevant in many real-world applications such as autonomous driving and robotic navigation. In this paper, we propose a multi-task learning-based model capable of performing two tasks:- sparse depth completion (i.e. generating complete dense scene depth given a sparse depth image as the input) and monocular depth estimation (i.e. predicting scene depth from a single RGB image) via two sub-networks jointly trained end to end using data randomly sampled from a publicly available corpus of synthetic and real-world images. The first sub-network generates a sparse depth image by learning lower level features from the scene and the second predicts a full dense depth image of the entire scene, leading to a better geometric and contextual understanding of the scene and, as a result, superior performance of the approach. The entire model can be used to infer complete scene depth from a single RGB image or the second network can be used alone to perform depth completion given a sparse depth input. Using adversarial training, a robust objective function, a deep architecture relying on skip connections and a blend of synthetic and real-world training data, our approach is capable of producing superior high quality scene depth. Extensive experimental evaluation demonstrates the efficacy of our approach compared to contemporary state-of-the-art techniques across both problem domains.



### Beyond Cartesian Representations for Local Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1908.05547v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05547v1)
- **Published**: 2019-08-15 13:56:40+00:00
- **Updated**: 2019-08-15 13:56:40+00:00
- **Authors**: Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, Eduard Trulls
- **Comment**: None
- **Journal**: None
- **Summary**: The dominant approach for learning local patch descriptors relies on small image regions whose scale must be properly estimated a priori by a keypoint detector. In other words, if two patches are not in correspondence, their descriptors will not match. A strategy often used to alleviate this problem is to "pool" the pixel-wise features over log-polar regions, rather than regularly spaced ones. By contrast, we propose to extract the "support region" directly with a log-polar sampling scheme. We show that this provides us with a better representation by simultaneously oversampling the immediate neighbourhood of the point and undersampling regions far away from it. We demonstrate that this representation is particularly amenable to learning descriptors with deep networks. Our models can match descriptors across a much wider range of scales than was possible before, and also leverage much larger support regions without suffering from occlusions. We report state-of-the-art results on three different datasets.



### Entropic Out-of-Distribution Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.05569v13
- **DOI**: 10.1109/IJCNN52387.2021.9533899
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05569v13)
- **Published**: 2019-08-15 14:54:52+00:00
- **Updated**: 2021-05-24 23:15:23+00:00
- **Authors**: David Macêdo, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I. Oliveira, Teresa Ludermir
- **Comment**: Accepted for publication in The International Joint Conference on
  Neural Networks (IJCNN), 2021
- **Journal**: None
- **Summary**: Out-of-distribution (OOD) detection approaches usually present special requirements (e.g., hyperparameter validation, collection of outlier data) and produce side effects (e.g., classification accuracy drop, slower energy-inefficient inferences). We argue that these issues are a consequence of the SoftMax loss anisotropy and disagreement with the maximum entropy principle. Thus, we propose the IsoMax loss and the entropic score. The seamless drop-in replacement of the SoftMax loss by IsoMax loss requires neither additional data collection nor hyperparameter validation. The trained models do not exhibit classification accuracy drop and produce fast energy-efficient inferences. Moreover, our experiments show that training neural networks with IsoMax loss significantly improves their OOD detection performance. The IsoMax loss exhibits state-of-the-art performance under the mentioned conditions (fast energy-efficient inference, no classification accuracy drop, no collection of outlier data, and no hyperparameter validation), which we call the seamless OOD detection task. In future work, current OOD detection methods may replace the SoftMax loss with the IsoMax loss to improve their performance on the commonly studied non-seamless OOD detection problem.



### FastPose: Towards Real-time Pose Estimation and Tracking via Scale-normalized Multi-task Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.05593v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05593v1)
- **Published**: 2019-08-15 15:42:57+00:00
- **Updated**: 2019-08-15 15:42:57+00:00
- **Authors**: Jiabin Zhang, Zheng Zhu, Wei Zou, Peng Li, Yanwei Li, Hu Su, Guan Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Both accuracy and efficiency are significant for pose estimation and tracking in videos. State-of-the-art performance is dominated by two-stages top-down methods. Despite the leading results, these methods are impractical for real-world applications due to their separated architectures and complicated calculation. This paper addresses the task of articulated multi-person pose estimation and tracking towards real-time speed. An end-to-end multi-task network (MTN) is designed to perform human detection, pose estimation, and person re-identification (Re-ID) tasks simultaneously. To alleviate the performance bottleneck caused by scale variation problem, a paradigm which exploits scale-normalized image and feature pyramids (SIFP) is proposed to boost both performance and speed. Given the results of MTN, we adopt an occlusion-aware Re-ID feature strategy in the pose tracking module, where pose information is utilized to infer the occlusion state to make better use of Re-ID feature. In experiments, we demonstrate that the pose estimation and tracking performance improves steadily utilizing SIFP through different backbones. Using ResNet-18 and ResNet-50 as backbones, the overall pose tracking framework achieves competitive performance with 29.4 FPS and 12.2 FPS, respectively. Additionally, occlusion-aware Re-ID feature decreases the identification switches by 37% in the pose tracking process.



### Resolving challenges in deep learning-based analyses of histopathological images using explanation methods
- **Arxiv ID**: http://arxiv.org/abs/1908.06943v2
- **DOI**: 10.1038/s41598-020-62724-2
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1908.06943v2)
- **Published**: 2019-08-15 15:46:40+00:00
- **Updated**: 2020-04-24 15:13:00+00:00
- **Authors**: Miriam Hägele, Philipp Seegerer, Sebastian Lapuschkin, Michael Bockmayr, Wojciech Samek, Frederick Klauschen, Klaus-Robert Müller, Alexander Binder
- **Comment**: None
- **Journal**: Sci Rep 10, 6423 (2020)
- **Summary**: Deep learning has recently gained popularity in digital pathology due to its high prediction quality. However, the medical domain requires explanation and insight for a better understanding beyond standard quantitative performance evaluation. Recently, explanation methods have emerged, which are so far still rarely used in medicine. This work shows their application to generate heatmaps that allow to resolve common challenges encountered in deep learning-based digital histopathology analyses. These challenges comprise biases typically inherent to histopathology data. We study binary classification tasks of tumor tissue discrimination in publicly available haematoxylin and eosin slides of various tumor entities and investigate three types of biases: (1) biases which affect the entire dataset, (2) biases which are by chance correlated with class labels and (3) sampling biases. While standard analyses focus on patch-level evaluation, we advocate pixel-wise heatmaps, which offer a more precise and versatile diagnostic instrument and furthermore help to reveal biases in the data. This insight is shown to not only detect but also to be helpful to remove the effects of common hidden biases, which improves generalization within and across datasets. For example, we could see a trend of improved area under the receiver operating characteristic curve by 5% when reducing a labeling bias. Explanation techniques are thus demonstrated to be a helpful and highly relevant tool for the development and the deployment phases within the life cycle of real-world applications in digital pathology.



### Deep Slice Interpolation via Marginal Super-Resolution, Fusion and Refinement
- **Arxiv ID**: http://arxiv.org/abs/1908.05599v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05599v1)
- **Published**: 2019-08-15 15:48:54+00:00
- **Updated**: 2019-08-15 15:48:54+00:00
- **Authors**: Cheng Peng, Wei-An Lin, Haofu Liao, Rama Chellappa, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a marginal super-resolution (MSR) approach based on 2D convolutional neural networks (CNNs) for interpolating an anisotropic brain magnetic resonance scan along the highly under-sampled direction, which is assumed to axial without loss of generality. Previous methods for slice interpolation only consider data from pairs of adjacent 2D slices. The possibility of fusing information from the direction orthogonal to the 2D slices remains unexplored. Our approach performs MSR in both sagittal and coronal directions, which provides an initial estimate for slice interpolation. The interpolated slices are then fused and refined in the axial direction for improved consistency. Since MSR consists of only 2D operations, it is more feasible in terms of GPU memory consumption and requires fewer training samples compared to 3D CNNs. Our experiments demonstrate that the proposed method outperforms traditional linear interpolation and baseline 2D/3D CNN-based approaches. We conclude by showcasing the method's practical utility in estimating brain volumes from under-sampled brain MR scans through semantic segmentation.



### R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object
- **Arxiv ID**: http://arxiv.org/abs/1908.05612v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05612v6)
- **Published**: 2019-08-15 15:56:37+00:00
- **Updated**: 2020-12-08 05:52:52+00:00
- **Authors**: Xue Yang, Junchi Yan, Ziming Feng, Tao He
- **Comment**: 13 pages, 12 figures, 9 tables
- **Journal**: Thirty-Five AAAI Conference on Artificial Intelligence (AAAI2021)
- **Summary**: Rotation detection is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. Though considerable progress has been made, for practical settings, there still exist challenges for rotating objects with large aspect ratio, dense distribution and category extremely imbalance. In this paper, we propose an end-to-end refined single-stage rotation detector for fast and accurate object detection by using a progressive regression approach from coarse to fine granularity. Considering the shortcoming of feature misalignment in existing refined single-stage detector, we design a feature refinement module to improve detection performance by getting more accurate features. The key idea of feature refinement module is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to realize feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. Experiments on three popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the effectiveness of our approach. Tensorflow and Pytorch version codes are available at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and https://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection, and R3Det is also integrated in our open source rotation detection benchmark: https://github.com/yangxue0827/RotationDetection.



### Towards multi-sequence MR image recovery from undersampled k-space data
- **Arxiv ID**: http://arxiv.org/abs/1908.05615v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05615v2)
- **Published**: 2019-08-15 16:01:47+00:00
- **Updated**: 2019-08-16 00:35:57+00:00
- **Authors**: Cheng Peng, Wei-An Lin, Rama Chellappa, S. Kevin Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Undersampled MR image recovery has been widely studied for accelerated MR acquisition. However, it has been mostly studied under a single sequence scenario, despite the fact that multi-sequence MR scan is common in practice. In this paper, we aim to optimize multi-sequence MR image recovery from undersampled k-space data under an overall time constraint while considering the difference in acquisition time for various sequences. We first formulate it as a constrained optimization problem and then show that finding the optimal sampling strategy for all sequences and the best recovery model at the same time is combinatorial and hence computationally prohibitive. To solve this problem, we propose a blind recovery model that simultaneously recovers multiple sequences, and an efficient approach to find proper combination of sampling strategy and recovery model. Our experiments demonstrate that the proposed method outperforms sequence-wise recovery, and sheds light on how to decide the undersampling strategy for sequences within an overall time budget.



### A deep learning model for segmentation of geographic atrophy to study its long-term natural history
- **Arxiv ID**: http://arxiv.org/abs/1908.05621v1
- **DOI**: 10.1016/j.ophtha.2020.02.009
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05621v1)
- **Published**: 2019-08-15 16:12:52+00:00
- **Updated**: 2019-08-15 16:12:52+00:00
- **Authors**: Bart Liefers, Johanna M. Colijn, Cristina González-Gonzalo, Timo Verzijden, Paul Mitchell, Carel B. Hoyng, Bram van Ginneken, Caroline C. W. Klaver, Clara I. Sánchez
- **Comment**: 22 pages, 3 tables, 4 figures, 1 supplemental figure
- **Journal**: Ophthalmology, Published February 14, 2020
- **Summary**: Purpose: To develop and validate a deep learning model for automatic segmentation of geographic atrophy (GA) in color fundus images (CFIs) and its application to study growth rate of GA. Participants: 409 CFIs of 238 eyes with GA from the Rotterdam Study (RS) and the Blue Mountain Eye Study (BMES) for model development, and 5,379 CFIs of 625 eyes from the Age-Related Eye Disease Study (AREDS) for analysis of GA growth rate. Methods: A deep learning model based on an ensemble of encoder-decoder architectures was implemented and optimized for the segmentation of GA in CFIs. Four experienced graders delineated GA in CFIs from RS and BMES. These manual delineations were used to evaluate the segmentation model using 5-fold cross-validation. The model was further applied to CFIs from the AREDS to study the growth rate of GA. Linear regression analysis was used to study associations between structural biomarkers at baseline and GA growth rate. A general estimate of the progression of GA area over time was made by combining growth rates of all eyes with GA from the AREDS set. Results: The model obtained an average Dice coefficient of 0.72 $\pm$ 0.26 on the BMES and RS. An intraclass correlation coefficient of 0.83 was reached between the automatically estimated GA area and the graders' consensus measures. Eight automatically calculated structural biomarkers (area, filled area, convex area, convex solidity, eccentricity, roundness, foveal involvement and perimeter) were significantly associated with growth rate. Combining all growth rates indicated that GA area grows quadratically up to an area of around 12 mm$^{2}$, after which growth rate stabilizes or decreases. Conclusion: The presented deep learning model allowed for fully automatic and robust segmentation of GA in CFIs. These segmentations can be used to extract structural characteristics of GA that predict its growth rate.



### IoU-balanced Loss Functions for Single-stage Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.05641v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05641v2)
- **Published**: 2019-08-15 17:08:22+00:00
- **Updated**: 2020-12-14 08:07:55+00:00
- **Authors**: Shengkai Wu, Jinrong Yang, Xinggang Wang, Xiaoping Li
- **Comment**: None
- **Journal**: None
- **Summary**: Single-stage object detectors have been widely applied in computer vision applications due to their high efficiency. However, we find that the loss functions adopted by single-stage object detectors hurt the localization accuracy seriously. Firstly, the standard cross-entropy loss for classification is independent of the localization task and drives all the positive examples to learn as high classification scores as possible regardless of localization accuracy during training. As a result, there will be many detections that have high classification scores but low IoU or detections that have low classification scores but high IoU. Secondly, for the standard smooth L1 loss, the gradient is dominated by the outliers that have poor localization accuracy during training. The above two problems will decrease the localization accuracy of single-stage detectors. In this work, IoU-balanced loss functions that consist of IoU-balanced classification loss and IoU-balanced localization loss are proposed to solve the above problems. The IoU-balanced classification loss pays more attention to positive examples with high IoU and can enhance the correlation between classification and localization tasks. The IoU-balanced localization loss decreases the gradient of examples with low IoU and increases the gradient of examples with high IoU, which can improve the localization accuracy of models. Extensive experiments on challenging public datasets such as MS COCO, PASCAL VOC and Cityscapes demonstrate that both IoU-balanced losses can bring substantial improvement for the popular single-stage detectors, especially for the localization accuracy. On COCO test-dev, the proposed methods can substantially improve AP by $1.0\%\sim1.7\%$ and AP75 by $1.0\%\sim2.4\%$. On PASCAL VOC, it can also substantially improve AP by $1.3\%\sim1.5\%$ and AP80, AP90 by $1.6\%\sim3.9\%$.



### A Multimodal Vision Sensor for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1908.05649v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05649v1)
- **Published**: 2019-08-15 17:45:17+00:00
- **Updated**: 2019-08-15 17:45:17+00:00
- **Authors**: Dongming Sun, Xiao Huang, Kailun Yang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a multimodal vision sensor that integrates three types of cameras, including a stereo camera, a polarization camera and a panoramic camera. Each sensor provides a specific dimension of information: the stereo camera measures depth per pixel, the polarization obtains the degree of polarization, and the panoramic camera captures a 360-degree landscape. Data fusion and advanced environment perception could be built upon the combination of sensors. Designed especially for autonomous driving, this vision sensor is shipped with a robust semantic segmentation network. In addition, we demonstrate how cross-modal enhancement could be achieved by registering the color image and the polarization image. An example of water hazard detection is given. To prove the multimodal vision sensor's compatibility with different devices, a brief runtime performance analysis is carried out.



### Semi-Supervised Semantic Segmentation with High- and Low-level Consistency
- **Arxiv ID**: http://arxiv.org/abs/1908.05724v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05724v1)
- **Published**: 2019-08-15 19:32:49+00:00
- **Updated**: 2019-08-15 19:32:49+00:00
- **Authors**: Sudhanshu Mittal, Maxim Tatarchenko, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to understand visual information from limited labeled data is an important aspect of machine learning. While image-level classification has been extensively studied in a semi-supervised setting, dense pixel-level classification with limited data has only drawn attention recently. In this work, we propose an approach for semi-supervised semantic segmentation that learns from limited pixel-wise annotated samples while exploiting additional annotation-free images. It uses two network branches that link semi-supervised classification with semi-supervised segmentation including self-training. The dual-branch approach reduces both the low-level and the high-level artifacts typical when training with few labels. The approach attains significant improvement over existing methods, especially when trained with very few labeled samples. On several standard benchmarks - PASCAL VOC 2012, PASCAL-Context, and Cityscapes - the approach achieves new state-of-the-art in semi-supervised learning.



### DeepHuMS: Deep Human Motion Signature for 3D Skeletal Sequences
- **Arxiv ID**: http://arxiv.org/abs/1908.05750v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05750v3)
- **Published**: 2019-08-15 20:34:22+00:00
- **Updated**: 2019-12-08 04:18:14+00:00
- **Authors**: Neeraj Battan, Abbhinav Venkat, Avinash Sharma
- **Comment**: Under Review, Conference
- **Journal**: None
- **Summary**: 3D Human Motion Indexing and Retrieval is an interesting problem due to the rise of several data-driven applications aimed at analyzing and/or re-utilizing 3D human skeletal data, such as data-driven animation, analysis of sports bio-mechanics, human surveillance etc. Spatio-temporal articulations of humans, noisy/missing data, different speeds of the same motion etc. make it challenging and several of the existing state of the art methods use hand-craft features along with optimization based or histogram based comparison in order to perform retrieval. Further, they demonstrate it only for very small datasets and few classes. We make a case for using a learned representation that should recognize the motion as well as enforce a discriminative ranking. To that end, we propose, a 3D human motion descriptor learned using a deep network. Our learned embedding is generalizable and applicable to real-world data - addressing the aforementioned challenges and further enables sub-motion searching in its embedding space using another network. Our model exploits the inter-class similarity using trajectory cues, and performs far superior in a self-supervised setting. State of the art results on all these fronts is shown on two large scale 3D human motion datasets - NTU RGB+D and HDM05.



### Discretely-constrained deep network for weakly supervised segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05770v1)
- **Published**: 2019-08-15 21:19:40+00:00
- **Updated**: 2019-08-15 21:19:40+00:00
- **Authors**: Jizong Peng, Hoel Kervadec, Jose Dolz, Ismail Ben Ayed, Marco Pedersoli, Christian Desrosiers
- **Comment**: None
- **Journal**: None
- **Summary**: An efficient strategy for weakly-supervised segmentation is to impose constraints or regularization priors on target regions. Recent efforts have focused on incorporating such constraints in the training of convolutional neural networks (CNN), however this has so far been done within a continuous optimization framework. Yet, various segmentation constraints and regularization can be modeled and optimized more efficiently in a discrete formulation. This paper proposes a method, based on the alternating direction method of multipliers (ADMM) algorithm, to train a CNN with discrete constraints and regularization priors. This method is applied to the segmentation of medical images with weak annotations, where both size constraints and boundary length regularization are enforced. Experiments on a benchmark cardiac segmentation dataset show our method to yield a performance near to full supervision.



### MimickNet, Matching Clinical Post-Processing Under Realistic Black-Box Constraints
- **Arxiv ID**: http://arxiv.org/abs/1908.05782v1
- **DOI**: 10.1109/TMI.2020.2970867
- **Categories**: **eess.IV**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05782v1)
- **Published**: 2019-08-15 22:10:41+00:00
- **Updated**: 2019-08-15 22:10:41+00:00
- **Authors**: Ouwen Huang, Will Long, Nick Bottenus, Gregg E. Trahey, Sina Farsiu, Mark L. Palmeri
- **Comment**: This work has been submitted to the IEEE Transactions on Medical
  Imaging on July 1st, 2019 for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible
- **Journal**: None
- **Summary**: Image post-processing is used in clinical-grade ultrasound scanners to improve image quality (e.g., reduce speckle noise and enhance contrast). These post-processing techniques vary across manufacturers and are generally kept proprietary, which presents a challenge for researchers looking to match current clinical-grade workflows. We introduce a deep learning framework, MimickNet, that transforms raw conventional delay-and-summed (DAS) beams into the approximate post-processed images found on clinical-grade scanners. Training MimickNet only requires post-processed image samples from a scanner of interest without the need for explicit pairing to raw DAS data. This flexibility allows it to hypothetically approximate any manufacturer's post-processing without access to the pre-processed data. MimickNet generates images with an average similarity index measurement (SSIM) of 0.930$\pm$0.0892 on a 300 cineloop test set, and it generalizes to cardiac cineloops outside of our train-test distribution achieving an SSIM of 0.967$\pm$0.002. We also explore the theoretical SSIM achievable by evaluating MimickNet performance when trained under gray-box constraints (i.e., when both pre-processed and post-processed images are available). To our knowledge, this is the first work to establish deep learning models that closely approximate current clinical-grade ultrasound post-processing under realistic black-box constraints where before and after post-processing data is unavailable. MimickNet serves as a clinical post-processing baseline for future works in ultrasound image formation to compare against. To this end, we have made the MimickNet software open source.



### TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.05786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05786v1)
- **Published**: 2019-08-15 22:30:50+00:00
- **Updated**: 2019-08-15 22:30:50+00:00
- **Authors**: Kyle Min, Jason J. Corso
- **Comment**: ICCV 2019 camera ready (Supplementary material: on CVF soon)
- **Journal**: None
- **Summary**: TASED-Net is a 3D fully-convolutional network architecture for video saliency detection. It consists of two building blocks: first, the encoder network extracts low-resolution spatiotemporal features from an input clip of several consecutive frames, and then the following prediction network decodes the encoded features spatially while aggregating all the temporal information. As a result, a single prediction map is produced from an input clip of multiple frames. Frame-wise saliency maps can be predicted by applying TASED-Net in a sliding-window fashion to a video. The proposed approach assumes that the saliency map of any frame can be predicted by considering a limited number of past frames. The results of our extensive experiments on video saliency detection validate this assumption and demonstrate that our fully-convolutional model with temporal aggregation method is effective. TASED-Net significantly outperforms previous state-of-the-art approaches on all three major large-scale datasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After analyzing the results qualitatively, we observe that our model is especially better at attending to salient moving objects.



### Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.05794v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05794v1)
- **Published**: 2019-08-15 23:26:59+00:00
- **Updated**: 2019-08-15 23:26:59+00:00
- **Authors**: Mihai Marian Puscas, Dan Xu, Andrea Pilzer, Nicu Sebe
- **Comment**: Accepted at 3DV 2019 as ORAL
- **Journal**: None
- **Summary**: Inspired by the success of adversarial learning, we propose a new end-to-end unsupervised deep learning framework for monocular depth estimation consisting of two Generative Adversarial Networks (GAN), deeply coupled with a structured Conditional Random Field (CRF) model. The two GANs aim at generating distinct and complementary disparity maps and at improving the generation quality via exploiting the adversarial learning strategy. The deep CRF coupling model is proposed to fuse the generative and discriminative outputs from the dual GAN nets. As such, the model implicitly constructs mutual constraints on the two network branches and between the generator and discriminator. This facilitates the optimization of the whole network for better disparity generation. Extensive experiments on the KITTI, Cityscapes, and Make3D datasets clearly demonstrate the effectiveness of the proposed approach and show superior performance compared to state of the art methods. The code and models are available at https://github.com/mihaipuscas/ 3dv---coupled-crf-disparity.



### Deep Sparse Band Selection for Hyperspectral Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.09630v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.09630v1)
- **Published**: 2019-08-15 23:51:33+00:00
- **Updated**: 2019-08-15 23:51:33+00:00
- **Authors**: Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Hyperspectral imaging systems collect and process information from specific wavelengths across the electromagnetic spectrum. The fusion of multi-spectral bands in the visible spectrum has been exploited to improve face recognition performance over all the conventional broad band face images. In this book chapter, we propose a new Convolutional Neural Network (CNN) framework which adopts a structural sparsity learning technique to select the optimal spectral bands to obtain the best face recognition performance over all of the spectral bands. Specifically, in this method, images from all bands are fed to a CNN, and the convolutional filters in the first layer of the CNN are then regularized by employing a group Lasso algorithm to zero out the redundant bands during the training of the network. Contrary to other methods which usually select the useful bands manually or in a greedy fashion, our method selects the optimal spectral bands automatically to achieve the best face recognition performance over all spectral bands. Moreover, experimental results demonstrate that our method outperforms state of the art band selection methods for face recognition on several publicly-available hyperspectral face image datasets.



