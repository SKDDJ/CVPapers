# Arxiv Papers in cs.CV on 2019-08-14
### FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age
- **Arxiv ID**: http://arxiv.org/abs/1908.04913v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04913v1)
- **Published**: 2019-08-14 01:42:41+00:00
- **Updated**: 2019-08-14 01:42:41+00:00
- **Authors**: Kimmo Kärkkäinen, Jungseock Joo
- **Comment**: None
- **Journal**: None
- **Summary**: Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups.



### HorNet: A Hierarchical Offshoot Recurrent Network for Improving Person Re-ID via Image Captioning
- **Arxiv ID**: http://arxiv.org/abs/1908.04915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04915v1)
- **Published**: 2019-08-14 01:44:50+00:00
- **Updated**: 2019-08-14 01:44:50+00:00
- **Authors**: Shiyang Yan, Jun Xu, Yuai Liu, Lin Xu
- **Comment**: 10 pages, 5 figures, published in IJCAI19
- **Journal**: None
- **Summary**: Person re-identification (re-ID) aims to recognize a person-of-interest across different cameras with notable appearance variance. Existing research works focused on the capability and robustness of visual representation. In this paper, instead, we propose a novel hierarchical offshoot recurrent network (HorNet) for improving person re-ID via image captioning. Image captions are semantically richer and more consistent than visual attributes, which could significantly alleviate the variance. We use the similarity preserving generative adversarial network (SPGAN) and an image captioner to fulfill domain transfer and language descriptions generation. Then the proposed HorNet can learn the visual and language representation from both the images and captions jointly, and thus enhance the performance of person re-ID. Extensive experiments are conducted on several benchmark datasets with or without image captions, i.e., CUHK03, Market-1501, and Duke-MTMC, demonstrating the superiority of the proposed method. Our method can generate and extract meaningful image captions while achieving state-of-the-art performance.



### A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading
- **Arxiv ID**: http://arxiv.org/abs/1908.04917v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.04917v2)
- **Published**: 2019-08-14 01:49:32+00:00
- **Updated**: 2019-11-28 01:31:38+00:00
- **Authors**: Ya Zhao, Rui Xu, Mingli Song
- **Comment**: ACM MM Asia 2019
- **Journal**: None
- **Summary**: Lip reading aims at decoding texts from the movement of a speaker's mouth. In recent years, lip reading methods have made great progress for English, at both word-level and sentence-level. Unlike English, however, Chinese Mandarin is a tone-based language and relies on pitches to distinguish lexical or grammatical meaning, which significantly increases the ambiguity for the lip reading task. In this paper, we propose a Cascade Sequence-to-Sequence Model for Chinese Mandarin (CSSMCM) lip reading, which explicitly models tones when predicting sentence. Tones are modeled based on visual information and syntactic structure, and are used to predict sentence along with visual information and syntactic structure. In order to evaluate CSSMCM, a dataset called CMLR (Chinese Mandarin Lip Reading) is collected and released, consisting of over 100,000 natural sentences from China Network Television website. When trained on CMLR dataset, the proposed CSSMCM surpasses the performance of state-of-the-art lip reading frameworks, which confirms the effectiveness of explicit modeling of tones for Chinese Mandarin lip reading.



### Towards Diverse and Accurate Image Captions via Reinforcing Determinantal Point Process
- **Arxiv ID**: http://arxiv.org/abs/1908.04919v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1908.04919v1)
- **Published**: 2019-08-14 01:51:49+00:00
- **Updated**: 2019-08-14 01:51:49+00:00
- **Authors**: Qingzhong Wang, Antoni B. Chan
- **Comment**: 14 pages. Code is comming soon,please pay attention to my personal
  page visal.cs.cityu.edu.hk/people/qingzhong-wang/
- **Journal**: None
- **Summary**: Although significant progress has been made in the field of automatic image captioning, it is still a challenging task. Previous works normally pay much attention to improving the quality of the generated captions but ignore the diversity of captions. In this paper, we combine determinantal point process (DPP) and reinforcement learning (RL) and propose a novel reinforcing DPP (R-DPP) approach to generate a set of captions with high quality and diversity for an image. We show that R-DPP performs better on accuracy and diversity than using noise as a control signal (GANs, VAEs). Moreover, R-DPP is able to preserve the modes of the learned distribution. Hence, beam search algorithm can be applied to generate a single accurate caption, which performs better than other RL-based models.



### 3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents
- **Arxiv ID**: http://arxiv.org/abs/1908.04929v1
- **DOI**: 10.1109/TCYB.2019.2931042
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1908.04929v1)
- **Published**: 2019-08-14 02:30:05+00:00
- **Updated**: 2019-08-14 02:30:05+00:00
- **Authors**: Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song, Jong-Hwan Kim
- **Comment**: Early Access
- **Journal**: None
- **Summary**: Intelligent agents gather information and perceive semantics within the environments before taking on given tasks. The agents store the collected information in the form of environment models that compactly represent the surrounding environments. The agents, however, can only conduct limited tasks without an efficient and effective environment model. Thus, such an environment model takes a crucial role for the autonomy systems of intelligent agents. We claim the following characteristics for a versatile environment model: accuracy, applicability, usability, and scalability. Although a number of researchers have attempted to develop such models that represent environments precisely to a certain degree, they lack broad applicability, intuitive usability, and satisfactory scalability. To tackle these limitations, we propose 3-D scene graph as an environment model and the 3-D scene graph construction framework. The concise and widely used graph structure readily guarantees usability as well as scalability for 3-D scene graph. We demonstrate the accuracy and applicability of the 3-D scene graph by exhibiting the deployment of the 3-D scene graph in practical applications. Moreover, we verify the performance of the proposed 3-D scene graph and the framework by conducting a series of comprehensive experiments under various conditions.



### Generalised Zero-Shot Learning with Domain Classification in a Joint Semantic and Visual Space
- **Arxiv ID**: http://arxiv.org/abs/1908.04930v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04930v1)
- **Published**: 2019-08-14 02:30:51+00:00
- **Updated**: 2019-08-14 02:30:51+00:00
- **Authors**: Rafael Felix, Ben Harwood, Michele Sasdelli, Gustavo Carneiro
- **Comment**: None
- **Journal**: None
- **Summary**: Generalised zero-shot learning (GZSL) is a classification problem where the learning stage relies on a set of seen visual classes and the inference stage aims to identify both the seen visual classes and a new set of unseen visual classes. Critically, both the learning and inference stages can leverage a semantic representation that is available for the seen and unseen classes. Most state-of-the-art GZSL approaches rely on a mapping between latent visual and semantic spaces without considering if a particular sample belongs to the set of seen or unseen classes. In this paper, we propose a novel GZSL method that learns a joint latent representation that combines both visual and semantic information. This mitigates the need for learning a mapping between the two spaces. Our method also introduces a domain classification that estimates whether a sample belongs to a seen or an unseen class. Our classifier then combines a class discriminator with this domain classifier with the goal of reducing the natural bias that GZSL approaches have toward the seen classes. Experiments show that our method achieves state-of-the-art results in terms of harmonic mean, the area under the seen and unseen curve and unseen classification accuracy on public GZSL benchmark data sets. Our code will be available upon acceptance of this paper.



### Skin Lesion Segmentation and Classification for ISIC 2018 by Combining Deep CNN and Handcrafted Features
- **Arxiv ID**: http://arxiv.org/abs/1908.05730v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05730v1)
- **Published**: 2019-08-14 02:48:49+00:00
- **Updated**: 2019-08-14 02:48:49+00:00
- **Authors**: Redha Ali, Russell C. Hardie, Manawaduge Supun De Silva, Temesguen Messay Kebede
- **Comment**: 4 pages and 3 figures
- **Journal**: None
- **Summary**: This short report describes our submission to the ISIC 2018 Challenge in Skin Lesion Analysis Towards Melanoma Detection for Task1 and Task 3. This work has been accomplished by a team of researchers at the University of Dayton Signal and Image Processing Lab. Our proposed approach is computationally efficient are combines information from both deep learning and handcrafted features. For Task3, we form a new type of image features, called hybrid features, which has stronger discrimination ability than single method features. These features are utilized as inputs to a decision-making model that is based on a multiclass Support Vector Machine (SVM) classifier. The proposed technique is evaluated on online validation databases. Our score was 0.841 with SVM classifier on the validation dataset.



### VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1908.04950v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04950v1)
- **Published**: 2019-08-14 04:44:26+00:00
- **Updated**: 2019-08-14 04:44:26+00:00
- **Authors**: Cătălina Cangea, Eugene Belilovsky, Pietro Liò, Aaron Courville
- **Comment**: To appear at BMVC 2019. 15 pages, 5 figures
- **Journal**: None
- **Summary**: Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm.



### Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy
- **Arxiv ID**: http://arxiv.org/abs/1908.04951v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04951v1)
- **Published**: 2019-08-14 04:44:37+00:00
- **Updated**: 2019-08-14 04:44:37+00:00
- **Authors**: Qing Yu, Kiyoharu Aizawa
- **Comment**: None
- **Journal**: ICCV2019
- **Summary**: Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution (OOD) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network (CNN) and maximize the discrepancy between the two classifiers to detect OOD inputs. We train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution (ID) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push OOD samples outside the manifold of the in-distribution (ID) samples, which enables us to detect OOD samples that are far from the support of the ID samples. Overall, our approach significantly outperforms other state-of-the-art methods on several OOD detection benchmarks and two cases of real-world simulation.



### Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks
- **Arxiv ID**: http://arxiv.org/abs/1908.04955v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04955v1)
- **Published**: 2019-08-14 04:58:20+00:00
- **Updated**: 2019-08-14 04:58:20+00:00
- **Authors**: Joseph Campbell, Simon Stepputtis, Heni Ben Amor
- **Comment**: Project website:
  http://interactive-robotics.engineering.asu.edu/interaction-primitives
  Accompanying video: https://youtu.be/r5AqfxTDfLA
- **Journal**: None
- **Summary**: Human-robot interaction benefits greatly from multimodal sensor inputs as they enable increased robustness and generalization accuracy. Despite this observation, few HRI methods are capable of efficiently performing inference for multimodal systems. In this work, we introduce a reformulation of Interaction Primitives which allows for learning from demonstration of interaction tasks, while also gracefully handling nonlinearities inherent to multimodal inference in such scenarios. We also empirically show that our method results in more accurate, more robust, and faster inference than standard Interaction Primitives and other common methods in challenging HRI scenarios.



### Learning Two-View Correspondences and Geometry Using Order-Aware Network
- **Arxiv ID**: http://arxiv.org/abs/1908.04964v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04964v1)
- **Published**: 2019-08-14 05:42:18+00:00
- **Updated**: 2019-08-14 05:42:18+00:00
- **Authors**: Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, Hongen Liao
- **Comment**: Accepted to ICCV 2019, and Winner solution to both tracks of CVPR IMW
  2019 Challenge. Code will be available soon at
  https://github.com/zjhthu/OANet.git
- **Journal**: None
- **Summary**: Establishing correspondences between two images requires both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential matrix. Specifically, this proposed network is built hierarchically and comprises three novel operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in a canonical order and invariant to input permutations. Next, the clusters are spatially correlated to form the global context of correspondences. After that, the context-encoded clusters are recovered back to the original size through a proposed upsampling operator. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts. Code will be available at https://github.com/zjhthu/OANet.git.



### Faster Unsupervised Semantic Inpainting: A GAN Based Approach
- **Arxiv ID**: http://arxiv.org/abs/1908.04968v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.04968v1)
- **Published**: 2019-08-14 06:06:37+00:00
- **Updated**: 2019-08-14 06:06:37+00:00
- **Authors**: Avisek Lahiri, Arnav Kumar Jain, Divyasri Nadendla, Prabir Kumar Biswas
- **Comment**: Accepted as full paper at IEEE ICIP, 2019
- **Journal**: None
- **Summary**: In this paper, we propose to improve the inference speed and visual quality of contemporary baseline of Generative Adversarial Networks (GAN) based unsupervised semantic inpainting. This is made possible with better initialization of the core iterative optimization involved in the framework. To our best knowledge, this is also the first attempt of GAN based video inpainting with consideration to temporal cues. On single image inpainting, we achieve about 4.5-5$\times$ speedup and 80$\times$ on videos compared to baseline. Simultaneously, our method has better spatial and temporal reconstruction qualities as found on three image and one video dataset.



### Harmonized Multimodal Learning with Gaussian Process Latent Variable Models
- **Arxiv ID**: http://arxiv.org/abs/1908.04979v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.04979v1)
- **Published**: 2019-08-14 06:40:28+00:00
- **Updated**: 2019-08-14 06:40:28+00:00
- **Authors**: Guoli Song, Shuhui Wang, Qingming Huang, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Multimodal learning aims to discover the relationship between multiple modalities. It has become an important research topic due to extensive multimodal applications such as cross-modal retrieval. This paper attempts to address the modality heterogeneity problem based on Gaussian process latent variable models (GPLVMs) to represent multimodal data in a common space. Previous multimodal GPLVM extensions generally adopt individual learning schemes on latent representations and kernel hyperparameters, which ignore their intrinsic relationship. To exploit strong complementarity among different modalities and GPLVM components, we develop a novel learning scheme called Harmonization, where latent model parameters are jointly learned from each other. Beyond the correlation fitting or intra-modal structure preservation paradigms widely used in existing studies, the harmonization is derived in a model-driven manner to encourage the agreement between modality-specific GP kernels and the similarity of latent representations. We present a range of multimodal learning models by incorporating the harmonization mechanism into several representative GPLVM-based approaches. Experimental results on four benchmark datasets show that the proposed models outperform the strong baselines for cross-modal retrieval tasks, and that the harmonized multimodal learning method is superior in discovering semantically consistent latent representation.



### Justlookup: One Millisecond Deep Feature Extraction for Point Clouds By Lookup Tables
- **Arxiv ID**: http://arxiv.org/abs/1908.08996v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GT, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.08996v1)
- **Published**: 2019-08-14 07:07:26+00:00
- **Updated**: 2019-08-14 07:07:26+00:00
- **Authors**: Hongxin Lin, Zelin Xiao, Yang Tan, Hongyang Chao, Shengyong Ding
- **Comment**: Accepted by ICME2019
- **Journal**: None
- **Summary**: Deep models are capable of fitting complex high dimensional functions while usually yielding large computation load. There is no way to speed up the inference process by classical lookup tables due to the high-dimensional input and limited memory size. Recently, a novel architecture (PointNet) for point clouds has demonstrated that it is possible to obtain a complicated deep function from a set of 3-variable functions. In this paper, we exploit this property and apply a lookup table to encode these 3-variable functions. This method ensures that the inference time is only determined by the memory access no matter how complicated the deep function is. We conduct extensive experiments on ModelNet and ShapeNet datasets and demonstrate that we can complete the inference process in 1.5 ms on an Intel i7-8700 CPU (single core mode), 32x speedup over the PointNet architecture without any performance degradation.



### Memory-Based Neighbourhood Embedding for Visual Recognition
- **Arxiv ID**: http://arxiv.org/abs/1908.04992v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.04992v1)
- **Published**: 2019-08-14 07:19:12+00:00
- **Updated**: 2019-08-14 07:19:12+00:00
- **Authors**: Suichan Li, Dapeng Chen, Bin Liu, Nenghai Yu, Rui Zhao
- **Comment**: Accepted by ICCV2019 for oral presentation
- **Journal**: None
- **Summary**: Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.



### Benchmarking the Robustness of Semantic Segmentation Models
- **Arxiv ID**: http://arxiv.org/abs/1908.05005v3
- **DOI**: 10.1109/CVPR42600.2020.00885
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05005v3)
- **Published**: 2019-08-14 07:51:56+00:00
- **Updated**: 2020-08-10 13:56:01+00:00
- **Authors**: Christoph Kamann, Carsten Rother
- **Comment**: CVPR 2020 camera ready
- **Journal**: None
- **Summary**: When designing a semantic segmentation module for a practical application, such as autonomous driving, it is crucial to understand the robustness of the module with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on the state-of-the-art model DeepLabv3+. To increase the realism of our study, we utilize almost 400,000 images generated from Cityscapes, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, contrary to full-image classification, model robustness increases with model performance, in most cases. Secondly, some architecture properties affect robustness significantly, such as a Dense Prediction Cell, which was designed to maximize performance on clean data only.



### Visualizing Image Content to Explain Novel Image Discovery
- **Arxiv ID**: http://arxiv.org/abs/1908.05006v2
- **DOI**: 10.1007/s10618-020-00700-0
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.05006v2)
- **Published**: 2019-08-14 07:53:05+00:00
- **Updated**: 2022-09-12 21:02:51+00:00
- **Authors**: Jake H. Lee, Kiri L. Wagstaff
- **Comment**: Published in DMKD
- **Journal**: Data Mining and Knowledge Discovery, vol. 34, pp. 1777-1804, 2020
- **Summary**: The initial analysis of any large data set can be divided into two phases: (1) the identification of common trends or patterns and (2) the identification of anomalies or outliers that deviate from those trends. We focus on the goal of detecting observations with novel content, which can alert us to artifacts in the data set or, potentially, the discovery of previously unknown phenomena. To aid in interpreting and diagnosing the novel aspect of these selected observations, we recommend the use of novelty detection methods that generate explanations. In the context of large image data sets, these explanations should highlight what aspect of a given image is new (color, shape, texture, content) in a human-comprehensible form. We propose DEMUD-VIS, the first method for providing visual explanations of novel image content by employing a convolutional neural network (CNN) to extract image features, a method that uses reconstruction error to detect novel content, and an up-convolutional network to convert CNN feature representations back into image space. We demonstrate this approach on diverse images from ImageNet, freshwater streams, and the surface of Mars.



### AdvFaces: Adversarial Face Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1908.05008v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05008v1)
- **Published**: 2019-08-14 07:58:00+00:00
- **Updated**: 2019-08-14 07:58:00+00:00
- **Authors**: Debayan Deb, Jianbang Zhang, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Face recognition systems have been shown to be vulnerable to adversarial examples resulting from adding small perturbations to probe images. Such adversarial images can lead state-of-the-art face recognition systems to falsely reject a genuine subject (obfuscation attack) or falsely match to an impostor (impersonation attack). Current approaches to crafting adversarial face images lack perceptual quality and take an unreasonable amount of time to generate them. We propose, AdvFaces, an automated adversarial face synthesis method that learns to generate minimal perturbations in the salient facial regions via Generative Adversarial Networks. Once AdvFaces is trained, it can automatically generate imperceptible perturbations that can evade state-of-the-art face matchers with attack success rates as high as 97.22% and 24.30% for obfuscation and impersonation attacks, respectively.



### Histographs: Graphs in Histopathology
- **Arxiv ID**: http://arxiv.org/abs/1908.05020v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05020v1)
- **Published**: 2019-08-14 08:56:59+00:00
- **Updated**: 2019-08-14 08:56:59+00:00
- **Authors**: Shrey Gadiya, Deepak Anand, Amit Sethi
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: Spatial arrangement of cells of various types, such as tumor infiltrating lymphocytes and the advancing edge of a tumor, are important features for detecting and characterizing cancers. However, convolutional neural networks (CNNs) do not explicitly extract intricate features of the spatial arrangements of the cells from histopathology images. In this work, we propose to classify cancers using graph convolutional networks (GCNs) by modeling a tissue section as a multi-attributed spatial graph of its constituent cells. Cells are detected using their nuclei in H&E stained tissue image, and each cell's appearance is captured as a multi-attributed high-dimensional vertex feature. The spatial relations between neighboring cells are captured as edge features based on their distances in a graph. We demonstrate the utility of this approach by obtaining classification accuracy that is competitive with CNNs, specifically, Inception-v3, on two tasks-cancerous versus non-cancerous and in situ versus invasive-on the BACH breast cancer dataset.



### Person Re-identification in Aerial Imagery
- **Arxiv ID**: http://arxiv.org/abs/1908.05024v3
- **DOI**: 10.1109/TMM.2020.2977528
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05024v3)
- **Published**: 2019-08-14 09:04:39+00:00
- **Updated**: 2020-04-12 14:00:22+00:00
- **Authors**: Shizhou Zhang, Qi Zhang, Yifei Yang, Xing Wei, Peng Wang, Bingliang Jiao, Yanning Zhang
- **Comment**: IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Nowadays, with the rapid development of consumer Unmanned Aerial Vehicles (UAVs), visual surveillance by utilizing the UAV platform has been very attractive. Most of the research works for UAV captured visual data are mainly focused on the tasks of object detection and tracking. However, limited attention has been paid to the task of person Re-identification (ReID) which has been widely studied in ordinary surveillance cameras with fixed emplacements. In this paper, to facilitate the research of person ReID in aerial imagery, we collect a large scale airborne person ReID dataset named as Person ReID for Aerial Imagery (PRAI-1581), which consists of 39,461 images of 1581 person identities. The images of the dataset are shot by two DJI consumer UAVs flying at an altitude ranging from 20 to 60 meters above the ground, which covers most of the real UAV surveillance scenarios. In addition, we propose to utilize subspace pooling of convolution feature maps to represent the input person images. Our method can learn a discriminative and compact feature representation for ReID in aerial imagery and can be trained in an end-to-end fashion efficiently. We conduct extensive experiments on the proposed dataset and the experimental results demonstrate that re-identify persons in aerial imagery is a challenging problem, where our method performs favorably against state of the arts. Our dataset can be accessed via \url{https://github.com/stormyoung/PRAI-1581}.



### Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1908.05033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05033v1)
- **Published**: 2019-08-14 09:22:41+00:00
- **Updated**: 2019-08-14 09:22:41+00:00
- **Authors**: Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, Junjie Yan
- **Comment**: IEEE ICCV 2019
- **Journal**: None
- **Summary**: Hardware-friendly network quantization (e.g., binary/uniform quantization) can efficiently accelerate the inference and meanwhile reduce memory consumption of the deep neural networks, which is crucial for model deployment on resource-limited devices like mobile phones. However, due to the discreteness of low-bit quantization, existing quantization methods often face the unstable training process and severe performance degradation. To address this problem, in this paper we propose Differentiable Soft Quantization (DSQ) to bridge the gap between the full-precision and low-bit networks. DSQ can automatically evolve during training to gradually approximate the standard quantization. Owing to its differentiable property, DSQ can help pursue the accurate gradients in backward propagation, and reduce the quantization loss in forward process with an appropriate clipping range. Extensive experiments over several popular network structures show that training low-bit neural networks with DSQ can consistently outperform state-of-the-art quantization methods. Besides, our first efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture achieves up to 1.7$\times$ speed up, compared with the open-source 8-bit high-performance inference framework NCNN. [31]



### Deep Generalized Max Pooling
- **Arxiv ID**: http://arxiv.org/abs/1908.05040v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05040v1)
- **Published**: 2019-08-14 09:33:41+00:00
- **Updated**: 2019-08-14 09:33:41+00:00
- **Authors**: Vincent Christlein, Lukas Spranger, Mathias Seuret, Anguelos Nicolaou, Pavel Král, Andreas Maier
- **Comment**: ICDAR'19
- **Journal**: None
- **Summary**: Global pooling layers are an essential part of Convolutional Neural Networks (CNN). They are used to aggregate activations of spatial locations to produce a fixed-size vector in several state-of-the-art CNNs. Global average pooling or global max pooling are commonly used for converting convolutional features of variable size images to a fix-sized embedding. However, both pooling layer types are computed spatially independent: each individual activation map is pooled and thus activations of different locations are pooled together. In contrast, we propose Deep Generalized Max Pooling that balances the contribution of all activations of a spatially coherent region by re-weighting all descriptors so that the impact of frequent and rare ones is equalized. We show that this layer is superior to both average and max pooling on the classification of Latin medieval manuscripts (CLAMM'16, CLAMM'17), as well as writer identification (Historical-WI'17).



### Fusion of Detected Objects in Text for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1908.05054v2
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05054v2)
- **Published**: 2019-08-14 10:03:12+00:00
- **Updated**: 2019-11-03 05:04:09+00:00
- **Authors**: Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter
- **Comment**: None
- **Journal**: None
- **Summary**: To advance models of multimodal context, we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The "Bounding Boxes in Text Transformer" (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark (https://visualcommonsense.com), achieving a new state-of-the-art with a 25% relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided (https://github.com/google-research/language/tree/master/language/question_answering/b2t2).



### Mask Mining for Improved Liver Lesion Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05062v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05062v4)
- **Published**: 2019-08-14 10:33:33+00:00
- **Updated**: 2020-03-11 23:10:32+00:00
- **Authors**: Karsten Roth, Jürgen Hesser, Tomasz Konopczyński
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel procedure to improve liver and lesion segmentation from CT scans for U-Net based models. Our method extends standard segmentation pipelines to focus on higher target recall or reduction of noisy false-positive predictions, boosting overall segmentation performance. To achieve this, we include segmentation errors into a new learning process appended to the main training setup, allowing the model to find features which explain away previous errors. We evaluate this on semantically distinct architectures: cascaded two- and three-dimensional as well as combined learning setups for multitask segmentation. Liver and lesion segmentation data are provided by the Liver Tumor Segmentation challenge (LiTS), with an increase in dice score of up to 2 points.



### Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling
- **Arxiv ID**: http://arxiv.org/abs/1908.05067v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05067v1)
- **Published**: 2019-08-14 10:58:14+00:00
- **Updated**: 2019-08-14 10:58:14+00:00
- **Authors**: Yi-Ting Yeh, Tzu-Chuan Lin, Hsiao-Hua Cheng, Yu-Hsuan Deng, Shang-Yu Su, Yun-Nung Chen
- **Comment**: Accepted for a poster session at the DSTC7 workshop at AAAI 2019
- **Journal**: None
- **Summary**: Visual question answering and visual dialogue tasks have been increasingly studied in the multimodal field towards more practical real-world scenarios. A more challenging task, audio visual scene-aware dialogue (AVSD), is proposed to further advance the technologies that connect audio, vision, and language, which introduces temporal video information and dialogue interactions between a questioner and an answerer. This paper proposes an intuitive mechanism that fuses features and attention in multiple stages in order to well integrate multimodal features, and the results demonstrate its capability in the experiments. Also, we apply several state-of-the-art models in other tasks to the AVSD task, and further analyze their generalization across different tasks.



### Segmentation of Multimodal Myocardial Images Using Shape-Transfer GAN
- **Arxiv ID**: http://arxiv.org/abs/1908.05094v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05094v1)
- **Published**: 2019-08-14 12:23:44+00:00
- **Updated**: 2019-08-14 12:23:44+00:00
- **Authors**: Xumin Tao, Hongrong Wei, Wufeng Xue, Dong Ni
- **Comment**: accepted by STACOM 21019
- **Journal**: None
- **Summary**: Myocardium segmentation of late gadolinium enhancement (LGE) Cardiac MR images is important for evaluation of infarction regions in clinical practice. The pathological myocardium in LGE images presents distinctive brightness and textures compared with the healthy tissues, making it much more challenging to be segment. Instead, the balanced-Steady State Free Precession (bSSFP) cine images show clearly boundaries and can be easily segmented. Given this fact, we propose a novel shape-transfer GAN for LGE images, which can 1) learn to generate realistic LGE images from bSSFP with the anatomical shape preserved, and 2) learn to segment the myocardium of LGE images from these generated images. It's worth to note that no segmentation label of the LGE images is used during this procedure. We test our model on dataset from the Multi-sequence Cardiac MR Segmentation Challenge. The results show that the proposed Shape-Transfer GAN can achieve accurate myocardium masks of LGE images.



### Shape-Aware Complementary-Task Learning for Multi-Organ Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05099v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05099v1)
- **Published**: 2019-08-14 12:47:58+00:00
- **Updated**: 2019-08-14 12:47:58+00:00
- **Authors**: Fernando Navarro, Suprosanna Shit, Ivan Ezhov, Johannes Paetzold, Andrei Gafita, Jan Peeken, Stephanie Combs, Bjoern Menze
- **Comment**: Accepted in MLMI Workshop 2019 MICCAI
- **Journal**: None
- **Summary**: Multi-organ segmentation in whole-body computed tomography (CT) is a constant pre-processing step which finds its application in organ-specific image retrieval, radiotherapy planning, and interventional image analysis. We address this problem from an organ-specific shape-prior learning perspective. We introduce the idea of complementary-task learning to enforce shape-prior leveraging the existing target labels. We propose two complementary-tasks namely i) distance map regression and ii) contour map detection to explicitly encode the geometric properties of each organ. We evaluate the proposed solution on the public VISCERAL dataset containing CT scans of multiple organs. We report a significant improvement of overall dice score from 0.8849 to 0.9018 due to the incorporation of complementary-task learning.



### D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05104v1
- **DOI**: 10.1109/TCBB.2019.2939522.
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05104v1)
- **Published**: 2019-08-14 12:54:04+00:00
- **Updated**: 2019-08-14 12:54:04+00:00
- **Authors**: Yongjin Zhou, Weijian Huang, Pei Dong, Yong Xia, Shanshan Wang
- **Comment**: None
- **Journal**: IEEE/ACM Transactions on Computational Biology and Bioinformatics
  (2019)
- **Summary**: Assessing the location and extent of lesions caused by chronic stroke is critical for medical diagnosis, surgical planning, and prognosis. In recent years, with the rapid development of 2D and 3D convolutional neural networks (CNN), the encoder-decoder structure has shown great potential in the field of medical image segmentation. However, the 2D CNN ignores the 3D information of medical images, while the 3D CNN suffers from high computational resource demands. This paper proposes a new architecture called dimension-fusion-UNet (D-UNet), which combines 2D and 3D convolution innovatively in the encoding stage. The proposed architecture achieves a better segmentation performance than 2D networks, while requiring significantly less computation time in comparison to 3D networks. Furthermore, to alleviate the data imbalance issue between positive and negative samples for the network training, we propose a new loss function called Enhance Mixing Loss (EML). This function adds a weighted focal coefficient and combines two traditional loss functions. The proposed method has been tested on the ATLAS dataset and compared to three state-of-the-art methods. The results demonstrate that the proposed method achieves the best quality performance in terms of DSC = 0.5349+0.2763 and precision = 0.6331+0.295).



### GreyReID: A Two-stream Deep Framework with RGB-grey Information for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1908.05142v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05142v2)
- **Published**: 2019-08-14 14:24:34+00:00
- **Updated**: 2020-08-28 07:42:17+00:00
- **Authors**: Lei Qi, Lei Wang, Jing Huo, Yinghuan Shi, Yang Gao
- **Comment**: Accepted by ACM TOMM
- **Journal**: None
- **Summary**: In this paper, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this paper, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. Firstly, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Secondly, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance.



### Directional TSDF: Modeling Surface Orientation for Coherent Meshes
- **Arxiv ID**: http://arxiv.org/abs/1908.05146v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05146v1)
- **Published**: 2019-08-14 14:27:52+00:00
- **Updated**: 2019-08-14 14:27:52+00:00
- **Authors**: Malte Splietker, Sven Behnke
- **Comment**: For supplementary material and videos, see this
  http://www.ais.uni-bonn.de/videos/IROS_2019_Splietker/
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Macau, China, November 2019
- **Summary**: Real-time 3D reconstruction from RGB-D sensor data plays an important role in many robotic applications, such as object modeling and mapping. The popular method of fusing depth information into a truncated signed distance function (TSDF) and applying the marching cubes algorithm for mesh extraction has severe issues with thin structures: not only does it lead to loss of accuracy, but it can generate completely wrong surfaces. To address this, we propose the directional TSDF - a novel representation that stores opposite surfaces separate from each other. The marching cubes algorithm is modified accordingly to retrieve a coherent mesh representation. We further increase the accuracy by using surface gradient-based ray casting for fusing new measurements. We show that our method outperforms state-of-the-art TSDF reconstruction algorithms in mesh accuracy.



### Semi-supervised Learning with Adaptive Neighborhood Graph Propagation Network
- **Arxiv ID**: http://arxiv.org/abs/1908.05153v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05153v2)
- **Published**: 2019-08-14 14:48:53+00:00
- **Updated**: 2019-11-21 07:51:14+00:00
- **Authors**: Bo Jiang, Leiling Wang, Jin Tang, Bin Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have been widely studied for compact data representation and semi-supervised learning tasks. However, existing GCNs usually use a fixed neighborhood graph which is not guaranteed to be optimal for semi-supervised learning tasks. In this paper, we first re-interpret graph convolution operation in GCNs as a composition of feature propagation and (non-linear) transformation. Based on this observation, we then propose a unified adaptive neighborhood feature propagation model and derive a novel Adaptive Neighborhood Graph Propagation Network (ANGPN) for data representation and semi-supervised learning. The aim of ANGPN is to conduct both graph construction and graph convolution simultaneously and cooperatively in a unified formulation and thus can learn an optimal neighborhood graph that best serves graph convolution for data representation and semi-supervised learning. One main benefit of ANGPN is that the learned (convolutional) representation can provide useful weakly supervised information for constructing a better neighborhood graph which meanwhile facilitates data representation and learning. Experimental results on four benchmark datasets demonstrate the effectiveness and benefit of the proposed ANGPN.



### A Tour of Convolutional Networks Guided by Linear Interpreters
- **Arxiv ID**: http://arxiv.org/abs/1908.05168v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NA, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1908.05168v1)
- **Published**: 2019-08-14 15:18:45+00:00
- **Updated**: 2019-08-14 15:18:45+00:00
- **Authors**: Pablo Navarrete Michelini, Hanwen Liu, Yunhua Lu, Xingqun Jiang
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the "articulations" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs.



### FaSTExt: Fast and Small Text Extractor
- **Arxiv ID**: http://arxiv.org/abs/1908.08994v1
- **DOI**: 10.1109/ICDARW.2019.30064
- **Categories**: **cs.CV**, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1908.08994v1)
- **Published**: 2019-08-14 15:25:24+00:00
- **Updated**: 2019-08-14 15:25:24+00:00
- **Authors**: Alexander Filonenko, Konstantin Gudkov, Aleksei Lebedev, Nikita Orlov, Ivan Zagaynov
- **Comment**: 6 pages, 8th International Workshop on Camera-Based Document Analysis
  & Recognition
- **Journal**: None
- **Summary**: Text detection in natural images is a challenging but necessary task for many applications. Existing approaches utilize large deep convolutional neural networks making it difficult to use them in real-world tasks. We propose a small yet relatively precise text extraction method. The basic component of it is a convolutional neural network which works in a fully-convolutional manner and produces results at multiple scales. Each scale output predicts whether a pixel is a part of some word, its geometry, and its relation to neighbors at the same scale and between scales. The key factor of reducing the complexity of the model was the utilization of depthwise separable convolution, linear bottlenecks, and inverted residuals. Experiments on public datasets show that the proposed network can effectively detect text while keeping the number of parameters in the range of 1.58 to 10.59 million in different configurations.



### Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once
- **Arxiv ID**: http://arxiv.org/abs/1908.05185v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05185v1)
- **Published**: 2019-08-14 15:59:21+00:00
- **Updated**: 2019-08-14 15:59:21+00:00
- **Authors**: Jiangfan Han, Xiaoyi Dong, Ruimao Zhang, Dongdong Chen, Weiming Zhang, Nenghai Yu, Ping Luo, Xiaogang Wang
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods.



### DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05195v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05195v4)
- **Published**: 2019-08-14 16:13:00+00:00
- **Updated**: 2020-04-07 07:01:28+00:00
- **Authors**: Seungju Cho, Tae Joon Jun, Byungsoo Oh, Daeyoung Kim
- **Comment**: Accepted to be published in: 2020 International Joint Conference on
  Neural Networks (IJCNN), Glasgow, July 19--24, 2020
- **Journal**: None
- **Summary**: Nowadays, Deep learning techniques show dramatic performance on computer vision area, and they even outperform human. But it is also vulnerable to some small perturbation called an adversarial attack. This is a problem combined with the safety of artificial intelligence, which has recently been studied a lot. These attacks have shown that they can fool models of image classification, semantic segmentation, and object detection. We point out this attack can be protected by denoise autoencoder, which is used for denoising the perturbation and restoring the original images. We experiment with various noise distributions and verify the effect of denoise autoencoder against adversarial attack in semantic segmentation.



### Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes
- **Arxiv ID**: http://arxiv.org/abs/1908.05217v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05217v1)
- **Published**: 2019-08-14 16:42:15+00:00
- **Updated**: 2019-08-14 16:42:15+00:00
- **Authors**: Hao Yang, Hao Wu, Hao Chen
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Recent advances in deep learning greatly boost the performance of object detection. State-of-the-art methods such as Faster-RCNN, FPN and R-FCN have achieved high accuracy in challenging benchmark datasets. However, these methods require fully annotated object bounding boxes for training, which are incredibly hard to scale up due to the high annotation cost. Weakly-supervised methods, on the other hand, only require image-level labels for training, but the performance is far below their fully-supervised counterparts. In this paper, we propose a semi-supervised large scale fine-grained detection method, which only needs bounding box annotations of a smaller number of coarse-grained classes and image-level labels of large scale fine-grained classes, and can detect all classes at nearly fully-supervised accuracy. We achieve this by utilizing the correlations between coarse-grained and fine-grained classes with shared backbone, soft-attention based proposal re-ranking, and a dual-level memory module. Experiment results show that our methods can achieve close accuracy on object detection to state-of-the-art fully-supervised methods on two large scale datasets, ImageNet and OpenImages, with only a small fraction of fully annotated classes.



### High Accurate Unhealthy Leaf Detection
- **Arxiv ID**: http://arxiv.org/abs/1908.09003v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.09003v1)
- **Published**: 2019-08-14 16:42:36+00:00
- **Updated**: 2019-08-14 16:42:36+00:00
- **Authors**: S. Mohan Sai, G. Gopichand, C. Vikas Reddy, K. Mona Teja
- **Comment**: Page 4, 5 with 1 figure, and page 6 with 2 figures
- **Journal**: None
- **Summary**: India is an agriculture-dependent country. As we all know that farming is the backbone of our country it is our responsibility to preserve the crops. However, we cannot stop the destruction of crops by natural calamities at least we have to try to protect our crops from diseases. To, detect a plant disease we need a fast automatic way. So, this paper presents a model to identify the particular disease of plant leaves at early stages so that we can prevent or take a remedy to stop spreading of the disease. This proposed model is made into five sessions. Image preprocessing includes the enhancement of the low light image done using inception modules in CNN. Low-resolution image enhancement is done using an Adversarial Neural Network. This also includes Conversion of RGB Image to YCrCb color space. Next, this paper presents a methodology for image segmentation which is an important aspect for identifying the disease symptoms. This segmentation is done using the genetic algorithm. Due to this process the segmentation of the leaf Image this helps in detection of the leaf mage automatically and classifying. Texture extraction is done using the statistical model called GLCM and finally, the classification of the diseases is done using the SVM using Different Kernels with the high accuracy.



### Few-Shot Learning with Global Class Representations
- **Arxiv ID**: http://arxiv.org/abs/1908.05257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05257v1)
- **Published**: 2019-08-14 17:36:37+00:00
- **Updated**: 2019-08-14 17:36:37+00:00
- **Authors**: Tiange Luo, Aoxue Li, Tao Xiang, Weiran Huang, Liwei Wang
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: In this paper, we propose to tackle the challenging few-shot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings.



### AutoCorrect: Deep Inductive Alignment of Noisy Geometric Annotations
- **Arxiv ID**: http://arxiv.org/abs/1908.05263v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05263v1)
- **Published**: 2019-08-14 17:40:10+00:00
- **Updated**: 2019-08-14 17:40:10+00:00
- **Authors**: Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman
- **Comment**: BMVC 2019 (Spotlight)
- **Journal**: None
- **Summary**: We propose AutoCorrect, a method to automatically learn object-annotation alignments from a dataset with annotations affected by geometric noise. The method is based on a consistency loss that enables deep neural networks to be trained, given only noisy annotations as input, to correct the annotations. When some noise-free annotations are available, we show that the consistency loss reduces to a stricter self-supervised loss. We also show that the method can implicitly leverage object symmetries to reduce the ambiguity arising in correcting noisy annotations. When multiple object-annotation pairs are present in an image, we introduce a spatial memory map that allows the network to correct annotations sequentially, one at a time, while accounting for all other annotations in the image and corrections performed so far. Through ablation, we show the benefit of these contributions, demonstrating excellent results on geo-spatial imagery. Specifically, we show results using a new Railway tracks dataset as well as the public INRIA Buildings benchmarks, achieving new state-of-the-art results for the latter.



### Local Unsupervised Learning for Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1908.08993v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.08993v1)
- **Published**: 2019-08-14 17:42:11+00:00
- **Updated**: 2019-08-14 17:42:11+00:00
- **Authors**: Leopold Grinberg, John Hopfield, Dmitry Krotov
- **Comment**: None
- **Journal**: None
- **Summary**: Local Hebbian learning is believed to be inferior in performance to end-to-end training using a backpropagation algorithm. We question this popular belief by designing a local algorithm that can learn convolutional filters at scale on large image datasets. These filters combined with patch normalization and very steep non-linearities result in a good classification accuracy for shallow networks trained locally, as opposed to end-to-end. The filters learned by our algorithm contain both orientation selective units and unoriented color units, resembling the responses of pyramidal neurons located in the cytochrome oxidase 'interblob' and 'blob' regions in the primary visual cortex of primates. It is shown that convolutional networks with patch normalization significantly outperform standard convolutional networks on the task of recovering the original classes when shadows are superimposed on top of standard CIFAR-10 images. Patch normalization approximates the retinal adaptation to the mean light intensity, important for human vision. We also demonstrate a successful transfer of learned representations between CIFAR-10 and ImageNet 32x32 datasets. All these results taken together hint at the possibility that local unsupervised training might be a powerful tool for learning general representations (without specifying the task) directly from unlabeled data.



### Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1908.05293v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05293v3)
- **Published**: 2019-08-14 18:13:57+00:00
- **Updated**: 2020-02-25 06:14:42+00:00
- **Authors**: Rahul Mitra, Nitesh B. Gundavarapu, Abhishek Sharma, Arjun Jain
- **Comment**: None
- **Journal**: None
- **Summary**: The best performing methods for 3D human pose estimation from monocular images require large amounts of in-the-wild 2D and controlled 3D pose annotated datasets which are costly and require sophisticated systems to acquire. To reduce this annotation dependency, we propose Multiview-Consistent Semi Supervised Learning (MCSS) framework that utilizes similarity in pose information from unannotated, uncalibrated but synchronized multi-view videos of human motions as additional weak supervision signal to guide 3D human pose regression. Our framework applies hard-negative mining based on temporal relations in multi-view videos to arrive at a multi-view consistent pose embedding. When jointly trained with limited 3D pose annotations, our approach improves the baseline by 25% and state-of-the-art by 8.7%, whilst using substantially smaller networks. Lastly, but importantly, we demonstrate the advantages of the learned embedding and establish view-invariant pose retrieval benchmarks on two popular, publicly available multi-view human pose datasets, Human 3.6M and MPI-INF-3DHP, to facilitate future research.



### Autonomous Learning for Face Recognition in the Wild via Ambient Wireless Cues
- **Arxiv ID**: http://arxiv.org/abs/1908.09002v1
- **DOI**: 10.1145/3308558.3313398
- **Categories**: **cs.CV**, cs.LG, cs.NI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1908.09002v1)
- **Published**: 2019-08-14 18:39:09+00:00
- **Updated**: 2019-08-14 18:39:09+00:00
- **Authors**: Chris Xiaoxuan Lu, Xuan Kan, Bowen Du, Changhao Chen, Hongkai Wen, Andrew Markham, Niki Trigoni, John Stankovic
- **Comment**: 11 pages, accepted in the Web Conference (WWW'2019)
- **Journal**: None
- **Summary**: Facial recognition is a key enabling component for emerging Internet of Things (IoT) services such as smart homes or responsive offices. Through the use of deep neural networks, facial recognition has achieved excellent performance. However, this is only possibly when trained with hundreds of images of each user in different viewing and lighting conditions. Clearly, this level of effort in enrolment and labelling is impossible for wide-spread deployment and adoption. Inspired by the fact that most people carry smart wireless devices with them, e.g. smartphones, we propose to use this wireless identifier as a supervisory label. This allows us to curate a dataset of facial images that are unique to a certain domain e.g. a set of people in a particular office. This custom corpus can then be used to finetune existing pre-trained models e.g. FaceNet. However, due to the vagaries of wireless propagation in buildings, the supervisory labels are noisy and weak.We propose a novel technique, AutoTune, which learns and refines the association between a face and wireless identifier over time, by increasing the inter-cluster separation and minimizing the intra-cluster distance. Through extensive experiments with multiple users on two sites, we demonstrate the ability of AutoTune to design an environment-specific, continually evolving facial recognition system with entirely no user effort.



### Automatic detection and diagnosis of sacroiliitis in CT scans as incidental findings
- **Arxiv ID**: http://arxiv.org/abs/1908.05663v1
- **DOI**: 10.1016/j.media.2019.07.007
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1908.05663v1)
- **Published**: 2019-08-14 18:51:34+00:00
- **Updated**: 2019-08-14 18:51:34+00:00
- **Authors**: Yigal Shenkman, Bilal Qutteineh, Leo Joskowicz, Adi Szeskin, Yusef Azraq, Arnaldo Mayer, Iris Eshed
- **Comment**: None
- **Journal**: None
- **Summary**: Early diagnosis of sacroiliitis may lead to preventive treatment which can significantly improve the patient's quality of life in the long run. Oftentimes, a CT scan of the lower back or abdomen is acquired for suspected back pain. However, since the differences between a healthy and an inflamed sacroiliac joint in the early stages are subtle, the condition may be missed. We have developed a new automatic algorithm for the diagnosis and grading of sacroiliitis CT scans as incidental findings, for patients who underwent CT scanning as part of their lower back pain workout. The method is based on supervised machine and deep learning techniques. The input is a CT scan that includes the patient's pelvis. The output is a diagnosis for each sacroiliac joint. The algorithm consists of four steps: 1) computation of an initial region of interest (ROI) that includes the pelvic joints region using heuristics and a U-Net classifier; 2) refinement of the ROI to detect both sacroiliac joints using a four-tree random forest; 3) individual sacroiliitis grading of each sacroiliac joint in each CT slice with a custom slice CNN classifier, and; 4) sacroiliitis diagnosis and grading by combining the individual slice grades using a random forest. Experimental results on 484 sacroiliac joints yield a binary and a 3-class case classification accuracy of 91.9% and 86%, a sensitivity of 95% and 82%, and an Area-Under-the-Curve of 0.97 and 0.57, respectively. Automatic computer-based analysis of CT scans has the potential of being a useful method for the diagnosis and grading of sacroiliitis as an incidental finding.



### Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1908.05311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05311v1)
- **Published**: 2019-08-14 18:54:52+00:00
- **Updated**: 2019-08-14 18:54:52+00:00
- **Authors**: Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana, Keerthi Ram, Jayaraj Joseph, Mohanasankar Sivaprakasam
- **Comment**: Accepted in MLMI 2019
- **Journal**: None
- **Summary**: For the task of medical image segmentation, fully convolutional network (FCN) based architectures have been extensively used with various modifications. A rising trend in these architectures is to employ joint-learning of the target region with an auxiliary task, a method commonly known as multi-task learning. These approaches help impose smoothness and shape priors, which vanilla FCN approaches do not necessarily incorporate. In this paper, we propose a novel plug-and-play module, which we term as Conv-MCD, which exploits structural information in two ways - i) using the contour map and ii) using the distance map, both of which can be obtained from ground truth segmentation maps with no additional annotation costs. The key benefit of our module is the ease of its addition to any state-of-the-art architecture, resulting in a significant improvement in performance with a minimal increase in parameters. To substantiate the above claim, we conduct extensive experiments using 4 state-of-the-art architectures across various evaluation metrics, and report a significant increase in performance in relation to the base networks. In addition to the aforementioned experiments, we also perform ablative studies and visualization of feature maps to further elucidate our approach.



### Recognition of Ischaemia and Infection in Diabetic Foot Ulcers: Dataset and Techniques
- **Arxiv ID**: http://arxiv.org/abs/1908.05317v4
- **DOI**: 10.1016/j.compbiomed.2020.103616
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05317v4)
- **Published**: 2019-08-14 19:11:36+00:00
- **Updated**: 2020-02-08 20:44:57+00:00
- **Authors**: Manu Goyal, Neil Reeves, Satyan Rajbhandari, Naseer Ahmad, Chuan Wang, Moi Hoon Yap
- **Comment**: 25 pages, 13 figures and 3 tables
- **Journal**: Computers in Biology and Medicine, Volume 117, February 2020,
  103616
- **Summary**: Recognition and analysis of Diabetic Foot Ulcers (DFU) using computerized methods is an emerging research area with the evolution of image-based machine learning algorithms. Existing research using visual computerized methods mainly focuses on recognition, detection, and segmentation of the visual appearance of the DFU as well as tissue classification. According to DFU medical classification systems, the presence of infection (bacteria in the wound) and ischaemia (inadequate blood supply) has important clinical implications for DFU assessment, which are used to predict the risk of amputation. In this work, we propose a new dataset and computer vision techniques to identify the presence of infection and ischaemia in DFU. This is the first time a DFU dataset with ground truth labels of ischaemia and infection cases is introduced for research purposes. For the handcrafted machine learning approach, we propose a new feature descriptor, namely the Superpixel Color Descriptor. Then we use the Ensemble Convolutional Neural Network (CNN) model for more effective recognition of ischaemia and infection. We propose to use a natural data-augmentation method, which identifies the region of interest on foot images and focuses on finding the salient features existing in this area. Finally, we evaluate the performance of our proposed techniques on binary classification, i.e. ischaemia versus non-ischaemia and infection versus non-infection. Overall, our method performed better in the classification of ischaemia than infection. We found that our proposed Ensemble CNN deep learning algorithms performed better for both classification tasks as compared to handcrafted machine learning algorithms, with 90% accuracy in ischaemia classification and 73% in infection classification.



### Dual Adversarial Inference for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1908.05324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1908.05324v1)
- **Published**: 2019-08-14 19:35:02+00:00
- **Updated**: 2019-08-14 19:35:02+00:00
- **Authors**: Qicheng Lao, Mohammad Havaei, Ahmad Pesaranghader, Francis Dutil, Lisa Di Jorio, Thomas Fevens
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Synthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated on Oxford-102, CUB and COCO datasets.



### Explanation based Handwriting Verification
- **Arxiv ID**: http://arxiv.org/abs/1909.02548v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1909.02548v1)
- **Published**: 2019-08-14 19:50:07+00:00
- **Updated**: 2019-08-14 19:50:07+00:00
- **Authors**: Mihir Chauhan, Mohammad Abuzar Shaikh, Sargur N. Srihari
- **Comment**: Presented at BMVC 2019: Workshop on Interpretable and Explainable
  Machine Vision, Cardiff, UK
- **Journal**: None
- **Summary**: Deep learning system have drawback that their output is not accompanied with ex-planation. In a domain such as forensic handwriting verification it is essential to provideexplanation to jurors. The goal of handwriting verification is to find a measure of confi-dence whether the given handwritten samples are written by the same or different writer.We propose a method to generate explanations for the confidence provided by convolu-tional neural network (CNN) which maps the input image to 15 annotations (features)provided by experts. Our system comprises of: (1) Feature learning network (FLN),a differentiable system, (2) Inference module for providing explanations. Furthermore,inference module provides two types of explanations: (a) Based on cosine similaritybetween categorical probabilities of each feature, (b) Based on Log-Likelihood Ratio(LLR) using directed probabilistic graphical model. We perform experiments using acombination of feature learning network (FLN) and each inference module. We evaluateour system using XAI-AND dataset, containing 13700 handwritten samples and 15 cor-responding expert examined features for each sample. The dataset is released for publicuse and the methods can be extended to provide explanations on other verification taskslike face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification. The code is available on github.



### Are Quantitative Features of Lung Nodules Reproducible at Different CT Acquisition and Reconstruction Parameters?
- **Arxiv ID**: http://arxiv.org/abs/1908.05667v1
- **DOI**: 10.1371/journal.pone.0240184
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05667v1)
- **Published**: 2019-08-14 20:14:19+00:00
- **Updated**: 2019-08-14 20:14:19+00:00
- **Authors**: Barbaros S. Erdal, Mutlu Demirer, Chiemezie C. Amadi, Gehan F. M. Ibrahim, Thomas P. O'Donnell, Rainer Grimmer, Andreas Wimmer, Kevin J. Little, Vikash Gupta, Matthew T. Bigelow, Luciano M. Prevedello, Richard D. White
- **Comment**: None
- **Journal**: None
- **Summary**: Consistency and duplicability in Computed Tomography (CT) output is essential to quantitative imaging for lung cancer detection and monitoring. This study of CT-detected lung nodules investigated the reproducibility of volume-, density-, and texture-based features (outcome variables) over routine ranges of radiation-dose, reconstruction kernel, and slice thickness. CT raw data of 23 nodules were reconstructed using 320 acquisition/reconstruction conditions (combinations of 4 doses, 10 kernels, and 8 thicknesses). Scans at 12.5%, 25%, and 50% of protocol dose were simulated; reduced-dose and full-dose data were reconstructed using conventional filtered back-projection and iterative-reconstruction kernels at a range of thicknesses (0.6-5.0 mm). Full-dose/B50f kernel reconstructions underwent expert segmentation for reference Region-Of-Interest (ROI) and nodule volume per thickness; each ROI was applied to 40 corresponding images (combinations of 4 doses and 10 kernels). Typical texture analysis metrics (including 5 histogram features, 13 Gray Level Co-occurrence Matrix, 5 Run Length Matrix, 2 Neighboring Gray-Level Dependence Matrix, and 2 Neighborhood Gray-Tone Difference Matrix) were computed per ROI. Reconstruction conditions resulting in no significant change in volume, density, or texture metrics were identified as "compatible pairs" for a given outcome variable. Our results indicate that as thickness increases, volumetric reproducibility decreases, while reproducibility of histogram- and texture-based features across different acquisition and reconstruction parameters improves. In order to achieve concomitant reproducibility of volumetric and radiomic results across studies, balanced standardization of the imaging acquisition parameters is required.



### Robust parametric modeling of Alzheimer's disease progression
- **Arxiv ID**: http://arxiv.org/abs/1908.05338v3
- **DOI**: None
- **Categories**: **stat.AP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05338v3)
- **Published**: 2019-08-14 20:26:21+00:00
- **Updated**: 2020-06-18 14:38:05+00:00
- **Authors**: Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, Marc Modat, M. Jorge Cardoso, Sébastien Ourselin, Lauge Sørensen
- **Comment**: None
- **Journal**: None
- **Summary**: Quantitative characterization of disease progression using longitudinal data can provide long-term predictions for the pathological stages of individuals. This work studies the robust modeling of Alzheimer's disease progression using parametric methods. The proposed method linearly maps the individual's age to a disease progression score (DPS) and jointly fits constrained generalized logistic functions to the longitudinal dynamics of biomarkers as functions of the DPS using M-estimation. Robustness of the estimates is quantified using bootstrapping via Monte Carlo resampling, and the estimated inflection points of the fitted functions are used to temporally order the modeled biomarkers in the disease course. Kernel density estimation is applied to the obtained DPSs for clinical status classification using a Bayesian classifier. Different M-estimators and logistic functions, including a novel type proposed in this study, called modified Stannard, are evaluated on the data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) for robust modeling of volumetric MRI and PET biomarkers, CSF measurements, as well as cognitive tests. The results show that the modified Stannard function fitted using the logistic loss achieves the best modeling performance with an average normalized MAE of 0.991 across all biomarkers and bootstraps. Applied to the ADNI test set, this model achieves a multiclass AUC of 0.934 in clinical status classification. The obtained results for the proposed model outperform almost all state-of-the-art results in predicting biomarker values and classifying clinical status. Finally, the experiments show that the proposed model, trained using abundant ADNI data, generalizes well to data from the National Alzheimer's Coordinating Center (NACC) with an average normalized MAE of 1.182 and a multiclass AUC of 0.929.



### Graph Convolutional Networks for Coronary Artery Segmentation in Cardiac CT Angiography
- **Arxiv ID**: http://arxiv.org/abs/1908.05343v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1908.05343v1)
- **Published**: 2019-08-14 20:48:27+00:00
- **Updated**: 2019-08-14 20:48:27+00:00
- **Authors**: Jelmer M. Wolterink, Tim Leiner, Ivana Išgum
- **Comment**: MICCAI 2019 Workshop on Graph Learning in Medical Image (GLMI)
- **Journal**: None
- **Summary**: Detection of coronary artery stenosis in coronary CT angiography (CCTA) requires highly personalized surface meshes enclosing the coronary lumen. In this work, we propose to use graph convolutional networks (GCNs) to predict the spatial location of vertices in a tubular surface mesh that segments the coronary artery lumen. Predictions for individual vertex locations are based on local image features as well as on features of neighboring vertices in the mesh graph. The method was trained and evaluated using the publicly available Coronary Artery Stenoses Detection and Quantification Evaluation Framework. Surface meshes enclosing the full coronary artery tree were automatically extracted. A quantitative evaluation on 78 coronary artery segments showed that these meshes corresponded closely to reference annotations, with a Dice similarity coefficient of 0.75/0.73, a mean surface distance of 0.25/0.28 mm, and a Hausdorff distance of 1.53/1.86 mm in healthy/diseased vessel segments. The results showed that inclusion of mesh information in a GCN improves segmentation overlap and accuracy over a baseline model without interaction on the mesh. The results indicate that GCNs allow efficient extraction of coronary artery surface meshes and that the use of GCNs leads to regular and more accurate meshes.



