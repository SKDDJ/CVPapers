# Arxiv Papers in cs.CV on 2019-02-17
### Structured Group Local Sparse Tracker
- **Arxiv ID**: http://arxiv.org/abs/1902.06182v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06182v2)
- **Published**: 2019-02-17 01:06:58+00:00
- **Updated**: 2019-03-01 02:16:02+00:00
- **Authors**: Mohammadreza Javanmardi, Xiaojun Qi
- **Comment**: This manuscript is submitted to IET Image Processing
- **Journal**: None
- **Summary**: Sparse representation is considered as a viable solution to visual tracking. In this paper, we propose a structured group local sparse tracker (SGLST), which exploits local patches inside target candidates in the particle filter framework. Unlike the conventional local sparse trackers, the proposed optimization model in SGLST not only adopts local and spatial information of the target candidates but also attains the spatial layout structure among them by employing a group-sparsity regularization term. To solve the optimization model, we propose an efficient numerical algorithm consisting of two subproblems with the closed-form solutions. Both qualitative and quantitative evaluations on the benchmarks of challenging image sequences demonstrate the superior performance of the proposed tracker against several state-of-the-art trackers.



### Online PCB Defect Detector On A New PCB Defect Dataset
- **Arxiv ID**: http://arxiv.org/abs/1902.06197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06197v1)
- **Published**: 2019-02-17 03:39:24+00:00
- **Updated**: 2019-02-17 03:39:24+00:00
- **Authors**: Sanli Tang, Fan He, Xiaolin Huang, Jie Yang
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: Previous works for PCB defect detection based on image difference and image processing techniques have already achieved promising performance. However, they sometimes fall short because of the unaccounted defect patterns or over-sensitivity about some hyper-parameters. In this work, we design a deep model that accurately detects PCB defects from an input pair of a detect-free template and a defective tested image. A novel group pyramid pooling module is proposed to efficiently extract features of a large range of resolutions, which are merged by group to predict PCB defect of corresponding scales. To train the deep model, a dataset is established, namely DeepPCB, which contains 1,500 image pairs with annotations including positions of 6 common types of PCB defects. Experiment results validate the effectiveness and efficiency of the proposed model by achieving $98.6\%$ mAP @ 62 FPS on DeepPCB dataset. This dataset is now available at: https://github.com/tangsanli5201/DeepPCB.



### Using Persistent Homology to Quantify a Diurnal Cycle in Hurricane Felix
- **Arxiv ID**: http://arxiv.org/abs/1902.06202v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/1902.06202v2)
- **Published**: 2019-02-17 04:31:12+00:00
- **Updated**: 2020-01-10 19:24:53+00:00
- **Authors**: Sarah Tymochko, Elizabeth Munch, Jason Dunion, Kristen Corbosiero, Ryan Torn
- **Comment**: None
- **Journal**: None
- **Summary**: The diurnal cycle of tropical cyclones (TCs) is a daily cycle in clouds that appears in satellite images and may have implications for TC structure and intensity. The diurnal pattern can be seen in infrared (IR) satellite imagery as cyclical pulses in the cloud field that propagate radially outward from the center of nearly all Atlantic-basin TCs. These diurnal pulses, a distinguishing characteristic of the TC diurnal cycle, begin forming in the storm's inner core near sunset each day and appear as a region of cooling cloud-top temperatures. The area of cooling takes on a ring-like appearance as cloud-top warming occurs on its inside edge and the cooling moves away from the storm overnight, reaching several hundred kilometers from the circulation center by the following afternoon. The state-of-the-art TC diurnal cycle measurement has a limited ability to analyze the behavior beyond qualitative observations. We present a method for quantifying the TC diurnal cycle using one-dimensional persistent homology, a tool from Topological Data Analysis, by tracking maximum persistence and quantifying the cycle using the discrete Fourier transform. Using Geostationary Operational Environmental Satellite IR imagery data from Hurricane Felix (2007), our method is able to detect an approximate daily cycle.



### Fast Pedestrian Detection based on T-CENTRIST in infrared image
- **Arxiv ID**: http://arxiv.org/abs/1902.06218v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06218v2)
- **Published**: 2019-02-17 07:20:59+00:00
- **Updated**: 2021-01-19 01:19:36+00:00
- **Authors**: Hongyin Ni, Fengping Li
- **Comment**: 14 pages, 6 figures
- **Journal**: None
- **Summary**: Pedestrian detection is a research hotspot and a difficult issue in the computer vision such as the Intelligent Surveillance System, the Intelligent Transport System, robotics, and automotive safety. However, the human body's position, angle, and dress in a video scene are complicated and changeable, which have a great influence on the detection accuracy. In this paper, through the analysis on the pros and cons of Census Transform Histogram (CENTRIST), a novel feature is presented for human detection Ternary CENTRIST (T-CENTRIST). The T-CENTRIST feature takes the relationship between each pixel and its neighborhood pixels into account. Meanwhile, it also considers the relevancy among these neighborhood pixels. Therefore, the proposed feature description method can reflect the silhouette of pedestrian more adequately and accurately than that of CENTRIST. Second, we propose a fast pedestrian detection framework based on T-CENTRIST in infrared image, which introduces the idea of extended blocks and the integral image. Finally, experimental results verify the effectiveness of the proposed pedestrian detection method.



### LapEPI-Net: A Laplacian Pyramid EPI structure for Learning-based Dense Light Field Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1902.06221v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06221v1)
- **Published**: 2019-02-17 08:28:31+00:00
- **Updated**: 2019-02-17 08:28:31+00:00
- **Authors**: Gaochang Wu, Yebin Liu, Lu Fang, Tianyou Chai
- **Comment**: 10 pages, 8 figures, 4 tables
- **Journal**: None
- **Summary**: For dense sampled light field (LF) reconstruction problem, existing approaches focus on a depth-free framework to achieve non-Lambertian performance. However, they trap in the trade-off "either aliasing or blurring" problem, i.e., pre-filtering the aliasing components (caused by the angular sparsity of the input LF) always leads to a blurry result. In this paper, we intend to solve this challenge by introducing an elaborately designed epipolar plane image (EPI) structure within a learning-based framework. Specifically, we start by analytically showing that decreasing the spatial scale of an EPI shows higher efficiency in addressing the aliasing problem than simply adopting pre-filtering. Accordingly, we design a Laplacian Pyramid EPI (LapEPI) structure that contains both low spatial scale EPI (for aliasing) and high-frequency residuals (for blurring) to solve the trade-off problem. We then propose a novel network architecture for the LapEPI structure, termed as LapEPI-net. To ensure the non-Lambertian performance, we adopt a transfer-learning strategy by first pre-training the network with natural images then fine-tuning it with unstructured LFs. Extensive experiments demonstrate the high performance and robustness of the proposed approach for tackling the aliasing-or-blurring problem as well as the non-Lambertian reconstruction.



### Detecting Colorized Images via Convolutional Neural Networks: Toward High Accuracy and Good Generalization
- **Arxiv ID**: http://arxiv.org/abs/1902.06222v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06222v1)
- **Published**: 2019-02-17 08:40:48+00:00
- **Updated**: 2019-02-17 08:40:48+00:00
- **Authors**: Weize Quan, Dong-Ming Yan, Kai Wang, Xiaopeng Zhang, Denis Pellerin
- **Comment**: 13 pages, 10 figures
- **Journal**: None
- **Summary**: Image colorization achieves more and more realistic results with the increasing computation power of recent deep learning techniques. It becomes more difficult to identify the fake colorized images by human eyes. In this work, we propose a novel forensic method to distinguish between natural images (NIs) and colorized images (CIs) based on convolutional neural network (CNN). Our method is able to achieve high classification accuracy and cope with the challenging scenario of blind detection, i.e., no training sample is available from "unknown" colorization algorithm that we may encounter during the testing phase. This blind detection performance can be regarded as a generalization performance. First, we design and implement a base network, which can attain better performance in terms of classification accuracy and generalization (in most cases) compared with state-of-the-art methods. Furthermore, we design a new branch, which analyzes smaller regions of extracted features, and insert it into the above base network. Consequently, our network can not only improve the classification accuracy, but also enhance the generalization in the vast majority of cases. To further improve the performance of blind detection, we propose to automatically construct negative samples through linear interpolation of paired natural and colorized images. Then, we progressively insert these negative samples into the original training dataset and continue to train the network. Experimental results demonstrate that our method can achieve stable and high generalization performance when tested against different state-of-the-art colorization algorithms.



### Exploring Stereovision-Based 3-D Scene Reconstruction for Augmented Reality
- **Arxiv ID**: http://arxiv.org/abs/1902.06255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06255v1)
- **Published**: 2019-02-17 13:09:16+00:00
- **Updated**: 2019-02-17 13:09:16+00:00
- **Authors**: Guang-Yu Nie, Yun Liu, Cong Wang, Yue Liu, Yongtian Wang
- **Comment**: To be published in IEEE VR2019 Conference as a Poster
- **Journal**: None
- **Summary**: Three-dimensional (3-D) scene reconstruction is one of the key techniques in Augmented Reality (AR), which is related to the integration of image processing and display systems of complex information. Stereo matching is a computer vision based approach for 3-D scene reconstruction. In this paper, we explore an improved stereo matching network, SLED-Net, in which a Single Long Encoder-Decoder is proposed to replace the stacked hourglass network in PSM-Net for better contextual information learning. We compare SLED-Net to state-of-the-art methods recently published, and demonstrate its superior performance on Scene Flow and KITTI2015 test sets.



### Fully-Featured Attribute Transfer
- **Arxiv ID**: http://arxiv.org/abs/1902.06258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06258v1)
- **Published**: 2019-02-17 13:20:24+00:00
- **Updated**: 2019-02-17 13:20:24+00:00
- **Authors**: De Xie, Muli Yang, Cheng Deng, Wei Liu, Dacheng Tao
- **Comment**: 9 pages, 8 figures
- **Journal**: None
- **Summary**: Image attribute transfer aims to change an input image to a target one with expected attributes, which has received significant attention in recent years. However, most of the existing methods lack the ability to de-correlate the target attributes and irrelevant information, i.e., the other attributes and background information, thus often suffering from blurs and artifacts. To address these issues, we propose a novel Attribute Manifold Encoding GAN (AME-GAN) for fully-featured attribute transfer, which can modify and adjust every detail in the images. Specifically, our method divides the input image into image attribute part and image background part on manifolds, which are controlled by attribute latent variables and background latent variables respectively. Through enforcing attribute latent variables to Gaussian distributions and background latent variables to uniform distributions respectively, the attribute transfer procedure becomes controllable and image generation is more photo-realistic. Furthermore, we adopt a conditional multi-scale discriminator to render accurate and high-quality target attribute images. Experimental results on three popular datasets demonstrate the superiority of our proposed method in both performances of the attribute transfer and image generation quality.



### Exploiting Unlabeled Data in CNNs by Self-supervised Learning to Rank
- **Arxiv ID**: http://arxiv.org/abs/1902.06285v1
- **DOI**: 10.1109/TPAMI.2019.2899857
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06285v1)
- **Published**: 2019-02-17 16:18:34+00:00
- **Updated**: 2019-02-17 16:18:34+00:00
- **Authors**: Xialei Liu, Joost van de Weijer, Andrew D. Bagdanov
- **Comment**: Accepted at TPAMI. (Keywords: Learning from rankings, image quality
  assessment, crowd counting, active learning). arXiv admin note: text overlap
  with arXiv:1803.03095
- **Journal**: None
- **Summary**: For many applications the collection of labeled data is expensive laborious. Exploitation of unlabeled data during training is thus a long pursued objective of machine learning. Self-supervised learning addresses this by positing an auxiliary task (different, but related to the supervised task) for which data is abundantly available. In this paper, we show how ranking can be used as a proxy task for some regression problems. As another contribution, we propose an efficient backpropagation technique for Siamese networks which prevents the redundant computation introduced by the multi-branch network architecture. We apply our framework to two regression problems: Image Quality Assessment (IQA) and Crowd Counting. For both we show how to automatically generate ranked image sets from unlabeled data. Our results show that networks trained to regress to the ground truth targets for labeled data and to simultaneously learn to rank unlabeled data obtain significantly better, state-of-the-art results for both IQA and crowd counting. In addition, we show that measuring network uncertainty on the self-supervised proxy task is a good measure of informativeness of unlabeled data. This can be used to drive an algorithm for active learning and we show that this reduces labeling effort by up to 50%.



### ProtoAttend: Attention-Based Prototypical Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.06292v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06292v4)
- **Published**: 2019-02-17 17:12:07+00:00
- **Updated**: 2019-09-26 01:39:46+00:00
- **Authors**: Sercan O. Arik, Tomas Pfister
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. The resulting model outperforms state of the art in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a sample-based interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All this can be achieved with minimal additional test time and a practically viable training time computational cost.



### Automated Detection of Regions of Interest for Brain Perfusion MR Images
- **Arxiv ID**: http://arxiv.org/abs/1902.06323v1
- **DOI**: 10.20535/1810-0546.2018.5.146185
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06323v1)
- **Published**: 2019-02-17 20:47:22+00:00
- **Updated**: 2019-02-17 20:47:22+00:00
- **Authors**: Svitlana M Alkhimova
- **Comment**: None
- **Journal**: Research Bulletin of the National Technical University of Ukraine"
  Kyiv Politechnic Institute" 5 (2018): 14-21
- **Summary**: Images with abnormal brain anatomy produce problems for automatic segmentation techniques, and as a result poor ROI detection affects both quantitative measurements and visual assessment of perfusion data. This paper presents a new approach for fully automated and relatively accurate ROI detection from dynamic susceptibility contrast perfusion magnetic resonance and can therefore be applied excellently in the perfusion analysis. In the proposed approach the segmentation output is a binary mask of perfusion ROI that has zero values for air pixels, pixels that represent non-brain tissues, and cerebrospinal fluid pixels. The process of binary mask producing starts with extracting low intensity pixels by thresholding. Optimal low-threshold value is solved by obtaining intensity pixels information from the approximate anatomical brain location. Holes filling algorithm and binary region growing algorithm are used to remove falsely detected regions and produce region of only brain tissues. Further, CSF pixels extraction is provided by thresholding of high intensity pixels from region of only brain tissues. Each time-point image of the perfusion sequence is used for adjustment of CSF pixels location. The segmentation results were compared with the manual segmentation performed by experienced radiologists, considered as the reference standard for evaluation of proposed approach. On average of 120 images the segmentation results have a good agreement with the reference standard. All detected perfusion ROIs were deemed by two experienced radiologists as satisfactory enough for clinical use. The results show that proposed approach is suitable to be used for perfusion ROI detection from DSC head scans. Segmentation tool based on the proposed approach can be implemented as a part of any automatic brain image processing system for clinical use.



### PIXOR: Real-time 3D Object Detection from Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1902.06326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06326v3)
- **Published**: 2019-02-17 21:17:55+00:00
- **Updated**: 2019-03-02 02:43:26+00:00
- **Authors**: Bin Yang, Wenjie Luo, Raquel Urtasun
- **Comment**: Update of CVPR2018 paper: correct timing, fix typos, add
  acknowledgement
- **Journal**: None
- **Summary**: We address the problem of real-time 3D object detection from point clouds in the context of autonomous driving. Computation speed is critical as detection is a necessary component for safety. Existing approaches are, however, expensive in computation due to high dimensionality of point clouds. We utilize the 3D data more efficiently by representing the scene from the Bird's Eye View (BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs oriented 3D object estimates decoded from pixel-wise neural network predictions. The input representation, network architecture, and model optimization are especially designed to balance high accuracy and real-time efficiency. We validate PIXOR on two datasets: the KITTI BEV object detection benchmark, and a large-scale 3D vehicle detection benchmark. In both datasets we show that the proposed detector surpasses other state-of-the-art methods notably in terms of Average Precision (AP), while still runs at >28 FPS.



### Unsupervised Domain Adaptation using Deep Networks with Cross-Grafted Stacks
- **Arxiv ID**: http://arxiv.org/abs/1902.06328v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06328v2)
- **Published**: 2019-02-17 21:29:06+00:00
- **Updated**: 2019-03-25 02:25:47+00:00
- **Authors**: Jinyong Hou, Xuejie Ding, Jeremiah D. Deng, Stephen Cranefield
- **Comment**: None
- **Journal**: None
- **Summary**: Current deep domain adaptation methods used in computer vision have mainly focused on learning discriminative and domain-invariant features across different domains. In this paper, we present a novel approach that bridges the domain gap by projecting the source and target domains into a common association space through an unsupervised ``cross-grafted representation stacking'' (CGRS) mechanism. Specifically, we construct variational auto-encoders (VAE) for the two domains, and form bidirectional associations by cross-grafting the VAEs' decoder stacks. Furthermore, generative adversarial networks (GAN) are employed for label alignment (LA), mapping the target domain data to the known label space of the source domain. The overall adaptation process hence consists of three phases: feature representation learning by VAEs, association generation, and association label alignment by GANs. Experimental results demonstrate that our CGRS-LA approach outperforms the state-of-the-art on a number of unsupervised domain adaptation benchmarks.



### Semantically Interpretable and Controllable Filter Sets
- **Arxiv ID**: http://arxiv.org/abs/1902.06334v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1902.06334v1)
- **Published**: 2019-02-17 21:49:41+00:00
- **Updated**: 2019-02-17 21:49:41+00:00
- **Authors**: Mohit Prabhushankar, Gukyeong Kwon, Dogancan Temel, Ghassan AlRegib
- **Comment**: 5 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: In this paper, we generate and control semantically interpretable filters that are directly learned from natural images in an unsupervised fashion. Each semantic filter learns a visually interpretable local structure in conjunction with other filters. The significance of learning these interpretable filter sets is demonstrated on two contrasting applications. The first application is image recognition under progressive decolorization, in which recognition algorithms should be color-insensitive to achieve a robust performance. The second application is image quality assessment where objective methods should be sensitive to color degradations. In the proposed work, the sensitivity and lack thereof are controlled by weighing the semantic filters based on the local structures they represent. To validate the proposed approach, we utilize the CURE-TSR dataset for image recognition and the TID 2013 dataset for image quality assessment. We show that the proposed semantic filter set achieves state-of-the-art performances in both datasets while maintaining its robustness across progressive distortions.



### Accurate Segmentation of Dermoscopic Images based on Local Binary Pattern Clustering
- **Arxiv ID**: http://arxiv.org/abs/1902.06347v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1902.06347v3)
- **Published**: 2019-02-17 23:12:15+00:00
- **Updated**: 2019-02-20 18:23:51+00:00
- **Authors**: Pedro M. M. Pereira, Rui Fonseca-Pinto, Rui Pedro Paiva, Luis M. N. Tavora, Pedro A. A. Assuncao, Sergio M. M. de Faria
- **Comment**: submitted to MIPRO DC 2019
- **Journal**: None
- **Summary**: Segmentation is a key stage in dermoscopic image processing, where the accuracy of the border line that defines skin lesions is of utmost importance for subsequent algorithms (e.g., classification) and computer-aided early diagnosis of serious medical conditions. This paper proposes a novel segmentation method based on Local Binary Patterns (LBP), where LBP and K-Means clustering are combined to achieve a detailed delineation in dermoscopic images. In comparison with usual dermatologist-like segmentation (i.e., the available ground-truth), the proposed method is capable of finding more realistic borders of skin lesions, i.e., with much more detail. The results also exhibit reduced variability amongst different performance measures and they are consistent across different images. The proposed method can be applied for cell-based like segmentation adapted to the lesion border growing specificities. Hence, the method is suitable to follow the growth dynamics associated with the lesion border geometry in skin melanocytic images.



