# Arxiv Papers in cs.CV on 2019-02-07
### Empirically Accelerating Scaled Gradient Projection Using Deep Neural Network For Inverse Problems In Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1902.02449v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.02449v3)
- **Published**: 2019-02-07 02:19:53+00:00
- **Updated**: 2021-04-22 03:00:39+00:00
- **Authors**: Byung Hyun Lee, Se Young Chun
- **Comment**: 10 pages, 6 figures, 3 tables, ICASSP 2021, this is a long version of
  it
- **Journal**: None
- **Summary**: Recently, deep neural networks (DNNs) have shown advantages in accelerating optimization algorithms. One approach is to unfold finite number of iterations of conventional optimization algorithms and to learn parameters in the algorithms. However, these are forward methods and are indeed neither iterative nor convergent. Here, we present a novel DNN-based convergent iterative algorithm that accelerates conventional optimization algorithms. We train a DNN to yield parameters in scaled gradient projection method. So far, these parameters have been chosen heuristically, but have shown to be crucial for good empirical performance. In simulation results, the proposed method significantly improves the empirical convergence rate over conventional optimization methods for various large-scale inverse problems in image processing.



### Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images
- **Arxiv ID**: http://arxiv.org/abs/1902.02452v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02452v2)
- **Published**: 2019-02-07 02:44:55+00:00
- **Updated**: 2019-09-06 11:33:32+00:00
- **Authors**: Magauiya Zhussip, Shakarim Soltanayev, Se Young Chun
- **Comment**: 10 pages, 2 figures
- **Journal**: None
- **Summary**: Recently, Stein's unbiased risk estimator (SURE) has been applied to unsupervised training of deep neural network Gaussian denoisers that outperformed classical non-deep learning based denoisers and yielded comparable performance to those trained with ground truth. While SURE requires only one noise realization per image for training, it does not take advantage of having multiple noise realizations per image when they are available (e.g., two uncorrelated noise realizations per image for Noise2Noise). Here, we propose an extended SURE (eSURE) to train deep denoisers with correlated pairs of noise realizations per image and applied it to the case with two uncorrelated realizations per image to achieve better performance than SURE based method and comparable results to Noise2Noise. Then, we further investigated the case with imperfect ground truth (i.e., mild noise in ground truth) that may be obtained considering painstaking, time-consuming, and even expensive processes of collecting ground truth images with multiple noisy images. For the case of generating noisy training data by adding synthetic noise to imperfect ground truth to yield correlated pairs of images, our proposed eSURE based training method outperformed conventional SURE based method as well as Noise2Noise.



### Towards Automatic Concept-based Explanations
- **Arxiv ID**: http://arxiv.org/abs/1902.03129v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03129v3)
- **Published**: 2019-02-07 03:18:54+00:00
- **Updated**: 2019-10-08 09:28:43+00:00
- **Authors**: Amirata Ghorbani, James Wexler, James Zou, Been Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions.   Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for \emph{concept} based explanation, which goes beyond per-sample features to identify higher-level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that \alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.



### A Simple Baseline for Bayesian Uncertainty in Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.02476v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.02476v2)
- **Published**: 2019-02-07 05:15:46+00:00
- **Updated**: 2019-12-31 08:28:19+00:00
- **Authors**: Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, Andrew Gordon Wilson
- **Comment**: Published at NeurIPS 2019
- **Journal**: None
- **Summary**: We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.



### CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.02497v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02497v2)
- **Published**: 2019-02-07 07:17:03+00:00
- **Updated**: 2019-12-13 19:01:48+00:00
- **Authors**: Xinrui Cui, Dan Wang, Z. Jane Wang
- **Comment**: 15 pages, 10 figures
- **Journal**: None
- **Summary**: With the widespread applications of deep convolutional neural networks (DCNNs), it becomes increasingly important for DCNNs not only to make accurate predictions but also to explain how they make their decisions. In this work, we propose a CHannel-wise disentangled InterPretation (CHIP) model to give the visual interpretation to the predictions of DCNNs. The proposed model distills the class-discriminative importance of channels in networks by utilizing the sparse regularization. Here, we first introduce the network perturbation technique to learn the model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present the class-discriminative visual interpretation for specific predictions of networks. It is noteworthy that the proposed model is able to interpret different layers of networks without re-training. By combining the distilled interpretation knowledge in different layers, we further propose the Refined CHIP visual interpretation that is both high-resolution and class-discriminative. Experimental results on the standard dataset demonstrate that the proposed model provides promising visual interpretation for the predictions of networks in image classification task compared with existing visual interpretation methods. Besides, the proposed method outperforms related approaches in the application of ILSVRC 2015 weakly-supervised localization task.



### Advances on CNN-based super-resolution of Sentinel-2 images
- **Arxiv ID**: http://arxiv.org/abs/1902.02513v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02513v1)
- **Published**: 2019-02-07 08:09:46+00:00
- **Updated**: 2019-02-07 08:09:46+00:00
- **Authors**: Massimiliano Gargiulo
- **Comment**: None
- **Journal**: None
- **Summary**: Thanks to their temporal-spatial coverage and free access, Sentinel-2 images are very interesting for the community. However, a relatively coarse spatial resolution, compared to that of state-of-the-art commercial products, motivates the study of super-resolution techniques to mitigate such a limitation. Specifically, thirtheen bands are sensed simultaneously but at different spatial resolutions: 10, 20, and 60 meters depending on the spectral location. Here, building upon our previous convolutional neural network (CNN) based method, we propose an improved CNN solution to super-resolve the 20-m resolution bands benefiting spatial details conveyed by the accompanying 10-m spectral bands.



### DoPAMINE: Double-sided Masked CNN for Pixel Adaptive Multiplicative Noise Despeckling
- **Arxiv ID**: http://arxiv.org/abs/1902.02530v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02530v1)
- **Published**: 2019-02-07 09:08:18+00:00
- **Updated**: 2019-02-07 09:08:18+00:00
- **Authors**: Sunghwan Joo, Sungmin Cha, Taesup Moon
- **Comment**: AAAI 2019 Camera Ready Version
- **Journal**: None
- **Summary**: We propose DoPAMINE, a new neural network based multiplicative noise despeckling algorithm. Our algorithm is inspired by Neural AIDE (N-AIDE), which is a recently proposed neural adaptive image denoiser. While the original N-AIDE was designed for the additive noise case, we show that the same framework, i.e., adaptively learning a network for pixel-wise affine denoisers by minimizing an unbiased estimate of MSE, can be applied to the multiplicative noise case as well. Moreover, we derive a double-sided masked CNN architecture which can control the variance of the activation values in each layer and converge fast to high denoising performance during supervised training. In the experimental results, we show our DoPAMINE possesses high adaptivity via fine-tuning the network parameters based on the given noisy image and achieves significantly better despeckling results compared to SAR-DRN, a state-of-the-art CNN-based algorithm.



### Online Clustering by Penalized Weighted GMM
- **Arxiv ID**: http://arxiv.org/abs/1902.02544v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.02544v1)
- **Published**: 2019-02-07 09:50:08+00:00
- **Updated**: 2019-02-07 09:50:08+00:00
- **Authors**: Shlomo Bugdary, Shay Maymon
- **Comment**: None
- **Journal**: None
- **Summary**: With the dawn of the Big Data era, data sets are growing rapidly. Data is streaming from everywhere - from cameras, mobile phones, cars, and other electronic devices. Clustering streaming data is a very challenging problem. Unlike the traditional clustering algorithms where the dataset can be stored and scanned multiple times, clustering streaming data has to satisfy constraints such as limit memory size, real-time response, unknown data statistics and an unknown number of clusters. In this paper, we present a novel online clustering algorithm which can be used to cluster streaming data without knowing the number of clusters a priori. Results on both synthetic and real datasets show that the proposed algorithm produces partitions which are close to what you could get if you clustered the whole data at one time.



### Fully Convolutional Neural Network for Semantic Segmentation of Anatomical Structure and Pathologies in Colour Fundus Images Associated with Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/1902.03122v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03122v1)
- **Published**: 2019-02-07 10:37:40+00:00
- **Updated**: 2019-02-07 10:37:40+00:00
- **Authors**: Oindrila Saha, Rachana Sathish, Debdoot Sheet
- **Comment**: arXiv admin note: text overlap with arXiv:1511.00561 by other authors
- **Journal**: None
- **Summary**: Diabetic retinopathy (DR) is the most common form of diabetic eye disease. Retinopathy can affect all diabetic patients and becomes particularly dangerous, increasing the risk of blindness, if it is left untreated. The success rate of its curability solemnly depends on diagnosis at an early stage. The development of automated computer aided disease diagnosis tools could help in faster detection of symptoms with a wider reach and reasonable cost. This paper proposes a method for the automated segmentation of retinal lesions and optic disk in fundus images using a deep fully convolutional neural network for semantic segmentation. This trainable segmentation pipeline consists of an encoder network, a corresponding decoder network followed by pixel-wise classification to segment microaneurysms, hemorrhages, hard exudates, soft exudates, optic disk from background. The network was trained using Binary cross entropy criterion with Sigmoid as the last layer, while during an additional SoftMax layer was used for boosting response of single class. The performance of the proposed method is evaluated using sensitivity, positive prediction value (PPV) and accuracy as the metrices. Further, the position of the Optic disk is localised using the segmented output map.



### License Plate Recognition with Compressive Sensing Based Feature Extraction
- **Arxiv ID**: http://arxiv.org/abs/1902.05386v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1902.05386v1)
- **Published**: 2019-02-07 11:58:56+00:00
- **Updated**: 2019-02-07 11:58:56+00:00
- **Authors**: Andrej Jokic, Nikola Vukovic
- **Comment**: Student paper submitted to The 8th Mediterranean Conference on
  Embedded Computing - MECO'2019
- **Journal**: None
- **Summary**: License plate recognition is the key component to many automatic traffic control systems. It enables the automatic identification of vehicles in many applications. Such systems must be able to identify vehicles from images taken in various conditions including low light, rain, snow, etc. In order to reduce the complexity and cost of the hardware required for such devices, the algorithm should be as efficient as possible. This paper proposes a license plate recognition system which uses a new approach based on compressive sensing techniques for dimensionality reduction and feature extraction. Dimensionality reduction will enable precise classification with less training data while demanding less computational power. Based on the extracted features, character recognition and classification is done by a Support Vector Machine classifier.



### Illumination Invariant Foreground Object Segmentation using ForeGANs
- **Arxiv ID**: http://arxiv.org/abs/1902.03120v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03120v3)
- **Published**: 2019-02-07 12:01:56+00:00
- **Updated**: 2019-10-05 08:36:44+00:00
- **Authors**: Maryam Sultana, Soon Ki Jung
- **Comment**: 3 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1811.01526
- **Journal**: None
- **Summary**: The foreground segmentation algorithms suffer performance degradation in the presence of various challenges such as dynamic backgrounds, and various illumination conditions. To handle these challenges, we present a foreground segmentation method, based on generative adversarial network (GAN). We aim to segment foreground objects in the presence of two aforementioned major challenges in background scenes in real environments. To address this problem, our presented GAN model is trained on background image samples with dynamic changes, after that for testing the GAN model has to generate the same background sample as test sample with similar conditions via back-propagation technique. The generated background sample is then subtracted from the given test sample to segment foreground objects. The comparison of our proposed method with five state-of-the-art methods highlights the strength of our algorithm for foreground segmentation in the presence of challenging dynamic background scenario.



### Unsupervised Data Uncertainty Learning in Visual Retrieval Systems
- **Arxiv ID**: http://arxiv.org/abs/1902.02586v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02586v1)
- **Published**: 2019-02-07 12:31:50+00:00
- **Updated**: 2019-02-07 12:31:50+00:00
- **Authors**: Ahmed Taha, Yi-Ting Chen, Teruhisa Misu, Abhinav Shrivastava, Larry Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce an unsupervised formulation to estimate heteroscedastic uncertainty in retrieval systems. We propose an extension to triplet loss that models data uncertainty for each input. Besides improving performance, our formulation models local noise in the embedding space. It quantifies input uncertainty and thus enhances interpretability of the system. This helps identify noisy observations in query and search databases. Evaluation on both image and video retrieval applications highlight the utility of our approach. We highlight our efficiency in modeling local noise using two real-world datasets: Clothing1M and Honda Driving datasets. Qualitative results illustrate our ability in identifying confusing scenarios in various domains. Uncertainty learning also enables data cleaning by detecting noisy training labels.



### Beholder-GAN: Generation and Beautification of Facial Images with Conditioning on Their Beauty Level
- **Arxiv ID**: http://arxiv.org/abs/1902.02593v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02593v3)
- **Published**: 2019-02-07 12:44:49+00:00
- **Updated**: 2019-02-25 20:33:47+00:00
- **Authors**: Nir Diamant, Dean Zadok, Chaim Baskin, Eli Schwartz, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Beauty is in the eye of the beholder. This maxim, emphasizing the subjectivity of the perception of beauty, has enjoyed a wide consensus since ancient times. In the digitalera, data-driven methods have been shown to be able to predict human-assigned beauty scores for facial images. In this work, we augment this ability and train a generative model that generates faces conditioned on a requested beauty score. In addition, we show how this trained generator can be used to beautify an input face image. By doing so, we achieve an unsupervised beautification model, in the sense that it relies on no ground truth target images.



### Matrix Cofactorization for Joint Representation Learning and Supervised Classification -- Application to Hyperspectral Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.02597v4
- **DOI**: 10.1016/j.neucom.2019.12.068
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.02597v4)
- **Published**: 2019-02-07 12:54:56+00:00
- **Updated**: 2020-02-13 18:53:51+00:00
- **Authors**: Adrien Lagrange, Mathieu Fauvel, Stéphane May, José Bioucas-Dias, Nicolas Dobigeon
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised classification and representation learning are two widely used classes of methods to analyze multivariate images. Although complementary, these methods have been scarcely considered jointly in a hierarchical modeling. In this paper, a method coupling these two approaches is designed using a matrix cofactorization formulation. Each task is modeled as a factorization matrix problem and a term relating both coding matrices is then introduced to drive an appropriate coupling. The link can be interpreted as a clustering operation over a low-dimensional representation vectors. The attribution vectors of the clustering are then used as features vectors for the classification task, i.e., the coding vectors of the corresponding factorization problem. A proximal gradient descent algorithm, ensuring convergence to a critical point of the objective function, is then derived to solve the resulting non-convex non-smooth optimization problem. An evaluation of the proposed method is finally conducted both on synthetic and real data in the specific context of hyperspectral image interpretation, unifying two standard analysis techniques, namely unmixing and classification.



### Single Network Panoptic Segmentation for Street Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1902.02678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02678v1)
- **Published**: 2019-02-07 15:18:57+00:00
- **Updated**: 2019-02-07 15:18:57+00:00
- **Authors**: Daan de Geus, Panagiotis Meletis, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we propose a single deep neural network for panoptic segmentation, for which the goal is to provide each individual pixel of an input image with a class label, as in semantic segmentation, as well as a unique identifier for specific objects in an image, following instance segmentation. Our network makes joint semantic and instance segmentation predictions and combines these to form an output in the panoptic format. This has two main benefits: firstly, the entire panoptic prediction is made in one pass, reducing the required computation time and resources; secondly, by learning the tasks jointly, information is shared between the two tasks, thereby improving performance. Our network is evaluated on two street scene datasets: Cityscapes and Mapillary Vistas. By leveraging information exchange and improving the merging heuristics, we increase the performance of the single network, and achieve a score of 23.9 on the Panoptic Quality (PQ) metric on Mapillary Vistas validation, with an input resolution of 640 x 900 pixels. On Cityscapes validation, our method achieves a PQ score of 45.9 with an input resolution of 512 x 1024 pixels. Moreover, our method decreases the prediction time by a factor of 2 with respect to separate networks.



### StampNet: unsupervised multi-class object discovery
- **Arxiv ID**: http://arxiv.org/abs/1902.02693v1
- **DOI**: 10.1109/ICIP.2019.8803767
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02693v1)
- **Published**: 2019-02-07 15:35:55+00:00
- **Updated**: 2019-02-07 15:35:55+00:00
- **Authors**: Joost Visser, Alessandro Corbetta, Vlado Menkovski, Federico Toschi
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP 2019), pp.
  2951-2955, 2019
- **Summary**: Unsupervised object discovery in images involves uncovering recurring patterns that define objects and discriminates them against the background. This is more challenging than image clustering as the size and the location of the objects are not known: this adds additional degrees of freedom and increases the problem complexity. In this work, we propose StampNet, a novel autoencoding neural network that localizes shapes (objects) over a simple background in images and categorizes them simultaneously. StampNet consists of a discrete latent space that is used to categorize objects and to determine the location of the objects. The object categories are formed during the training, resulting in the discovery of a fixed set of objects. We present a set of experiments that demonstrate that StampNet is able to localize and cluster multiple overlapping shapes with varying complexity including the digits from the MNIST dataset. We also present an application of StampNet in the localization of pedestrians in overhead depth-maps.



### Virtual Training for a Real Application: Accurate Object-Robot Relative Localization without Calibration
- **Arxiv ID**: http://arxiv.org/abs/1902.02711v1
- **DOI**: 10.1007/s11263-018-1102-6
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02711v1)
- **Published**: 2019-02-07 16:17:01+00:00
- **Updated**: 2019-02-07 16:17:01+00:00
- **Authors**: Vianney Loing, Renaud Marlet, Mathieu Aubry
- **Comment**: None
- **Journal**: Int J Comput Vis (2018) 126: 1045
- **Summary**: Localizing an object accurately with respect to a robot is a key step for autonomous robotic manipulation. In this work, we propose to tackle this task knowing only 3D models of the robot and object in the particular case where the scene is viewed from uncalibrated cameras -- a situation which would be typical in an uncontrolled environment, e.g., on a construction site. We demonstrate that this localization can be performed very accurately, with millimetric errors, without using a single real image for training, a strong advantage since acquiring representative training data is a long and expensive process. Our approach relies on a classification Convolutional Neural Network (CNN) trained using hundreds of thousands of synthetically rendered scenes with randomized parameters. To evaluate our approach quantitatively and make it comparable to alternative approaches, we build a new rich dataset of real robot images with accurately localized blocks.



### Reversible GANs for Memory-efficient Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1902.02729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02729v1)
- **Published**: 2019-02-07 17:03:05+00:00
- **Updated**: 2019-02-07 17:03:05+00:00
- **Authors**: Tycho F. A. van der Ouderaa, Daniel E. Worrall
- **Comment**: None
- **Journal**: None
- **Summary**: The Pix2pix and CycleGAN losses have vastly improved the qualitative and quantitative visual quality of results in image-to-image translation tasks. We extend this framework by exploring approximately invertible architectures which are well suited to these losses. These architectures are approximately invertible by design and thus partially satisfy cycle-consistency before training even begins. Furthermore, since invertible architectures have constant memory complexity in depth, these models can be built arbitrarily deep. We are able to demonstrate superior quantitative output on the Cityscapes and Maps datasets at near constant memory budget.



### Neural Inverse Knitting: From Images to Manufacturing Instructions
- **Arxiv ID**: http://arxiv.org/abs/1902.02752v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02752v2)
- **Published**: 2019-02-07 18:05:17+00:00
- **Updated**: 2019-06-19 19:32:16+00:00
- **Authors**: Alexandre Kaspar, Tae-Hyun Oh, Liane Makatura, Petr Kellnhofer, Jacqueline Aslarus, Wojciech Matusik
- **Comment**: Project page: http://deepknitting.csail.mit.edu
- **Journal**: None
- **Summary**: Motivated by the recent potential of mass customization brought by whole-garment knitting machines, we introduce the new problem of automatic machine instruction generation using a single image of the desired physical product, which we apply to machine knitting. We propose to tackle this problem by directly learning to synthesize regular machine instructions from real images. We create a cured dataset of real samples with their instruction counterpart and propose to use synthetic images to augment it in a novel way. We theoretically motivate our data mixing framework and show empirical results suggesting that making real images look more synthetic is beneficial in our problem setup.



### FDDB-360: Face Detection in 360-degree Fisheye Images
- **Arxiv ID**: http://arxiv.org/abs/1902.02777v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02777v1)
- **Published**: 2019-02-07 18:59:10+00:00
- **Updated**: 2019-02-07 18:59:10+00:00
- **Authors**: Jianglin Fu, Saeed Ranjbar Alvar, Ivan V. Bajic, Rodney G. Vaughan
- **Comment**: None
- **Journal**: None
- **Summary**: 360-degree cameras offer the possibility to cover a large area, for example an entire room, without using multiple distributed vision sensors. However, geometric distortions introduced by their lenses make computer vision problems more challenging. In this paper we address face detection in 360-degree fisheye images. We show how a face detector trained on regular images can be re-trained for this purpose, and we also provide a 360-degree fisheye-like version of the popular FDDB face detection dataset, which we call FDDB-360.



### SiamVGG: Visual Tracking using Deeper Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.02804v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02804v4)
- **Published**: 2019-02-07 19:08:34+00:00
- **Updated**: 2022-07-04 04:20:23+00:00
- **Authors**: Yuhong Li, Xiaofan Zhang, Deming Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, we have seen a rapid development of Deep Neural Network (DNN) based visual tracking solutions. Some trackers combine the DNN-based solutions with Discriminative Correlation Filters (DCF) to extract semantic features and successfully deliver the state-of-the-art tracking accuracy. However, these solutions are highly compute-intensive, which require long processing time, resulting unsecured real-time performance. To deliver both high accuracy and reliable real-time performance, we propose a novel tracker called SiamVGG\footnote{https://github.com/leeyeehoo/SiamVGG}. It combines a Convolutional Neural Network (CNN) backbone and a cross-correlation operator, and takes advantage of the features from exemplary images for more accurate object tracking. The architecture of SiamVGG is customized from VGG-16 with the parameters shared by both exemplary images and desired input video frames. We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017 datasets with the state-of-the-art accuracy while maintaining a decent real-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve 2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in VOT2017 Challenge.



### Robustness Of Saak Transform Against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1902.02826v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02826v1)
- **Published**: 2019-02-07 20:11:03+00:00
- **Updated**: 2019-02-07 20:11:03+00:00
- **Authors**: Thiyagarajan Ramanathan, Abinaya Manimaran, Suya You, C-C Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Image classification is vulnerable to adversarial attacks. This work investigates the robustness of Saak transform against adversarial attacks towards high performance image classification. We develop a complete image classification system based on multi-stage Saak transform. In the Saak transform domain, clean and adversarial images demonstrate different distributions at different spectral dimensions. Selection of the spectral dimensions at every stage can be viewed as an automatic denoising process. Motivated by this observation, we carefully design strategies of feature extraction, representation and classification that increase adversarial robustness. The performances with well-known datasets and attacks are demonstrated by extensive experimental evaluations.



### Evaluating Crowd Density Estimators via Their Uncertainty Bounds
- **Arxiv ID**: http://arxiv.org/abs/1902.02831v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02831v1)
- **Published**: 2019-02-07 20:22:15+00:00
- **Updated**: 2019-02-07 20:22:15+00:00
- **Authors**: Jennifer Vandoni, Emanuel Aldea, Sylvie Le Hégarat-Mascle
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we use the Belief Function Theory which extends the probabilistic framework in order to provide uncertainty bounds to different categories of crowd density estimators. Our method allows us to compare the multi-scale performance of the estimators, and also to characterize their reliability for crowd monitoring applications requiring varying degrees of prudence.



### 3D Human Pose Estimation from Deep Multi-View 2D Pose
- **Arxiv ID**: http://arxiv.org/abs/1902.02841v1
- **DOI**: 10.1109/ICPR.2018.8545631
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02841v1)
- **Published**: 2019-02-07 20:55:19+00:00
- **Updated**: 2019-02-07 20:55:19+00:00
- **Authors**: Steven Schwarcz, Thomas Pollard
- **Comment**: None
- **Journal**: 2018 24th International Conference on Pattern Recognition (ICPR),
  Beijing, 2018, pp. 2326-2331
- **Summary**: Human pose estimation - the process of recognizing a human's limb positions and orientations in a video - has many important applications including surveillance, diagnosis of movement disorders, and computer animation. While deep learning has lead to great advances in 2D and 3D pose estimation from single video sources, the problem of estimating 3D human pose from multiple video sensors with overlapping fields of view has received less attention. When the application allows use of multiple cameras, 3D human pose estimates may be greatly improved through fusion of multi-view pose estimates and observation of limbs that are fully or partially occluded in some views. Past approaches to multi-view 3D pose estimation have used probabilistic graphical models to reason over constraints, including per-image pose estimates, temporal smoothness, and limb length. In this paper, we present a pipeline for multi-view 3D pose estimation of multiple individuals which combines a state-of-art 2D pose detector with a factor graph of 3D limb constraints optimized with belief propagation. We evaluate our results on the TUM-Campus and Shelf datasets for multi-person 3D pose estimation and show that our system significantly out-performs the previous state-of-the-art with a simpler model of limb dependency.



### FaceSpoof Buster: a Presentation Attack Detector Based on Intrinsic Image Properties and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.02845v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02845v1)
- **Published**: 2019-02-07 21:11:07+00:00
- **Updated**: 2019-02-07 21:11:07+00:00
- **Authors**: Rodrigo Bresan, Allan Pinto, Anderson Rocha, Carlos Beluzo, Tiago Carvalho
- **Comment**: 7 pages, 1 figure, 7 tables
- **Journal**: None
- **Summary**: Nowadays, the adoption of face recognition for biometric authentication systems is usual, mainly because this is one of the most accessible biometric modalities. Techniques that rely on trespassing these kind of systems by using a forged biometric sample, such as a printed paper or a recorded video of a genuine access, are known as presentation attacks, but may be also referred in the literature as face spoofing. Presentation attack detection is a crucial step for preventing this kind of unauthorized accesses into restricted areas and/or devices. In this paper, we propose a novel approach which relies in a combination between intrinsic image properties and deep neural networks to detect presentation attack attempts. Our method explores depth, salience and illumination maps, associated with a pre-trained Convolutional Neural Network in order to produce robust and discriminant features. Each one of these properties are individually classified and, in the end of the process, they are combined by a meta learning classifier, which achieves outstanding results on the most popular datasets for PAD. Results show that proposed method is able to overpass state-of-the-art results in an inter-dataset protocol, which is defined as the most challenging in the literature.



### Visual search and recognition for robot task execution and monitoring
- **Arxiv ID**: http://arxiv.org/abs/1902.02870v1
- **DOI**: 10.3233/978-1-61499-929-4-94
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.02870v1)
- **Published**: 2019-02-07 22:35:51+00:00
- **Updated**: 2019-02-07 22:35:51+00:00
- **Authors**: Lorenzo Mauro, Francesco Puja, Simone Grazioso, Valsamis Ntouskos, Marta Sanzari, Edoardo Alati, Fiora Pirri
- **Comment**: None
- **Journal**: Frontiers in Artificial Intelligence and Applications 310 (2018)
  94-109
- **Summary**: Visual search of relevant targets in the environment is a crucial robot skill. We propose a preliminary framework for the execution monitor of a robot task, taking care of the robot attitude to visually searching the environment for targets involved in the task. Visual search is also relevant to recover from a failure. The framework exploits deep reinforcement learning to acquire a "common sense" scene structure and it takes advantage of a deep convolutional network to detect objects and relevant relations holding between them. The framework builds on these methods to introduce a vision-based execution monitoring, which uses classical planning as a backbone for task execution. Experiments show that with the proposed vision-based execution monitor the robot can complete simple tasks and can recover from failures in autonomy.



### Deep execution monitor for robot assistive tasks
- **Arxiv ID**: http://arxiv.org/abs/1902.02877v1
- **DOI**: 10.1007/978-3-030-11024-6_11
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.02877v1)
- **Published**: 2019-02-07 23:02:47+00:00
- **Updated**: 2019-02-07 23:02:47+00:00
- **Authors**: Lorenzo Mauro, Edoardo Alati, Marta Sanzari, Valsamis Ntouskos, Gianluca Massimiani, Fiora Pirri
- **Comment**: None
- **Journal**: None
- **Summary**: We consider a novel approach to high-level robot task execution for a robot assistive task. In this work we explore the problem of learning to predict the next subtask by introducing a deep model for both sequencing goals and for visually evaluating the state of a task. We show that deep learning for monitoring robot tasks execution very well supports the interconnection between task-level planning and robot operations. These solutions can also cope with the natural non-determinism of the execution monitor. We show that a deep execution monitor leverages robot performance. We measure the improvement taking into account some robot helping tasks performed at a warehouse.



### HYDRA: Hybrid Deep Magnetic Resonance Fingerprinting
- **Arxiv ID**: http://arxiv.org/abs/1902.02882v2
- **DOI**: 10.1002/mp.13727
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1902.02882v2)
- **Published**: 2019-02-07 23:15:24+00:00
- **Updated**: 2019-08-01 15:44:04+00:00
- **Authors**: Pingfan Song, Yonina C. Eldar, Gal Mazor, Miguel Rodrigues
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Magnetic resonance fingerprinting (MRF) methods typically rely on dictio-nary matching to map the temporal MRF signals to quantitative tissue parameters. Such approaches suffer from inherent discretization errors, as well as high computational complexity as the dictionary size grows. To alleviate these issues, we propose a HYbrid Deep magnetic ResonAnce fingerprinting approach, referred to as HYDRA.   Methods: HYDRA involves two stages: a model-based signature restoration phase and a learning-based parameter restoration phase. Signal restoration is implemented using low-rank based de-aliasing techniques while parameter restoration is performed using a deep nonlocal residual convolutional neural network. The designed network is trained on synthesized MRF data simulated with the Bloch equations and fast imaging with steady state precession (FISP) sequences. In test mode, it takes a temporal MRF signal as input and produces the corresponding tissue parameters.   Results: We validated our approach on both synthetic data and anatomical data generated from a healthy subject. The results demonstrate that, in contrast to conventional dictionary-matching based MRF techniques, our approach significantly improves inference speed by eliminating the time-consuming dictionary matching operation, and alleviates discretization errors by outputting continuous-valued parameters. We further avoid the need to store a large dictionary, thus reducing memory requirements.   Conclusions: Our approach demonstrates advantages in terms of inference speed, accuracy and storage requirements over competing MRF methods



