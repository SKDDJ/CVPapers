# Arxiv Papers in cs.CV on 2019-02-13
### Forensic Similarity for Digital Images
- **Arxiv ID**: http://arxiv.org/abs/1902.04684v2
- **DOI**: 10.1109/TIFS.2019.2924552
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.04684v2)
- **Published**: 2019-02-13 00:21:02+00:00
- **Updated**: 2019-07-19 18:15:23+00:00
- **Authors**: Owen Mayer, Matthew C. Stamm
- **Comment**: 16 pages, Accepted for publication with IEEE T-IFS (IEEE Transactions
  on Information Forensics and Security, 2019)
- **Journal**: IEEE Transactions on Information Forensics and Security (2019)
  Volume: 15, Pages: 1331 - 1346
- **Summary**: In this paper we introduce a new digital image forensics approach called forensic similarity, which determines whether two image patches contain the same forensic trace or different forensic traces. One benefit of this approach is that prior knowledge, e.g. training samples, of a forensic trace are not required to make a forensic similarity decision on it in the future. To do this, we propose a two part deep-learning system composed of a CNN-based feature extractor and a three-layer neural network, called the similarity network. This system maps pairs of image patches to a score indicating whether they contain the same or different forensic traces. We evaluated system accuracy of determining whether two image patches were 1) captured by the same or different camera model, 2) manipulated by the same or different editing operation, and 3) manipulated by the same or different manipulation parameter, given a particular editing operation. Experiments demonstrate applicability to a variety of forensic traces, and importantly show efficacy on "unknown" forensic traces that were not used to train the system. Experiments also show that the proposed system significantly improves upon prior art, reducing error rates by more than half. Furthermore, we demonstrated the utility of the forensic similarity approach in two practical applications: forgery detection and localization, and database consistency verification.



### Automated Detection of Pre-Disaster Building Images from Google Street View
- **Arxiv ID**: http://arxiv.org/abs/1902.10816v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10816v1)
- **Published**: 2019-02-13 01:14:23+00:00
- **Updated**: 2019-02-13 01:14:23+00:00
- **Authors**: Chul Min Yeum, Ali Lenjani, Shirley J. Dyke, Ilias Bilionis
- **Comment**: None
- **Journal**: None
- **Summary**: After a disaster, teams of structural engineers collect vast amounts of images from damaged buildings to obtain lessons and gain knowledge from the event. Images of damaged buildings and components provide valuable evidence to understand the consequences on our structures. However, in many cases, images of damaged buildings are often captured without sufficient spatial context. Also, they may be hard to recognize in cases with severe damage. Incorporating past images showing a pre-disaster condition of such buildings is helpful to accurately evaluate possible circumstances related to a building's failure. One of the best resources to observe the pre-disaster condition of the buildings is Google Street View. A sequence of 360 panorama images which are captured along streets enables all-around views at each location on the street. Once a user knows the GPS information near the building, all external views of the building can be made available. In this study, we develop an automated technique to extract past building images from 360 panorama images serviced by Google Street View. Users only need to provide a geo-tagged image, collected near the target building, and the rest of the process is fully automated. High-quality and undistorted building images are extracted from past panoramas. Since the panoramas are collected from various locations near the building along the street, the user can identify its pre-disaster conditions from the full set of external views.



### Self-adaptive Single and Multi-illuminant Estimation Framework based on Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.04705v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04705v1)
- **Published**: 2019-02-13 02:12:55+00:00
- **Updated**: 2019-02-13 02:12:55+00:00
- **Authors**: Yongjie Liu, Sijie Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Illuminant estimation plays a key role in digital camera pipeline system, it aims at reducing color casting effect due to the influence of non-white illuminant. Recent researches handle this task by using Convolution Neural Network (CNN) as a mapping function from input image to a single illumination vector. However, global mapping approaches are difficult to deal with scenes under multi-light-sources. In this paper, we proposed a self-adaptive single and multi-illuminant estimation framework, which includes the following novelties: (1) Learning local self-adaptive kernels from the entire image for illuminant estimation with encoder-decoder CNN structure; (2) Providing confidence measurement for the prediction; (3) Clustering-based iterative fitting for computing single and multi-illumination vectors. The proposed global-to-local aggregation is able to predict multi-illuminant regionally by utilizing global information instead of training in patches, as well as brings significant improvement for single illuminant estimation. We outperform the state-of-the-art methods on standard benchmarks with the largest relative improvement of 16%. In addition, we collect a dataset contains over 13k images for illuminant estimation and evaluation. The code and dataset is available on https://github.com/LiamLYJ/KPF_WB



### Predicting Food Security Outcomes Using Convolutional Neural Networks (CNNs) for Satellite Tasking
- **Arxiv ID**: http://arxiv.org/abs/1902.05433v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05433v2)
- **Published**: 2019-02-13 02:22:12+00:00
- **Updated**: 2019-04-25 19:47:36+00:00
- **Authors**: Swetava Ganguli, Jared Dunnmon, Darren Hau
- **Comment**: Research performed as part of the Sustainability and Artificial
  Intelligence Laboratory (SAIL) at Stanford University. Second revised version
  corrects typographical errors and adds a few references
- **Journal**: None
- **Summary**: Obtaining reliable data describing local Food Security Metrics (FSM) at a granularity that is informative to policy-makers requires expensive and logistically difficult surveys, particularly in the developing world. We train a CNN on publicly available satellite data describing land cover classification and use both transfer learning and direct training to build a model for FSM prediction purely from satellite imagery data. We then propose efficient tasking algorithms for high resolution satellite assets via transfer learning, Markovian search algorithms, and Bayesian networks.



### Automated Segmentation of the Optic Disk and Cup using Dual-Stage Fully Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.04713v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04713v1)
- **Published**: 2019-02-13 02:29:59+00:00
- **Updated**: 2019-02-13 02:29:59+00:00
- **Authors**: Lei Bi, Yuyu Guo, Qian Wang, Dagan Feng, Michael Fulham, Jinman Kim
- **Comment**: REFUGE Challenge
- **Journal**: None
- **Summary**: Automated segmentation of the optic cup and disk on retinal fundus images is fundamental for the automated detection / analysis of glaucoma. Traditional segmentation approaches depend heavily upon hand-crafted features and a priori knowledge of the user. As such, these methods are difficult to be adapt to the clinical environment. Recently, deep learning methods based on fully convolutional networks (FCNs) have been successful in resolving segmentation problems. However, the reliance on large annotated training data is problematic when dealing with medical images. If a sufficient amount of annotated training data to cover all possible variations is not available, FCNs do not provide accurate segmentation. In addition, FCNs have a large receptive field in the convolutional layers, and hence produce coarse outputs of boundaries. Hence, we propose a new fully automated method that we refer to as a dual-stage fully convolutional networks (DSFCN). Our approach leverages deep residual architectures and FCNs and learns and infers the location of the optic cup and disk in a step-wise manner with fine-grained details. During training, our approach learns from the training data and the estimated results derived from the previous iteration. The ability to learn from the previous iteration optimizes the learning of the optic cup and the disk boundaries. During testing (prediction), DSFCN uses test (input) images and the estimated probability map derived from previous iterations to gradually improve the segmentation accuracy. Our method achieved an average Dice co-efficient of 0.8488 and 0.9441 for optic cup and disk segmentation and an area under curve (AUC) of 0.9513 for glaucoma detection.



### Accurate 3D Cell Segmentation using Deep Feature and CRF Refinement
- **Arxiv ID**: http://arxiv.org/abs/1902.04729v1
- **DOI**: 10.1109/ICIP.2019.8803095
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04729v1)
- **Published**: 2019-02-13 03:38:46+00:00
- **Updated**: 2019-02-13 03:38:46+00:00
- **Authors**: Jiaxiang Jiang, Po-Yu Kao, Samuel A. Belteton, Daniel B. Szymanski, B. S. Manjunath
- **Comment**: 5 pages, 5 figures, 3 tables
- **Journal**: 2019 IEEE International Conference on Image Processing (ICIP)
- **Summary**: We consider the problem of accurately identifying cell boundaries and labeling individual cells in confocal microscopy images, specifically, 3D image stacks of cells with tagged cell membranes. Precise identification of cell boundaries, their shapes, and quantifying inter-cellular space leads to a better understanding of cell morphogenesis. Towards this, we outline a cell segmentation method that uses a deep neural network architecture to extract a confidence map of cell boundaries, followed by a 3D watershed algorithm and a final refinement using a conditional random field. In addition to improving the accuracy of segmentation compared to other state-of-the-art methods, the proposed approach also generalizes well to different datasets without the need to retrain the network for each dataset. Detailed experimental results are provided, and the source code is available on GitHub.



### Multi-Prototype Networks for Unconstrained Set-based Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.04755v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04755v4)
- **Published**: 2019-02-13 06:02:46+00:00
- **Updated**: 2019-03-23 03:54:17+00:00
- **Authors**: Jian Zhao, Jianshu Li, Xiaoguang Tu, Fang Zhao, Yuan Xin, Junliang Xing, Hengzhu Liu, Shuicheng Yan, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the challenging unconstrained set-based face recognition problem where each subject face is instantiated by a set of media (images and videos) instead of a single image. Naively aggregating information from all the media within a set would suffer from the large intra-set variance caused by heterogeneous factors (e.g., varying media modalities, poses and illuminations) and fail to learn discriminative face representations. A novel Multi-Prototype Network (MPNet) model is thus proposed to learn multiple prototype face representations adaptively from the media sets. Each learned prototype is representative for the subject face under certain condition in terms of pose, illumination and media modality. Instead of handcrafting the set partition for prototype learning, MPNet introduces a Dense SubGraph (DSG) learning sub-net that implicitly untangles inconsistent media and learns a number of representative prototypes. Qualitative and quantitative experiments clearly demonstrate superiority of the proposed model over state-of-the-arts.



### On the Convergence of Extended Variational Inference for Non-Gaussian Statistical Models
- **Arxiv ID**: http://arxiv.org/abs/1902.05068v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05068v2)
- **Published**: 2019-02-13 07:35:54+00:00
- **Updated**: 2020-01-30 15:13:08+00:00
- **Authors**: Zhanyu Ma, Jalil Taghia, Jun Guo
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Variational inference (VI) is a widely used framework in Bayesian estimation. For most of the non-Gaussian statistical models, it is infeasible to find an analytically tractable solution to estimate the posterior distributions of the parameters. Recently, an improved framework, namely the extended variational inference (EVI), has been introduced and applied to derive analytically tractable solution by employing lower-bound approximation to the variational objective function. Two conditions required for EVI implementation, namely the weak condition and the strong condition, are discussed and compared in this paper. In practical implementation, the convergence of the EVI depends on the selection of the lower-bound approximation, no matter with the weak condition or the strong condition. In general, two approximation strategies, the single lower-bound (SLB) approximation and the multiple lower-bounds (MLB) approximation, can be applied to carry out the lower-bound approximation. To clarify the differences between the SLB and the MLB, we will also discuss the convergence properties of the aforementioned two approximations. Extensive comparisons are made based on some existing EVI-based non-Gaussian statistical models. Theoretical analysis are conducted to demonstrate the differences between the weak and the strong conditions. Qualitative and quantitative experimental results are presented to show the advantages of the SLB approximation.



### Structured Bayesian Compression for Deep models in mobile enabled devices for connected healthcare
- **Arxiv ID**: http://arxiv.org/abs/1902.05429v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05429v1)
- **Published**: 2019-02-13 09:09:52+00:00
- **Updated**: 2019-02-13 09:09:52+00:00
- **Authors**: Sijia Chen, Bin Song, Xiaojiang Du, Nadra Guizani
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Models, typically Deep neural networks, have millions of parameters, analyze medical data accurately, yet in a time-consuming method. However, energy cost effectiveness and computational efficiency are important for prerequisites developing and deploying mobile-enabled devices, the mainstream trend in connected healthcare.



### Highly Efficient Follicular Segmentation in Thyroid Cytopathological Whole Slide Image
- **Arxiv ID**: http://arxiv.org/abs/1902.05431v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05431v1)
- **Published**: 2019-02-13 10:16:54+00:00
- **Updated**: 2019-02-13 10:16:54+00:00
- **Authors**: Siyan Tao, Yao Guo, Chuang Zhu, Huang Chen, Yue Zhang, Jie Yang, Jun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel method for highly efficient follicular segmentation of thyroid cytopathological WSIs. Firstly, we propose a hybrid segmentation architecture, which integrates a classifier into Deeplab V3 by adding a branch. A large amount of the WSI segmentation time is saved by skipping the irrelevant areas using the classification branch. Secondly, we merge the low scale fine features into the original atrous spatial pyramid pooling (ASPP) in Deeplab V3 to accurately represent the details in cytopathological images. Thirdly, our hybrid model is trained by a criterion-oriented adaptive loss function, which leads the model converging much faster. Experimental results on a collection of thyroid patches demonstrate that the proposed model reaches 80.9% on the segmentation accuracy. Besides, 93% time is reduced for the WSI segmentation by using our proposed method, and the WSI-level accuracy achieves 53.4%.



### Person Re-identification in Videos by Analyzing Spatio-Temporal Tubes
- **Arxiv ID**: http://arxiv.org/abs/1902.04856v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04856v1)
- **Published**: 2019-02-13 10:55:53+00:00
- **Updated**: 2019-02-13 10:55:53+00:00
- **Authors**: Sk. Arif Ahmed, Debi Prosad Dogra, Heeseung Choi, Seungho Chae, Ig-Jae Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Typical person re-identification frameworks search for k best matches in a gallery of images that are often collected in varying conditions. The gallery may contain image sequences when re-identification is done on videos. However, such a process is time consuming as re-identification has to be carried out multiple times. In this paper, we extract spatio-temporal sequences of frames (referred to as tubes) of moving persons and apply a multi-stage processing to match a given query tube with a gallery of stored tubes recorded through other cameras. Initially, we apply a binary classifier to remove noisy images from the input query tube. In the next step, we use a key-pose detection-based query minimization. This reduces the length of the query tube by removing redundant frames. Finally, a 3-stage hierarchical re-identification framework is used to rank the output tubes as per the matching scores. Experiments with publicly available video re-identification datasets reveal that our framework is better than state-of-the-art methods. It ranks the tubes with an increased CMC accuracy of 6-8% across multiple datasets. Also, our method significantly reduces the number of false positives. A new video re-identification dataset, named Tube-based Reidentification Video Dataset (TRiViD), has been prepared with an aim to help the re-identification research community



### Multi-views Embedding for Cattle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1902.04886v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04886v1)
- **Published**: 2019-02-13 13:16:50+00:00
- **Updated**: 2019-02-13 13:16:50+00:00
- **Authors**: Luca Bergamini, Angelo Porrello, Andrea Capobianco Dondona, Ercole Del Negro, Mauro Mattioli, Nicola D'Alterio, Simone Calderara
- **Comment**: 8 pages, 3 figures. Accepted in the 14th International Conference on
  SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS-2018)
- **Journal**: None
- **Summary**: People re-identification task has seen enormous improvements in the latest years, mainly due to the development of better image features extraction from deep Convolutional Neural Networks (CNN) and the availability of large datasets. However, little research has been conducted on animal identification and re-identification, even if this knowledge may be useful in a rich variety of different scenarios. Here, we tackle cattle re-identification exploiting deep CNN and show how this task is poorly related with the human one, presenting unique challenges that makes it far from being solved. We present various baselines, both based on deep architectures or on standard machine learning algorithms, and compared them with our solution. Finally, a rich ablation study has been conducted to further investigate the unique peculiarities of this task.



### Why are Saliency Maps Noisy? Cause of and Solution to Noisy Saliency Maps
- **Arxiv ID**: http://arxiv.org/abs/1902.04893v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04893v3)
- **Published**: 2019-02-13 13:25:39+00:00
- **Updated**: 2019-09-14 15:55:32+00:00
- **Authors**: Beomsu Kim, Junghoon Seo, SeungHyun Jeon, Jamyoung Koo, Jeongyeol Choe, Taegyun Jeon
- **Comment**: Accepted at the 2019 ICCV Workshop on Interpreting and Explaining
  Visual AI Models (VXAI 2019)
- **Journal**: None
- **Summary**: Saliency Map, the gradient of the score function with respect to the input, is the most basic technique for interpreting deep neural network decisions. However, saliency maps are often visually noisy. Although several hypotheses were proposed to account for this phenomenon, there are few works that provide rigorous analyses of noisy saliency maps. In this paper, we firstly propose a new hypothesis that noise may occur in saliency maps when irrelevant features pass through ReLU activation functions. Then, we propose Rectified Gradient, a method that alleviates this problem through layer-wise thresholding during backpropagation. Experiments with neural networks trained on CIFAR-10 and ImageNet showed effectiveness of our method and its superiority to other attribution methods.



### Super-Resolution of Brain MRI Images using Overcomplete Dictionaries and Nonlocal Similarity
- **Arxiv ID**: http://arxiv.org/abs/1902.04902v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1902.04902v1)
- **Published**: 2019-02-13 13:39:45+00:00
- **Updated**: 2019-02-13 13:39:45+00:00
- **Authors**: Yinghua Li, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, the Magnetic Resonance Imaging (MRI) images have limited and unsatisfactory resolutions due to various constraints such as physical, technological and economic considerations. Super-resolution techniques can obtain high-resolution MRI images. The traditional methods obtained the resolution enhancement of brain MRI by interpolations, affecting the accuracy of the following diagnose process. The requirement for brain image quality is fast increasing. In this paper, we propose an image super-resolution (SR) method based on overcomplete dictionaries and inherent similarity of an image to recover the high-resolution (HR) image from a single low-resolution (LR) image. We explore the nonlocal similarity of the image to tentatively search for similar blocks in the whole image and present a joint reconstruction method based on compressive sensing (CS) and similarity constraints. The sparsity and self-similarity of the image blocks are taken as the constraints. The proposed method is summarized in the following steps. First, a dictionary classification method based on the measurement domain is presented. The image blocks are classified into smooth, texture and edge parts by analyzing their features in the measurement domain. Then, the corresponding dictionaries are trained using the classified image blocks. Equally important, in the reconstruction part, we use the CS reconstruction method to recover the HR brain MRI image, considering both nonlocal similarity and the sparsity of an image as the constraints. This method performs better both visually and quantitatively than some existing methods.



### 3D Face Modeling From Diverse Raw Scan Data
- **Arxiv ID**: http://arxiv.org/abs/1902.04943v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04943v3)
- **Published**: 2019-02-13 15:04:12+00:00
- **Updated**: 2019-08-13 20:02:49+00:00
- **Authors**: Feng Liu, Luan Tran, Xiaoming Liu
- **Comment**: to appear in ICCV 2019
- **Journal**: None
- **Summary**: Traditional 3D face models learn a latent representation of faces using linear subspaces from limited scans of a single database. The main roadblock of building a large-scale face model from diverse 3D databases lies in the lack of dense correspondence among raw scans. To address these problems, this paper proposes an innovative framework to jointly learn a nonlinear face model from a diverse set of raw 3D scan databases and establish dense point-to-point correspondence among their scans. Specifically, by treating input scans as unorganized point clouds, we explore the use of PointNet architectures for converting point clouds to identity and expression feature representations, from which the decoder networks recover their 3D face shapes. Further, we propose a weakly supervised learning approach that does not require correspondence label for the scans. We demonstrate the superior dense correspondence and representation power of our proposed method, and its contribution to single-image 3D face reconstruction.



### Can We Automate Diagrammatic Reasoning?
- **Arxiv ID**: http://arxiv.org/abs/1902.04955v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04955v1)
- **Published**: 2019-02-13 15:43:11+00:00
- **Updated**: 2019-02-13 15:43:11+00:00
- **Authors**: Sk. Arif Ahmed, Debi Prosad Dogra, Samarjit Kar, Partha Pratim Roy, Dilip K. Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Learning to solve diagrammatic reasoning (DR) can be a challenging but interesting problem to the computer vision research community. It is believed that next generation pattern recognition applications should be able to simulate human brain to understand and analyze reasoning of images. However, due to the lack of benchmarks of diagrammatic reasoning, the present research primarily focuses on visual reasoning that can be applied to real-world objects. In this paper, we present a diagrammatic reasoning dataset that provides a large variety of DR problems. In addition, we also propose a Knowledge-based Long Short Term Memory (KLSTM) to solve diagrammatic reasoning problems. Our proposed analysis is arguably the first work in this research area. Several state-of-the-art learning frameworks have been used to compare with the proposed KLSTM framework in the present context. Preliminary results indicate that the domain is highly related to computer vision and pattern recognition research with several challenging avenues.



### 3D Robot Pose Estimation from 2D Images
- **Arxiv ID**: http://arxiv.org/abs/1902.04987v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.04987v1)
- **Published**: 2019-02-13 16:26:31+00:00
- **Updated**: 2019-02-13 16:26:31+00:00
- **Authors**: Christoph Heindl, Sebastian Zambal, Thomas Ponitz, Andreas Pichler, Josef Scharinger
- **Comment**: None
- **Journal**: None
- **Summary**: This paper considers the task of locating articulated poses of multiple robots in images. Our approach simultaneously infers the number of robots in a scene, identifies joint locations and estimates sparse depth maps around joint locations. The proposed method applies staged convolutional feature detectors to 2D image inputs and computes robot instance masks using a recurrent network architecture. In addition, regression maps of most likely joint locations in pixel coordinates together with depth information are computed. Compositing 3D robot joint kinematics is accomplished by applying masks to joint readout maps. Our end-to-end formulation is in contrast to previous work in which the composition of robot joints into kinematics is performed in a separate post-processing step. Despite the fact that our models are trained on artificial data, we demonstrate generalizability to real world images.



### Learning to see across Domains and Modalities
- **Arxiv ID**: http://arxiv.org/abs/1902.04992v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04992v1)
- **Published**: 2019-02-13 16:32:32+00:00
- **Updated**: 2019-02-13 16:32:32+00:00
- **Authors**: Fabio Maria Carlucci
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has raised hopes and expectations as a general solution for many applications; indeed it has proven effective, but it also showed a strong dependence on large quantities of data. Luckily, it has been shown that, even when data is scarce, a successful model can be trained by reusing prior knowledge. Thus, developing techniques for transfer learning, in its broadest definition, is a crucial element towards the deployment of effective and accurate intelligent systems. This thesis will focus on a family of transfer learning methods applied to the task of visual object recognition, specifically image classification. Transfer learning is a general term, and specific settings have been given specific names: when the learner has only access to unlabeled data from the a target domain and labeled data from a different domain (the source), the problem is known as that of "unsupervised domain adaptation" (DA). The first part of this work will focus on three methods for this setting: one of these methods deals with features, one with images while the third one uses both. The second part will focus on the real life issues of robotic perception, specifically RGB-D recognition. Robotic platforms are usually not limited to color perception; very often they also carry a Depth camera. Unfortunately, the depth modality is rarely used for visual recognition due to the lack of pretrained models from which to transfer and little data to train one on from scratch. Two methods for dealing with this scenario will be presented: one using synthetic data and the other exploiting cross-modality transfer learning.



### Gated2Depth: Real-time Dense Lidar from Gated Images
- **Arxiv ID**: http://arxiv.org/abs/1902.04997v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04997v3)
- **Published**: 2019-02-13 16:41:52+00:00
- **Updated**: 2019-10-27 12:56:14+00:00
- **Authors**: Tobias Gruber, Frank Julca-Aguilar, Mario Bijelic, Werner Ritter, Klaus Dietmayer, Felix Heide
- **Comment**: ICCV 2019 (oral), Authorship changed due to ICCV policy
- **Journal**: None
- **Summary**: We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.



### Unsupervised 3D End-to-End Medical Image Registration with Volume Tweening Network
- **Arxiv ID**: http://arxiv.org/abs/1902.05020v3
- **DOI**: 10.1109/JBHI.2019.2951024
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05020v3)
- **Published**: 2019-02-13 17:30:00+00:00
- **Updated**: 2019-10-30 13:54:32+00:00
- **Authors**: Shengyu Zhao, Tingfung Lau, Ji Luo, Eric I-Chao Chang, Yan Xu
- **Comment**: Journal of Biomedical and Health Informatics
- **Journal**: IEEE Journal of Biomedical and Health Informatics (Volume: 24,
  Issue: 5, May 2020)
- **Summary**: 3D medical image registration is of great clinical importance. However, supervised learning methods require a large amount of accurately annotated corresponding control points (or morphing), which are very difficult to obtain. Unsupervised learning methods ease the burden of manual annotation by exploiting unlabeled data without supervision. In this paper, we propose a new unsupervised learning method using convolutional neural networks under an end-to-end framework, Volume Tweening Network (VTN), for 3D medical image registration. We propose three innovative technical components: (1) An end-to-end cascading scheme that resolves large displacement; (2) An efficient integration of affine registration network; and (3) An additional invertibility loss that encourages backward consistency. Experiments demonstrate that our algorithm is 880x faster (or 3.3x faster without GPU acceleration) than traditional optimization-based methods and achieves state-of-theart performance in medical image registration.



### Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model
- **Arxiv ID**: http://arxiv.org/abs/1902.05437v1
- **DOI**: 10.3217/978-3-85125-652-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05437v1)
- **Published**: 2019-02-13 17:57:50+00:00
- **Updated**: 2019-02-13 17:57:50+00:00
- **Authors**: Sirin Haddad, Meiqing Wu, He Wei, Siew Kei Lam
- **Comment**: None
- **Journal**: in 24th Computer Vision Winter Workshop (CVWW), 2019, pp. 4-13
- **Summary**: Pedestrian trajectory prediction is essential for collision avoidance in autonomous driving and robot navigation. However, predicting a pedestrian's trajectory in crowded environments is non-trivial as it is influenced by other pedestrians' motion and static structures that are present in the scene. Such human-human and human-space interactions lead to non-linearities in the trajectories. In this paper, we present a new spatio-temporal graph based Long Short-Term Memory (LSTM) network for predicting pedestrian trajectory in crowded environments, which takes into account the interaction with static (physical objects) and dynamic (other pedestrians) elements in the scene. Our results are based on two widely-used datasets to demonstrate that the proposed method outperforms the state-of-the-art approaches in human trajectory prediction. In particular, our method leads to a reduction in Average Displacement Error (ADE) and Final Displacement Error (FDE) of up to 55% and 61% respectively over state-of-the-art approaches.



### DeeperLab: Single-Shot Image Parser
- **Arxiv ID**: http://arxiv.org/abs/1902.05093v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05093v2)
- **Published**: 2019-02-13 19:34:07+00:00
- **Updated**: 2019-03-12 23:22:42+00:00
- **Authors**: Tien-Ju Yang, Maxwell D. Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu, Xiao Zhang, Vivienne Sze, George Papandreou, Liang-Chieh Chen
- **Comment**: 20 pages. The code of the proposed Parsing Covering metric is
  available at http://deeperlab.mit.edu
- **Journal**: None
- **Summary**: We present a single-shot, bottom-up approach for whole image parsing. Whole image parsing, also known as Panoptic Segmentation, generalizes the tasks of semantic segmentation for 'stuff' classes and instance segmentation for 'thing' classes, assigning both semantic and instance labels to every pixel in an image. Recent approaches to whole image parsing typically employ separate standalone modules for the constituent semantic and instance segmentation tasks and require multiple passes of inference. Instead, the proposed DeeperLab image parser performs whole image parsing with a significantly simpler, fully convolutional approach that jointly addresses the semantic and instance segmentation tasks in a single-shot manner, resulting in a streamlined system that better lends itself to fast processing. For quantitative evaluation, we use both the instance-based Panoptic Quality (PQ) metric and the proposed region-based Parsing Covering (PC) metric, which better captures the image parsing quality on 'stuff' classes and larger object instances. We report experimental results on the challenging Mapillary Vistas dataset, in which our single model achieves 31.95% (val) / 31.6% PQ (test) and 55.26% PC (val) with 3 frames per second (fps) on GPU or near real-time speed (22.6 fps on GPU) with reduced accuracy.



### Do ImageNet Classifiers Generalize to ImageNet?
- **Arxiv ID**: http://arxiv.org/abs/1902.10811v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10811v2)
- **Published**: 2019-02-13 20:35:44+00:00
- **Updated**: 2019-06-12 17:42:33+00:00
- **Authors**: Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar
- **Comment**: None
- **Journal**: None
- **Summary**: We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.



### Machine Learning on Biomedical Images: Interactive Learning, Transfer Learning, Class Imbalance, and Beyond
- **Arxiv ID**: http://arxiv.org/abs/1902.05908v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05908v1)
- **Published**: 2019-02-13 21:23:07+00:00
- **Updated**: 2019-02-13 21:23:07+00:00
- **Authors**: Naimul Mefraz Khan, Nabila Abraham, Ling Guan
- **Comment**: Accepted at IEEE MIPR 2019. arXiv admin note: text overlap with
  arXiv:1810.07842
- **Journal**: None
- **Summary**: In this paper, we highlight three issues that limit performance of machine learning on biomedical images, and tackle them through 3 case studies: 1) Interactive Machine Learning (IML): we show how IML can drastically improve exploration time and quality of direct volume rendering. 2) transfer learning: we show how transfer learning along with intelligent pre-processing can result in better Alzheimer's diagnosis using a much smaller training set 3) data imbalance: we show how our novel focal Tversky loss function can provide better segmentation results taking into account the imbalanced nature of segmentation datasets. The case studies are accompanied by in-depth analytical discussion of results with possible future directions.



### Semi-Supervised Multitask Learning on Multispectral Satellite Images Using Wasserstein Generative Adversarial Networks (GANs) for Predicting Poverty
- **Arxiv ID**: http://arxiv.org/abs/1902.11110v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11110v2)
- **Published**: 2019-02-13 21:52:17+00:00
- **Updated**: 2019-04-25 19:27:01+00:00
- **Authors**: Anthony Perez, Swetava Ganguli, Stefano Ermon, George Azzari, Marshall Burke, David Lobell
- **Comment**: This project was recognized as the best two-person project during the
  Spring 2017 offering of CS 231N Convolutional Neural Networks for Visual
  Recognition. Second revised version corrects typographical errors and adds a
  few additional references
- **Journal**: None
- **Summary**: Obtaining reliable data describing local poverty metrics at a granularity that is informative to policy-makers requires expensive and logistically difficult surveys, particularly in the developing world. Not surprisingly, the poverty stricken regions are also the ones which have a high probability of being a war zone, have poor infrastructure and sometimes have governments that do not cooperate with internationally funded development efforts. We train a CNN on free and publicly available daytime satellite images of the African continent from Landsat 7 to build a model for predicting local economic livelihoods. Only 5% of the satellite images can be associated with labels (which are obtained from DHS Surveys) and thus a semi-supervised approach using a GAN (similar to the approach of Salimans, et al. (2016)), albeit with a more stable-to-train flavor of GANs called the Wasserstein GAN regularized with gradient penalty(Gulrajani, et al. (2017)) is used. The method of multitask learning is employed to regularize the network and also create an end-to-end model for the prediction of multiple poverty metrics.



