# Arxiv Papers in cs.CV on 2019-02-10
### Deep learning and face recognition: the state of the art
- **Arxiv ID**: http://arxiv.org/abs/1902.03524v1
- **DOI**: 10.1117/12.2181526
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03524v1)
- **Published**: 2019-02-10 01:07:15+00:00
- **Updated**: 2019-02-10 01:07:15+00:00
- **Authors**: Stephen Balaban
- **Comment**: Published May 15th 2015 in the Proc. SPIE 9457, Biometric and
  Surveillance Technology for Human and Activity Identification XII, 94570B;
  Ioannis A. Kakadiaris; Ajay Kumar; Walter J. Scheirer, Editor(s)
- **Journal**: Proc. SPIE 9457, Biometric and Surveillance Technology for Human
  and Activity Identification XII, 94570B (15 May 2015)
- **Summary**: Deep Neural Networks (DNNs) have established themselves as a dominant technique in machine learning. DNNs have been top performers on a wide variety of tasks including image classification, speech recognition, and face recognition. Convolutional neural networks (CNNs) have been used in nearly all of the top performing methods on the Labeled Faces in the Wild (LFW) dataset. In this talk and accompanying paper, I attempt to provide a review and summary of the deep learning techniques used in the state-of-the-art. In addition, I highlight the need for both larger and more challenging public datasets to benchmark these systems. The high accuracy (99.63% for FaceNet at the time of publishing) and utilization of outside data (hundreds of millions of images in the case of Google's FaceNet) suggest that current face verification benchmarks such as LFW may not be challenging enough, nor provide enough data, for current techniques. There exist a variety of organizations with mobile photo sharing applications that would be capable of releasing a very large scale and highly diverse dataset of facial images captured on mobile devices. Such an "ImageNet for Face Recognition" would likely receive a warm welcome from researchers and practitioners alike.



### Cross-spectral Face Completion for NIR-VIS Heterogeneous Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.03565v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03565v1)
- **Published**: 2019-02-10 10:20:38+00:00
- **Updated**: 2019-02-10 10:20:38+00:00
- **Authors**: Ran He, Jie Cao, Lingxiao Song, Zhenan Sun, Tieniu Tan
- **Comment**: None
- **Journal**: None
- **Summary**: Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the process of matching NIR to VIS face images. Current heterogeneous methods try to extend VIS face recognition methods to the NIR spectrum by synthesizing VIS images from NIR images. However, due to self-occlusion and sensing gap, NIR face images lose some visible lighting contents so that they are always incomplete compared to VIS face images. This paper models high resolution heterogeneous face synthesis as a complementary combination of two components, a texture inpainting component and pose correction component. The inpainting component synthesizes and inpaints VIS image textures from NIR image textures. The correction component maps any pose in NIR images to a frontal pose in VIS images, resulting in paired NIR and VIS textures. A warping procedure is developed to integrate the two components into an end-to-end deep network. A fine-grained discriminator and a wavelet-based discriminator are designed to supervise intra-class variance and visual quality respectively. One UV loss, two adversarial losses and one pixel loss are imposed to ensure synthesis results. We demonstrate that by attaching the correction component, we can simplify heterogeneous face synthesis from one-to-many unpaired image translation to one-to-one paired image translation, and minimize spectral and pose discrepancy during heterogeneous recognition. Extensive experimental results show that our network not only generates high-resolution VIS face images and but also facilitates the accuracy improvement of heterogeneous face recognition.



### EvalAI: Towards Better Evaluation Systems for AI Agents
- **Arxiv ID**: http://arxiv.org/abs/1902.03570v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03570v1)
- **Published**: 2019-02-10 10:34:54+00:00
- **Updated**: 2019-02-10 10:34:54+00:00
- **Authors**: Deshraj Yadav, Rishabh Jain, Harsh Agrawal, Prithvijit Chattopadhyay, Taranjeet Singh, Akash Jain, Shiv Baran Singh, Stefan Lee, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce EvalAI, an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a scalable solution to the research community to fulfill the critical need of evaluating machine learning models and agents acting in an environment against annotations or with a human-in-the-loop. This will help researchers, students, and data scientists to create, collaborate, and participate in AI challenges organized around the globe. By simplifying and standardizing the process of benchmarking these models, EvalAI seeks to lower the barrier to entry for participating in the global scientific effort to push the frontiers of machine learning and artificial intelligence, thereby increasing the rate of measurable progress in this domain.



### Colorectal Cancer Outcome Prediction from H&E Whole Slide Images using Machine Learning and Automatically Inferred Phenotype Profiles
- **Arxiv ID**: http://arxiv.org/abs/1902.03582v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.03582v2)
- **Published**: 2019-02-10 12:05:02+00:00
- **Updated**: 2019-03-09 18:18:52+00:00
- **Authors**: Xingzhi Yue, Neofytos Dimitriou, Ognjen Arandjelovic
- **Comment**: 2019
- **Journal**: None
- **Summary**: Digital pathology (DP) is a new research area which falls under the broad umbrella of health informatics. Owing to its potential for major public health impact, in recent years DP has been attracting much research attention. Nevertheless, a wide breadth of significant conceptual and technical challenges remain, few of them greater than those encountered in the field of oncology. The automatic analysis of digital pathology slides of cancerous tissues is particularly problematic due to the inherent heterogeneity of the disease, extremely large images, amongst numerous others. In this paper we introduce a novel machine learning based framework for the prediction of colorectal cancer outcome from whole digitized haematoxylin & eosin (H&E) stained histopathology slides. Using a real-world data set we demonstrate the effectiveness of the method and present a detailed analysis of its different elements which corroborate its ability to extract and learn salient, discriminative, and clinically meaningful content.



### Angle-Closure Detection in Anterior Segment OCT based on Multi-Level Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1902.03585v1
- **DOI**: 10.1109/TCYB.2019.2897162
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03585v1)
- **Published**: 2019-02-10 12:28:52+00:00
- **Updated**: 2019-02-10 12:28:52+00:00
- **Authors**: Huazhu Fu, Yanwu Xu, Stephen Lin, Damon Wing Kee Wong, Mani Baskaran, Meenakshi Mahesh, Tin Aung, Jiang Liu
- **Comment**: 9 pages, accepted by IEEE Transactions on Cybernetics
- **Journal**: None
- **Summary**: Irreversible visual impairment is often caused by primary angle-closure glaucoma, which could be detected via Anterior Segment Optical Coherence Tomography (AS-OCT). In this paper, an automated system based on deep learning is presented for angle-closure detection in AS-OCT images. Our system learns a discriminative representation from training data that captures subtle visual cues not modeled by handcrafted features. A Multi-Level Deep Network (MLDN) is proposed to formulate this learning, which utilizes three particular AS-OCT regions based on clinical priors: the global anterior segment structure, local iris region, and anterior chamber angle (ACA) patch. In our method, a sliding window based detector is designed to localize the ACA region, which addresses ACA detection as a regression task. Then, three parallel sub-networks are applied to extract AS-OCT representations for the global image and at clinically-relevant local regions. Finally, the extracted deep features of these sub-networks are concatenated into one fully connected layer to predict the angle-closure detection result. In the experiments, our system is shown to surpass previous detection methods and other deep learning systems on two clinical AS-OCT datasets.



### NeurAll: Towards a Unified Model for Visual Perception in Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.03589v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03589v2)
- **Published**: 2019-02-10 12:45:49+00:00
- **Updated**: 2019-07-17 16:39:56+00:00
- **Authors**: Ganesh Sistu, Isabelle Leang, Sumanth Chennupati, Ciaran Hughes, Stefan Milz, Senthil Yogamani, Samir Rawashdeh
- **Comment**: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2019
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) are successfully used for the important automotive visual perception tasks including object recognition, motion and depth estimation, visual SLAM, etc. However, these tasks are typically independently explored and modeled. In this paper, we propose a joint multi-task network design for learning several tasks simultaneously. Our main motivation is the computational efficiency achieved by sharing the expensive initial convolutional layers between all tasks. Indeed, the main bottleneck in automated driving systems is the limited processing power available on deployment hardware. There is also some evidence for other benefits in improving accuracy for some tasks and easing development effort. It also offers scalability to add more tasks leveraging existing features and achieving better generalization. We survey various CNN based solutions for visual perception tasks in automated driving. Then we propose a unified CNN model for the important tasks and discuss several advanced optimization and architecture design techniques to improve the baseline model. The paper is partly review and partly positional with demonstration of several preliminary results promising for future research. We first demonstrate results of multi-stream learning and auxiliary learning which are important ingredients to scale to a large multi-task model. Finally, we implement a two-stream three-task network which performs better in many cases compared to their corresponding single-task models, while maintaining network size.



### Vulnerable road user detection: state-of-the-art and open challenges
- **Arxiv ID**: http://arxiv.org/abs/1902.03601v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03601v1)
- **Published**: 2019-02-10 13:51:47+00:00
- **Updated**: 2019-02-10 13:51:47+00:00
- **Authors**: Patrick Mannion
- **Comment**: None
- **Journal**: None
- **Summary**: Correctly identifying vulnerable road users (VRUs), e.g. cyclists and pedestrians, remains one of the most challenging environment perception tasks for autonomous vehicles (AVs). This work surveys the current state-of-the-art in VRU detection, covering topics such as benchmarks and datasets, object detection techniques and relevant machine learning algorithms. The article concludes with a discussion of remaining open challenges and promising future research directions for this domain.



### MOTS: Multi-Object Tracking and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.03604v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03604v2)
- **Published**: 2019-02-10 14:01:22+00:00
- **Updated**: 2019-04-08 15:01:48+00:00
- **Authors**: Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe
- **Comment**: CVPR 2019 camera-ready version
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2019
- **Summary**: This paper extends the popular task of multi-object tracking to multi-object tracking and segmentation (MOTS). Towards this goal, we create dense pixel-level annotations for two existing tracking datasets using a semi-automatic annotation procedure. Our new annotations comprise 65,213 pixel masks for 977 distinct objects (cars and pedestrians) in 10,870 video frames. For evaluation, we extend existing multi-object tracking metrics to this new task. Moreover, we propose a new baseline method which jointly addresses detection, tracking, and segmentation with a single convolutional network. We demonstrate the value of our datasets by achieving improvements in performance when training on MOTS annotations. We believe that our datasets, metrics and baseline will become a valuable resource towards developing multi-object tracking approaches that go beyond 2D bounding boxes. We make our annotations, code, and models available at https://www.vision.rwth-aachen.de/page/mots.



### Towards Automatic Lesion Classification in the Upper Aerodigestive Tract Using OCT and Deep Transfer Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1902.03618v1
- **DOI**: 10.1007/s11548-019-01969-3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03618v1)
- **Published**: 2019-02-10 15:14:43+00:00
- **Updated**: 2019-02-10 15:14:43+00:00
- **Authors**: Nils Gessert, Matthias Schlüter, Sarah Latus, Veronika Volgger, Christian Betz, Alexander Schlaefer
- **Comment**: Accepted for publication at CARS 2019
- **Journal**: None
- **Summary**: Early detection of cancer is crucial for treatment and overall patient survival. In the upper aerodigestive tract (UADT) the gold standard for identification of malignant tissue is an invasive biopsy. Recently, non-invasive imaging techniques such as confocal laser microscopy and optical coherence tomography (OCT) have been used for tissue assessment. In particular, in a recent study experts classified lesions in the UADT with respect to their invasiveness using OCT images only. As the results were promising, automatic classification of lesions might be feasible which could assist experts in their decision making. Therefore, we address the problem of automatic lesion classification from OCT images. This task is very challenging as the available dataset is extremely small and the data quality is limited. However, as similar issues are typical in many clinical scenarios we study to what extent deep learning approaches can still be trained and used for decision support.



### A Decoupled 3D Facial Shape Model by Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1902.03619v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03619v3)
- **Published**: 2019-02-10 15:15:44+00:00
- **Updated**: 2019-09-07 17:19:26+00:00
- **Authors**: Victoria Fernandez Abrevaya, Adnane Boukhayma, Stefanie Wuhrer, Edmond Boyer
- **Comment**: camera-ready version for ICCV'19
- **Journal**: None
- **Summary**: Data-driven generative 3D face models are used to compactly encode facial shape data into meaningful parametric representations. A desirable property of these models is their ability to effectively decouple natural sources of variation, in particular identity and expression. While factorized representations have been proposed for that purpose, they are still limited in the variability they can capture and may present modeling artifacts when applied to tasks such as expression transfer. In this work, we explore a new direction with Generative Adversarial Networks and show that they contribute to better face modeling performances, especially in decoupling natural factors, while also achieving more diverse samples. To train the model we introduce a novel architecture that combines a 3D generator with a 2D discriminator that leverages conventional CNNs, where the two components are bridged by a geometry mapping layer. We further present a training scheme, based on auxiliary classifiers, to explicitly disentangle identity and expression attributes. Through quantitative and qualitative results on standard face datasets, we illustrate the benefits of our model and demonstrate that it outperforms competing state of the art methods in terms of decoupling and diversity.



### Shallow Triple Stream Three-dimensional CNN (STSTNet) for Micro-expression Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.03634v2
- **DOI**: 10.1109/FG.2019.8756567
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03634v2)
- **Published**: 2019-02-10 17:26:39+00:00
- **Updated**: 2019-08-21 10:45:38+00:00
- **Authors**: Sze-Teng Liong, Y. S. Gan, John See, Huai-Qian Khor, Yen-Chang Huang
- **Comment**: 5 pages, 1 figure, Accepted and published in IEEE FG 2019
- **Journal**: None
- **Summary**: In the recent year, state-of-the-art for facial micro-expression recognition have been significantly advanced by deep neural networks. The robustness of deep learning has yielded promising performance beyond that of traditional handcrafted approaches. Most works in literature emphasized on increasing the depth of networks and employing highly complex objective functions to learn more features. In this paper, we design a Shallow Triple Stream Three-dimensional CNN (STSTNet) that is computationally light whilst capable of extracting discriminative high level features and details of micro-expressions. The network learns from three optical flow features (i.e., optical strain, horizontal and vertical optical flow fields) computed based on the onset and apex frames of each video. Our experimental results demonstrate the effectiveness of the proposed STSTNet, which obtained an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on the composite database consisting of 442 samples from the SMIC, CASME II and SAMM databases.



### Context-Aware Visual Compatibility Prediction
- **Arxiv ID**: http://arxiv.org/abs/1902.03646v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03646v2)
- **Published**: 2019-02-10 18:14:15+00:00
- **Updated**: 2019-02-12 14:17:03+00:00
- **Authors**: Guillem Cucurull, Perouz Taslakian, David Vazquez
- **Comment**: None
- **Journal**: None
- **Summary**: How do we determine whether two or more clothing items are compatible or visually appealing? Part of the answer lies in understanding of visual aesthetics, and is biased by personal preferences shaped by social attitudes, time, and place. In this work we propose a method that predicts compatibility between two items based on their visual features, as well as their context. We define context as the products that are known to be compatible with each of these item. Our model is in contrast to other metric learning approaches that rely on pairwise comparisons between item features alone. We address the compatibility prediction problem using a graph neural network that learns to generate product embeddings conditioned on their context. We present results for two prediction tasks (fill in the blank and outfit compatibility) tested on two fashion datasets Polyvore and Fashion-Gen, and on a subset of the Amazon dataset; we achieve state of the art results when using context information and show how test performance improves as more context is used.



### Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion
- **Arxiv ID**: http://arxiv.org/abs/1902.03680v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03680v3)
- **Published**: 2019-02-10 23:01:33+00:00
- **Updated**: 2019-06-17 07:34:07+00:00
- **Authors**: Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C. Alexander, Nathan Silberman
- **Comment**: CVPR 2019, code snippets included
- **Journal**: None
- **Summary**: The predictive performance of supervised learning algorithms depends on the quality of labels. In a typical label collection process, multiple annotators provide subjective noisy estimates of the "truth" under the influence of their varying skill-levels and biases. Blindly treating these noisy labels as the ground truth limits the accuracy of learning algorithms in the presence of strong disagreement. This problem is critical for applications in domains such as medical imaging where both the annotation cost and inter-observer variability are high. In this work, we present a method for simultaneously learning the individual annotator model and the underlying true label distribution, using only noisy observations. Each annotator is modeled by a confusion matrix that is jointly estimated along with the classifier predictions. We propose to add a regularization term to the loss function that encourages convergence to the true annotator confusion matrix. We provide a theoretical argument as to how the regularization is essential to our approach both for the case of single annotator and multiple annotators. Despite the simplicity of the idea, experiments on image classification tasks with both simulated and real labels show that our method either outperforms or performs on par with the state-of-the-art methods and is capable of estimating the skills of annotators even with a single label available per image.



### Paradigm shift in electron-based crystallography via machine learning
- **Arxiv ID**: http://arxiv.org/abs/1902.03682v1
- **DOI**: 10.1126/science.aay3062
- **Categories**: **cond-mat.mtrl-sci**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.03682v1)
- **Published**: 2019-02-10 23:16:41+00:00
- **Updated**: 2019-02-10 23:16:41+00:00
- **Authors**: Kevin Kaufmann, Chaoyi Zhu, Alexander S. Rosengarten, Daniel Maryanovsky, Tyler J. Harrington, Eduardo Marin, Kenneth S. Vecchio
- **Comment**: 35 pages, 17 figures with extended data included
- **Journal**: Science 367 (2020) 564-568
- **Summary**: Accurately determining the crystallographic structure of a material, organic or inorganic, is a critical primary step in material development and analysis. The most common practices involve analysis of diffraction patterns produced in laboratory XRD, TEM, and synchrotron X-ray sources. However, these techniques are slow, require careful sample preparation, can be difficult to access, and are prone to human error during analysis. This paper presents a newly developed methodology that represents a paradigm change in electron diffraction-based structure analysis techniques, with the potential to revolutionize multiple crystallography-related fields. A machine learning-based approach for rapid and autonomous identification of the crystal structure of metals and alloys, ceramics, and geological specimens, without any prior knowledge of the sample, is presented and demonstrated utilizing the electron backscatter diffraction (EBSD) technique. Electron backscatter diffraction patterns are collected from materials with well-known crystal structures, then a deep neural network model is constructed for classification to a specific Bravais lattice or point group. The applicability of this approach is evaluated on diffraction patterns from samples unknown to the computer without any human input or data filtering. This is in comparison to traditional Hough transform EBSD, which requires that you have already determined the phases present in your sample. The internal operations of the neural network are elucidated through visualizing the symmetry features learned by the convolutional neural network. It is determined that the model looks for the same features a crystallographer would use, even though it is not explicitly programmed to do so. This study opens the door to fully automated, high-throughput determination of crystal structures via several electron-based diffraction techniques.



