# Arxiv Papers in cs.CV on 2019-02-12
### De-identification without losing faces
- **Arxiv ID**: http://arxiv.org/abs/1902.04202v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04202v1)
- **Published**: 2019-02-12 01:15:15+00:00
- **Updated**: 2019-02-12 01:15:15+00:00
- **Authors**: Yuezun Li, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: Training of deep learning models for computer vision requires large image or video datasets from real world. Often, in collecting such datasets, we need to protect the privacy of the people captured in the images or videos, while still preserve the useful attributes such as facial expressions. In this work, we describe a new face de-identification method that can preserve essential facial attributes in the faces while concealing the identities. Our method takes advantage of the recent advances in face attribute transfer models, while maintaining a high visual quality. Instead of changing factors of the original faces or synthesizing faces completely, our method use a trained facial attribute transfer model to map non-identity related facial attributes to the face of donors, who are a small number (usually 2 to 3) of consented subjects. Using the donors' faces ensures that the natural appearance of the synthesized faces, while ensuring the identity of the synthesized faces are changed. On the other hand, the FATM blends the donors' facial attributes to those of the original faces to diversify the appearance of the synthesized faces. Experimental results on several sets of images and videos demonstrate the effectiveness of our face de-ID algorithm.



### Brain MRI Segmentation using Rule-Based Hybrid Approach
- **Arxiv ID**: http://arxiv.org/abs/1902.04207v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04207v1)
- **Published**: 2019-02-12 01:30:18+00:00
- **Updated**: 2019-02-12 01:30:18+00:00
- **Authors**: Mustansar Fiaz, Kamran Ali, Abdul Rehman, M. Junaid Gul, Soon Ki Jung
- **Comment**: 8 figures
- **Journal**: None
- **Summary**: Medical image segmentation being a substantial component of image processing plays a significant role to analyze gross anatomy, to locate an infirmity and to plan the surgical procedures. Segmentation of brain Magnetic Resonance Imaging (MRI) is of considerable importance for the accurate diagnosis. However, precise and accurate segmentation of brain MRI is a challenging task. Here, we present an efficient framework for segmentation of brain MR images. For this purpose, Gabor transform method is used to compute features of brain MRI. Then, these features are classified by using four different classifiers i.e., Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN), Probabilistic Neural Network (PNN), and Support Vector Machine (SVM). Performance of these classifiers is investigated over different images of brain MRI and the variation in the performance of these classifiers is observed for different brain tissues. Thus, we proposed a rule-based hybrid approach to segment brain MRI. Experimental results show that the performance of these classifiers varies over each tissue MRI and the proposed rule-based hybrid approach exhibits better segmentation of brain MRI tissues.



### MaCow: Masked Convolutional Generative Flow
- **Arxiv ID**: http://arxiv.org/abs/1902.04208v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04208v5)
- **Published**: 2019-02-12 01:31:06+00:00
- **Updated**: 2019-10-27 00:45:11+00:00
- **Authors**: Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard Hovy
- **Comment**: In Proceedings of Thirty-third Conference on Neural Information
  Processing Systems (NeurIPS-2019)
- **Journal**: NeurIPS 2019
- **Summary**: Flow-based generative models, conceptually attractive due to tractability of both the exact log-likelihood computation and latent-variable inference, and efficiency of both training and sampling, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. Despite their computational efficiency, the density estimation performance of flow-based generative models significantly falls behind those of state-of-the-art autoregressive models. In this work, we introduce masked convolutional generative flow (MaCow), a simple yet effective architecture of generative flow using masked convolution. By restricting the local connectivity in a small kernel, MaCow enjoys the properties of fast and stable training, and efficient sampling, while achieving significant improvements over Glow for density estimation on standard image benchmarks, considerably narrowing the gap to autoregressive models.



### You Only Look & Listen Once: Towards Fast and Accurate Visual Grounding
- **Arxiv ID**: http://arxiv.org/abs/1902.04213v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04213v3)
- **Published**: 2019-02-12 02:04:49+00:00
- **Updated**: 2019-03-17 06:05:50+00:00
- **Authors**: Chaorui Deng, Qi Wu, Guanghui Xu, Zhuliang Yu, Yanwu Xu, Kui Jia, Mingkui Tan
- **Comment**: Wrong experimental results
- **Journal**: None
- **Summary**: Visual Grounding (VG) aims to locate the most relevant region in an image, based on a flexible natural language query but not a pre-defined label, thus it can be a more useful technique than object detection in practice. Most state-of-the-art methods in VG operate in a two-stage manner, wherein the first stage an object detector is adopted to generate a set of object proposals from the input image and the second stage is simply formulated as a cross-modal matching problem that finds the best match between the language query and all region proposals. This is rather inefficient because there might be hundreds of proposals produced in the first stage that need to be compared in the second stage, not to mention this strategy performs inaccurately. In this paper, we propose an simple, intuitive and much more elegant one-stage detection based method that joints the region proposal and matching stage as a single detection network. The detection is conditioned on the input query with a stack of novel Relation-to-Attention modules that transform the image-to-query relationship to an relation map, which is used to predict the bounding box directly without proposing large numbers of useless region proposals. During the inference, our approach is about 20x ~ 30x faster than previous methods and, remarkably, it achieves 18% ~ 41% absolute performance improvement on top of the state-of-the-art results on several benchmark datasets. We release our code and all the pre-trained models at https://github.com/openblack/rvg.



### RespNet: A deep learning model for extraction of respiration from photoplethysmogram
- **Arxiv ID**: http://arxiv.org/abs/1902.04236v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04236v2)
- **Published**: 2019-02-12 04:36:34+00:00
- **Updated**: 2019-02-20 10:11:24+00:00
- **Authors**: Vignesh Ravichandran, Balamurali Murugesan, Vaishali Balakarthikeyan, Sharath M Shankaranarayana, Keerthi Ram, Preejith S. P, Jayaraj Joseph, Mohanasankar Sivaprakasam
- **Comment**: Under review at EMBC
- **Journal**: None
- **Summary**: Respiratory ailments afflict a wide range of people and manifests itself through conditions like asthma and sleep apnea. Continuous monitoring of chronic respiratory ailments is seldom used outside the intensive care ward due to the large size and cost of the monitoring system. While Electrocardiogram (ECG) based respiration extraction is a validated approach, its adoption is limited by access to a suitable continuous ECG monitor. Recently, due to the widespread adoption of wearable smartwatches with in-built Photoplethysmogram (PPG) sensor, it is being considered as a viable candidate for continuous and unobtrusive respiration monitoring. Research in this domain, however, has been predominantly focussed on estimating respiration rate from PPG. In this work, a novel end-to-end deep learning network called RespNet is proposed to perform the task of extracting the respiration signal from a given input PPG as opposed to extracting respiration rate. The proposed network was trained and tested on two different datasets utilizing different modalities of reference respiration signal recordings. Also, the similarity and performance of the proposed network against two conventional signal processing approaches for extracting respiration signal were studied. The proposed method was tested on two independent datasets with a Mean Squared Error of 0.262 and 0.145. The Cross-Correlation coefficient of the respective datasets were found to be 0.933 and 0.931. The reported errors and similarity was found to be better than conventional approaches. The proposed approach would aid clinicians to provide comprehensive evaluation of sleep-related respiratory conditions and chronic respiratory ailments while being comfortable and inexpensive for the patient.



### Enhancement Mask for Hippocampus Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04244v1)
- **Published**: 2019-02-12 05:25:29+00:00
- **Updated**: 2019-02-12 05:25:29+00:00
- **Authors**: Dengsheng Chen, Wenxi Liu, You Huang, Tong Tong, Yuanlong Yu
- **Comment**: This paper has been published in the proceedings of IEEE
  International Conference on Information and Automation 2018
- **Journal**: 2018 IEEE International Conference on Information and Automation
  (ICIA)
- **Summary**: Detection and segmentation of the hippocampal structures in volumetric brain images is a challenging problem in the area of medical imaging. In this paper, we propose a two-stage 3D fully convolutional neural network that efficiently detects and segments the hippocampal structures. In particular, our approach first localizes the hippocampus from the whole volumetric image while obtaining a proposal for a rough segmentation. After localization, we apply the proposal as an enhancement mask to extract the fine structure of the hippocampus. The proposed method has been evaluated on a public dataset and compares with state-of-the-art approaches. Results indicate the effectiveness of the proposed method, which yields mean Dice Similarity Coefficients (i.e. DSC) of $0.897$ and $0.900$ for the left and right hippocampus, respectively. Furthermore, extensive experiments manifest that the proposed enhancement mask layer has remarkable benefits for accelerating training process and obtaining more accurate segmentation results.



### A system for generating complex physically accurate sensor images for automotive applications
- **Arxiv ID**: http://arxiv.org/abs/1902.04258v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04258v1)
- **Published**: 2019-02-12 06:50:47+00:00
- **Updated**: 2019-02-12 06:50:47+00:00
- **Authors**: Zhenyi Liu, Minghao Shen, Jiaqi Zhang, Shuangting Liu, Henryk Blasinski, Trisha Lian, Brian Wandell
- **Comment**: 5 pages, 10 figures, IS&T Electronic Imaging conference 2019
- **Journal**: None
- **Summary**: We describe an open-source simulator that creates sensor irradiance and sensor images of typical automotive scenes in urban settings. The purpose of the system is to support camera design and testing for automotive applications. The user can specify scene parameters (e.g., scene type, road type, traffic density, time of day) to assemble a large number of random scenes from graphics assets stored in a database. The sensor irradiance is generated using quantitative computer graphics methods, and the sensor images are created using image systems sensor simulation. The synthetic sensor images have pixel level annotations; hence, they can be used to train and evaluate neural networks for imaging tasks, such as object detection and classification. The end-to-end simulation system supports quantitative assessment, from scene to camera to network accuracy, for automotive applications.



### Towards Self-Supervised High Level Sensor Fusion
- **Arxiv ID**: http://arxiv.org/abs/1902.04272v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04272v1)
- **Published**: 2019-02-12 07:53:55+00:00
- **Updated**: 2019-02-12 07:53:55+00:00
- **Authors**: Qadeer Khan, Torsten Schön, Patrick Wenzel
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a framework to control a self-driving car by fusing raw information from RGB images and depth maps. A deep neural network architecture is used for mapping the vision and depth information, respectively, to steering commands. This fusion of information from two sensor sources allows to provide redundancy and fault tolerance in the presence of sensor failures. Even if one of the input sensors fails to produce the correct output, the other functioning sensor would still be able to maneuver the car. Such redundancy is crucial in the critical application of self-driving cars. The experimental results have showed that our method is capable of learning to use the relevant sensor information even when one of the sensors fail without any explicit signal.



### Unpriortized Autoencoder For Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1902.04294v2
- **DOI**: 10.1109/ICIP40778.2020.9191173
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04294v2)
- **Published**: 2019-02-12 09:41:36+00:00
- **Updated**: 2021-08-26 08:36:46+00:00
- **Authors**: Jaeyoung Yoo, Hojun Lee, Nojun Kwak
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we treat the image generation task using an autoencoder, a representative latent model. Unlike many studies regularizing the latent variable's distribution by assuming a manually specified prior, we approach the image generation task using an autoencoder by directly estimating the latent distribution. To this end, we introduce 'latent density estimator' which captures latent distribution explicitly and propose its structure. Through experiments, we show that our generative model generates images with the improved visual quality compared to previous autoencoder-based generative models.



### Improving Facial Emotion Recognition Systems Using Gradient and Laplacian Images
- **Arxiv ID**: http://arxiv.org/abs/1902.05411v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05411v1)
- **Published**: 2019-02-12 10:00:34+00:00
- **Updated**: 2019-02-12 10:00:34+00:00
- **Authors**: Ram Krishna Pandey, Souvik Karmakar, A G Ramakrishnan, Nabagata Saha
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we have proposed several enhancements to improve the performance of any facial emotion recognition (FER) system. We believe that the changes in the positions of the fiducial points and the intensities capture the crucial information regarding the emotion of a face image. We propose the use of the gradient and the Laplacian of the input image together with the original input into a convolutional neural network (CNN). These modifications help the network learn additional information from the gradient and Laplacian of the images. However, the plain CNN is not able to extract this information from the raw images. We have performed a number of experiments on two well known datasets KDEF and FERplus. Our approach enhances the already high performance of state-of-the-art FER systems by 3 to 5%.



### GAN- vs. JPEG2000 Image Compression for Distributed Automotive Perception: Higher Peak SNR Does Not Mean Better Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04311v1)
- **Published**: 2019-02-12 10:09:37+00:00
- **Updated**: 2019-02-12 10:09:37+00:00
- **Authors**: Jonas Löhdefink, Andreas Bär, Nico M. Schmidt, Fabian Hüger, Peter Schlicht, Tim Fingscheidt
- **Comment**: None
- **Journal**: None
- **Summary**: The high amount of sensors required for autonomous driving poses enormous challenges on the capacity of automotive bus systems. There is a need to understand tradeoffs between bitrate and perception performance. In this paper, we compare the image compression standards JPEG, JPEG2000, and WebP to a modern encoder/decoder image compression approach based on generative adversarial networks (GANs). We evaluate both the pure compression performance using typical metrics such as peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and others, but also the performance of a subsequent perception function, namely a semantic segmentation (characterized by the mean intersection over union (mIoU) measure). Not surprisingly, for all investigated compression methods, a higher bitrate means better results in all investigated quality metrics. Interestingly, however, we show that the semantic segmentation mIoU of the GAN autoencoder in the highly relevant low-bitrate regime (at 0.0625 bit/pixel) is better by 3.9% absolute than JPEG2000, although the latter still is considerably better in terms of PSNR (5.91 dB difference). This effect can greatly be enlarged by training the semantic segmentation model with images originating from the decoder, so that the mIoU using the segmentation model trained by GAN reconstructions exceeds the use of the model trained with original images by almost 20% absolute. We conclude that distributed perception in future autonomous driving will most probably not provide a solution to the automotive bus capacity bottleneck by using standard compression schemes such as JPEG2000, but requires modern coding approaches, with the GAN encoder/decoder method being a promising candidate.



### Real-time tracker with fast recovery from target loss
- **Arxiv ID**: http://arxiv.org/abs/1902.04570v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04570v1)
- **Published**: 2019-02-12 11:28:15+00:00
- **Updated**: 2019-02-12 11:28:15+00:00
- **Authors**: Alessandro Bay, Panagiotis Sidiropoulos, Eduard Vazquez, Michele Sasdelli
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1806.07844
- **Journal**: None
- **Summary**: In this paper, we introduce a variation of a state-of-the-art real-time tracker (CFNet), which adds to the original algorithm robustness to target loss without a significant computational overhead. The new method is based on the assumption that the feature map can be used to estimate the tracking confidence more accurately. When the confidence is low, we avoid updating the object's position through the feature map; instead, the tracker passes to a single-frame failure mode, during which the patch's low-level visual content is used to swiftly update the object's position, before recovering from the target loss in the next frame. The experimental evidence provided by evaluating the method on several tracking datasets validates both the theoretical assumption that the feature map is associated to tracking confidence, and that the proposed implementation can achieve target recovery in multiple scenarios, without compromising the real-time performance.



### The effect of scene context on weakly supervised semantic segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04356v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04356v2)
- **Published**: 2019-02-12 12:28:12+00:00
- **Updated**: 2019-06-07 08:45:28+00:00
- **Authors**: Mohammad Kamalzare, Reza Kahani, Alireza Talebpour, Ahmad Mahmoudi-Aznaveh
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Image semantic segmentation is parsing image into several partitions in such a way that each region of which involves a semantic concept. In a weakly supervised manner, since only image-level labels are available, discriminating objects from the background is challenging, and in some cases, much more difficult. More specifically, some objects which are commonly seen in one specific scene (e.g. 'train' typically is seen on 'railroad track') are much more likely to be confused. In this paper, we propose a method to add the target-specific scenes in order to overcome the aforementioned problem. Actually, we propose a scene recommender which suggests to add some specific scene contexts to the target dataset in order to train the model more accurately. It is notable that this idea could be a complementary part of the baselines of many other methods. The experiments validate the effectiveness of the proposed method for the objects for which the scene context is added.



### Manifestation of Image Contrast in Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.04378v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04378v1)
- **Published**: 2019-02-12 13:35:26+00:00
- **Updated**: 2019-02-12 13:35:26+00:00
- **Authors**: Arash Akbarinia, Karl R. Gegenfurtner
- **Comment**: None
- **Journal**: None
- **Summary**: Contrast is subject to dramatic changes across the visual field, depending on the source of light and scene configurations. Hence, the human visual system has evolved to be more sensitive to contrast than absolute luminance. This feature is equally desired for machine vision: the ability to recognise patterns even when aspects of them are transformed due to variation in local and global contrast. In this work, we thoroughly investigate the impact of image contrast on prominent deep convolutional networks, both during the training and testing phase. The results of conducted experiments testify to an evident deterioration in the accuracy of all state-of-the-art networks at low-contrast images. We demonstrate that "contrast-augmentation" is a sufficient condition to endow a network with invariance to contrast. This practice shows no negative side effects, quite the contrary, it might allow a model to refrain from other illuminance related over-fittings. This ability can also be achieved by a short fine-tuning procedure, which opens new lines of investigation on mechanisms involved in two networks whose weights are over 99.9% correlated, yet astonishingly produce utterly different outcomes. Our further analysis suggests that the optimisation algorithm is an influential factor, however with a significantly lower effect; and while the choice of an architecture manifests a negligible impact on this phenomenon, the first layers appear to be more critical.



### To Ensemble or Not Ensemble: When does End-To-End Training Fail?
- **Arxiv ID**: http://arxiv.org/abs/1902.04422v4
- **DOI**: 10.13140/RG.2.2.28091.46880
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04422v4)
- **Published**: 2019-02-12 14:56:06+00:00
- **Updated**: 2020-08-06 09:48:03+00:00
- **Authors**: Andrew M. Webb, Charles Reynolds, Wenlin Chen, Henry Reeve, Dan-Andrei Iliescu, Mikel Lujan, Gavin Brown
- **Comment**: Code: https://github.com/grey-area/modular-loss-experiments. Preprint
  updated to reflect version accepted for publication at ECML
- **Journal**: None
- **Summary**: End-to-End training (E2E) is becoming more and more popular to train complex Deep Network architectures. An interesting question is whether this trend will continue-are there any clear failure cases for E2E training? We study this question in depth, for the specific case of E2E training an ensemble of networks. Our strategy is to blend the gradient smoothly in between two extremes: from independent training of the networks, up to to full E2E training. We find clear failure cases, where over-parameterized models cannot be trained E2E. A surprising result is that the optimum can sometimes lie in between the two, neither an ensemble or an E2E system. The work also uncovers links to Dropout, and raises questions around the nature of ensemble diversity and multi-branch networks.



### A new Backdoor Attack in CNNs by training set corruption without label poisoning
- **Arxiv ID**: http://arxiv.org/abs/1902.11237v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11237v1)
- **Published**: 2019-02-12 16:27:41+00:00
- **Updated**: 2019-02-12 16:27:41+00:00
- **Authors**: Mauro Barni, Kassem Kallas, Benedetta Tondi
- **Comment**: None
- **Journal**: None
- **Summary**: Backdoor attacks against CNNs represent a new threat against deep learning systems, due to the possibility of corrupting the training set so to induce an incorrect behaviour at test time. To avoid that the trainer recognises the presence of the corrupted samples, the corruption of the training set must be as stealthy as possible. Previous works have focused on the stealthiness of the perturbation injected into the training samples, however they all assume that the labels of the corrupted samples are also poisoned. This greatly reduces the stealthiness of the attack, since samples whose content does not agree with the label can be identified by visual inspection of the training set or by running a pre-classification step. In this paper we present a new backdoor attack without label poisoning Since the attack works by corrupting only samples of the target class, it has the additional advantage that it does not need to identify beforehand the class of the samples to be attacked at test time. Results obtained on the MNIST digits recognition task and the traffic signs classification task show that backdoor attacks without label poisoning are indeed possible, thus raising a new alarm regarding the use of deep learning in security-critical applications.



### MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04478v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04478v1)
- **Published**: 2019-02-12 16:30:54+00:00
- **Updated**: 2019-02-12 16:30:54+00:00
- **Authors**: Chen Liu, Yasutaka Furukawa
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach for 3D instance segmentation based on sparse convolution and point affinity prediction, which indicates the likelihood of two points belonging to the same instance. The proposed network, built upon submanifold sparse convolution [3], processes a voxelized point cloud and predicts semantic scores for each occupied voxel as well as the affinity between neighboring voxels at different scales. A simple yet effective clustering algorithm segments points into instances based on the predicted affinity and the mesh topology. The semantic for each instance is determined by the semantic prediction. Experiments show that our method outperforms the state-of-the-art instance segmentation methods by a large margin on the widely used ScanNet benchmark [2]. We share our code publicly at https://github.com/art-programmer/MASC.



### Extended 2D Consensus Hippocampus Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04487v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04487v5)
- **Published**: 2019-02-12 16:44:35+00:00
- **Updated**: 2020-05-13 22:01:47+00:00
- **Authors**: Diedre Carmo, Bruna Silva, Clarissa Yasuda, Letícia Rittner, Roberto Lotufo
- **Comment**: This was published as an extended abstract in MIDL 2019
  [arXiv:1907.08612]. An alpha version of the code is available at
  https://github.com/dscarmo/e2dhipseg. More experiments on improvements to the
  method and code are ongoing. Future updates are to be expected. A new, more
  complete paper is published in arXiv:2001.05058
- **Journal**: None
- **Summary**: Hippocampus segmentation plays a key role in diagnosing various brain disorders such as Alzheimer's disease, epilepsy, multiple sclerosis, cancer, depression and others. Nowadays, segmentation is still mainly performed manually by specialists. Segmentation done by experts is considered to be a gold-standard when evaluating automated methods, buts it is a time consuming and arduos task, requiring specialized personnel. In recent years, efforts have been made to achieve reliable automated segmentation. For years the best performing authomatic methods were multi atlas based with around 80-85% Dice coefficient and very time consuming, but machine learning methods are recently rising with promising time and accuracy performance. A method for volumetric hippocampus segmentation is presented, based on the consensus of tri-planar U-Net inspired fully convolutional networks (FCNNs), with some modifications, including residual connections, VGG weight transfers, batch normalization and a patch extraction technique employing data from neighbor patches. A study on the impact of our modifications to the classical U-Net architecture was performed. Our method achieves cutting edge performance in our dataset, with around 96% volumetric Dice accuracy in our test data. In a public validation dataset, HARP, we achieve 87.48% DICE. GPU execution time is in the order of seconds per volume, and source code is publicly available. Also, masks are shown to be similar to other recent state-of-the-art hippocampus segmentation methods in a third dataset, without manual annotations.



### Fast-SCNN: Fast Semantic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1902.04502v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04502v1)
- **Published**: 2019-02-12 17:08:26+00:00
- **Updated**: 2019-02-12 17:08:26+00:00
- **Authors**: Rudra P K Poudel, Stephan Liwicki, Roberto Cipolla
- **Comment**: None
- **Journal**: None
- **Summary**: The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.



### Deep learning algorithms out-perform veterinary pathologists in detecting the mitotically most active tumor region
- **Arxiv ID**: http://arxiv.org/abs/1902.05414v3
- **DOI**: 10.1038/s41598-020-73246-2
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05414v3)
- **Published**: 2019-02-12 17:37:20+00:00
- **Updated**: 2020-10-21 05:49:22+00:00
- **Authors**: Marc Aubreville, Christof A. Bertram, Christian Marzahl, Corinne Gurtner, Martina Dettwiler, Anja Schmidt, Florian Bartenschlager, Sophie Merz, Marco Fragoso, Olivia Kershaw, Robert Klopfleisch, Andreas Maier
- **Comment**: 13 pages, 7 figures
- **Journal**: Sci Rep. 2020 Oct 5;10(1):16447
- **Summary**: Manual count of mitotic figures, which is determined in the tumor region with the highest mitotic activity, is a key parameter of most tumor grading schemes. It can be, however, strongly dependent on the area selection due to uneven mitotic figure distribution in the tumor section.We aimed to assess the question, how significantly the area selection could impact the mitotic count, which has a known high inter-rater disagreement. On a data set of 32 whole slide images of H&E-stained canine cutaneous mast cell tumor, fully annotated for mitotic figures, we asked eight veterinary pathologists (five board-certified, three in training) to select a field of interest for the mitotic count. To assess the potential difference on the mitotic count, we compared the mitotic count of the selected regions to the overall distribution on the slide.Additionally, we evaluated three deep learning-based methods for the assessment of highest mitotic density: In one approach, the model would directly try to predict the mitotic count for the presented image patches as a regression task. The second method aims at deriving a segmentation mask for mitotic figures, which is then used to obtain a mitotic density. Finally, we evaluated a two-stage object-detection pipeline based on state-of-the-art architectures to identify individual mitotic figures. We found that the predictions by all models were, on average, better than those of the experts. The two-stage object detector performed best and outperformed most of the human pathologists on the majority of tumor cases. The correlation between the predicted and the ground truth mitotic count was also best for this approach (0.963 to 0.979). Further, we found considerable differences in position selection between pathologists, which could partially explain the high variance that has been reported for the manual mitotic count.



### Center of circle after perspective transformation
- **Arxiv ID**: http://arxiv.org/abs/1902.04541v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04541v1)
- **Published**: 2019-02-12 18:38:04+00:00
- **Updated**: 2019-02-12 18:38:04+00:00
- **Authors**: Xi Wang, Albert Chern, Marc Alexa
- **Comment**: None
- **Journal**: None
- **Summary**: Video-based glint-free eye tracking commonly estimates gaze direction based on the pupil center. The boundary of the pupil is fitted with an ellipse and the euclidean center of the ellipse in the image is taken as the center of the pupil. However, the center of the pupil is generally not mapped to the center of the ellipse by the projective camera transformation. This error resulting from using a point that is not the true center of the pupil directly affects eye tracking accuracy. We investigate the underlying geometric problem of determining the center of a circular object based on its projective image. The main idea is to exploit two concentric circles -- in the application scenario these are the pupil and the iris. We show that it is possible to computed the center and the ratio of the radii from the mapped concentric circles with a direct method that is fast and robust in practice. We evaluate our method on synthetically generated data and find that it improves systematically over using the center of the fitted ellipse. Apart from applications of eye tracking we estimate that our approach will be useful in other tracking applications.



### Progressively Growing Generative Adversarial Networks for High Resolution Semantic Segmentation of Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1902.04604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04604v1)
- **Published**: 2019-02-12 19:39:07+00:00
- **Updated**: 2019-02-12 19:39:07+00:00
- **Authors**: Edward Collier, Kate Duffy, Sangram Ganguly, Geri Madanguit, Subodh Kalia, Gayaka Shreekant, Ramakrishna Nemani, Andrew Michaelis, Shuang Li, Auroop Ganguly, Supratik Mukhopadhyay
- **Comment**: Accepted too and presented at DMESS 2018 as part of IEEE ICDM 2018
- **Journal**: None
- **Summary**: Machine learning has proven to be useful in classification and segmentation of images. In this paper, we evaluate a training methodology for pixel-wise segmentation on high resolution satellite images using progressive growing of generative adversarial networks. We apply our model to segmenting building rooftops and compare these results to conventional methods for rooftop segmentation. We present our findings using the SpaceNet version 2 dataset. Progressive GAN training achieved a test accuracy of 93% compared to 89% for traditional GAN training.



### Direct Automatic Coronary Calcium Scoring in Cardiac and Chest CT
- **Arxiv ID**: http://arxiv.org/abs/1902.05408v1
- **DOI**: 10.1109/TMI.2019.2899534
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05408v1)
- **Published**: 2019-02-12 21:50:21+00:00
- **Updated**: 2019-02-12 21:50:21+00:00
- **Authors**: Bob D. de Vos, Jelmer M. Wolterink, Tim Leiner, Pim A. de Jong, Nikolas Lessmann, Ivana Isgum
- **Comment**: IEEE Transactions on Medical Imaging (In press)
- **Journal**: IEEE Transactions on Medical Imaging, Volume: 38, Issue: 9, Sept.
  2019, Pages: 2127 - 2138
- **Summary**: Cardiovascular disease (CVD) is the global leading cause of death. A strong risk factor for CVD events is the amount of coronary artery calcium (CAC). To meet demands of the increasing interest in quantification of CAC, i.e. coronary calcium scoring, especially as an unrequested finding for screening and research, automatic methods have been proposed. Current automatic calcium scoring methods are relatively computationally expensive and only provide scores for one type of CT. To address this, we propose a computationally efficient method that employs two ConvNets: the first performs registration to align the fields of view of input CTs and the second performs direct regression of the calcium score, thereby circumventing time-consuming intermediate CAC segmentation. Optional decision feedback provides insight in the regions that contributed to the calcium score. Experiments were performed using 903 cardiac CT and 1,687 chest CT scans. The method predicted calcium scores in less than 0.3 s. Intra-class correlation coefficient between predicted and manual calcium scores was 0.98 for both cardiac and chest CT. The method showed almost perfect agreement between automatic and manual CVD risk categorization in both datasets, with a linearly weighted Cohen's kappa of 0.95 in cardiac CT and 0.93 in chest CT. Performance is similar to that of state-of-the-art methods, but the proposed method is hundreds of times faster. By providing visual feedback, insight is given in the decision process, making it readily implementable in clinical and research settings.



