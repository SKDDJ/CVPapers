# Arxiv Papers in cs.CV on 2019-02-19
### WIDER Face and Pedestrian Challenge 2018: Methods and Results
- **Arxiv ID**: http://arxiv.org/abs/1902.06854v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06854v1)
- **Published**: 2019-02-19 01:09:56+00:00
- **Updated**: 2019-02-19 01:09:56+00:00
- **Authors**: Chen Change Loy, Dahua Lin, Wanli Ouyang, Yuanjun Xiong, Shuo Yang, Qingqiu Huang, Dongzhan Zhou, Wei Xia, Quanquan Li, Ping Luo, Junjie Yan, Jianfeng Wang, Zuoxin Li, Ye Yuan, Boxun Li, Shuai Shao, Gang Yu, Fangyun Wei, Xiang Ming, Dong Chen, Shifeng Zhang, Cheng Chi, Zhen Lei, Stan Z. Li, Hongkai Zhang, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen, Wu Liu, Boyan Zhou, Huaxiong Li, Peng Cheng, Tao Mei, Artem Kukharenko, Artem Vasenin, Nikolay Sergievskiy, Hua Yang, Liangqi Li, Qiling Xu, Yuan Hong, Lin Chen, Mingjun Sun, Yirong Mao, Shiying Luo, Yongjun Li, Ruiping Wang, Qiaokang Xie, Ziyang Wu, Lei Lu, Yiheng Liu, Wengang Zhou
- **Comment**: Report of ECCV 2018 workshop: WIDER Face and Pedestrian Challenge
- **Journal**: None
- **Summary**: This paper presents a review of the 2018 WIDER Challenge on Face and Pedestrian. The challenge focuses on the problem of precise localization of human faces and bodies, and accurate association of identities. It comprises of three tracks: (i) WIDER Face which aims at soliciting new approaches to advance the state-of-the-art in face detection, (ii) WIDER Pedestrian which aims to find effective and efficient approaches to address the problem of pedestrian detection in unconstrained environments, and (iii) WIDER Person Search which presents an exciting challenge of searching persons across 192 movies. In total, 73 teams made valid submissions to the challenge tracks. We summarize the winning solutions for all three tracks. and present discussions on open problems and potential research directions in these topics.



### Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions
- **Arxiv ID**: http://arxiv.org/abs/1902.06857v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, I.2; I.4; I.5
- **Links**: [PDF](http://arxiv.org/pdf/1902.06857v2)
- **Published**: 2019-02-19 01:46:57+00:00
- **Updated**: 2019-08-28 21:23:28+00:00
- **Authors**: Dogancan Temel, Tariq Alshawi, Min-Hung Chen, Ghassan AlRegib
- **Comment**: 26 pages, 22 figures, 7 tables
- **Journal**: None
- **Summary**: State-of-the-art algorithms successfully localize and recognize traffic signs over existing datasets, which are limited in terms of challenging condition type and severity. Therefore, it is not possible to estimate the performance of traffic sign detection algorithms under overlooked challenging conditions. Another shortcoming of existing datasets is the limited utilization of temporal information and the unavailability of consecutive frames and annotations. To overcome these shortcomings, we generated the CURE-TSD video dataset and hosted the first IEEE Video and Image Processing (VIP) Cup within the IEEE Signal Processing Society. In this paper, we provide a detailed description of the CURE-TSD dataset, analyze the characteristics of the top performing algorithms, and provide a performance benchmark. Moreover, we investigate the robustness of the benchmarked algorithms with respect to sign size, challenge type and severity. Benchmarked algorithms are based on state-of-the-art and custom convolutional neural networks that achieved a precision of 0.55 and a recall of 0.32, F0.5 score of 0.48 and F2 score of 0.35. Experimental results show that benchmarked algorithms are highly sensitive to tested challenging conditions, which result in an average performance drop of 0.17 in terms of precision and a performance drop of 0.28 in recall under severe conditions. The dataset is publicly available at https://github.com/olivesgatech/CURE-TSD.



### Predicting city safety perception based on visual image content
- **Arxiv ID**: http://arxiv.org/abs/1902.06871v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06871v1)
- **Published**: 2019-02-19 02:59:44+00:00
- **Updated**: 2019-02-19 02:59:44+00:00
- **Authors**: Sergio Acosta, Jorge E. Camargo
- **Comment**: CIARP 2018
- **Journal**: None
- **Summary**: Safety perception measurement has been a subject of interest in many cities of the world. This is due to its social relevance, and to its effect on some local economic activities. Even though people safety perception is a subjective topic, sometimes it is possible to find out common patterns given a restricted geographical and sociocultural context. This paper presents an approach that makes use of image processing and machine learning techniques to detect with high accuracy urban environment patterns that could affect citizen's safety perception.



### Appearance-based Gesture recognition in the compressed domain
- **Arxiv ID**: http://arxiv.org/abs/1903.00100v1
- **DOI**: 10.1109/ICASSP.2017.7952451
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00100v1)
- **Published**: 2019-02-19 06:05:12+00:00
- **Updated**: 2019-02-19 06:05:12+00:00
- **Authors**: Shaojie Xu, Anvesha Amaravati, Justin Romberg, Arijit Raychowdhury
- **Comment**: arXiv admin note: text overlap with arXiv:1605.08313
- **Journal**: 2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), New Orleans, LA, 2017, pp. 1722-1726
- **Summary**: We propose a novel appearance-based gesture recognition algorithm using compressed domain signal processing techniques. Gesture features are extracted directly from the compressed measurements, which are the block averages and the coded linear combinations of the image sensor's pixel values. We also improve both the computational efficiency and the memory requirement of the previous DTW-based K-NN gesture classifiers. Both simulation testing and hardware implementation strongly support the proposed algorithm.



### Fast Compressive Sensing Recovery Using Generative Models with Structured Latent Variables
- **Arxiv ID**: http://arxiv.org/abs/1902.06913v4
- **DOI**: 10.1109/ICASSP.2019.8683641
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.06913v4)
- **Published**: 2019-02-19 06:18:59+00:00
- **Updated**: 2020-03-19 16:16:54+00:00
- **Authors**: Shaojie Xu, Sihan Zeng, Justin Romberg
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning models have significantly improved the visual quality and accuracy on compressive sensing recovery. In this paper, we propose an algorithm for signal reconstruction from compressed measurements with image priors captured by a generative model. We search and constrain on latent variable space to make the method stable when the number of compressed measurements is extremely limited. We show that, by exploiting certain structures of the latent variables, the proposed method produces improved reconstruction accuracy and preserves realistic and non-smooth features in the image. Our algorithm achieves high computation speed by projecting between the original signal space and the latent variable space in an alternating fashion.



### 2D LiDAR Map Prediction via Estimating Motion Flow with GRU
- **Arxiv ID**: http://arxiv.org/abs/1902.06919v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06919v1)
- **Published**: 2019-02-19 06:47:49+00:00
- **Updated**: 2019-02-19 06:47:49+00:00
- **Authors**: Yafei Song, Yonghong Tian, Gang Wang, Mingyang Li
- **Comment**: None
- **Journal**: None
- **Summary**: It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDAR-FlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.



### Using Conditional Generative Adversarial Networks to Generate Ground-Level Views From Overhead Imagery
- **Arxiv ID**: http://arxiv.org/abs/1902.06923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06923v1)
- **Published**: 2019-02-19 06:58:36+00:00
- **Updated**: 2019-02-19 06:58:36+00:00
- **Authors**: Xueqing Deng, Yi Zhu, Shawn Newsam
- **Comment**: 5 pages. arXiv admin note: text overlap with arXiv:1806.05129
- **Journal**: None
- **Summary**: This paper develops a deep-learning framework to synthesize a ground-level view of a location given an overhead image. We propose a novel conditional generative adversarial network (cGAN) in which the trained generator generates realistic looking and representative ground-level images using overhead imagery as auxiliary information. The generator is an encoder-decoder network which allows us to compare low- and high-level features as well as their concatenation for encoding the overhead imagery. We also demonstrate how our framework can be used to perform land cover classification by modifying the trained cGAN to extract features from overhead imagery. This is interesting because, although we are using this modified cGAN as a feature extractor for overhead imagery, it incorporates knowledge of how locations look from the ground.



### Anomaly Detection with Adversarial Dual Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1902.06924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06924v1)
- **Published**: 2019-02-19 06:59:30+00:00
- **Updated**: 2019-02-19 06:59:30+00:00
- **Authors**: Ha Son Vu, Daisuke Ueta, Kiyoshi Hashimoto, Kazuki Maeno, Sugiri Pranata, Sheng Mei Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Semi-supervised and unsupervised Generative Adversarial Networks (GAN)-based methods have been gaining popularity in anomaly detection task recently. However, GAN training is somewhat challenging and unstable. Inspired from previous work in GAN-based image generation, we introduce a GAN-based anomaly detection framework - Adversarial Dual Autoencoders (ADAE) - consists of two autoencoders as generator and discriminator to increase training stability. We also employ discriminator reconstruction error as anomaly score for better detection performance. Experiments across different datasets of varying complexity show strong evidence of a robust model that can be used in different scenarios, one of which is brain tumor detection.



### Predicting tongue motion in unlabeled ultrasound videos using convolutional LSTM neural network
- **Arxiv ID**: http://arxiv.org/abs/1902.06927v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1902.06927v1)
- **Published**: 2019-02-19 07:11:28+00:00
- **Updated**: 2019-02-19 07:11:28+00:00
- **Authors**: Chaojie Zhao, Peng Zhang, Jian Zhu, Chengrui Wu, Huaimin Wang, Kele Xu
- **Comment**: Accepted by ICASSP 2019
- **Journal**: None
- **Summary**: A challenge in speech production research is to predict future tongue movements based on a short period of past tongue movements. This study tackles speaker-dependent tongue motion prediction problem in unlabeled ultrasound videos with convolutional long short-term memory (ConvLSTM) networks. The model has been tested on two different ultrasound corpora. ConvLSTM outperforms 3-dimensional convolutional neural network (3DCNN) in predicting the 9\textsuperscript{th} frames based on 8 preceding frames, and also demonstrates good capacity to predict only the tongue contours in future frames. Further tests reveal that ConvLSTM can also learn to predict tongue movements in more distant frames beyond the immediately following frames. Our codes are available at: https://github.com/shuiliwanwu/ConvLstm-ultrasound-videos.



### Air Quality Measurement Based on Double-Channel Convolutional Neural Network Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.06942v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06942v3)
- **Published**: 2019-02-19 08:05:54+00:00
- **Updated**: 2019-02-28 09:39:00+00:00
- **Authors**: Zhenyu Wang, Wei Zheng, Chunfeng Song
- **Comment**: None
- **Journal**: None
- **Summary**: Environmental air quality affects people's life, obtaining real-time and accurate environmental air quality has a profound guiding significance for the development of social activities. At present, environmental air quality measurement mainly adopts the method that setting air quality detector at specific monitoring points in cities and timing sampling analysis, which is easy to be restricted by time and space factors. Some air quality measurement algorithms related to deep learning mostly adopt a single convolutional neural network to train the whole image, which will ignore the difference of different parts of the image. In this paper, we propose a method for air quality measurement based on double-channel convolutional neural network ensemble learning to solve the problem of feature extraction for different parts of environmental images. Our method mainly includes two aspects: ensemble learning of double-channel convolutional neural network and self-learning weighted feature fusion. We constructed a double-channel convolutional neural network, used each channel to train different parts of the environment images for feature extraction. We propose a feature weight self-learning method, which weights and concatenates the extracted feature vectors, and uses the fused feature vectors to measure air quality. Our method can be applied to the two tasks of air quality grade measurement and air quality index (AQI) measurement. Moreover, we build an environmental image dataset of random time and location condition. The experiments show that our method can achieve nearly 82% accuracy and a small mean absolute error (MAE) on our test dataset. At the same time, through comparative experiment, we proved that our proposed method gained considerable improvement in performance compared with single channel convolutional neural network air quality measurements.



### Geometry of Deep Generative Models for Disentangled Representations
- **Arxiv ID**: http://arxiv.org/abs/1902.06964v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06964v1)
- **Published**: 2019-02-19 09:29:57+00:00
- **Updated**: 2019-02-19 09:29:57+00:00
- **Authors**: Ankita Shukla, Shagun Uppal, Sarthak Bhagat, Saket Anand, Pavan Turaga
- **Comment**: Accepted at ICVGIP, 2018
- **Journal**: None
- **Summary**: Deep generative models like variational autoencoders approximate the intrinsic geometry of high dimensional data manifolds by learning low-dimensional latent-space variables and an embedding function. The geometric properties of these latent spaces has been studied under the lens of Riemannian geometry; via analysis of the non-linearity of the generator function. In new developments, deep generative models have been used for learning semantically meaningful `disentangled' representations; that capture task relevant attributes while being invariant to other attributes. In this work, we explore the geometry of popular generative models for disentangled representation learning. We use several metrics to compare the properties of latent spaces of disentangled representation models in terms of class separability and curvature of the latent-space. The results we obtain establish that the class distinguishable features in the disentangled latent space exhibits higher curvature as opposed to a variational autoencoder. We evaluate and compare the geometry of three such models with variational autoencoder on two different datasets. Further, our results show that distances and interpolation in the latent space are significantly improved with Riemannian metrics derived from the curvature of the space. We expect these results will have implications on understanding how deep-networks can be made more robust, generalizable, as well as interpretable.



### Evaluating the Effectiveness of Automated Identity Masking (AIM) Methods with Human Perception and a Deep Convolutional Neural Network (CNN)
- **Arxiv ID**: http://arxiv.org/abs/1902.06967v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06967v3)
- **Published**: 2019-02-19 09:34:12+00:00
- **Updated**: 2019-11-04 17:38:58+00:00
- **Authors**: Kimberley D. Orsten-Hooge, Asal Baragchizadeh, Thomas P. Karnowski, David S. Bolme, Regina Ferrell, Parisa R. Jesudasen, Carlos D. Castillo, Alice J. O'Toole
- **Comment**: *K.O.H and A.B. contributed equally to this work.10 pages, 4 tables,
  7 figures
- **Journal**: None
- **Summary**: Face de-identification algorithms have been developed in response to the prevalent use of public video recordings and surveillance cameras. Here, we evaluated the success of identity masking in the context of monitoring drivers as they actively operate a motor vehicle. We studied the effectiveness of eight de-identification algorithms using human perceivers and a state-of-the-art deep convolutional neural network (CNN). We used a standard face recognition experiment in which human subjects studied high-resolution (studio-style) images to learn driver identities. Subjects were tested subsequently on their ability to recognize those identities in low-resolution videos depicting the drivers operating a motor vehicle. The videos were in either unmasked format, or were masked by one of the eight de-identification algorithms. All masking algorithms lowered identification accuracy substantially, relative to the unmasked video. In all cases, identifications were made with stringent decision criteria indicating the subjects had low confidence in their decisions. When matching the identities in high-resolution still images to those in the masked videos, the CNN performed at chance. Next, we examined CNN performance on the same task, but using the unmasked videos and their masked counterparts. In this case, the network scored surprisingly well on a subset of mask conditions. We conclude that carefully tested de-identification approaches, used alone or in combination, can be an effective tool for protecting the privacy of individuals captured in videos. We note that no approach is equally effective in masking all stimuli, and that future work should examine possible methods for determining the most effective mask per individual stimulus.



### Detector-in-Detector: Multi-Level Analysis for Human-Parts
- **Arxiv ID**: http://arxiv.org/abs/1902.07017v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07017v1)
- **Published**: 2019-02-19 12:23:59+00:00
- **Updated**: 2019-02-19 12:23:59+00:00
- **Authors**: Xiaojie Li, Lu Yang, Qing Song, Fuqiang Zhou
- **Comment**: 13 pages, 4 figures, accepted by ACCV2018
- **Journal**: None
- **Summary**: Vision-based person, hand or face detection approaches have achieved incredible success in recent years with the development of deep convolutional neural network (CNN). In this paper, we take the inherent correlation between the body and body parts into account and propose a new framework to boost up the detection performance of the multi-level objects. In particular, we adopt a region-based object detection structure with two carefully designed detectors to separately pay attention to the human body and body parts in a coarse-to-fine manner, which we call Detector-in-Detector network (DID-Net). The first detector is designed to detect human body, hand, and face. The second detector, based on the body detection results of the first detector, mainly focus on the detection of small hand and face inside each body. The framework is trained in an end-to-end way by optimizing a multi-task loss. Due to the lack of human body, face and hand detection dataset, we have collected and labeled a new large dataset named Human-Parts with 14,962 images and 106,879 annotations. Experiments show that our method can achieve excellent performance on Human-Parts.



### Variational Regularized Transmission Refinement for Image Dehazing
- **Arxiv ID**: http://arxiv.org/abs/1902.07069v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07069v1)
- **Published**: 2019-02-19 14:28:57+00:00
- **Updated**: 2019-02-19 14:28:57+00:00
- **Authors**: Qiaoling Shu, Chuansheng Wu, Zhe Xiao, Ryan Wen Liu
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: High-quality dehazing performance is highly dependent upon the accurate estimation of transmission map. In this work, the coarse estimation version is first obtained by weightedly fusing two different transmission maps, which are generated from foreground and sky regions, respectively. A hybrid variational model with promoted regularization terms is then proposed to assisting in refining transmission map. The resulting complicated optimization problem is effectively solved via an alternating direction algorithm. The final haze-free image can be effectively obtained according to the refined transmission map and atmospheric scattering model. Our dehazing framework has the capacity of preserving important image details while suppressing undesirable artifacts, even for hazy images with large sky regions. Experiments on both synthetic and realistic images have illustrated that the proposed method is competitive with or even outperforms the state-of-the-art dehazing techniques under different imaging conditions.



### Directional Regularized Tensor Modeling for Video Rain Streaks Removal
- **Arxiv ID**: http://arxiv.org/abs/1902.07090v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07090v1)
- **Published**: 2019-02-19 15:10:33+00:00
- **Updated**: 2019-02-19 15:10:33+00:00
- **Authors**: Zhaoyang Sun, Shengwu Xiong, Ryan Wen Liu
- **Comment**: 5 pages, 3 figures
- **Journal**: None
- **Summary**: Outdoor videos sometimes contain unexpected rain streaks due to the rainy weather, which bring negative effects on subsequent computer vision applications, e.g., video surveillance, object recognition and tracking, etc. In this paper, we propose a directional regularized tensor-based video deraining model by taking into consideration the arbitrary direction of rain streaks. In particular, the sparsity of rain streaks in spatial and derivative domains, the spatiotemporal sparsity and low-rank property of video background are incorporated into the proposed method. Different from many previous methods under the assumption of vertically falling rain streaks, we consider a more realistic assumption that all the rain streaks in a video fall in an approximately similar arbitrary direction. The resulting complicated optimization problem will be effectively solved through an alternating direction method. Comprehensive experiments on both synthetic and realistic datasets have demonstrated the superiority of the proposed deraining method.



### BusyHands: A Hand-Tool Interaction Database for Assembly Tasks Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.07262v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07262v1)
- **Published**: 2019-02-19 19:59:46+00:00
- **Updated**: 2019-02-19 19:59:46+00:00
- **Authors**: Roy Shilkrot, Zhi Chai, Minh Hoai
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: Visual segmentation has seen tremendous advancement recently with ready solutions for a wide variety of scene types, including human hands and other body parts. However, focus on segmentation of human hands while performing complex tasks, such as manual assembly, is still severely lacking. Segmenting hands from tools, work pieces, background and other body parts is extremely difficult because of self-occlusions and intricate hand grips and poses. In this paper we introduce BusyHands, a large open dataset of pixel-level annotated images of hands performing 13 different tool-based assembly tasks, from both real-world captures and virtual-world renderings. A total of 7906 samples are included in our first-in-kind dataset, with both RGB and depth images as obtained from a Kinect V2 camera and Blender. We evaluate several state-of-the-art semantic segmentation methods on our dataset as a proposed performance benchmark.



### Graph Spectral Characterization of Brain Cortical Morphology
- **Arxiv ID**: http://arxiv.org/abs/1902.07283v1
- **DOI**: None
- **Categories**: **q-bio.NC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.07283v1)
- **Published**: 2019-02-19 21:04:26+00:00
- **Updated**: 2019-02-19 21:04:26+00:00
- **Authors**: Sevil Maghsadhagh, Anders Eklund, Hamid Behjat
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1810.10339
- **Journal**: None
- **Summary**: The human brain cortical layer has a convoluted morphology that is unique to each individual. Characterization of the cortical morphology is necessary in longitudinal studies of structural brain change, as well as in discriminating individuals in health and disease. A method for encoding the cortical morphology in the form of a graph is presented. The design of graphs that encode the global cerebral hemisphere cortices as well as localized cortical regions is proposed. Spectral metrics derived from these graphs are then studied and proposed as descriptors of cortical morphology. As proof-of-concept of their applicability in characterizing cortical morphology, the metrics are studied in the context of hemispheric asymmetry as well as gender dependent discrimination of cortical morphology.



### Accurate Automatic Segmentation of Amygdala Subnuclei and Modeling of Uncertainty via Bayesian Fully Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1902.07289v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07289v1)
- **Published**: 2019-02-19 21:16:55+00:00
- **Updated**: 2019-02-19 21:16:55+00:00
- **Authors**: Yilin Liu, Gengyan Zhao, Brendon M. Nacewicz, Nagesh Adluru, Gregory R. Kirk, Peter A Ferrazzano, Martin Styner, Andrew L. Alexander
- **Comment**: None
- **Journal**: None
- **Summary**: Recent advances in deep learning have improved the segmentation accuracy of subcortical brain structures, which would be useful in neuroimaging studies of many neurological disorders. However, most of the previous deep learning work does not investigate the specific difficulties that exist in segmenting extremely small but important brain regions such as the amygdala and its subregions. To tackle this challenging task, a novel 3D Bayesian fully convolutional neural network was developed to apply a dilated dualpathway approach that retains fine details and utilizes both local and more global contextual information to automatically segment the amygdala and its subregions at high precision. The proposed method provides insights on network design and sampling strategy that target segmentations of small 3D structures. In particular, this study confirms that a large context, enabled by a large field of view, is beneficial for segmenting small objects; furthermore, precise contextual information enabled by dilated convolutions allows for better boundary localization, which is critical for examining the morphology of the structure. In addition, it is demonstrated that the uncertainty information estimated from our network may be leveraged to identify atypicality in data. Our method was compared with two state-of-the-art deep learning models and a traditional multi-atlas approach, and exhibited excellent performance as measured both by Dice overlap as well as average symmetric surface distance. To the best of our knowledge, this work is the first deep learning-based approach that targets the subregions of the amygdala.



### Augmentation for small object detection
- **Arxiv ID**: http://arxiv.org/abs/1902.07296v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07296v1)
- **Published**: 2019-02-19 21:47:31+00:00
- **Updated**: 2019-02-19 21:47:31+00:00
- **Authors**: Mate Kisantal, Zbigniew Wojna, Jakub Murawski, Jacek Naruniec, Kyunghyun Cho
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, object detection has experienced impressive progress. Despite these improvements, there is still a significant gap in the performance between the detection of small and large objects. We analyze the current state-of-the-art model, Mask-RCNN, on a challenging dataset, MS COCO. We show that the overlap between small ground-truth objects and the predicted anchors is much lower than the expected IoU threshold. We conjecture this is due to two factors; (1) only a few images are containing small objects, and (2) small objects do not appear enough even within each image containing them. We thus propose to oversample those images with small objects and augment each of those images by copy-pasting small objects many times. It allows us to trade off the quality of the detector on large objects with that on small objects. We evaluate different pasting augmentation strategies, and ultimately, we achieve 9.7\% relative improvement on the instance segmentation and 7.1\% on the object detection of small objects, compared to the current state of the art method on MS COCO.



### DeepBall: Deep Neural-Network Ball Detector
- **Arxiv ID**: http://arxiv.org/abs/1902.07304v1
- **DOI**: 10.5220/0007348902970304
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07304v1)
- **Published**: 2019-02-19 21:57:48+00:00
- **Updated**: 2019-02-19 21:57:48+00:00
- **Authors**: Jacek Komorowski, Grzegorz Kurzejamski, Grzegorz Sarwas
- **Comment**: Conference: VISAPP 2019
- **Journal**: VISIGRAPP 2019 - Proceedings of the 14th International Joint
  Conference on Computer Vision, Imaging and Computer Graphics Theory and
  Applications, vol. 5, 2019, SciTePress, ISBN 978-989-758-354-4, p. 297-304
- **Summary**: The paper describes a deep network based object detector specialized for ball detection in long shot videos. Due to its fully convolutional design, the method operates on images of any size and produces \emph{ball confidence map} encoding the position of detected ball. The network uses hypercolumn concept, where feature maps from different hierarchy levels of the deep convolutional network are combined and jointly fed to the convolutional classification layer. This allows boosting the detection accuracy as larger visual context around the object of interest is taken into account. The method achieves state-of-the-art results when tested on publicly available ISSIA-CNR Soccer Dataset.



### Large-scale mammography CAD with Deformable Conv-Nets
- **Arxiv ID**: http://arxiv.org/abs/1902.07323v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07323v1)
- **Published**: 2019-02-19 22:18:22+00:00
- **Updated**: 2019-02-19 22:18:22+00:00
- **Authors**: Stephen Morrell, Zbigniew Wojna, Can Son Khoo, Sebastien Ourselin, Juan Eugenio Iglesias
- **Comment**: None
- **Journal**: None
- **Summary**: State-of-the-art deep learning methods for image processing are evolving into increasingly complex meta-architectures with a growing number of modules. Among them, region-based fully convolutional networks (R-FCN) and deformable convolutional nets (DCN) can improve CAD for mammography: R-FCN optimizes for speed and low consumption of memory, which is crucial for processing the high resolutions of to 50 micrometers used by radiologists. Deformable convolution and pooling can model a wide range of mammographic findings of different morphology and scales, thanks to their versatility. In this study, we present a neural net architecture based on R-FCN / DCN, that we have adapted from the natural image domain to suit mammograms -- particularly their larger image size -- without compromising resolution. We trained the network on a large, recently released dataset (Optimam) including 6,500 cancerous mammograms. By combining our modern architecture with such a rich dataset, we achieved an area under the ROC curve of 0.879 for breast-wise detection in the DREAMS challenge (130,000 withheld images), which surpassed all other submissions in the competitive phase.



### Video Face Recognition: Component-wise Feature Aggregation Network (C-FAN)
- **Arxiv ID**: http://arxiv.org/abs/1902.07327v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07327v3)
- **Published**: 2019-02-19 22:23:59+00:00
- **Updated**: 2019-07-02 22:12:34+00:00
- **Authors**: Sixue Gong, Yichun Shi, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a new approach to video face recognition. Our component-wise feature aggregation network (C-FAN) accepts a set of face images of a subject as an input, and outputs a single feature vector as the face representation of the set for the recognition task. The whole network is trained in two steps: (i) train a base CNN for still image face recognition; (ii) add an aggregation module to the base network to learn the quality value for each feature component, which adaptively aggregates deep feature vectors into a single vector to represent the face in a video. C-FAN automatically learns to retain salient face features with high quality scores while suppressing features with low quality scores. The experimental results on three benchmark datasets, YouTube Faces, IJB-A, and IJB-S show that the proposed C-FAN network is capable of generating a compact feature vector with 512 dimensions for a video sequence by efficiently aggregating feature vectors of all the video frames to achieve state of the art performance.



### Adaptive Masked Proxies for Few-Shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.11123v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11123v5)
- **Published**: 2019-02-19 22:28:02+00:00
- **Updated**: 2019-10-14 19:56:53+00:00
- **Authors**: Mennatullah Siam, Boris Oreshkin, Martin Jagersand
- **Comment**: Accepted to ICCV'19
- **Journal**: None
- **Summary**: Deep learning has thrived by training on large-scale datasets. However, in robotics applications sample efficiency is critical. We propose a novel adaptive masked proxies method that constructs the final segmentation layer weights from few labelled samples. It utilizes multi-resolution average pooling on base embeddings masked with the label to act as a positive proxy for the new class, while fusing it with the previously learned class signatures. Our method is evaluated on PASCAL-$5^i$ dataset and outperforms the state-of-the-art in the few-shot semantic segmentation. Unlike previous methods, our approach does not require a second branch to estimate parameters or prototypes, which enables it to be used with 2-stream motion and appearance based segmentation networks. We further propose a novel setup for evaluating continual learning of object segmentation which we name incremental PASCAL (iPASCAL) where our method outperforms the baseline method. Our code is publicly available at https://github.com/MSiam/AdaptiveMaskedProxies.



