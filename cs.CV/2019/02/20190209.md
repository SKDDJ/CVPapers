# Arxiv Papers in cs.CV on 2019-02-09
### Photorealistic Image Synthesis for Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.03334v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.03334v1)
- **Published**: 2019-02-09 00:14:46+00:00
- **Updated**: 2019-02-09 00:14:46+00:00
- **Authors**: Tomas Hodan, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon Hanzelka, Treb Connell, Pedro Urbina, Sudipta N. Sinha, Brian Guenter
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to synthesize highly photorealistic images of 3D object models, which we use to train a convolutional neural network for detecting the objects in real images. The proposed approach has three key ingredients: (1) 3D object models are rendered in 3D models of complete scenes with realistic materials and lighting, (2) plausible geometric configuration of objects and cameras in a scene is generated using physics simulations, and (3) high photorealism of the synthesized images achieved by physically based rendering. When trained on images synthesized by the proposed approach, the Faster R-CNN object detector achieves a 24% absolute improvement of mAP@.75IoU on Rutgers APC and 11% on LineMod-Occluded datasets, compared to a baseline where the training images are synthesized by rendering object models on top of random photographs. This work is a step towards being able to effectively train object detectors without capturing or annotating any real images. A dataset of 600K synthetic images with ground truth annotations for various computer vision tasks will be released on the project website: thodan.github.io/objectsynth.



### Challenges in Partially-Automated Roadway Feature Mapping Using Mobile Laser Scanning and Vehicle Trajectory Data
- **Arxiv ID**: http://arxiv.org/abs/1902.03346v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03346v1)
- **Published**: 2019-02-09 01:18:36+00:00
- **Updated**: 2019-02-09 01:18:36+00:00
- **Authors**: Mohammad Billah, Farzana Rahman, Arash Maskooki, Michael Todd, Matthew Barth, Jay A. Farrell
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: Connected vehicle and driver's assistance applications are greatly facilitated by Enhanced Digital Maps (EDMs) that represent roadway features (e.g., lane edges or centerlines, stop bars). Due to the large number of signalized intersections and miles of roadway, manual development of EDMs on a global basis is not feasible. Mobile Terrestrial Laser Scanning (MTLS) is the preferred data acquisition method to provide data for automated EDM development. Such systems provide an MTLS trajectory and a point cloud for the roadway environment. The challenge is to automatically convert these data into an EDM. This article presents a new processing and feature extraction method, experimental demonstration providing SAE-J2735 map messages for eleven example intersections, and a discussion of the results that points out remaining challenges and suggests directions for future research.



### Improving Deep Image Clustering With Spatial Transformer Layers
- **Arxiv ID**: http://arxiv.org/abs/1902.05401v2
- **DOI**: 10.1007/978-3-030-30490-4_51
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05401v2)
- **Published**: 2019-02-09 01:56:24+00:00
- **Updated**: 2019-10-24 13:43:23+00:00
- **Authors**: Thiago V. M. Souza, Cleber Zanchettin
- **Comment**: None
- **Journal**: Artificial Neural Networks and Machine Learning -- ICANN 2019:
  Text and Time Series:641--654,2019
- **Summary**: Image clustering is an important but challenging task in machine learning. As in most image processing areas, the latest improvements came from models based on the deep learning approach. However, classical deep learning methods have problems to deal with spatial image transformations like scale and rotation. In this paper, we propose the use of visual attention techniques to reduce this problem in image clustering methods. We evaluate the combination of a deep image clustering model called Deep Adaptive Clustering (DAC) with the Spatial Transformer Networks (STN). The proposed model is evaluated in the datasets MNIST and FashionMNIST and outperformed the baseline model.



### Image Decomposition and Classification through a Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1902.03361v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03361v1)
- **Published**: 2019-02-09 02:40:04+00:00
- **Updated**: 2019-02-09 02:40:04+00:00
- **Authors**: Houpu Yao, Malcolm Regan, Yezhou Yang, Yi Ren
- **Comment**: None
- **Journal**: None
- **Summary**: We demonstrate in this paper that a generative model can be designed to perform classification tasks under challenging settings, including adversarial attacks and input distribution shifts. Specifically, we propose a conditional variational autoencoder that learns both the decomposition of inputs and the distributions of the resulting components. During test, we jointly optimize the latent variables of the generator and the relaxed component labels to find the best match between the given input and the output of the generator. The model demonstrates promising performance at recognizing overlapping components from the multiMNIST dataset, and novel component combinations from a traffic sign dataset. Experiments also show that the proposed model achieves high robustness on MNIST and NORB datasets, in particular for high-strength gradient attacks and non-gradient attacks.



### Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)
- **Arxiv ID**: http://arxiv.org/abs/1902.03368v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03368v2)
- **Published**: 2019-02-09 04:18:10+00:00
- **Updated**: 2019-03-29 17:36:27+00:00
- **Authors**: Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, Harald Kittler, Allan Halpern
- **Comment**: https://challenge2018.isic-archive.com/
- **Journal**: None
- **Summary**: This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.



### Region based Ensemble Learning Network for Fine-grained Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.03377v1
- **DOI**: 10.1109/CAC.2018.8623687
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03377v1)
- **Published**: 2019-02-09 06:11:32+00:00
- **Updated**: 2019-02-09 06:11:32+00:00
- **Authors**: Weikuang Li, Tian Wang, Chuanyun Wang, Guangcun Shan, Mengyi Zhang, Hichem Snoussi
- **Comment**: 6 pages, 3 figures, 2018 Chinese Automation Congress (CAC)
- **Journal**: None
- **Summary**: As an important research topic in computer vision, fine-grained classification which aims to recognition subordinate-level categories has attracted significant attention. We propose a novel region based ensemble learning network for fine-grained classification. Our approach contains a detection module and a module for classification. The detection module is based on the faster R-CNN framework to locate the semantic regions of the object. The classification module using an ensemble learning method, which trains a set of sub-classifiers for different semantic regions and combines them together to get a stronger classifier. In the evaluation, we implement experiments on the CUB-2011 dataset and the result of experiments proves our method s efficient for fine-grained classification. We also extend our approach to remote scene recognition and evaluate it on the NWPU-RESISC45 dataset.



### When Causal Intervention Meets Adversarial Examples and Image Masking for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.03380v3
- **DOI**: 10.1109/ICIP.2019.8803554
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.SC
- **Links**: [PDF](http://arxiv.org/pdf/1902.03380v3)
- **Published**: 2019-02-09 06:44:13+00:00
- **Updated**: 2019-06-25 15:07:42+00:00
- **Authors**: Chao-Han Huck Yang, Yi-Chieh Liu, Pin-Yu Chen, Xiaoli Ma, Yi-Chang James Tsai
- **Comment**: Noted our camera-ready version has changed the title. "When Causal
  Intervention Meets Adversarial Examples and Image Masking for Deep Neural
  Networks" as the v3 official paper title in IEEE Proceeding. Please use it in
  your formal reference. Accepted at IEEE ICIP 2019. Pytorch code has released
  on https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg
- **Journal**: 2019 26th IEEE International Conference on Image Processing
  (ICIP). IEEE
- **Summary**: Discovering and exploiting the causality in deep neural networks (DNNs) are crucial challenges for understanding and reasoning causal effects (CE) on an explainable visual model. "Intervention" has been widely used for recognizing a causal relation ontologically. In this paper, we propose a causal inference framework for visual reasoning via do-calculus. To study the intervention effects on pixel-level features for causal reasoning, we introduce pixel-wise masking and adversarial perturbation. In our framework, CE is calculated using features in a latent space and perturbed prediction from a DNN-based model. We further provide the first look into the characteristics of discovered CE of adversarially perturbed images generated by gradient-based methods \footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}. Experimental results show that CE is a competitive and robust index for understanding DNNs when compared with conventional methods such as class-activation mappings (CAMs) on the Chest X-Ray-14 dataset for human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds promises for detecting adversarial examples as it possesses distinct characteristics in the presence of adversarial perturbations.



### Iteratively reweighted penalty alternating minimization methods with continuation for image deblurring
- **Arxiv ID**: http://arxiv.org/abs/1902.04062v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.04062v1)
- **Published**: 2019-02-09 08:41:35+00:00
- **Updated**: 2019-02-09 08:41:35+00:00
- **Authors**: Tao Sun, Dongsheng Li, Hao Jiang, Zhe Quan
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider a class of nonconvex problems with linear constraints appearing frequently in the area of image processing. We solve this problem by the penalty method and propose the iteratively reweighted alternating minimization algorithm. To speed up the algorithm, we also apply the continuation strategy to the penalty parameter. A convergence result is proved for the algorithm. Compared with the nonconvex ADMM, the proposed algorithm enjoys both theoretical and computational advantages like weaker convergence requirements and faster speed. Numerical results demonstrate the efficiency of the proposed algorithm.



### Data-Driven Vehicle Trajectory Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1902.05400v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05400v1)
- **Published**: 2019-02-09 12:09:48+00:00
- **Updated**: 2019-02-09 12:09:48+00:00
- **Authors**: Shayan Jawed, Eya Boumaiza, Josif Grabocka, Lars Schmidt-Thieme
- **Comment**: Published in ECML KNOWMe: 2nd International Workshop on Knowledge
  Discovery from Mobility and Transportation Systems 2018
- **Journal**: None
- **Summary**: An active area of research is to increase the safety of self-driving vehicles. Although safety cannot be guarenteed completely, the capability of a vehicle to predict the future trajectories of its surrounding vehicles could help ensure this notion of safety to a greater deal. We cast the trajectory forecast problem in a multi-time step forecasting problem and develop a Convolutional Neural Network based approach to learn from trajectory sequences generated from completely raw dataset in real-time. Results show improvement over baselines.



### Yes, we GAN: Applying Adversarial Techniques for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.03442v2
- **DOI**: 10.2352/ISSN.2470-1173.2019.15.AVM-048
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03442v2)
- **Published**: 2019-02-09 16:42:47+00:00
- **Updated**: 2020-02-02 18:22:01+00:00
- **Authors**: Michal Uricar, Pavel Krizek, David Hurych, Ibrahim Sobh, Senthil Yogamani, Patrick Denny
- **Comment**: Accepted for publication in Electronic Imaging, Autonomous Vehicles
  and Machines 2019. arXiv admin note: text overlap with arXiv:1606.05908 by
  other authors
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have gained a lot of popularity from their introduction in 2014 till present. Research on GAN is rapidly growing and there are many variants of the original GAN focusing on various aspects of deep learning. GAN are perceived as the most impactful direction of machine learning in the last decade. This paper focuses on the application of GAN in autonomous driving including topics such as advanced data augmentation, loss function learning, semi-supervised learning, etc. We formalize and review key applications of adversarial techniques and discuss challenges and open problems to be addressed.



### 3D Hand Shape and Pose from Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1902.03451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03451v1)
- **Published**: 2019-02-09 17:30:16+00:00
- **Updated**: 2019-02-09 17:30:16+00:00
- **Authors**: Adnane Boukhayma, Rodrigo de Bem, Philip H. S. Torr
- **Comment**: None
- **Journal**: None
- **Summary**: We present in this work the first end-to-end deep learning based method that predicts both 3D hand shape and pose from RGB images in the wild. Our network consists of the concatenation of a deep convolutional encoder, and a fixed model-based decoder. Given an input image, and optionally 2D joint detections obtained from an independent CNN, the encoder predicts a set of hand and view parameters. The decoder has two components: A pre-computed articulated mesh deformation hand model that generates a 3D mesh from the hand parameters, and a re-projection module controlled by the view parameters that projects the generated hand into the image domain. We show that using the shape and pose prior knowledge encoded in the hand model within a deep learning framework yields state-of-the-art performance in 3D pose prediction from images on standard benchmarks, and produces geometrically valid and plausible 3D reconstructions. Additionally, we show that training with weak supervision in the form of 2D joint annotations on datasets of images in the wild, in conjunction with full supervision in the form of 3D joint annotations on limited available datasets allows for good generalization to 3D shape and pose predictions on images in the wild.



### Super-realtime facial landmark detection and shape fitting by deep regression of shape model parameters
- **Arxiv ID**: http://arxiv.org/abs/1902.03459v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.03459v1)
- **Published**: 2019-02-09 17:59:07+00:00
- **Updated**: 2019-02-09 17:59:07+00:00
- **Authors**: Marcin Kopaczka, Justus Schock, Dorit Merhof
- **Comment**: https://github.com/justusschock/shapenet
- **Journal**: None
- **Summary**: We present a method for highly efficient landmark detection that combines deep convolutional neural networks with well established model-based fitting algorithms. Motivated by established model-based fitting methods such as active shapes, we use a PCA of the landmark positions to allow generative modeling of facial landmarks. Instead of computing the model parameters using iterative optimization, the PCA is included in a deep neural network using a novel layer type. The network predicts model parameters in a single forward pass, thereby allowing facial landmark detection at several hundreds of frames per second. Our architecture allows direct end-to-end training of a model-based landmark detection method and shows that deep neural networks can be used to reliably predict model parameters directly without the need for an iterative optimization. The method is evaluated on different datasets for facial landmark detection and medical image segmentation. PyTorch code is freely available at https://github.com/justusschock/shapenet



### Hierarchical Multi-task Deep Neural Network Architecture for End-to-End Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.03466v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03466v2)
- **Published**: 2019-02-09 18:23:51+00:00
- **Updated**: 2020-12-02 01:26:54+00:00
- **Authors**: Jose Solomon, Francois Charette
- **Comment**: 18 pages, 17 plots and figures
- **Journal**: None
- **Summary**: A novel hierarchical Deep Neural Network (DNN) model is presented to address the task of end-to-end driving. The model consists of a master classifier network which determines the driving task required from an input stereo image and directs said image to one of a set of subservient network regression models that perform inference and output a steering command. These subservient networks are designed and trained for a specific driving task: straightaway, swerve maneuver, tight turn, gradual turn, and chicane. Using this modular network strategy allows for two primary advantages: an overall reduction in the amount of data required to train the complete system, and for model tailoring where more complex models can be used for more challenging tasks while simplified networks can handle more mundane tasks. It is this latter facet of the model that makes the approach attractive to a number of applications beyond the current vehicle steering strategy.



### Depth-Map Generation using Pixel Matching in Stereoscopic Pair of Images
- **Arxiv ID**: http://arxiv.org/abs/1902.03471v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03471v3)
- **Published**: 2019-02-09 18:44:35+00:00
- **Updated**: 2019-05-15 10:29:37+00:00
- **Authors**: Asra Aslam, Mohd. Samar Ansari
- **Comment**: 5 Pages, pre-print, to be submitted to a conference later
- **Journal**: None
- **Summary**: Modern day multimedia content generation and dissemination is moving towards the presentation of more and more `realistic' scenarios. The switch from 2-dimensional (2D) to 3-dimensional (3D) has been a major driving force in that direction. Over the recent past, a large number of approaches have been proposed for creating 3D images/videos most of which are based on the generation of depth-maps. This paper presents a new algorithm for obtaining depth information pertaining to a depicted scene from a set of available pair of stereoscopic images. The proposed algorithm performs a pixel-to-pixel matching of the two images in the stereo pair for estimation of depth. It is shown that the obtained depth-maps show improvements over the reported counterparts.



### The Omniglot challenge: a 3-year progress report
- **Arxiv ID**: http://arxiv.org/abs/1902.03477v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.03477v2)
- **Published**: 2019-02-09 19:13:31+00:00
- **Updated**: 2019-05-31 20:01:27+00:00
- **Authors**: Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum
- **Comment**: In press at Current Opinion in Behavioral Sciences
- **Journal**: None
- **Summary**: Three years ago, we released the Omniglot dataset for one-shot learning, along with five challenge tasks and a computational model that addresses these tasks. The model was not meant to be the final word on Omniglot; we hoped that the community would build on our work and develop new approaches. In the time since, we have been pleased to see wide adoption of the dataset. There has been notable progress on one-shot classification, but researchers have adopted new splits and procedures that make the task easier. There has been less progress on the other four tasks. We conclude that recent approaches are still far from human-like concept learning on Omniglot, a challenge that requires performing many tasks with a single model.



### An Algorithm Unrolling Approach to Deep Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1902.05399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05399v2)
- **Published**: 2019-02-09 21:19:11+00:00
- **Updated**: 2019-02-15 18:58:12+00:00
- **Authors**: Yuelong Li, Mohammad Tofighi, Vishal Monga, Yonina C. Eldar
- **Comment**: IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP)
- **Journal**: None
- **Summary**: While neural networks have achieved vastly enhanced performance over traditional iterative methods in many cases, they are generally empirically designed and the underlying structures are difficult to interpret. The algorithm unrolling approach has helped connect iterative algorithms to neural network architectures. However, such connections have not been made yet for blind image deblurring. In this paper, we propose a neural network architecture that advances this idea. We first present an iterative algorithm that may be considered a generalization of the traditional total-variation regularization method on the gradient domain, and subsequently unroll the half-quadratic splitting algorithm to construct a neural network. Our proposed deep network achieves significant practical performance gains while enjoying interpretability at the same time. Experimental results show that our approach outperforms many state-of-the-art methods.



### Deep Algorithm Unrolling for Blind Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1902.03493v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03493v3)
- **Published**: 2019-02-09 21:19:35+00:00
- **Updated**: 2019-05-29 16:40:59+00:00
- **Authors**: Yuelong Li, Mohammad Tofighi, Junyi Geng, Vishal Monga, Yonina C. Eldar
- **Comment**: None
- **Journal**: None
- **Summary**: Blind image deblurring remains a topic of enduring interest. Learning based approaches, especially those that employ neural networks have emerged to complement traditional model based methods and in many cases achieve vastly enhanced performance. That said, neural network approaches are generally empirically designed and the underlying structures are difficult to interpret. In recent years, a promising technique called algorithm unrolling has been developed that has helped connect iterative algorithms such as those for sparse coding to neural network architectures. However, such connections have not been made yet for blind image deblurring. In this paper, we propose a neural network architecture based on this idea. We first present an iterative algorithm that may be considered as a generalization of the traditional total-variation regularization method in the gradient domain. We then unroll the algorithm to construct a neural network for image deblurring which we refer to as Deep Unrolling for Blind Deblurring (DUBLID). Key algorithm parameters are learned with the help of training images. Our proposed deep network DUBLID achieves significant practical performance gains while enjoying interpretability at the same time. Extensive experimental results show that DUBLID outperforms many state-of-the-art methods and in addition is computationally faster.



### Inverse Projection Representation and Category Contribution Rate for Robust Tumor Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.03510v2
- **DOI**: 10.1109/TCBB.2018.2886334
- **Categories**: **q-bio.QM**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03510v2)
- **Published**: 2019-02-09 23:07:22+00:00
- **Updated**: 2019-06-27 04:07:28+00:00
- **Authors**: Xiao-Hui Yang, Li Tian, Yun-Mei Chen, Li-Jun Yang, Shuang Xu, Wen-Ming Wu
- **Comment**: 14 pages, 19 figures, 10 tables
- **Journal**: IEEE/ACM Transactions on Computational Biology and Bioinformatics,
  2018
- **Summary**: Sparse representation based classification (SRC) methods have achieved remarkable results. SRC, however, still suffer from requiring enough training samples, insufficient use of test samples and instability of representation. In this paper, a stable inverse projection representation based classification (IPRC) is presented to tackle these problems by effectively using test samples. An IPR is firstly proposed and its feasibility and stability are analyzed. A classification criterion named category contribution rate is constructed to match the IPR and complete classification. Moreover, a statistical measure is introduced to quantify the stability of representation-based classification methods. Based on the IPRC technique, a robust tumor recognition framework is presented by interpreting microarray gene expression data, where a two-stage hybrid gene selection method is introduced to select informative genes. Finally, the functional analysis of candidate's pathogenicity-related genes is given. Extensive experiments on six public tumor microarray gene expression datasets demonstrate the proposed technique is competitive with state-of-the-art methods.



### Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with Visual Memory
- **Arxiv ID**: http://arxiv.org/abs/1902.03514v2
- **DOI**: 10.1109/ICASSP.2019.8683737
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03514v2)
- **Published**: 2019-02-09 23:46:01+00:00
- **Updated**: 2019-04-19 02:23:37+00:00
- **Authors**: Sauradip Nag, Ayan Kumar Bhunia, Aishik Konwer, Partha Pratim Roy
- **Comment**: International Conference on Acoustics, Speech, and Signal
  Processing(ICASSP), 2019
- **Journal**: None
- **Summary**: Facial micro-expressions are sudden involuntary minute muscle movements which reveal true emotions that people try to conceal. Spotting a micro-expression and recognizing it is a major challenge owing to its short duration and intensity. Many works pursued traditional and deep learning based approaches to solve this issue but compromised on learning low-level features and higher accuracy due to unavailability of datasets. This motivated us to propose a novel joint architecture of spatial and temporal network which extracts time-contrasted features from the feature maps to contrast out micro-expression from rapid muscle movements. The usage of time contrasted features greatly improved the spotting of micro-expression from inconspicuous facial movements. Also, we include a memory module to predict the class and intensity of the micro-expression across the temporal frames of the micro-expression clip. Our method achieves superior performance in comparison to other conventional approaches on CASMEII dataset.



