# Arxiv Papers in cs.CV on 2019-02-11
### Towards Segmenting Anything That Moves
- **Arxiv ID**: http://arxiv.org/abs/1902.03715v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03715v4)
- **Published**: 2019-02-11 03:40:48+00:00
- **Updated**: 2020-04-01 01:19:41+00:00
- **Authors**: Achal Dave, Pavel Tokmakov, Deva Ramanan
- **Comment**: Website: http://www.achaldave.com/projects/anything-that-moves/.
  Code: https://github.com/achalddave/segment-any-moving
- **Journal**: None
- **Summary**: Detecting and segmenting individual objects, regardless of their category, is crucial for many applications such as action detection or robotic interaction. While this problem has been well-studied under the classic formulation of spatio-temporal grouping, state-of-the-art approaches do not make use of learning-based methods. To bridge this gap, we propose a simple learning-based approach for spatio-temporal grouping. Our approach leverages motion cues from optical flow as a bottom-up signal for separating objects from each other. Motion cues are then combined with appearance cues that provide a generic objectness prior for capturing the full extent of objects. We show that our approach outperforms all prior work on the benchmark FBMS dataset. One potential worry with learning-based methods is that they might overfit to the particular type of objects that they have been trained on. To address this concern, we propose two new benchmarks for generic, moving object detection, and show that our model matches top-down methods on common categories, while significantly out-performing both top-down and bottom-up methods on never-before-seen categories.



### UcoSLAM: Simultaneous Localization and Mapping by Fusion of KeyPoints and Squared Planar Markers
- **Arxiv ID**: http://arxiv.org/abs/1902.03729v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03729v1)
- **Published**: 2019-02-11 04:48:38+00:00
- **Updated**: 2019-02-11 04:48:38+00:00
- **Authors**: Rafael Munoz-Salinas, Rafael Medina-Carnicer
- **Comment**: Paper submitted to Pattern Recognition
- **Journal**: None
- **Summary**: This paper proposes a novel approach for Simultaneous Localization and Mapping by fusing natural and artificial landmarks. Most of the SLAM approaches use natural landmarks (such as keypoints). However, they are unstable over time, repetitive in many cases or insufficient for a robust tracking (e.g. in indoor buildings). On the other hand, other approaches have employed artificial landmarks (such as squared fiducial markers) placed in the environment to help tracking and relocalization. We propose a method that integrates both approaches in order to achieve long-term robust tracking in many scenarios.   Our method has been compared to the start-of-the-art methods ORB-SLAM2 and LDSO in the public dataset Kitti, Euroc-MAV, TUM and SPM, obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently.



### Visual SLAM: Why Bundle Adjust?
- **Arxiv ID**: http://arxiv.org/abs/1902.03747v2
- **DOI**: 10.1109/ICRA.2019.8793749
- **Categories**: **cs.CV**, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1902.03747v2)
- **Published**: 2019-02-11 06:58:38+00:00
- **Updated**: 2019-06-14 03:28:08+00:00
- **Authors**: Álvaro Parra, Tat-Jun Chin, Anders Eriksson, Ian Reid
- **Comment**: Accepted to ICRA 2019
- **Journal**: None
- **Summary**: Bundle adjustment plays a vital role in feature-based monocular SLAM. In many modern SLAM pipelines, bundle adjustment is performed to estimate the 6DOF camera trajectory and 3D map (3D point cloud) from the input feature tracks. However, two fundamental weaknesses plague SLAM systems based on bundle adjustment. First, the need to carefully initialise bundle adjustment means that all variables, in particular the map, must be estimated as accurately as possible and maintained over time, which makes the overall algorithm cumbersome. Second, since estimating the 3D structure (which requires sufficient baseline) is inherent in bundle adjustment, the SLAM algorithm will encounter difficulties during periods of slow motion or pure rotational motion.   We propose a different SLAM optimisation core: instead of bundle adjustment, we conduct rotation averaging to incrementally optimise only camera orientations. Given the orientations, we estimate the camera positions and 3D points via a quasi-convex formulation that can be solved efficiently and globally optimally. Our approach not only obviates the need to estimate and maintain the positions and 3D map at keyframe rate (which enables simpler SLAM systems), it is also more capable of handling slow motions or pure rotational motions.



### Peeking into the Future: Predicting Future Person Activities and Locations in Videos
- **Arxiv ID**: http://arxiv.org/abs/1902.03748v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03748v3)
- **Published**: 2019-02-11 07:02:18+00:00
- **Updated**: 2019-05-31 22:53:41+00:00
- **Authors**: Junwei Liang, Lu Jiang, Juan Carlos Niebles, Alexander Hauptmann, Li Fei-Fei
- **Comment**: In CVPR 2019. Code, models and more results are available at:
  https://next.cs.cmu.edu/
- **Journal**: None
- **Summary**: Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with an auxiliary task of predicting future location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that joint modeling of paths and activities benefits future path prediction.



### Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded
- **Arxiv ID**: http://arxiv.org/abs/1902.03751v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03751v2)
- **Published**: 2019-02-11 07:28:03+00:00
- **Updated**: 2019-10-28 10:30:28+00:00
- **Authors**: Ramprasaath R. Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, Devi Parikh
- **Comment**: Published at ICCV'2019
- **Journal**: The IEEE International Conference on Computer Vision (ICCV) 2019
- **Summary**: Many vision and language models suffer from poor visual grounding - often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances - ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data.



### Latent Space Reinforcement Learning for Steering Angle Prediction
- **Arxiv ID**: http://arxiv.org/abs/1902.03765v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03765v1)
- **Published**: 2019-02-11 08:14:34+00:00
- **Updated**: 2019-02-11 08:14:34+00:00
- **Authors**: Qadeer Khan, Torsten Schön, Patrick Wenzel
- **Comment**: None
- **Journal**: None
- **Summary**: Model-free reinforcement learning has recently been shown to successfully learn navigation policies from raw sensor data. In this work, we address the problem of learning driving policies for an autonomous agent in a high-fidelity simulator. Building upon recent research that applies deep reinforcement learning to navigation problems, we present a modular deep reinforcement learning approach to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image. The control module trained with reinforcement learning takes the latent vector as input to predict the correct steering angle. The experimental results have showed that our method is capable of learning to maneuver the car without any human control signals.



### Pornographic Image Recognition via Weighted Multiple Instance Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.03771v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03771v1)
- **Published**: 2019-02-11 08:24:53+00:00
- **Updated**: 2019-02-11 08:24:53+00:00
- **Authors**: Jin Xin, Wang Yuhui, Tan Xiaoyang
- **Comment**: 9 pages, 3 figures
- **Journal**: IEEE transactions on cybernetics, 2018
- **Summary**: In the era of Internet, recognizing pornographic images is of great significance for protecting children's physical and mental health. However, this task is very challenging as the key pornographic contents (e.g., breast and private part) in an image often lie in local regions of small size. In this paper, we model each image as a bag of regions, and follow a multiple instance learning (MIL) approach to train a generic region-based recognition model. Specifically, we take into account the region's degree of pornography, and make three main contributions. First, we show that based on very few annotations of the key pornographic contents in a training image, we can generate a bag of properly sized regions, among which the potential positive regions usually contain useful contexts that can aid recognition. Second, we present a simple quantitative measure of a region's degree of pornography, which can be used to weigh the importance of different regions in a positive image. Third, we formulate the recognition task as a weighted MIL problem under the convolutional neural network framework, with a bag probability function introduced to combine the importance of different regions. Experiments on our newly collected large scale dataset demonstrate the effectiveness of the proposed method, achieving an accuracy with 97.52% true positive rate at 1% false positive rate, tested on 100K pornographic images and 100K normal images.



### Semantic Label Reduction Techniques for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.03777v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03777v1)
- **Published**: 2019-02-11 08:38:26+00:00
- **Updated**: 2019-02-11 08:38:26+00:00
- **Authors**: Qadeer Khan, Torsten Schön, Patrick Wenzel
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation maps can be used as input to models for maneuvering the controls of a car. However, not all labels may be necessary for making the control decision. One would expect that certain labels such as road lanes or sidewalks would be more critical in comparison with labels for vegetation or buildings which may not have a direct influence on the car's driving decision. In this appendix, we evaluate and quantify how sensitive and important the different semantic labels are for controlling the car. Labels that do not influence the driving decision are remapped to other classes, thereby simplifying the task by reducing to only labels critical for driving of the vehicle.



### Exploring Explicit Domain Supervision for Latent Space Disentanglement in Unpaired Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1902.03782v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03782v4)
- **Published**: 2019-02-11 09:07:30+00:00
- **Updated**: 2019-10-22 08:24:52+00:00
- **Authors**: Jianxin Lin, Zhibo Chen, Yingce Xia, Sen Liu, Tao Qin, Jiebo Luo
- **Comment**: Accepted by IEEE Transaction on Pattern Analysis and Machine
  Intelligence (TPAMI).13 pages, 11 figures, 7 Tables
- **Journal**: None
- **Summary**: Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs). However, existing approaches are mostly designed in an unsupervised manner while little attention has been paid to domain information within unpaired data. In this paper, we treat domain information as explicit supervision and design an unpaired image-to-image translation framework, Domain-supervised GAN (DosGAN), which takes the first step towards the exploration of explicit domain supervision. In contrast to representing domain characteristics using different generators or domain codes, we pre-train a classification network to explicitly classify the domain of an image. After pre-training, this network is used to extract the domain-specific features of each image. Such features, together with the domain-independent features extracted by another encoder (shared across different domains), are used to generate image in target domain. Extensive experiments on multiple facial attribute translation, multiple identity translation, multiple season translation and conditional edges-to-shoes/handbags demonstrate the effectiveness of our method. In addition, we can transfer the domain-specific feature extractor obtained on the Facescrub dataset with domain supervision information to unseen domains, such as faces in the CelebA dataset. We also succeed in achieving conditional translation with any two images in CelebA, while previous models like StarGAN cannot handle this task.



### Dense Depth Estimation of a Complex Dynamic Scene without Explicit 3D Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.03791v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03791v2)
- **Published**: 2019-02-11 09:45:52+00:00
- **Updated**: 2019-03-23 10:15:21+00:00
- **Authors**: Suryansh Kumar, Ram Srivatsav Ghorakavi, Yuchao Dai, Hongdong Li
- **Comment**: Extension to our ICCV'17 work. 10 pages, 10 Figures, 2 tables
- **Journal**: None
- **Summary**: Recent geometric methods need reliable estimates of 3D motion parameters to procure accurate dense depth map of a complex dynamic scene from monocular images \cite{kumar2017monocular, ranftl2016dense}. Generally, to estimate \textbf{precise} measurements of relative 3D motion parameters and to validate its accuracy using image data is a challenging task. In this work, we propose an alternative approach that circumvents the 3D motion estimation requirement to obtain a dense depth map of a dynamic scene. Given per-pixel optical flow correspondences between two consecutive frames and, the sparse depth prior for the reference frame, we show that, we can effectively recover the dense depth map for the successive frames without solving for 3D motion parameters. Our method assumes a piece-wise planar model of a dynamic scene, which undergoes rigid transformation locally, and as-rigid-as-possible transformation globally between two successive frames. Under our assumption, we can avoid the explicit estimation of 3D rotation and translation to estimate scene depth. In essence, our formulation provides an unconventional way to think and recover the dense depth map of a complex dynamic scene which is incremental and motion free in nature. Our proposed method does not make object level or any other high-level prior assumption about the dynamic scene, as a result, it is applicable to a wide range of scenarios. Experimental results on the benchmarks dataset show the competence of our approach for multiple frames.



### Additional Baseline Metrics for the paper "Extended YouTube Faces: a Dataset for Heterogeneous Open-Set Face Identification"
- **Arxiv ID**: http://arxiv.org/abs/1902.03804v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03804v1)
- **Published**: 2019-02-11 10:26:21+00:00
- **Updated**: 2019-02-11 10:26:21+00:00
- **Authors**: Claudio Ferrari, Stefano Berretti, Alberto Del Bimbo
- **Comment**: 3 pages, 2 figures
- **Journal**: None
- **Summary**: In this report, we provide additional and corrected results for the paper "Extended YouTube Faces: a Dataset for Heterogeneous Open-Set Face Identification". After further investigations, we discovered and corrected wrongly labeled images and incorrect identities. This forced us to re-generate the evaluation protocol for the new data; in doing so, we also reproduced and extended the experimental results with other standard metrics and measures used in the literature. The reader can refer to the original paper for additional details regarding the data collection procedure and recognition pipeline.



### GET-AID: Visual Recognition of Human Rights Abuses via Global Emotional Traits
- **Arxiv ID**: http://arxiv.org/abs/1902.03817v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03817v1)
- **Published**: 2019-02-11 11:15:09+00:00
- **Updated**: 2019-02-11 11:15:09+00:00
- **Authors**: Grigorios Kalliatakis, Shoaib Ehsan, Maria Fasli, Klaus D. McDonald-Maier
- **Comment**: 10 pages, 6 figures
- **Journal**: None
- **Summary**: In the era of social media and big data, the use of visual evidence to document conflict and human rights abuse has become an important element for human rights organizations and advocates. In this paper, we address the task of detecting two types of human rights abuses in challenging, everyday photos: (1) child labour, and (2) displaced populations. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the emotional state of a person -- how positive or pleasant an emotion is, and the control level of the situation by the person -- are powerful cues for perceiving potential human rights violations. To exploit these cues, our model learns to predict global emotional traits over a given image based on the joint analysis of every detected person and the whole scene. By integrating these predictions with a data-driven convolutional neural network (CNN) classifier, our system efficiently infers potential human rights abuses in a clean, end-to-end system we call GET-AID (from Global Emotional Traits for Abuse IDentification). Extensive experiments are performed to verify our method on the recently introduced subset of Human Rights Archive (HRA) dataset (2 violation categories with the same number of positive and negative samples), where we show quantitatively compelling results. Compared with previous works and the sole use of a CNN classifier, this paper improves the coverage up to 23.73% for child labour and 57.21% for displaced populations. Our dataset, codes and trained models are available online at https://github.com/GKalliatakis/GET-AID.



### Semi-Supervised and Task-Driven Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.05396v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05396v2)
- **Published**: 2019-02-11 11:21:23+00:00
- **Updated**: 2019-02-28 10:08:23+00:00
- **Authors**: Krishna Chaitanya, Neerav Karani, Christian Baumgartner, Olivio Donati, Anton Becker, Ender Konukoglu
- **Comment**: 13 pages, 3 figures, 1 table, This article has been accepted at the
  26th international conference on Information Processing in Medical Imaging
  (IPMI) 2019
- **Journal**: None
- **Summary**: Supervised deep learning methods for segmentation require large amounts of labelled training data, without which they are prone to overfitting, not generalizing well to unseen images. In practice, obtaining a large number of annotations from clinical experts is expensive and time-consuming. One way to address scarcity of annotated examples is data augmentation using random spatial and intensity transformations. Recently, it has been proposed to use generative models to synthesize realistic training examples, complementing the random augmentation. So far, these methods have yielded limited gains over the random augmentation. However, there is potential to improve the approach by (i) explicitly modeling deformation fields (non-affine spatial transformation) and intensity transformations and (ii) leveraging unlabelled data during the generative process. With this motivation, we propose a novel task-driven data augmentation method where to synthesize new training examples, a generative network explicitly models and applies deformation fields and additive intensity masks on existing labelled data, modeling shape and intensity variations, respectively. Crucially, the generative model is optimized to be conducive to the task, in this case segmentation, and constrained to match the distribution of images observed from labelled and unlabelled samples. Furthermore, explicit modeling of deformation fields allow synthesizing segmentation masks and images in exact correspondence by simply applying the generated transformation to an input image and the corresponding annotation. Our experiments on cardiac magnetic resonance images (MRI) showed that, for the task of segmentation in small training data scenarios, the proposed method substantially outperforms conventional augmentation techniques.



### Semantic Hierarchical Priors for Intrinsic Image Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1902.03830v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03830v2)
- **Published**: 2019-02-11 11:46:52+00:00
- **Updated**: 2019-06-06 09:35:34+00:00
- **Authors**: Saurabh Saini, P. J. Narayanan
- **Comment**: None
- **Journal**: None
- **Summary**: Intrinsic Image Decomposition (IID) is a challenging and interesting computer vision problem with various applications in several fields. We present novel semantic priors and an integrated approach for single image IID that involves analyzing image at three hierarchical context levels. Local context priors capture scene properties at each pixel within a small neighbourhood. Mid-level context priors encode object level semantics. Global context priors establish correspondences at the scene level. Our semantic priors are designed on both fixed and flexible regions, using selective search method and Convolutional Neural Network features. Our IID method is an iterative multistage optimization scheme and consists of two complementary formulations: $L_2$ smoothing for shading and $L_1$ sparsity for reflectance. Experiments and analysis of our method indicate the utility of our semantic priors and structured hierarchical analysis in an IID framework. We compare our method with other contemporary IID solutions and show results with lesser artifacts. Finally, we highlight that proper choice and encoding of prior knowledge can produce competitive results even when compared to end-to-end deep learning IID methods, signifying the importance of such priors. We believe that the insights and techniques presented in this paper would be useful in the future IID research.



### Robust statistics and no-reference image quality assessment in Curvelet domain
- **Arxiv ID**: http://arxiv.org/abs/1902.03842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03842v1)
- **Published**: 2019-02-11 12:30:01+00:00
- **Updated**: 2019-02-11 12:30:01+00:00
- **Authors**: Ramon Giostri Campos, Evandro Ottoni Teatini Salles
- **Comment**: Article published in the XIV Workshop of Computer Vision, ISBN:
  978-85-7455-514-0, this version has 7 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: This paper uses robust statistics and curvelet transform to learn a general-purpose no-reference (NR) image quality assessment (IQA) model. The new approach, here called M1, competes with the Curvelet Quality Assessment proposed in 2014 (Curvelet2014). The central idea is to use descriptors based on robust statistics to extract features and predict the human opinion about degraded images. To show the consistency of the method the model is tested with 3 different datasets, LIVE IQA, TID2013 and CSIQ. To test evaluation, it is used the Wilcoxon test to verify the statistical significance of results and promote an accurate comparison between new model M1 and Curvelet2014. The results show a gain when robust statistics are used as descriptor.



### Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.03932v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ME, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.03932v2)
- **Published**: 2019-02-11 15:03:30+00:00
- **Updated**: 2020-05-11 20:49:28+00:00
- **Authors**: Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, Andrew Gordon Wilson
- **Comment**: Published at ICLR 2020
- **Journal**: None
- **Summary**: The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We also prove non-asymptotic convergence of our proposed algorithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the scalability and effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.



### MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1902.03938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03938v1)
- **Published**: 2019-02-11 15:15:45+00:00
- **Updated**: 2019-02-11 15:15:45+00:00
- **Authors**: Sanghyeon Na, Seungjoo Yoo, Jaegul Choo
- **Comment**: None
- **Journal**: None
- **Summary**: Unpaired multimodal image-to-image translation is a task of translating a given image in a source domain into diverse images in the target domain, overcoming the limitation of one-to-one mapping. Existing multimodal translation models are mainly based on the disentangled representations with an image reconstruction loss. We propose two approaches to improve multimodal translation quality. First, we use a content representation from the source domain conditioned on a style representation from the target domain. Second, rather than using a typical image reconstruction loss, we design MILO (Mutual Information LOss), a new stochastically-defined loss function based on information theory. This loss function directly reflects the interpretation of latent variables as a random variable. We show that our proposed model Mutual Information with StOchastic Style Representation(MISO) achieves state-of-the-art performance through extensive experiments on various real-world datasets.



### Color Image and Multispectral Image Denoising Using Block Diagonal Representation
- **Arxiv ID**: http://arxiv.org/abs/1902.03954v1
- **DOI**: 10.1109/TIP.2019.2907478
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.03954v1)
- **Published**: 2019-02-11 15:50:10+00:00
- **Updated**: 2019-02-11 15:50:10+00:00
- **Authors**: Zhaoming Kong, Xiaowei Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Filtering images of more than one channel is challenging in terms of both efficiency and effectiveness. By grouping similar patches to utilize the self-similarity and sparse linear approximation of natural images, recent nonlocal and transform-domain methods have been widely used in color and multispectral image (MSI) denoising. Many related methods focus on the modeling of group level correlation to enhance sparsity, which often resorts to a recursive strategy with a large number of similar patches. The importance of the patch level representation is understated. In this paper, we mainly investigate the influence and potential of representation at patch level by considering a general formulation with block diagonal matrix. We further show that by training a proper global patch basis, along with a local principal component analysis transform in the grouping dimension, a simple transform-threshold-inverse method could produce very competitive results. Fast implementation is also developed to reduce computational complexity. Extensive experiments on both simulated and real datasets demonstrate its robustness, effectiveness and efficiency.



### Gauge Equivariant Convolutional Networks and the Icosahedral CNN
- **Arxiv ID**: http://arxiv.org/abs/1902.04615v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04615v3)
- **Published**: 2019-02-11 17:01:05+00:00
- **Updated**: 2019-05-13 23:03:52+00:00
- **Authors**: Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling
- **Comment**: Proceedings of the International Conference on Machine Learning
  (ICML), 2019
- **Journal**: None
- **Summary**: The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.



### Deep Learning Methods for Event Verification and Image Repurposing Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.04038v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04038v1)
- **Published**: 2019-02-11 18:32:30+00:00
- **Updated**: 2019-02-11 18:32:30+00:00
- **Authors**: M. Goebel, A. Flenner, L. Nataraj, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: The authenticity of images posted on social media is an issue of growing concern. Many algorithms have been developed to detect manipulated images, but few have investigated the ability of deep neural network based approaches to verify the authenticity of image labels, such as event names. In this paper, we propose several novel methods to predict if an image was captured at one of several noteworthy events. We use a set of images from several recorded events such as storms, marathons, protests, and other large public gatherings. Two strategies of applying pre-trained Imagenet network for event verification are presented, with two modifications for each strategy. The first method uses the features from the last convolutional layer of a pre-trained network as input to a classifier. We also consider the effects of tuning the convolutional weights of the pre-trained network to improve classification. The second method combines many features extracted from smaller scales and uses the output of a pre-trained network as the input to a second classifier. For both methods, we investigated several different classifiers and tested many different pre-trained networks. Our experiments demonstrate both these approaches are effective for event verification and image re-purposing detection. The classification at the global scale tends to marginally outperform our tested local methods and fine tuning the network further improves the results.



### Registration-free Face-SSD: Single shot analysis of smiles, facial attributes, and affect in the wild
- **Arxiv ID**: http://arxiv.org/abs/1902.04042v1
- **DOI**: 10.1016/j.cviu.2019.01.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04042v1)
- **Published**: 2019-02-11 18:43:31+00:00
- **Updated**: 2019-02-11 18:43:31+00:00
- **Authors**: Youngkyoon Jang, Hatice Gunes, Ioannis Patras
- **Comment**: 14 pages, 9 figures, 8 tables, accepted for Elsevier CVIU 2019
- **Journal**: None
- **Summary**: In this paper, we present a novel single shot face-related task analysis method, called Face-SSD, for detecting faces and for performing various face-related (classification/regression) tasks including smile recognition, face attribute prediction and valence-arousal estimation in the wild. Face-SSD uses a Fully Convolutional Neural Network (FCNN) to detect multiple faces of different sizes and recognise/regress one or more face-related classes. Face-SSD has two parallel branches that share the same low-level filters, one branch dealing with face detection and the other one with face analysis tasks. The outputs of both branches are spatially aligned heatmaps that are produced in parallel - therefore Face-SSD does not require that face detection, facial region extraction, size normalisation, and facial region processing are performed in subsequent steps. Our contributions are threefold: 1) Face-SSD is the first network to perform face analysis without relying on pre-processing such as face detection and registration in advance - Face-SSD is a simple and a single FCNN architecture simultaneously performing face detection and face-related task analysis - those are conventionally treated as separate consecutive tasks; 2) Face-SSD is a generalised architecture that is applicable for various face analysis tasks without modifying the network structure - this is in contrast to designing task-specific architectures; and 3) Face-SSD achieves real-time performance (21 FPS) even when detecting multiple faces and recognising multiple classes in a given image. Experimental results show that Face-SSD achieves state-of-the-art performance in various face analysis tasks by reaching a recognition accuracy of 95.76% for smile detection, 90.29% for attribute prediction, and Root Mean Square (RMS) error of 0.44 and 0.39 for valence and arousal estimation.



### MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04049v1
- **DOI**: 10.1016/j.neunet.2019.08.025
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04049v1)
- **Published**: 2019-02-11 18:50:11+00:00
- **Updated**: 2019-02-11 18:50:11+00:00
- **Authors**: Nabil Ibtehaz, M. Sohel Rahman
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. U-Net is the most prominent deep network in this regard, which has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, from extensive experimentations on challenging datasets, we found out that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Hence, following the modifications we develop a novel architecture MultiResUNet as the potential successor to the successful U-Net architecture. We have compared our proposed architecture MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Albeit slight improvements in the cases of ideal images, a remarkable gain in performance has been attained for challenging images. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%, 1.41%, and 0.62% respectively.



### S-DOD-CNN: Doubly Injecting Spatially-Preserved Object Information for Event Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.04051v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04051v2)
- **Published**: 2019-02-11 18:52:59+00:00
- **Updated**: 2020-02-01 00:35:19+00:00
- **Authors**: Hyungtae Lee, Sungmin Eum, Heesung Kwon
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: We present a novel event recognition approach called Spatially-preserved Doubly-injected Object Detection CNN (S-DOD-CNN), which incorporates the spatially preserved object detection information in both a direct and an indirect way. Indirect injection is carried out by simply sharing the weights between the object detection modules and the event recognition module. Meanwhile, our novelty lies in the fact that we have preserved the spatial information for the direct injection. Once multiple regions-of-intereset (RoIs) are acquired, their feature maps are computed and then projected onto a spatially-preserving combined feature map using one of the four RoI Projection approaches we present. In our architecture, combined feature maps are generated for object detection which are directly injected to the event recognition module. Our method provides the state-of-the-art accuracy for malicious event recognition.



### Psi-Net: Shape and boundary aware joint multi-task deep network for medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.04099v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04099v3)
- **Published**: 2019-02-11 19:11:49+00:00
- **Updated**: 2019-08-14 17:17:37+00:00
- **Authors**: Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Accepted at EMBC 2019
- **Journal**: None
- **Summary**: Image segmentation is a primary task in many medical applications. Recently, many deep networks derived from U-Net have been extensively used in various medical image segmentation tasks. However, in most of the cases, networks similar to U-net produce coarse and non-smooth segmentations with lots of discontinuities. To improve and refine the performance of U-Net like networks, we propose the use of parallel decoders which along with performing the mask predictions also perform contour prediction and distance map estimation. The contour and distance map aid in ensuring smoothness in the segmentation predictions. To facilitate joint training of three tasks, we propose a novel architecture called Psi-Net with a single encoder and three parallel decoders (thus having a shape of $\Psi$), one decoder to learns the segmentation mask prediction and other two decoders to learn the auxiliary tasks of contour detection and distance map estimation. The learning of these auxiliary tasks helps in capturing the shape and the boundary information. We also propose a new joint loss function for the proposed architecture. The loss function consists of a weighted combination of Negative Log likelihood and Mean Square Error loss. We have used two publicly available datasets: 1) Origa dataset for the task of optic cup and disc segmentation and 2) Endovis segment dataset for the task of polyp segmentation to evaluate our model. We have conducted extensive experiments using our network to show our model gives better results in terms of segmentation, boundary and shape metrics.



### Bag of Freebies for Training Object Detection Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.04103v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04103v3)
- **Published**: 2019-02-11 19:20:09+00:00
- **Updated**: 2019-04-12 18:04:05+00:00
- **Authors**: Zhi Zhang, Tong He, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Training heuristics greatly improve various image classification model accuracies~\cite{he2018bag}. Object detection models, however, have more complex neural network structures and optimization targets. The training strategies and pipelines dramatically vary among different models. In this works, we explore training tweaks that apply to various models including Faster R-CNN and YOLOv3. These tweaks do not change the model architectures, therefore, the inference costs remain the same. Our empirical results demonstrate that, however, these freebies can improve up to 5% absolute precision compared to state-of-the-art baselines.



### Yelp Food Identification via Image Feature Extraction and Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.05413v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05413v1)
- **Published**: 2019-02-11 19:34:34+00:00
- **Updated**: 2019-02-11 19:34:34+00:00
- **Authors**: Fanbo Sun, Zhixiang Gu, Bo Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Yelp has been one of the most popular local service search engine in US since 2004. It is powered by crowd-sourced text reviews and photo reviews. Restaurant customers and business owners upload photo images to Yelp, including reviewing or advertising either food, drinks, or inside and outside decorations. It is obviously not so effective that labels for food photos rely on human editors, which is an issue should be addressed by innovative machine learning approaches. In this paper, we present a simple but effective approach which can identify up to ten kinds of food via raw photos from the challenge dataset. We use 1) image pre-processing techniques, including filtering and image augmentation, 2) feature extraction via convolutional neural networks (CNN), and 3) three ways of classification algorithms. Then, we illustrate the classification accuracy by tuning parameters for augmentations, CNN, and classification. Our experimental results show this simple but effective approach to identify up to 10 food types from images.



### Using Deep Cross Modal Hashing and Error Correcting Codes for Improving the Efficiency of Attribute Guided Facial Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1902.04139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04139v1)
- **Published**: 2019-02-11 20:42:52+00:00
- **Updated**: 2019-02-11 20:42:52+00:00
- **Authors**: Veeru Talreja, Fariborz Taherkhani, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: To be published in Proc. IEEE Global SIP 2018
- **Journal**: None
- **Summary**: With benefits of fast query speed and low storage cost, hashing-based image retrieval approaches have garnered considerable attention from the research community. In this paper, we propose a novel Error-Corrected Deep Cross Modal Hashing (CMH-ECC) method which uses a bitmap specifying the presence of certain facial attributes as an input query to retrieve relevant face images from the database. In this architecture, we generate compact hash codes using an end-to-end deep learning module, which effectively captures the inherent relationships between the face and attribute modality. We also integrate our deep learning module with forward error correction codes to further reduce the distance between different modalities of the same subject. Specifically, the properties of deep hashing and forward error correction codes are exploited to design a cross modal hashing framework with high retrieval performance. Experimental results using two standard datasets with facial attributes-image modalities indicate that our CMH-ECC face image retrieval model outperforms most of the current attribute-based face image retrieval approaches.



### Max-C and Min-D Projection Autoassociative Fuzzy Morphological Memories: Theory and an Application for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.04144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04144v2)
- **Published**: 2019-02-11 21:00:43+00:00
- **Updated**: 2019-08-31 00:46:32+00:00
- **Authors**: Alex Santana dos Santos, Marcos Eduardo Valle
- **Comment**: None
- **Journal**: None
- **Summary**: Max-C and min-D projection autoassociative fuzzy morphological memories (max-C and min-D PAFMMs) are two layer feedforward fuzzy morphological neural networks able to implement an associative memory designed for the storage and retrieval of finite fuzzy sets or vectors on a hypercube. In this paper we address the main features of these autoassociative memories, which include unlimited absolute storage capacity, fast retrieval of stored items, few spurious memories, and an excellent tolerance to either dilative noise or erosive noise. Particular attention is given to the so-called PAFMM of Zadeh which, besides performing no floating-point operations, exhibit the largest noise tolerance among max-C and min-D PAFMMs. Computational experiments reveal that Zadeh's max-C PFAMM, combined with a noise masking strategy, yields a fast and robust classifier with strong potential for face recognition.



### Synthesizing New Retinal Symptom Images by Multiple Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1902.04147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.04147v1)
- **Published**: 2019-02-11 21:07:14+00:00
- **Updated**: 2019-02-11 21:07:14+00:00
- **Authors**: Yi-Chieh Liu, Hao-Hsiang Yang, Chao-Han Huck Yang, Jia-Hong Huang, Meng Tian, Hiromasa Morikawa, Yi-Chang James Tsai, Jesper Tegner
- **Comment**: None
- **Journal**: AI for Retinal Image Analysis Workshop ACCV 2018
- **Summary**: Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease which may result in loss of vision. There is limited access to high-quality relevant retinal images and poor understanding of the features defining sub-classes of this disease. Motivated by recent advances in machine learning we specifically explore the potential of generative modeling, using Generative Adversarial Networks (GANs) and style transferring, to facilitate clinical diagnosis and disease understanding by feature extraction. We design an analytic pipeline which first generates synthetic retinal images from clinical images; a subsequent verification step is applied. In the synthesizing step we merge GANs (DCGANs and WGANs architectures) and style transferring for the image generation, whereas the verified step controls the accuracy of the generated images. We find that the generated images contain sufficient pathological details to facilitate ophthalmologists' task of disease classification and in discovery of disease relevant features. In particular, our system predicts the drusen and geographic atrophy sub-classes of AMD. Furthermore, the performance using CFP images for GANs outperforms the classification based on using only the original clinical dataset. Our results are evaluated using existing classifier of retinal diseases and class activated maps, supporting the predictive power of the synthetic images and their utility for feature extraction. Our code examples are available online.



### Learning to Authenticate with Deep Multibiometric Hashing and Neural Network Decoding
- **Arxiv ID**: http://arxiv.org/abs/1902.04149v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.04149v3)
- **Published**: 2019-02-11 21:08:10+00:00
- **Updated**: 2019-03-07 15:54:15+00:00
- **Authors**: Veeru Talreja, Sobhan Soleymani, Matthew C. Valenti, Nasser M. Nasrabadi
- **Comment**: To be published in Proc. IEEE ICC 2019
- **Journal**: None
- **Summary**: In this paper, we propose a novel multimodal deep hashing neural decoder (MDHND) architecture, which integrates a deep hashing framework with a neural network decoder (NND) to create an effective multibiometric authentication system. The MDHND consists of two separate modules: a multimodal deep hashing (MDH) module, which is used for feature-level fusion and binarization of multiple biometrics, and a neural network decoder (NND) module, which is used to refine the intermediate binary codes generated by the MDH and compensate for the difference between enrollment and probe biometrics (variations in pose, illumination, etc.). Use of NND helps to improve the performance of the overall multimodal authentication system. The MDHND framework is trained in 3 steps using joint optimization of the two modules. In Step 1, the MDH parameters are trained and learned to generate a shared multimodal latent code; in Step 2, the latent codes from Step 1 are passed through a conventional error-correcting code (ECC) decoder to generate the ground truth to train a neural network decoder (NND); in Step 3, the NND decoder is trained using the ground truth from Step 2 and the MDH and NND are jointly optimized. Experimental results on a standard multimodal dataset demonstrate the superiority of our method relative to other current multimodal authentication systems



### ReStoCNet: Residual Stochastic Binary Convolutional Spiking Neural Network for Memory-Efficient Neuromorphic Computing
- **Arxiv ID**: http://arxiv.org/abs/1902.04161v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.04161v1)
- **Published**: 2019-02-11 21:54:48+00:00
- **Updated**: 2019-02-11 21:54:48+00:00
- **Authors**: Gopalakrishnan Srinivasan, Kaushik Roy
- **Comment**: 27 pages, 11 figures, and 6 tables
- **Journal**: None
- **Summary**: In this work, we propose ReStoCNet, a residual stochastic multilayer convolutional Spiking Neural Network (SNN) composed of binary kernels, to reduce the synaptic memory footprint and enhance the computational efficiency of SNNs for complex pattern recognition tasks. ReStoCNet consists of an input layer followed by stacked convolutional layers for hierarchical input feature extraction, pooling layers for dimensionality reduction, and fully-connected layer for inference. In addition, we introduce residual connections between the stacked convolutional layers to improve the hierarchical feature learning capability of deep SNNs. We propose Spike Timing Dependent Plasticity (STDP) based probabilistic learning algorithm, referred to as Hybrid-STDP (HB-STDP), incorporating Hebbian and anti-Hebbian learning mechanisms, to train the binary kernels forming ReStoCNet in a layer-wise unsupervised manner. We demonstrate the efficacy of ReStoCNet and the presented HB-STDP based unsupervised training methodology on the MNIST and CIFAR-10 datasets. We show that residual connections enable the deeper convolutional layers to self-learn useful high-level input features and mitigate the accuracy loss observed in deep SNNs devoid of residual connections. The proposed ReStoCNet offers >20x kernel memory compression compared to full-precision (32-bit) SNN while yielding high enough classification accuracy on the chosen pattern recognition tasks.



### Riemannian joint dimensionality reduction and dictionary learning on symmetric positive definite manifold
- **Arxiv ID**: http://arxiv.org/abs/1902.04186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.04186v1)
- **Published**: 2019-02-11 23:49:03+00:00
- **Updated**: 2019-02-11 23:49:03+00:00
- **Authors**: Hiroyuki Kasai, Bamdev Mishra
- **Comment**: European Signal Processing Conference (EUSIPCO 2018)
- **Journal**: None
- **Summary**: Dictionary leaning (DL) and dimensionality reduction (DR) are powerful tools to analyze high-dimensional noisy signals. This paper presents a proposal of a novel Riemannian joint dimensionality reduction and dictionary learning (R-JDRDL) on symmetric positive definite (SPD) manifolds for classification tasks. The joint learning considers the interaction between dimensionality reduction and dictionary learning procedures by connecting them into a unified framework. We exploit a Riemannian optimization framework for solving DL and DR problems jointly. Finally, we demonstrate that the proposed R-JDRDL outperforms existing state-of-the-arts algorithms when used for image classification tasks.



