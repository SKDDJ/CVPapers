# Arxiv Papers in cs.CV on 2019-02-01
### Deep Triplet Quantization
- **Arxiv ID**: http://arxiv.org/abs/1902.00153v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00153v1)
- **Published**: 2019-02-01 02:29:35+00:00
- **Updated**: 2019-02-01 02:29:35+00:00
- **Authors**: Bin Liu, Yue Cao, Mingsheng Long, Jianmin Wang, Jingdong Wang
- **Comment**: Accepted by ACM Multimedia 2018 as oral paper
- **Journal**: None
- **Summary**: Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution, focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ), a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training, we design a new triplet selection approach, Group Hard, that randomly selects hard triplets in each image group. To generate compact binary codes, we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes, which yields state-of-the-art image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.



### Lift-the-flap: what, where and when for context reasoning
- **Arxiv ID**: http://arxiv.org/abs/1902.00163v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1902.00163v2)
- **Published**: 2019-02-01 03:37:17+00:00
- **Updated**: 2019-09-25 01:34:56+00:00
- **Authors**: Mengmi Zhang, Claire Tseng, Karla Montejo, Joseph Kwon, Gabriel Kreiman
- **Comment**: None
- **Journal**: None
- **Summary**: Context reasoning is critical in a wide variety of applications where current inputs need to be interpreted in the light of previous experience and knowledge. Both spatial and temporal contextual information play a critical role in the domain of visual recognition. Here we investigate spatial constraints (what image features provide contextual information and where they are located), and temporal constraints (when different contextual cues matter) for visual recognition. The task is to reason about the scene context and infer what a target object hidden behind a flap is in a natural image. To tackle this problem, we first describe an online human psychophysics experiment recording active sampling via mouse clicks in lift-the-flap games and identify clicking patterns and features which are diagnostic for high contextual reasoning accuracy. As a proof of the usefulness of these clicking patterns and visual features, we extend a state-of-the-art recurrent model capable of attending to salient context regions, dynamically integrating useful information, making inferences, and predicting class label for the target object over multiple clicks. The proposed model achieves human-level contextual reasoning accuracy, shares human-like sampling behavior and learns interpretable features for contextual reasoning.



### Dataset Culling: Towards Efficient Training Of Distillation-Based Domain Specific Models
- **Arxiv ID**: http://arxiv.org/abs/1902.00173v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00173v3)
- **Published**: 2019-02-01 04:23:32+00:00
- **Updated**: 2019-05-16 09:30:16+00:00
- **Authors**: Kentaro Yoshioka, Edward Lee, Simon Wong, Mark Horowitz
- **Comment**: accepted to IEEE ICIP 2019. 5 pages
- **Journal**: None
- **Summary**: Real-time CNN-based object detection models for applications like surveillance can achieve high accuracy but are computationally expensive. Recent works have shown 10 to 100x reduction in computation cost for inference by using domain-specific networks. However, prior works have focused on inference only. If the domain model requires frequent retraining, training costs can pose a significant bottleneck. To address this, we propose Dataset Culling: a pipeline to reduce the size of the dataset for training, based on the prediction difficulty. Images that are easy to classify are filtered out since they contribute little to improving the accuracy. The difficulty is measured using our proposed confidence loss metric with little computational overhead. Dataset Culling is extended to optimize the image resolution to further improve training and inference costs. We develop fixed-angle, long-duration video datasets across several domains, and we show that the dataset size can be culled by a factor of 300x to reduce the total training time by 47x with no accuracy loss or even with slight improvement. Codes are available: https://github.com/kentaroy47/DatasetCulling



### Fast and Optimal Laplacian Solver for Gradient-Domain Image Editing using Green Function Convolution
- **Arxiv ID**: http://arxiv.org/abs/1902.00176v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DM
- **Links**: [PDF](http://arxiv.org/pdf/1902.00176v2)
- **Published**: 2019-02-01 04:36:56+00:00
- **Updated**: 2019-07-02 00:41:44+00:00
- **Authors**: Dominique Beaini, Sofiane Achiche, Fabrice Nonez, Olivier Brochu Dufour, Cédric Leblond-Ménard, Mahdis Asaadi, Maxime Raison
- **Comment**: 17 pages, single column scientific paper. Patent submitted
- **Journal**: None
- **Summary**: In computer vision, the gradient and Laplacian of an image are used in different applications, such as edge detection, feature extraction, and seamless image cloning. Computing the gradient of an image is straightforward since numerical derivatives are available in most computer vision toolboxes. However, the reverse problem is more difficult, since computing an image from its gradient requires to solve the Laplacian equation, also called Poisson equation. Current discrete methods are either slow or require heavy parallel computing. The objective of this paper is to present a novel fast and robust method of solving the image gradient or Laplacian with minimal error, which can be used for gradient domain editing. By using a single convolution based on a numerical Green's function, the whole process is faster and straightforward to implement with different computer vision libraries. It can also be optimized on a GPU using fast Fourier transforms and can easily be generalized for an n dimension image. The tests show that, for images of resolution 801x1200, the proposed GFC can solve 100 Laplacian in parallel in around 1.0 milliseconds ms. This is orders of magnitude faster than our nearest competitor which requires 294ms for a single image. Furthermore, we prove mathematically and demonstrate empirically that the proposed method is the least error solver for gradient domain editing. The developed method is also validated with examples of Poisson blending, gradient removal, and the proposed gradient domain merging GDM. Finally, we present how the GDM can be leveraged in future works for convolutional neural networks CNN.



### A Classification Supervised Auto-Encoder Based on Predefined Evenly-Distributed Class Centroids
- **Arxiv ID**: http://arxiv.org/abs/1902.00220v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00220v3)
- **Published**: 2019-02-01 08:26:47+00:00
- **Updated**: 2020-01-10 06:34:30+00:00
- **Authors**: Qiuyu Zhu, Ruixin Zhang
- **Comment**: 16 pages,12 figures, 4 tables
- **Journal**: None
- **Summary**: Classic variational autoencoders are used to learn complex data distributions, that are built on standard function approximators. Especially, VAE has shown promise on a lot of complex task. In this paper, a new autoencoder model - classification supervised autoencoder (CSAE) based on predefined evenly-distributed class centroids (PEDCC) is proposed. Our method uses PEDCC of latent variables to train the network to ensure the maximization of inter-class distance and the minimization of inner-class distance. Instead of learning mean/variance of latent variables distribution and taking reparameterization of VAE, latent variables of CSAE are directly used to classify and as input of decoder. In addition, a new loss function is proposed to combine the loss function of classification. Based on the basic structure of the universal autoencoder, we realized the comprehensive optimal results of encoding, decoding, classification, and good model generalization performance at the same time. Theoretical advantages are reflected in experimental results.



### Natural and Adversarial Error Detection using Invariance to Image Transformations
- **Arxiv ID**: http://arxiv.org/abs/1902.00236v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.00236v1)
- **Published**: 2019-02-01 09:00:54+00:00
- **Updated**: 2019-02-01 09:00:54+00:00
- **Authors**: Yuval Bahat, Michal Irani, Gregory Shakhnarovich
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach to distinguish between correct and incorrect image classifications. Our approach can detect misclassifications which either occur $\it{unintentionally}$ ("natural errors"), or due to $\it{intentional~adversarial~attacks}$ ("adversarial errors"), both in a single $\it{unified~framework}$. Our approach is based on the observation that correctly classified images tend to exhibit robust and consistent classifications under certain image transformations (e.g., horizontal flip, small image translation, etc.). In contrast, incorrectly classified images (whether due to adversarial errors or natural errors) tend to exhibit large variations in classification results under such transformations. Our approach does not require any modifications or retraining of the classifier, hence can be applied to any pre-trained classifier. We further use state of the art targeted adversarial attacks to demonstrate that even when the adversary has full knowledge of our method, the adversarial distortion needed for bypassing our detector is $\it{no~longer~imperceptible~to~the~human~eye}$. Our approach obtains state-of-the-art results compared to previous adversarial detection methods, surpassing them by a large margin.



### ColorNet: Investigating the importance of color spaces for image classification
- **Arxiv ID**: http://arxiv.org/abs/1902.00267v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00267v1)
- **Published**: 2019-02-01 10:35:18+00:00
- **Updated**: 2019-02-01 10:35:18+00:00
- **Authors**: Shreyank N Gowda, Chun Yuan
- **Comment**: None
- **Journal**: Asian Conference on Computer Vision 2018
- **Summary**: Image classification is a fundamental application in computer vision. Recently, deeper networks and highly connected networks have shown state of the art performance for image classification tasks. Most datasets these days consist of a finite number of color images. These color images are taken as input in the form of RGB images and classification is done without modifying them. We explore the importance of color spaces and show that color spaces (essentially transformations of original RGB images) can significantly affect classification accuracy. Further, we show that certain classes of images are better represented in particular color spaces and for a dataset with a highly varying number of classes such as CIFAR and Imagenet, using a model that considers multiple color spaces within the same model gives excellent levels of accuracy. Also, we show that such a model, where the input is preprocessed into multiple color spaces simultaneously, needs far fewer parameters to obtain high accuracy for classification. For example, our model with 1.75M parameters significantly outperforms DenseNet 100-12 that has 12M parameters and gives results comparable to Densenet-BC-190-40 that has 25.6M parameters for classification of four competitive image classification datasets namely: CIFAR-10, CIFAR-100, SVHN and Imagenet. Our model essentially takes an RGB image as input, simultaneously converts the image into 7 different color spaces and uses these as inputs to individual densenets. We use small and wide densenets to reduce computation overhead and number of hyperparameters required. We obtain significant improvement on current state of the art results on these datasets as well.



### Adaptive Gradient for Adversarial Perturbations Generation
- **Arxiv ID**: http://arxiv.org/abs/1902.01220v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR
- **Links**: [PDF](http://arxiv.org/pdf/1902.01220v6)
- **Published**: 2019-02-01 10:47:21+00:00
- **Updated**: 2019-05-20 01:34:19+00:00
- **Authors**: Yatie Xiao, Chi-Man Pun
- **Comment**: arXiv admin note: text overlap with arXiv:1901.03706 The formula in
  Algorithm 1 lacks important representations
- **Journal**: None
- **Summary**: Deep Neural Networks have achieved remarkable success in computer vision, natural language processing, and audio tasks.



### Instance Segmentation as Image Segmentation Annotation
- **Arxiv ID**: http://arxiv.org/abs/1902.05498v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05498v1)
- **Published**: 2019-02-01 10:53:25+00:00
- **Updated**: 2019-02-01 10:53:25+00:00
- **Authors**: Thomio Watanabe, Denis Wolf
- **Comment**: None
- **Journal**: None
- **Summary**: The instance segmentation problem intends to precisely detect and delineate objects in images. Most of the current solutions rely on deep convolutional neural networks but despite this fact proposed solutions are very diverse. Some solutions approach the problem as a network problem, where they use several networks or specialize a single network to solve several tasks. A different approach tries to solve the problem as an annotation problem, where the instance information is encoded in a mathematical representation. This work proposes a solution based in the DCME technique to solve the instance segmentation with a single segmentation network. Different from others, the segmentation network decoder is not specialized in a multi-task network. Instead, the network encoder is repurposed to classify image objects, reducing the computational cost of the solution.



### Deep Learning Solutions for TanDEM-X-based Forest Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.00274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00274v1)
- **Published**: 2019-02-01 11:12:30+00:00
- **Updated**: 2019-02-01 11:12:30+00:00
- **Authors**: Antonio Mazza, Francescopaolo Sica
- **Comment**: None
- **Journal**: None
- **Summary**: In the last few years, deep learning (DL) has been successfully and massively employed in computer vision for discriminative tasks, such as image classification or object detection. This kind of problems are core to many remote sensing (RS) applications as well, though with domain-specific peculiarities. Therefore, there is a growing interest on the use of DL methods for RS tasks. Here, we consider the forest/non-forest classification problem with TanDEM-X data, and test two state-of-the-art DL models, suitably adapting them to the specific task. Our experiments confirm the great potential of DL methods for RS applications.



### End-to-end Lane Detection through Differentiable Least-Squares Fitting
- **Arxiv ID**: http://arxiv.org/abs/1902.00293v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00293v3)
- **Published**: 2019-02-01 12:05:47+00:00
- **Updated**: 2019-09-05 07:57:31+00:00
- **Authors**: Wouter Van Gansbeke, Bert De Brabandere, Davy Neven, Marc Proesmans, Luc Van Gool
- **Comment**: Accepted at ICCVW 2019 (CVRSUAD-Road Scene Understanding and
  Autonomous Driving)
- **Journal**: None
- **Summary**: Lane detection is typically tackled with a two-step pipeline in which a segmentation mask of the lane markings is predicted first, and a lane line model (like a parabola or spline) is fitted to the post-processed mask next. The problem with such a two-step approach is that the parameters of the network are not optimized for the true task of interest (estimating the lane curvature parameters) but for a proxy task (segmenting the lane markings), resulting in sub-optimal performance. In this work, we propose a method to train a lane detector in an end-to-end manner, directly regressing the lane parameters. The architecture consists of two components: a deep network that predicts a segmentation-like weight map for each lane line, and a differentiable least-squares fitting module that returns for each map the parameters of the best-fitting curve in the weighted least-squares sense. These parameters can subsequently be supervised with a loss function of choice. Our method relies on the observation that it is possible to backpropagate through a least-squares fitting procedure. This leads to an end-to-end method where the features are optimized for the true task of interest: the network implicitly learns to generate features that prevent instabilities during the model fitting step, as opposed to two-step pipelines that need to handle outliers with heuristics. Additionally, the system is not just a black box but offers a degree of interpretability because the intermediately generated segmentation-like weight maps can be inspected and visualized. Code and a video is available at github.com/wvangansbeke/LaneDetection_End2End.



### Deep Hyperspectral Prior: Denoising, Inpainting, Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1902.00301v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00301v2)
- **Published**: 2019-02-01 12:20:38+00:00
- **Updated**: 2019-12-04 06:55:58+00:00
- **Authors**: Oleksii Sidorov, Jon Yngve Hardeberg
- **Comment**: Published in ICCV 2019 Workshops
- **Journal**: None
- **Summary**: Deep learning algorithms have demonstrated state-of-the-art performance in various tasks of image restoration. This was made possible through the ability of CNNs to learn from large exemplar sets. However, the latter becomes an issue for hyperspectral image processing where datasets commonly consist of just a few images. In this work, we propose a new approach to denoising, inpainting, and super-resolution of hyperspectral image data using intrinsic properties of a CNN without any training. The performance of the given algorithm is shown to be comparable to the performance of trained networks, while its application is not restricted by the availability of training data. This work is an extension of original "deep prior" algorithm to HSI domain and 3D-convolutional networks.



### Generative Smoke Removal
- **Arxiv ID**: http://arxiv.org/abs/1902.00311v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00311v2)
- **Published**: 2019-02-01 13:05:10+00:00
- **Updated**: 2019-12-04 06:47:45+00:00
- **Authors**: Oleksii Sidorov, Congcong Wang, Faouzi Alaya Cheikh
- **Comment**: Presented at NeurIPS Workshop and published in Proceedings of Machine
  Learning Research
- **Journal**: None
- **Summary**: In minimally invasive surgery, the use of tissue dissection tools causes smoke, which inevitably degrades the image quality. This could reduce the visibility of the operation field for surgeons and introduces errors for the computer vision algorithms used in surgical navigation systems. In this paper, we propose a novel approach for computational smoke removal using supervised image-to-image translation. We demonstrate that straightforward application of existing generative algorithms allows removing smoke but decreases image quality and introduces synthetic noise (grid-structure). Thus, we propose to solve this issue by modification of GAN's architecture and adding perceptual image quality metric to the loss function. Obtained results demonstrate that proposed method efficiently removes smoke as well as preserves perceptually sufficient image quality.



### VrR-VG: Refocusing Visually-Relevant Relationships
- **Arxiv ID**: http://arxiv.org/abs/1902.00313v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00313v2)
- **Published**: 2019-02-01 13:10:05+00:00
- **Updated**: 2019-08-26 07:24:33+00:00
- **Authors**: Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Relationships encode the interactions among individual instances, and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, existing methods tend to fit the statistical bias rather than ``learning'' to ``infer'' the relationships from images. To encourage further development in visual relationships, we propose a novel method to automatically mine more valuable relationships by pruning visually-irrelevant ones. We construct a new scene-graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved with a large margin, which demonstrates the gain of our dataset and the features embedding schema. VrR-VG is available via http://vrr-vg.com/.



### SensitiveNets: Learning Agnostic Representations with Application to Face Images
- **Arxiv ID**: http://arxiv.org/abs/1902.00334v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00334v3)
- **Published**: 2019-02-01 14:01:43+00:00
- **Updated**: 2020-08-06 19:44:13+00:00
- **Authors**: Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana
- **Comment**: Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence
- **Journal**: None
- **Summary**: This work proposes a novel privacy-preserving neural network feature representation to suppress the sensitive information of a learned space while maintaining the utility of the data. The new international regulation for personal data protection forces data controllers to guarantee privacy and avoid discriminative hazards while managing sensitive data of users. In our approach, privacy and discrimination are related to each other. Instead of existing approaches aimed directly at fairness improvement, the proposed feature representation enforces the privacy of selected attributes. This way fairness is not the objective, but the result of a privacy-preserving learning method. This approach guarantees that sensitive information cannot be exploited by any agent who process the output of the model, ensuring both privacy and equality of opportunity. Our method is based on an adversarial regularizer that introduces a sensitive information removal function in the learning objective. The method is evaluated on three different primary tasks (identity, attractiveness, and smiling) and three publicly available benchmarks. In addition, we present a new face annotation dataset with balanced distribution between genders and ethnic origins. The experiments demonstrate that it is possible to improve the privacy and equality of opportunity while retaining competitive performance independently of the task.



### Learning icons appearance similarity
- **Arxiv ID**: http://arxiv.org/abs/1902.05378v1
- **DOI**: 10.1007/s11042-018-6628-7
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05378v1)
- **Published**: 2019-02-01 14:17:26+00:00
- **Updated**: 2019-02-01 14:17:26+00:00
- **Authors**: Manuel Lagunas, Elena Garces, Diego Gutierrez
- **Comment**: 12 pages, 11 figures
- **Journal**: Multimedia Tools and Applications, pages: 1-19, year: 2018,
  publisher: Springer
- **Summary**: Selecting an optimal set of icons is a crucial step in the pipeline of visual design to structure and navigate through content. However, designing the icons sets is usually a difficult task for which expert knowledge is required. In this work, to ease the process of icon set selection to the users, we propose a similarity metric which captures the properties of style and visual identity. We train a Siamese Neural Network with an online dataset of icons organized in visually coherent collections that are used to adaptively sample training data and optimize the training process. As the dataset contains noise, we further collect human-rated information on the perception of icon's similarity which will be used for evaluating and testing the proposed model. We present several results and applications based on searches, kernel visualizations and optimized set proposals that can be helpful for designers and non-expert users while exploring large collections of icons.



### Projection-Based 2.5D U-net Architecture for Fast Volumetric Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.00347v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.00347v2)
- **Published**: 2019-02-01 14:19:00+00:00
- **Updated**: 2019-08-05 16:34:40+00:00
- **Authors**: Christoph Angermann, Markus Haltmeier, Ruth Steiger, Sergiy Pereverzyev Jr, Elke Gizewski
- **Comment**: presented at the SAMPTA 2019 conference
- **Journal**: None
- **Summary**: Convolutional neural networks are state-of-the-art for various segmentation tasks. While for 2D images these networks are also computationally efficient, 3D convolutions have huge storage requirements and require long training time. To overcome this issue, we introduce a network structure for volumetric data without 3D convolutional layers. The main idea is to include maximum intensity projections from different directions to transform the volumetric data to a sequence of images, where each image contains information of the full data. We then apply 2D convolutions to these projection images and lift them again to volumetric data using a trainable reconstruction algorithm.The proposed network architecture has less storage requirements than network structures using 3D convolutions. For a tested binary segmentation task, it even shows better performance than the 3D U-net and can be trained much faster.



### Learnable Embedding Space for Efficient Neural Architecture Compression
- **Arxiv ID**: http://arxiv.org/abs/1902.00383v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.00383v2)
- **Published**: 2019-02-01 14:54:17+00:00
- **Updated**: 2019-04-25 15:03:04+00:00
- **Authors**: Shengcao Cao, Xiaofang Wang, Kris M. Kitani
- **Comment**: ICLR 2019 - Code available here:
  https://github.com/Friedrich1006/ESNAC
- **Journal**: None
- **Summary**: We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (Ashok et al., 2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training. Code is publicly available here: https://github.com/Friedrich1006/ESNAC .



### Scalable Learning-Based Sampling Optimization for Compressive Dynamic MRI
- **Arxiv ID**: http://arxiv.org/abs/1902.00386v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.00386v5)
- **Published**: 2019-02-01 14:58:24+00:00
- **Updated**: 2020-03-16 13:15:46+00:00
- **Authors**: Thomas Sanchez, Baran Gözcü, Ruud B. van Heeswijk, Armin Eftekhari, Efe Ilıcak, Tolga Çukur, Volkan Cevher
- **Comment**: 13 pages, 16 figures, ICASSP 2020 - Session on "Learning and
  Optimization in Non-Convex Environments". Code available at
  https://github.com/t-sanchez/stochasticGreedyMRI.git
- **Journal**: None
- **Summary**: Compressed sensing applied to magnetic resonance imaging (MRI) allows to reduce the scanning time by enabling images to be reconstructed from highly undersampled data. In this paper, we tackle the problem of designing a sampling mask for an arbitrary reconstruction method and a limited acquisition budget. Namely, we look for an optimal probability distribution from which a mask with a fixed cardinality is drawn. We demonstrate that this problem admits a compactly supported solution, which leads to a deterministic optimal sampling mask. We then propose a stochastic greedy algorithm that (i) provides an approximate solution to this problem, and (ii) resolves the scaling issues of [1,2]. We validate its performance on in vivo dynamic MRI with retrospective undersampling, showing that our method preserves the performance of [1,2] while reducing the computational burden by a factor close to 200.



### Do We Train on Test Data? Purging CIFAR of Near-Duplicates
- **Arxiv ID**: http://arxiv.org/abs/1902.00423v2
- **DOI**: 10.3390/jimaging6060041
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00423v2)
- **Published**: 2019-02-01 16:00:34+00:00
- **Updated**: 2020-06-02 16:29:07+00:00
- **Authors**: Björn Barz, Joachim Denzler
- **Comment**: Journal of Imaging
- **Journal**: Journal of Imaging. 2020; 6(6):41
- **Summary**: The CIFAR-10 and CIFAR-100 datasets are two of the most heavily benchmarked datasets in computer vision and are often used to evaluate novel methods and model architectures in the field of deep learning. However, we find that 3.3% and 10% of the images from the test sets of these datasets have duplicates in the training set. These duplicates are easily recognizable by memorization and may, hence, bias the comparison of image recognition techniques regarding their generalization capability. To eliminate this bias, we provide the "fair CIFAR" (ciFAIR) dataset, where we replaced all duplicates in the test sets with new images sampled from the same domain. We then re-evaluate the classification performance of various popular state-of-the-art CNN architectures on these new test sets to investigate whether recent research has overfitted to memorizing data instead of learning abstract concepts. We find a significant drop in classification accuracy of between 9% and 14% relative to the original performance on the duplicate-free test set. The ciFAIR dataset and pre-trained models are available at https://cvjena.github.io/cifair/, where we also maintain a leaderboard.



### SCATGAN for Reconstruction of Ultrasound Scatterers Using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.00469v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.00469v1)
- **Published**: 2019-02-01 17:38:17+00:00
- **Updated**: 2019-02-01 17:38:17+00:00
- **Authors**: Andrawes Al Bahou, Christine Tanner, Orcun Goksel
- **Comment**: None
- **Journal**: None
- **Summary**: Computational simulation of ultrasound (US) echography is essential for training sonographers. Realistic simulation of US interaction with microscopic tissue structures is often modeled by a tissue representation in the form of point scatterers, convolved with a spatially varying point spread function. This yields a realistic US B-mode speckle texture, given that a scatterer representation for a particular tissue type is readily available. This is often not the case and scatterers are nontrivial to determine. In this work we propose to estimate scatterer maps from sample US B-mode images of a tissue, by formulating this inverse mapping problem as image translation, where we learn the mapping with Generative Adversarial Networks, using a US simulation software for training. We demonstrate robust reconstruction results, invariant to US viewing and imaging settings such as imaging direction and center frequency. Our method is shown to generalize beyond the trained imaging settings, demonstrated on in-vivo US data. Our inference runs orders of magnitude faster than optimization-based techniques, enabling future extensions for reconstructing 3D B-mode volumes with only linear computational complexity.



### Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus
- **Arxiv ID**: http://arxiv.org/abs/1902.00487v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00487v2)
- **Published**: 2019-02-01 18:14:35+00:00
- **Updated**: 2019-04-19 19:48:12+00:00
- **Authors**: Dongfang Yang, Linhui Li, Keith Redmill, Ümit Özgüner
- **Comment**: This paper was accepted into the 30th IEEE Intelligent Vehicles
  Symposium. Personal use of this material is permitted. Permission from IEEE
  must be obtained for all other uses
- **Journal**: None
- **Summary**: Predicting the collective motion of a group of pedestrians (a crowd) under the vehicle influence is essential for the development of autonomous vehicles to deal with mixed urban scenarios where interpersonal interaction and vehicle-crowd interaction (VCI) are significant. This usually requires a model that can describe individual pedestrian motion under the influence of nearby pedestrians and the vehicle. This study proposed two pedestrian trajectory datasets, CITR dataset and DUT dataset, so that the pedestrian motion models can be further calibrated and verified, especially when vehicle influence on pedestrians plays an important role. CITR dataset consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI. DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration. The trajectories of pedestrians, as well as vehicles, were extracted by processing video frames that come from a down-facing camera mounted on a hovering drone as the recording equipment. The final trajectories of pedestrians and vehicles were refined by Kalman filters with linear point-mass model and nonlinear bicycle model, respectively, in which xy-velocity of pedestrians and longitudinal speed and orientation of vehicles were estimated. The statistics of the velocity magnitude distribution demonstrated the validity of the proposed dataset. In total, there are approximate 340 pedestrian trajectories in CITR dataset and 1793 pedestrian trajectories in DUT dataset. The dataset is available at GitHub.



### Differentiable Grammars for Videos
- **Arxiv ID**: http://arxiv.org/abs/1902.00505v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.00505v2)
- **Published**: 2019-02-01 18:58:18+00:00
- **Updated**: 2020-02-15 16:38:35+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo
- **Comment**: None
- **Journal**: AAAI-2020
- **Summary**: This paper proposes a novel algorithm which learns a formal regular grammar from real-world continuous data, such as videos. Learning latent terminals, non-terminals, and production rules directly from continuous data allows the construction of a generative model capturing sequential structures with multiple possibilities. Our model is fully differentiable, and provides easily interpretable results which are important in order to understand the learned structures. It outperforms the state-of-the-art on several challenging datasets and is more accurate for forecasting future activities in videos. We plan to open-source the code. https://sites.google.com/view/differentiable-grammars



### Comparison of Patch-Based Conditional Generative Adversarial Neural Net Models with Emphasis on Model Robustness for Use in Head and Neck Cases for MR-Only planning
- **Arxiv ID**: http://arxiv.org/abs/1902.00536v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00536v4)
- **Published**: 2019-02-01 19:42:17+00:00
- **Updated**: 2019-02-27 18:14:56+00:00
- **Authors**: Peter Klages, Ilyes Benslimane, Sadegh Riyahi, Jue Jiang, Margie Hunt, Joe Deasy, Harini Veeraraghavan, Neelam Tyagi
- **Comment**: submitted to PMB
- **Journal**: None
- **Summary**: A total of twenty paired CT and MR images were used in this study to investigate two conditional generative adversarial networks, Pix2Pix, and Cycle GAN, for generating synthetic CT images for Headand Neck cancer cases. Ten of the patient cases were used for training and included such common artifacts as dental implants; the remaining ten testing cases were used for testing and included a larger range of image features commonly found in clinical head and neck cases. These features included strong metal artifacts from dental implants, one case with a metal implant, and one case with abnormal anatomy. The original CT images were deformably registered to the mDixon FFE MR images to minimize the effects of processing the MR images. The sCT generation accuracy and robustness were evaluated using Mean Absolute Error (MAE) based on the Hounsfield Units (HU) for three regions (whole body, bone, and air within the body), Mean Error (ME) to observe systematic average offset errors in the sCT generation, and dosimetric evaluation of all clinically relevant structures. For the test set the MAE for the Pix2Pix and Cycle GAN models were 92.4 $\pm$ 13.5 HU, and 100.7 $\pm$ 14.6 HU, respectively, for the body region, 166.3 $\pm$ 31.8 HU, and 184 $\pm$ 31.9 HU, respectively, for the bone region, and 183.7 $\pm$ 41.3 HU and 185.4 $\pm$ 37.9 HU for the air regions. The ME for Pix2Pix and Cycle GAN were 21.0 $\pm$ 11.8 HU and 37.5 $\pm$ 14.9 HU, respectively. Absolute Percent Mean/Max Dose Errors were less than 2% for the PTV and all critical structures for both models, and DRRs generated from these models looked qualitatively similar to CT generated DRRs showing these methods are promising for MR-only planning.



### The Efficacy of SHIELD under Different Threat Models
- **Arxiv ID**: http://arxiv.org/abs/1902.00541v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.00541v2)
- **Published**: 2019-02-01 20:10:12+00:00
- **Updated**: 2019-08-02 20:48:26+00:00
- **Authors**: Cory Cornelius, Nilaksh Das, Shang-Tse Chen, Li Chen, Michael E. Kounavis, Duen Horng Chau
- **Comment**: Appraisal paper of existing method accepted for oral presentation at
  KDD LEMINCS 2019
- **Journal**: None
- **Summary**: In this appraisal paper, we evaluate the efficacy of SHIELD, a compression-based defense framework for countering adversarial attacks on image classification models, which was published at KDD 2018. Here, we consider alternative threat models not studied in the original work, where we assume that an adaptive adversary is aware of the ensemble defense approach, the defensive pre-processing, and the architecture and weights of the models used in the ensemble. We define scenarios with varying levels of threat and empirically analyze the proposed defense by varying the degree of information available to the attacker, spanning from a full white-box attack to the gray-box threat model described in the original work. To evaluate the robustness of the defense against an adaptive attacker, we consider the targeted-attack success rate of the Projected Gradient Descent (PGD) attack, which is a strong gradient-based adversarial attack proposed in adversarial machine learning research. We also experiment with training the SHIELD ensemble from scratch, which is different from re-training using a pre-trained model as done in the original work. We find that the targeted PGD attack has a success rate of 64.3% against the original SHIELD ensemble in the full white box scenario, but this drops to 48.9% if the models used in the ensemble are trained from scratch instead of being retrained. Our experiments further reveal that an ensemble whose models are re-trained indeed have higher correlation in the cosine similarity space, and models that are trained from scratch are less vulnerable to targeted attacks in the white-box and gray-box scenarios.



### 2D and 3D Vascular Structures Enhancement via Multiscale Fractional Anisotropy Tensor
- **Arxiv ID**: http://arxiv.org/abs/1902.00550v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00550v1)
- **Published**: 2019-02-01 20:17:41+00:00
- **Updated**: 2019-02-01 20:17:41+00:00
- **Authors**: Haifa F. Alhasson, Shuaa S. Alharbi, Boguslaw Obara
- **Comment**: ECCV 2018 Workshops,Munich, Germany, Sept. 2018
- **Journal**: None
- **Summary**: The detection of vascular structures from noisy images is a fundamental process for extracting meaningful information in many applications. Most well-known vascular enhancing techniques often rely on Hessian-based filters. This paper investigates the feasibility and deficiencies of detecting curve-like structures using a Hessian matrix. The main contribution is a novel enhancement function, which overcomes the deficiencies of established methods. Our approach has been evaluated quantitatively and qualitatively using synthetic examples and a wide range of real 2D and 3D biomedical images. Compared with other existing approaches, the experimental results prove that our proposed approach achieves high-quality curvilinear structure enhancement.



### Robustness of Generalized Learning Vector Quantization Models against Adversarial Attacks
- **Arxiv ID**: http://arxiv.org/abs/1902.00577v2
- **DOI**: 10.1007/978-3-030-19642-4_19
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.00577v2)
- **Published**: 2019-02-01 22:28:56+00:00
- **Updated**: 2019-03-09 23:29:01+00:00
- **Authors**: Sascha Saralajew, Lars Holdijk, Maike Rees, Thomas Villmann
- **Comment**: to be published in 13th International Workshop on Self-Organizing
  Maps and Learning Vector Quantization, Clustering and Data Visualization
- **Journal**: None
- **Summary**: Adversarial attacks and the development of (deep) neural networks robust against them are currently two widely researched topics. The robustness of Learning Vector Quantization (LVQ) models against adversarial attacks has however not yet been studied to the same extent. We therefore present an extensive evaluation of three LVQ models: Generalized LVQ, Generalized Matrix LVQ and Generalized Tangent LVQ. The evaluation suggests that both Generalized LVQ and Generalized Tangent LVQ have a high base robustness, on par with the current state-of-the-art in robust neural network methods. In contrast to this, Generalized Matrix LVQ shows a high susceptibility to adversarial attacks, scoring consistently behind all other models. Additionally, our numerical evaluation indicates that increasing the number of prototypes per class improves the robustness of the models.



### Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1902.00579v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1902.00579v2)
- **Published**: 2019-02-01 22:48:26+00:00
- **Updated**: 2019-06-04 05:54:02+00:00
- **Authors**: Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, Jianfeng Gao
- **Comment**: Accepted to ACL 2019
- **Journal**: None
- **Summary**: This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.



