# Arxiv Papers in cs.CV on 2019-02-15
### Going Deep in Medical Image Analysis: Concepts, Methods, Challenges and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/1902.05655v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05655v1)
- **Published**: 2019-02-15 01:05:25+00:00
- **Updated**: 2019-02-15 01:05:25+00:00
- **Authors**: Fouzia Altaf, Syed M. S. Islam, Naveed Akhtar, Naeem K. Janjua
- **Comment**: None
- **Journal**: None
- **Summary**: Medical Image Analysis is currently experiencing a paradigm shift due to Deep Learning. This technology has recently attracted so much interest of the Medical Imaging community that it led to a specialized conference in `Medical Imaging with Deep Learning' in the year 2018. This article surveys the recent developments in this direction, and provides a critical review of the related major aspects. We organize the reviewed literature according to the underlying Pattern Recognition tasks, and further sub-categorize it following a taxonomy based on human anatomy. This article does not assume prior knowledge of Deep Learning and makes a significant contribution in explaining the core Deep Learning concepts to the non-experts in the Medical community. Unique to this study is the Computer Vision/Machine Learning perspective taken on the advances of Deep Learning in Medical Imaging. This enables us to single out `lack of appropriately annotated large-scale datasets' as the core challenge (among other challenges) in this research direction. We draw on the insights from the sister research fields of Computer Vision, Pattern Recognition and Machine Learning etc.; where the techniques of dealing with such challenges have already matured, to provide promising directions for the Medical Imaging community to fully harness Deep Learning in the future.



### TMAV: Temporal Motionless Analysis of Video using CNN in MPSoC
- **Arxiv ID**: http://arxiv.org/abs/1902.05657v2
- **DOI**: None
- **Categories**: **cs.CV**, I.4; I.2.1; C.1.4
- **Links**: [PDF](http://arxiv.org/pdf/1902.05657v2)
- **Published**: 2019-02-15 01:14:04+00:00
- **Updated**: 2019-02-18 15:16:30+00:00
- **Authors**: Somdip Dey, Amit K. Singh, Dilip K. Prasad, Klaus D. McDonald-Maier
- **Comment**: 11 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Analyzing video for traffic categorization is an important pillar of Intelligent Transport Systems. However, it is difficult to analyze and predict traffic based on image frames because the representation of each frame may vary significantly within a short time period. This also would inaccurately represent the traffic over a longer period of time such as the case of video. We propose a novel bio-inspired methodology that integrates analysis of the previous image frames of the video to represent the analysis of the current image frame, the same way a human being analyzes the current situation based on past experience. In our proposed methodology, called IRON-MAN (Integrated Rational prediction and Motionless ANalysis), we utilize Bayesian update on top of the individual image frame analysis in the videos and this has resulted in highly accurate prediction of Temporal Motionless Analysis of the Videos (TMAV) for most of the chosen test cases. The proposed approach could be used for TMAV using Convolutional Neural Network (CNN) for applications where the number of objects in an image is the deciding factor for prediction and results also show that our proposed approach outperforms the state-of-the-art for the chosen test case. We also introduce a new metric named, Energy Consumption per Training Image (ECTI). Since, different CNN based models have different training capability and computing resource utilization, some of the models are more suitable for embedded device implementation than the others, and ECTI metric is useful to assess the suitability of using a CNN model in multi-processor systems-on-chips (MPSoCs) with a focus on energy consumption and reliability in terms of lifespan of the embedded device using these MPSoCs.



### Massively Parallel Benders Decomposition for Correlation Clustering
- **Arxiv ID**: http://arxiv.org/abs/1902.05659v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.DS
- **Links**: [PDF](http://arxiv.org/pdf/1902.05659v2)
- **Published**: 2019-02-15 01:50:07+00:00
- **Updated**: 2019-08-02 17:44:22+00:00
- **Authors**: Margret Keuper, Jovita Lukasik, Maneesh Singh, Julian Yarkony
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance's graph, which are solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to the negative weight edges attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows, to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and allows for massive parallelization.



### Cycle-Consistency for Robust Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1902.05660v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05660v1)
- **Published**: 2019-02-15 02:07:18+00:00
- **Updated**: 2019-02-15 02:07:18+00:00
- **Authors**: Meet Shah, Xinlei Chen, Marcus Rohrbach, Devi Parikh
- **Comment**: Technical Report
- **Journal**: None
- **Summary**: Despite significant progress in Visual Question Answering over the years, robustness of today's VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions spanning 40k images from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional annotations, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset.



### Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution via LSTM Modelling on Epipolar Plane Images
- **Arxiv ID**: http://arxiv.org/abs/1902.05672v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1902.05672v1)
- **Published**: 2019-02-15 03:36:00+00:00
- **Updated**: 2019-02-15 03:36:00+00:00
- **Authors**: Hao Zhu, Mantang Guo, Hongdong Li, Qing Wang, Antonio Robles-Kelly
- **Comment**: 10 pages, 12 figures
- **Journal**: None
- **Summary**: Light-field cameras (LFC) have received increasing attention due to their wide-spread applications. However, current LFCs suffer from the well-known spatio-angular trade-off, which is considered as an inherent and fundamental limit for LFC designs. In this paper, by doing a detailed geometrical optical analysis of the sampling process in an LFC, we show that the effective sampling resolution is generally higher than the number of micro-lenses. This contribution makes it theoretically possible to break the resolution trade-off. Our second contribution is an epipolar plane image (EPI) based super-resolution method, which can super-resolve the spatial and angular dimensions simultaneously. We prove that the light field is a 2D series, thus, a specifically designed CNN-LSTM network is proposed to capture the continuity property of the EPI. Rather than leveraging semantic information, our network focuses on extracting geometric continuity in the EPI. This gives our method an improved generalization ability and makes it applicable to a wide range of previously unseen scenes. Experiments on both synthetic and real light fields demonstrate the improvements over state-of-the-art, especially in large disparity areas.



### Lipschitz Generative Adversarial Nets
- **Arxiv ID**: http://arxiv.org/abs/1902.05687v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05687v4)
- **Published**: 2019-02-15 05:19:21+00:00
- **Updated**: 2019-06-24 07:55:51+00:00
- **Authors**: Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu, Zhihua Zhang
- **Comment**: Published as a conference paper at ICML 2019
- **Journal**: None
- **Summary**: In this paper, we study the convergence of generative adversarial networks (GANs) from the perspective of the informativeness of the gradient of the optimal discriminative function. We show that GANs without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.



### Lightweight Feature Fusion Network for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1902.05694v2
- **DOI**: 10.1109/LSP.2018.2890770
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05694v2)
- **Published**: 2019-02-15 05:55:45+00:00
- **Updated**: 2019-04-13 08:42:55+00:00
- **Authors**: Wenming Yang, Wei Wang, Xuechen Zhang, Shuifa Sun, Qingmin Liao
- **Comment**: Accepted by IEEE Signal Processing Letters (Volume:26, Issue:4, April
  2019)
- **Journal**: None
- **Summary**: Single image super-resolution(SISR) has witnessed great progress as convolutional neural network(CNN) gets deeper and wider. However, enormous parameters hinder its application to real world problems. In this letter, We propose a lightweight feature fusion network (LFFN) that can fully explore multi-scale contextual information and greatly reduce network parameters while maximizing SISR results. LFFN is built on spindle blocks and a softmax feature fusion module (SFFM). Specifically, a spindle block is composed of a dimension extension unit, a feature exploration unit and a feature refinement unit. The dimension extension layer expands low dimension to high dimension and implicitly learns the feature maps which is suitable for the next unit. The feature exploration unit performs linear and nonlinear feature exploration aimed at different feature maps. The feature refinement layer is used to fuse and refine features. SFFM fuses the features from different modules in a self-adaptive learning manner with softmax function, making full use of hierarchical information with a small amount of parameter cost. Both qualitative and quantitative experiments on benchmark datasets show that LFFN achieves favorable performance against state-of-the-art methods with similar parameters.



### Network Offloading Policies for Cloud Robotics: a Learning-based Approach
- **Arxiv ID**: http://arxiv.org/abs/1902.05703v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/1902.05703v1)
- **Published**: 2019-02-15 06:34:31+00:00
- **Updated**: 2019-02-15 06:34:31+00:00
- **Authors**: Sandeep Chinchali, Apoorva Sharma, James Harrison, Amine Elhafsi, Daniel Kang, Evgenya Pergament, Eyal Cidon, Sachin Katti, Marco Pavone
- **Comment**: None
- **Journal**: None
- **Summary**: Today's robotic systems are increasingly turning to computationally expensive models such as deep neural networks (DNNs) for tasks like localization, perception, planning, and object detection. However, resource-constrained robots, like low-power drones, often have insufficient on-board compute resources or power reserves to scalably run the most accurate, state-of-the art neural network compute models. Cloud robotics allows mobile robots the benefit of offloading compute to centralized servers if they are uncertain locally or want to run more accurate, compute-intensive models. However, cloud robotics comes with a key, often understated cost: communicating with the cloud over congested wireless networks may result in latency or loss of data. In fact, sending high data-rate video or LIDAR from multiple robots over congested networks can lead to prohibitive delay for real-time applications, which we measure experimentally. In this paper, we formulate a novel Robot Offloading Problem --- how and when should robots offload sensing tasks, especially if they are uncertain, to improve accuracy while minimizing the cost of cloud communication? We formulate offloading as a sequential decision making problem for robots, and propose a solution using deep reinforcement learning. In both simulations and hardware experiments using state-of-the art vision DNNs, our offloading strategy improves vision task performance by between 1.3-2.6x of benchmark offloading strategies, allowing robots the potential to significantly transcend their on-board sensing accuracy but with limited cost of cloud communication.



### Unsupervised shape and motion analysis of 3822 cardiac 4D MRIs of UK Biobank
- **Arxiv ID**: http://arxiv.org/abs/1902.05811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05811v1)
- **Published**: 2019-02-15 13:56:04+00:00
- **Updated**: 2019-02-15 13:56:04+00:00
- **Authors**: Qiao Zheng, Hervé Delingette, Kenneth Fung, Steffen E. Petersen, Nicholas Ayache
- **Comment**: None
- **Journal**: None
- **Summary**: We perform unsupervised analysis of image-derived shape and motion features extracted from 3822 cardiac 4D MRIs of the UK Biobank. First, with a feature extraction method previously published based on deep learning models, we extract from each case 9 feature values characterizing both the cardiac shape and motion. Second, a feature selection is performed to remove highly correlated feature pairs. Third, clustering is carried out using a Gaussian mixture model on the selected features. After analysis, we identify two small clusters which probably correspond to two pathological categories. Further confirmation using a trained classification model and dimensionality reduction tools is carried out to support this discovery. Moreover, we examine the differences between the other large clusters and compare our measures with the ground-truth.



### Enhancing Remote Sensing Image Retrieval with Triplet Deep Metric Learning Network
- **Arxiv ID**: http://arxiv.org/abs/1902.05818v1
- **DOI**: 10.1080/2150704X.2019.1647368
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05818v1)
- **Published**: 2019-02-15 14:29:02+00:00
- **Updated**: 2019-02-15 14:29:02+00:00
- **Authors**: Rui Cao, Qian Zhang, Jiasong Zhu, Qing Li, Qingquan Li, Bozhi Liu, Guoping Qiu
- **Comment**: 5 pages, 7 figures, 3 tables
- **Journal**: International Journal of Remote Sensing, 2020, Vol. 41, No. 2, pp.
  740-751
- **Summary**: With the rapid growing of remotely sensed imagery data, there is a high demand for effective and efficient image retrieval tools to manage and exploit such data. In this letter, we present a novel content-based remote sensing image retrieval method based on Triplet deep metric learning convolutional neural network (CNN). By constructing a Triplet network with metric learning objective function, we extract the representative features of the images in a semantic space in which images from the same class are close to each other while those from different classes are far apart. In such a semantic space, simple metric measures such as Euclidean distance can be used directly to compare the similarity of images and effectively retrieve images of the same class. We also investigate a supervised and an unsupervised learning methods for reducing the dimensionality of the learned semantic features. We present comprehensive experimental results on two publicly available remote sensing image retrieval datasets and show that our method significantly outperforms state-of-the-art.



### Deeply Supervised Multimodal Attentional Translation Embeddings for Visual Relationship Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.05829v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05829v1)
- **Published**: 2019-02-15 14:53:40+00:00
- **Updated**: 2019-02-15 14:53:40+00:00
- **Authors**: Nikolaos Gkanatsios, Vassilis Pitsikalis, Petros Koutras, Athanasia Zlatintsi, Petros Maragos
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting visual relationships, i.e. <Subject, Predicate, Object> triplets, is a challenging Scene Understanding task approached in the past via linguistic priors or spatial information in a single feature branch. We introduce a new deeply supervised two-branch architecture, the Multimodal Attentional Translation Embeddings, where the visual features of each branch are driven by a multimodal attentional mechanism that exploits spatio-linguistic similarities in a low-dimensional space. We present a variety of experiments comparing against all related approaches in the literature, as well as by re-implementing and fine-tuning several of them. Results on the commonly employed VRD dataset [1] show that the proposed method clearly outperforms all others, while we also justify our claims both quantitatively and qualitatively.



### Estimation of blood oxygenation with learned spectral decoloring for quantitative photoacoustic imaging (LSD-qPAI)
- **Arxiv ID**: http://arxiv.org/abs/1902.05839v1
- **DOI**: 10.1038/s41598-021-83405-8
- **Categories**: **physics.med-ph**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05839v1)
- **Published**: 2019-02-15 15:15:11+00:00
- **Updated**: 2019-02-15 15:15:11+00:00
- **Authors**: Janek Gröhl, Thomas Kirchner, Tim Adler, Lena Maier-Hein
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: One of the main applications of photoacoustic (PA) imaging is the recovery of functional tissue properties, such as blood oxygenation (sO2). This is typically achieved by linear spectral unmixing of relevant chromophores from multispectral photoacoustic images. Despite the progress that has been made towards quantitative PA imaging (qPAI), most sO2 estimation methods yield poor results in realistic settings. In this work, we tackle the challenge by employing learned spectral decoloring for quantitative photoacoustic imaging (LSD-qPAI) to obtain quantitative estimates for blood oxygenation. LSD-qPAI computes sO2 directly from pixel-wise initial pressure spectra Sp0, which are vectors comprised of the initial pressure at the same spatial location over all recorded wavelengths. Initial results suggest that LSD-qPAI is able to obtain accurate sO2 estimates directly from multispectral photoacoustic measurements in silico and plausible estimates in vivo.



### Street Scene: A new dataset and evaluation protocol for video anomaly detection
- **Arxiv ID**: http://arxiv.org/abs/1902.05872v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05872v3)
- **Published**: 2019-02-15 16:18:36+00:00
- **Updated**: 2020-01-24 15:25:39+00:00
- **Authors**: Bharathkumar Ramachandra, Michael Jones
- **Comment**: accepted to WACV 2020
- **Journal**: None
- **Summary**: Progress in video anomaly detection research is currently slowed by small datasets that lack a wide variety of activities as well as flawed evaluation criteria. This paper aims to help move this research effort forward by introducing a large and varied new dataset called Street Scene, as well as two new evaluation criteria that provide a better estimate of how an algorithm will perform in practice. In addition to the new dataset and evaluation criteria, we present two variations of a novel baseline video anomaly detection algorithm and show they are much more accurate on Street Scene than two state-of-the-art algorithms from the literature.



### DeepFault: Fault Localization for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.05974v1
- **DOI**: None
- **Categories**: **cs.SE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05974v1)
- **Published**: 2019-02-15 19:42:45+00:00
- **Updated**: 2019-02-15 19:42:45+00:00
- **Authors**: Hasan Ferit Eniser, Simos Gerasimou, Alper Sen
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) are increasingly deployed in safety-critical applications including autonomous vehicles and medical diagnostics. To reduce the residual risk for unexpected DNN behaviour and provide evidence for their trustworthy operation, DNNs should be thoroughly tested. The DeepFault whitebox DNN testing approach presented in our paper addresses this challenge by employing suspiciousness measures inspired by fault localization to establish the hit spectrum of neurons and identify suspicious neurons whose weights have not been calibrated correctly and thus are considered responsible for inadequate DNN performance. DeepFault also uses a suspiciousness-guided algorithm to synthesize new inputs, from correctly classified inputs, that increase the activation values of suspicious neurons. Our empirical evaluation on several DNN instances trained on MNIST and CIFAR-10 datasets shows that DeepFault is effective in identifying suspicious neurons. Also, the inputs synthesized by DeepFault closely resemble the original inputs, exercise the identified suspicious neurons and are highly adversarial.



### GANFIT: Generative Adversarial Network Fitting for High Fidelity 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1902.05978v2
- **DOI**: 10.1109/CVPR.2019.00125
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05978v2)
- **Published**: 2019-02-15 19:53:45+00:00
- **Updated**: 2019-04-06 19:05:21+00:00
- **Authors**: Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou
- **Comment**: CVPR 2019 camera ready; Check project page:
  https://github.com/barisgecer/ganfit for full resolution results and more
- **Journal**: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR), Long Beach, CA, USA, 2019, pp. 1155-1164
- **Summary**: In the past few years, a lot of work has been done towards reconstructing the 3D facial structure from single images by capitalizing on the power of Deep Convolutional Neural Networks (DCNNs). In the most recent works, differentiable renderers were employed in order to learn the relationship between the facial identity features and the parameters of a 3D morphable model for shape and texture. The texture features either correspond to components of a linear texture space or are learned by auto-encoders directly from in-the-wild images. In all cases, the quality of the facial texture reconstruction of the state-of-the-art methods is still not capable of modeling textures in high fidelity. In this paper, we take a radically different approach and harness the power of Generative Adversarial Networks (GANs) and DCNNs in order to reconstruct the facial texture and shape from single images. That is, we utilize GANs to train a very powerful generator of facial texture in UV space. Then, we revisit the original 3D Morphable Models (3DMMs) fitting approaches making use of non-linear optimization to find the optimal latent parameters that best reconstruct the test image but under a new perspective. We optimize the parameters with the supervision of pretrained deep identity features through our end-to-end differentiable framework. We demonstrate excellent results in photorealistic and identity preserving 3D face reconstructions and achieve for the first time, to the best of our knowledge, facial texture reconstruction with high-frequency details.



### Operational Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.11106v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11106v2)
- **Published**: 2019-02-15 20:13:51+00:00
- **Updated**: 2019-10-18 11:19:56+00:00
- **Authors**: Serkan Kiranyaz, Turker Ince, Alexandros Iosifidis, Moncef Gabbouj
- **Comment**: 21 pages
- **Journal**: None
- **Summary**: Feed-forward, fully-connected Artificial Neural Networks (ANNs) or the so-called Multi-Layer Perceptrons (MLPs) are well-known universal approximators. However, their learning performance varies significantly depending on the function or the solution space that they attempt to approximate. This is mainly because of their homogenous configuration based solely on the linear neuron model. Therefore, while they learn very well those problems with a monotonous, relatively simple and linearly separable solution space, they may entirely fail to do so when the solution space is highly nonlinear and complex. Sharing the same linear neuron model with two additional constraints (local connections and weight sharing), this is also true for the conventional Convolutional Neural Networks (CNNs) and, it is, therefore, not surprising that in many challenging problems only the deep CNNs with a massive complexity and depth can achieve the required diversity and the learning performance. In order to address this drawback and also to accomplish a more generalized model over the convolutional neurons, this study proposes a novel network model, called Operational Neural Networks (ONNs), which can be heterogeneous and encapsulate neurons with any set of operators to boost diversity and to learn highly complex and multi-modal functions or spaces with minimal network complexity and training data. Finally, a novel training method is formulated to back-propagate the error through the operational layers of ONNs. Experimental results over highly challenging problems demonstrate the superior learning capabilities of ONNs even with few neurons and hidden layers.



