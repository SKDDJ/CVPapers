# Arxiv Papers in cs.CV on 2019-02-24
### Medical Multimodal Classifiers Under Scarce Data Condition
- **Arxiv ID**: http://arxiv.org/abs/1902.08888v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.08888v1)
- **Published**: 2019-02-24 03:38:41+00:00
- **Updated**: 2019-02-24 03:38:41+00:00
- **Authors**: Faik Aydin, Maggie Zhang, Michelle Ananda-Rajah, Gholamreza Haffari
- **Comment**: None
- **Journal**: None
- **Summary**: Data is one of the essential ingredients to power deep learning research. Small datasets, especially specific to medical institutes, bring challenges to deep learning training stage. This work aims to develop a practical deep multimodal that can classify patients into abnormal and normal categories accurately as well as assist radiologists to detect visual and textual anomalies by locating areas of interest. The detection of the anomalies is achieved through a novel technique which extends the integrated gradients methodology with an unsupervised clustering algorithm. This technique also introduces a tuning parameter which trades off true positive signals to denoise false positive signals in the detection process. To overcome the challenges of the small training dataset which only has 3K frontal X-ray images and medical reports in pairs, we have adopted transfer learning for the multimodal which concatenates the layers of image and text submodels. The image submodel was trained on the vast ChestX-ray14 dataset, while the text submodel transferred a pertained word embedding layer from a hospital-specific corpus. Experimental results show that our multimodal improves the accuracy of the classification by 4% and 7% on average of 50 epochs, compared to the individual text and image model, respectively.



### TBNet:Pulmonary Tuberculosis Diagnosing System using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08897v1)
- **Published**: 2019-02-24 05:45:29+00:00
- **Updated**: 2019-02-24 05:45:29+00:00
- **Authors**: Ram Srivatsav Ghorakavi
- **Comment**: 9 pages, 8 figures, 2 Numerical table, 1 algorithm table
- **Journal**: None
- **Summary**: Tuberculosis is a deadly infectious disease prevalent around the world. Due to the lack of proper technology in place, the early detection of this disease is unattainable. Also, the available methods to detect Tuberculosis is not up-to a commendable standards due to their dependency on unnecessary features, this make such technology obsolete for a reliable health-care technology. In this paper, I propose a deep-learning based system which diagnoses tuberculosis based on the important features in Chest X-rays along with original chest X-rays. Employing our system will accelerate the process of tuberculosis diagnosis by overcoming the need to perform the time-consuming sputum-based testing method (Diagnostic Microbiology). In contrast to the previous methods \cite{kant2018towards, melendez2016automated}, our work utilizes the state-of-the-art ResNet \cite{he2016deep} with proper data augmentation using traditional robust features like Haar \cite{viola2005detecting,viola2001rapid} and LBP \cite{ojala1994performance,ojala1996comparative}. I observed that such a procedure enhances the rate of tuberculosis detection to a highly satisfactory level. Our work uses the publicly available pulmonary chest X-ray dataset to train our network \cite{jaeger2014two}. Nevertheless, the publicly available dataset is very small and is inadequate to achieve the best accuracy. To overcome this issue I have devised an intuitive feature based data augmentation pipeline. Our approach shall help the deep neural network \cite{lecun2015deep,he2016deep,krizhevsky2012imagenet} to focus its training on tuberculosis affected regions making it more robust and accurate, when compared to other conventional methods that use procedures like mirroring and rotation. By using our simple yet powerful techniques, I observed a 10\% boost in performance accuracy.



### Image Classification on IoT Edge Devices: Profiling and Modeling
- **Arxiv ID**: http://arxiv.org/abs/1902.11119v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1902.11119v2)
- **Published**: 2019-02-24 06:41:29+00:00
- **Updated**: 2019-08-13 05:28:16+00:00
- **Authors**: Salma Abdel Magid, Francesco Petrini, Behnam Dezfouli
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of powerful, low-cost IoT systems, processing data closer to where the data originates, known as edge computing, has become an increasingly viable option. In addition to lowering the cost of networking infrastructures, edge computing reduces edge-cloud delay, which is essential for mission-critical applications. In this paper, we show the feasibility and study the performance of image classification using IoT devices. Specifically, we explore the relationships between various factors of image classification algorithms that may affect energy consumption such as dataset size, image resolution, algorithm type, algorithm phase, and device hardware. Our experiments show a strong, positive linear relationship between three predictor variables, namely model complexity, image resolution, and dataset size, with respect to energy consumption. In addition, in order to provide a means of predicting the energy consumption of an edge device performing image classification, we investigate the usage of three machine learning algorithms using the data generated from our experiments. The performance as well as the trade offs for using linear regression, Gaussian process, and random forests are discussed and validated. Our results indicate that the random forest model outperforms the two former algorithms, with an R-squared value of 0.95 and 0.79 for two different validation datasets.



### 3D Guided Fine-Grained Face Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1902.08900v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08900v1)
- **Published**: 2019-02-24 06:47:45+00:00
- **Updated**: 2019-02-24 06:47:45+00:00
- **Authors**: Zhenglin Geng, Chen Cao, Sergey Tulyakov
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for fine-grained face manipulation. Given a face image with an arbitrary expression, our method can synthesize another arbitrary expression by the same person. This is achieved by first fitting a 3D face model and then disentangling the face into a texture and a shape. We then learn different networks in these two spaces. In the texture space, we use a conditional generative network to change the appearance, and carefully design input formats and loss functions to achieve the best results. In the shape space, we use a fully connected network to predict the accurate shapes and use the available depth data for supervision. Both networks are conditioned on expression coefficients rather than discrete labels, allowing us to generate an unlimited amount of expressions. We show the superiority of this disentangling approach through both quantitative and qualitative studies. In a user study, our method is preferred in 85% of cases when compared to the most recent work. When compared to the ground truth, annotators cannot reliably distinguish between our synthesized images and real images, preferring our method in 53% of the cases.



### Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather
- **Arxiv ID**: http://arxiv.org/abs/1902.08913v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08913v3)
- **Published**: 2019-02-24 10:05:18+00:00
- **Updated**: 2020-06-30 14:23:45+00:00
- **Authors**: Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, Werner Ritter, Klaus Dietmayer, Felix Heide
- **Comment**: None
- **Journal**: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2020
- **Summary**: The fusion of multimodal sensor streams, such as camera, lidar, and radar measurements, plays a critical role in object detection for autonomous vehicles, which base their decision making on these inputs. While existing methods exploit redundant information in good environmental conditions, they fail in adverse weather where the sensory streams can be asymmetrically distorted. These rare "edge-case" scenarios are not represented in available datasets, and existing fusion architectures are not designed to handle them. To address this challenge we present a novel multimodal dataset acquired in over 10,000km of driving in northern Europe. Although this dataset is the first large multimodal dataset in adverse weather, with 100k labels for lidar, camera, radar, and gated NIR sensors, it does not facilitate training as extreme weather is rare. To this end, we present a deep fusion network for robust fusion without a large corpus of labeled training data covering all asymmetric distortions. Departing from proposal-level fusion, we propose a single-shot model that adaptively fuses features, driven by measurement entropy. We validate the proposed method, trained on clean data, on our extensive validation dataset. Code and data are available here https://github.com/princeton-computational-imaging/SeeingThroughFog.



### Bi-Skip: A Motion Deblurring Network Using Self-paced Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.08915v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08915v1)
- **Published**: 2019-02-24 10:28:04+00:00
- **Updated**: 2019-02-24 10:28:04+00:00
- **Authors**: Yiwei Zhang, Chunbiao Zhu, Ge Li, Yuan Zhao, Haifeng Shen
- **Comment**: None
- **Journal**: None
- **Summary**: A fast and effective motion deblurring method has great application values in real life. This work presents an innovative approach in which a self-paced learning is combined with GAN to deblur image. First, We explain that a proper generator can be used as deep priors and point out that the solution for pixel-based loss is not same with the one for perception-based loss. By using these ideas as starting points, a Bi-Skip network is proposed to improve the generating ability and a bi-level loss is adopted to solve the problem that common conditions are non-identical. Second, considering that the complex motion blur will perturb the network in the training process, a self-paced mechanism is adopted to enhance the robustness of the network. Through extensive evaluations on both qualitative and quantitative criteria, it is demonstrated that our approach has a competitive advantage over state-of-the-art methods.



### Transferability of Deep Learning Algorithms for Malignancy Detection in Confocal Laser Endomicroscopy Images from Different Anatomical Locations of the Upper Gastrointestinal Tract
- **Arxiv ID**: http://arxiv.org/abs/1902.08985v2
- **DOI**: 10.1007/978-3-030-29196-9_4
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08985v2)
- **Published**: 2019-02-24 17:38:25+00:00
- **Updated**: 2020-01-03 13:38:45+00:00
- **Authors**: Marc Aubreville, Miguel Goncalves, Christian Knipfer, Nicolai Oetter, Helmut Neumann, Florian Stelzle, Christopher Bohr, Andreas Maier
- **Comment**: Erratum for version 1, correcting the number of CLE image sequences
  used in one data set
- **Journal**: BIOSTEC 2018: Biomedical Engineering Systems and Technologies
- **Summary**: Squamous Cell Carcinoma (SCC) is the most common cancer type of the epithelium and is often detected at a late stage. Besides invasive diagnosis of SCC by means of biopsy and histo-pathologic assessment, Confocal Laser Endomicroscopy (CLE) has emerged as noninvasive method that was successfully used to diagnose SCC in vivo. For interpretation of CLE images, however, extensive training is required, which limits its applicability and use in clinical practice of the method. To aid diagnosis of SCC in a broader scope, automatic detection methods have been proposed. This work compares two methods with regard to their applicability in a transfer learning sense, i.e. training on one tissue type (from one clinical team) and applying the learnt classification system to another entity (different anatomy, different clinical team). Besides a previously proposed, patch-based method based on convolutional neural networks, a novel classification method on image level (based on a pre-trained Inception V.3 network with dedicated preprocessing and interpretation of class activation maps) is proposed and evaluated. The newly presented approach improves recognition performance, yielding accuracies of 91.63% on the first data set (oral cavity) and 92.63% on a joint data set. The generalization from oral cavity to the second data set (vocal folds) lead to similar area-under-the-ROC curve values than a direct training on the vocal folds data set, indicating good generalization.



### U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instrument
- **Arxiv ID**: http://arxiv.org/abs/1902.08994v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08994v1)
- **Published**: 2019-02-24 18:57:19+00:00
- **Updated**: 2019-02-24 18:57:19+00:00
- **Authors**: S. M. Kamrul Hasan, Cristian A. Linte
- **Comment**: 7 pages, 6 figures, IEEE conference submission
- **Journal**: None
- **Summary**: Conventional therapy approaches limit surgeons' dexterity control due to limited field-of-view. With the advent of robot-assisted surgery, there has been a paradigm shift in medical technology for minimally invasive surgery. However, it is very challenging to track the position of the surgical instruments in a surgical scene, and accurate detection & identification of surgical tools is paramount. Deep learning-based semantic segmentation in frames of surgery videos has the potential to facilitate this task. In this work, we modify the U-Net architecture named U-NetPlus, by introducing a pre-trained encoder and re-design the decoder part, by replacing the transposed convolution operation with an upsampling operation based on nearest-neighbor (NN) interpolation. To further improve performance, we also employ a very fast and flexible data augmentation technique. We trained the framework on 8 x 225 frame sequences of robotic surgical videos, available through the MICCAI 2017 EndoVis Challenge dataset and tested it on 8 x 75 frame and 2 x 300 frame videos. Using our U-NetPlus architecture, we report a 90.20% DICE for binary segmentation, 76.26% DICE for instrument part segmentation, and 46.07% for instrument type (i.e., all instruments) segmentation, outperforming the results of previous techniques implemented and tested on these data.



### Automatic ISP image quality tuning using non-linear optimization
- **Arxiv ID**: http://arxiv.org/abs/1902.09023v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09023v1)
- **Published**: 2019-02-24 22:06:07+00:00
- **Updated**: 2019-02-24 22:06:07+00:00
- **Authors**: Jun Nishimura, Timo Gerasimow, Sushma Rao, Aleksandar Sutic, Chyuan-Tyng Wu, Gilad Michael
- **Comment**: 5 pages, 2018 25th IEEE International Conference on Image Processing
  (ICIP), 2471-2475
- **Journal**: 2018 25th IEEE International Conference on Image Processing
  (ICIP), 2471-2475
- **Summary**: Image Signal Processor (ISP) comprises of various blocks to reconstruct image sensor raw data to final image consumed by human visual system or computer vision applications. Each block typically has many tuning parameters due to the complexity of the operation. These need to be hand tuned by Image Quality (IQ) experts, which takes considerable amount of time. In this paper, we present an automatic IQ tuning using nonlinear optimization and automatic reference generation algorithms. The proposed method can produce high quality IQ in minutes as compared with weeks of hand-tuned results by IQ experts. In addition, the proposed method can work with any algorithms without being aware of their specific implementation. It was found successful on multiple different processing blocks such as noise reduction, demosaic, and sharpening.



