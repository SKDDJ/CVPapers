# Arxiv Papers in cs.CV on 2019-02-02
### Confidence Trigger Detection: An Approach to Build Real-time Tracking-by-detection System
- **Arxiv ID**: http://arxiv.org/abs/1902.00615v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00615v1)
- **Published**: 2019-02-02 01:52:53+00:00
- **Updated**: 2019-02-02 01:52:53+00:00
- **Authors**: Zhicheng Ding, Edward Wong
- **Comment**: 9 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: With deep learning based image analysis getting popular in recent years, a lot of multiple objects tracking applications are in demand. Some of these applications (e.g., surveillance camera, intelligent robotics, and autonomous driving) require the system runs in real-time. Though recent proposed methods reach fairly high accuracy, the speed is still slower than real-time application requirement. In order to increase tracking-by-detection system's speed for real-time tracking, we proposed confidence trigger detection (CTD) approach which uses confidence of tracker to decide when to trigger object detection. Using this approach, system can safely skip detection of images frames that objects barely move. We had studied the influence of different confidences in three popular detectors separately. Though we found trade-off between speed and accuracy, our approach reaches higher accuracy at given speed.



### Supervised Quantization for Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1902.00617v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00617v1)
- **Published**: 2019-02-02 02:07:51+00:00
- **Updated**: 2019-02-02 02:07:51+00:00
- **Authors**: Xiaojuan Wang, Ting Zhang, Guo-Jun Q, Jinhui Tang, Jingdong Wang
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: In this paper, we address the problem of searching for semantically similar images from a large database. We present a compact coding approach, supervised quantization. Our approach simultaneously learns feature selection that linearly transforms the database points into a low-dimensional discriminative subspace, and quantizes the data points in the transformed space. The optimization criterion is that the quantized points not only approximate the transformed points accurately, but also are semantically separable: the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes, which is formulated as a classification problem. The experiments on several standard datasets show the superiority of our approach over the state-of-the art supervised hashing and unsupervised quantization algorithms.



### Collaborative Quantization for Cross-Modal Similarity Search
- **Arxiv ID**: http://arxiv.org/abs/1902.00623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00623v1)
- **Published**: 2019-02-02 02:20:25+00:00
- **Updated**: 2019-02-02 02:20:25+00:00
- **Authors**: Ting Zhang, Jingdong Wang
- **Comment**: CVPR 2016
- **Journal**: None
- **Summary**: Cross-modal similarity search is a problem about designing a search system supporting querying across content modalities, e.g., using an image to search for texts or using a text to search for images. This paper presents a compact coding solution for efficient search, with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in the single-modal similarity search. We propose a cross-modal quantization approach, which is among the early attempts to introduce quantization into cross-modal search. The major contribution lies in jointly learning the quantizers for both modalities through aligning the quantized representations for each pair of image and text belonging to a document. In addition, our approach simultaneously learns the common space for both modalities in which quantization is conducted to enable efficient and effective search using the Euclidean distance computed in the common space with fast distance table lookup. Experimental results compared with several competitive algorithms over three benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance.



### Nonparametric Curve Alignment
- **Arxiv ID**: http://arxiv.org/abs/1902.00626v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.00626v1)
- **Published**: 2019-02-02 02:29:24+00:00
- **Updated**: 2019-02-02 02:29:24+00:00
- **Authors**: Marwan Mattar, Michael Ross, Erik Learned-Miller
- **Comment**: 4 pages, IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2009
- **Journal**: None
- **Summary**: Congealing is a flexible nonparametric data-driven framework for the joint alignment of data. It has been successfully applied to the joint alignment of binary images of digits, binary images of object silhouettes, grayscale MRI images, color images of cars and faces, and 3D brain volumes. This research enhances congealing to practically and effectively apply it to curve data. We develop a parameterized set of nonlinear transformations that allow us to apply congealing to this type of data. We present positive results on aligning synthetic and real curve data sets and conclude with a discussion on extending this work to simultaneous alignment and clustering.



### Pairwise Teacher-Student Network for Semi-Supervised Hashing
- **Arxiv ID**: http://arxiv.org/abs/1902.00643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00643v1)
- **Published**: 2019-02-02 05:21:26+00:00
- **Updated**: 2019-02-02 05:21:26+00:00
- **Authors**: Shifeng Zhang, Jianmin Li, Bo Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Hashing method maps similar high-dimensional data to binary hashcodes with smaller hamming distance, and it has received broad attention due to its low storage cost and fast retrieval speed. Pairwise similarity is easily obtained and widely used for retrieval, and most supervised hashing algorithms are carefully designed for the pairwise supervisions. As labeling all data pairs is difficult, semi-supervised hashing is proposed which aims at learning efficient codes with limited labeled pairs and abundant unlabeled ones. Existing methods build graphs to capture the structure of dataset, but they are not working well for complex data as the graph is built based on the data representations and determining the representations of complex data is difficult. In this paper, we propose a novel teacher-student semi-supervised hashing framework in which the student is trained with the pairwise information produced by the teacher network. The network follows the smoothness assumption, which achieves consistent distances for similar data pairs so that the retrieval results are similar for neighborhood queries. Experiments on large-scale datasets show that the proposed method reaches impressive gain over the supervised baselines and is superior to state-of-the-art semi-supervised hashing methods.



### Hierarchical Photo-Scene Encoder for Album Storytelling
- **Arxiv ID**: http://arxiv.org/abs/1902.00669v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00669v1)
- **Published**: 2019-02-02 08:42:58+00:00
- **Updated**: 2019-02-02 08:42:58+00:00
- **Authors**: Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Feng Zhang
- **Comment**: 8 pages, 4 figures
- **Journal**: None
- **Summary**: In this paper, we propose a novel model with a hierarchical photo-scene encoder and a reconstructor for the task of album storytelling. The photo-scene encoder contains two sub-encoders, namely the photo and scene encoders, which are stacked together and behave hierarchically to fully exploit the structure information of the photos within an album. Specifically, the photo encoder generates semantic representation for each photo while exploiting temporal relationships among them. The scene encoder, relying on the obtained photo representations, is responsible for detecting the scene changes and generating scene representations. Subsequently, the decoder dynamically and attentively summarizes the encoded photo and scene representations to generate a sequence of album representations, based on which a story consisting of multiple coherent sentences is generated. In order to fully extract the useful semantic information from an album, a reconstructor is employed to reproduce the summarized album representations based on the hidden states of the decoder. The proposed model can be trained in an end-to-end manner, which results in an improved performance over the state-of-the-arts on the public visual storytelling (VIST) dataset. Ablation studies further demonstrate the effectiveness of the proposed hierarchical photo-scene encoder and reconstructor.



### A Layer-Based Sequential Framework for Scene Generation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1902.00671v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00671v1)
- **Published**: 2019-02-02 08:49:56+00:00
- **Updated**: 2019-02-02 08:49:56+00:00
- **Authors**: Mehmet Ozgur Turkoglu, William Thong, Luuk Spreeuwers, Berkay Kicanaoglu
- **Comment**: This paper was accepted at AAAI 2019
- **Journal**: None
- **Summary**: The visual world we sense, interpret and interact everyday is a complex composition of interleaved physical entities. Therefore, it is a very challenging task to generate vivid scenes of similar complexity using computers. In this work, we present a scene generation framework based on Generative Adversarial Networks (GANs) to sequentially compose a scene, breaking down the underlying problem into smaller ones. Different than the existing approaches, our framework offers an explicit control over the elements of a scene through separate background and foreground generators. Starting with an initially generated background, foreground objects then populate the scene one-by-one in a sequential manner. Via quantitative and qualitative experiments on a subset of the MS-COCO dataset, we show that our proposed framework produces not only more diverse images but also copes better with affine transformations and occlusion artifacts of foreground objects than its counterparts.



### Self-Binarizing Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.00730v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00730v1)
- **Published**: 2019-02-02 14:48:16+00:00
- **Updated**: 2019-02-02 14:48:16+00:00
- **Authors**: Fayez Lahoud, Radhakrishna Achanta, Pablo Márquez-Neila, Sabine Süsstrunk
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: We present a method to train self-binarizing neural networks, that is, networks that evolve their weights and activations during training to become binary. To obtain similar binary networks, existing methods rely on the sign activation function. This function, however, has no gradients for non-zero values, which makes standard backpropagation impossible. To circumvent the difficulty of training a network relying on the sign activation function, these methods alternate between floating-point and binary representations of the network during training, which is sub-optimal and inefficient. We approach the binarization task by training on a unique representation involving a smooth activation function, which is iteratively sharpened during training until it becomes a binary representation equivalent to the sign activation function. Additionally, we introduce a new technique to perform binary batch normalization that simplifies the conventional batch normalization by transforming it into a simple comparison operation. This is unlike existing methods, which are forced to the retain the conventional floating-point-based batch normalization. Our binary networks, apart from displaying advantages of lower memory and computation as compared to conventional floating-point and binary networks, also show higher classification accuracy than existing state-of-the-art methods on multiple benchmark datasets.



### Online Multi-Object Tracking with Dual Matching Attention Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.00749v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00749v1)
- **Published**: 2019-02-02 16:25:46+00:00
- **Updated**: 2019-02-02 16:25:46+00:00
- **Authors**: Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang, Ming-Hsuan Yang
- **Comment**: Accepted by ECCV 2018
- **Journal**: None
- **Summary**: In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a unified framework to handle noisy detections and frequent interactions between targets. Specifically, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates different levels of attention to different samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and offline trackers in terms of identity-preserving metrics.



### Domain invariant hierarchical embedding for grocery products recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.00760v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00760v1)
- **Published**: 2019-02-02 18:19:41+00:00
- **Updated**: 2019-02-02 18:19:41+00:00
- **Authors**: Alessio Tonioni, Luigi Di Stefano
- **Comment**: None
- **Journal**: None
- **Summary**: Recognizing packaged grocery products based solely on appearance is still an open issue for modern computer vision systems due to peculiar challenges. Firstly, the number of different items to be recognized is huge (i.e., in the order of thousands) and rapidly changing over time. Moreover, there exist a significant domain shift between the images that should be recognized at test time, taken in stores by cheap cameras, and those available for training, usually just one or a few studio-quality images per product. We propose an end-to-end architecture comprising a GAN to address the domain shift at training time and a deep CNN trained on the samples generated by the GAN to learn an embedding of product images that enforces a hierarchy between product categories. At test time, we perform recognition by means of K-NN search against a database consisting of just one reference image per product. Experiments addressing recognition of products present in the training datasets as well as different ones unseen at training time show that our approach compares favourably to state-of-the-art methods on the grocery recognition task and generalize fairly well to similar ones.



### DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion
- **Arxiv ID**: http://arxiv.org/abs/1902.00761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00761v2)
- **Published**: 2019-02-02 18:25:31+00:00
- **Updated**: 2019-07-10 17:02:49+00:00
- **Authors**: Shreyas S. Shivakumar, Ty Nguyen, Ian D. Miller, Steven W. Chen, Vijay Kumar, Camillo J. Taylor
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: In this paper we propose a convolutional neural network that is designed to upsample a series of sparse range measurements based on the contextual cues gleaned from a high resolution intensity image. Our approach draws inspiration from related work on super-resolution and in-painting. We propose a novel architecture that seeks to pull contextual cues separately from the intensity image and the depth features and then fuse them later in the network. We argue that this approach effectively exploits the relationship between the two modalities and produces accurate results while respecting salient image structures. We present experimental results to demonstrate that our approach is comparable with state of the art methods and generalizes well across multiple datasets.



### Automatic Lesion Boundary Segmentation in Dermoscopic Images with Ensemble Deep Learning Methods
- **Arxiv ID**: http://arxiv.org/abs/1902.00809v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.00809v2)
- **Published**: 2019-02-02 23:17:19+00:00
- **Updated**: 2019-07-29 11:02:06+00:00
- **Authors**: Manu Goyal, Amanda Oakley, Priyanka Bansal, Darren Dancey, Moi Hoon Yap
- **Comment**: 7 pages, 8 figures and 4 tables. arXiv admin note: text overlap with
  arXiv:1711.10449
- **Journal**: None
- **Summary**: Early detection of skin cancer, particularly melanoma, is crucial to enable advanced treatment. Due to the rapid growth in the numbers of skin cancers, there is a growing need of computerized analysis for skin lesions. The state-of-the-art public available datasets for skin lesions are often accompanied with very limited amount of segmentation ground truth labeling as it is laborious and expensive. The lesion boundary segmentation is vital to locate the lesion accurately in dermoscopic images and lesion diagnosis of different skin lesion types. In this work, we propose the use of fully automated deep learning ensemble methods for accurate lesion boundary segmentation in dermoscopic images. We trained the Mask-RCNN and DeepLabv3+ methods on ISIC-2017 segmentation training set and evaluate the performance of the ensemble networks on ISIC-2017 testing set. Our results showed that the best proposed ensemble method segmented the skin lesions with Jaccard index of 79.58% for the ISIC-2017 testing set. The proposed ensemble method outperformed FrCN, FCN, U-Net, and SegNet in Jaccard Index by 2.48%, 7.42%, 17.95%, and 9.96% respectively. Furthermore, the proposed ensemble method achieved an accuracy of 95.6% for some representative clinically benign cases, 90.78% for the melanoma cases, and 91.29% for the seborrheic keratosis cases on ISIC-2017 testing set, exhibiting better performance than FrCN, FCN, U-Net, and SegNet.



### Collaborative Sampling in Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.00813v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.00813v3)
- **Published**: 2019-02-02 23:43:25+00:00
- **Updated**: 2019-11-22 15:54:24+00:00
- **Authors**: Yuejiang Liu, Parth Kothari, Alexandre Alahi
- **Comment**: Accepted to AAAI 2020
- **Journal**: None
- **Summary**: The standard practice in Generative Adversarial Networks (GANs) discards the discriminator during sampling. However, this sampling method loses valuable information learned by the discriminator regarding the data distribution. In this work, we propose a collaborative sampling scheme between the generator and the discriminator for improved data generation. Guided by the discriminator, our approach refines the generated samples through gradient-based updates at a particular layer of the generator, shifting the generator distribution closer to the real data distribution. Additionally, we present a practical discriminator shaping method that can smoothen the loss landscape provided by the discriminator for effective sample refinement. Through extensive experiments on synthetic and image datasets, we demonstrate that our proposed method can improve generated samples both quantitatively and qualitatively, offering a new degree of freedom in GAN sampling.



