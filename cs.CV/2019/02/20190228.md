# Arxiv Papers in cs.CV on 2019-02-28
### Deep Interpretable Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1902.10840v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10840v1)
- **Published**: 2019-02-28 00:04:24+00:00
- **Updated**: 2019-02-28 00:04:24+00:00
- **Authors**: Chen Kong, Simon Lucey
- **Comment**: None
- **Journal**: None
- **Summary**: All current non-rigid structure from motion (NRSfM) algorithms are limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works. The considerable model capacity of our approach affords remarkable generalization to unseen data. We propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction. Once the network's weights are estimated (for a non-rigid object) we show how our approach can effectively recover 3D shape from a single image -- outperforming comparable methods that rely on direct 3D supervision.



### Robust Re-identification of Manta Rays from Natural Markings by Learning Pose Invariant Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1902.10847v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10847v1)
- **Published**: 2019-02-28 00:37:32+00:00
- **Updated**: 2019-02-28 00:37:32+00:00
- **Authors**: Olga Moskvyak, Frederic Maire, Asia O. Armstrong, Feras Dayoub, Mahsa Baktashmotlagh
- **Comment**: 12 pages, 15 figures
- **Journal**: None
- **Summary**: Visual identification of individual animals that bear unique natural body markings is an important task in wildlife conservation. The photo databases of animal markings grow larger and each new observation has to be matched against thousands of images. Existing photo-identification solutions have constraints on image quality and appearance of the pattern of interest in the image. These constraints limit the use of photos from citizen scientists. We present a novel system for visual re-identification based on unique natural markings that is robust to occlusions, viewpoint and illumination changes. We adapt methods developed for face re-identification and implement a deep convolutional neural network (CNN) to learn embeddings for images of natural markings. The distance between the learned embedding points provides a dissimilarity measure between the corresponding input images. The network is optimized using the triplet loss function and the online semi-hard triplet mining strategy. The proposed re-identification method is generic and not species specific. We evaluate the proposed system on image databases of manta ray belly patterns and humpback whale flukes. To be of practical value and adopted by marine biologists, a re-identification system needs to have a top-10 accuracy of at least 95%. The proposed system achieves this performance standard.



### Machine-assisted annotation of forensic imagery
- **Arxiv ID**: http://arxiv.org/abs/1902.10848v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10848v1)
- **Published**: 2019-02-28 00:39:53+00:00
- **Updated**: 2019-02-28 00:39:53+00:00
- **Authors**: Sara Mousavi, Ramin Nabati, Megan Kleeschulte, Audris Mockus
- **Comment**: Submitted to ICIP 2019
- **Journal**: None
- **Summary**: Image collections, if critical aspects of image content are exposed, can spur research and practical applications in many domains. Supervised machine learning may be the only feasible way to annotate very large collections, but leading approaches rely on large samples of completely and accurately annotated images. In the case of a large forensic collection, we are aiming to annotate, neither the complete annotation nor the large training samples can be feasibly produced. We, therefore, investigate ways to assist manual annotation efforts done by forensic experts. We present a method that can propose both images and areas within an image likely to contain desired classes. Evaluation of the method with human annotators showed highly accurate classification that was strongly helped by transfer learning. The segmentation precision (mAP) was improved by adding a separate class capturing background, but that did not affect the recall (mAR). Further work is needed to both increase the accuracy of segmentation and enhances prediction with additional covariates affecting decomposition. We hope this effort to be of help in other domains that require weak segmentation and have limited availability of qualified annotators.



### Cascaded Recurrent Neural Networks for Hyperspectral Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.10858v1
- **DOI**: 10.1109/TGRS.2019.2899129
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10858v1)
- **Published**: 2019-02-28 01:29:59+00:00
- **Updated**: 2019-02-28 01:29:59+00:00
- **Authors**: Renlong Hang, Qingshan Liu, Danfeng Hong, Pedram Ghamisi
- **Comment**: None
- **Journal**: None
- **Summary**: By considering the spectral signature as a sequence, recurrent neural networks (RNNs) have been successfully used to learn discriminative features from hyperspectral images (HSIs) recently. However, most of these models only input the whole spectral bands into RNNs directly, which may not fully explore the specific properties of HSIs. In this paper, we propose a cascaded RNN model using gated recurrent units (GRUs) to explore the redundant and complementary information of HSIs. It mainly consists of two RNN layers. The first RNN layer is used to eliminate redundant information between adjacent spectral bands, while the second RNN layer aims to learn the complementary information from non-adjacent spectral bands. To improve the discriminative ability of the learned features, we design two strategies for the proposed model. Besides, considering the rich spatial information contained in HSIs, we further extend the proposed model to its spectral-spatial counterpart by incorporating some convolutional layers. To test the effectiveness of our proposed models, we conduct experiments on two widely used HSIs. The experimental results show that our proposed models can achieve better results than the compared models.



### PFLD: A Practical Facial Landmark Detector
- **Arxiv ID**: http://arxiv.org/abs/1902.10859v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10859v2)
- **Published**: 2019-02-28 01:45:55+00:00
- **Updated**: 2019-03-03 05:37:17+00:00
- **Authors**: Xiaojie Guo, Siyuan Li, Jinke Yu, Jiawan Zhang, Jiayi Ma, Lin Ma, Wei Liu, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Being accurate, efficient, and compact is essential to a facial landmark detector for practical use. To simultaneously consider the three concerns, this paper investigates a neat model with promising detection accuracy under wild environments e.g., unconstrained pose, expression, lighting, and occlusion conditions) and super real-time speed on a mobile device. More concretely, we customize an end-to-end single stage network associated with acceleration techniques. During the training phase, for each sample, rotation information is estimated for geometrically regularizing landmark localization, which is then NOT involved in the testing phase. A novel loss is designed to, besides considering the geometrical regularization, mitigate the issue of data imbalance by adjusting weights of samples to different states, such as large pose, extreme lighting, and occlusion, in the training set. Extensive experiments are conducted to demonstrate the efficacy of our design and reveal its superior performance over state-of-the-art alternatives on widely-adopted challenging benchmarks, i.e., 300W (including iBUG, LFPW, AFW, HELEN, and XM2VTS) and AFLW. Our model can be merely 2.1Mb of size and reach over 140 fps per face on a mobile phone (Qualcomm ARM 845 processor) with high precision, making it attractive for large-scale or real-time applications. We have made our practical system based on PFLD 0.25X model publicly available at \url{http://sites.google.com/view/xjguo/fld} for encouraging comparisons and improvements from the community.



### Transformation Consistent Self-ensembling Model for Semi-supervised Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.00348v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00348v3)
- **Published**: 2019-02-28 03:49:40+00:00
- **Updated**: 2020-05-08 21:46:55+00:00
- **Authors**: Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, Pheng-Ann Heng
- **Comment**: Accept at IEEE Transactions on Neural Networks and Learning Systems
- **Journal**: None
- **Summary**: Deep convolutional neural networks have achieved remarkable progress on a variety of medical image computing tasks. A common problem when applying supervised deep learning methods to medical images is the lack of labeled data, which is very expensive and time-consuming to be collected. In this paper, we present a novel semi-supervised method for medical image segmentation, where the network is optimized by the weighted combination of a common supervised loss for labeled inputs only and a regularization loss for both labeled and unlabeled data. To utilize the unlabeled data, our method encourages the consistent predictions of the network-in-training for the same input under different regularizations. Aiming for the semi-supervised segmentation problem, we enhance the effect of regularization for pixel-level predictions by introducing a transformation, including rotation and flipping, consistent scheme in our self-ensembling model. With the aim of semi-supervised segmentation tasks, we introduce a transformation consistent strategy in our self-ensembling model to enhance the regularization effect for pixel-level predictions. We have extensively validated the proposed semi-supervised method on three typical yet challenging medical image segmentation tasks: (i) skin lesion segmentation from dermoscopy images on International Skin Imaging Collaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus images on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver segmentation from volumetric CT scans on Liver Tumor Segmentation Challenge (LiTS) dataset. Compared to the state-of-the-arts, our proposed method shows superior segmentation performance on challenging 2D/3D medical images, demonstrating the effectiveness of our semi-supervised method for medical image segmentation.



### Face Recognition Under Varying Blur, Illumination and Expression in an Unconstrained Environment
- **Arxiv ID**: http://arxiv.org/abs/1902.10885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10885v1)
- **Published**: 2019-02-28 04:18:28+00:00
- **Updated**: 2019-02-28 04:18:28+00:00
- **Authors**: Anubha Pearline. S, Hemalatha. M
- **Comment**: None
- **Journal**: Special Issue International Journal of Computer Science and
  Information Security (IJCSIS) 2016
- **Summary**: Face recognition system is one of the esteemed research areas in pattern recognition and computer vision as long as its major challenges. A few challenges in recognizing faces are blur, illumination, and varied expressions. Blur is natural while taking photographs using cameras, mobile phones, etc. Blur can be uniform and non-uniform. Usually non-uniform blur happens in images taken using handheld image devices. Distinguishing or handling a blurred image in a face recognition system is generally tough. Under varying lighting conditions, it is challenging to identify the person correctly. Diversified facial expressions such as happiness, sad, surprise, fear, anger changes or deforms the faces from normal images. Identifying faces with facial expressions is also a challenging task, due to the deformation caused by the facial expressions. To solve these issues, a pre-processing step was carried out after which Blur and Illumination-Robust Face recognition (BIRFR) algorithm was performed. The test image and training images with facial expression are transformed to neutral face using Facial expression removal (FER) peration. Every training image is transformed based on the optimal Transformation Spread Function (TSF), and illumination coefficients. Local Binary Pattern (LBP) features extracted from test image and transformed training image is used for classification.



### Towards Robust ResNet: A Small Step but A Giant Leap
- **Arxiv ID**: http://arxiv.org/abs/1902.10887v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10887v3)
- **Published**: 2019-02-28 04:24:46+00:00
- **Updated**: 2019-07-02 03:02:31+00:00
- **Authors**: Jingfeng Zhang, Bo Han, Laura Wynter, Kian Hsiang Low, Mohan Kankanhalli
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a simple yet principled approach to boosting the robustness of the residual network (ResNet) that is motivated by the dynamical system perspective. Namely, a deep neural network can be interpreted using a partial differential equation, which naturally inspires us to characterize ResNet by an explicit Euler method. Our analytical studies reveal that the step factor h in the Euler method is able to control the robustness of ResNet in both its training and generalization. Specifically, we prove that a small step factor h can benefit the training robustness for back-propagation; from the view of forward-propagation, a small h can aid in the robustness of the model generalization. A comprehensive empirical evaluation on both vision CIFAR-10 and text AG-NEWS datasets confirms that a small h aids both the training and generalization robustness.



### Sparse Depth Enhanced Direct Thermal-infrared SLAM Beyond the Visible Spectrum
- **Arxiv ID**: http://arxiv.org/abs/1902.10892v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10892v1)
- **Published**: 2019-02-28 05:05:05+00:00
- **Updated**: 2019-02-28 05:05:05+00:00
- **Authors**: Young-Sik Shin, Ayoung Kim
- **Comment**: 8 pages, 7 figures, Submitted to 2019 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2019) and IEEE Robotics
  and Automation Letters
- **Journal**: None
- **Summary**: In this paper, we propose a thermal-infrared simultaneous localization and mapping (SLAM) system enhanced by sparse depth measurements from Light Detection and Ranging (LiDAR). Thermal-infrared cameras are relatively robust against fog, smoke, and dynamic lighting conditions compared to RGB cameras operating under the visible spectrum. Due to the advantages of thermal-infrared cameras, exploiting them for motion estimation and mapping is highly appealing. However, operating a thermal-infrared camera directly in existing vision-based methods is difficult because of the modality difference. This paper proposes a method to use sparse depth measurement for 6-DOF motion estimation by directly tracking under 14- bit raw measurement of the thermal camera. In addition, we perform a refinement to improve the local accuracy and include a loop closure to maintain global consistency. The experimental results demonstrate that the system is not only robust under various lighting conditions such as day and night, but also overcomes the scale problem of monocular cameras. The video is available at https://youtu.be/oO7lT3uAzLc.



### What you get is not always what you see: pitfalls in solar array assessment using overhead imagery
- **Arxiv ID**: http://arxiv.org/abs/1902.10895v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10895v2)
- **Published**: 2019-02-28 05:10:08+00:00
- **Updated**: 2022-07-25 22:09:37+00:00
- **Authors**: Wei Hu, Kyle Bradbury, Jordan M. Malof, Boning Li, Bohao Huang, Artem Streltsov, K. Sydny Fujita, Ben Hoen
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Effective integration planning for small, distributed solar photovoltaic (PV) arrays into electric power grids requires access to high quality data: the location and power capacity of individual solar PV arrays. Unfortunately, national databases of small-scale solar PV do not exist; those that do are limited in their spatial resolution, typically aggregated up to state or national levels. While several promising approaches for solar PV detection have been published, strategies for evaluating the performance of these models are often highly heterogeneous from study to study. The resulting comparison of these methods for practical applications for energy assessments becomes challenging and may imply that the reported performance evaluations are overly optimistic. The heterogeneity comes in many forms, each of which we explore in this work: the level of spatial aggregation, the validation of ground truth, inconsistencies in the training and validation datasets, and the degree of diversity of the locations and sensors from which the training and validation data originate. For each, we discuss emerging practices from the literature to address them or suggest directions of future research. As part of our investigation, we evaluate solar PV identification performance in two large regions. Our findings suggest that traditional performance evaluation of the automated identification of solar PV from satellite imagery may be optimistic due to common limitations in the validation process. The takeaways from this work are intended to inform and catalyze the large-scale practical application of automated solar PV assessment techniques by energy researchers and professionals.



### Adversarial Attack and Defense on Point Sets
- **Arxiv ID**: http://arxiv.org/abs/1902.10899v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10899v4)
- **Published**: 2019-02-28 05:27:40+00:00
- **Updated**: 2021-05-31 16:03:23+00:00
- **Authors**: Jiancheng Yang, Qiang Zhang, Rongyao Fang, Bingbing Ni, Jinxian Liu, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Emergence of the utility of 3D point cloud data in safety-critical vision tasks (e.g., ADAS) urges researchers to pay more attention to the robustness of 3D representations and deep networks. To this end, we develop an attack and defense scheme, dedicated to 3D point cloud data, for preventing 3D point clouds from manipulated as well as pursuing noise-tolerable 3D representation. A set of novel 3D point cloud attack operations are proposed via pointwise gradient perturbation and adversarial point attachment / detachment. We then develop a flexible perturbation-measurement scheme for 3D point cloud data to detect potential attack data or noisy sensing data. Notably, the proposed defense methods are even effective to detect the adversarial point clouds generated by a proof-of-concept attack directly targeting the defense. Transferability of adversarial attacks between several point cloud networks is addressed, and we propose an momentum-enhanced pointwise gradient to improve the attack transferability. We further analyze the transferability from adversarial point clouds to grid CNNs and the inverse. Extensive experimental results on common point cloud benchmarks demonstrate the validity of the proposed 3D attack and defense framework.



### Bi-Directional Cascade Network for Perceptual Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.10903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10903v1)
- **Published**: 2019-02-28 05:35:15+00:00
- **Updated**: 2019-02-28 05:35:15+00:00
- **Authors**: Jianzhong He, Shiliang Zhang, Ming Yang, Yanhu Shan, Tiejun Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Exploiting multi-scale representations is critical to improve edge detection for objects at different scales. To extract edges at dramatically different scales, we propose a Bi-Directional Cascade Network (BDCN) structure, where an individual layer is supervised by labeled edges at its specific scale, rather than directly applying the same supervision to all CNN outputs. Furthermore, to enrich multi-scale representations learned by BDCN, we introduce a Scale Enhancement Module (SEM) which utilizes dilated convolution to generate multi-scale features, instead of using deeper CNNs or explicitly fusing multi-scale edge maps. These new approaches encourage the learning of multi-scale representations in different layers and detect edges that are well delineated by their scales. Learning scale dedicated layers also results in compact network with a fraction of parameters. We evaluate our method on three datasets, i.e., BSDS500, NYUDv2, and Multicue, and achieve ODS Fmeasure of 0.828, 1.3% higher than current state-of-the art on BSDS500. The code has been available at https://github.com/pkuCactus/BDCN.



### SweepNet: Wide-baseline Omnidirectional Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.10904v2
- **DOI**: 10.1109/ICRA.2019.8793823
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10904v2)
- **Published**: 2019-02-28 05:36:19+00:00
- **Updated**: 2019-08-16 17:07:50+00:00
- **Authors**: Changhee Won, Jongbin Ryu, Jongwoo Lim
- **Comment**: Accepted by ICRA 2019
- **Journal**: None
- **Summary**: Omnidirectional depth sensing has its advantage over the conventional stereo systems since it enables us to recognize the objects of interest in all directions without any blind regions. In this paper, we propose a novel wide-baseline omnidirectional stereo algorithm which computes the dense depth estimate from the fisheye images using a deep convolutional neural network. The capture system consists of multiple cameras mounted on a wide-baseline rig with ultrawide field of view (FOV) lenses, and we present the calibration algorithm for the extrinsic parameters based on the bundle adjustment. Instead of estimating depth maps from multiple sets of rectified images and stitching them, our approach directly generates one dense omnidirectional depth map with full 360-degree coverage at the rig global coordinate system. To this end, the proposed neural network is designed to output the cost volume from the warped images in the sphere sweeping method, and the final depth map is estimated by taking the minimum cost indices of the aggregated cost volume by SGM. For training the deep neural network and testing the entire system, realistic synthetic urban datasets are rendered using Blender. The experiments using the synthetic and real-world datasets show that our algorithm outperforms the conventional depth estimation methods and generate highly accurate depth maps.



### PixelSteganalysis: Pixel-wise Hidden Information Removal with Low Visual Degradation
- **Arxiv ID**: http://arxiv.org/abs/1902.10905v3
- **DOI**: None
- **Categories**: **cs.MM**, cs.CR, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10905v3)
- **Published**: 2019-02-28 05:36:29+00:00
- **Updated**: 2021-12-09 07:08:59+00:00
- **Authors**: Dahuin Jung, Ho Bae, Hyun-Soo Choi, Sungroh Yoon
- **Comment**: IEEE TDSC
- **Journal**: None
- **Summary**: Recently, the field of steganography has experienced rapid developments based on deep learning (DL). DL based steganography distributes secret information over all the available bits of the cover image, thereby posing difficulties in using conventional steganalysis methods to detect, extract or remove hidden secret images. However, our proposed framework is the first to effectively disable covert communications and transactions that use DL based steganography. We propose a DL based steganalysis technique that effectively removes secret images by restoring the distribution of the original images. We formulate a problem and address it by exploiting sophisticated pixel distributions and an edge distribution of images by using a deep neural network. Based on the given information, we remove the hidden secret information at the pixel level. We evaluate our technique by comparing it with conventional steganalysis methods using three public benchmarks. As the decoding method of DL based steganography is approximate (lossy) and is different from the decoding method of conventional steganography, we also introduce a new quantitative metric called the destruction rate (DT). The experimental results demonstrate performance improvements of 10-20% in both the decoded rate and the DT.



### High dynamic range image forensics using cnn
- **Arxiv ID**: http://arxiv.org/abs/1902.10938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10938v1)
- **Published**: 2019-02-28 07:53:42+00:00
- **Updated**: 2019-02-28 07:53:42+00:00
- **Authors**: Yongqing Huo, Xiaofeng Zhu
- **Comment**: 6 pages,4 figures,3 tables,conference
- **Journal**: None
- **Summary**: High dynamic range (HDR) imaging has recently drawn much attention in multimedia community. In this paper, we proposed a HDR image forensics method based on convolutional neural network (CNN).To our best knowledge, this is the first time to apply deep learning method on HDR image forensics. The proposed algorithm uses CNN to distinguish HDR images generated by multiple low dynamic range (LDR) images from that expanded by single LDR image using inverse tone mapping (iTM). To do this, we learn the change of statistical characteristics extracted by the proposed CNN architectures and classify two kinds of HDR images. Comparision results with some traditional statistical characteristics shows efficiency of the proposed method in HDR image source identification.



### A security steganography scheme based on hdr image
- **Arxiv ID**: http://arxiv.org/abs/1902.10943v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10943v1)
- **Published**: 2019-02-28 08:05:33+00:00
- **Updated**: 2019-02-28 08:05:33+00:00
- **Authors**: Wei Gao, Yongqing Huo, Yan Qiao
- **Comment**: 5 pages,6 figures,1 table,conference
- **Journal**: None
- **Summary**: It is widely recognized that the image format is crucial to steganography for that each individual format has its unique properities. Nowadays, the most famous approach of digital image steganography is to combine a well-defined distortion function with efficient practical codes such as STC. And numerous researches are concentrated on spatial domain and jpeg domain. However, whether in spatial domain or jpeg domain, high payload (e.g., 0.5 bit per pixel) is not secure enough. In this paper, we propose a novel adaptive steganography scheme based on 32-bit HDR (High dynamic range) format and Norm IEEE 754. Experiments show that the steganographic method can achieve satisfactory security under payload from 0.3bpp to 0.5bpp.



### Learning Representations from Persian Handwriting for Offline Signature Verification, a Deep Transfer Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1903.06249v1
- **DOI**: 10.1109/PRIA.2019.8785979
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06249v1)
- **Published**: 2019-02-28 08:13:55+00:00
- **Updated**: 2019-02-28 08:13:55+00:00
- **Authors**: Omid Mersa, Farhood Etaati, Saeed Masoudnia, Babak N. Araabi
- **Comment**: None
- **Journal**: 2019 4th International Conference on Pattern Recognition and Image
  Analysis (IPRIA)
- **Summary**: Offline Signature Verification (OSV) is a challenging pattern recognition task, especially when it is expected to generalize well on the skilled forgeries that are not available during the training. Its challenges also include small training sample and large intra-class variations. Considering the limitations, we suggest a novel transfer learning approach from Persian handwriting domain to multi-language OSV domain. We train two Residual CNNs on the source domain separately based on two different tasks of word classification and writer identification. Since identifying a person signature resembles identifying ones handwriting, it seems perfectly convenient to use handwriting for the feature learning phase. The learned representation on the more varied and plentiful handwriting dataset can compensate for the lack of training data in the original task, i.e. OSV, without sacrificing the generalizability. Our proposed OSV system includes two steps: learning representation and verification of the input signature. For the first step, the signature images are fed into the trained Residual CNNs. The output representations are then used to train SVMs for the verification. We test our OSV system on three different signature datasets, including MCYT (a Spanish signature dataset), UTSig (a Persian one) and GPDS-Synthetic (an artificial dataset). On UT-SIG, we achieved 9.80% Equal Error Rate (EER) which showed substantial improvement over the best EER in the literature, 17.45%. Our proposed method surpassed state-of-the-arts by 6% on GPDS-Synthetic, achieving 6.81%. On MCYT, EER of 3.98% was obtained which is comparable to the best previously reported results.



### Look, Investigate, and Classify: A Deep Hybrid Attention Method for Breast Cancer Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.10946v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10946v1)
- **Published**: 2019-02-28 08:24:24+00:00
- **Updated**: 2019-02-28 08:24:24+00:00
- **Authors**: Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, Jon Garibaldi, Ian O. Ellis, Andy Green, Linlin Shen, Guoping Qiu
- **Comment**: Accepted to ISBI'19
- **Journal**: None
- **Summary**: One issue with computer based histopathology image analysis is that the size of the raw image is usually very large. Taking the raw image as input to the deep learning model would be computationally expensive while resizing the raw image to low resolution would incur information loss. In this paper, we present a novel deep hybrid attention approach to breast cancer classification. It first adaptively selects a sequence of coarse regions from the raw image by a hard visual attention algorithm, and then for each such region it is able to investigate the abnormal parts based on a soft-attention mechanism. A recurrent network is then built to make decisions to classify the image region and also to predict the location of the image region to be investigated at the next time step. As the region selection process is non-differentiable, we optimize the whole network through a reinforcement approach to learn an optimal policy to classify the regions. Based on this novel Look, Investigate and Classify approach, we only need to process a fraction of the pixels in the raw image resulting in significant saving in computational resources without sacrificing performances. Our approach is evaluated on a public breast cancer histopathology database, where it demonstrates superior performance to the state-of-the-art deep learning approaches, achieving around 96\% classification accuracy while only 15% of raw pixels are used.



### Dynamic Multi-path Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1902.10949v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10949v3)
- **Published**: 2019-02-28 08:48:18+00:00
- **Updated**: 2019-04-07 07:23:06+00:00
- **Authors**: Yingcheng Su, Shunfeng Zhou, Yichao Wu, Tian Su, Ding Liang, Jiaheng Liu, Dixin Zheng, Yingxu Wang, Junjie Yan, Xiaolin Hu
- **Comment**: None
- **Journal**: None
- **Summary**: Although deeper and larger neural networks have achieved better performance, the complex network structure and increasing computational cost cannot meet the demands of many resource-constrained applications. Existing methods usually choose to execute or skip an entire specific layer, which can only alter the depth of the network. In this paper, we propose a novel method called Dynamic Multi-path Neural Network (DMNN), which provides more path selection choices in terms of network width and depth during inference. The inference path of the network is determined by a controller, which takes into account both previous state and object category information. The proposed method can be easily incorporated into most modern network architectures. Experimental results on ImageNet and CIFAR-100 demonstrate the superiority of our method on both efficiency and overall classification accuracy. To be specific, DMNN-101 significantly outperforms ResNet-101 with an encouraging 45.1% FLOPs reduction, and DMNN-50 performs comparably to ResNet-101 while saving 42.1% parameters.



### Extended Gaze Following: Detecting Objects in Videos Beyond the Camera Field of View
- **Arxiv ID**: http://arxiv.org/abs/1902.10953v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10953v1)
- **Published**: 2019-02-28 08:59:59+00:00
- **Updated**: 2019-02-28 08:59:59+00:00
- **Authors**: Benoit Massé, Stéphane Lathuilière, Pablo Mesejo, Radu Horaud
- **Comment**: FG 2019
- **Journal**: None
- **Summary**: In this paper we address the problems of detecting objects of interest in a video and of estimating their locations, solely from the gaze directions of people present in the video. Objects can be indistinctly located inside or outside the camera field of view. We refer to this problem as extended gaze following. The contributions of the paper are the followings. First, we propose a novel spatial representation of the gaze directions adopting a top-view perspective. Second, we develop several convolutional encoder/decoder networks to predict object locations and compare them with heuristics and with classical learning-based approaches. Third, in order to train the proposed models, we generate a very large number of synthetic scenarios employing a probabilistic formulation. Finally, our methodology is empirically validated using a publicly available dataset.



### Octree guided CNN with Spherical Kernels for 3D Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1903.00343v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00343v1)
- **Published**: 2019-02-28 09:53:28+00:00
- **Updated**: 2019-02-28 09:53:28+00:00
- **Authors**: Huan Lei, Naveed Akhtar, Ajmal Mian
- **Comment**: Accepted in IEEE CVPR 2019. arXiv admin note: substantial text
  overlap with arXiv:1805.07872
- **Journal**: None
- **Summary**: We propose an octree guided neural network architecture and spherical convolutional kernel for machine learning from arbitrary 3D point clouds. The network architecture capitalizes on the sparse nature of irregular point clouds, and hierarchically coarsens the data representation with space partitioning. At the same time, the proposed spherical kernels systematically quantize point neighborhoods to identify local geometric structures in the data, while maintaining the properties of translation-invariance and asymmetry. We specify spherical kernels with the help of network neurons that in turn are associated with spatial locations. We exploit this association to avert dynamic kernel generation during network training that enables efficient learning with high resolution point clouds. The effectiveness of the proposed technique is established on the benchmark tasks of 3D object classification and segmentation, achieving new state-of-the-art on ShapeNet and RueMonge2014 datasets.



### End-to-End Efficient Representation Learning via Cascading Combinatorial Optimization
- **Arxiv ID**: http://arxiv.org/abs/1902.10990v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10990v2)
- **Published**: 2019-02-28 10:14:31+00:00
- **Updated**: 2019-03-07 07:34:14+00:00
- **Authors**: Yeonwoo Jeong, Yoonsung Kim, Hyun Oh Song
- **Comment**: Accepted and to appear at CVPR 2019
- **Journal**: None
- **Summary**: We develop hierarchically quantized efficient embedding representations for similarity-based search and show that this representation provides not only the state of the art performance on the search accuracy but also provides several orders of speed up during inference. The idea is to hierarchically quantize the representation so that the quantization granularity is greatly increased while maintaining the accuracy and keeping the computational complexity low. We also show that the problem of finding the optimal sparse compound hash code respecting the hierarchical structure can be optimized in polynomial time via minimum cost flow in an equivalent flow network. This allows us to train the method end-to-end in a mini-batch stochastic gradient descent setting. Our experiments on Cifar100 and ImageNet datasets show the state of the art search accuracy while providing several orders of magnitude search speedup respectively over exhaustive linear search over the dataset.



### Salient object detection on hyperspectral images using features learned from unsupervised segmentation task
- **Arxiv ID**: http://arxiv.org/abs/1902.10993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10993v1)
- **Published**: 2019-02-28 10:23:00+00:00
- **Updated**: 2019-02-28 10:23:00+00:00
- **Authors**: Nevrez Imamoglu, Guanqun Ding, Yuming Fang, Asako Kanezaki, Toru Kouyama, Ryosuke Nakamura
- **Comment**: 5 pages, 3 figures, accepted to appear in IEEE ICASSP 2019 (accepted
  version)
- **Journal**: None
- **Summary**: Various saliency detection algorithms from color images have been proposed to mimic eye fixation or attentive object detection response of human observers for the same scenes. However, developments on hyperspectral imaging systems enable us to obtain redundant spectral information of the observed scenes from the reflected light source from objects. A few studies using low-level features on hyperspectral images demonstrated that salient object detection can be achieved. In this work, we proposed a salient object detection model on hyperspectral images by applying manifold ranking (MR) on self-supervised Convolutional Neural Network (CNN) features (high-level features) from unsupervised image segmentation task. Self-supervision of CNN continues until clustering loss or saliency maps converges to a defined error between each iteration. Finally, saliency estimations is done as the saliency map at last iteration when the self-supervision procedure terminates with convergence. Experimental evaluations demonstrated that proposed saliency detection algorithm on hyperspectral images is outperforming state-of-the-arts hyperspectral saliency models including the original MR based saliency model.



### 3D High-Resolution Cardiac Segmentation Reconstruction from 2D Views using Conditional Variational Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1902.11000v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11000v1)
- **Published**: 2019-02-28 10:39:51+00:00
- **Updated**: 2019-02-28 10:39:51+00:00
- **Authors**: Carlo Biffi, Juan J. Cerrolaza, Giacomo Tarroni, Antonio de Marvao, Stuart A. Cook, Declan P. O'Regan, Daniel Rueckert
- **Comment**: Accepted in IEEE International Symposium on Biomedical Imaging (ISBI
  2019)
- **Journal**: None
- **Summary**: Accurate segmentation of heart structures imaged by cardiac MR is key for the quantitative analysis of pathology. High-resolution 3D MR sequences enable whole-heart structural imaging but are time-consuming, expensive to acquire and they often require long breath holds that are not suitable for patients. Consequently, multiplanar breath-hold 2D cine sequences are standard practice but are disadvantaged by lack of whole-heart coverage and low through-plane resolution. To address this, we propose a conditional variational autoencoder architecture able to learn a generative model of 3D high-resolution left ventricular (LV) segmentations which is conditioned on three 2D LV segmentations of one short-axis and two long-axis images. By only employing these three 2D segmentations, our model can efficiently reconstruct the 3D high-resolution LV segmentation of a subject. When evaluated on 400 unseen healthy volunteers, our model yielded an average Dice score of $87.92 \pm 0.15$ and outperformed competing architectures.



### MassFace: an efficient implementation using triplet loss for face recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.11007v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11007v1)
- **Published**: 2019-02-28 10:49:39+00:00
- **Updated**: 2019-02-28 10:49:39+00:00
- **Authors**: Yule Li
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we present an efficient implementation using triplet loss for face recognition. We conduct the practical experiment to analyze the factors that influence the training of triplet loss. All models are trained on CASIA-Webface dataset and tested on LFW. We analyze the experiment results and give some insights to help others balance the factors when they apply triplet loss to their own problem especially for face recognition task. Code has been released in https://github.com/yule-li/MassFace.



### DPOD: 6D Pose Object Detector and Refiner
- **Arxiv ID**: http://arxiv.org/abs/1902.11020v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.11020v3)
- **Published**: 2019-02-28 11:15:02+00:00
- **Updated**: 2019-08-20 17:40:55+00:00
- **Authors**: Sergey Zakharov, Ivan Shugurov, Slobodan Ilic
- **Comment**: ICCV 2019. 8 pages + supplementary material + references. The first
  two authors contributed equally to this work
- **Journal**: None
- **Summary**: In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.



### Towards Multi-pose Guided Virtual Try-on Network
- **Arxiv ID**: http://arxiv.org/abs/1902.11026v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11026v1)
- **Published**: 2019-02-28 11:34:52+00:00
- **Updated**: 2019-02-28 11:34:52+00:00
- **Authors**: Haoye Dong, Xiaodan Liang, Bochao Wang, Hanjiang Lai, Jia Zhu, Jian Yin
- **Comment**: 11 pages, 10 figures
- **Journal**: None
- **Summary**: Virtual try-on system under arbitrary human poses has huge application potential, yet raises quite a lot of challenges, e.g. self-occlusions, heavy misalignment among diverse poses, and diverse clothes textures. Existing methods aim at fitting new clothes into a person can only transfer clothes on the fixed human pose, but still show unsatisfactory performances which often fail to preserve the identity, lose the texture details, and decrease the diversity of poses. In this paper, we make the first attempt towards multi-pose guided virtual try-on system, which enables transfer clothes on a person image under diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-on Network (MG-VTON) can generate a new person image after fitting the desired clothes into the input image and manipulating human poses. Our MG-VTON is constructed in three stages: 1) a desired human parsing map of the target image is synthesized to match both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and desired human pose; 3) a refinement render utilizing multi-pose composition masks recovers the texture details of clothes and removes some artifacts. Extensive experiments on well-known datasets and our newly collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively with promising multi-pose virtual try-on performances.



### Multispectral snapshot demosaicing via non-convex matrix completion
- **Arxiv ID**: http://arxiv.org/abs/1902.11032v2
- **DOI**: 10.1109/DSW.2019.8755561
- **Categories**: **cs.CV**, 94A08, 15A83, I.4.5; I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1902.11032v2)
- **Published**: 2019-02-28 11:56:53+00:00
- **Updated**: 2019-04-23 12:14:46+00:00
- **Authors**: Giancarlo A. Antonucci, Simon Vary, David Humphreys, Robert A. Lamb, Jonathan Piper, Jared Tanner
- **Comment**: 5 pages, 2 figures, 1 table
- **Journal**: None
- **Summary**: Snapshot mosaic multispectral imagery acquires an undersampled data cube by acquiring a single spectral measurement per spatial pixel. Sensors which acquire $p$ frequencies, therefore, suffer from severe $1/p$ undersampling of the full data cube. We show that the missing entries can be accurately imputed using non-convex techniques from sparse approximation and matrix completion initialised with traditional demosaicing algorithms. In particular, we observe the peak signal-to-noise ratio can typically be improved by 2 to 5 dB over current state-of-the-art methods when simulating a $p=16$ mosaic sensor measuring both high and low altitude urban and rural scenes as well as ground-based scenes.



### Unsupervised Abnormality Detection through Mixed Structure Regularization (MSR) in Deep Sparse Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1902.11036v1
- **DOI**: 10.1002/mp.13464
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11036v1)
- **Published**: 2019-02-28 12:01:48+00:00
- **Updated**: 2019-02-28 12:01:48+00:00
- **Authors**: Moti Freiman, Ravindra Manjeshwar, Liran Goshen
- **Comment**: Accepted for publication in the journal: "Medical Physics" (2019)
- **Journal**: None
- **Summary**: Deep sparse auto-encoders with mixed structure regularization (MSR) in addition to explicit sparsity regularization term and stochastic corruption of the input data with Gaussian noise have the potential to improve unsupervised abnormality detection. Unsupervised abnormality detection based on identifying outliers using deep sparse auto-encoders is a very appealing approach for medical computer aided detection systems as it requires only healthy data for training rather than expert annotated abnormality. In the task of detecting coronary artery disease from Coronary Computed Tomography Angiography (CCTA), our results suggests that the MSR has the potential to improve overall performance by 20-30% compared to deep sparse and denoising auto-encoders.



### GCNv2: Efficient Correspondence Prediction for Real-Time SLAM
- **Arxiv ID**: http://arxiv.org/abs/1902.11046v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.11046v3)
- **Published**: 2019-02-28 12:23:53+00:00
- **Updated**: 2019-06-18 12:37:50+00:00
- **Authors**: Jiexiong Tang, Ludvig Ericson, John Folkesson, Patric Jensfelt
- **Comment**: Project page: https://github.com/jiexiong2016/GCNv2_SLAM
- **Journal**: None
- **Summary**: In this paper, we present a deep learning-based network, GCNv2, for generation of keypoints and descriptors. GCNv2 is built on our previous method, GCN, a network trained for 3D projective geometry. GCNv2 is designed with a binary descriptor vector as the ORB feature so that it can easily replace ORB in systems such as ORB-SLAM2. GCNv2 significantly improves the computational efficiency over GCN that was only able to run on desktop hardware. We show how a modified version of ORB-SLAM2 using GCNv2 features runs on a Jetson TX2, an embedded low-power platform. Experimental results show that GCNv2 retains comparable accuracy as GCN and that it is robust enough to use for control of a flying drone.



### Segmentation of Roots in Soil with U-Net
- **Arxiv ID**: http://arxiv.org/abs/1902.11050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11050v2)
- **Published**: 2019-02-28 12:34:31+00:00
- **Updated**: 2019-03-18 15:08:51+00:00
- **Authors**: Abraham George Smith, Jens Petersen, Raghavendra Selvan, Camilla Ruø Rasmussen
- **Comment**: None
- **Journal**: None
- **Summary**: Plant root research can provide a way to attain stress-tolerant crops that produce greater yield in a diverse array of conditions. Phenotyping roots in soil is often challenging due to the roots being difficult to access and the use of time consuming manual methods. Rhizotrons allow visual inspection of root growth through transparent surfaces. Agronomists currently manually label photographs of roots obtained from rhizotrons using a line-intersect method to obtain root length density and rooting depth measurements which are essential for their experiments. We investigate the effectiveness of an automated image segmentation method based on the U-Net Convolutional Neural Network (CNN) architecture to enable such measurements. We design a data-set of 50 annotated Chicory (Cichorium intybus L.) root images which we use to train, validate and test the system and compare against a baseline built using the Frangi vesselness filter. We obtain metrics using manual annotations and line-intersect counts. Our results on the held out data show our proposed automated segmentation system to be a viable solution for detecting and quantifying roots. We evaluate our system using 867 images for which we have obtained line-intersect counts, attaining a Spearman rank correlation of 0.9748 and an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the automated segmentation to the manual annotations, with our automated segmentation system producing segmentations with higher quality than the manual annotations for large portions of the image.



### Biometric Presentation Attack Detection: Beyond the Visible Spectrum
- **Arxiv ID**: http://arxiv.org/abs/1902.11065v1
- **DOI**: 10.1109/TIFS.2019.2934867
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11065v1)
- **Published**: 2019-02-28 13:12:11+00:00
- **Updated**: 2019-02-28 13:12:11+00:00
- **Authors**: Ruben Tolosana, Marta Gomez-Barrero, Christoph Busch, Javier Ortega-Garcia
- **Comment**: None
- **Journal**: None
- **Summary**: The increased need for unattended authentication in multiple scenarios has motivated a wide deployment of biometric systems in the last few years. This has in turn led to the disclosure of security concerns specifically related to biometric systems. Among them, Presentation Attacks (PAs, i.e., attempts to log into the system with a fake biometric characteristic or presentation attack instrument) pose a severe threat to the security of the system: any person could eventually fabricate or order a gummy finger or face mask to impersonate someone else. The biometrics community has thus made a considerable effort to the development of automatic Presentation Attack Detection (PAD) mechanisms, for instance through the international LivDet competitions.   In this context, we present a novel fingerprint PAD scheme based on $i)$ a new capture device able to acquire images within the short wave infrared (SWIR) spectrum, and $ii)$ an in-depth analysis of several state-of-the-art techniques based on both handcrafted and deep learning features. The approach is evaluated on a database comprising over 4700 samples, stemming from 562 different subjects and 35 different presentation attack instrument (PAI) species. The results show the soundness of the proposed approach with a detection equal error rate (D-EER) as low as 1.36\% even in a realistic scenario where five different PAI species are considered only for testing purposes (i.e., unknown attacks).



### Active Transfer Learning for Persian Offline Signature Verification
- **Arxiv ID**: http://arxiv.org/abs/1903.06255v1
- **DOI**: 10.1109/PRIA.2019.8786013
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.06255v1)
- **Published**: 2019-02-28 13:49:46+00:00
- **Updated**: 2019-02-28 13:49:46+00:00
- **Authors**: Taraneh Younesian, Saeed Masoudnia, Reshad Hosseini, Babak N. Araabi
- **Comment**: None
- **Journal**: 2019 4th International Conference on Pattern Recognition and Image
  Analysis (IPRIA)
- **Summary**: Offline Signature Verification (OSV) remains a challenging pattern recognition task, especially in the presence of skilled forgeries that are not available during the training. This challenge is aggravated when there are small labeled training data available but with large intra-personal variations. In this study, we address this issue by employing an active learning approach, which selects the most informative instances to label and therefore reduces the human labeling effort significantly. Our proposed OSV includes three steps: feature learning, active learning, and final verification. We benefit from transfer learning using a pre-trained CNN for feature learning. We also propose SVM-based active learning for each user to separate his genuine signatures from the random forgeries. We finally used the SVMs to verify the authenticity of the questioned signature. We examined our proposed active transfer learning method on UTSig: A Persian offline signature dataset. We achieved near 13% improvement compared to the random selection of instances. Our results also showed 1% improvement over the state-of-the-art method in which a fully supervised setting with five more labeled instances per user was used.



### Rolling Shutter Camera Synchronization with Sub-millisecond Accuracy
- **Arxiv ID**: http://arxiv.org/abs/1902.11084v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11084v1)
- **Published**: 2019-02-28 13:57:58+00:00
- **Updated**: 2019-02-28 13:57:58+00:00
- **Authors**: Matej Smid, Jiri Matas
- **Comment**: 8 pages, 10 figures, published at VISAPP 2017
- **Journal**: None
- **Summary**: A simple method for synchronization of video streams with a precision better than one millisecond is proposed. The method is applicable to any number of rolling shutter cameras and when a few photographic flashes or other abrupt lighting changes are present in the video. The approach exploits the rolling shutter sensor property that every sensor row starts its exposure with a small delay after the onset of the previous row. The cameras may have different frame rates and resolutions, and need not have overlapping fields of view. The method was validated on five minutes of four streams from an ice hockey match. The found transformation maps events visible in all cameras to a reference time with a standard deviation of the temporal error in the range of 0.3 to 0.5 milliseconds. The quality of the synchronization is demonstrated on temporally and spatially overlapping images of a fast moving puck observed in two cameras.



### Real-time 3D Shape Instantiation for Partially-deployed Stent Segment from a Single 2D Fluoroscopic Image in Robot-assisted Fenestrated Endovascular Aortic Repair
- **Arxiv ID**: http://arxiv.org/abs/1902.11089v1
- **DOI**: 10.1109/LRA.2019.2950499
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11089v1)
- **Published**: 2019-02-28 14:06:36+00:00
- **Updated**: 2019-02-28 14:06:36+00:00
- **Authors**: Jian-Qing Zheng, Xiao-Yun Zhou, Guang-Zhong Yang
- **Comment**: 8 pages, 10 figures
- **Journal**: None
- **Summary**: In robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR), accurate alignment of stent graft fenestrations or scallops with aortic branches is essential for establishing complete blood flow perfusion. Current navigation is largely based on 2D fluoroscopic images, which lacks 3D anatomical information, thus causing longer operation time as well as high risks of radiation exposure. Previously, 3D shape instantiation frameworks for real-time 3D shape reconstruction of fully-deployed or fully-compressed stent graft from a single 2D fluoroscopic image have been proposed for 3D navigation in robot-assisted FEVAR. However, these methods could not instantiate partially-deployed stent segments, as the 3D marker references are unknown. In this paper, an adapted Graph Convolutional Network (GCN) is proposed to predict 3D marker references from 3D fully-deployed markers. As original GCN is for classification, in this paper, the coarsening layers are removed and the softmax function at the network end is replaced with linear mapping for the regression task. The derived 3D and the 2D marker references are used to instantiate partially-deployed stent segment shape with the existing 3D shape instantiation framework. Validations were performed on three commonly used stent grafts and five patient-specific 3D printed aortic aneurysm phantoms. Comparable performances with average mesh distance errors of 1$\sim$3mm and average angular errors around 7degree were achieved.



### ROVO: Robust Omnidirectional Visual Odometry for Wide-baseline Wide-FOV Camera Systems
- **Arxiv ID**: http://arxiv.org/abs/1902.11154v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.11154v2)
- **Published**: 2019-02-28 15:29:27+00:00
- **Updated**: 2019-03-01 15:45:42+00:00
- **Authors**: Hochang Seok, Jongwoo Lim
- **Comment**: 7 pages, 9 figures, ICRA 2019 accepted
- **Journal**: None
- **Summary**: In this paper we propose a robust visual odometry system for a wide-baseline camera rig with wide field-of-view (FOV) fisheye lenses, which provides full omnidirectional stereo observations of the environment. For more robust and accurate ego-motion estimation we adds three components to the standard VO pipeline, 1) the hybrid projection model for improved feature matching, 2) multi-view P3P RANSAC algorithm for pose estimation, and 3) online update of rig extrinsic parameters. The hybrid projection model combines the perspective and cylindrical projection to maximize the overlap between views and minimize the image distortion that degrades feature matching performance. The multi-view P3P RANSAC algorithm extends the conventional P3P RANSAC to multi-view images so that all feature matches in all views are considered in the inlier counting for robust pose estimation. Finally the online extrinsic calibration is seamlessly integrated in the backend optimization framework so that the changes in camera poses due to shocks or vibrations can be corrected automatically. The proposed system is extensively evaluated with synthetic datasets with ground-truth and real sequences of highly dynamic environment, and its superior performance is demonstrated.



### FaceLiveNet+: A Holistic Networks For Face Authentication Based On Dynamic Multi-task Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.11179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11179v1)
- **Published**: 2019-02-28 15:58:08+00:00
- **Updated**: 2019-02-28 15:58:08+00:00
- **Authors**: Zuheng Ming, Junshi Xia, Muhammad Muzzamil Luqman, Jean-Christophe Burie, Kaixing Zhao
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: This paper proposes a holistic multi-task Convolutional Neural Networks (CNNs) with the dynamic weights of the tasks,namely FaceLiveNet+, for face authentication. FaceLiveNet+ can employ face verification and facial expression recognition as a solution of liveness control simultaneously. Comparing to the single-task learning, the proposed multi-task learning can better capture the feature representation for all of the tasks. The experimental results show the superiority of the multi-task learning to the single-task learning for both the face verification task and facial expression recognition task. Rather using a conventional multi-task learning with fixed weights for the tasks, this work proposes a so called dynamic-weight-unit to automatically learn the weights of the tasks. The experiments have shown the effectiveness of the dynamic weights for training the networks. Finally, the holistic evaluation for face authentication based on the proposed protocol has shown the feasibility to apply the FaceLiveNet+ for face authentication.



### Two-phase Hair Image Synthesis by Self-Enhancing Generative Model
- **Arxiv ID**: http://arxiv.org/abs/1902.11203v1
- **DOI**: 10.1111/cgf.13847
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11203v1)
- **Published**: 2019-02-28 16:41:23+00:00
- **Updated**: 2019-02-28 16:41:23+00:00
- **Authors**: Haonan Qiu, Chuan Wang, Hang Zhu, Xiangyu Zhu, Jinjin Gu, Xiaoguang Han
- **Comment**: None
- **Journal**: None
- **Summary**: Generating plausible hair image given limited guidance, such as sparse sketches or low-resolution image, has been made possible with the rise of Generative Adversarial Networks (GANs). Traditional image-to-image translation networks can generate recognizable results, but finer textures are usually lost and blur artifacts commonly exist. In this paper, we propose a two-phase generative model for high-quality hair image synthesis. The two-phase pipeline first generates a coarse image by an existing image translation model, then applies a re-generating network with self-enhancing capability to the coarse image. The self-enhancing capability is achieved by a proposed structure extraction layer, which extracts the texture and orientation map from a hair image. Extensive experiments on two tasks, Sketch2Hair and Hair Super-Resolution, demonstrate that our approach is able to synthesize plausible hair image with finer details, and outperforms the state-of-the-art.



### No Padding Please: Efficient Neural Handwriting Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.11208v1
- **DOI**: 10.1109/ICDAR.2019.00064
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1902.11208v1)
- **Published**: 2019-02-28 16:46:43+00:00
- **Updated**: 2019-02-28 16:46:43+00:00
- **Authors**: Gideon Maillette de Buy Wenniger, Lambert Schomaker, Andy Way
- **Comment**: None
- **Journal**: None
- **Summary**: Neural handwriting recognition (NHR) is the recognition of handwritten text with deep learning models, such as multi-dimensional long short-term memory (MDLSTM) recurrent neural networks. Models with MDLSTM layers have achieved state-of-the art results on handwritten text recognition tasks. While multi-directional MDLSTM-layers have an unbeaten ability to capture the complete context in all directions, this strength limits the possibilities for parallelization, and therefore comes at a high computational cost. In this work we develop methods to create efficient MDLSTM-based models for NHR, particularly a method aimed at eliminating computation waste that results from padding. This proposed method, called example-packing, replaces wasteful stacking of padded examples with efficient tiling in a 2-dimensional grid. For word-based NHR this yields a speed improvement of factor 6.6 over an already efficient baseline of minimal padding for each batch separately. For line-based NHR the savings are more modest, but still significant. In addition to example-packing, we propose: 1) a technique to optimize parallelization for dynamic graph definition frameworks including PyTorch, using convolutions with grouping, 2) a method for parallelization across GPUs for variable-length example batches. All our techniques are thoroughly tested on our own PyTorch re-implementation of MDLSTM-based NHR models. A thorough evaluation on the IAM dataset shows that our models are performing similar to earlier implementations of state-of-the-art models. Our efficient NHR model and some of the reusable techniques discussed with it offer ways to realize relatively efficient models for the omnipresent scenario of variable-length inputs in deep learning.



### Large-Scale Object Mining for Object Discovery from Unlabeled Video
- **Arxiv ID**: http://arxiv.org/abs/1903.00362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00362v2)
- **Published**: 2019-02-28 16:53:48+00:00
- **Updated**: 2019-04-29 13:46:27+00:00
- **Authors**: Aljosa Osep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers, Bastian Leibe
- **Comment**: Updated version of ICRA'19 paper (additional qualitative results);
  arXiv admin note: text overlap with arXiv:1712.08832
- **Journal**: None
- **Summary**: This paper addresses the problem of object discovery from unlabeled driving videos captured in a realistic automotive setting. Identifying recurring object categories in such raw video streams is a very challenging problem. Not only do object candidates first have to be localized in the input images, but many interesting object categories occur relatively infrequently. Object discovery will therefore have to deal with the difficulties of operating in the long tail of the object distribution. We demonstrate the feasibility of performing fully automatic object discovery in such a setting by mining object tracks using a generic object tracker. In order to facilitate further research in object discovery, we release a collection of more than 360,000 automatically mined object tracks from 10+ hours of video data (560,000 frames). We use this dataset to evaluate the suitability of different feature representations and clustering strategies for object discovery.



### Application-level Studies of Cellular Neural Network-based Hardware Accelerators
- **Arxiv ID**: http://arxiv.org/abs/1903.06649v2
- **DOI**: None
- **Categories**: **cs.ET**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1903.06649v2)
- **Published**: 2019-02-28 17:07:33+00:00
- **Updated**: 2019-06-12 21:23:42+00:00
- **Authors**: Qiuwen Lou, Indranil Palit, Tang Li, Andras Horvath, Michael Niemier, X. Sharon Hu
- **Comment**: None
- **Journal**: None
- **Summary**: As cost and performance benefits associated with Moore's Law scaling slow, researchers are studying alternative architectures (e.g., based on analog and/or spiking circuits) and/or computational models (e.g., convolutional and recurrent neural networks) to perform application-level tasks faster, more energy efficiently, and/or more accurately. We investigate cellular neural network (CeNN)-based co-processors at the application-level for these metrics. While it is well-known that CeNNs can be well-suited for spatio-temporal information processing, few (if any) studies have quantified the energy/delay/accuracy of a CeNN-friendly algorithm and compared the CeNN-based approach to the best von Neumann algorithm at the application level. We present an evaluation framework for such studies. As a case study, a CeNN-friendly target-tracking algorithm was developed and mapped to an array architecture developed in conjunction with the algorithm. We compare the energy, delay, and accuracy of our architecture/algorithm (assuming all overheads) to the most accurate von Neumann algorithm (Struck). Von Neumann CPU data is measured on an Intel i5 chip. The CeNN approach is capable of matching the accuracy of Struck, and can offer approximately 1000x improvements in energy-delay product.



### Efficient Parameter-free Clustering Using First Neighbor Relations
- **Arxiv ID**: http://arxiv.org/abs/1902.11266v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11266v1)
- **Published**: 2019-02-28 18:12:57+00:00
- **Updated**: 2019-02-28 18:12:57+00:00
- **Authors**: M. Saquib Sarfraz, Vivek Sharma, Rainer Stiefelhagen
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: We present a new clustering method in the form of a single clustering equation that is able to directly discover groupings in the data. The main proposition is that the first neighbor of each sample is all one needs to discover large chains and finding the groups in the data. In contrast to most existing clustering algorithms our method does not require any hyper-parameters, distance thresholds and/or the need to specify the number of clusters. The proposed algorithm belongs to the family of hierarchical agglomerative methods. The technique has a very low computational overhead, is easily scalable and applicable to large practical problems. Evaluation on well known datasets from different domains ranging between 1077 and 8.1 million samples shows substantial performance gains when compared to the existing clustering techniques.



### CircConv: A Structured Convolution with Low Complexity
- **Arxiv ID**: http://arxiv.org/abs/1902.11268v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11268v1)
- **Published**: 2019-02-28 18:18:51+00:00
- **Updated**: 2019-02-28 18:18:51+00:00
- **Authors**: Siyu Liao, Zhe Li, Liang Zhao, Qinru Qiu, Yanzhi Wang, Bo Yuan
- **Comment**: None
- **Journal**: Published in AAAI 2019
- **Summary**: Deep neural networks (DNNs), especially deep convolutional neural networks (CNNs), have emerged as the powerful technique in various machine learning applications. However, the large model sizes of DNNs yield high demands on computation resource and weight storage, thereby limiting the practical deployment of DNNs. To overcome these limitations, this paper proposes to impose the circulant structure to the construction of convolutional layers, and hence leads to circulant convolutional layers (CircConvs) and circulant CNNs. The circulant structure and models can be either trained from scratch or re-trained from a pre-trained non-circulant model, thereby making it very flexible for different training environments. Through extensive experiments, such strong structure-imposing approach is proved to be able to substantially reduce the number of parameters of convolutional layers and enable significant saving of computational cost by using fast multiplication of the circulant tensor.



### A Novel Multi-Attention Driven System For Multi-Label Remote Sensing Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.11274v3
- **DOI**: 10.1109/IGARSS.2019.8898188
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11274v3)
- **Published**: 2019-02-28 18:25:19+00:00
- **Updated**: 2019-05-29 08:39:36+00:00
- **Authors**: Gencer Sumbul, Begüm Demir
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2019
- **Journal**: None
- **Summary**: This paper presents a novel multi-attention driven system that jointly exploits Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) in the context of multi-label remote sensing (RS) image classification. The proposed system consists of four main modules. The first module aims to extract preliminary local descriptors of RS image bands that can be associated to different spatial resolutions. To this end, we introduce a K-Branch CNN, in which each branch extracts descriptors of image bands that have the same spatial resolution. The second module aims to model spatial relationship among local descriptors. This is achieved by a bidirectional RNN architecture, in which Long Short-Term Memory nodes enrich local descriptors by considering spatial relationships of local areas (image patches). The third module aims to define multiple attention scores for local descriptors. This is achieved by a novel patch-based multi-attention mechanism that takes into account the joint occurrence of multiple land-cover classes and provides the attention-based local descriptors. The last module exploits these descriptors for multi-label RS image classification. Experimental results obtained on the BigEarthNet that is a large-scale Sentinel-2 benchmark archive show the effectiveness of the proposed method compared to a state of the art method.



### SPDA: Superpixel-based Data Augmentation for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1903.00035v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00035v1)
- **Published**: 2019-02-28 19:17:51+00:00
- **Updated**: 2019-02-28 19:17:51+00:00
- **Authors**: Yizhe Zhang, Lin Yang, Hao Zheng, Peixian Liang, Colleen Mangold, Raquel G. Loreto, David P. Hughes, Danny Z. Chen
- **Comment**: To appear in MIDL2019 and PMLR
- **Journal**: None
- **Summary**: Supervised training a deep neural network aims to "teach" the network to mimic human visual perception that is represented by image-and-label pairs in the training data. Superpixelized (SP) images are visually perceivable to humans, but a conventionally trained deep learning model often performs poorly when working on SP images. To better mimic human visual perception, we think it is desirable for the deep learning model to be able to perceive not only raw images but also SP images. In this paper, we propose a new superpixel-based data augmentation (SPDA) method for training deep learning models for biomedical image segmentation. Our method applies a superpixel generation scheme to all the original training images to generate superpixelized images. The SP images thus obtained are then jointly used with the original training images to train a deep learning model. Our experiments of SPDA on four biomedical image datasets show that SPDA is effective and can consistently improve the performance of state-of-the-art fully convolutional networks for biomedical image segmentation in 2D and 3D images. Additional studies also demonstrate that SPDA can practically reduce the generalization gap.



### Speeding up Deep Learning with Transient Servers
- **Arxiv ID**: http://arxiv.org/abs/1903.00045v2
- **DOI**: None
- **Categories**: **cs.PF**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00045v2)
- **Published**: 2019-02-28 19:47:59+00:00
- **Updated**: 2019-05-05 07:20:37+00:00
- **Authors**: Shijian Li, Robert J. Walls, Lijie Xu, Tian Guo
- **Comment**: Accepted to ICAC'19. 11 pages, 8 figures, 5 tables
- **Journal**: None
- **Summary**: Distributed training frameworks, like TensorFlow, have been proposed as a means to reduce the training time of deep learning models by using a cluster of GPU servers. While such speedups are often desirable---e.g., for rapidly evaluating new model designs---they often come with significantly higher monetary costs due to sublinear scalability. In this paper, we investigate the feasibility of using training clusters composed of cheaper transient GPU servers to get the benefits of distributed training without the high costs.   We conduct the first large-scale empirical analysis, launching more than a thousand GPU servers of various capacities, aimed at understanding the characteristics of transient GPU servers and their impact on distributed training performance. Our study demonstrates the potential of transient servers with a speedup of 7.7X with more than 62.9% monetary savings for some cluster configurations. We also identify a number of important challenges and opportunities for redesigning distributed training frameworks to be transient-aware. For example, the dynamic cost and availability characteristics of transient servers suggest the need for frameworks to dynamically change cluster configurations to best take advantage of current conditions.



### On the Effectiveness of Low Frequency Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1903.00073v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00073v2)
- **Published**: 2019-02-28 21:25:45+00:00
- **Updated**: 2019-06-01 00:36:16+00:00
- **Authors**: Yash Sharma, Gavin Weiguang Ding, Marcus Brubaker
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: Carefully crafted, often imperceptible, adversarial perturbations have been shown to cause state-of-the-art models to yield extremely inaccurate outputs, rendering them unsuitable for safety-critical application domains. In addition, recent work has shown that constraining the attack space to a low frequency regime is particularly effective. Yet, it remains unclear whether this is due to generally constraining the attack search space or specifically removing high frequency components from consideration. By systematically controlling the frequency components of the perturbation, evaluating against the top-placing defense submissions in the NeurIPS 2017 competition, we empirically show that performance improvements in both the white-box and black-box transfer settings are yielded only when low frequency components are preserved. In fact, the defended models based on adversarial training are roughly as vulnerable to low frequency perturbations as undefended models, suggesting that the purported robustness of state-of-the-art ImageNet defenses is reliant upon adversarial perturbations being high frequency in nature. We do find that under $\ell_\infty$ $\epsilon=16/255$, the competition distortion bound, low frequency perturbations are indeed perceptible. This questions the use of the $\ell_\infty$-norm, in particular, as a distortion metric, and, in turn, suggests that explicitly considering the frequency space is promising for learning robust models which better align with human perception.



### Broad Neural Network for Change Detection in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/1903.00087v2
- **DOI**: 10.1007/978-981-13-7403-6_31
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.00087v2)
- **Published**: 2019-02-28 22:16:56+00:00
- **Updated**: 2019-07-20 13:51:47+00:00
- **Authors**: Shailesh Shrivastava, Alakh Aggarwal, Pratik Chattopadhyay
- **Comment**: $\textbf{Accepted at}$: IEMGraph (International Conference on
  Emerging Technology in Modelling and Graphics) 2018 $$ $$ $\textbf{Date of
  Conference}$: 6-7 September, 2018 $$ $$ $\textbf{Location of Conference}$:
  Kolkatta, India
- **Journal**: Advances in Intelligent Systems and Computing book series (AISC,
  volume 937), Springer, Singapore, 2019
- **Summary**: A change detection system takes as input two images of a region captured at two different times, and predicts which pixels in the region have undergone change over the time period. Since pixel-based analysis can be erroneous due to noise, illumination difference and other factors, contextual information is usually used to determine the class of a pixel (changed or not). This contextual information is taken into account by considering a pixel of the difference image along with its neighborhood. With the help of ground truth information, the labeled patterns are generated. Finally, Broad Learning classifier is used to get prediction about the class of each pixel. Results show that Broad Learning can classify the data set with a significantly higher F-Score than that of Multilayer Perceptron. Performance comparison has also been made with other popular classifiers, namely Multilayer Perceptron and Random Forest.



### GAN Based Image Deblurring Using Dark Channel Prior
- **Arxiv ID**: http://arxiv.org/abs/1903.00107v1
- **DOI**: 10.2352/ISSN.2470-1173.2019.13.COIMG-136
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00107v1)
- **Published**: 2019-02-28 23:21:30+00:00
- **Updated**: 2019-02-28 23:21:30+00:00
- **Authors**: Shuang Zhang, Ada Zhen, Robert L. Stevenson
- **Comment**: 5 pages, 3 figures. Conference: Electronic Imaging
- **Journal**: None
- **Summary**: A conditional general adversarial network (GAN) is proposed for image deblurring problem. It is tailored for image deblurring instead of just applying GAN on the deblurring problem. Motivated by that, dark channel prior is carefully picked to be incorporated into the loss function for network training. To make it more compatible with neuron networks, its original indifferentiable form is discarded and L2 norm is adopted instead. On both synthetic datasets and noisy natural images, the proposed network shows improved deblurring performance and robustness to image noise qualitatively and quantitatively. Additionally, compared to the existing end-to-end deblurring networks, our network structure is light-weight, which ensures less training and testing time.



