# Arxiv Papers in cs.CV on 2019-02-25
### A large annotated medical image dataset for the development and evaluation of segmentation algorithms
- **Arxiv ID**: http://arxiv.org/abs/1902.09063v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.09063v1)
- **Published**: 2019-02-25 02:34:48+00:00
- **Updated**: 2019-02-25 02:34:48+00:00
- **Authors**: Amber L. Simpson, Michela Antonelli, Spyridon Bakas, Michel Bilello, Keyvan Farahani, Bram van Ginneken, Annette Kopp-Schneider, Bennett A. Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M. Summers, Patrick Bilic, Patrick F. Christ, Richard K. G. Do, Marc Gollub, Jennifer Golia-Pernicka, Stephan H. Heckers, William R. Jarnagin, Maureen K. McHugo, Sandy Napel, Eugene Vorontsov, Lena Maier-Hein, M. Jorge Cardoso
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.



### SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.09080v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09080v3)
- **Published**: 2019-02-25 04:13:25+00:00
- **Updated**: 2019-06-06 01:28:28+00:00
- **Authors**: Chengju Zhou, Meiqing Wu, Siew-Kei Lam
- **Comment**: wrong setting in CityPersons experiments
- **Journal**: None
- **Summary**: Pedestrian detection plays an important role in many applications such as autonomous driving. We propose a method that explores semantic segmentation results as self-attention cues to significantly improve the pedestrian detection performance. Specifically, a multi-task network is designed to jointly learn semantic segmentation and pedestrian detection from image datasets with weak box-wise annotations. The semantic segmentation feature maps are concatenated with corresponding convolution features maps to provide more discriminative features for pedestrian detection and pedestrian classification. By jointly learning segmentation and detection, our proposed pedestrian self-attention mechanism can effectively identify pedestrian regions and suppress backgrounds. In addition, we propose to incorporate semantic attention information from multi-scale layers into deep convolution neural network to boost pedestrian detection. Experiment results show that the proposed method achieves the best detection performance with MR of 6.27% on Caltech dataset and obtain competitive performance on CityPersons dataset while maintaining high computational efficiency.



### Privacy-Preserving Action Recognition using Coded Aperture Videos
- **Arxiv ID**: http://arxiv.org/abs/1902.09085v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09085v2)
- **Published**: 2019-02-25 04:44:34+00:00
- **Updated**: 2019-04-16 23:40:00+00:00
- **Authors**: Zihao W. Wang, Vibhav Vineet, Francesco Pittaluga, Sudipta Sinha, Oliver Cossairt, Sing Bing Kang
- **Comment**: CVCOPS2019
- **Journal**: None
- **Summary**: The risk of unauthorized remote access of streaming video from networked cameras underlines the need for stronger privacy safeguards. We propose a lens-free coded aperture camera system for human action recognition that is privacy-preserving. While coded aperture systems exist, we believe ours is the first system designed for action recognition without the need for image restoration as an intermediate step. Action recognition is done using a deep network that takes in as input, non-invertible motion features between pairs of frames computed using phase correlation and log-polar transformation. Phase correlation encodes translation while the log polar transformation encodes in-plane rotation and scaling. We show that the translation features are independent of the coded aperture design, as long as its spectral response within the bandwidth has no zeros. Stacking motion features computed on frames at multiple different strides in the video can improve accuracy. Preliminary results on simulated data based on a subset of the UCF and NTU datasets are promising. We also describe our prototype lens-free coded aperture camera system, and results for real captured videos are mixed.



### Generative Models for Low-Rank Video Representation and Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1902.11132v1
- **DOI**: 10.1109/TSP.2020.2977256
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11132v1)
- **Published**: 2019-02-25 05:48:23+00:00
- **Updated**: 2019-02-25 05:48:23+00:00
- **Authors**: Rakib Hyder, M. Salman Asif
- **Comment**: None
- **Journal**: None
- **Summary**: Finding compact representation of videos is an essential component in almost every problem related to video processing or understanding. In this paper, we propose a generative model to learn compact latent codes that can efficiently represent and reconstruct a video sequence from its missing or under-sampled measurements. We use a generative network that is trained to map a compact code into an image. We first demonstrate that if a video sequence belongs to the range of the pretrained generative network, then we can recover it by estimating the underlying compact latent codes. Then we demonstrate that even if the video sequence does not belong to the range of a pretrained network, we can still recover the true video sequence by jointly updating the latent codes and the weights of the generative network. To avoid overfitting in our model, we regularize the recovery problem by imposing low-rank and similarity constraints on the latent codes of the neighboring frames in the video sequence. We use our methods to recover a variety of videos from compressive measurements at different compression rates. We also demonstrate that we can generate missing frames in a video sequence by interpolating the latent codes of the observed frames in the low-dimensional space.



### Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.09103v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.09103v1)
- **Published**: 2019-02-25 06:22:52+00:00
- **Updated**: 2019-02-25 06:22:52+00:00
- **Authors**: Tianwei Shen, Zixin Luo, Lei Zhou, Hanyu Deng, Runze Zhang, Tian Fang, Long Quan
- **Comment**: Accepted by ICRA 2019
- **Journal**: None
- **Summary**: Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised ego-motion estimation methods by a large margin. The code and data are available at https://github.com/hlzz/DeepMatchVO.



### Dynamic Feature Fusion for Semantic Edge Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.09104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09104v1)
- **Published**: 2019-02-25 06:36:13+00:00
- **Updated**: 2019-02-25 06:36:13+00:00
- **Authors**: Yuan Hu, Yunpeng Chen, Xiang Li, Jiashi Feng
- **Comment**: None
- **Journal**: None
- **Summary**: Features from multiple scales can greatly benefit the semantic edge detection task if they are well fused. However, the prevalent semantic edge detection methods apply a fixed weight fusion strategy where images with different semantics are forced to share the same weights, resulting in universal fusion weights for all images and locations regardless of their different semantics or local context. In this work, we propose a novel dynamic feature fusion strategy that assigns different fusion weights for different input images and locations adaptively. This is achieved by a proposed weight learner to infer proper fusion weights over multi-level features for each location of the feature map, conditioned on the specific input. In this way, the heterogeneity in contributions made by different locations of feature maps and input images can be better considered and thus help produce more accurate and sharper edge predictions. We show that our model with the novel dynamic feature fusion is superior to fixed weight fusion and also the na\"ive location-invariant weight fusion methods, via comprehensive experiments on benchmarks Cityscapes and SBD. In particular, our method outperforms all existing well established methods and achieves new state-of-the-art.



### Visualization, Discriminability and Applications of Interpretable Saak Features
- **Arxiv ID**: http://arxiv.org/abs/1902.09107v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09107v3)
- **Published**: 2019-02-25 06:43:49+00:00
- **Updated**: 2019-03-03 05:24:56+00:00
- **Authors**: Abinaya Manimaran, Thiyagarajan Ramanathan, Suya You, C-C Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we study the power of Saak features as an effort towards interpretable deep learning. Being inspired by the operations of convolutional layers of convolutional neural networks, multi-stage Saak transform was proposed. Based on this foundation, we provide an in-depth examination on Saak features, which are coefficients of the Saak transform, by analyzing their properties through visualization and demonstrating their applications in image classification. Being similar to CNN features, Saak features at later stages have larger receptive fields, yet they are obtained in a one-pass feedforward manner without backpropagation. The whole feature extraction process is transparent and is of extremely low complexity. The discriminant power of Saak features is demonstrated, and their classification performance in three well-known datasets (namely, MNIST, CIFAR-10 and STL-10) is shown by experimental results.



### An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.09130v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09130v2)
- **Published**: 2019-02-25 08:08:50+00:00
- **Updated**: 2019-03-29 06:38:38+00:00
- **Authors**: Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, Tieniu Tan
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Skeleton-based action recognition is an important task that requires the adequate understanding of movement characteristics of a human action from the given skeleton sequence. Recent studies have shown that exploring spatial and temporal features of the skeleton sequence is vital for this task. Nevertheless, how to effectively extract discriminative spatial and temporal features is still a challenging problem. In this paper, we propose a novel Attention Enhanced Graph Convolutional LSTM Network (AGC-LSTM) for human action recognition from skeleton data. The proposed AGC-LSTM can not only capture discriminative features in spatial configuration and temporal dynamics but also explore the co-occurrence relationship between spatial and temporal domains. We also present a temporal hierarchical architecture to increases temporal receptive fields of the top AGC-LSTM layer, which boosts the ability to learn the high-level semantic representation and significantly reduces the computation cost. Furthermore, to select discriminative spatial information, the attention mechanism is employed to enhance information of key joints in each AGC-LSTM layer. Experimental results on two datasets are provided: NTU RGB+D dataset and Northwestern-UCLA dataset. The comparison results demonstrate the effectiveness of our approach and show that our approach outperforms the state-of-the-art methods on both datasets.



### A Dual Symmetric Gauss-Seidel Alternating Direction Method of Multipliers for Hyperspectral Sparse Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1902.09135v2
- **DOI**: None
- **Categories**: **cs.NA**, cs.CV, eess.IV, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1902.09135v2)
- **Published**: 2019-02-25 08:28:01+00:00
- **Updated**: 2020-07-11 11:33:28+00:00
- **Authors**: Longfei Ren, Chengjing Wang, Peipei Tang, Zheng Ma
- **Comment**: 30 pages, 6 figures
- **Journal**: None
- **Summary**: Since sparse unmixing has emerged as a promising approach to hyperspectral unmixing, some spatial-contextual information in the hyperspectral images has been exploited to improve the performance of the unmixing recently. The total variation (TV) has been widely used to promote the spatial homogeneity as well as the smoothness between adjacent pixels. However, the computation task for hyperspectral sparse unmixing with a TV regularization term is heavy. Besides, the convergence of the primal alternating direction method of multipliers (ADMM) for the hyperspectral sparse unmixing with a TV regularization term has not been explained in details. In this paper, we design an efficient and convergent dual symmetric Gauss-Seidel ADMM (sGS-ADMM) for hyperspectral sparse unmixing with a TV regularization term. We also present the global convergence and local linear convergence rate analysis for this algorithm. As demonstrated in numerical experiments, our algorithm can obviously improve the efficiency of the unmixing compared with the state-of-the-art algorithm. More importantly, we can obtain images with higher quality.



### DDFlow: Learning Optical Flow with Unlabeled Data Distillation
- **Arxiv ID**: http://arxiv.org/abs/1902.09145v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09145v1)
- **Published**: 2019-02-25 08:58:10+00:00
- **Updated**: 2019-02-25 08:58:10+00:00
- **Authors**: Pengpeng Liu, Irwin King, Michael R. Lyu, Jia Xu
- **Comment**: 8 pages, AAAI 19
- **Journal**: None
- **Summary**: We present DDFlow, a data distillation approach to learning optical flow estimation from unlabeled data. The approach distills reliable predictions from a teacher network, and uses these predictions as annotations to guide a student network to learn optical flow. Unlike existing work relying on hand-crafted energy terms to handle occlusion, our approach is data-driven, and learns optical flow for occluded pixels. This enables us to train our model with a much simpler loss function, and achieve a much higher accuracy. We conduct a rigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012 and 2015 benchmarks, and show that our approach significantly outperforms all existing unsupervised learning methods, while running at real time.



### A Survey of Crowdsourcing in Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.09159v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/1902.09159v2)
- **Published**: 2019-02-25 09:21:09+00:00
- **Updated**: 2019-09-04 12:47:16+00:00
- **Authors**: Silas Ørting, Andrew Doyle, Arno van Hilten, Matthias Hirth, Oana Inel, Christopher R. Madan, Panagiotis Mavridis, Helen Spiers, Veronika Cheplygina
- **Comment**: Submitted to Human Computation
- **Journal**: None
- **Summary**: Rapid advances in image processing capabilities have been seen across many domains, fostered by the application of machine learning algorithms to "big-data". However, within the realm of medical image analysis, advances have been curtailed, in part, due to the limited availability of large-scale, well-annotated datasets. One of the main reasons for this is the high cost often associated with producing large amounts of high-quality meta-data. Recently, there has been growing interest in the application of crowdsourcing for this purpose; a technique that has proven effective for creating large-scale datasets across a range of disciplines, from computer vision to astrophysics. Despite the growing popularity of this approach, there has not yet been a comprehensive literature review to provide guidance to researchers considering using crowdsourcing methodologies in their own medical imaging analysis. In this survey, we review studies applying crowdsourcing to the analysis of medical images, published prior to July 2018. We identify common approaches, challenges and considerations, providing guidance of utility to researchers adopting this approach. Finally, we discuss future opportunities for development within this emerging domain.



### GFCN: A New Graph Convolutional Network Based on Parallel Flows
- **Arxiv ID**: http://arxiv.org/abs/1902.09173v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.09173v4)
- **Published**: 2019-02-25 10:06:15+00:00
- **Updated**: 2020-03-06 09:50:15+00:00
- **Authors**: Feng Ji, Jielong Yang, Qiang Zhang, Wee Peng Tay
- **Comment**: None
- **Journal**: None
- **Summary**: In view of the huge success of convolution neural networks (CNN) for image classification and object recognition, there have been attempts to generalize the method to general graph-structured data. One major direction is based on spectral graph theory and graph signal processing. In this paper, we study the problem from a completely different perspective, by introducing parallel flow decomposition of graphs. The essential idea is to decompose a graph into families of non-intersecting one dimensional (1D) paths, after which, we may apply a 1D CNN along each family of paths. We demonstrate that the our method, which we call GraphFlow, is able to transfer CNN architectures to general graphs. To show the effectiveness of our approach, we test our method on the classical MNIST dataset, synthetic datasets on network information propagation and a news article classification dataset.



### Towards Corner Case Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.09184v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09184v2)
- **Published**: 2019-02-25 10:34:36+00:00
- **Updated**: 2019-02-26 08:18:36+00:00
- **Authors**: Jan-Aike Bolte, Andreas Bär, Daniel Lipinski, Tim Fingscheidt
- **Comment**: None
- **Journal**: None
- **Summary**: The progress in autonomous driving is also due to the increased availability of vast amounts of training data for the underlying machine learning approaches. Machine learning systems are generally known to lack robustness, e.g., if the training data did rarely or not at all cover critical situations. The challenging task of corner case detection in video, which is also somehow related to unusual event or anomaly detection, aims at detecting these unusual situations, which could become critical, and to communicate this to the autonomous driving system (online use case). Such a system, however, could be also used in offline mode to screen vast amounts of data and select only the relevant situations for storing and (re)training machine learning algorithms. So far, the approaches for corner case detection have been limited to videos recorded from a fixed camera, mostly for security surveillance. In this paper, we provide a formal definition of a corner case and propose a system framework for both the online and the offline use case that can handle video signals from front cameras of a naturally moving vehicle and can output a corner case score.



### Deep High-Resolution Representation Learning for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.09212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09212v1)
- **Published**: 2019-02-25 11:55:28+00:00
- **Updated**: 2019-02-25 11:55:28+00:00
- **Authors**: Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang
- **Comment**: accepted by CVPR2019
- **Journal**: None
- **Summary**: This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.



### Bengali Handwritten Character Classification using Transfer Learning on Deep Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1902.11133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11133v1)
- **Published**: 2019-02-25 13:52:53+00:00
- **Updated**: 2019-02-25 13:52:53+00:00
- **Authors**: Swagato Chatterjee, Rwik Kumar Dutta, Debayan Ganguly, Kingshuk Chatterjee, Sudipta Roy
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a solution which uses state-of-the-art techniques in Deep Learning to tackle the problem of Bengali Handwritten Character Recognition ( HCR ). Our method uses lesser iterations to train than most other comparable methods. We employ Transfer Learning on ResNet 50, a state-of-the-art deep Convolutional Neural Network Model, pretrained on ImageNet dataset. We also use other techniques like a modified version of One Cycle Policy, varying the input image sizes etc. to ensure that our training occurs fast. We use the BanglaLekha-Isolated Dataset for evaluation of our technique which consists of 84 classes (50 Basic, 10 Numerals and 24 Compound Characters). We are able to achieve 96.12% accuracy in just 47 epochs on BanglaLekha-Isolated dataset. When comparing our method with that of other researchers, considering number of classes and without using Ensemble Learning, the proposed solution achieves state of the art result for Handwritten Bengali Character Recognition. Code and weight files are available at https://github.com/swagato-c/bangla-hwcr-present.



### Adversarial attacks hidden in plain sight
- **Arxiv ID**: http://arxiv.org/abs/1902.09286v3
- **DOI**: 10.1007/978-3-030-44584-3_19
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.09286v3)
- **Published**: 2019-02-25 14:27:05+00:00
- **Updated**: 2020-04-26 13:45:21+00:00
- **Authors**: Jan Philip Göpfert, André Artelt, Heiko Wersing, Barbara Hammer
- **Comment**: None
- **Journal**: Advances in Intelligent Data Analysis XVIII (2020) Pages 235-247
- **Summary**: Convolutional neural networks have been used to achieve a string of successes during recent years, but their lack of interpretability remains a serious issue. Adversarial examples are designed to deliberately fool neural networks into making any desired incorrect classification, potentially with very high certainty. Several defensive approaches increase robustness against adversarial attacks, demanding attacks of greater magnitude, which lead to visible artifacts. By considering human visual perception, we compose a technique that allows to hide such adversarial attacks in regions of high complexity, such that they are imperceptible even to an astute observer. We carry out a user study on classifying adversarially modified images to validate the perceptual quality of our approach and find significant evidence for its concealment with regards to human visual perception.



### End-to-end Hand Mesh Recovery from a Monocular RGB Image
- **Arxiv ID**: http://arxiv.org/abs/1902.09305v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09305v3)
- **Published**: 2019-02-25 14:47:11+00:00
- **Updated**: 2019-09-07 12:46:32+00:00
- **Authors**: Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, Wen Zheng
- **Comment**: 11 pages;
- **Journal**: None
- **Summary**: In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the problem of reconstructing the full 3D mesh of a human hand from a single RGB image. In contrast to existing research on 2D or 3D hand pose estimation from RGB or/and depth image data, HAMR can provide a more expressive and useful mesh representation for monocular hand image understanding. In particular, the mesh representation is achieved by parameterizing a generic 3D hand model with shape and relative 3D joint angles. By utilizing this mesh representation, we can easily compute the 3D joint locations via linear interpolations between the vertexes of the mesh, while obtain the 2D joint locations with a projection of the 3D joints.To this end, a differentiable re-projection loss can be defined in terms of the derived representations and the ground-truth labels, thus making our framework end-to-end trainable.Qualitative experiments show that our framework is capable of recovering appealing 3D hand mesh even in the presence of severe occlusions.Quantitatively, our approach also outperforms the state-of-the-art methods for both 2D and 3D hand pose estimation from a monocular RGB image on several benchmark datasets.



### Making History Matter: History-Advantage Sequence Training for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1902.09326v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09326v3)
- **Published**: 2019-02-25 14:58:35+00:00
- **Updated**: 2019-04-17 15:09:16+00:00
- **Authors**: Tianhao Yang, Zheng-Jun Zha, Hanwang Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage-a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.



### Dual Attention Networks for Visual Reference Resolution in Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1902.09368v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09368v3)
- **Published**: 2019-02-25 15:32:56+00:00
- **Updated**: 2019-08-29 02:24:23+00:00
- **Authors**: Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang
- **Comment**: EMNLP 2019
- **Journal**: None
- **Summary**: Visual dialog (VisDial) is a task which requires an AI agent to answer a series of questions grounded in an image. Unlike in visual question answering (VQA), the series of questions should be able to capture a temporal context from a dialog history and exploit visually-grounded information. A problem called visual reference resolution involves these challenges, requiring the agent to resolve ambiguous references in a given question and find the references in a given image. In this paper, we propose Dual Attention Networks (DAN) for visual reference resolution. DAN consists of two kinds of attention networks, REFER and FIND. Specifically, REFER module learns latent relationships between a given question and a dialog history by employing a self-attention mechanism. FIND module takes image features and reference-aware representations (i.e., the output of REFER module) as input, and performs visual grounding via bottom-up attention mechanism. We qualitatively and quantitatively evaluate our model on VisDial v1.0 and v0.9 datasets, showing that DAN outperforms the previous state-of-the-art model by a significant margin.



### Data augmentation using learned transformations for one-shot medical image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.09383v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09383v2)
- **Published**: 2019-02-25 15:49:47+00:00
- **Updated**: 2019-04-06 21:49:38+00:00
- **Authors**: Amy Zhao, Guha Balakrishnan, Frédo Durand, John V. Guttag, Adrian V. Dalca
- **Comment**: 9 pages, CVPR 2019
- **Journal**: None
- **Summary**: Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images.   We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation. Our code is available at https://github.com/xamyzhao/brainstorm.



### A Review on Automatic License Plate Recognition System
- **Arxiv ID**: http://arxiv.org/abs/1902.09385v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09385v1)
- **Published**: 2019-02-25 15:51:14+00:00
- **Updated**: 2019-02-25 15:51:14+00:00
- **Authors**: Satadal Saha
- **Comment**: In Proceedings of Students' Article Competition
- **Journal**: None
- **Summary**: Automatic License Plate Recognition (ALPR) is a challenging problem to the research community due to its potential applicability in the diverse geographical condition over the globe with varying license plate parameters. Any ALPR system includes three main modules, viz. localization of the license plate, segmentation of the characters therein and recognition of the segmented characters. In real life applications where the images are captured over days and nights in an outdoor environment with varying lighting and weather conditions, varying pollution level and wind turbulences, localization, segmentation and recognition become challenging tasks. The tasks become more complex if the license plate is not in conformity with the standards laid by corresponding Motor Vehicles Department in terms of various features, e.g. area and aspect ratio of the license plate, background color, foreground color, shape, number of lines, font face/ size of characters, spacing between characters etc. Besides, license plates are often dirty or broken or having scratches or bent or tilted at its position. All these add to the challenges in developing an effective ALPR system.



### MUREL: Multimodal Relational Reasoning for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1902.09487v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09487v1)
- **Published**: 2019-02-25 18:04:05+00:00
- **Updated**: 2019-02-25 18:04:05+00:00
- **Authors**: Remi Cadene, Hedi Ben-younes, Matthieu Cord, Nicolas Thome
- **Comment**: CVPR2019 accepted paper
- **Journal**: None
- **Summary**: Multimodal attentional networks are currently state-of-the-art models for Visual Question Answering (VQA) tasks involving real images. Although attention allows to focus on the visual content relevant to the question, this simple mechanism is arguably insufficient to model complex reasoning features required for VQA or other high-level tasks.   In this paper, we propose MuRel, a multimodal relational network which is learned end-to-end to reason over real images. Our first contribution is the introduction of the MuRel cell, an atomic reasoning primitive representing interactions between question and image regions by a rich vectorial representation, and modeling region relations with pairwise combinations. Secondly, we incorporate the cell into a full MuRel network, which progressively refines visual and question interactions, and can be leveraged to define visualization schemes finer than mere attention maps.   We validate the relevance of our approach with various ablation studies, and show its superiority to attention-based methods on three datasets: VQA 2.0, VQA-CP v2 and TDIUC. Our final MuRel network is competitive to or outperforms state-of-the-art results in this challenging context.   Our code is available: https://github.com/Cadene/murel.bootstrap.pytorch



### GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1902.09506v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09506v3)
- **Published**: 2019-02-25 18:37:49+00:00
- **Updated**: 2019-05-10 22:24:55+00:00
- **Authors**: Drew A. Hudson, Christopher D. Manning
- **Comment**: Published as a conference paper at CVPR 2019 (oral)
- **Journal**: None
- **Summary**: We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.



### Using Deep Object Features for Image Descriptions
- **Arxiv ID**: http://arxiv.org/abs/1902.09969v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09969v1)
- **Published**: 2019-02-25 18:40:25+00:00
- **Updated**: 2019-02-25 18:40:25+00:00
- **Authors**: Ashutosh Mishra, Marcus Liwicki
- **Comment**: arXiv admin note: text overlap with arXiv:1411.2539, arXiv:1609.06647
  by other authors
- **Journal**: None
- **Summary**: Inspired by recent advances in leveraging multiple modalities in machine translation, we introduce an encoder-decoder pipeline that uses (1) specific objects within an image and their object labels, (2) a language model for decoding joint embedding of object features and the object labels. Our pipeline merges prior detected objects from the image and their object labels and then learns the sequences of captions describing the particular image. The decoder model learns to extract descriptions for the image from scratch by decoding the joint representation of the object visual features and their object classes conditioned by the encoder component. The idea of the model is to concentrate only on the specific objects of the image and their labels for generating descriptions of the image rather than visual feature of the entire image. The model needs to be calibrated more by adjusting the parameters and settings to result in better accuracy and performance.



### FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.09513v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09513v2)
- **Published**: 2019-02-25 18:50:40+00:00
- **Updated**: 2019-04-08 13:50:21+00:00
- **Authors**: Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen
- **Comment**: CVPR 2019 camera-ready version
- **Journal**: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2019
- **Summary**: Many of the recent successful methods for video object segmentation (VOS) are overly complicated, heavily rely on fine-tuning on the first frame, and/or are slow, and are hence of limited practical use. In this work, we propose FEELVOS as a simple and fast method which does not rely on fine-tuning. In order to segment a video, for each frame FEELVOS uses a semantic pixel-wise embedding together with a global and a local matching mechanism to transfer information from the first frame and from the previous frame of the video to the current frame. In contrast to previous work, our embedding is only used as an internal guidance of a convolutional network. Our novel dynamic segmentation head allows us to train the network, including the embedding, end-to-end for the multiple object segmentation task with a cross entropy loss. We achieve a new state of the art in video object segmentation without fine-tuning with a J&F measure of 71.5% on the DAVIS 2017 validation set. We make our code and models available at https://github.com/tensorflow/models/tree/master/research/feelvos.



### Condition-Invariant Multi-View Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.09516v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09516v1)
- **Published**: 2019-02-25 18:56:55+00:00
- **Updated**: 2019-02-25 18:56:55+00:00
- **Authors**: Jose M. Facil, Daniel Olid, Luis Montesano, Javier Civera
- **Comment**: Project website: http://webdiis.unizar.es/~jmfacil/cimvpr/ In
  submission
- **Journal**: None
- **Summary**: Visual place recognition is particularly challenging when places suffer changes in its appearance. Such changes are indeed common, e.g., due to weather, night/day or seasons. In this paper we leverage on recent research using deep networks, and explore how they can be improved by exploiting the temporal sequence information. Specifically, we propose 3 different alternatives (Descriptor Grouping, Fusion and Recurrent Descriptors) for deep networks to use several frames of a sequence. We show that our approaches produce more compact and best performing descriptors than single- and multi-view baselines in the literature in two public databases.



### Quantifying error contributions of computational steps, algorithms and hyperparameter choices in image classification pipelines
- **Arxiv ID**: http://arxiv.org/abs/1903.02521v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.02521v1)
- **Published**: 2019-02-25 19:16:58+00:00
- **Updated**: 2019-02-25 19:16:58+00:00
- **Authors**: Aritra Chowdhury, Malik Magdin-Ismail, Bulent Yener
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1903.00405
- **Journal**: None
- **Summary**: Data science relies on pipelines that are organized in the form of interdependent computational steps. Each step consists of various candidate algorithms that maybe used for performing a particular function. Each algorithm consists of several hyperparameters. Algorithms and hyperparameters must be optimized as a whole to produce the best performance. Typical machine learning pipelines typically consist of complex algorithms in each of the steps. Not only is the selection process combinatorial, but it is also important to interpret and understand the pipelines. We propose a method to quantify the importance of different layers in the pipeline, by computing an error contribution relative to an agnostic choice of algorithms in that layer. We demonstrate our methodology on image classification pipelines. The agnostic methodology quantifies the error contributions from the computational steps, algorithms and hyperparameters in the image classification pipeline. We show that algorithm selection and hyper-parameter optimization methods can be used to quantify the error contribution and that random search is able to quantify the contribution more accurately than Bayesian optimization. This methodology can be used by domain experts to understand machine learning and data analysis pipelines in terms of their individual components, which can help in prioritizing different components of the pipeline.



### A detailed comparative study of open source deep learning frameworks
- **Arxiv ID**: http://arxiv.org/abs/1903.00102v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1903.00102v2)
- **Published**: 2019-02-25 20:10:54+00:00
- **Updated**: 2020-05-06 11:57:54+00:00
- **Authors**: Ghadeer Al-Bdour, Raffi Al-Qurran, Mahmoud Al-Ayyoub, Ali Shatnawi
- **Comment**: 26 pages, 25 figures, 4 tables
- **Journal**: None
- **Summary**: Deep Learning (DL) is one of the hottest trends in machine learning as DL approaches produced results superior to the state-of-the-art in problematic areas such as image processing and natural language processing (NLP). To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three of the most popular and most comprehensive DL frameworks (namely Google's TensorFlow, University of Montreal's Theano and Microsoft's CNTK). The ultimate goal of this work is to help end users make an informed decision about the best DL framework that suits their needs and resources. To ensure that our study is as comprehensive as possible, we conduct several experiments using multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.



### Unsupervised learning-based long-term superpixel tracking
- **Arxiv ID**: http://arxiv.org/abs/1902.09596v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09596v1)
- **Published**: 2019-02-25 20:11:12+00:00
- **Updated**: 2019-02-25 20:11:12+00:00
- **Authors**: Pierre-Henri Conze, Florian Tilquin, Mathieu Lamard, Fabrice Heitz, Gwenolé Quellec
- **Comment**: None
- **Journal**: None
- **Summary**: Finding correspondences between structural entities decomposing images is of high interest for computer vision applications. In particular, we analyze how to accurately track superpixels - visual primitives generated by aggregating adjacent pixels sharing similar characteristics - over extended time periods relying on unsupervised learning and temporal integration. A two-step video processing pipeline dedicated to long-term superpixel tracking is proposed. First, unsupervised learning-based superpixel matching provides correspondences between consecutive and distant frames using new context-rich features extended from greyscale to multi-channel and forward-backward consistency contraints. Resulting elementary matches are then combined along multi-step paths running through the whole sequence with various inter-frame distances. This produces a large set of candidate long-term superpixel pairings upon which majority voting is performed. Video object tracking experiments demonstrate the accuracy of our elementary estimator against state-of-the-art methods and proves the ability of multi-step integration to provide accurate long-term superpixel matches compared to usual direct and sequential integration.



### Convolutional Neural Networks for Automatic Meter Reading
- **Arxiv ID**: http://arxiv.org/abs/1902.09600v1
- **DOI**: 10.1117/1.JEI.28.1.013023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09600v1)
- **Published**: 2019-02-25 20:32:42+00:00
- **Updated**: 2019-02-25 20:32:42+00:00
- **Authors**: Rayson Laroca, Victor Barroso, Matheus A. Diniz, Gabriel R. Gonçalves, William Robson Schwartz, David Menotti
- **Comment**: None
- **Journal**: Journal of Electronic Imaging 28(1), 013023 (5 February 2019)
- **Summary**: In this paper, we tackle Automatic Meter Reading (AMR) by leveraging the high capability of Convolutional Neural Networks (CNNs). We design a two-stage approach that employs the Fast-YOLO object detector for counter detection and evaluates three different CNN-based approaches for counter recognition. In the AMR literature, most datasets are not available to the research community since the images belong to a service company. In this sense, we introduce a new public dataset, called UFPR-AMR dataset, with 2,000 fully and manually annotated images. This dataset is, to the best of our knowledge, three times larger than the largest public dataset found in the literature and contains a well-defined evaluation protocol to assist the development and evaluation of AMR methods. Furthermore, we propose the use of a data augmentation technique to generate a balanced training set with many more examples to train the CNN models for counter recognition. In the proposed dataset, impressive results were obtained and a detailed speed/accuracy trade-off evaluation of each model was performed. In a public dataset, state-of-the-art results were achieved using less than 200 images for training.



### Deep Learning for Low-Dose CT Denoising
- **Arxiv ID**: http://arxiv.org/abs/1902.10127v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10127v1)
- **Published**: 2019-02-25 21:14:45+00:00
- **Updated**: 2019-02-25 21:14:45+00:00
- **Authors**: Maryam Gholizadeh-Ansari, Javad Alirezaie, Paul Babyn
- **Comment**: None
- **Journal**: None
- **Summary**: Low-dose CT denoising is a challenging task that has been studied by many researchers. Some studies have used deep neural networks to improve the quality of low-dose CT images and achieved fruitful results. In this paper, we propose a deep neural network that uses dilated convolutions with different dilation rates instead of standard convolution helping to capture more contextual information in fewer layers. Also, we have employed residual learning by creating shortcut connections to transmit image information from the early layers to later ones. To further improve the performance of the network, we have introduced a non-trainable edge detection layer that extracts edges in horizontal, vertical, and diagonal directions. Finally, we demonstrate that optimizing the network by a combination of mean-square error loss and perceptual loss preserves many structural details in the CT image. This objective function does not suffer from over smoothing and blurring effects caused by per-pixel loss and grid-like artifacts resulting from perceptual loss. The experiments show that each modification to the network improves the outcome while only minimally changing the complexity of the network.



### Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression
- **Arxiv ID**: http://arxiv.org/abs/1902.09630v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09630v2)
- **Published**: 2019-02-25 21:47:33+00:00
- **Updated**: 2019-04-15 03:18:03+00:00
- **Authors**: Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, Silvio Savarese
- **Comment**: accepted in CVPR 2019
- **Journal**: None
- **Summary**: Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that $IoU$ can be directly used as a regression loss. However, $IoU$ has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the weaknesses of $IoU$ by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, $IoU$ based, and new, $GIoU$ based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.



### TraVeLGAN: Image-to-image Translation by Transformation Vector Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.09631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09631v1)
- **Published**: 2019-02-25 21:48:32+00:00
- **Updated**: 2019-02-25 21:48:32+00:00
- **Authors**: Matthew Amodio, Smita Krishnaswamy
- **Comment**: None
- **Journal**: None
- **Summary**: Interest in image-to-image translation has grown substantially in recent years with the success of unsupervised models based on the cycle-consistency assumption. The achievements of these models have been limited to a particular subset of domains where this assumption yields good results, namely homogeneous domains that are characterized by style or texture differences. We tackle the challenging problem of image-to-image translation where the domains are defined by high-level shapes and contexts, as well as including significant clutter and heterogeneity. For this purpose, we introduce a novel GAN based on preserving intra-domain vector transformations in a latent space learned by a siamese network. The traditional GAN system introduced a discriminator network to guide the generator into generating images in the target domain. To this two-network system we add a third: a siamese network that guides the generator so that each original image shares semantics with its generated version. With this new three-network system, we no longer need to constrain the generators with the ubiquitous cycle-consistency restraint. As a result, the generators can learn mappings between more complex domains that differ from each other by large differences - not just style or texture.



### Stochastic Prediction of Multi-Agent Interactions from Partial Observations
- **Arxiv ID**: http://arxiv.org/abs/1902.09641v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.09641v1)
- **Published**: 2019-02-25 22:17:34+00:00
- **Updated**: 2019-02-25 22:17:34+00:00
- **Authors**: Chen Sun, Per Karlsson, Jiajun Wu, Joshua B Tenenbaum, Kevin Murphy
- **Comment**: ICLR 2019 camera ready
- **Journal**: None
- **Summary**: We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.



### Detecting Lesion Bounding Ellipses With Gaussian Proposal Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.09658v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09658v1)
- **Published**: 2019-02-25 23:17:41+00:00
- **Updated**: 2019-02-25 23:17:41+00:00
- **Authors**: Yi Li
- **Comment**: None
- **Journal**: None
- **Summary**: Lesions characterized by computed tomography (CT) scans, are arguably often elliptical objects. However, current lesion detection systems are predominantly adopted from the popular Region Proposal Networks (RPNs) that only propose bounding boxes without fully leveraging the elliptical geometry of lesions. In this paper, we present Gaussian Proposal Networks (GPNs), a novel extension to RPNs, to detect lesion bounding ellipses. Instead of directly regressing the rotation angle of the ellipse as the common practice, GPN represents bounding ellipses as 2D Gaussian distributions on the image plain and minimizes the Kullback-Leibler (KL) divergence between the proposed Gaussian and the ground truth Gaussian for object localization. We show the KL divergence loss approximately incarnates the regression loss in the RPN framework when the rotation angle is 0. Experiments on the DeepLesion dataset show that GPN significantly outperforms RPN for lesion bounding ellipse detection thanks to lower localization error. GPN is open sourced at https://github.com/baidu-research/GPN



