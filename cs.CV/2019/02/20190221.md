# Arxiv Papers in cs.CV on 2019-02-21
### Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd Domains
- **Arxiv ID**: http://arxiv.org/abs/1902.08192v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1902.08192v1)
- **Published**: 2019-02-21 01:03:04+00:00
- **Updated**: 2019-02-21 01:03:04+00:00
- **Authors**: Yoojin Choi, Mostafa El-Khamy, Jungwon Lee
- **Comment**: IEEE ICASSP 2019. arXiv admin note: substantial text overlap with
  arXiv:1805.08303
- **Journal**: None
- **Summary**: We consider the optimization of deep convolutional neural networks (CNNs) such that they provide good performance while having reduced complexity if deployed on either conventional systems with spatial-domain convolution or lower-complexity systems designed for Winograd convolution. The proposed framework produces one compressed model whose convolutional filters can be made sparse either in the spatial domain or in the Winograd domain. Hence, the compressed model can be deployed universally on any platform, without need for re-training on the deployed platform. To get a better compression ratio, the sparse model is compressed in the spatial domain that has a fewer number of parameters. From our experiments, we obtain $24.2\times$ and $47.7\times$ compressed models for ResNet-18 and AlexNet trained on the ImageNet dataset, while their computational cost is also reduced by $4.5\times$ and $5.1\times$, respectively.



### Cascade Feature Aggregation for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.07837v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07837v3)
- **Published**: 2019-02-21 01:34:27+00:00
- **Updated**: 2019-05-20 14:38:37+00:00
- **Authors**: Zhihui Su, Ming Ye, Guohui Zhang, Lei Dai, Jianda Sheng
- **Comment**: None
- **Journal**: None
- **Summary**: Human pose estimation plays an important role in many computer vision tasks and has been studied for many decades. However, due to complex appearance variations from poses, illuminations, occlusions and low resolutions, it still remains a challenging problem. Taking the advantage of high-level semantic information from deep convolutional neural networks is an effective way to improve the accuracy of human pose estimation. In this paper, we propose a novel Cascade Feature Aggregation (CFA) method, which cascades several hourglass networks for robust human pose estimation. Features from different stages are aggregated to obtain abundant contextual information, leading to robustness to poses, partial occlusions and low resolution. Moreover, results from different stages are fused to further improve the localization accuracy. The extensive experiments on MPII datasets and LIP datasets demonstrate that our proposed CFA outperforms the state-of-the-art and achieves the best performance on the state-of-the-art benchmark MPII.



### Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1902.07864v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.07864v2)
- **Published**: 2019-02-21 04:55:56+00:00
- **Updated**: 2019-06-27 22:12:00+00:00
- **Authors**: Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, Devi Parikh
- **Comment**: ICML 2019 Camera Ready + Appendix
- **Journal**: None
- **Summary**: We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring lesser number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.



### Atrial Scar Quantification via Multi-scale CNN in the Graph-cuts Framework
- **Arxiv ID**: http://arxiv.org/abs/1902.07877v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07877v1)
- **Published**: 2019-02-21 06:13:35+00:00
- **Updated**: 2019-02-21 06:13:35+00:00
- **Authors**: Lei Li, Fuping Wu, Guang Yang, Lingchao Xu, Tom Wong, Raad Mohiaddin, David Firmin, Jennifer Keegan, Xiahai Zhuang
- **Comment**: 13 pages, 10 figures, submitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Late gadolinium enhancement magnetic resonance imaging (LGE MRI) appears to be a promising alternative for scar assessment in patients with atrial fibrillation (AF). Automating the quantification and analysis of atrial scars can be challenging due to the low image quality. In this work, we propose a fully automated method based on the graph-cuts framework, where the potentials of the graph are learned on a surface mesh of the left atrium (LA) using a multi-scale convolutional neural network (MS-CNN). For validation, we have employed fifty-eight images with manual delineations. MS-CNN, which can efficiently incorporate both the local and global texture information of the images, has been shown to evidently improve the segmentation accuracy of the proposed graph-cuts based method. The segmentation could be further improved when the contribution between the t-link and n-link weights of the graph is balanced. The proposed method achieves a mean accuracy of 0.856 +- 0.033 and mean Dice score of 0.702 +- 0.071 for LA scar quantification. Compared with the conventional methods, which are based on the manual delineation of LA for initialization, our method is fully automatic and has demonstrated significantly better Dice score and accuracy (p < 0.01). The method is promising and can be useful in diagnosis and prognosis of AF.



### Evaluation of Algorithms for Multi-Modality Whole Heart Segmentation: An Open-Access Grand Challenge
- **Arxiv ID**: http://arxiv.org/abs/1902.07880v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07880v1)
- **Published**: 2019-02-21 06:18:30+00:00
- **Updated**: 2019-02-21 06:18:30+00:00
- **Authors**: Xiahai Zhuang, Lei Li, Christian Payer, Darko Stern, Martin Urschler, Mattias P. Heinrich, Julien Oster, Chunliang Wang, Orjan Smedby, Cheng Bian, Xin Yang, Pheng-Ann Heng, Aliasghar Mortazi, Ulas Bagci, Guanyu Yang, Chenchen Sun, Gaetan Galisot, Jean-Yves Ramel, Thierry Brouard, Qianqian Tong, Weixin Si, Xiangyun Liao, Guodong Zeng, Zenglin Shi, Guoyan Zheng, Chengjia Wang, Tom MacGillivray, David Newby, Kawal Rhode, Sebastien Ourselin, Raad Mohiaddin, Jennifer Keegan, David Firmin, Guang Yang
- **Comment**: 14 pages, 7 figures, sumitted to Medical Image Analysis
- **Journal**: None
- **Summary**: Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be arduous due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, a set of training data is generally needed for constructing priors or for training. In addition, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provides 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results show that many of the deep learning (DL) based methods achieved high accuracy, even though the number of training datasets was limited. A number of them also reported poor results in the blinded evaluation, probably due to overfitting in their training. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated robust and stable performance, even though the accuracy is not as good as the best DL method in CT segmentation. The challenge, including the provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage (\url{www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/}).



### Towards Real-time Eyeblink Detection in The Wild:Dataset,Theory and Practices
- **Arxiv ID**: http://arxiv.org/abs/1902.07891v3
- **DOI**: 10.1109/TIFS.2019.29599778
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07891v3)
- **Published**: 2019-02-21 07:15:19+00:00
- **Updated**: 2019-12-18 12:10:35+00:00
- **Authors**: Guilei Hu, Yang Xiao, Zhiguo Cao, Lubin Meng, Zhiwen Fang, Joey Tianyi Zhou, Junsong Yuan
- **Comment**: None
- **Journal**: IEEE Transactions on Information Forensics and Security 2019
- **Summary**: Effective and real-time eyeblink detection is of wide-range applications, such as deception detection, drive fatigue detection, face anti-spoofing, etc. Although numerous of efforts have already been paid, most of them focus on addressing the eyeblink detection problem under the constrained indoor conditions with the relative consistent subject and environment setup. Nevertheless, towards the practical applications eyeblink detection in the wild is more required, and of greater challenges. However, to our knowledge this has not been well studied before. In this paper, we shed the light to this research topic. A labelled eyeblink in the wild dataset (i.e., HUST-LEBW) of 673 eyeblink video samples (i.e., 381 positives, and 292 negatives) is first established by us. These samples are captured from the unconstrained movies, with the dramatic variation on human attribute, human pose, illumination condition, imaging configuration, etc. Then, we formulate eyeblink detection task as a spatial-temporal pattern recognition problem. After locating and tracking human eye using SeetaFace engine and KCF tracker respectively, a modified LSTM model able to capture the multi-scale temporal information is proposed to execute eyeblink verification. A feature extraction approach that reveals appearance and motion characteristics simultaneously is also proposed. The experiments on HUST-LEBW reveal the superiority and efficiency of our approach. It also verifies that, the existing eyeblink detection methods cannot achieve satisfactory performance in the wild.



### Long-Bone Fracture Detection using Artificial Neural Networks based on Contour Features of X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/1902.07897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07897v1)
- **Published**: 2019-02-21 07:48:10+00:00
- **Updated**: 2019-02-21 07:48:10+00:00
- **Authors**: Alice Yi Yang, Ling Cheng
- **Comment**: None
- **Journal**: None
- **Summary**: The following paper proposes two contour-based fracture detection schemes. The development of the contour-based fracture is based on the line-based fracture detection schemes proposed in arXiv:1902.07458. Existing Computer Aided Diagnosis (CAD) systems commonly employs Convolutional Neural Networks (CNN), although the cost to obtain a high accuracy is the amount of training data required. The purpose of the proposed schemes is to obtain a high classification accuracy with a reduced number of training data through the use of detected contours in X-ray images. There are two contour-based fracture detection schemes. The first is the Standard Contour Histogram Feature-Based (CHFB) and the second is the improved CHFB scheme. The difference between the two schemes is the removal of the surrounding detected flesh contours from the leg region in the improved CHFB scheme. The flesh contours are automatically classified as non-fractures. The contours are further refined to give a precise representation of the image edge objects. A total of 19 features are extracted from each refined contour. 8 out of the 19 features are based on the number of occurrences for particular detected gradients in the contour. Moreover, the occurrence of the 0-degree gradient in the contours are employed for the separation of the knee, leg and foot region. The features are a summary representation of the contour, in which it is used as inputs into the Artificial Neural Network (ANN). Both Standard CHFB and improved CHFB schemes are evaluated with the same experimental set-ups. The average system accuracy for the Standard CHFB scheme is 80.7%, whilst the improved CHFB scheme has an average accuracy of 82.98%. Additionally, the hierarchical clustering technique is adopted to highlight the fractured region within the X-ray image, using extracted 0-degree gradients from fractured contours.



### ComplexFace: a Multi-Representation Approach for Image Classification with Small Dataset
- **Arxiv ID**: http://arxiv.org/abs/1902.07902v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07902v2)
- **Published**: 2019-02-21 07:52:56+00:00
- **Updated**: 2020-12-28 11:26:48+00:00
- **Authors**: Guiying Zhang, Yuxin Cui, Yong Zhao, Jianjun Hu
- **Comment**: This paper includes 12 pages,6 figures and 5 tables
- **Journal**: None
- **Summary**: State-of-the-art face recognition algorithms are able to achieve good performance when sufficient training images are provided. Unfortunately, the number of facial images is limited in some real face recognition applications. In this paper, we propose ComplexFace, a novel and effective algorithm for face recognition with limited samples using complex number based data augmentation. The algorithm first generates new representations from original samples and then fuse both into complex numbers, which avoids the difficulty of weight setting in other fusion approaches. A test sample can then be expressed by the linear combination of all the training samples, which mapped the sample to the new representation space for classification by the kernel function. The collaborative representation based classifier is then built to make predictions. Extensive experiments on the Georgia Tech (GT) face database and the ORL face database show that our algorithm significantly outperforms existing methods: the average errors of previous approaches ranging from 31.66% to 41.75% are reduced to 14.54% over the GT database; the average errors of previous approaches ranging from 5.21% to 10.99% are reduced to 1.67% over the ORL database. In other words, our algorithm has decreased the average errors by up to 84.80% on the ORL database.



### Deep Discriminative Representation Learning with Attention Map for Scene Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.07967v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.07967v1)
- **Published**: 2019-02-21 11:09:18+00:00
- **Updated**: 2019-02-21 11:09:18+00:00
- **Authors**: Jun Li, Daoyu Lin, Yang Wang, Guangluan Xu, Chibiao Ding
- **Comment**: None
- **Journal**: None
- **Summary**: Learning powerful discriminative features for remote sensing image scene classification is a challenging computer vision problem. In the past, most classification approaches were based on handcrafted features. However, most recent approaches to remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is only to use original RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show class activation map (CAM) encoded CNN models, codenamed DDRL-AM, trained using original RGB patches and attention map based class information provide complementary information to the standard RGB deep models. To the best of our knowledge, we are the first to investigate attention information encoded CNNs. Additionally, to enhance the discriminability, we further employ a recently developed object function called "center loss," which has proved to be very useful in face recognition. Finally, our framework provides attention guidance to the model in an end-to-end fashion. Extensive experiments on two benchmark datasets show that our approach matches or exceeds the performance of other methods.



### A Joint Deep Learning Approach for Automated Liver and Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.07971v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1902.07971v2)
- **Published**: 2019-02-21 11:20:11+00:00
- **Updated**: 2020-03-13 12:48:16+00:00
- **Authors**: Nadja Gruber, Stephan Antholzer, Werner Jaschke, Christian Kremser, Markus Haltmeier
- **Comment**: To appear in the SAMPTA 2019 proceedings
- **Journal**: None
- **Summary**: Hepatocellular carcinoma (HCC) is the most common type of primary liver cancer in adults, and the most common cause of death of people suffering from cirrhosis. The segmentation of liver lesions in CT images allows assessment of tumor load, treatment planning, prognosis and monitoring of treatment response. Manual segmentation is a very time-consuming task and in many cases, prone to inaccuracies and automatic tools for tumor detection and segmentation are desirable. In this paper, we compare two network architectures, one that is composed of one neural network and manages the segmentation task in one step and one that consists of two consecutive fully convolutional neural networks. The first network segments the liver whereas the second network segments the actual tumor inside the liver. Our networks are trained on a subset of the LiTS (Liver Tumor Segmentation) Challenge and evaluated on data.



### Multiple-image encryption and hiding with an optical diffractive neural network
- **Arxiv ID**: http://arxiv.org/abs/1902.07985v2
- **DOI**: 10.1016/j.optcom.2020.125476
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.07985v2)
- **Published**: 2019-02-21 11:55:09+00:00
- **Updated**: 2020-02-10 23:35:13+00:00
- **Authors**: Yang Gao, Shuming Jiao, Juncheng Fang, Ting Lei, Zhenwei Xie, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: A cascaded phase-only mask architecture (or an optical diffractive neural network) can be employed for different optical information processing tasks such as pattern recognition, orbital angular momentum (OAM) mode conversion, image salience detection and image encryption. However, for optical encryption and watermarking applications, such a system usually cannot process multiple pairs of input images and output images simultaneously. In our proposed scheme, multiple input images can be simultaneously fed to an optical diffractive neural network (DNN) system and each corresponding output image will be displayed in a non-overlap sub-region in the output imaging plane. Each input image undergoes a different optical transform in an independent channel within the same system. The multiple cascaded phase masks in the system can be effectively optimized by a wavefront matching algorithm. Similar to recent optical pattern recognition and mode conversion works, the orthogonality property is employed to design a multiplexed DNN.



### Learning representations of irregular particle-detector geometry with distance-weighted graph networks
- **Arxiv ID**: http://arxiv.org/abs/1902.07987v2
- **DOI**: 10.1140/epjc/s10052-019-7113-9
- **Categories**: **physics.data-an**, cs.CV, cs.LG, hep-ex, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.07987v2)
- **Published**: 2019-02-21 11:57:12+00:00
- **Updated**: 2019-07-24 15:43:05+00:00
- **Authors**: Shah Rukh Qasim, Jan Kieseler, Yutaro Iiyama, Maurizio Pierini
- **Comment**: 10p, v2: published version
- **Journal**: Eur. Phys. J. C, 79 7 (2019) 608
- **Summary**: We explore the use of graph networks to deal with irregular-geometry detectors in the context of particle reconstruction. Thanks to their representation-learning capabilities, graph networks can exploit the full detector granularity, while natively managing the event sparsity and arbitrarily complex detector geometries. We introduce two distance-weighted graph network architectures, dubbed GarNet and GravNet layers, and apply them to a typical particle reconstruction task. The performance of the new architectures is evaluated on a data set of simulated particle interactions on a toy model of a highly granular calorimeter, loosely inspired by the endcap calorimeter to be installed in the CMS detector for the High-Luminosity LHC phase. We study the clustering of energy depositions, which is the basis for calorimetric particle reconstruction, and provide a quantitative comparison to alternative approaches. The proposed algorithms provide an interesting alternative to existing methods, offering equally performing or less resource-demanding solutions with less underlying assumptions on the detector geometry and, consequently, the possibility to generalize to other detectors.



### GSLAM: A General SLAM Framework and Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1902.07995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.07995v1)
- **Published**: 2019-02-21 12:10:28+00:00
- **Updated**: 2019-02-21 12:10:28+00:00
- **Authors**: Yong Zhao, Shibiao Xu, Shuhui Bu, Hongkai Jiang, Pengcheng Han
- **Comment**: None
- **Journal**: None
- **Summary**: SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their own SLAM systems. The core contribution of GSLAM is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM.



### CMR motion artifact correction using generative adversarial nets
- **Arxiv ID**: http://arxiv.org/abs/1902.11121v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.11121v1)
- **Published**: 2019-02-21 12:39:23+00:00
- **Updated**: 2019-02-21 12:39:23+00:00
- **Authors**: Yunxuan Zhang, Weiliang Zhang, Qinyan Zhang, Jijiang Yang, Xiuyu Chen, Shihua Zhao
- **Comment**: 6 pages, 7 figures
- **Journal**: None
- **Summary**: Cardiovascular Magnetic Resonance (CMR) plays an important role in the diagnoses and treatment of cardiovascular diseases while motion artifacts which are formed during the scanning process of CMR seriously affects doctors to find the exact focus. The current correction methods mainly focus on the K-space which is a grid of raw data obtained from the MR signal directly and then transfer to CMR image by inverse Fourier transform. They are neither effective nor efficient and can not be utilized in clinic. In this paper, we propose a novel approach for CMR motion artifact correction using deep learning. Specially, we use deep residual network (ResNet) as net framework and train our model in adversarial manner. Our approach is motivated by the connection between image motion blur and CMR motion artifact, so we can transfer methods from motion-deblur where deep learning has made great progress to CMR motion-correction successfully. To evaluate motion artifact correction methods, we propose a novel algorithm on how edge detection results are improved by deblurred algorithm. Boosted by deep learning and adversarial training algorithm, our model is trainable in an end-to-end manner, can be tested in real-time and achieves the state-of-art results for CMR correction.



### Cloud-Based Autonomous Indoor Navigation: A Case Study
- **Arxiv ID**: http://arxiv.org/abs/1902.08052v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.08052v1)
- **Published**: 2019-02-21 13:56:31+00:00
- **Updated**: 2019-02-21 13:56:31+00:00
- **Authors**: Uthman Baroudi, M. Alharbi, K. Alhouty, H. Baafeef, K. Alofi
- **Comment**: None
- **Journal**: None
- **Summary**: In this case study, we design, integrate and implement a cloud-enabled autonomous robotic navigation system. The system has the following features: map generation and robot coordination via cloud service and video streaming to allow online monitoring and control in case of emergency. The system has been tested to generate a map for a long corridor using two modes: manual and autonomous. The autonomous mode has shown more accurate map. In addition, the field experiments confirm the benefit of offloading the heavy computation to the cloud by significantly shortening the time required to build the map.



### Quantifying contribution and propagation of error from computational steps, algorithms and hyperparameter choices in image classification pipelines
- **Arxiv ID**: http://arxiv.org/abs/1903.00405v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1903.00405v1)
- **Published**: 2019-02-21 14:42:52+00:00
- **Updated**: 2019-02-21 14:42:52+00:00
- **Authors**: Aritra Chowdhury, Malik Magdon-Ismail, Bulent Yener
- **Comment**: None
- **Journal**: None
- **Summary**: Data science relies on pipelines that are organized in the form of interdependent computational steps. Each step consists of various candidate algorithms that maybe used for performing a particular function. Each algorithm consists of several hyperparameters. Algorithms and hyperparameters must be optimized as a whole to produce the best performance. Typical machine learning pipelines consist of complex algorithms in each of the steps. Not only is the selection process combinatorial, but it is also important to interpret and understand the pipelines. We propose a method to quantify the importance of different components in the pipeline, by computing an error contribution relative to an agnostic choice of computational steps, algorithms and hyperparameters. We also propose a methodology to quantify the propagation of error from individual components of the pipeline with the help of a naive set of benchmark algorithms not involved in the pipeline. We demonstrate our methodology on image classification pipelines. The agnostic and naive methodologies quantify the error contribution and propagation respectively from the computational steps, algorithms and hyperparameters in the image classification pipeline. We show that algorithm selection and hyperparameter optimization methods like grid search, random search and Bayesian optimization can be used to quantify the error contribution and propagation, and that random search is able to quantify them more accurately than Bayesian optimization. This methodology can be used by domain experts to understand machine learning and data analysis pipelines in terms of their individual components, which can help in prioritizing different components of the pipeline.



### Cross-Sensor Periocular Biometrics in a Global Pandemic: Comparative Benchmark and Novel Multialgorithmic Approach
- **Arxiv ID**: http://arxiv.org/abs/1902.08123v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08123v5)
- **Published**: 2019-02-21 16:24:40+00:00
- **Updated**: 2022-03-30 13:02:15+00:00
- **Authors**: Fernando Alonso-Fernandez, Kiran B. Raja, R. Raghavendra, Cristoph Busch, Josef Bigun, Ruben Vera-Rodriguez, Julian Fierrez
- **Comment**: Accepted for publication at Elsevier Information Fusion
- **Journal**: None
- **Summary**: The massive availability of cameras results in a wide variability of imaging conditions, producing large intra-class variations and a significant performance drop if heterogeneous images are compared for person recognition. However, as biometrics is deployed, it is common to replace damaged or obsolete hardware, or to exchange information between heterogeneous applications. Variations in spectral bands can also occur. For example, surveillance face images (typically acquired in the visible spectrum, VIS) may need to be compared against a legacy iris database (typically acquired in near-infrared, NIR). Here, we propose a multialgorithmic approach to cope with periocular images from different sensors. With face masks in the front line against COVID-19, periocular recognition is regaining popularity since it is the only face region that remains visible. We integrate different comparators with a fusion scheme based on linear logistic regression, in which scores are represented by log-likelihood ratios. This allows easy interpretation of scores and the use of Bayes thresholds for optimal decision-making since scores from different comparators are in the same probabilistic range. We evaluate our approach in the context of the Cross-Eyed Competition, whose aim was to compare recognition approaches when NIR and VIS periocular images are matched. Our approach achieves EER=0.2% and FRR of just 0.47% at FAR=0.01%, representing the best overall approach of the competition. Experiments are also reported with a database of VIS images from different smartphones. We also discuss the impact of template size and computation times, with the most computationally heavy comparator playing an important role in the results. Lastly, the proposed method is shown to outperform other popular fusion approaches, such as the average of scores, SVMs or Random Forest.



### Boundary-weighted Domain Adaptive Neural Network for Prostate MR Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.08128v2
- **DOI**: 10.1109/TMI.2019.2935018
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08128v2)
- **Published**: 2019-02-21 16:27:35+00:00
- **Updated**: 2019-08-15 02:00:05+00:00
- **Authors**: Qikui Zhu, Bo Du, Pingkun Yan
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate segmentation of the prostate from magnetic resonance (MR) images provides useful information for prostate cancer diagnosis and treatment. However, automated prostate segmentation from 3D MR images still faces several challenges. For instance, a lack of clear edge between the prostate and other anatomical structures makes it challenging to accurately extract the boundaries. The complex background texture and large variation in size, shape and intensity distribution of the prostate itself make segmentation even further complicated. With deep learning, especially convolutional neural networks (CNNs), emerging as commonly used methods for medical image segmentation, the difficulty in obtaining large number of annotated medical images for training CNNs has become much more pronounced that ever before. Since large-scale dataset is one of the critical components for the success of deep learning, lack of sufficient training data makes it difficult to fully train complex CNNs. To tackle the above challenges, in this paper, we propose a boundary-weighted domain adaptive neural network (BOWDA-Net). To make the network more sensitive to the boundaries during segmentation, a boundary-weighted segmentation loss (BWL) is proposed. Furthermore, an advanced boundary-weighted transfer leaning approach is introduced to address the problem of small medical imaging datasets. We evaluate our proposed model on the publicly available MICCAI 2012 Prostate MR Image Segmentation (PROMISE12) challenge dataset. Our experimental results demonstrate that the proposed model is more sensitive to boundary information and outperformed other state-of-the-art methods.



### Domain Partitioning Network
- **Arxiv ID**: http://arxiv.org/abs/1902.08134v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.08134v1)
- **Published**: 2019-02-21 16:45:20+00:00
- **Updated**: 2019-02-21 16:45:20+00:00
- **Authors**: Botos Csaba, Adnane Boukhayma, Viveka Kulharia, András Horváth, Philip H. S. Torr
- **Comment**: 18 pages, 13 figures
- **Journal**: None
- **Summary**: Standard adversarial training involves two agents, namely a generator and a discriminator, playing a mini-max game. However, even if the players converge to an equilibrium, the generator may only recover a part of the target data distribution, in a situation commonly referred to as mode collapse. In this work, we present the Domain Partitioning Network (DoPaNet), a new approach to deal with mode collapse in generative adversarial learning. We employ multiple discriminators, each encouraging the generator to cover a different part of the target distribution. To ensure these parts do not overlap and collapse into the same mode, we add a classifier as a third agent in the game. The classifier decides which discriminator the generator is trained against for each sample. Through experiments on toy examples and real images, we show the merits of DoPaNet in covering the real distribution and its superiority with respect to the competing methods. Besides, we also show that we can control the modes from which samples are generated using DoPaNet.



### Deep CNN-based Speech Balloon Detection and Segmentation for Comic Books
- **Arxiv ID**: http://arxiv.org/abs/1902.08137v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, q-bio.NC, 68T45 (Primary) 68T05, 91E30 (Secondary)
- **Links**: [PDF](http://arxiv.org/pdf/1902.08137v1)
- **Published**: 2019-02-21 16:49:12+00:00
- **Updated**: 2019-02-21 16:49:12+00:00
- **Authors**: David Dubray, Jochen Laubrock
- **Comment**: 10 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: We develop a method for the automated detection and segmentation of speech balloons in comic books, including their carrier and tails. Our method is based on a deep convolutional neural network that was trained on annotated pages of the Graphic Narrative Corpus. More precisely, we are using a fully convolutional network approach inspired by the U-Net architecture, combined with a VGG-16 based encoder. The trained model delivers state-of-the-art performance with an F1-score of over 0.94. Qualitative results suggest that wiggly tails, curved corners, and even illusory contours do not pose a major problem. Furthermore, the model has learned to distinguish speech balloons from captions. We compare our model to earlier results and discuss some possible applications.



### Blind Hyperspectral-Multispectral Image Fusion via Graph Laplacian Regularization
- **Arxiv ID**: http://arxiv.org/abs/1902.08224v1
- **DOI**: None
- **Categories**: **cs.CV**, 65D18, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1902.08224v1)
- **Published**: 2019-02-21 19:27:59+00:00
- **Updated**: 2019-02-21 19:27:59+00:00
- **Authors**: Chandrajit Bajaj, Tianming Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Fusing a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) of the same scene leads to a super-resolution image (SRI), which is information rich spatially and spectrally. In this paper, we super-resolve the HSI using the graph Laplacian defined on the MSI. Unlike many existing works, we don't assume prior knowledge about the spatial degradation from SRI to HSI, nor a perfectly aligned HSI and MSI pair. Our algorithm progressively alternates between finding the blur kernel and fusing HSI with MSI, generating accurate estimations of the blur kernel and the SRI at convergence. Experiments on various datasets demonstrate the advantages of the proposed algorithm in the quality of fusion and its capability in dealing with unknown spatial degradation.



### Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment
- **Arxiv ID**: http://arxiv.org/abs/1902.08231v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08231v1)
- **Published**: 2019-02-21 19:50:18+00:00
- **Updated**: 2019-02-21 19:50:18+00:00
- **Authors**: Peng Chu, Heng Fan, Chiu C Tan, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: Recent progresses in model-free single object tracking (SOT) algorithms have largely inspired applying SOT to \emph{multi-object tracking} (MOT) to improve the robustness as well as relieving dependency on external detector. However, SOT algorithms are generally designed for distinguishing a target from its environment, and hence meet problems when a target is spatially mixed with similar objects as observed frequently in MOT. To address this issue, in this paper we propose an instance-aware tracker to integrate SOT techniques for MOT by encoding awareness both within and between target models. In particular, we construct each target model by fusing information for distinguishing target both from background and other instances (tracking targets). To conserve uniqueness of all target models, our instance-aware tracker considers response maps from all target models and assigns spatial locations exclusively to optimize the overall accuracy. Another contribution we make is a dynamic model refreshing strategy learned by a convolutional neural network. This strategy helps to eliminate initialization noise as well as to adapt to the variation of target size and appearance. To show the effectiveness of the proposed approach, it is evaluated on the popular MOT15 and MOT16 challenge benchmarks. On both benchmarks, our approach achieves the best overall performances in comparison with published results.



### Similarity Learning Networks for Animal Individual Re-Identification -- Beyond the Capabilities of a Human Observer
- **Arxiv ID**: http://arxiv.org/abs/1902.09324v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09324v4)
- **Published**: 2019-02-21 19:52:23+00:00
- **Updated**: 2020-07-01 15:53:15+00:00
- **Authors**: Stefan Schneider, Graham W. Taylor, Stefan Linquist, Stefan C. Kremer
- **Comment**: 9 pages, 4 figures, 3 table. WACV 2020 - Deep Learning for Animal
  Re-ID Workshop
- **Journal**: None
- **Summary**: Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identification, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identification, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal re-identification methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different species: humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any species-specific modifications, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identification. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal re-identification from camera trap data.



### Lung Cancer Detection using Co-learning from Chest CT Images and Clinical Demographics
- **Arxiv ID**: http://arxiv.org/abs/1902.08236v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.08236v1)
- **Published**: 2019-02-21 20:01:35+00:00
- **Updated**: 2019-02-21 20:01:35+00:00
- **Authors**: Jiachen Wang, Riqiang Gao, Yuankai Huo, Shunxing Bao, Yunxi Xiong, Sanja L. Antic, Travis J. Osterman, Pierre P. Massion, Bennett A. Landman
- **Comment**: SPIE Medical Image, oral presentation
- **Journal**: None
- **Summary**: Early detection of lung cancer is essential in reducing mortality. Recent studies have demonstrated the clinical utility of low-dose computed tomography (CT) to detect lung cancer among individuals selected based on very limited clinical information. However, this strategy yields high false positive rates, which can lead to unnecessary and potentially harmful procedures. To address such challenges, we established a pipeline that co-learns from detailed clinical demographics and 3D CT images. Toward this end, we leveraged data from the Consortium for Molecular and Cellular Characterization of Screen-Detected Lesions (MCL), which focuses on early detection of lung cancer. A 3D attention-based deep convolutional neural net (DCNN) is proposed to identify lung cancer from the chest CT scan without prior anatomical location of the suspicious nodule. To improve upon the non-invasive discrimination between benign and malignant, we applied a random forest classifier to a dataset integrating clinical information to imaging data. The results show that the AUC obtained from clinical demographics alone was 0.635 while the attention network alone reached an accuracy of 0.687. In contrast when applying our proposed pipeline integrating clinical and imaging variables, we reached an AUC of 0.787 on the testing dataset. The proposed network both efficiently captures anatomical information for classification and also generates attention maps that explain the features that drive performance.



### Predictive Inequity in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.11097v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11097v1)
- **Published**: 2019-02-21 21:11:16+00:00
- **Updated**: 2019-02-21 21:11:16+00:00
- **Authors**: Benjamin Wilson, Judy Hoffman, Jamie Morgenstern
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate whether state-of-the-art object detection systems have equitable predictive performance on pedestrians with different skin tones. This work is motivated by many recent examples of ML and vision systems displaying higher error rates for certain demographic groups than others. We annotate an existing large scale dataset which contains pedestrians, BDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide an in-depth comparative analysis of performance between these two skin tone groupings, finding that neither time of day nor occlusion explain this behavior, suggesting this disparity is not merely the result of pedestrians in the 4-6 range appearing in more difficult scenes for detection. We investigate to what extent time of day, occlusion, and reweighting the supervised loss during training affect this predictive bias.



### End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data
- **Arxiv ID**: http://arxiv.org/abs/1902.08276v2
- **DOI**: 10.1016/j.nima.2020.164304
- **Categories**: **hep-ex**, cs.CV, cs.LG, physics.data-an
- **Links**: [PDF](http://arxiv.org/pdf/1902.08276v2)
- **Published**: 2019-02-21 21:43:09+00:00
- **Updated**: 2020-10-23 23:42:48+00:00
- **Authors**: Michael Andrews, John Alison, Sitong An, Patrick Bryant, Bjorn Burkle, Sergei Gleyzer, Meenakshi Narain, Manfred Paulini, Barnabas Poczos, Emanuele Usai
- **Comment**: 10 pages, 5 figures, 7 tables; v2: published version
- **Journal**: Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020)
- **Summary**: We describe the construction of end-to-end jet image classifiers based on simulated low-level detector data to discriminate quark- vs. gluon-initiated jets with high-fidelity simulated CMS Open Data. We highlight the importance of precise spatial information and demonstrate competitive performance to existing state-of-the-art jet classifiers. We further generalize the end-to-end approach to event-level classification of quark vs. gluon di-jet QCD events. We compare the fully end-to-end approach to using hand-engineered features and demonstrate that the end-to-end algorithm is robust against the effects of underlying event and pile-up.



