# Arxiv Papers in cs.CV on 2019-02-16
### R$^2$-CNN: Fast Tiny Object Detection in Large-Scale Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/1902.06042v3
- **DOI**: 10.1109/TGRS.2019.2899955
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06042v3)
- **Published**: 2019-02-16 04:59:13+00:00
- **Updated**: 2019-03-30 11:43:11+00:00
- **Authors**: Jiangmiao Pang, Cong Li, Jianping Shi, Zhihai Xu, Huajun Feng
- **Comment**: 13 pages. Accepted to IEEE Transactions on Geoscience and Remote
  Sensing
- **Journal**: None
- **Summary**: Recently, the convolutional neural network has brought impressive improvements for object detection. However, detecting tiny objects in large-scale remote sensing images still remains challenging. First, the extreme large input size makes the existing object detection solutions too slow for practical use. Second, the massive and complex backgrounds cause serious false alarms. Moreover, the ultratiny objects increase the difficulty of accurate detection. To tackle these problems, we propose a unified and self-reinforced network called remote sensing region-based convolutional neural network ($\mathcal{R}^2$-CNN), composing of backbone Tiny-Net, intermediate global attention block, and final classifier and detector. Tiny-Net is a lightweight residual structure, which enables fast and powerful features extraction from inputs. Global attention block is built upon Tiny-Net to inhibit false positives. Classifier is then used to predict the existence of targets in each patch, and detector is followed to locate them accurately if available. The classifier and detector are mutually reinforced with end-to-end training, which further speed up the process and avoid false alarms. Effectiveness of $\mathcal{R}^2$-CNN is validated on hundreds of GF-1 images and GF-2 images that are 18 000 $\times$ 18 192 pixels, 2.0-m resolution, and 27 620 $\times$ 29 200 pixels, 0.8-m resolution, respectively. Specifically, we can process a GF-1 image in 29.4 s on Titian X just with single thread. According to our knowledge, no previous solution can detect the tiny object on such huge remote sensing images gracefully. We believe that it is a significant step toward practical real-time remote sensing systems.



### Min-Entropy Latent Model for Weakly Supervised Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.06057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06057v1)
- **Published**: 2019-02-16 07:32:39+00:00
- **Updated**: 2019-02-16 07:32:39+00:00
- **Authors**: Fang Wan, Pengxu Wei, Zhenjun Han, Jianbin Jiao, Qixiang Ye
- **Comment**: Accepted by TPAMI
- **Journal**: None
- **Summary**: Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces significant randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy serves as a model to learn object locations and a metric to measure the randomness of object localization during learning. It aims to principally reduce the variance of learned instances and alleviate the ambiguity of detectors. MELM is decomposed into three components including proposal clique partition, object clique discovery, and object localization. MELM is optimized with a recurrent learning algorithm, which leverages continuation optimization to solve the challenging non-convexity problem. Experiments demonstrate that MELM significantly improves the performance of weakly supervised object detection, weakly supervised object localization, and image classification, against the state-of-the-art approaches.



### Towards Automated Melanoma Detection with Deep Learning: Data Purification and Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.06061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06061v2)
- **Published**: 2019-02-16 07:57:12+00:00
- **Updated**: 2019-05-14 18:58:58+00:00
- **Authors**: Devansh Bisla, Anna Choromanska, Jennifer A. Stein, David Polsky, Russell Berman
- **Comment**: Accepted to CVPR ISIC Workshop - 2019
- **Journal**: None
- **Summary**: Melanoma is one of the ten most common cancers in the US. Early detection is crucial for survival, but often the cancer is diagnosed in the fatal stage. Deep learning has the potential to improve cancer detection rates, but its applicability to melanoma detection is compromised by the limitations of the available skin lesion databases, which are small, heavily imbalanced, and contain images with occlusions. We build deep-learning-based tools for data purification and augmentation to counter-act these limitations. The developed tools can be utilized in a deep learning system for lesion classification and we show how to build such a system. The system heavily relies on the processing unit for removing image occlusions and the data generation unit, based on generative adversarial networks, for populating scarce lesion classes, or equivalently creating virtual patients with pre-defined types of lesions. We empirically verify our approach and show that incorporating these two units into melanoma detection system results in the superior performance over common baselines.



### RES-SE-NET: Boosting Performance of Resnets by Enhancing Bridge-connections
- **Arxiv ID**: http://arxiv.org/abs/1902.06066v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.06066v1)
- **Published**: 2019-02-16 08:25:16+00:00
- **Updated**: 2019-02-16 08:25:16+00:00
- **Authors**: Varshaneya V, Balasubramanian S, Darshan Gera
- **Comment**: None
- **Journal**: None
- **Summary**: One of the ways to train deep neural networks effectively is to use residual connections. Residual connections can be classified as being either identity connections or bridge-connections with a reshaping convolution. Empirical observations on CIFAR-10 and CIFAR-100 datasets using a baseline Resnet model, with bridge-connections removed, have shown a significant reduction in accuracy. This reduction is due to lack of contribution, in the form of feature maps, by the bridge-connections. Hence bridge-connections are vital for Resnet. However, all feature maps in the bridge-connections are considered to be equally important. In this work, an upgraded architecture "Res-SE-Net" is proposed to further strengthen the contribution from the bridge-connections by quantifying the importance of each feature map and weighting them accordingly using Squeeze-and-Excitation (SE) block. It is demonstrated that Res-SE-Net generalizes much better than Resnet and SE-Resnet on the benchmark CIFAR-10 and CIFAR-100 datasets.



### Deep Learning for Image Super-resolution: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1902.06068v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06068v2)
- **Published**: 2019-02-16 08:39:36+00:00
- **Updated**: 2020-02-08 04:43:33+00:00
- **Authors**: Zhihao Wang, Jian Chen, Steven C. H. Hoi
- **Comment**: Accepted by IEEE TPAMI
- **Journal**: None
- **Summary**: Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.



### Local Fourier Slice Photography
- **Arxiv ID**: http://arxiv.org/abs/1902.06082v2
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.NA, math.NA
- **Links**: [PDF](http://arxiv.org/pdf/1902.06082v2)
- **Published**: 2019-02-16 10:37:00+00:00
- **Updated**: 2019-10-10 20:22:56+00:00
- **Authors**: Christian Lessig
- **Comment**: images with reduced quality (please contact the author for a high
  resolution version)
- **Journal**: None
- **Summary**: Light field cameras provide intriguing possibilities, such as post-capture refocus or the ability to synthesize images from novel viewpoints. This comes, however, at the price of significant storage requirements. Compression techniques can be used to reduce these but refocusing and reconstruction require so far again a dense pixel representation. To avoid this, we introduce local Fourier slice photography that allows for refocused image reconstruction directly from a sparse wavelet representation of a light field, either to obtain an image or a compressed representation of it. The result is made possible by wavelets that respect the "slicing's" intrinsic structure and enable us to derive exact reconstruction filters for the refocused image in closed form. Image reconstruction then amounts to applying these filters to the light field's wavelet coefficients, and hence no reconstruction of a dense pixel representation is required. We demonstrate that this substantially reduces storage requirements and also computation times. We furthermore analyze the computational complexity of our algorithm and show that it scales linearly with the size of the reconstructed region and the non-negligible wavelet coefficients, i.e. with the visual complexity.



### DC-AL GAN: Pseudoprogression and True Tumor Progression of Glioblastoma Multiform Image Classification Based on DCGAN and AlexNet
- **Arxiv ID**: http://arxiv.org/abs/1902.06085v4
- **DOI**: 10.1002/mp.14003
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06085v4)
- **Published**: 2019-02-16 10:43:33+00:00
- **Updated**: 2019-05-18 05:48:12+00:00
- **Authors**: Meiyu Li, Hailiang Tang, Michael D. Chan, Xiaobo Zhou, Xiaohua Qian
- **Comment**: None
- **Journal**: None
- **Summary**: Pseudoprogression (PsP) occurs in 20-30% of patients with glioblastoma multiforme (GBM) after receiving the standard treatment. In the course of post-treatment magnetic resonance imaging (MRI), PsP exhibits similarities in shape and intensity to the true tumor progression (TTP) of GBM. So, these similarities pose challenges on the differentiation of these types of progression and hence the selection of the appropriate clinical treatment strategy. In this paper, we introduce DC-AL GAN, a novel feature learning method based on deep convolutional generative adversarial network (DCGAN) and AlexNet, to discriminate between PsP and TTP in MRI images. Due to the adversarial relationship between the generator and the discriminator of DCGAN, high-level discriminative features of PsP and TTP can be derived for the discriminator with AlexNet. Also, a feature fusion scheme is used to combine higher-layer features with lower-layer information, leading to more powerful features that are used for effectively discriminating between PsP and TTP. The experimental results show that DC-AL GAN achieves desirable PsP and TTP classification performance that is superior to other state-of-the-art methods.



### Semi-supervised Learning on Graph with an Alternating Diffusion Process
- **Arxiv ID**: http://arxiv.org/abs/1902.06105v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06105v1)
- **Published**: 2019-02-16 14:26:47+00:00
- **Updated**: 2019-02-16 14:26:47+00:00
- **Authors**: Qilin Li, Senjian An, Ling Li, Wanquan Liu
- **Comment**: 7 pages, 2 figures, 2 tables
- **Journal**: None
- **Summary**: Graph-based semi-supervised learning usually involves two separate stages, constructing an affinity graph and then propagating labels for transductive inference on the graph. It is suboptimal to solve them independently, as the correlation between the affinity graph and labels are not fully exploited. In this paper, we integrate the two stages into one unified framework by formulating the graph construction as a regularized function estimation problem similar to label propagation. We propose an alternating diffusion process to solve the two problems simultaneously, which allows us to learn the graph and unknown labels in an iterative fashion. With the proposed framework, we are able to adequately leverage both the given labels and estimated labels to construct a better graph, and effectively propagate labels on such a dynamic graph updated simultaneously with the newly obtained labels. Extensive experiments on various real-world datasets have demonstrated the superiority of the proposed method compared to other state-of-the-art methods.



### Atlas-based automated detection of swim bladder in Medaka embryo
- **Arxiv ID**: http://arxiv.org/abs/1902.06130v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1902.06130v1)
- **Published**: 2019-02-16 17:30:53+00:00
- **Updated**: 2019-02-16 17:30:53+00:00
- **Authors**: Diane Genest, Marc Léonard, Jean Cousty, Noémie De Crozé, Hugues Talbot
- **Comment**: None
- **Journal**: None
- **Summary**: Fish embryo models are increasingly being used both for the assessment of chemicals efficacy and potential toxicity. This article proposes a methodology to automatically detect the swim bladder on 2D images of Medaka fish embryos seen either in dorsal view or in lateral view. After embryo segmentation and for each studied orientation, the method builds an atlas of a healthy embryo. This atlas is then used to define the region of interest and to guide the swim bladder segmentation with a discrete globally optimal active contour. Descriptors are subsequently designed from this segmentation. An automated random forest clas-sifier is built from these descriptors in order to classify embryos with and without a swim bladder. The proposed method is assessed on a dataset of 261 images, containing 202 embryos with a swim bladder (where 196 are in dorsal view and 6 are in lateral view) and 59 without (where 43 are in dorsal view and 16 are in lateral view). We obtain an average precision rate of 95% in the total dataset following 5-fold cross-validation.



### LISA: a MATLAB package for Longitudinal Image Sequence Analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.06131v1
- **DOI**: None
- **Categories**: **stat.CO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.06131v1)
- **Published**: 2019-02-16 17:50:19+00:00
- **Updated**: 2019-02-16 17:50:19+00:00
- **Authors**: Jang Ik Cho, Xiaofeng Wang, Yifan Xu, Jiayang Sun
- **Comment**: 18 pages, 17 figures made from 35 png files
- **Journal**: None
- **Summary**: Large sequences of images (or movies) can now be obtained on an unprecedented scale, which poses fundamental challenges to the existing image analysis techniques. The challenges include heterogeneity, (automatic) alignment, multiple comparisons, potential artifacts, and hidden noises. This paper introduces our MATLAB package, Longitudinal Image Sequence Analysis (LISA), as a one-stop ensemble of image processing and analysis tool for comparing a general class of images from either different times, sessions, or subjects. Given two contrasting sequences of images, the image processing in LISA starts with selecting a region of interest in two representative images, followed by automatic or manual segmentation and registration. Automatic segmentation de-noises an image using a mixture of Gaussian distributions of the pixel intensity values, while manual segmentation applies a user-chosen intensity cut-off value to filter out noises. Automatic registration aligns the contrasting images based on a mid-line regression whereas manual registration lines up the images along a reference line formed by two user-selected points. The processed images are then rendered for simultaneous statistical comparisons to generate D, S, T, and P-maps. The D map represents a curated difference of contrasting images, the S map is the non-parametrically smoothed differences, the T map presents the variance-adjusted, smoothed differences, and the P-map provides multiplicity-controlled p-values. These maps reveal the regions with significant differences due to either longitudinal, subject-specific, or treatment changes. A user can skip the image processing step to dive directly into the statistical analysis step if the images have already been processed. Hence, LISA offers flexibility in applying other image pre-processing tools. LISA also has a parallel computing option for high definition images.



### Neuromodulated Goal-Driven Perception in Uncertain Domains
- **Arxiv ID**: http://arxiv.org/abs/1903.00068v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1903.00068v1)
- **Published**: 2019-02-16 19:46:09+00:00
- **Updated**: 2019-02-16 19:46:09+00:00
- **Authors**: Xinyun Zou, Soheil Kolouri, Praveen K. Pilly, Jeffrey L. Krichmar
- **Comment**: None
- **Journal**: None
- **Summary**: In uncertain domains, the goals are often unknown and need to be predicted by the organism or system. In this paper, contrastive excitation backprop (c-EB) was used in a goal-driven perception task with pairs of noisy MNIST digits, where the system had to increase attention to one of the two digits corresponding to a goal (i.e., even, odd, low value, or high value) and decrease attention to the distractor digit or noisy background pixels. Because the valid goal was unknown, an online learning model based on the cholinergic and noradrenergic neuromodulatory systems was used to predict a noisy goal (expected uncertainty) and re-adapt when the goal changed (unexpected uncertainty). This neurobiologically plausible model demonstrates how neuromodulatory systems can predict goals in uncertain domains and how attentional mechanisms can enhance the perception of that goal.



### BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding
- **Arxiv ID**: http://arxiv.org/abs/1902.06148v3
- **DOI**: 10.1109/IGARSS.2019.8900532
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06148v3)
- **Published**: 2019-02-16 20:06:55+00:00
- **Updated**: 2019-06-06 20:56:02+00:00
- **Authors**: Gencer Sumbul, Marcela Charfuelan, Begüm Demir, Volker Markl
- **Comment**: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium (IGARSS) 2019
- **Journal**: None
- **Summary**: This paper presents the BigEarthNet that is a new large-scale multi-label Sentinel-2 benchmark archive. The BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. Unlike most of the existing archives, each image patch is annotated by multiple land-cover classes (i.e., multi-labels) that are provided from the CORINE Land Cover database of the year 2018 (CLC 2018). The BigEarthNet is significantly larger than the existing archives in remote sensing (RS) and thus is much more convenient to be used as a training source in the context of deep learning. This paper first addresses the limitations of the existing archives and then describes the properties of the BigEarthNet. Experimental results obtained in the framework of RS image scene classification problems show that a shallow Convolutional Neural Network (CNN) architecture trained on the BigEarthNet provides much higher accuracy compared to a state-of-the-art CNN model pre-trained on the ImageNet (which is a very popular large-scale benchmark archive in computer vision). The BigEarthNet opens up promising directions to advance operational RS applications and research in massive Sentinel-2 image archives.



### Deep Generalized Convolutional Sum-Product Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.06155v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.06155v4)
- **Published**: 2019-02-16 20:55:53+00:00
- **Updated**: 2020-09-22 19:09:17+00:00
- **Authors**: Jos van de Wolfshaar, Andrzej Pronobis
- **Comment**: None
- **Journal**: None
- **Summary**: Sum-Product Networks (SPNs) are hierarchical, graphical models that combine benefits of deep learning and probabilistic modeling. SPNs offer unique advantages to applications demanding exact probabilistic inference over high-dimensional, noisy inputs. Yet, compared to convolutional neural nets, they struggle with capturing complex spatial relationships in image data. To alleviate this issue, we introduce Deep Generalized Convolutional Sum-Product Networks (DGC-SPNs), which encode spatial features in a way similar to CNNs, while preserving the validity of the probabilistic SPN model. As opposed to existing SPN-based image representations, DGC-SPNs allow for overlapping convolution patches through a novel parameterization of dilations and strides, resulting in significantly improved feature coverage and feature resolution. DGC-SPNs substantially outperform other SPN architectures across several visual datasets and for both generative and discriminative tasks, including image inpainting and classification. These contributions are reinforced by the first simple, scalable, and GPU-optimized implementation of SPNs, integrated with the widely used Keras/TensorFlow framework. The resulting model is fully probabilistic and versatile, yet efficient and straightforward to apply in practical applications in place of traditional deep nets.



### Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1902.06162v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.06162v1)
- **Published**: 2019-02-16 21:30:18+00:00
- **Updated**: 2019-02-16 21:30:18+00:00
- **Authors**: Longlong Jing, Yingli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.



