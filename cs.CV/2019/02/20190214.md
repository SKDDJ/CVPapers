# Arxiv Papers in cs.CV on 2019-02-14
### Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.05176v2
- **DOI**: 10.1109/LRA.2019.2925305
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.05176v2)
- **Published**: 2019-02-14 00:53:07+00:00
- **Updated**: 2019-06-26 04:39:37+00:00
- **Authors**: Behnoosh Parsa, Ekta U. Samani, Rose Hendrix, Cameron Devine, Shashi M. Singh, Santosh Devasia, Ashis G. Banerjee
- **Comment**: None
- **Journal**: None
- **Summary**: Automated real-time prediction of the ergonomic risks of manipulating objects is a key unsolved challenge in developing effective human-robot collaboration systems for logistics and manufacturing applications. We present a foundational paradigm to address this challenge by formulating the problem as one of action segmentation from RGB-D camera videos. Spatial features are first learned using a deep convolutional model from the video frames, which are then fed sequentially to temporal convolutional networks to semantically segment the frames into a hierarchy of actions, which are either ergonomically safe, require monitoring, or need immediate attention. For performance evaluation, in addition to an open-source kitchen dataset, we collected a new dataset comprising twenty individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights. Results show very high (87-94)\% F1 overlap scores among the ground truth and predicted frame labels for videos lasting over two minutes and consisting of a large number of actions.



### Transfusion: Understanding Transfer Learning for Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1902.07208v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.07208v3)
- **Published**: 2019-02-14 01:54:53+00:00
- **Updated**: 2019-10-29 20:16:30+00:00
- **Authors**: Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, Samy Bengio
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Transfer learning from natural image datasets, particularly ImageNet, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to ImageNet architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.



### Non-contact photoplethysmogram and instantaneous heart rate estimation from infrared face video
- **Arxiv ID**: http://arxiv.org/abs/1902.05194v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05194v1)
- **Published**: 2019-02-14 03:01:28+00:00
- **Updated**: 2019-02-14 03:01:28+00:00
- **Authors**: Natalia Martinez, Martin Bertran, Guillermo Sapiro, Hau-Tieng Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Extracting the instantaneous heart rate (iHR) from face videos has been well studied in recent years. It is well known that changes in skin color due to blood flow can be captured using conventional cameras. One of the main limitations of methods that rely on this principle is the need of an illumination source. Moreover, they have to be able to operate under different light conditions. One way to avoid these constraints is using infrared cameras, allowing the monitoring of iHR under low light conditions. In this work, we present a simple, principled signal extraction method that recovers the iHR from infrared face videos. We tested the procedure on 7 participants, for whom we recorded an electrocardiogram simultaneously with their infrared face video. We checked that the recovered signal matched the ground truth iHR, showing that infrared is a promising alternative to conventional video imaging for heart rate monitoring, especially in low light conditions. Code is available at https://github.com/natalialmg/IR_iHR



### Long and Short Memory Balancing in Visual Co-Tracking using Q-Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.05211v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05211v1)
- **Published**: 2019-02-14 04:17:29+00:00
- **Updated**: 2019-02-14 04:17:29+00:00
- **Authors**: Kourosh Meshgi, Maryam Sadat Mirzaei, Shigeyuki Oba
- **Comment**: Submitted to ICIP 2019
- **Journal**: None
- **Summary**: Employing one or more additional classifiers to break the self-learning loop in tracing-by-detection has gained considerable attention. Most of such trackers merely utilize the redundancy to address the accumulating label error in the tracking loop, and suffer from high computational complexity as well as tracking challenges that may interrupt all classifiers (e.g. temporal occlusions). We propose the active co-tracking framework, in which the main classifier of the tracker labels samples of the video sequence, and only consults auxiliary classifier when it is uncertain. Based on the source of the uncertainty and the differences of two classifiers (e.g. accuracy, speed, update frequency, etc.), different policies should be taken to exchange the information between two classifiers. Here, we introduce a reinforcement learning approach to find the appropriate policy by considering the state of the tracker in a specific sequence. The proposed method yields promising results in comparison to the best tracking-by-detection approaches.



### Actions Generation from Captions
- **Arxiv ID**: http://arxiv.org/abs/1902.11109v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1902.11109v1)
- **Published**: 2019-02-14 04:37:49+00:00
- **Updated**: 2019-02-14 04:37:49+00:00
- **Authors**: Xuan Liang, Yida Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Sequence transduction models have been widely explored in many natural language processing tasks. However, the target sequence usually consists of discrete tokens which represent word indices in a given vocabulary. We barely see the case where target sequence is composed of continuous vectors, where each vector is an element of a time series taken successively in a temporal domain. In this work, we introduce a new data set, named Action Generation Data Set (AGDS) which is specifically designed to carry out the task of caption-to-action generation. This data set contains caption-action pairs. The caption is comprised of a sequence of words describing the interactive movement between two people, and the action is a captured sequence of poses representing the movement. This data set is introduced to study the ability of generating continuous sequences through sequence transduction models. We also propose a model to innovatively combine Multi-Head Attention (MHA) and Generative Adversarial Network (GAN) together. In our model, we have one generator to generate actions from captions and three discriminators where each of them is designed to carry out a unique functionality: caption-action consistency discriminator, pose discriminator and pose transition discriminator. This novel design allowed us to achieve plausible generation performance which is demonstrated in the experiments.



### Graph-RISE: Graph-Regularized Image Semantic Embedding
- **Arxiv ID**: http://arxiv.org/abs/1902.10814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10814v1)
- **Published**: 2019-02-14 04:55:28+00:00
- **Updated**: 2019-02-14 04:55:28+00:00
- **Authors**: Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, Sujith Ravi
- **Comment**: 9 pages, 7 figures
- **Journal**: None
- **Summary**: Learning image representations to capture fine-grained semantics has been a challenging and important task enabling many applications such as image search and clustering. In this paper, we present Graph-Regularized Image Semantic Embedding (Graph-RISE), a large-scale neural graph learning framework that allows us to train embeddings to discriminate an unprecedented O(40M) ultra-fine-grained semantic labels. Graph-RISE outperforms state-of-the-art image embedding algorithms on several evaluation tasks, including image classification and triplet ranking. We provide case studies to demonstrate that, qualitatively, image retrieval based on Graph-RISE effectively captures semantics and, compared to the state-of-the-art, differentiates nuances at levels that are closer to human-perception.



### 3D Graph Embedding Learning with a Structure-aware Loss Function for Point Cloud Semantic Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1902.05247v1
- **DOI**: 10.1109/LRA.2020.3004802
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05247v1)
- **Published**: 2019-02-14 07:29:40+00:00
- **Updated**: 2019-02-14 07:29:40+00:00
- **Authors**: Zhidong Liang, Ming Yang, Chunxiang Wang
- **Comment**: None
- **Journal**: Title: 3D Instance Embedding Learning With a Structure-Aware Loss
  Function for Point Cloud Segmentation; Published in: IEEE Robotics and
  Automation Letters ( Volume: 5, Issue: 3, July 2020)
- **Summary**: This paper introduces a novel approach for 3D semantic instance segmentation on point clouds. A 3D convolutional neural network called submanifold sparse convolutional network is used to generate semantic predictions and instance embeddings simultaneously. To obtain discriminative embeddings for each 3D instance, a structure-aware loss function is proposed which considers both the structure information and the embedding information. To get more consistent embeddings for each 3D instance, attention-based k nearest neighbour (KNN) is proposed to assign different weights for different neighbours. Based on the attention-based KNN, we add a graph convolutional network after the sparse convolutional network to get refined embeddings. Our network can be trained end-to-end. A simple mean-shift algorithm is utilized to cluster refined embeddings to get final instance predictions. As a result, our framework can output both the semantic prediction and the instance prediction. Experiments show that our approach outperforms all state-of-art methods on ScanNet benchmark and NYUv2 dataset.



### Computed tomography data collection of the complete human mandible and valid clinical ground truth models
- **Arxiv ID**: http://arxiv.org/abs/1902.05255v1
- **DOI**: 10.1038/sdata.2019.3
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05255v1)
- **Published**: 2019-02-14 08:10:39+00:00
- **Updated**: 2019-02-14 08:10:39+00:00
- **Authors**: Jürgen Wallner, Irene Mischak, Jan Egger
- **Comment**: 14 pages, 8 Figures, 4 Tables, 55 References
- **Journal**: Wallner, J. et al. Computed tomography data collection of the
  complete human mandible and valid clinical ground truth models. Sci. Data.
  6:190003 https://doi.org/10.1038/sdata.2019.3 (2019)
- **Summary**: Image-based algorithmic software segmentation is an increasingly important topic in many medical fields. Algorithmic segmentation is used for medical three-dimensional visualization, diagnosis or treatment support, especially in complex medical cases. However, accessible medical databases are limited, and valid medical ground truth databases for the evaluation of algorithms are rare and usually comprise only a few images. Inaccuracy or invalidity of medical ground truth data and image-based artefacts also limit the creation of such databases, which is especially relevant for CT data sets of the maxillomandibular complex. This contribution provides a unique and accessible data set of the complete mandible, including 20 valid ground truth segmentation models originating from 10 CT scans from clinical practice without artefacts or faulty slices. From each CT scan, two 3D ground truth models were created by clinical experts through independent manual slice-by-slice segmentation, and the models were statistically compared to prove their validity. These data could be used to conduct serial image studies of the human mandible, evaluating segmentation algorithms and developing adequate image tools.



### Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.05291v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05291v1)
- **Published**: 2019-02-14 10:25:26+00:00
- **Updated**: 2019-02-14 10:25:26+00:00
- **Authors**: Yanpeng Cao, Dayan Guan, Yulun Wu, Jiangxin Yang, Yanlong Cao, Michael Ying Yang
- **Comment**: None
- **Journal**: None
- **Summary**: Effective fusion of complementary information captured by multi-modal sensors (visible and infrared cameras) enables robust pedestrian detection under various surveillance situations (e.g. daytime and nighttime). In this paper, we present a novel box-level segmentation supervised learning framework for accurate and real-time multispectral pedestrian detection by incorporating features extracted in visible and infrared channels. Specifically, our method takes pairs of aligned visible and infrared images with easily obtained bounding box annotations as input and estimates accurate prediction maps to highlight the existence of pedestrians. It offers two major advantages over the existing anchor box based multispectral detection methods. Firstly, it overcomes the hyperparameter setting problem occurred during the training phase of anchor box based detectors and can obtain more accurate detection results, especially for small and occluded pedestrian instances. Secondly, it is capable of generating accurate detection results using small-size input images, leading to improvement of computational efficiency for real-time autonomous driving applications. Experimental results on KAIST multispectral dataset show that our proposed method outperforms state-of-the-art approaches in terms of both accuracy and speed.



### On instabilities of deep learning in image reconstruction - Does AI come at a cost?
- **Arxiv ID**: http://arxiv.org/abs/1902.05300v1
- **DOI**: 10.1073/pnas.1907377117
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05300v1)
- **Published**: 2019-02-14 11:14:58+00:00
- **Updated**: 2019-02-14 11:14:58+00:00
- **Authors**: Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock, Anders C. Hansen
- **Comment**: None
- **Journal**: Proc. Natl. Acad. Sci. USA, 2020
- **Summary**: Deep learning, due to its unprecedented success in tasks such as image classification, has emerged as a new tool in image reconstruction with potential to change the field. In this paper we demonstrate a crucial phenomenon: deep learning typically yields unstablemethods for image reconstruction. The instabilities usually occur in several forms: (1) tiny, almost undetectable perturbations, both in the image and sampling domain, may result in severe artefacts in the reconstruction, (2) a small structural change, for example a tumour, may not be captured in the reconstructed image and (3) (a counterintuitive type of instability) more samples may yield poorer performance. Our new stability test with algorithms and easy to use software detects the instability phenomena. The test is aimed at researchers to test their networks for instabilities and for government agencies, such as the Food and Drug Administration (FDA), to secure safe use of deep learning methods.



### A Novel Just-Noticeable-Difference-based Saliency-Channel Attention Residual Network for Full-Reference Image Quality Predictions
- **Arxiv ID**: http://arxiv.org/abs/1902.05316v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05316v4)
- **Published**: 2019-02-14 11:50:49+00:00
- **Updated**: 2020-10-16 06:34:33+00:00
- **Authors**: Soomin Seo, Sehwan Ki, Munchurl Kim
- **Comment**: Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)
- **Journal**: None
- **Summary**: Recently, due to the strength of deep convolutional neural networks (CNN), many CNN-based image quality assessment (IQA) models have been studied. However, previous CNN-based IQA models likely have yet to utilize the characteristics of the human visual system (HVS) fully for IQA problems when they simply entrust everything to the CNN, expecting it to learn from a training dataset. However, in this paper, we propose a novel saliency-channel attention residual network based on the just-noticeable-difference (JND) concept for full-reference image quality assessments (FR-IQA). It is referred to as JND-SalCAR and shows significant improvements in large IQA datasets with various types of distortion. The proposed JND-SalCAR effectively learns how to incorporate human psychophysical characteristics, such as visual saliency and JND, into image quality predictions. In the proposed network, a SalCAR block is devised so that perceptually important features can be extracted with the help of saliency-based spatial attention and channel attention schemes. In addition, a saliency map serves as a guideline for predicting a patch weight map in order to afford stable training of end-to-end optimization for the JND-SalCAR. To the best of our knowledge, our work presents the first HVS-inspired trainable FR-IQA network that considers both visual saliency and the JND characteristics of the HVS. When the visual saliency map and the JND probability map are explicitly given as priors, they can be usefully combined to predict IQA scores rated by humans more precisely, eventually leading to performance improvements and faster convergence. The experimental results show that the proposed JND-SalCAR significantly outperforms all recent state-of-the-art FR-IQA methods on large IQA datasets in terms of the Spearman rank order coefficient (SRCC) and the Pearson linear correlation coefficient (PLCC).



### Artist Style Transfer Via Quadratic Potential
- **Arxiv ID**: http://arxiv.org/abs/1902.11108v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11108v2)
- **Published**: 2019-02-14 12:09:13+00:00
- **Updated**: 2019-03-05 05:09:17+00:00
- **Authors**: Rahul Bhalley, Jianlin Su
- **Comment**: 8 pages, 3 figures, uses nips_2018.sty, renamed the network to
  CycleGAN-QP for maintaining consistency with work
- **Journal**: None
- **Summary**: In this paper we address the problem of artist style transfer where the painting style of a given artist is applied on a real world photograph. We train our neural networks in adversarial setting via recently introduced quadratic potential divergence for stable learning process. To further improve the quality of generated artist stylized images we also integrate some of the recently introduced deep learning techniques in our method. To our best knowledge this is the first attempt towards artist style transfer via quadratic potential divergence. We provide some stylized image samples in the supplementary material. The source code for experimentation was written in PyTorch and is available online in my GitHub repository.



### Breast Cancer: Model Reconstruction and Image Registration from Segmented Deformed Image using Visual and Force based Analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.05340v4
- **DOI**: 10.1109/TMI.2019.2946629
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05340v4)
- **Published**: 2019-02-14 13:09:20+00:00
- **Updated**: 2019-10-15 07:28:27+00:00
- **Authors**: Shuvendu Rana, Rory Hampson, Gordon Dobie
- **Comment**: 12 pages, 16 figures
- **Journal**: IEEE Transactions on Medical Imaging 2019
- **Summary**: Breast lesion localization using tactile imaging is a new and developing direction in medical science. To achieve the goal, proper image reconstruction and image registration can be a valuable asset. In this paper, a new approach of the segmentation-based image surface reconstruction algorithm is used to reconstruct the surface of a breast phantom. In breast tissue, the sub-dermal vein network is used as a distinguishable pattern for reconstruction. The proposed image capturing device contacts the surface of the phantom, and surface deformation will occur due to applied force at the time of scanning. A novel force based surface rectification system is used to reconstruct a deformed surface image to its original structure. For the construction of the full surface from rectified images, advanced affine scale-invariant feature transform (A-SIFT) is proposed to reduce the affine effect in time when data capturing. Camera position based image stitching approach is applied to construct the final original non-rigid surface. The proposed model is validated in theoretical models and real scenarios, to demonstrate its advantages with respect to competing methods. The result of the proposed method, applied to path reconstruction, ends with a positioning accuracy of 99.7%



### Automatic Labeled LiDAR Data Generation based on Precise Human Model
- **Arxiv ID**: http://arxiv.org/abs/1902.05341v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05341v1)
- **Published**: 2019-02-14 13:11:14+00:00
- **Updated**: 2019-02-14 13:11:14+00:00
- **Authors**: Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki
- **Comment**: Accepted at ICRA2019
- **Journal**: None
- **Summary**: Following improvements in deep neural networks, state-of-the-art networks have been proposed for human recognition using point clouds captured by LiDAR. However, the performance of these networks strongly depends on the training data. An issue with collecting training data is labeling. Labeling by humans is necessary to obtain the ground truth label; however, labeling requires huge costs. Therefore, we propose an automatic labeled data generation pipeline, for which we can change any parameters or data generation environments. Our approach uses a human model named Dhaiba and a background of Miraikan and consequently generated realistic artificial data. We present 500k+ data generated by the proposed pipeline. This paper also describes the specification of the pipeline and data details with evaluations of various approaches.



### Study of dynamical system based obstacle avoidance via manipulating orthogonal coordinates
- **Arxiv ID**: http://arxiv.org/abs/1902.05343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY
- **Links**: [PDF](http://arxiv.org/pdf/1902.05343v2)
- **Published**: 2019-02-14 13:16:19+00:00
- **Updated**: 2019-02-16 13:35:13+00:00
- **Authors**: Weiya Ren
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we consider the general problem of obstacle avoidance based on dynamical system. The modulation matrix is developed by introducing orthogonal coordinates, which makes the modulation matrix more reasonable. The new trajectory's direction can be represented by the linear combination of orthogonal coordinates. A orthogonal coordinates manipulating approach is proposed by introducing rotating matrix to solve the local minimal problem and provide more reasonable motions in 3-D or higher dimension space. The proposed method also provide a solution for patrolling around a convex shape. Experimental results on several designed dynamical systems demonstrate the effectiveness of the proposed approach.



### Sparse and noisy LiDAR completion with RGB guidance and uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1902.05356v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05356v1)
- **Published**: 2019-02-14 13:55:22+00:00
- **Updated**: 2019-02-14 13:55:22+00:00
- **Authors**: Wouter Van Gansbeke, Davy Neven, Bert De Brabandere, Luc Van Gool
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: This work proposes a new method to accurately complete sparse LiDAR maps guided by RGB images. For autonomous vehicles and robotics the use of LiDAR is indispensable in order to achieve precise depth predictions. A multitude of applications depend on the awareness of their surroundings, and use depth cues to reason and react accordingly. On the one hand, monocular depth prediction methods fail to generate absolute and precise depth maps. On the other hand, stereoscopic approaches are still significantly outperformed by LiDAR based approaches. The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds which are mapped to a 2D plane. We propose a new framework which extracts both global and local information in order to produce proper depth maps. We argue that simple depth completion does not require a deep network. However, we additionally propose a fusion method with RGB guidance from a monocular camera in order to leverage object information and to correct mistakes in the sparse input. This improves the accuracy significantly. Moreover, confidence masks are exploited in order to take into account the uncertainty in the depth predictions from each modality. This fusion method outperforms the state-of-the-art and ranks first on the KITTI depth completion benchmark. Our code with visualizations is available.



### Channel Max Pooling Layer for Fine-Grained Vehicle Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.11107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11107v2)
- **Published**: 2019-02-14 14:47:04+00:00
- **Updated**: 2020-01-25 12:01:36+00:00
- **Authors**: Zhanyu Ma, Dongliang Chang, Xiaoxu Li
- **Comment**: Report of ongoing work
- **Journal**: None
- **Summary**: Deep convolutional networks have recently shown excellent performance on Fine-Grained Vehicle Classification. Based on these existing works, we consider that the back-probation algorithm does not focus on extracting less discriminative feature as much as possible, but focus on that the loss function equals zero. Intuitively, if we can learn less discriminative features, and these features still could fit the training data well, the generalization ability of neural network could be improved. Therefore, we propose a new layer which is placed between fully connected layers and convolutional layers, called as Chanel Max Pooling. The proposed layer groups the features map first and then compress each group into a new feature map by computing maximum of pixels with same positions in the group of feature maps. Meanwhile, the proposed layer has an advantage that it could help neural network reduce massive parameters. Experimental results on two fine-grained vehicle datasets, the Stanford Cars-196 dataset and the Comp Cars dataset, demonstrate that the proposed layer could improve classification accuracies of deep neural networks on fine-grained vehicle classification in the situation that a massive of parameters are reduced. Moreover, it has a competitive performance with the-state-of-art performance on the two datasets.



### Exploring Frame Segmentation Networks for Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1902.05488v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05488v1)
- **Published**: 2019-02-14 16:46:34+00:00
- **Updated**: 2019-02-14 16:46:34+00:00
- **Authors**: Ke Yang, Xiaolong Shen, Peng Qiao, Shijie Li, Dongsheng Li, Yong Dou
- **Comment**: Accepted by Journal of Visual Communication and Image Representation
- **Journal**: None
- **Summary**: Temporal action localization is an important task of computer vision. Though many methods have been proposed, it still remains an open question how to predict the temporal location of action segments precisely. Most state-of-the-art works train action classifiers on video segments pre-determined by action proposal. However, recent work found that a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. In this paper, we propose a Frame Segmentation Network (FSN) that places a temporal CNN on top of the 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in spatial dimension while temporal CNN is responsible for introducing temporal context information and performing dense predictions. The proposed FSN can make dense predictions at frame-level for a video clip using both spatial and temporal context information. FSN is trained in an end-to-end manner, so the model can be optimized in spatial and temporal domain jointly. We also adapt FSN to use it in weakly supervised scenario (WFSN), where only video level labels are provided when training. Experiment results on public dataset show that FSN achieves superior performance in both frame-level action localization and temporal action localization.



### Integrating Propositional and Relational Label Side Information for Hierarchical Zero-Shot Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.05492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05492v1)
- **Published**: 2019-02-14 16:54:19+00:00
- **Updated**: 2019-02-14 16:54:19+00:00
- **Authors**: Colin Samplawski, Heesung Kwon, Erik Learned-Miller, Benjamin M. Marlin
- **Comment**: None
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) is one of the most extreme forms of learning from scarce labeled data. It enables predicting that images belong to classes for which no labeled training instances are available. In this paper, we present a new ZSL framework that leverages both label attribute side information and a semantic label hierarchy. We present two methods, lifted zero-shot prediction and a custom conditional random field (CRF) model, that integrate both forms of side information. We propose benchmark tasks for this framework that focus on making predictions across a range of semantic levels. We show that lifted zero-shot prediction can dramatically outperform baseline methods when making predictions within specified semantic levels, and that the probability distribution provided by the CRF model can be leveraged to yield further performance improvements when making unconstrained predictions over the hierarchy.



### MultiGrain: a unified image embedding for classes and instances
- **Arxiv ID**: http://arxiv.org/abs/1902.05509v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05509v2)
- **Published**: 2019-02-14 17:32:21+00:00
- **Updated**: 2019-04-03 08:07:29+00:00
- **Authors**: Maxim Berman, Hervé Jégou, Andrea Vedaldi, Iasonas Kokkinos, Matthijs Douze
- **Comment**: None
- **Journal**: None
- **Summary**: MultiGrain is a network architecture producing compact vector representations that are suited both for image classification and particular object retrieval. It builds on a standard classification trunk. The top of the network produces an embedding containing coarse and fine-grained information, so that images can be recognized based on the object class, particular object, or if they are distorted copies. Our joint training is simple: we minimize a cross-entropy loss for classification and a ranking loss that determines if two images are identical up to data augmentation, with no need for additional labels. A key component of MultiGrain is a pooling layer that takes advantage of high-resolution images with a network trained at a lower resolution.   When fed to a linear classifier, the learned embeddings provide state-of-the-art classification accuracy. For instance, we obtain 79.4% top-1 accuracy with a ResNet-50 learned on Imagenet, which is a +1.8% absolute improvement over the AutoAugment method. When compared with the cosine similarity, the same embeddings perform on par with the state-of-the-art for image retrieval at moderate resolutions.



### Deep Generative Endmember Modeling: An Application to Unsupervised Spectral Unmixing
- **Arxiv ID**: http://arxiv.org/abs/1902.05528v3
- **DOI**: 10.1109/TCI.2019.2948726
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05528v3)
- **Published**: 2019-02-14 18:08:22+00:00
- **Updated**: 2019-11-21 15:24:26+00:00
- **Authors**: Ricardo Augusto Borsoi, Tales Imbiriba, José Carlos Moreira Bermudez
- **Comment**: None
- **Journal**: None
- **Summary**: Endmember (EM) spectral variability can greatly impact the performance of standard hyperspectral image analysis algorithms. Extended parametric models have been successfully applied to account for the EM spectral variability. However, these models still lack the compromise between flexibility and low-dimensional representation that is necessary to properly explore the fact that spectral variability is often confined to a low-dimensional manifold in real scenes. In this paper we propose to learn a spectral variability model directly form the observed data, instead of imposing it \emph{a priori}. This is achieved through a deep generative EM model, which is estimated using a variational autoencoder (VAE). The encoder and decoder that compose the generative model are trained using pure pixel information extracted directly from the observed image, what allows for an unsupervised formulation. The proposed EM model is applied to the solution of a spectral unmixing problem, which we cast as an alternating nonlinear least-squares problem that is solved iteratively with respect to the abundances and to the low-dimensional representations of the EMs in the latent space of the deep generative model. Simulations using both synthetic and real data indicate that the proposed strategy can outperform the competing state-of-the-art algorithms.



### Unsupervised Visuomotor Control through Distributional Planning Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.05542v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05542v1)
- **Published**: 2019-02-14 18:54:54+00:00
- **Updated**: 2019-02-14 18:54:54+00:00
- **Authors**: Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, Chelsea Finn
- **Comment**: Videos available at https://sites.google.com/view/dpn-public/
- **Journal**: None
- **Summary**: While reinforcement learning (RL) has the potential to enable robots to autonomously acquire a wide range of skills, in practice, RL usually requires manual, per-task engineering of reward functions, especially in real world settings where aspects of the environment needed to compute progress are not directly accessible. To enable robots to autonomously learn skills, we instead consider the problem of reinforcement learning without access to rewards. We aim to learn an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our approach explicitly optimizes for a metric space under which action sequences that reach a particular state are optimal when the goal is the final state reached. This enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms. Our experiments on three simulated environments and two real-world manipulation problems show that our method can learn effective goal metrics from unlabeled interaction, and use the learned goal metrics for autonomous reinforcement learning.



### Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity
- **Arxiv ID**: http://arxiv.org/abs/1902.05546v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05546v2)
- **Published**: 2019-02-14 18:59:05+00:00
- **Updated**: 2019-11-21 21:35:27+00:00
- **Authors**: Deepak Pathak, Chris Lu, Trevor Darrell, Phillip Isola, Alexei A. Efros
- **Comment**: NeurIPS 2019 (Spotlight). Videos at
  https://pathak22.github.io/modular-assemblies/
- **Journal**: None
- **Summary**: Contemporary sensorimotor learning approaches typically start with an existing complex agent (e.g., a robotic arm), which they learn to control. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to dynamically self-assemble into composite bodies while also learning to coordinate their behavior to control these bodies. Each primitive agent consists of a limb with a motor attached at one end. Limbs may choose to link up to form collectives. When a limb initiates a link-up action, and there is another limb nearby, the latter is magnetically connected to the 'parent' limb's motor. This forms a new single agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. We evaluate the performance of these dynamic and modular agents in simulated environments. We demonstrate better generalization to test-time changes both in the environment, as well as in the structure of the agent, compared to static and monolithic baselines. Project video and code are available at https://pathak22.github.io/modular-assemblies/



### Adversarially Approximated Autoencoder for Image Generation and Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1902.05581v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05581v1)
- **Published**: 2019-02-14 19:54:13+00:00
- **Updated**: 2019-02-14 19:54:13+00:00
- **Authors**: Wenju Xu, Shawn Keshmiri, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Regularized autoencoders learn the latent codes, a structure with the regularization under the distribution, which enables them the capability to infer the latent codes given observations and generate new samples given the codes. However, they are sometimes ambiguous as they tend to produce reconstructions that are not necessarily faithful reproduction of the inputs. The main reason is to enforce the learned latent code distribution to match a prior distribution while the true distribution remains unknown. To improve the reconstruction quality and learn the latent space a manifold structure, this work present a novel approach using the adversarially approximated autoencoder (AAAE) to investigate the latent codes with adversarial approximation. Instead of regularizing the latent codes by penalizing on the distance between the distributions of the model and the target, AAAE learns the autoencoder flexibly and approximates the latent space with a simpler generator. The ratio is estimated using generative adversarial network (GAN) to enforce the similarity of the distributions. Additionally, the image space is regularized with an additional adversarial regularizer. The proposed approach unifies two deep generative models for both latent space inference and diverse generation. The learning scheme is realized without regularization on the latent codes, which also encourages faithful reconstruction. Extensive validation experiments on four real-world datasets demonstrate the superior performance of AAAE. In comparison to the state-of-the-art approaches, AAAE generates samples with better quality and shares the properties of regularized autoencoder with a nice latent manifold structure.



### Improving Catheter Segmentation & Localization in 3D Cardiac Ultrasound Using Direction-Fused FCN
- **Arxiv ID**: http://arxiv.org/abs/1902.05582v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05582v1)
- **Published**: 2019-02-14 19:57:14+00:00
- **Updated**: 2019-02-14 19:57:14+00:00
- **Authors**: Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With
- **Comment**: ISBI 2019 accepted
- **Journal**: None
- **Summary**: Fast and accurate catheter detection in cardiac catheterization using harmless 3D ultrasound (US) can improve the efficiency and outcome of the intervention. However, the low image quality of US requires extra training for sonographers to localize the catheter. In this paper, we propose a catheter detection method based on a pre-trained VGG network, which exploits 3D information through re-organized cross-sections to segment the catheter by a shared fully convolutional network (FCN), which is called a Direction-Fused FCN (DF-FCN). Based on the segmented image of DF-FCN, the catheter can be localized by model fitting. Our experiments show that the proposed method can successfully detect an ablation catheter in a challenging ex-vivo 3D US dataset, which was collected on the porcine heart. Extensive analysis shows that the proposed method achieves a Dice score of 57.7%, which offers at least an 11.8 % improvement when compared to state-of-the-art instrument detection methods. Due to the improved segmentation performance by the DF-FCN, the catheter can be localized with an error of only 1.4 mm.



### Can Intelligent Hyperparameter Selection Improve Resistance to Adversarial Examples?
- **Arxiv ID**: http://arxiv.org/abs/1902.05586v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML, I.5.2; K.6.5
- **Links**: [PDF](http://arxiv.org/pdf/1902.05586v1)
- **Published**: 2019-02-14 20:11:12+00:00
- **Updated**: 2019-02-14 20:11:12+00:00
- **Authors**: Cody Burkard, Brent Lagesse
- **Comment**: 37 pages, 11 figures, 9 tables
- **Journal**: None
- **Summary**: Convolutional Neural Networks and Deep Learning classification systems in general have been shown to be vulnerable to attack by specially crafted data samples that appear to belong to one class but are instead classified as another, commonly known as adversarial examples. A variety of attack strategies have been proposed to craft these samples; however, there is no standard model that is used to compare the success of each type of attack. Furthermore, there is no literature currently available that evaluates how common hyperparameters and optimization strategies may impact a model's ability to resist these samples. This research bridges that lack of awareness and provides a means for the selection of training and model parameters in future research on evasion attacks against convolutional neural networks. The findings of this work indicate that the selection of model hyperparameters does impact the ability of a model to resist attack, although they alone cannot prevent the existence of adversarial examples.



### GeoGAN: A Conditional GAN with Reconstruction and Style Loss to Generate Standard Layer of Maps from Satellite Images
- **Arxiv ID**: http://arxiv.org/abs/1902.05611v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05611v2)
- **Published**: 2019-02-14 21:41:18+00:00
- **Updated**: 2019-04-25 18:54:53+00:00
- **Authors**: Swetava Ganguli, Pedro Garzon, Noa Glaser
- **Comment**: Version 2 of paper submitted incorporating minor revisions. Corrected
  typographical errors and added some additional references
- **Journal**: None
- **Summary**: Automatically generating maps from satellite images is an important task. There is a body of literature which tries to address this challenge. We created a more expansive survey of the task by experimenting with different models and adding new loss functions to improve results. We created a database of pairs of satellite images and the corresponding map of the area. Our model translates the satellite image to the corresponding standard layer map image using three main model architectures: (i) a conditional Generative Adversarial Network (GAN) which compresses the images down to a learned embedding, (ii) a generator which is trained as a normalizing flow (RealNVP) model, and (iii) a conditional GAN where the generator translates via a series of convolutions to the standard layer of a map and the discriminator input is the concatenation of the real/generated map and the satellite image. Model (iii) was by far the most promising of three models. To improve the results we also added a reconstruction loss and style transfer loss in addition to the GAN losses. The third model architecture produced the best quality of sampled images. In contrast to the other generative model where evaluation of the model is a challenging problem. since we have access to the real map for a given satellite image, we are able to assign a quantitative metric to the quality of the generated images in addition to inspecting them visually. While we are continuing to work on increasing the accuracy of the model, one challenge has been the coarse resolution of the data which upper-bounds the quality of the results of our model. Nevertheless, as will be seen in the results, the generated map is more accurate in the features it produces since the generator architecture demands a pixel-wise image translation/pixel-wise coloring. A video presentation summarizing this paper is available at: https://youtu.be/Ur0flOX-Ji0



