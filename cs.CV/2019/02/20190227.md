# Arxiv Papers in cs.CV on 2019-02-27
### Zero-shot Learning of 3D Point Cloud Objects
- **Arxiv ID**: http://arxiv.org/abs/1902.10272v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10272v1)
- **Published**: 2019-02-27 00:15:31+00:00
- **Updated**: 2019-02-27 00:15:31+00:00
- **Authors**: Ali Cheraghian, Shafin Rahman, Lars Petersson
- **Comment**: None
- **Journal**: International Conference on Machine Vision Applications (MVA)
  (2019)
- **Summary**: Recent deep learning architectures can recognize instances of 3D point cloud objects of previously seen classes quite well. At the same time, current 3D depth camera technology allows generating/segmenting a large amount of 3D point cloud objects from an arbitrary scene, for which there is no previously seen training data. A challenge for a 3D point cloud recognition system is, then, to classify objects from new, unseen, classes. This issue can be resolved by adopting a zero-shot learning (ZSL) approach for 3D data, similar to the 2D image version of the same problem. ZSL attempts to classify unseen objects by comparing semantic information (attribute/word vector) of seen and unseen classes. Here, we adapt several recent 3D point cloud recognition systems to the ZSL setting with some changes to their architectures. To the best of our knowledge, this is the first attempt to classify unseen 3D point cloud objects in the ZSL setting. A standard protocol (which includes the choice of datasets and the seen/unseen split) to evaluate such systems is also proposed. Baseline performances are reported using the new protocol on the investigated models. This investigation throws a new challenge to the 3D point cloud recognition community that may instigate numerous future works.



### Non-Rigid Structure from Motion: Prior-Free Factorization Method Revisited
- **Arxiv ID**: http://arxiv.org/abs/1902.10274v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10274v5)
- **Published**: 2019-02-27 00:20:37+00:00
- **Updated**: 2019-12-21 11:19:56+00:00
- **Authors**: Suryansh Kumar
- **Comment**: Accepted for publication in IEEE, WACV 2020
- **Journal**: None
- **Summary**: A simple prior free factorization algorithm \cite{dai2014simple} is quite often cited work in the field of Non-Rigid Structure from Motion (NRSfM). The benefit of this work lies in its simplicity of implementation, strong theoretical justification to the motion and structure estimation, and its invincible originality. Despite this, the prevailing view is, that it performs exceedingly inferior to other methods on several benchmark datasets \cite{jensen2018benchmark,akhter2009nonrigid}. However, our subtle investigation provides some empirical statistics which made us think against such views. The statistical results we obtained supersedes Dai {\it{et al.}}\cite{dai2014simple} originally reported results on the benchmark datasets by a significant margin under some elementary changes in their core algorithmic idea \cite{dai2014simple}. Now, these results not only exposes some unrevealed areas for research in NRSfM but also give rise to new mathematical challenges for NRSfM researchers. We argue that by \textbf{properly} utilizing the well-established assumptions about a non-rigidly deforming shape i.e, it deforms smoothly over frames \cite{rabaud2008re} and it spans a low-rank space, the simple prior-free idea can provide results which is comparable to the best available algorithms. In this paper, we explore some of the hidden intricacies missed by Dai {\it{et. al.}} work \cite{dai2014simple} and how some elementary measures and modifications can enhance its performance, as high as approx. 18\% on the benchmark dataset. The improved performance is justified and empirically verified by extensive experiments on several datasets. We believe our work has both practical and theoretical importance for the development of better NRSfM algorithms.



### Aurora Guard: Real-Time Face Anti-Spoofing via Light Reflection
- **Arxiv ID**: http://arxiv.org/abs/1902.10311v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10311v1)
- **Published**: 2019-02-27 02:29:17+00:00
- **Updated**: 2019-02-27 02:29:17+00:00
- **Authors**: Yao Liu, Ying Tai, Jilin Li, Shouhong Ding, Chengjie Wang, Feiyue Huang, Dongyang Li, Wenshuai Qi, Rongrong Ji
- **Comment**: Technical report that introduces our face anti-spoofing system:
  Aurora Guard. Simple demo video can be found at:
  https://drive.google.com/file/d/1wLXwGvy2zsh5xkDhRCPNzQXGUihZNq1g/view?usp=sharing
- **Journal**: None
- **Summary**: In this paper, we propose a light reflection based face anti-spoofing method named Aurora Guard (AG), which is fast, simple yet effective that has already been deployed in real-world systems serving for millions of users. Specifically, our method first extracts the normal cues via light reflection analysis, and then uses an end-to-end trainable multi-task Convolutional Neural Network (CNN) to not only recover subjects' depth maps to assist liveness classification, but also provide the light CAPTCHA checking mechanism in the regression branch to further improve the system reliability. Moreover, we further collect a large-scale dataset containing $12,000$ live and spoofing samples, which covers abundant imaging qualities and Presentation Attack Instruments (PAI). Extensive experiments on both public and our datasets demonstrate the superiority of our proposed method over the state of the arts.



### FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.11128v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11128v1)
- **Published**: 2019-02-27 02:42:33+00:00
- **Updated**: 2019-02-27 02:42:33+00:00
- **Authors**: Paul N. Whatmough, Chuteng Zhou, Patrick Hansen, Shreyas Kolala Venkataramanaiah, Jae-sun Seo, Matthew Mattina
- **Comment**: 10 pages, 8 figures, paper accepted at SysML2019 conference
- **Journal**: None
- **Summary**: The computational demands of computer vision tasks based on state-of-the-art Convolutional Neural Network (CNN) image classification far exceed the energy budgets of mobile devices. This paper proposes FixyNN, which consists of a fixed-weight feature extractor that generates ubiquitous CNN features, and a conventional programmable CNN accelerator which processes a dataset-specific CNN. Image classification models for FixyNN are trained end-to-end via transfer learning, with the common feature extractor representing the transfered part, and the programmable part being learnt on the target dataset. Experimental results demonstrate FixyNN hardware can achieve very high energy efficiencies up to 26.6 TOPS/W ($4.81 \times$ better than iso-area programmable accelerator). Over a suite of six datasets we trained models via transfer learning with an accuracy loss of $<1\%$ resulting in up to 11.2 TOPS/W - nearly $2 \times$ more efficient than a conventional programmable CNN accelerator of the same area.



### Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1902.10322v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10322v2)
- **Published**: 2019-02-27 03:52:08+00:00
- **Updated**: 2019-04-29 05:20:00+00:00
- **Authors**: Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, Ajmal Mian
- **Comment**: Accepted in CVPR-2019 (Camera Ready)
- **Journal**: None
- **Summary**: Automatic generation of video captions is a fundamental challenge in computer vision. Recent techniques typically employ a combination of Convolutional Neural Networks (CNNs) and Recursive Neural Networks (RNNs) for video captioning. These methods mainly focus on tailoring sequence learning through RNNs for better caption generation, whereas off-the-shelf visual features are borrowed from CNNs. We argue that careful designing of visual features for this task is equally important, and present a visual feature encoding technique to generate semantically rich captions using Gated Recurrent Units (GRUs). Our method embeds rich temporal dynamics in visual features by hierarchically applying Short Fourier Transform to CNN features of the whole video. It additionally derives high level semantics from an object detector to enrich the representation with spatial dynamics of the detected objects. The final representation is projected to a compact space and fed to a language model. By learning a relatively simple language model comprising two GRU layers, we establish new state-of-the-art on MSVD and MSR-VTT datasets for METEOR and ROUGE_L metrics.



### Disentangled Deep Autoencoding Regularization for Robust Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.11134v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11134v1)
- **Published**: 2019-02-27 04:49:57+00:00
- **Updated**: 2019-02-27 04:49:57+00:00
- **Authors**: Zhenyu Duan, Martin Renqiang Min, Li Erran Li, Mingbo Cai, Yi Xu, Bingbing Ni
- **Comment**: 9 pages
- **Journal**: None
- **Summary**: In spite of achieving revolutionary successes in machine learning, deep convolutional neural networks have been recently found to be vulnerable to adversarial attacks and difficult to generalize to novel test images with reasonably large geometric transformations. Inspired by a recent neuroscience discovery revealing that primate brain employs disentangled shape and appearance representations for object recognition, we propose a general disentangled deep autoencoding regularization framework that can be easily applied to any deep embedding based classification model for improving the robustness of deep neural networks. Our framework effectively learns disentangled appearance code and geometric code for robust image classification, which is the first disentangling based method defending against adversarial attacks and complementary to standard defense methods. Extensive experiments on several benchmark datasets show that, our proposed regularization framework leveraging disentangled embedding significantly outperforms traditional unregularized convolutional neural networks for image classification on robustness against adversarial attacks and generalization to novel test data.



### On the generalization of GAN image forensics
- **Arxiv ID**: http://arxiv.org/abs/1902.11153v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11153v2)
- **Published**: 2019-02-27 07:05:38+00:00
- **Updated**: 2019-12-10 08:19:47+00:00
- **Authors**: Xinsheng Xuan, Bo Peng, Wei Wang, Jing Dong
- **Comment**: None
- **Journal**: None
- **Summary**: Recently the GAN generated face images are more and more realistic with high-quality, even hard for human eyes to detect. On the other hand, the forensics community keeps on developing methods to detect these generated fake images and try to guarantee the credibility of visual contents. Although researchers have developed some methods to detect generated images, few of them explore the important problem of generalization ability of forensics model. As new types of GANs are emerging fast, the generalization ability of forensics models to detect new types of GAN images is absolutely an essential research topic. In this paper, we explore this problem and propose to use preprocessed images to train a forensic CNN model. By applying similar image level preprocessing to both real and fake training images, the forensics model is forced to learn more intrinsic features to classify the generated and real face images. Our experimental results also prove the effectiveness of the proposed method.



### The Importance of Metric Learning for Robotic Vision: Open Set Recognition and Active Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.10363v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10363v1)
- **Published**: 2019-02-27 07:18:24+00:00
- **Updated**: 2019-02-27 07:18:24+00:00
- **Authors**: Benjamin J. Meyer, Tom Drummond
- **Comment**: Accepted to the 2019 IEEE International Conference on Robotics and
  Automation (ICRA)
- **Journal**: None
- **Summary**: State-of-the-art deep neural network recognition systems are designed for a static and closed world. It is usually assumed that the distribution at test time will be the same as the distribution during training. As a result, classifiers are forced to categorise observations into one out of a set of predefined semantic classes. Robotic problems are dynamic and open world; a robot will likely observe objects that are from outside of the training set distribution. Classifier outputs in robotic applications can lead to real-world robotic action and as such, a practical recognition system should not silently fail by confidently misclassifying novel observations. We show how a deep metric learning classification system can be applied to such open set recognition problems, allowing the classifier to label novel observations as unknown. Further to detecting novel examples, we propose an open set active learning approach that allows a robot to efficiently query a user about unknown observations. Our approach enables a robot to improve its understanding of the true distribution of data in the environment, from a small number of label queries. Experimental results show that our approach significantly outperforms comparable methods in both the open set recognition and active learning problems.



### Multi-loss-aware Channel Pruning of Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.10364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10364v1)
- **Published**: 2019-02-27 07:22:24+00:00
- **Updated**: 2019-02-27 07:22:24+00:00
- **Authors**: Yiming Hu, Siyang Sun, Jianquan Li, Jiagang Zhu, Xingang Wang, Qingyi Gu
- **Comment**: 4 pages, 2 figures
- **Journal**: IEEE ICIP 2019
- **Summary**: Channel pruning, which seeks to reduce the model size by removing redundant channels, is a popular solution for deep networks compression. Existing channel pruning methods usually conduct layer-wise channel selection by directly minimizing the reconstruction error of feature maps between the baseline model and the pruned one. However, they ignore the feature and semantic distributions within feature maps and real contribution of channels to the overall performance. In this paper, we propose a new channel pruning method by explicitly using both intermediate outputs of the baseline model and the classification loss of the pruned model to supervise layer-wise channel selection. Particularly, we introduce an additional loss to encode the differences in the feature and semantic distributions within feature maps between the baseline model and the pruned one. By considering the reconstruction error, the additional loss and the classification loss at the same time, our approach can significantly improve the performance of the pruned model. Comprehensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method.



### Cluster Regularized Quantization for Deep Networks Compression
- **Arxiv ID**: http://arxiv.org/abs/1902.10370v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10370v1)
- **Published**: 2019-02-27 07:40:19+00:00
- **Updated**: 2019-02-27 07:40:19+00:00
- **Authors**: Yiming Hu, Jianquan Li, Xianlei Long, Shenhua Hu, Jiagang Zhu, Xingang Wang, Qingyi Gu
- **Comment**: 4 pages, 1 figure
- **Journal**: IEEE ICIP 2019
- **Summary**: Deep neural networks (DNNs) have achieved great success in a wide range of computer vision areas, but the applications to mobile devices is limited due to their high storage and computational cost. Much efforts have been devoted to compress DNNs. In this paper, we propose a simple yet effective method for deep networks compression, named Cluster Regularized Quantization (CRQ), which can reduce the presentation precision of a full-precision model to ternary values without significant accuracy drop. In particular, the proposed method aims at reducing the quantization error by introducing a cluster regularization term, which is imposed on the full-precision weights to enable them naturally concentrate around the target values. Through explicitly regularizing the weights during the re-training stage, the full-precision model can achieve the smooth transition to the low-bit one. Comprehensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method.



### Hypernetwork functional image representation
- **Arxiv ID**: http://arxiv.org/abs/1902.10404v3
- **DOI**: 10.1007/978-3-030-30493-5_48
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10404v3)
- **Published**: 2019-02-27 09:12:29+00:00
- **Updated**: 2019-06-03 13:11:41+00:00
- **Authors**: Sylwester Klocek, Łukasz Maziarka, Maciej Wołczyk, Jacek Tabor, Jakub Nowak, Marek Śmieja
- **Comment**: None
- **Journal**: Artificial Neural Networks and Machine Learning -- ICANN 2019:
  Workshop and Special Sessions
- **Summary**: Motivated by the human way of memorizing images we introduce their functional representation, where an image is represented by a neural network. For this purpose, we construct a hypernetwork which takes an image and returns weights to the target network, which maps point from the plane (representing positions of the pixel) into its corresponding color in the image. Since the obtained representation is continuous, one can easily inspect the image at various resolutions and perform on it arbitrary continuous operations. Moreover, by inspecting interpolations we show that such representation has some properties characteristic to generative models. To evaluate the proposed mechanism experimentally, we apply it to image super-resolution problem. Despite using a single model for various scaling factors, we obtained results comparable to existing super-resolution methods.



### Computing Nonlinear Eigenfunctions via Gradient Flow Extinction
- **Arxiv ID**: http://arxiv.org/abs/1902.10414v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, 35P10, 35P30, 62H30, 68T10
- **Links**: [PDF](http://arxiv.org/pdf/1902.10414v1)
- **Published**: 2019-02-27 09:50:38+00:00
- **Updated**: 2019-02-27 09:50:38+00:00
- **Authors**: Leon Bungert, Martin Burger, Daniel Tenbrinck
- **Comment**: 12 pages, 5 figure, accepted for publication in SSVM conference
  proceedings 2019
- **Journal**: None
- **Summary**: In this work we investigate the computation of nonlinear eigenfunctions via the extinction profiles of gradient flows. We analyze a scheme that recursively subtracts such eigenfunctions from given data and show that this procedure yields a decomposition of the data into eigenfunctions in some cases as the 1-dimensional total variation, for instance. We discuss results of numerical experiments in which we use extinction profiles and the gradient flow for the task of spectral graph clustering as used, e.g., in machine learning applications.



### Equi-normalization of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.10416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10416v1)
- **Published**: 2019-02-27 09:52:43+00:00
- **Updated**: 2019-02-27 09:52:43+00:00
- **Authors**: Pierre Stock, Benjamin Graham, Rémi Gribonval, Hervé Jégou
- **Comment**: ICLR 2019 camera-ready
- **Journal**: None
- **Summary**: Modern neural networks are over-parametrized. In particular, each rectified linear hidden unit can be modified by a multiplicative factor by adjusting input and output weights, without changing the rest of the network. Inspired by the Sinkhorn-Knopp algorithm, we introduce a fast iterative method for minimizing the L2 norm of the weights, equivalently the weight decay regularizer. It provably converges to a unique solution. Interleaving our algorithm with SGD during training improves the test accuracy. For small batches, our approach offers an alternative to batch-and group-normalization on CIFAR-10 and ImageNet with a ResNet-18.



### FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference
- **Arxiv ID**: http://arxiv.org/abs/1902.10421v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10421v2)
- **Published**: 2019-02-27 09:59:21+00:00
- **Updated**: 2019-03-02 02:55:16+00:00
- **Authors**: Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon
- **Comment**: To appear in CVPR 2019
- **Journal**: None
- **Summary**: The main obstacle to weakly supervised semantic image segmentation is the difficulty of obtaining pixel-level information from coarse image-level annotations. Most methods based on image-level annotations use localization maps obtained from the classifier, but these only focus on the small discriminative parts of objects and do not capture precise boundaries. FickleNet explores diverse combinations of locations on feature maps created by generic deep neural networks. It selects hidden units randomly and then uses them to obtain activation scores for image classification. FickleNet implicitly learns the coherence of each location in the feature maps, resulting in a localization map which identifies both discriminative and other parts of objects. The ensemble effects are obtained from a single network by selecting random hidden unit pairs, which means that a variety of localization maps are generated from a single image. Our approach does not require any additional training steps and only adds a simple layer to a standard convolutional neural network; nevertheless it outperforms recent comparable techniques on the Pascal VOC 2012 benchmark in both weakly and semi-supervised settings.



### Single-frame Regularization for Temporally Stable CNNs
- **Arxiv ID**: http://arxiv.org/abs/1902.10424v2
- **DOI**: 10.1109/CVPR.2019.01143
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10424v2)
- **Published**: 2019-02-27 10:01:49+00:00
- **Updated**: 2019-06-08 05:39:31+00:00
- **Authors**: Gabriel Eilertsen, Rafał K. Mantiuk, Jonas Unger
- **Comment**: Project web: http://hdrv.org/hdrcnn/cvpr2019/
- **Journal**: The IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2019
- **Summary**: Convolutional neural networks (CNNs) can model complicated non-linear relations between images. However, they are notoriously sensitive to small changes in the input. Most CNNs trained to describe image-to-image mappings generate temporally unstable results when applied to video sequences, leading to flickering artifacts and other inconsistencies over time. In order to use CNNs for video material, previous methods have relied on estimating dense frame-to-frame motion information (optical flow) in the training and/or the inference phase, or by exploring recurrent learning structures. We take a different approach to the problem, posing temporal stability as a regularization of the cost function. The regularization is formulated to account for different types of motion that can occur between frames, so that temporally stable CNNs can be trained without the need for video material or expensive motion estimation. The training can be performed as a fine-tuning operation, without architectural modifications of the CNN. Our evaluation shows that the training strategy leads to large improvements in temporal smoothness. Moreover, for small datasets the regularization can help in boosting the generalization performance to a much larger extent than what is possible with na\"ive augmentation strategies.



### StyleRemix: An Interpretable Representation for Neural Image Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1902.10425v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10425v3)
- **Published**: 2019-02-27 10:02:14+00:00
- **Updated**: 2019-03-25 02:40:12+00:00
- **Authors**: Hongmin Xu, Qiang Li, Wenbo Zhang, Wen Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Style Transfer (MST) intents to capture the high-level visual vocabulary of different styles and expresses these vocabularies in a joint model to transfer each specific style. Recently, Style Embedding Learning (SEL) based methods represent each style with an explicit set of parameters to perform MST task. However, most existing SEL methods either learn explicit style representation with numerous independent parameters or learn a relatively black-box style representation, which makes them difficult to control the stylized results. In this paper, we outline a novel MST model, StyleRemix, to compactly and explicitly integrate multiple styles into one network. By decomposing diverse styles into the same basis, StyleRemix represents a specific style in a continuous vector space with 1-dimensional coefficients. With the interpretable style representation, StyleRemix not only enables the style visualization task but also allows several ways of remixing styles in the smooth style embedding space.~Extensive experiments demonstrate the effectiveness of StyleRemix on various MST tasks compared to state-of-the-art SEL approaches.



### Fix Your Features: Stationary and Maximally Discriminative Embeddings using Regular Polytope (Fixed Classifier) Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.10441v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10441v2)
- **Published**: 2019-02-27 10:33:46+00:00
- **Updated**: 2019-03-01 09:45:22+00:00
- **Authors**: Federico Pernici, Matteo Bruni, Claudio Baecchi, Alberto Del Bimbo
- **Comment**: None
- **Journal**: None
- **Summary**: Neural networks are widely used as a model for classification in a large variety of tasks. Typically, a learnable transformation (i.e. the classifier) is placed at the end of such models returning a value for each class used for classification. This transformation plays an important role in determining how the generated features change during the learning process.   In this work we argue that this transformation not only can be fixed (i.e. set as non trainable) with no loss of accuracy, but it can also be used to learn stationary and maximally discriminative embeddings.   We show that the stationarity of the embedding and its maximal discriminative representation can be theoretically justified by setting the weights of the fixed classifier to values taken from the coordinate vertices of three regular polytopes available in $\mathbb{R}^d$, namely: the $d$-Simplex, the $d$-Cube and the $d$-Orthoplex. These regular polytopes have the maximal amount of symmetry that can be exploited to generate stationary features angularly centered around their corresponding fixed weights.   Our approach improves and broadens the concept of a fixed classifier, recently proposed in \cite{hoffer2018fix}, to a larger class of fixed classifier models. Experimental results confirm both the theoretical analysis and the generalization capability of the proposed method.



### Modulated binary cliquenet
- **Arxiv ID**: http://arxiv.org/abs/1902.10460v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10460v1)
- **Published**: 2019-02-27 11:14:01+00:00
- **Updated**: 2019-02-27 11:14:01+00:00
- **Authors**: Jinpeng Xia, Jiasong Wu, Youyong Kong, Pinzheng Zhang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 5 pages, 3 figures, 2 tables
- **Journal**: None
- **Summary**: Although Convolutional Neural Networks (CNNs) achieve effectiveness in various computer vision tasks, the significant requirement of storage of such networks hinders the deployment on computationally limited devices. In this paper, we propose a new compact and portable deep learning network named Modulated Binary Cliquenet (MBCliqueNet) aiming to improve the portability of CNNs based on binarized filters while achieving comparable performance with the full-precision CNNs like Resnet. In MBCliqueNet, we introduce a novel modulated operation to approximate the unbinarized filters and gives an initialization method to speed up its convergence. We reduce the extra parameters caused by modulated operation with parameters sharing. As a result, the proposed MBCliqueNet can reduce the required storage space of convolutional filters by a factor of at least 32, in contrast to the full-precision model, and achieve better performance than other state-of-the-art binarized models. More importantly, our model compares even better with some full-precision models like Resnet on the dataset we used.



### Flash Lightens Gray Pixels
- **Arxiv ID**: http://arxiv.org/abs/1902.10466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10466v1)
- **Published**: 2019-02-27 11:30:13+00:00
- **Updated**: 2019-02-27 11:30:13+00:00
- **Authors**: Yanlin Qian, Song Yan, Joni-Kristian Kämäräinen, Jiri Matas
- **Comment**: 5 pages including refs, 4 figures, submitted to International
  Conference on Image Processing
- **Journal**: None
- **Summary**: In the real world, a scene is usually cast by multiple illuminants and herein we address the problem of spatial illumination estimation. Our solution is based on detecting gray pixels with the help of flash photography. We show that flash photography significantly improves the performance of gray pixel detection without illuminant prior, training data or calibration of the flash. We also introduce a novel flash photography dataset generated from the MIT intrinsic dataset.



### Generative Collaborative Networks for Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1902.10467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10467v2)
- **Published**: 2019-02-27 11:34:10+00:00
- **Updated**: 2019-03-12 13:07:24+00:00
- **Authors**: Mohamed El Amine Seddik, Mohamed Tamaazousti, John Lin
- **Comment**: None
- **Journal**: None
- **Summary**: A common issue of deep neural networks-based methods for the problem of Single Image Super-Resolution (SISR), is the recovery of finer texture details when super-resolving at large upscaling factors. This issue is particularly related to the choice of the objective loss function. In particular, recent works proposed the use of a VGG loss which consists in minimizing the error between the generated high resolution images and ground-truth in the feature space of a Convolutional Neural Network (VGG19), pre-trained on the very "large" ImageNet dataset. When considering the problem of super-resolving images with a distribution "far" from the ImageNet images distribution (\textit{e.g.,} satellite images), their proposed \textit{fixed} VGG loss is no longer relevant. In this paper, we present a general framework named \textit{Generative Collaborative Networks} (GCN), where the idea consists in optimizing the \textit{generator} (the mapping of interest) in the feature space of a \textit{features extractor} network. The two networks (generator and extractor) are \textit{collaborative} in the sense that the latter "helps" the former, by constructing discriminative and relevant features (not necessarily \textit{fixed} and possibly learned \textit{mutually} with the generator). We evaluate the GCN framework in the context of SISR, and we show that it results in a method that is adapted to super-resolution domains that are "far" from the ImageNet domain.



### Fractional spectral graph wavelets and their applications
- **Arxiv ID**: http://arxiv.org/abs/1902.10471v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10471v1)
- **Published**: 2019-02-27 11:47:41+00:00
- **Updated**: 2019-02-27 11:47:41+00:00
- **Authors**: Jiasong Wu, Fuzhi Wu, Qihan Yang, Youyong Kong, Xilin Liu, Yan Zhang, Lotfi Senhadji, Huazhong Shu
- **Comment**: 27 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: One of the key challenges in the area of signal processing on graphs is to design transforms and dictionaries methods to identify and exploit structure in signals on weighted graphs. In this paper, we first generalize graph Fourier transform (GFT) to graph fractional Fourier transform (GFRFT), which is then used to define a novel transform named spectral graph fractional wavelet transform (SGFRWT), which is a generalized and extended version of spectral graph wavelet transform (SGWT). A fast algorithm for SGFRWT is also derived and implemented based on Fourier series approximation. The potential applications of SGFRWT are also presented.



### Attributes-aided Part Detection and Refinement for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1902.10528v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10528v1)
- **Published**: 2019-02-27 13:51:12+00:00
- **Updated**: 2019-02-27 13:51:12+00:00
- **Authors**: Shuzhao Li, Huimin Yu, Wei Huang, Jing Zhang
- **Comment**: 10 pages, 3 figures, 3 tables
- **Journal**: None
- **Summary**: Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local features to handle the body part misalignment problem, which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.



### Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference
- **Arxiv ID**: http://arxiv.org/abs/1902.10556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10556v1)
- **Published**: 2019-02-27 14:34:50+00:00
- **Updated**: 2019-02-27 14:34:50+00:00
- **Authors**: Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: Deep learning has recently demonstrated its excellent performance for multi-view stereo (MVS). However, one major limitation of current learned MVS approaches is the scalability: the memory-consuming cost volume regularization makes the learned MVS hard to be applied to high-resolution scenes. In this paper, we introduce a scalable multi-view stereo framework based on the recurrent neural network. Instead of regularizing the entire 3D cost volume in one go, the proposed Recurrent Multi-view Stereo Network (R-MVSNet) sequentially regularizes the 2D cost maps along the depth direction via the gated recurrent unit (GRU). This reduces dramatically the memory consumption and makes high-resolution reconstruction feasible. We first show the state-of-the-art performance achieved by the proposed R-MVSNet on the recent MVS benchmarks. Then, we further demonstrate the scalability of the proposed method on several large-scale scenarios, where previous learned approaches often fail due to the memory constraint. Code is available at https://github.com/YoYo000/MVSNet.



### DeepLO: Geometry-Aware Deep LiDAR Odometry
- **Arxiv ID**: http://arxiv.org/abs/1902.10562v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10562v1)
- **Published**: 2019-02-27 14:48:42+00:00
- **Updated**: 2019-02-27 14:48:42+00:00
- **Authors**: Younggun Cho, Giseop Kim, Ayoung Kim
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Recently, learning-based ego-motion estimation approaches have drawn strong interest from studies mostly focusing on visual perception. These groundbreaking works focus on unsupervised learning for odometry estimation but mostly for visual sensors. Compared to images, a learning-based approach using Light Detection and Ranging (LiDAR) has been reported in a few studies where, most often, a supervised learning framework is proposed. In this paper, we propose a novel approach to geometry-aware deep LiDAR odometry trainable via both supervised and unsupervised frameworks. We incorporate the Iterated Closest Point (ICP) algorithm into a deep-learning framework and show the reliability of the proposed pipeline. We provide two loss functions that allow switching between supervised and unsupervised learning depending on the ground-truth validity in the training phase. An evaluation using the KITTI and Oxford RobotCar dataset demonstrates the prominent performance and efficiency of the proposed method when achieving pose accuracy.



### Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information
- **Arxiv ID**: http://arxiv.org/abs/1902.10586v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10586v2)
- **Published**: 2019-02-27 15:27:00+00:00
- **Updated**: 2019-03-06 01:40:52+00:00
- **Authors**: Jinyong Jeong, Lucas Y. Cho, Ayoung Kim
- **Comment**: 8 pages, 11 figures, calibration paper for complex urban dataset
  (http://irap.kaist.ac.kr/dataset)
- **Journal**: None
- **Summary**: This paper presents a framework for the targetless extrinsic calibration of stereo cameras and Light Detection and Ranging (LiDAR) sensors with a non-overlapping Field of View (FOV). In order to solve the extrinsic calibrations problem under such challenging configuration, the proposed solution exploits road markings as static and robust features among the various dynamic objects that are present in urban environment. First, this study utilizes road markings that are commonly captured by the two sensor modalities to select informative images for estimating the extrinsic parameters. In order to accomplish stable optimization, multiple cost functions are defined, including Normalized Information Distance (NID), edge alignment and, plane fitting cost. Therefore a smooth cost curve is formed for global optimization to prevent convergence to the local optimal point. We further evaluate each cost function by examining parameter sensitivity near the optimal point. Another key characteristic of extrinsic calibration, repeatability, is analyzed by conducting the proposed method multiple times with varying randomly perturbed initial points.



### Neural Imaging Pipelines - the Scourge or Hope of Forensics?
- **Arxiv ID**: http://arxiv.org/abs/1902.10707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1902.10707v1)
- **Published**: 2019-02-27 17:08:09+00:00
- **Updated**: 2019-02-27 17:08:09+00:00
- **Authors**: Pawel Korus, Nasir Memon
- **Comment**: Manuscript + supplement; currently under review; compressed figures
  to minimize file size. arXiv admin note: text overlap with arXiv:1812.01516
- **Journal**: None
- **Summary**: Forensic analysis of digital photographs relies on intrinsic statistical traces introduced at the time of their acquisition or subsequent editing. Such traces are often removed by post-processing (e.g., down-sampling and re-compression applied upon distribution in the Web) which inhibits reliable provenance analysis. Increasing adoption of computational methods within digital cameras further complicates the process and renders explicit mathematical modeling infeasible. While this trend challenges forensic analysis even in near-acquisition conditions, it also creates new opportunities. This paper explores end-to-end optimization of the entire image acquisition and distribution workflow to facilitate reliable forensic analysis at the end of the distribution channel, where state-of-the-art forensic techniques fail. We demonstrate that a neural network can be trained to replace the entire photo development pipeline, and jointly optimized for high-fidelity photo rendering and reliable provenance analysis. Such optimized neural imaging pipeline allowed us to increase image manipulation detection accuracy from approx. 45% to over 90%. The network learns to introduce carefully crafted artifacts, akin to digital watermarks, which facilitate subsequent manipulation detection. Analysis of performance trade-offs indicates that most of the gains can be obtained with only minor distortion. The findings encourage further research towards building more reliable imaging pipelines with explicit provenance-guaranteeing properties.



### Efficient Video Classification Using Fewer Frames
- **Arxiv ID**: http://arxiv.org/abs/1902.10640v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10640v1)
- **Published**: 2019-02-27 17:13:17+00:00
- **Updated**: 2019-02-27 17:13:17+00:00
- **Authors**: Shweta Bhardwaj, Mukundhan Srinivasan, Mitesh M. Khapra
- **Comment**: To Appear in Proceedings of IEEE International Conference on Computer
  Vision and Pattern Recognition (CVPR'2019)
- **Journal**: None
- **Summary**: Recently,there has been a lot of interest in building compact models for video classification which have a small memory footprint (<1 GB). While these models are compact, they typically operate by repeated application of a small weight matrix to all the frames in a video. E.g. recurrent neural network based methods compute a hidden state for every frame of the video using a recurrent weight matrix. Similarly, cluster-and-aggregate based methods such as NetVLAD, have a learnable clustering matrix which is used to assign soft-clusters to every frame in the video. Since these models look at every frame in the video, the number of floating point operations (FLOPs) is still large even though the memory footprint is small. We focus on building compute-efficient video classification models which process fewer frames and hence have less number of FLOPs. Similar to memory efficient models, we use the idea of distillation albeit in a different setting. Specifically, in our case, a compute-heavy teacher which looks at all the frames in the video is used to train a compute-efficient student which looks at only a small fraction of frames in the video. This is in contrast to a typical memory efficient Teacher-Student setting, wherein both the teacher and the student look at all the frames in the video but the student has fewer parameters. Our work thus complements the research on memory efficient video classification. We do an extensive evaluation with three types of models for video classification,viz.(i) recurrent models (ii) cluster-and-aggregate models and (iii) memory-efficient cluster-and-aggregate models and show that in each of these cases, a see-it-all teacher can be used to train a compute efficient see-very-little student. We show that the proposed student network can reduce the inference time by 30% and the number of FLOPs by approximately 90% with a negligible drop in the performance.



### From explanation to synthesis: Compositional program induction for learning from demonstration
- **Arxiv ID**: http://arxiv.org/abs/1902.10657v2
- **DOI**: 10.15607/RSS.2019.XV.015
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10657v2)
- **Published**: 2019-02-27 17:43:30+00:00
- **Updated**: 2019-05-29 10:04:56+00:00
- **Authors**: Michael Burke, Svetlin Penkov, Subramanian Ramamoorthy
- **Comment**: None
- **Journal**: Proceedings of Robotics: Science and Systems (2019)
- **Summary**: Hybrid systems are a compact and natural mechanism with which to address problems in robotics. This work introduces an approach to learning hybrid systems from demonstrations, with an emphasis on extracting models that are explicitly verifiable and easily interpreted by robot operators. We fit a sequence of controllers using sequential importance sampling under a generative switching proportional controller task model. Here, we parameterise controllers using a proportional gain and a visually verifiable joint angle goal. Inference under this model is challenging, but we address this by introducing an attribution prior extracted from a neural end-to-end visuomotor control model. Given the sequence of controllers comprising a task, we simplify the trace using grammar parsing strategies, taking advantage of the sequence compositionality, before grounding the controllers by training perception networks to predict goals given images. Using this approach, we are successfully able to induce a program for a visuomotor reaching task involving loops and conditionals from a single demonstration and a neural end-to-end model. In addition, we are able to discover the program used for a tower building task. We argue that computer program-like control systems are more interpretable than alternative end-to-end learning approaches, and that hybrid systems inherently allow for better generalisation across task configurations.



### Regularity Normalization: Neuroscience-Inspired Unsupervised Attention across Neural Network Layers
- **Arxiv ID**: http://arxiv.org/abs/1902.10658v13
- **DOI**: 10.3390/e24010059
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, q-bio.NC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10658v13)
- **Published**: 2019-02-27 17:44:50+00:00
- **Updated**: 2021-12-23 11:20:37+00:00
- **Authors**: Baihan Lin
- **Comment**: Accepted and to be published by Entropy. Codes at
  https://github.com/doerlbh/UnsupervisedAttentionMechanism
- **Journal**: Entropy. 2022; 24(1):59
- **Summary**: Inspired by the adaptation phenomenon of neuronal firing, we propose the regularity normalization (RN) as an unsupervised attention mechanism (UAM) which computes the statistical regularity in the implicit space of neural networks under the Minimum Description Length (MDL) principle. Treating the neural network optimization process as a partially observable model selection problem, the regularity normalization constrains the implicit space by a normalization factor, the universal code length. We compute this universal code incrementally across neural network layers and demonstrate the flexibility to include data priors such as top-down attention and other oracle information. Empirically, our approach outperforms existing normalization methods in tackling limited, imbalanced and non-stationary input distribution in image classification, classic control, procedurally-generated reinforcement learning, generative modeling, handwriting generation and question answering tasks with various neural network architectures. Lastly, the unsupervised attention mechanisms is a useful probing tool for neural networks by tracking the dependency and critical learning stages across layers and recurrent time steps of deep networks.



### Shallow Water Bathymetry Mapping from UAV Imagery based on Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1902.10733v3
- **DOI**: 10.5194/isprs-archives-XLII-2-W10-9-2019
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10733v3)
- **Published**: 2019-02-27 19:09:13+00:00
- **Updated**: 2020-02-18 15:13:29+00:00
- **Authors**: Panagiotis Agrafiotis, Dimitrios Skarlatos, Andreas Georgopoulos, Konstantinos Karantzalos
- **Comment**: 8 pages, 9 figures
- **Journal**: Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-2/W10,
  9-16, 2019
- **Summary**: The determination of accurate bathymetric information is a key element for near offshore activities, hydrological studies such as coastal engineering applications, sedimentary processes, hydrographic surveying as well as archaeological mapping and biological research. UAV imagery processed with Structure from Motion (SfM) and Multi View Stereo (MVS) techniques can provide a low-cost alternative to established shallow seabed mapping techniques offering as well the important visual information. Nevertheless, water refraction poses significant challenges on depth determination. Till now, this problem has been addressed through customized image-based refraction correction algorithms or by modifying the collinearity equation. In this paper, in order to overcome the water refraction errors, we employ machine learning tools that are able to learn the systematic underestimation of the estimated depths. In the proposed approach, based on known depth observations from bathymetric LiDAR surveys, an SVR model was developed able to estimate more accurately the real depths of point clouds derived from SfM-MVS procedures. Experimental results over two test sites along with the performed quantitative validation indicated the high potential of the developed approach.



### A Replication Study: Machine Learning Models Are Capable of Predicting Sexual Orientation From Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1902.10739v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10739v1)
- **Published**: 2019-02-27 19:24:41+00:00
- **Updated**: 2019-02-27 19:24:41+00:00
- **Authors**: John Leuner
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research used machine learning methods to predict a person's sexual orientation from their photograph (Wang and Kosinski, 2017). To verify this result, two of these models are replicated, one based on a deep neural network (DNN) and one on facial morphology (FM). Using a new dataset of 20,910 photographs from dating websites, the ability to predict sexual orientation is confirmed (DNN accuracy male 68%, female 77%, FM male 62%, female 72%). To investigate whether facial features such as brightness or predominant colours are predictive of sexual orientation, a new model based on highly blurred facial images was created. This model was also able to predict sexual orientation (male 63%, female 72%). The tested models are invariant to intentional changes to a subject's makeup, eyewear, facial hair and head pose (angle that the photograph is taken at). It is shown that the head pose is not correlated with sexual orientation. While demonstrating that dating profile images carry rich information about sexual orientation these results leave open the question of how much is determined by facial morphology and how much by differences in grooming, presentation and lifestyle. The advent of new technology that is able to detect sexual orientation in this way may have serious implications for the privacy and safety of gay men and women.



### Object-driven Text-to-Image Synthesis via Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1902.10740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10740v1)
- **Published**: 2019-02-27 19:25:52+00:00
- **Updated**: 2019-02-27 19:25:52+00:00
- **Authors**: Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, Jianfeng Gao
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we propose Object-driven Attentive Generative Adversarial Newtorks (Obj-GANs) that allow object-centered text-to-image synthesis for complex scenes. Following the two-step (layout-image) generation process, a novel object-driven attentive image generator is proposed to synthesize salient objects by paying attention to the most relevant words in the text description and the pre-generated semantic layout. In addition, a new Fast R-CNN based object-wise discriminator is proposed to provide rich object-wise discrimination signals on whether the synthesized object matches the text description and the pre-generated layout. The proposed Obj-GAN significantly outperforms the previous state of the art in various metrics on the large-scale COCO benchmark, increasing the Inception score by 27% and decreasing the FID score by 11%. A thorough comparison between the traditional grid attention and the new object-driven attention is provided through analyzing their mechanisms and visualizing their attention layers, showing insights of how the proposed model generates complex scenes in high quality.



### Joint Face Detection and Facial Motion Retargeting for Multiple Faces
- **Arxiv ID**: http://arxiv.org/abs/1902.10744v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10744v1)
- **Published**: 2019-02-27 19:29:42+00:00
- **Updated**: 2019-02-27 19:29:42+00:00
- **Authors**: Bindita Chaudhuri, Noranart Vesdapunt, Baoyuan Wang
- **Comment**: Accepted to CVPR 2019
- **Journal**: None
- **Summary**: Facial motion retargeting is an important problem in both computer graphics and vision, which involves capturing the performance of a human face and transferring it to another 3D character. Learning 3D morphable model (3DMM) parameters from 2D face images using convolutional neural networks is common in 2D face alignment, 3D face reconstruction etc. However, existing methods either require an additional face detection step before retargeting or use a cascade of separate networks to perform detection followed by retargeting in a sequence. In this paper, we present a single end-to-end network to jointly predict the bounding box locations and 3DMM parameters for multiple faces. First, we design a novel multitask learning framework that learns a disentangled representation of 3DMM parameters for a single face. Then, we leverage the trained single face model to generate ground truth 3DMM parameters for multiple faces to train another network that performs joint face detection and motion retargeting for images with multiple faces. Experimental results show that our joint detection and retargeting network has high face detection accuracy and is robust to extreme expressions and poses while being faster than state-of-the-art methods.



### Nonlinear Markov Random Fields Learned via Backpropagation
- **Arxiv ID**: http://arxiv.org/abs/1902.10747v2
- **DOI**: 10.1007/978-3-030-20351-1_63
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10747v2)
- **Published**: 2019-02-27 19:34:22+00:00
- **Updated**: 2019-03-08 12:34:18+00:00
- **Authors**: Mikael Brudfors, Yaël Balbastre, John Ashburner
- **Comment**: Accepted for the international conference on Information Processing
  in Medical Imaging (IPMI) 2019, camera ready version
- **Journal**: None
- **Summary**: Although convolutional neural networks (CNNs) currently dominate competitions on image segmentation, for neuroimaging analysis tasks, more classical generative approaches based on mixture models are still used in practice to parcellate brains. To bridge the gap between the two, in this paper we propose a marriage between a probabilistic generative model, which has been shown to be robust to variability among magnetic resonance (MR) images acquired via different imaging protocols, and a CNN. The link is in the prior distribution over the unknown tissue classes, which are classically modelled using a Markov random field. In this work we model the interactions among neighbouring pixels by a type of recurrent CNN, which can encode more complex spatial interactions. We validate our proposed model on publicly available MR data, from different centres, and show that it generalises across imaging protocols. This result demonstrates a successful and principled inclusion of a CNN in a generative model, which in turn could be adapted by any probabilistic generative approach for image segmentation.



### Semi-supervised Learning for Quantification of Pulmonary Edema in Chest X-Ray Images
- **Arxiv ID**: http://arxiv.org/abs/1902.10785v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10785v3)
- **Published**: 2019-02-27 21:03:40+00:00
- **Updated**: 2019-04-10 01:47:21+00:00
- **Authors**: Ruizhi Liao, Jonathan Rubin, Grace Lam, Seth Berkowitz, Sandeep Dalal, William Wells, Steven Horng, Polina Golland
- **Comment**: None
- **Journal**: None
- **Summary**: We propose and demonstrate machine learning algorithms to assess the severity of pulmonary edema in chest x-ray images of congestive heart failure patients. Accurate assessment of pulmonary edema in heart failure is critical when making treatment and disposition decisions. Our work is grounded in a large-scale clinical dataset of over 300,000 x-ray images with associated radiology reports. While edema severity labels can be extracted unambiguously from a small fraction of the radiology reports, accurate annotation is challenging in most cases. To take advantage of the unlabeled images, we develop a Bayesian model that includes a variational auto-encoder for learning a latent representation from the entire image set trained jointly with a regressor that employs this representation for predicting pulmonary edema severity. Our experimental results suggest that modeling the distribution of images jointly with the limited labels improves the accuracy of pulmonary edema scoring compared to a strictly supervised approach. To the best of our knowledge, this is the first attempt to employ machine learning algorithms to automatically and quantitatively assess the severity of pulmonary edema in chest x-ray images.



### Dynamic Deep Multi-modal Fusion for Image Privacy Prediction
- **Arxiv ID**: http://arxiv.org/abs/1902.10796v2
- **DOI**: 10.1145/3308558.3313691
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1902.10796v2)
- **Published**: 2019-02-27 21:42:08+00:00
- **Updated**: 2019-03-06 15:54:24+00:00
- **Authors**: Ashwini Tonge, Cornelia Caragea
- **Comment**: Accepted by The Web Conference (WWW) 2019
- **Journal**: None
- **Summary**: With millions of images that are shared online on social networking sites, effective methods for image privacy prediction are highly needed. In this paper, we propose an approach for fusing object, scene context, and image tags modalities derived from convolutional neural networks for accurately predicting the privacy of images shared online. Specifically, our approach identifies the set of most competent modalities on the fly, according to each new target image whose privacy has to be predicted. The approach considers three stages to predict the privacy of a target image, wherein we first identify the neighborhood images that are visually similar and/or have similar sensitive content as the target image. Then, we estimate the competence of the modalities based on the neighborhood images. Finally, we fuse the decisions of the most competent modalities and predict the privacy label for the target image. Experimental results show that our approach predicts the sensitive (or private) content more accurately than the models trained on individual modalities (object, scene, and tags) and prior privacy prediction works. Also, our approach outperforms strong baselines, that train meta-classifiers to obtain an optimal combination of modalities.



