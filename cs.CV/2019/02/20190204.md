# Arxiv Papers in cs.CV on 2019-02-04
### A Tangent Distance Preserving Dimensionality Reduction Algorithm
- **Arxiv ID**: http://arxiv.org/abs/1902.05373v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05373v1)
- **Published**: 2019-02-04 01:52:42+00:00
- **Updated**: 2019-02-04 01:52:42+00:00
- **Authors**: Xu Zhao, Zongli Jiang
- **Comment**: Signal and Image Processing (SIP 2008)
- **Journal**: None
- **Summary**: This paper considers the problem of nonlinear dimensionality reduction. Unlike existing methods, such as LLE, ISOMAP, which attempt to unfold the true manifold in the low dimensional space, our algorithm tries to preserve the nonlinear structure of the manifold, and shows how the manifold is folded in the high dimensional space. We call this method Tangent Distance Preserving Mapping (TDPM). TDPM uses tangent distance instead of geodesic distance, and then applies MDS to the tangent distance matrix to map the manifold into a low dimensional space in which we can get its nonlinear structure.



### Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1902.01019v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01019v1)
- **Published**: 2019-02-04 03:15:13+00:00
- **Updated**: 2019-02-04 03:15:13+00:00
- **Authors**: Shervin Minaee, Amirali Abdolrashidi
- **Comment**: None
- **Journal**: None
- **Summary**: Facial expression recognition has been an active research area over the past few decades, and it is still challenging due to the high intra-class variation.   Traditional approaches for this problem rely on hand-crafted features such as SIFT, HOG and LBP, followed by a classifier trained on a database of images or videos.   Most of these works perform reasonably well on datasets of images captured in a controlled condition, but fail to perform as good on more challenging datasets with more image variation and partial faces.   In recent years, several works proposed an end-to-end framework for facial expression recognition, using deep learning models.   Despite the better performance of these works, there still seems to be a great room for improvement.   In this work, we propose a deep learning approach based on attentional convolutional network, which is able to focus on important parts of the face, and achieves significant improvement over previous models on multiple datasets, including FER-2013, CK+, FERG, and JAFFE.   We also use a visualization technique which is able to find important face regions for detecting different emotions, based on the classifier's output.   Through experimental results, we show that different emotions seems to be sensitive to different parts of the face.



### Towards Pedestrian Detection Using RetinaNet in ECCV 2018 Wider Pedestrian Detection Challenge
- **Arxiv ID**: http://arxiv.org/abs/1902.01031v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01031v1)
- **Published**: 2019-02-04 04:49:59+00:00
- **Updated**: 2019-02-04 04:49:59+00:00
- **Authors**: Md Ashraful Alam Milton
- **Comment**: ECCV Wider pedestrian detection challenege submission
- **Journal**: None
- **Summary**: The main essence of this paper is to investigate the performance of RetinaNet based object detectors on pedestrian detection. Pedestrian detection is an important research topic as it provides a baseline for general object detection and has a great number of practical applications like autonomous car, robotics and Security camera. Though extensive research has made huge progress in pedestrian detection, there are still many issues and open for more research and improvement. Recent deep learning based methods have shown state-of-the-art performance in computer vision tasks such as image classification, object detection, and segmentation. Wider pedestrian detection challenge aims at finding improve solutions for pedestrian detection problem. In this paper, We propose a pedestrian detection system based on RetinaNet. Our solution has scored 0.4061 mAP. The code is available at https://github.com/miltonbd/ECCV_2018_pedestrian_detection_challenege.



### End-to-end feature fusion siamese network for adaptive visual tracking
- **Arxiv ID**: http://arxiv.org/abs/1902.01057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01057v1)
- **Published**: 2019-02-04 07:28:20+00:00
- **Updated**: 2019-02-04 07:28:20+00:00
- **Authors**: Dongyan Guo, Jun Wang, Weixuan Zhao, Ying Cui, Zhenhua Wang, Shengyong Chen
- **Comment**: None
- **Journal**: None
- **Summary**: According to observations, different visual objects have different salient features in different scenarios. Even for the same object, its salient shape and appearance features may change greatly from time to time in a long-term tracking task. Motivated by them, we proposed an end-to-end feature fusion framework based on Siamese network, named FF-Siam, which can effectively fuse different features for adaptive visual tracking. The framework consists of four layers. A feature extraction layer is designed to extract the different features of the target region and search region. The extracted features are then put into a weight generation layer to obtain the channel weights, which indicate the importance of different feature channels. Both features and the channel weights are utilized in a template generation layer to generate a discriminative template. Finally, the corresponding response maps created by the convolution of the search region features and the template are applied with a fusion layer to obtain the final response map for locating the target. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance on the popular Temple-Color, OTB50 and UAV123 benchmarks.



### 3D point cloud registration with shape constraint
- **Arxiv ID**: http://arxiv.org/abs/1902.01061v1
- **DOI**: 10.1109/ICIP.2017.8296672
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01061v1)
- **Published**: 2019-02-04 07:44:54+00:00
- **Updated**: 2019-02-04 07:44:54+00:00
- **Authors**: Swapna Agarwal, Brojeshwar Bhowmick
- **Comment**: Published in ICIP 2017
- **Journal**: None
- **Summary**: In this paper, a shape-constrained iterative algorithm is proposed to register a rigid template point-cloud to a given reference point-cloud. The algorithm embeds a shape-based similarity constraint into the principle of gravitation. The shape-constrained gravitation, as induced by the reference, controls the movement of the template such that at each iteration, the template better aligns with the reference in terms of shape. This constraint enables the alignment in difficult conditions indtroduced by change (presence of outliers and/or missing parts), translation, rotation and scaling. We discuss efficient implementation techniques with least manual intervention. The registration is shown to be useful for change detection in the 3D point-cloud. The algorithm is compared with three state-of-the-art registration approaches. The experiments are done on both synthetic and real-world data. The proposed algorithm is shown to perform better in the presence of big rotation, structured and unstructured outliers and missing data.



### Jumping Manifolds: Geometry Aware Dense Non-Rigid Structure from Motion
- **Arxiv ID**: http://arxiv.org/abs/1902.01077v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01077v3)
- **Published**: 2019-02-04 08:19:46+00:00
- **Updated**: 2019-04-28 01:24:05+00:00
- **Authors**: Suryansh Kumar
- **Comment**: New version with corrected typo. 10 Pages, 7 Figures, 1 Table.
  Accepted for publication in IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) 2019. Acknowledgement added. Supplementary material is
  available at https://suryanshkumar.github.io/
- **Journal**: None
- **Summary**: Given dense image feature correspondences of a non-rigidly moving object across multiple frames, this paper proposes an algorithm to estimate its 3D shape for each frame. To solve this problem accurately, the recent state-of-the-art algorithm reduces this task to set of local linear subspace reconstruction and clustering problem using Grassmann manifold representation \cite{kumar2018scalable}. Unfortunately, their method missed on some of the critical issues associated with the modeling of surface deformations, for e.g., the dependence of a local surface deformation on its neighbors. Furthermore, their representation to group high dimensional data points inevitably introduce the drawbacks of categorizing samples on the high-dimensional Grassmann manifold \cite{huang2015projection, harandi2014manifold}. Hence, to deal with such limitations with \cite{kumar2018scalable}, we propose an algorithm that jointly exploits the benefit of high-dimensional Grassmann manifold to perform reconstruction, and its equivalent lower-dimensional representation to infer suitable clusters. To accomplish this, we project each Grassmannians onto a lower-dimensional Grassmann manifold which preserves and respects the deformation of the structure w.r.t its neighbors. These Grassmann points in the lower-dimension then act as a representative for the selection of high-dimensional Grassmann samples to perform each local reconstruction. In practice, our algorithm provides a geometrically efficient way to solve dense NRSfM by switching between manifolds based on its benefit and usage. Experimental results show that the proposed algorithm is very effective in handling noise with reconstruction accuracy as good as or better than the competing methods.



### Saliency Tubes: Visual Explanations for Spatio-Temporal Convolutions
- **Arxiv ID**: http://arxiv.org/abs/1902.01078v2
- **DOI**: 10.1109/ICIP.2019.8803153
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01078v2)
- **Published**: 2019-02-04 08:19:50+00:00
- **Updated**: 2019-05-12 10:38:02+00:00
- **Authors**: Alexandros Stergiou, Georgios Kapidis, Grigorios Kalliatakis, Christos Chrysoulas, Remco Veltkamp, Ronald Poppe
- **Comment**: None
- **Journal**: IEEE International Conference on Image Processing (ICIP 2019)
- **Summary**: Deep learning approaches have been established as the main methodology for video classification and recognition. Recently, 3-dimensional convolutions have been used to achieve state-of-the-art performance in many challenging video datasets. Because of the high level of complexity of these methods, as the convolution operations are also extended to additional dimension in order to extract features from them as well, providing a visualization for the signals that the network interpret as informative, is a challenging task. An effective notion of understanding the network's inner-workings would be to isolate the spatio-temporal regions on the video that the network finds most informative. We propose a method called Saliency Tubes which demonstrate the foremost points and regions in both frame level and over time that are found to be the main focus points of the network. We demonstrate our findings on widely used datasets for third-person and egocentric action classification and enhance the set of methods and visualizations that improve 3D Convolutional Neural Networks (CNNs) intelligibility.



### Compatible and Diverse Fashion Image Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1902.01096v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01096v2)
- **Published**: 2019-02-04 09:29:22+00:00
- **Updated**: 2019-04-24 04:21:49+00:00
- **Authors**: Xintong Han, Zuxuan Wu, Weilin Huang, Matthew R. Scott, Larry S. Davis
- **Comment**: None
- **Journal**: None
- **Summary**: Visual compatibility is critical for fashion analysis, yet is missing in existing fashion image synthesis systems. In this paper, we propose to explicitly model visual compatibility through fashion image inpainting. To this end, we present Fashion Inpainting Networks (FiNet), a two-stage image-to-image generation framework that is able to perform compatible and diverse inpainting. Disentangling the generation of shape and appearance to ensure photorealistic results, our framework consists of a shape generation network and an appearance generation network. More importantly, for each generation network, we introduce two encoders interacting with one another to learn latent code in a shared compatibility space. The latent representations are jointly optimized with the corresponding generation network to condition the synthesis process, encouraging a diverse set of generated results that are visually compatible with existing fashion garments. In addition, our framework is readily extended to clothing reconstruction and fashion transfer, with impressive results. Extensive experiments with comparisons with state-of-the-art approaches on fashion synthesis task quantitatively and qualitatively demonstrate the effectiveness of our method.



### Dual Path Multi-Scale Fusion Networks with Attention for Crowd Counting
- **Arxiv ID**: http://arxiv.org/abs/1902.01115v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01115v1)
- **Published**: 2019-02-04 10:34:56+00:00
- **Updated**: 2019-02-04 10:34:56+00:00
- **Authors**: Liang Zhu, Zhijian Zhao, Chao Lu, Yining Lin, Yao Peng, Tangren Yao
- **Comment**: None
- **Journal**: None
- **Summary**: The task of crowd counting in varying density scenes is an extremely difficult challenge due to large scale variations. In this paper, we propose a novel dual path multi-scale fusion network architecture with attention mechanism named SFANet that can perform accurate count estimation as well as present high-resolution density maps for highly congested crowd scenes. The proposed SFANet contains two main components: a VGG backbone convolutional neural network (CNN) as the front-end feature map extractor and a dual path multi-scale fusion networks as the back-end to generate density map. These dual path multi-scale fusion networks have the same structure, one path is responsible for generating attention map by highlighting crowd regions in images, the other path is responsible for fusing multi-scale features as well as attention map to generate the final high-quality high-resolution density maps. SFANet can be easily trained in an end-to-end way by dual path joint training. We have evaluated our method on four crowd counting datasets (ShanghaiTech, UCF CC 50, UCSD and UCF-QRNF). The results demonstrate that with attention mechanism and multi-scale feature fusion, the proposed SFANet achieves the best performance on all these datasets and generates better quality density maps compared with other state-of-the-art approaches.



### Object Detection and 3D Estimation via an FMCW Radar Using a Fully Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1902.05394v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05394v1)
- **Published**: 2019-02-04 10:56:20+00:00
- **Updated**: 2019-02-04 10:56:20+00:00
- **Authors**: Guoqiang Zhang, Haopeng Li, Fabian Wenger
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This paper considers object detection and 3D estimation using an FMCW radar. The state-of-the-art deep learning framework is employed instead of using traditional signal processing. In preparing the radar training data, the ground truth of an object orientation in 3D space is provided by conducting image analysis, of which the images are obtained through a coupled camera to the radar device. To ensure successful training of a fully convolutional network (FCN), we propose a normalization method, which is found to be essential to be applied to the radar signal before feeding into the neural network. The system after proper training is able to first detect the presence of an object in an environment. If it does, the system then further produces an estimation of its 3D position. Experimental results show that the proposed system can be successfully trained and employed for detecting a car and further estimating its 3D position in a noisy environment.



### Realistic Image Generation using Region-phrase Attention
- **Arxiv ID**: http://arxiv.org/abs/1902.05395v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05395v1)
- **Published**: 2019-02-04 11:23:00+00:00
- **Updated**: 2019-02-04 11:23:00+00:00
- **Authors**: Wanming Huang, Yida Xu, Ian Oppermann
- **Comment**: None
- **Journal**: None
- **Summary**: The Generative Adversarial Network (GAN) has recently been applied to generate synthetic images from text. Despite significant advances, most current state-of-the-art algorithms are regular-grid region based; when attention is used, it is mainly applied between individual regular-grid regions and a word. These approaches are sufficient to generate images that contain a single object in its foreground, such as a "bird" or "flower". However, natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image. Therefore, the regular-grid based image attention weights may not necessarily concentrate on the intended foreground region(s), which in turn, results in an unnatural looking image. Additionally, individual words such as "a", "blue" and "shirt" do not necessarily provide a full visual context unless they are applied together. For this reason, in our paper, we proposed a novel method in which we introduced an additional set of attentions between true-grid regions and word phrases. The true-grid region is derived using a set of auxiliary bounding boxes. These auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases. Word phrases are derived from analysing Part-of-Speech (POS) results. We perform experiments on this novel network architecture using the Microsoft Common Objects in Context (MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a short sentence description. Our proposed approach is capable of generating more realistic images compared with the current state-of-the-art algorithms.



### Implicit 3D Orientation Learning for 6D Object Detection from RGB Images
- **Arxiv ID**: http://arxiv.org/abs/1902.01275v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01275v2)
- **Published**: 2019-02-04 16:03:57+00:00
- **Updated**: 2019-07-17 14:12:26+00:00
- **Authors**: Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, Rudolph Triebel
- **Comment**: Code available at: https://github.com/DLR-RM/AugmentedAutoencoder
- **Journal**: None
- **Summary**: We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results.



### Real-time Prediction of Automotive Collision Risk from Monocular Video
- **Arxiv ID**: http://arxiv.org/abs/1902.01293v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.01293v1)
- **Published**: 2019-02-04 16:35:19+00:00
- **Updated**: 2019-02-04 16:35:19+00:00
- **Authors**: Derek J. Phillips, Juan Carlos Aragon, Anjali Roychowdhury, Regina Madigan, Sunil Chintakindi, Mykel J. Kochenderfer
- **Comment**: Submitted to IV2019. 7 pages, 4 figures, 3 tables
- **Journal**: None
- **Summary**: Many automotive applications, such as Advanced Driver Assistance Systems (ADAS) for collision avoidance and warnings, require estimating the future automotive risk of a driving scene. We present a low-cost system that predicts the collision risk over an intermediate time horizon from a monocular video source, such as a dashboard-mounted camera. The modular system includes components for object detection, object tracking, and state estimation. We introduce solutions to the object tracking and distance estimation problems. Advanced approaches to the other tasks are used to produce real-time predictions of the automotive risk for the next 10 s at over 5 Hz. The system is designed such that alternative components can be substituted with minimal effort. It is demonstrated on common physical hardware, specifically an off-the-shelf gaming laptop and a webcam. We extend the framework to support absolute speed estimation and more advanced risk estimation techniques.



### 'Squeeze & Excite' Guided Few-Shot Segmentation of Volumetric Images
- **Arxiv ID**: http://arxiv.org/abs/1902.01314v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01314v2)
- **Published**: 2019-02-04 17:11:32+00:00
- **Updated**: 2019-10-11 12:51:22+00:00
- **Authors**: Abhijit Guha Roy, Shayan Siddiqui, Sebastian Pölsterl, Nassir Navab, Christian Wachinger
- **Comment**: Accepted for publication at Medical Image Analysis
- **Journal**: None
- **Summary**: Deep neural networks enable highly accurate image segmentation, but require large amounts of manually annotated data for supervised training. Few-shot learning aims to address this shortcoming by learning a new class from a few annotated support examples. We introduce, a novel few-shot framework, for the segmentation of volumetric medical images with only a few annotated slices. Compared to other related works in computer vision, the major challenges are the absence of pre-trained networks and the volumetric nature of medical scans. We address these challenges by proposing a new architecture for few-shot segmentation that incorporates 'squeeze & excite' blocks. Our two-armed architecture consists of a conditioner arm, which processes the annotated support input and generates a task-specific representation. This representation is passed on to the segmenter arm that uses this information to segment the new query image. To facilitate efficient interaction between the conditioner and the segmenter arm, we propose to use 'channel squeeze & spatial excitation' blocks - a light-weight computational module - that enables heavy interaction between both the arms with negligible increase in model complexity. This contribution allows us to perform image segmentation without relying on a pre-trained model, which generally is unavailable for medical scans. Furthermore, we propose an efficient strategy for volumetric segmentation by optimally pairing a few slices of the support volume to all the slices of the query volume. We perform experiments for organ segmentation on whole-body contrast-enhanced CT scans from the Visceral Dataset. Our proposed model outperforms multiple baselines and existing approaches with respect to the segmentation accuracy by a significant margin. The source code is available at https://github.com/abhi4ssj/few-shot-segmentation.



### Precise Proximal Femur Fracture Classification for Interactive Training and Surgical Planning
- **Arxiv ID**: http://arxiv.org/abs/1902.01338v2
- **DOI**: 10.1007/s11548-020-02150-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01338v2)
- **Published**: 2019-02-04 18:00:24+00:00
- **Updated**: 2020-04-01 14:16:19+00:00
- **Authors**: Amelia Jiménez-Sánchez, Anees Kazi, Shadi Albarqouni, Chlodwig Kirchhoff, Peter Biberthaler, Nassir Navab, Sonja Kirchhoff, Diana Mateus
- **Comment**: Accepted at IPCAI 2020 and IJCARS
- **Journal**: None
- **Summary**: We demonstrate the feasibility of a fully automatic computer-aided diagnosis (CAD) tool, based on deep learning, that localizes and classifies proximal femur fractures on X-ray images according to the AO classification. The proposed framework aims to improve patient treatment planning and provide support for the training of trauma surgeon residents. A database of 1347 clinical radiographic studies was collected. Radiologists and trauma surgeons annotated all fractures with bounding boxes, and provided a classification according to the AO standard. The proposed CAD tool for the classification of radiographs into types "A", "B" and "not-fractured", reaches a F1-score of 87% and AUC of 0.95, when classifying fractures versus not-fractured cases it improves up to 94% and 0.98. Prior localization of the fracture results in an improvement with respect to full image classification. 100% of the predicted centers of the region of interest are contained in the manually provided bounding boxes. The system retrieves on average 9 relevant images (from the same class) out of 10 cases. Our CAD scheme localizes, detects and further classifies proximal femur fractures achieving results comparable to expert-level and state-of-the-art performance. Our auxiliary localization model was highly accurate predicting the region of interest in the radiograph. We further investigated several strategies of verification for its adoption into the daily clinical routine. A sensitivity analysis of the size of the ROI and image retrieval as a clinical use case were presented.



### Partial Fingerprint Detection Using Core Point Location
- **Arxiv ID**: http://arxiv.org/abs/1902.01400v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01400v1)
- **Published**: 2019-02-04 18:02:41+00:00
- **Updated**: 2019-02-04 18:02:41+00:00
- **Authors**: Wajih Ullah Baig, Adeel Ejaz, Umar Munir, Kashif Sardar
- **Comment**: 5 pages. 17 figures
- **Journal**: None
- **Summary**: In Biometric identification, fingerprints based identification has been the widely accepted mechanism. Automated fingerprints identification/verification techniques are widely adopted in many civilian and forensic applications. In forensic applications fingerprints are usually incomplete, broken, unclear or degraded which are known as partial fingerprints. Fingerprints identification/verification largely suffer from the problem of handling partial fingerprints. In this paper a novel and simple approach is presented for detecting partial fingerprints using core point location. Our techniques is particularly useful during the acquisition stage as to determine whether a user needs to re-align the finger to ensure a complete capture of fingerprint area.This technique is tested on FVC-2002 DB1A. The results are very accurate which are presented in the Results sections.



### End-to-End Single Image Fog Removal using Enhanced Cycle Consistent Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.01374v1
- **DOI**: 10.1109/TIP.2020.3007844
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01374v1)
- **Published**: 2019-02-04 18:43:07+00:00
- **Updated**: 2019-02-04 18:43:07+00:00
- **Authors**: Wei Liu, Xianxu Hou, Jiang Duan, Guoping Qiu
- **Comment**: Submitted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Single image defogging is a classical and challenging problem in computer vision. Existing methods towards this problem mainly include handcrafted priors based methods that rely on the use of the atmospheric degradation model and learning based approaches that require paired fog-fogfree training example images. In practice, however, prior-based methods are prone to failure due to their own limitations and paired training data are extremely difficult to acquire. Inspired by the principle of CycleGAN network, we have developed an end-to-end learning system that uses unpaired fog and fogfree training images, adversarial discriminators and cycle consistency losses to automatically construct a fog removal system. Similar to CycleGAN, our system has two transformation paths; one maps fog images to a fogfree image domain and the other maps fogfree images to a fog image domain. Instead of one stage mapping, our system uses a two stage mapping strategy in each transformation path to enhance the effectiveness of fog removal. Furthermore, we make explicit use of prior knowledge in the networks by embedding the atmospheric degradation principle and a sky prior for mapping fogfree images to the fog images domain. In addition, we also contribute the first real world nature fog-fogfree image dataset for defogging research. Our multiple real fog images dataset (MRFID) contains images of 200 natural outdoor scenes. For each scene, there are one clear image and corresponding four foggy images of different fog densities manually selected from a sequence of images taken by a fixed camera over the course of one year. Qualitative and quantitative comparison against several state-of-the-art methods on both synthetic and real world images demonstrate that our approach is effective and performs favorably for recovering a clear image from a foggy image.



### Very Long Term Field of View Prediction for 360-degree Video Streaming
- **Arxiv ID**: http://arxiv.org/abs/1902.01439v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01439v1)
- **Published**: 2019-02-04 19:43:40+00:00
- **Updated**: 2019-02-04 19:43:40+00:00
- **Authors**: Chenge Li, Weixi Zhang, Yong Liu, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: 360-degree videos have gained increasing popularity in recent years with the developments and advances in Virtual Reality (VR) and Augmented Reality (AR) technologies. In such applications, a user only watches a video scene within a field of view (FoV) centered in a certain direction. Predicting the future FoV in a long time horizon (more than seconds ahead) can help save bandwidth resources in on-demand video streaming while minimizing video freezing in networks with significant bandwidth variations. In this work, we treat the FoV prediction as a sequence learning problem, and propose to predict the target user's future FoV not only based on the user's own past FoV center trajectory but also other users' future FoV locations. We propose multiple prediction models based on two different FoV representations: one using FoV center trajectories and another using equirectangular heatmaps that represent the FoV center distributions. Extensive evaluations with two public datasets demonstrate that the proposed models can significantly outperform benchmark models, and other users' FoVs are very helpful for improving long-term predictions.



### TrackNet: Simultaneous Object Detection and Tracking and Its Application in Traffic Video Analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.01466v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01466v1)
- **Published**: 2019-02-04 21:39:17+00:00
- **Updated**: 2019-02-04 21:39:17+00:00
- **Authors**: Chenge Li, Gregory Dobler, Xin Feng, Yao Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection and object tracking are usually treated as two separate processes. Significant progress has been made for object detection in 2D images using deep learning networks. The usual tracking-by-detection pipeline for object tracking requires that the object is successfully detected in the first frame and all subsequent frames, and tracking is done by associating detection results. Performing object detection and object tracking through a single network remains a challenging open question. We propose a novel network structure named trackNet that can directly detect a 3D tube enclosing a moving object in a video segment by extending the faster R-CNN framework. A Tube Proposal Network (TPN) inside the trackNet is proposed to predict the objectness of each candidate tube and location parameters specifying the bounding tube. The proposed framework is applicable for detecting and tracking any object and in this paper, we focus on its application for traffic video analysis. The proposed model is trained and tested on UA-DETRAC, a large traffic video dataset available for multi-vehicle detection and tracking, and obtained very promising results.



### Detection of perfusion ROI as a quality control in perfusion analysis
- **Arxiv ID**: http://arxiv.org/abs/1902.01855v1
- **DOI**: None
- **Categories**: **physics.med-ph**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.01855v1)
- **Published**: 2019-02-04 22:18:04+00:00
- **Updated**: 2019-02-04 22:18:04+00:00
- **Authors**: Svitlana Alkhimova
- **Comment**: None
- **Journal**: in Proc. Science, Research, Development. Technics and Technology,
  Berlin, Germany, Jan 30, 2018, pp.57-59
- **Summary**: In perfusion analysis automated approaches for image processing is preferable due to reduce time-consuming tasks for radiologists. Assessment of perfusion results quality is important step in development of algorithms for automated processing. One of them is an assessment of perfusion maps quality based on detection of perfusion ROI.



### A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras
- **Arxiv ID**: http://arxiv.org/abs/1902.01496v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01496v4)
- **Published**: 2019-02-04 23:44:00+00:00
- **Updated**: 2019-05-15 13:54:05+00:00
- **Authors**: Icaro O. de Oliveira, Keiko V. O. Fonseca, Rodrigo Minetto
- **Comment**: 5 pages, 6 figures, To appear in IEEE International Conference on
  Image Processing (ICIP), Sept. 22-25, 2019, Taipei, Taiwan
- **Journal**: None
- **Summary**: We describe in this paper a Two-Stream Siamese Neural Network for vehicle re-identification. The proposed network is fed simultaneously with small coarse patches of the vehicle shape's, with 96 x 96 pixels, in one stream, and fine features extracted from license plate patches, easily readable by humans, with 96 x 48 pixels, in the other one. Then, we combined the strengths of both streams by merging the Siamese distance descriptors with a sequence of fully connected layers, as an attempt to tackle a major problem in the field, false alarms caused by a huge number of car design and models with nearly the same appearance or by similar license plate strings. In our experiments, with 2 hours of videos containing 2982 vehicles, extracted from two low-cost cameras in the same roadway, 546 ft away, we achieved a F-measure and accuracy of 92.6% and 98.7%, respectively. We show that the proposed network, available at https://github.com/icarofua/siamese-two-stream, outperforms other One-Stream architectures, even if they use higher resolution image features.



