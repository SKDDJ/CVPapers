# Arxiv Papers in cs.CV on 2019-02-03
### DeepPBM: Deep Probabilistic Background Model Estimation from Video Sequences
- **Arxiv ID**: http://arxiv.org/abs/1902.00820v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1902.00820v1)
- **Published**: 2019-02-03 00:25:53+00:00
- **Updated**: 2019-02-03 00:25:53+00:00
- **Authors**: Amirreza Farnoosh, Behnaz Rezaei, Sarah Ostadabbas
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel unsupervised probabilistic model estimation of visual background in video sequences using a variational autoencoder framework. Due to the redundant nature of the backgrounds in surveillance videos, visual information of the background can be compressed into a low-dimensional subspace in the encoder part of the variational autoencoder, while the highly variant information of its moving foreground gets filtered throughout its encoding-decoding process. Our deep probabilistic background model (DeepPBM) estimation approach is enabled by the power of deep neural networks in learning compressed representations of video frames and reconstructing them back to the original domain. We evaluated the performance of our DeepPBM in background subtraction on 9 surveillance videos from the background model challenge (BMC2012) dataset, and compared that with a standard subspace learning technique, robust principle component analysis (RPCA), which similarly estimates a deterministic low dimensional representation of the background in videos and is widely used for this application. Our method outperforms RPCA on BMC2012 dataset with 23% in average in F-measure score, emphasizing that background subtraction using the trained model can be done in more than 10 times faster.



### Automatic trajectory measurement of large numbers of crowded objects
- **Arxiv ID**: http://arxiv.org/abs/1902.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00835v1)
- **Published**: 2019-02-03 03:30:50+00:00
- **Updated**: 2019-02-03 03:30:50+00:00
- **Authors**: Hui Li, Ye Liu, Yan Qiu Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Complex motion patterns of natural systems, such as fish schools, bird flocks, and cell groups, have attracted great attention from scientists for years. Trajectory measurement of individuals is vital for quantitative and high-throughput study of their collective behaviors. However, such data are rare mainly due to the challenges of detection and tracking of large numbers of objects with similar visual features and frequent occlusions. We present an automatic and effective framework to measure trajectories of large numbers of crowded oval-shaped objects, such as fish and cells. We first use a novel dual ellipse locator to detect the coarse position of each individual and then propose a variance minimization active contour method to obtain the optimal segmentation results. For tracking, cost matrix of assignment between consecutive frames is trainable via a random forest classifier with many spatial, texture, and shape features. The optimal trajectories are found for the whole image sequence by solving two linear assignment problems. We evaluate the proposed method on many challenging data sets.



### Real-Time Freespace Segmentation on Autonomous Robots for Detection of Obstacles and Drop-Offs
- **Arxiv ID**: http://arxiv.org/abs/1902.00842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.00842v1)
- **Published**: 2019-02-03 04:42:13+00:00
- **Updated**: 2019-02-03 04:42:13+00:00
- **Authors**: Anish Singhani
- **Comment**: None
- **Journal**: None
- **Summary**: Mobile robots navigating in indoor and outdoor environments must be able to identify and avoid unsafe terrain. Although a significant amount of work has been done on the detection of standing obstacles (solid obstructions), not much work has been done on the detection of negative obstacles (e.g. dropoffs, ledges, downward stairs). We propose a method of terrain safety segmentation using deep convolutional networks. Our custom semantic segmentation architecture uses a single camera as input and creates a freespace map distinguishing safe terrain and obstacles. We then show how this freespace map can be used for real-time navigation on an indoor robot. The results show that our system generalizes well, is suitable for real-time operation, and runs at around 55 fps on a small indoor robot powered by a low-power embedded GPU.



### Night Time Haze and Glow Removal using Deep Dilated Convolutional Network
- **Arxiv ID**: http://arxiv.org/abs/1902.00855v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00855v1)
- **Published**: 2019-02-03 06:24:46+00:00
- **Updated**: 2019-02-03 06:24:46+00:00
- **Authors**: Shiba Kuanar, K. R. Rao, Dwarikanath Mahapatra, Monalisa Bilas
- **Comment**: 13 pages, 10 figures, 2 Tables
- **Journal**: None
- **Summary**: In this paper, we address the single image haze removal problem in a nighttime scene. The night haze removal is a severely ill-posed problem especially due to the presence of various visible light sources with varying colors and non-uniform illumination. These light sources are of different shapes and introduce noticeable glow in night scenes. To address these effects we introduce a deep learning based DeGlow-DeHaze iterative architecture which accounts for varying color illumination and glows. First, our convolution neural network (CNN) based DeGlow model is able to remove the glow effect significantly and on top of it a separate DeHaze network is included to remove the haze effect. For our recurrent network training, the hazy images and the corresponding transmission maps are synthesized from the NYU depth datasets and consequently restored a high-quality haze-free image. The experimental results demonstrate that our hybrid CNN model outperforms other state-of-the-art methods in terms of computation speed and image quality. We also show the effectiveness of our model on a number of real images and compare our results with the existing night haze heuristic models.



### MICIK: MIning Cross-Layer Inherent Similarity Knowledge for Deep Model Compression
- **Arxiv ID**: http://arxiv.org/abs/1902.00918v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00918v1)
- **Published**: 2019-02-03 16:26:21+00:00
- **Updated**: 2019-02-03 16:26:21+00:00
- **Authors**: Jie Zhang, Xiaolong Wang, Dawei Li, Shalini Ghosh, Abhishek Kolagunda, Yalin Wang
- **Comment**: 10 pages, 4 figures
- **Journal**: None
- **Summary**: State-of-the-art deep model compression methods exploit the low-rank approximation and sparsity pruning to remove redundant parameters from a learned hidden layer. However, they process each hidden layer individually while neglecting the common components across layers, and thus are not able to fully exploit the potential redundancy space for compression. To solve the above problem and enable further compression of a model, removing the cross-layer redundancy and mining the layer-wise inheritance knowledge is necessary. In this paper, we introduce a holistic model compression framework, namely MIning Cross-layer Inherent similarity Knowledge (MICIK), to fully excavate the potential redundancy space. The proposed MICIK framework simultaneously, (1) learns the common and unique weight components across deep neural network layers to increase compression rate; (2) preserves the inherent similarity knowledge of nearby layers and distant layers to minimize the accuracy loss and (3) can be complementary to other existing compression techniques such as knowledge distillation. Extensive experiments on large-scale convolutional neural networks demonstrate that MICIK is superior over state-of-the-art model compression approaches with 16X parameter reduction on VGG-16 and 6X on GoogLeNet, all without accuracy loss.



### Depthwise Convolution is All You Need for Learning Multiple Visual Domains
- **Arxiv ID**: http://arxiv.org/abs/1902.00927v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.00927v2)
- **Published**: 2019-02-03 16:58:19+00:00
- **Updated**: 2019-02-19 18:45:10+00:00
- **Authors**: Yunhui Guo, Yandong Li, Rogerio Feris, Liqiang Wang, Tajana Rosing
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing interest in designing models that can deal with images from different visual domains. If there exists a universal structure in different visual domains that can be captured via a common parameterization, then we can use a single model for all domains rather than one model per domain. A model aware of the relationships between different domains can also be trained to work on new domains with less resources. However, to identify the reusable structure in a model is not easy. In this paper, we propose a multi-domain learning architecture based on depthwise separable convolution. The proposed approach is based on the assumption that images from different domains share cross-channel correlations but have domain-specific spatial correlations. The proposed model is compact and has minimal overhead when being applied to new domains. Additionally, we introduce a gating mechanism to promote soft sharing between different domains. We evaluate our approach on Visual Decathlon Challenge, a benchmark for testing the ability of multi-domain models. The experiments show that our approach can achieve the highest score while only requiring 50% of the parameters compared with the state-of-the-art approaches.



