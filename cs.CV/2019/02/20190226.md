# Arxiv Papers in cs.CV on 2019-02-26
### Event-driven Video Frame Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1902.09680v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09680v2)
- **Published**: 2019-02-26 00:54:09+00:00
- **Updated**: 2019-07-06 02:11:05+00:00
- **Authors**: Zihao W. Wang, Weixin Jiang, Kuan He, Boxin Shi, Aggelos Katsaggelos, Oliver Cossairt
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: Temporal Video Frame Synthesis (TVFS) aims at synthesizing novel frames at timestamps different from existing frames, which has wide applications in video codec, editing and analysis. In this paper, we propose a high framerate TVFS framework which takes hybrid input data from a low-speed frame-based sensor and a high-speed event-based sensor. Compared to frame-based sensors, event-based sensors report brightness changes at very high speed, which may well provide useful spatio-temoral information for high framerate TVFS. In our framework, we first introduce a differentiable forward model to approximate the physical sensing process, fusing the two different modes of data as well as unifying a variety of TVFS tasks, i.e., interpolation, prediction and motion deblur. We leverage autodifferentiation which propagates the gradients of a loss defined on the measured data back to the latent high framerate video. We show results with better performance compared to state-of-the-art. Second, we develop a deep learning-based strategy to enhance the results from the first step, which we refer as a residual "denoising" process. Our trained "denoiser" is beyond Gaussian denoising and shows properties such as contrast enhancement and motion awareness. We show that our framework is capable of handling challenging scenes including both fast motion and strong occlusions.



### Self-Selective Correlation Ship Tracking Method for Smart Ocean System
- **Arxiv ID**: http://arxiv.org/abs/1902.09690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09690v1)
- **Published**: 2019-02-26 01:19:35+00:00
- **Updated**: 2019-02-26 01:19:35+00:00
- **Authors**: Xu Kang, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, with the development of the marine industry, navigation environment becomes more complicated. Some artificial intelligence technologies, such as computer vision, can recognize, track and count the sailing ships to ensure the maritime security and facilitates the management for Smart Ocean System. Aiming at the scaling problem and boundary effect problem of traditional correlation filtering methods, we propose a self-selective correlation filtering method based on box regression (BRCF). The proposed method mainly include: 1) A self-selective model with negative samples mining method which effectively reduces the boundary effect in strengthening the classification ability of classifier at the same time; 2) A bounding box regression method combined with a key points matching method for the scale prediction, leading to a fast and efficient calculation. The experimental results show that the proposed method can effectively deal with the problem of ship size changes and background interference. The success rates and precisions were higher than Discriminative Scale Space Tracking (DSST) by over 8 percentage points on the marine traffic dataset of our laboratory. In terms of processing speed, the proposed method is higher than DSST by nearly 22 Frames Per Second (FPS).



### QLMC-HD: Quasi Large Margin Classifier based on Hyperdisk
- **Arxiv ID**: http://arxiv.org/abs/1902.09692v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09692v4)
- **Published**: 2019-02-26 01:23:14+00:00
- **Updated**: 2020-08-29 23:15:53+00:00
- **Authors**: Hassan Ataeian, Shahriar Esmaeili, Saeideh Roshanfekr, Neda Maleki Khas, Ali Amiri, Hossein Safari
- **Comment**: 12 pages, 1 figures, 10 tables
- **Journal**: None
- **Summary**: In the area of data classification, the different classifiers have been developed by their own strengths and weaknesses. Among these classifiers, we propose a method that is based on the maximum margin between two classes. One of the main challenges in this area is dealt with noisy data. In this paper, our aim is to optimize the method of large margin classifiers based on hyperdisk (LMC-HD) and combine it into a quasisupport vector data description (QSVDD) method. In the proposed method, the bounding hypersphere is calculated based on the QSVDD method. So our convex class model is more robust compared with the support vector machine (SVM) and less tight than LMC-HD. Large margin classifiers aim to maximize the margin and minimizing the risk. Since our proposed method ignores the effect of outliers and noises, so this method has the widest margin compared with other large margin classifiers. In the end, we compare our proposed method with other popular large margin classifiers by the experiments on a set of standard data which indicates our results are more efficient than the others



### Beyond the Self: Using Grounded Affordances to Interpret and Describe Others' Actions
- **Arxiv ID**: http://arxiv.org/abs/1902.09705v1
- **DOI**: 10.1109/TCDS.2018.2882140
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09705v1)
- **Published**: 2019-02-26 02:14:10+00:00
- **Updated**: 2019-02-26 02:14:10+00:00
- **Authors**: Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino, Giampiero Salvi
- **Comment**: code available at https://github.com/gsaponaro/tcds-gestures, IEEE
  Transactions on Cognitive and Developmental Systems
- **Journal**: IEEE Transactions on Cognitive and Developmental Systems, vol. 12,
  no. 2, pp. 209-221, June 2020
- **Summary**: We propose a developmental approach that allows a robot to interpret and describe the actions of human agents by reusing previous experience. The robot first learns the association between words and object affordances by manipulating the objects in its environment. It then uses this information to learn a mapping between its own actions and those performed by a human in a shared environment. It finally fuses the information from these two models to interpret and describe human actions in light of its own experience. In our experiments, we show that the model can be used flexibly to do inference on different aspects of the scene. We can predict the effects of an action on the basis of object properties. We can revise the belief that a certain action occurred, given the observed effects of the human action. In an early action recognition fashion, we can anticipate the effects when the action has only been partially observed. By estimating the probability of words given the evidence and feeding them into a pre-defined grammar, we can generate relevant descriptions of the scene. We believe that this is a step towards providing robots with the fundamental skills to engage in social collaboration with humans.



### MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video
- **Arxiv ID**: http://arxiv.org/abs/1902.09707v6
- **DOI**: 10.1109/TPAMI.2019.2944806
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1902.09707v6)
- **Published**: 2019-02-26 02:35:55+00:00
- **Updated**: 2020-10-03 13:17:46+00:00
- **Authors**: Qunliang Xing, Zhenyu Guan, Mai Xu, Ren Yang, Tie Liu, Zulin Wang
- **Comment**: Accepted to TPAMI in September, 2019. v6 updates: correct units in
  Fig. 11; correct author info; delete bio photos. arXiv admin note: text
  overlap with arXiv:1803.04680
- **Journal**: None
- **Summary**: The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, not considering the similarity between consecutive frames. Since heavy fluctuation exists across compressed video frames as investigated in this paper, frame similarity can be utilized for quality enhancement of low-quality frames given their neighboring high-quality frames. This task is Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as the first attempt in this direction. In our approach, we firstly develop a Bidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are the input. In MF-CNN, motion between the non-PQF and PQFs is compensated by a motion compensation subnet. Subsequently, a quality enhancement subnet fuses the non-PQF and compensated PQFs, and then reduces the compression artifacts of the non-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments validate the effectiveness and generalization ability of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code is available at https://github.com/RyanXingQL/MFQEv2.0.git.



### Learning a Deep ConvNet for Multi-label Classification with Partial Labels
- **Arxiv ID**: http://arxiv.org/abs/1902.09720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09720v1)
- **Published**: 2019-02-26 03:52:16+00:00
- **Updated**: 2019-02-26 03:52:16+00:00
- **Authors**: Thibaut Durand, Nazanin Mehrasa, Greg Mori
- **Comment**: CVPR 2019
- **Journal**: None
- **Summary**: Deep ConvNets have shown great performance for single-label image classification (e.g. ImageNet), but it is necessary to move beyond the single-label classification task because pictures of everyday life are inherently multi-label. Multi-label classification is a more difficult task than single-label classification because both the input images and output label spaces are more complex. Furthermore, collecting clean multi-label annotations is more difficult to scale-up than single-label annotations. To reduce the annotation cost, we propose to train a model with partial labels i.e. only some labels are known per image. We first empirically compare different labeling strategies to show the potential for using partial labels on multi-label datasets. Then to learn with partial labels, we introduce a new classification loss that exploits the proportion of known labels per example. Our approach allows the use of the same training settings as when learning with all the annotations. We further explore several curriculum learning based strategies to predict missing labels. Experiments are performed on three large-scale multi-label datasets: MS COCO, NUS-WIDE and Open Images.



### Harmonic Unpaired Image-to-image Translation
- **Arxiv ID**: http://arxiv.org/abs/1902.09727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09727v1)
- **Published**: 2019-02-26 04:49:56+00:00
- **Updated**: 2019-02-26 04:49:56+00:00
- **Authors**: Rui Zhang, Tomas Pfister, Jia Li
- **Comment**: None
- **Journal**: None
- **Summary**: The recent direction of unpaired image-to-image translation is on one hand very exciting as it alleviates the big burden in obtaining label-intensive pixel-to-pixel supervision, but it is on the other hand not fully satisfactory due to the presence of artifacts and degenerated transformations. In this paper, we take a manifold view of the problem by introducing a smoothness term over the sample graph to attain harmonic functions to enforce consistent mappings during the translation. We develop HarmonicGAN to learn bi-directional translations between the source and the target domains. With the help of similarity-consistency, the inherent self-consistency property of samples can be maintained. Distance metrics defined on two types of features including histogram and CNN are exploited. Under an identical problem setting as CycleGAN, without additional manual inputs and only at a small training-time cost, HarmonicGAN demonstrates a significant qualitative and quantitative improvement over the state of the art, as well as improved interpretability. We show experimental results in a number of applications including medical imaging, object transfiguration, and semantic labeling. We outperform the competing methods in all tasks, and for a medical imaging task in particular our method turns CycleGAN from a failure to a success, halving the mean-squared error, and generating images that radiologists prefer over competing methods in 95% of cases.



### Stereo R-CNN based 3D Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.09738v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1902.09738v2)
- **Published**: 2019-02-26 05:19:12+00:00
- **Updated**: 2019-04-10 10:20:11+00:00
- **Authors**: Peiliang Li, Xiaozhi Chen, Shaojie Shen
- **Comment**: Accepted by cvpr2019
- **Journal**: None
- **Summary**: We propose a 3D object detection method for autonomous driving by fully exploiting the sparse and dense, semantic and geometry information in stereo imagery. Our method, called Stereo R-CNN, extends Faster R-CNN for stereo inputs to simultaneously detect and associate object in left and right images. We add extra branches after stereo Region Proposal Network (RPN) to predict sparse keypoints, viewpoints, and object dimensions, which are combined with 2D left-right boxes to calculate a coarse 3D object bounding box. We then recover the accurate 3D bounding box by a region-based photometric alignment using left and right RoIs. Our method does not require depth input and 3D position supervision, however, outperforms all existing fully supervised image-based methods. Experiments on the challenging KITTI dataset show that our method outperforms the state-of-the-art stereo-based method by around 30% AP on both 3D detection and 3D localization tasks. Code has been released at https://github.com/HKUST-Aerial-Robotics/Stereo-RCNN.



### Image-Question-Answer Synergistic Network for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1902.09774v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.09774v1)
- **Published**: 2019-02-26 07:30:43+00:00
- **Updated**: 2019-02-26 07:30:43+00:00
- **Authors**: Dalu Guo, Chang Xu, Dacheng Tao
- **Comment**: Accepted by cvpr2019
- **Journal**: None
- **Summary**: The image, question (combined with the history for de-referencing), and the corresponding answer are three vital components of visual dialog. Classical visual dialog systems integrate the image, question, and history to search for or generate the best matched answer, and so, this approach significantly ignores the role of the answer. In this paper, we devise a novel image-question-answer synergistic network to value the role of the answer for precise visual dialog. We extend the traditional one-stage solution to a two-stage solution. In the first stage, candidate answers are coarsely scored according to their relevance to the image and question pair. Afterward, in the second stage, answers with high probability of being correct are re-ranked by synergizing with image and question. On the Visual Dialog v1.0 dataset, the proposed synergistic network boosts the discriminative visual dialog model to achieve a new state-of-the-art of 57.88\% normalized discounted cumulative gain. A generative visual dialog model equipped with the proposed technique also shows promising improvements.



### Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding
- **Arxiv ID**: http://arxiv.org/abs/1902.09777v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09777v3)
- **Published**: 2019-02-26 07:39:11+00:00
- **Updated**: 2019-04-24 11:47:13+00:00
- **Authors**: Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, Shenghua Gao
- **Comment**: Minor Revision
- **Journal**: None
- **Summary**: Single-image piece-wise planar 3D reconstruction aims to simultaneously segment plane instances and recover 3D plane parameters from an image. Most recent approaches leverage convolutional neural networks (CNNs) and achieve promising results. However, these methods are limited to detecting a fixed number of planes with certain learned order. To tackle this problem, we propose a novel two-stage method based on associative embedding, inspired by its recent success in instance segmentation. In the first stage, we train a CNN to map each pixel to an embedding space where pixels from the same plane instance have similar embeddings. Then, the plane instances are obtained by grouping the embedding vectors in planar regions via an efficient mean shift clustering algorithm. In the second stage, we estimate the parameter for each plane instance by considering both pixel-level and instance-level consistencies. With the proposed method, we are able to detect an arbitrary number of planes. Extensive experiments on public datasets validate the effectiveness and efficiency of our method. Furthermore, our method runs at 30 fps at the testing time, thus could facilitate many real-time applications such as visual SLAM and human-robot interaction. Code is available at https://github.com/svip-lab/PlanarReconstruction.



### BoostGAN for Occlusive Profile Face Frontalization and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.09782v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09782v1)
- **Published**: 2019-02-26 07:59:47+00:00
- **Updated**: 2019-02-26 07:59:47+00:00
- **Authors**: Qingyan Duan, Lei Zhang
- **Comment**: 9 pages, 7 figures, 7 tables
- **Journal**: None
- **Summary**: There are many facts affecting human face recognition, such as pose, occlusion, illumination, age, etc. First and foremost are large pose and occlusion problems, which can even result in more than 10% performance degradation. Pose-invariant feature representation and face frontalization with generative adversarial networks (GAN) have been widely used to solve the pose problem. However, the synthesis and recognition of occlusive but profile faces is still an uninvestigated problem. To address this issue, in this paper, we aim to contribute an effective solution on how to recognize occlusive but profile faces, even with facial keypoint region (e.g. eyes, nose, etc.) corrupted. Specifically, we propose a boosting Generative Adversarial Network (BoostGAN) for de-occlusion, frontalization, and recognition of faces. Upon the assumption that facial occlusion is partial and incomplete, multiple patch occluded images are fed as inputs for knowledge boosting, such as identity and texture information. A new aggregation structure composed of a deep GAN for coarse face synthesis and a shallow boosting net for fine face generation is further designed. Exhaustive experiments demonstrate that the proposed approach not only presents clear perceptual photo-realistic results but also shows state-of-the-art recognition performance for occlusive but profile faces.



### Bi-stream Pose Guided Region Ensemble Network for Fingertip Localization from Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/1902.09795v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09795v1)
- **Published**: 2019-02-26 08:28:53+00:00
- **Updated**: 2019-02-26 08:28:53+00:00
- **Authors**: Guijin Wang, Cairong Zhang, Xinghao Chen, Xiangyang Ji, Jing-Hao Xue, Hang Wang
- **Comment**: Cairong Zhang and Xinghao Chen are equally contributed
- **Journal**: None
- **Summary**: In human-computer interaction, it is important to accurately estimate the hand pose especially fingertips. However, traditional approaches for fingertip localization mainly rely on depth images and thus suffer considerably from the noise and missing values. Instead of depth images, stereo images can also provide 3D information of hands and promote 3D hand pose estimation. There are nevertheless limitations on the dataset size, global viewpoints, hand articulations and hand shapes in the publicly available stereo-based hand pose datasets. To mitigate these limitations and promote further research on hand pose estimation from stereo images, we propose a new large-scale binocular hand pose dataset called THU-Bi-Hand, offering a new perspective for fingertip localization. In the THU-Bi-Hand dataset, there are 447k pairs of stereo images of different hand shapes from 10 subjects with accurate 3D location annotations of the wrist and five fingertips. Captured with minimal restriction on the range of hand motion, the dataset covers large global viewpoint space and hand articulation space. To better present the performance of fingertip localization on THU-Bi-Hand, we propose a novel scheme termed Bi-stream Pose Guided Region Ensemble Network (Bi-Pose-REN). It extracts more representative feature regions around joint points in the feature maps under the guidance of the previously estimated pose. The feature regions are integrated hierarchically according to the topology of hand joints to regress the refined hand pose. Bi-Pose-REN and several existing methods are evaluated on THU-Bi-Hand so that benchmarks are provided for further research. Experimental results show that our new method has achieved the best performance on THU-Bi-Hand.



### Recurrent Convolution for Compact and Cost-Adjustable Neural Networks: An Empirical Study
- **Arxiv ID**: http://arxiv.org/abs/1902.09809v1
- **DOI**: None
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.09809v1)
- **Published**: 2019-02-26 09:09:24+00:00
- **Updated**: 2019-02-26 09:09:24+00:00
- **Authors**: Zhendong Zhang, Cheolkon Jung
- **Comment**: 8 pages; preprint; work in progress
- **Journal**: None
- **Summary**: Recurrent convolution (RC) shares the same convolutional kernels and unrolls them multiple steps, which is originally proposed to model time-space signals. We argue that RC can be viewed as a model compression strategy for deep convolutional neural networks. RC reduces the redundancy across layers. However, the performance of an RC network is not satisfactory if we directly unroll the same kernels multiple steps. We propose a simple yet effective variant which improves the RC networks: the batch normalization layers of an RC module are learned independently (not shared) for different unrolling steps. Moreover, we verify that RC can perform cost-adjustable inference which is achieved by varying its unrolling steps. We learn double independent BN layers for cost-adjustable RC networks, i.e. independent w.r.t both the unrolling steps of current cell and upstream cell. We provide insights on why the proposed method works successfully. Experiments on both image classification and image denoise demonstrate the effectiveness of our method.



### LaSO: Label-Set Operations networks for multi-label few-shot learning
- **Arxiv ID**: http://arxiv.org/abs/1902.09811v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09811v1)
- **Published**: 2019-02-26 09:12:09+00:00
- **Updated**: 2019-02-26 09:12:09+00:00
- **Authors**: Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok, Sivan Harary, Rogerio Feris, Raja Giryes, Alex M. Bronstein
- **Comment**: None
- **Journal**: None
- **Summary**: Example synthesis is one of the leading methods to tackle the problem of few-shot learning, where only a small number of samples per class are available. However, current synthesis approaches only address the scenario of a single category label per image. In this work, we propose a novel technique for synthesizing samples with multiple labels for the (yet unhandled) multi-label few-shot classification scenario. We propose to combine pairs of given examples in feature space, so that the resulting synthesized feature vectors will correspond to examples whose label sets are obtained through certain set operations on the label sets of the corresponding input pairs. Thus, our method is capable of producing a sample containing the intersection, union or set-difference of labels present in two input samples. As we show, these set operations generalize to labels unseen during training. This enables performing augmentation on examples of novel categories, thus, facilitating multi-label few-shot classifier learning. We conduct numerous experiments showing promising results for the label-set manipulation capabilities of the proposed approach, both directly (using the classification and retrieval metrics), and in the context of performing data augmentation for multi-label few-shot learning. We propose a benchmark for this new and challenging task and show that our method compares favorably to all the common baselines.



### GCN-LASE: Towards Adequately Incorporating Link Attributes in Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.09817v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.SI, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.09817v2)
- **Published**: 2019-02-26 09:21:27+00:00
- **Updated**: 2019-05-30 15:27:56+00:00
- **Authors**: Ziyao Li, Liang Zhang, Guojie Song
- **Comment**: IJCAI2019 Accepted Paper
- **Journal**: None
- **Summary**: Graph Convolutional Networks (GCNs) have proved to be a most powerful architecture in aggregating local neighborhood information for individual graph nodes. Low-rank proximities and node features are successfully leveraged in existing GCNs, however, attributes that graph links may carry are commonly ignored, as almost all of these models simplify graph links into binary or scalar values describing node connectedness. In our paper instead, links are reverted to hypostatic relationships between entities with descriptional attributes. We propose GCN-LASE (GCN with Link Attributes and Sampling Estimation), a novel GCN model taking both node and link attributes as inputs. To adequately captures the interactions between link and node attributes, their tensor product is used as neighbor features, based on which we define several graph kernels and further develop according architectures for LASE. Besides, to accelerate the training process, the sum of features in entire neighborhoods are estimated through Monte Carlo method, with novel sampling strategies designed for LASE to minimize the estimation variance. Our experiments show that LASE outperforms strong baselines over various graph datasets, and further experiments corroborate the informativeness of link attributes and our model's ability of adequately leveraging them.



### Generative Visual Dialogue System via Adaptive Reasoning and Weighted Likelihood Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.09818v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09818v2)
- **Published**: 2019-02-26 09:23:28+00:00
- **Updated**: 2019-08-14 01:13:44+00:00
- **Authors**: Heming Zhang, Shalini Ghosh, Larry Heck, Stephen Walsh, Junting Zhang, Jie Zhang, C. -C. Jay Kuo
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: The key challenge of generative Visual Dialogue (VD) systems is to respond to human queries with informative answers in natural and contiguous conversation flow. Traditional Maximum Likelihood Estimation (MLE)-based methods only learn from positive responses but ignore the negative responses, and consequently tend to yield safe or generic responses. To address this issue, we propose a novel training scheme in conjunction with weighted likelihood estimation (WLE) method. Furthermore, an adaptive multi-modal reasoning module is designed, to accommodate various dialogue scenarios automatically and select relevant information accordingly. The experimental results on the VisDial benchmark demonstrate the superiority of our proposed algorithm over other state-of-the-art approaches, with an improvement of 5.81% on recall@10.



### Capsule Neural Network based Height Classification using Low-Cost Automotive Ultrasonic Sensors
- **Arxiv ID**: http://arxiv.org/abs/1902.09839v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09839v1)
- **Published**: 2019-02-26 10:15:32+00:00
- **Updated**: 2019-02-26 10:15:32+00:00
- **Authors**: Maximilian Pöpperl, Raghavendra Gulagundi, Senthil Yogamani, Stefan Milz
- **Comment**: None
- **Journal**: None
- **Summary**: High performance ultrasonic sensor hardware is mainly used in medical applications. Although, the development in automotive scenarios is towards autonomous driving, the ultrasonic sensor hardware still stays low-cost and low-performance, respectively. To overcome the strict hardware limitations, we propose to use capsule neural networks. By the high classification capability of this network architecture, we can achieve outstanding results for performing a detailed height analysis of detected objects. We apply a novel resorting and reshaping method to feed the neural network with ultrasonic data. This increases classification performance and computation speed. We tested the approach under different environmental conditions to verify that the proposed method is working independent of external parameters that is needed for autonomous driving.



### Realistic Ultrasonic Environment Simulation Using Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.09842v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09842v1)
- **Published**: 2019-02-26 10:22:25+00:00
- **Updated**: 2019-02-26 10:22:25+00:00
- **Authors**: Maximilian Pöpperl, Raghavendra Gulagundi, Senthil Yogamani, Stefan Milz
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, realistic data augmentation using neural networks especially generative neural networks (GAN) has achieved outstanding results. The communities main research focus is visual image processing. However, automotive cars and robots are equipped with a large suite of sensors to achieve a high redundancy. In addition to others, ultrasonic sensors are often used due to their low-costs and reliable near field distance measuring capabilities. Hence, Pattern recognition needs to be applied to ultrasonic signals as well. Machine Learning requires extensive data sets and those measurements are time-consuming, expensive and not flexible to hardware and environmental changes. On the other hand, there exists no method to simulate those signals deterministically. We present a novel approach for synthetic ultrasonic signal simulation using conditional GANs (cGANs). For the best of our knowledge, we present the first realistic data augmentation for automotive ultrasonics. The performance of cGANs allows us to bring the realistic environment simulation to a new level. By using setup and environmental parameters as condition, the proposed approach is flexible to external influences. Due to the low complexity and time effort for data generation, we outperform other simulation algorithms, such as finite element method. We verify the outstanding accuracy and realism of our method by applying a detailed statistical analysis and comparing the generated data to an extensive amount of measured signals.



### Associatively Segmenting Instances and Semantics in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1902.09852v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09852v2)
- **Published**: 2019-02-26 10:38:26+00:00
- **Updated**: 2019-02-28 07:04:18+00:00
- **Authors**: Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, Jiaya Jia
- **Comment**: Accepted by CVPR2019
- **Journal**: None
- **Summary**: A 3D point cloud describes the real scene precisely and intuitively.To date how to segment diversified elements in such an informative 3D scene is rarely discussed. In this paper, we first introduce a simple and flexible framework to segment instances and semantics in point clouds simultaneously. Then, we propose two approaches which make the two tasks take advantage of each other, leading to a win-win situation. Specifically, we make instance segmentation benefit from semantic segmentation through learning semantic-aware point-level instance embedding. Meanwhile, semantic features of the points belonging to the same instance are fused together to make more accurate per-point semantic predictions. Our method largely outperforms the state-of-the-art method in 3D instance segmentation along with a significant improvement in 3D semantic segmentation. Code has been made available at: https://github.com/WXinlong/ASIS.



### Learning More with Less: Conditional PGGAN-based Data Augmentation for Brain Metastases Detection Using Highly-Rough Annotation on MR Images
- **Arxiv ID**: http://arxiv.org/abs/1902.09856v5
- **DOI**: 10.1145/3357384.3357890
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.09856v5)
- **Published**: 2019-02-26 10:46:53+00:00
- **Updated**: 2019-08-22 11:01:29+00:00
- **Authors**: Changhee Han, Kohei Murao, Tomoyuki Noguchi, Yusuke Kawata, Fumiya Uchiyama, Leonardo Rundo, Hideki Nakayama, Shin'ichi Satoh
- **Comment**: 9 pages, 7 figures, accepted to CIKM 2019 (acceptance rate: 19%)
- **Journal**: None
- **Summary**: Accurate Computer-Assisted Diagnosis, associated with proper data wrangling, can alleviate the risk of overlooking the diagnosis in a clinical environment. Towards this, as a Data Augmentation (DA) technique, Generative Adversarial Networks (GANs) can synthesize additional training data to handle the small/fragmented medical imaging datasets collected from various scanners; those images are realistic but completely different from the original ones, filling the data lack in the real image distribution. However, we cannot easily use them to locate disease areas, considering expert physicians' expensive annotation cost. Therefore, this paper proposes Conditional Progressive Growing of GANs (CPGGANs), incorporating highly-rough bounding box conditions incrementally into PGGANs to place brain metastases at desired positions/sizes on 256 X 256 Magnetic Resonance (MR) images, for Convolutional Neural Network-based tumor detection; this first GAN-based medical DA using automatic bounding box annotation improves the training robustness. The results show that CPGGAN-based DA can boost 10% sensitivity in diagnosis with clinically acceptable additional False Positives. Surprisingly, further tumor realism, achieved with additional normal brain MR images for CPGGAN training, does not contribute to detection performance, while even three physicians cannot accurately distinguish them from the real ones in Visual Turing Test.



### Variational Multi-Phase Segmentation using High-Dimensional Local Features
- **Arxiv ID**: http://arxiv.org/abs/1902.09863v1
- **DOI**: 10.1109/WACV.2016.7477729
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09863v1)
- **Published**: 2019-02-26 11:19:20+00:00
- **Updated**: 2019-02-26 11:19:20+00:00
- **Authors**: Niklas Mevenkamp, Benjamin Berkels
- **Comment**: None
- **Journal**: 2016 IEEE Winter Conference on Applications of Computer Vision
  (WACV)
- **Summary**: We propose a novel method for multi-phase segmentation of images based on high-dimensional local feature vectors. While the method was developed for the segmentation of extremely noisy crystal images based on localized Fourier transforms, the resulting framework is not tied to specific feature descriptors. For instance, using local spectral histograms as features, it allows for robust texture segmentation. The segmentation itself is based on the multi-phase Mumford-Shah model. Initializing the high-dimensional mean features directly is computationally too demanding and ill-posed in practice. This is resolved by projecting the features onto a low-dimensional space using principle component analysis. The resulting objective functional is minimized using a convexification and the Chambolle-Pock algorithm. Numerical results are presented, illustrating that the algorithm is very competitive in texture segmentation with state-of-the-art performance on the Prague benchmark and provides new possibilities in crystal segmentation, being robust to extreme noise and requiring no prior knowledge of the crystal structure.



### RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.09868v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09868v2)
- **Published**: 2019-02-26 11:23:54+00:00
- **Updated**: 2019-03-12 14:20:04+00:00
- **Authors**: Bastian Wandt, Bodo Rosenhahn
- **Comment**: accepted to CVPR 2019
- **Journal**: None
- **Summary**: This paper addresses the problem of 3D human pose estimation from single images. While for a long time human skeletons were parameterized and fitted to the observation by satisfying a reprojection error, nowadays researchers directly use neural networks to infer the 3D pose from the observations. However, most of these approaches ignore the fact that a reprojection constraint has to be satisfied and are sensitive to overfitting. We tackle the overfitting problem by ignoring 2D to 3D correspondences. This efficiently avoids a simple memorization of the training data and allows for a weakly supervised training. One part of the proposed reprojection network (RepNet) learns a mapping from a distribution of 2D poses to a distribution of 3D poses using an adversarial training approach. Another part of the network estimates the camera. This allows for the definition of a network layer that performs the reprojection of the estimated 3D pose back to 2D which results in a reprojection loss function. Our experiments show that RepNet generalizes well to unknown data and outperforms state-of-the-art methods when applied to unseen data. Moreover, our implementation runs in real-time on a standard desktop PC.



### MC-ISTA-Net: Adaptive Measurement and Initialization and Channel Attention Optimization inspired Neural Network for Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1902.09878v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09878v3)
- **Published**: 2019-02-26 12:02:39+00:00
- **Updated**: 2019-08-11 23:30:00+00:00
- **Authors**: Nanyu Li, Cuiyin Liu
- **Comment**: We request withdraw this paper for the reasons stated as following.
  The paper is not published in any journal. Some errors and insufficient
  experiments are found in the recently work. The rectified work and relevant
  supplementary experiments will be done for this paper in the future. After
  these, we will submit the new version of this paper
- **Journal**: None
- **Summary**: The optimization inspired network can bridge convex optimization and neural networks in Compressive Sensing (CS) reconstruction of natural image, like ISTA-Net+, which mapping optimization algorithm: iterative shrinkage-thresholding algorithm (ISTA) into network. However, measurement matrix and input initialization are still hand-crafted, and multi-channel feature map contain information at different frequencies, which is treated equally across channels, hindering the ability of CS reconstruction in optimization-inspired networks. In order to solve the above problems, we proposed MC-ISTA-Net



### Disentangled Representation Learning for 3D Face Shape
- **Arxiv ID**: http://arxiv.org/abs/1902.09887v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09887v2)
- **Published**: 2019-02-26 12:22:18+00:00
- **Updated**: 2019-03-03 06:38:24+00:00
- **Authors**: Zi-Hang Jiang, Qianyi Wu, Keyu Chen, Juyong Zhang
- **Comment**: 15 pages, 8 figures. CVPR 2019
- **Journal**: None
- **Summary**: In this paper, we present a novel strategy to design disentangled 3D face shape representation. Specifically, a given 3D face shape is decomposed into identity part and expression part, which are both encoded and decoded in a nonlinear way. To solve this problem, we propose an attribute decomposition framework for 3D face mesh. To better represent face shapes which are usually nonlinear deformed between each other, the face shapes are represented by a vertex based deformation representation rather than Euclidean coordinates. The experimental results demonstrate that our method has better performance than existing methods on decomposing the identity and expression parts. Moreover, more natural expression transfer results can be achieved with our method than existing methods.



### Imaging and Classification Techniques for Seagrass Mapping and Monitoring: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/1902.11114v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.11114v2)
- **Published**: 2019-02-26 12:29:15+00:00
- **Updated**: 2019-03-01 08:25:50+00:00
- **Authors**: Md Moniruzzaman, S. M. Shamsul Islam, Paul Lavery, Mohammed Bennamoun, C. Peng Lam
- **Comment**: 36 pages, 14 figures, 8tables
- **Journal**: None
- **Summary**: Monitoring underwater habitats is a vital part of observing the condition of the environment. The detection and mapping of underwater vegetation, especially seagrass has drawn the attention of the research community as early as the nineteen eighties. Initially, this monitoring relied on in situ observation by experts. Later, advances in remote-sensing technology, satellite-monitoring techniques and, digital photo- and video-based techniques opened a window to quicker, cheaper, and, potentially, more accurate seagrass-monitoring methods. So far, for seagrass detection and mapping, digital images from airborne cameras, spectral images from satellites, acoustic image data using underwater sonar technology, and digital underwater photo and video images have been used to map the seagrass meadows or monitor their condition. In this article, we have reviewed the recent approaches to seagrass detection and mapping to understand the gaps of the present approaches and determine further research scope to monitor the ocean health more easily. We have identified four classes of approach to seagrass mapping and assessment: still image-, video data-, acoustic image-, and spectral image data-based techniques. We have critically analysed the surveyed approaches and found the research gaps including the need for quick, cheap and effective imaging techniques robust to depth, turbidity, location and weather conditions, fully automated seagrass detectors that can work in real-time, accurate techniques for estimating the seagrass density, and the availability of high computation facilities for processing large scale data. For addressing these gaps, future research should focus on developing cheaper image and video data collection techniques, deep learning based automatic annotation and classification, and real-time percentage-cover calculation.



### Unsupervised Segmentation Algorithms' Implementation in ITK for Tissue Classification via Human Head MRI Scans
- **Arxiv ID**: http://arxiv.org/abs/1902.11131v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1902.11131v4)
- **Published**: 2019-02-26 12:48:43+00:00
- **Updated**: 2020-01-25 12:11:55+00:00
- **Authors**: Shadman Sakib, Md. Abu Bakr Siddique
- **Comment**: 4 Pages, 2 Tables
- **Journal**: None
- **Summary**: Tissue classification is one of the significant tasks in the field of biomedical image analysis. Magnetic Resonance Imaging (MRI) is of great importance in tissue classification especially in the areas of brain tissue classification which is able to recognize anatomical areas of interest such as surgical planning, monitoring therapy, clinical drug trials, image registration, stereotactic neurosurgery, radiotherapy etc. The task of this paper is to implement different unsupervised classification algorithms in ITK and perform tissue classification (white matter, gray matter, cerebrospinal fluid (CSF) and background of the human brain). For this purpose, 5 grayscale head MRI scans are provided. In order of classifying brain tissues, three algorithms are used. These are: Otsu thresholding, Bayesian classification and Bayesian classification with Gaussian smoothing. The obtained classification results are analyzed in the results and discussion section.



### Diagnosis of Alzheimer's Disease via Multi-modality 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1902.09904v1
- **DOI**: 10.3389/fnins.2019.00509
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09904v1)
- **Published**: 2019-02-26 13:01:44+00:00
- **Updated**: 2019-02-26 13:01:44+00:00
- **Authors**: Yechong Huang, Jiahang Xu, Yuncheng Zhou, Tong Tong, Xiahai Zhuang, the Alzheimer's Disease Neuroimaging Initiative
- **Comment**: 21 pages, 5 figures, 9 tables
- **Journal**: https://www.frontiersin.org/articles/10.3389/fnins.2019.00509/full
- **Summary**: Alzheimer's Disease (AD) is one of the most concerned neurodegenerative diseases. In the last decade, studies on AD diagnosis attached great significance to artificial intelligence (AI)-based diagnostic algorithms. Among the diverse modality imaging data, T1-weighted MRI and 18F-FDGPET are widely researched for this task. In this paper, we propose a novel convolutional neural network (CNN) to fuse the multi-modality information including T1-MRI and FDG-PDT images around the hippocampal area for the diagnosis of AD. Different from the traditional machine learning algorithms, this method does not require manually extracted features, and utilizes the stateof-art 3D image-processing CNNs to learn features for the diagnosis and prognosis of AD. To validate the performance of the proposed network, we trained the classifier with paired T1-MRI and FDG-PET images using the ADNI datasets, including 731 Normal (NL) subjects, 647 AD subjects, 441 stable MCI (sMCI) subjects and 326 progressive MCI (pMCI) subjects. We obtained the maximal accuracies of 90.10% for NL/AD task, 87.46% for NL/pMCI task, and 76.90% for sMCI/pMCI task. The proposed framework yields comparative results against state-of-the-art approaches. Moreover, the experimental results have demonstrated that (1) segmentation is not a prerequisite by using CNN, (2) the hippocampal area provides enough information to give a reference to AD diagnosis. Keywords: Alzheimer's Disease, Multi-modality, Image Classification, CNN, Deep Learning, Hippocampal



### Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos
- **Arxiv ID**: http://arxiv.org/abs/1902.09907v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09907v2)
- **Published**: 2019-02-26 13:03:15+00:00
- **Updated**: 2019-05-23 12:24:00+00:00
- **Authors**: Haofei Xu, Jianmin Zheng, Jianfei Cai, Juyong Zhang
- **Comment**: IJCAI 2019
- **Journal**: None
- **Summary**: While learning based depth estimation from images/videos has achieved substantial progress, there still exist intrinsic limitations. Supervised methods are limited by a small amount of ground truth or labeled data and unsupervised methods for monocular videos are mostly based on the static scene assumption, not performing well on real world scenarios with the presence of dynamic objects. In this paper, we propose a new learning based method consisting of DepthNet, PoseNet and Region Deformer Networks (RDN) to estimate depth from unconstrained monocular videos without ground truth supervision. The core contribution lies in RDN for proper handling of rigid and non-rigid motions of various objects such as rigidly moving cars and deformable humans. In particular, a deformation based motion representation is proposed to model individual object motion on 2D images. This representation enables our method to be applicable to diverse unconstrained monocular videos. Our method can not only achieve the state-of-the-art results on standard benchmarks KITTI and Cityscapes, but also show promising results on a crowded pedestrian tracking dataset, which demonstrates the effectiveness of the deformation based motion representation. Code and trained models are available at https://github.com/haofeixu/rdn4depth.



### IF-TTN: Information Fused Temporal Transformation Network for Video Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.09928v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09928v2)
- **Published**: 2019-02-26 13:44:08+00:00
- **Updated**: 2019-04-11 16:35:39+00:00
- **Authors**: Ke Yang, Peng Qiao, Dongsheng Li, Yong Dou
- **Comment**: None
- **Journal**: None
- **Summary**: Effective spatiotemporal feature representation is crucial to the video-based action recognition task. Focusing on discriminate spatiotemporal feature learning, we propose Information Fused Temporal Transformation Network (IF-TTN) for action recognition on top of popular Temporal Segment Network (TSN) framework. In the network, Information Fusion Module (IFM) is designed to fuse the appearance and motion features at multiple ConvNet levels for each video snippet, forming a short-term video descriptor. With fused features as inputs, Temporal Transformation Networks (TTN) are employed to model middle-term temporal transformation between the neighboring snippets following a sequential order. As TSN itself depicts long-term temporal structure by segmental consensus, the proposed network comprehensively considers multiple granularity temporal features. Our IF-TTN achieves the state-of-the-art results on two most popular action recognition datasets: UCF101 and HMDB51. Empirical investigation reveals that our architecture is robust to the input motion map quality. Replacing optical flow with the motion vectors from compressed video stream, the performance is still comparable to the flow-based methods while the testing speed is 10x faster.



### SuperTML: Two-Dimensional Word Embedding for the Precognition on Structured Tabular Data
- **Arxiv ID**: http://arxiv.org/abs/1903.06246v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1903.06246v3)
- **Published**: 2019-02-26 13:57:28+00:00
- **Updated**: 2019-06-04 02:16:48+00:00
- **Authors**: Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, Jason Dong
- **Comment**: 9 pages, 5 figures, 3 tables. Accepted by CVPR2019 Precognition
  Workshop
- **Journal**: None
- **Summary**: Tabular data is the most commonly used form of data in industry. Gradient Boosting Trees, Support Vector Machine, Random Forest, and Logistic Regression are typically used for classification tasks on tabular data. DNN models using categorical embeddings are also applied in this task, but all attempts thus far have used one-dimensional embeddings. The recent work of Super Characters method using two-dimensional word embeddings achieved the state of art result in text classification tasks, showcasing the promise of this new approach. In this paper, we propose the SuperTML method, which borrows the idea of Super Characters method and two-dimensional embeddings to address the problem of classification on tabular data. For each input of tabular data, the features are first projected into two-dimensional embeddings like an image, and then this image is fed into fine-tuned two-dimensional CNN models for classification. Experimental results have shown that the proposed SuperTML method had achieved state-of-the-art results on both large and small datasets.



### Unsupervised Part Mining for Fine-grained Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1902.09941v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09941v2)
- **Published**: 2019-02-26 14:04:58+00:00
- **Updated**: 2022-03-31 13:20:38+00:00
- **Authors**: Runsheng Zhang, jian zhang, Yaping Huang, Qi Zou
- **Comment**: 10 pages,4 figures
- **Journal**: None
- **Summary**: Fine-grained image classification remains challenging due to the large intra-class variance and small inter-class variance. Since the subtle visual differences are only in local regions of discriminative parts among subcategories, part localization is a key issue for fine-grained image classification. Most existing approaches localize object or parts in an image with object or part annotations, which are expensive and labor-consuming. To tackle this issue, we propose a fully unsupervised part mining (UPM) approach to localize the discriminative parts without even image-level annotations, which largely improves the fine-grained classification performance. We first utilize pattern mining techniques to discover frequent patterns, i.e., co-occurrence highlighted regions, in the feature maps extracted from a pre-trained convolutional neural network (CNN) model. Inspired by the fact that these relevant meaningful patterns typically hold appearance and spatial consistency, we then cluster the mined regions to obtain the cluster centers and the discriminative parts surrounding the cluster centers are generated. Importantly, any annotations and sophisticated training procedures are not used in our proposed part localization approach. Finally, a multi-stream classification network is built for aggregating the original, object-level and part-level features simultaneously. Compared with other state-of-the-art approaches, our UPM approach achieves the competitive performance.



### An Annotation Saved is an Annotation Earned: Using Fully Synthetic Training for Object Instance Detection
- **Arxiv ID**: http://arxiv.org/abs/1902.09967v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09967v1)
- **Published**: 2019-02-26 14:36:35+00:00
- **Updated**: 2019-02-26 14:36:35+00:00
- **Authors**: Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina Marek, Martin Bokeloh
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning methods typically require vast amounts of training data to reach their full potential. While some publicly available datasets exists, domain specific data always needs to be collected and manually labeled, an expensive, time consuming and error prone process. Training with synthetic data is therefore very lucrative, as dataset creation and labeling comes for free. We propose a novel method for creating purely synthetic training data for object detection. We leverage a large dataset of 3D background models and densely render them using full domain randomization. This yields background images with realistic shapes and texture on top of which we render the objects of interest. During training, the data generation process follows a curriculum strategy guaranteeing that all foreground models are presented to the network equally under all possible poses and conditions with increasing complexity. As a result, we entirely control the underlying statistics and we create optimal training samples at every stage of training. Using a set of 64 retail objects, we demonstrate that our simple approach enables the training of detectors that outperform models trained with real data on a challenging evaluation dataset.



### Object Discovery From a Single Unlabeled Image by Mining Frequent Itemset With Multi-scale Features
- **Arxiv ID**: http://arxiv.org/abs/1902.09968v3
- **DOI**: 10.1109/TIP.2020.3015543
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.09968v3)
- **Published**: 2019-02-26 14:37:01+00:00
- **Updated**: 2020-08-08 05:05:12+00:00
- **Authors**: Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang, Qingji Guan, Qi Zou, Haibin Ling
- **Comment**: None
- **Journal**: None
- **Summary**: TThe goal of our work is to discover dominant objects in a very general setting where only a single unlabeled image is given. This is far more challenge than typical co-localization or weakly-supervised localization tasks. To tackle this problem, we propose a simple but effective pattern mining-based method, called Object Location Mining (OLM), which exploits the advantages of data mining and feature representation of pre-trained convolutional neural networks (CNNs). Specifically, we first convert the feature maps from a pre-trained CNN model into a set of transactions, and then discovers frequent patterns from transaction database through pattern mining techniques. We observe that those discovered patterns, i.e., co-occurrence highlighted regions, typically hold appearance and spatial consistency. Motivated by this observation, we can easily discover and localize possible objects by merging relevant meaningful patterns. Extensive experiments on a variety of benchmarks demonstrate that OLM achieves competitive localization performance compared with the state-of-the-art methods. We also evaluate our approach compared with unsupervised saliency detection methods and achieves competitive results on seven benchmark datasets. Moreover, we conduct experiments on fine-grained classification to show that our proposed method can locate the entire object and parts accurately, which can benefit to improving the classification results significantly.



### Anomalous Situation Detection in Complex Scenes
- **Arxiv ID**: http://arxiv.org/abs/1902.10016v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10016v1)
- **Published**: 2019-02-26 15:56:27+00:00
- **Updated**: 2019-02-26 15:56:27+00:00
- **Authors**: Michalis Voutouris, Giovanni Sachi, Hina Afridi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper we investigate a robust method to identify anomalies in complex scenes. This task is performed by evaluating the collective behavior by extracting the local binary patterns (LBP) and Laplacian of Gaussian (LoG) features. We fuse both features together which are exploited to train an MLP neural network during the training stage, and the anomaly is identified on the test samples. Considering the challenge of tracking individuals in dense crowded scenes due to multiple occlusions and clutter, in this paper we extract LBP and LoG features and use them as an approximate representation of the anomalous situation. These features well match the appearance of anomaly and their consistency, and accuracy is higher both in regular and irregular areas compared to other descriptors. In this paper, these features are exploited as input prior to train the neural network. The MLP neural network is subsequently explored to consider these features that can detect the anomalous situation. The experimental tests are conducted on a set of benchmark video sequences commonly used for anomaly situation detection.



### STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection
- **Arxiv ID**: http://arxiv.org/abs/1902.10024v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10024v1)
- **Published**: 2019-02-26 16:06:29+00:00
- **Updated**: 2019-02-26 16:06:29+00:00
- **Authors**: William McNally, Alexander Wong, John McPhee
- **Comment**: None
- **Journal**: None
- **Summary**: While depth cameras and inertial sensors have been frequently leveraged for human action recognition, these sensing modalities are impractical in many scenarios where cost or environmental constraints prohibit their use. As such, there has been recent interest on human action recognition using low-cost, readily-available RGB cameras via deep convolutional neural networks. However, many of the deep convolutional neural networks proposed for action recognition thus far have relied heavily on learning global appearance cues directly from imaging data, resulting in highly complex network architectures that are computationally expensive and difficult to train. Motivated to reduce network complexity and achieve higher performance, we introduce the concept of spatio-temporal activation reprojection (STAR). More specifically, we reproject the spatio-temporal activations generated by human pose estimation layers in space and time using a stack of 3D convolutions. Experimental results on UTD-MHAD and J-HMDB demonstrate that an end-to-end architecture based on the proposed STAR framework (which we nickname STAR-Net) is proficient in single-environment and small-scale applications. On UTD-MHAD, STAR-Net outperforms several methods using richer data modalities such as depth and inertial sensors.



### A framework for information extraction from tables in biomedical literature
- **Arxiv ID**: http://arxiv.org/abs/1902.10031v1
- **DOI**: 10.1007/s10032-019-00317-0
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10031v1)
- **Published**: 2019-02-26 16:22:15+00:00
- **Updated**: 2019-02-26 16:22:15+00:00
- **Authors**: Nikola Milosevic, Cassie Gregson, Robert Hernandez, Goran Nenadic
- **Comment**: 24 pages
- **Journal**: 2019, International Journal on Document Analysis and Recognition
  (IJDAR)
- **Summary**: The scientific literature is growing exponentially, and professionals are no more able to cope with the current amount of publications. Text mining provided in the past methods to retrieve and extract information from text; however, most of these approaches ignored tables and figures. The research done in mining table data still does not have an integrated approach for mining that would consider all complexities and challenges of a table. Our research is examining the methods for extracting numerical (number of patients, age, gender distribution) and textual (adverse reactions) information from tables in the clinical literature. We present a requirement analysis template and an integral methodology for information extraction from tables in clinical domain that contains 7 steps: (1) table detection, (2) functional processing, (3) structural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell selection and (7) syntactic processing and extraction. Our approach performed with the F-measure ranged between 82 and 92%, depending on the variable, task and its complexity.



### TCDCaps: Visual Tracking via Cascaded Dense Capsules
- **Arxiv ID**: http://arxiv.org/abs/1902.10054v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10054v2)
- **Published**: 2019-02-26 17:02:05+00:00
- **Updated**: 2019-11-23 06:58:30+00:00
- **Authors**: Ding Ma, Xiangqian Wu
- **Comment**: The description of DCaps have some mistakes
- **Journal**: None
- **Summary**: The critical challenge in tracking-by-detection framework is how to avoid drift problem during online learning, where the robust features for a variety of appearance changes are difficult to be learned and a reasonable intersection over union (IoU) threshold that defines the true/false positives is hard to set. This paper presents the TCDCaps method to address the problems above via a cascaded dense capsule architecture. To get robust features, we extend original capsules with dense-connected routing, which are referred as DCaps. Depending on the preservation of part-whole relationships in the Capsule Networks, our dense-connected capsules can capture a variety of appearance variations. In addition, to handle the issue of IoU threshold, a cascaded DCaps model (CDCaps) is proposed to improve the quality of candidates, it consists of sequential DCaps trained with increasing IoU thresholds so as to sequentially improve the quality of candidates. Extensive experiments on 3 popular benchmarks demonstrate the robustness of the proposed TCDCaps.



### A Multi-Domain Feature Learning Method for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.10058v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10058v1)
- **Published**: 2019-02-26 17:09:49+00:00
- **Updated**: 2019-02-26 17:09:49+00:00
- **Authors**: Peng Yin, Lingyun Xu, Xueqian Li, Chen Yin, Yingli Li, Rangaprasad Arun Srivatsan, Lu Li, Jianmin Ji, Yuqing He
- **Comment**: 6 pages, 5 figures, ICRA 2019 accepted
- **Journal**: None
- **Summary**: Visual Place Recognition (VPR) is an important component in both computer vision and robotics applications, thanks to its ability to determine whether a place has been visited and where specifically. A major challenge in VPR is to handle changes of environmental conditions including weather, season and illumination. Most VPR methods try to improve the place recognition performance by ignoring the environmental factors, leading to decreased accuracy decreases when environmental conditions change significantly, such as day versus night. To this end, we propose an end-to-end conditional visual place recognition method. Specifically, we introduce the multi-domain feature learning method (MDFL) to capture multiple attribute-descriptions for a given place, and then use a feature detaching module to separate the environmental condition-related features from those that are not. The only label required within this feature learning pipeline is the environmental condition. Evaluation of the proposed method is conducted on the multi-season \textit{NORDLAND} dataset, and the multi-weather \textit{GTAV} dataset. Experimental results show that our method improves the feature robustness against variant environmental conditions.



### MRS-VPR: a multi-resolution sampling based global visual place recognition method
- **Arxiv ID**: http://arxiv.org/abs/1902.10059v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.10059v1)
- **Published**: 2019-02-26 17:10:16+00:00
- **Updated**: 2019-02-26 17:10:16+00:00
- **Authors**: Peng Yin, Rangaprasad Arun Srivatsan, Yin Chen, Xueqian Li, Hongda Zhang, Lingyun Xu, Lu Li, Zhenzhong Jia, Jianmin Ji, Yuqing He
- **Comment**: 6 pages, 5 figures, ICRA 2019, accepted
- **Journal**: None
- **Summary**: Place recognition and loop closure detection are challenging for long-term visual navigation tasks. SeqSLAM is considered to be one of the most successful approaches to achieving long-term localization under varying environmental conditions and changing viewpoints. It depends on a brute-force, time-consuming sequential matching method. We propose MRS-VPR, a multi-resolution, sampling-based place recognition method, which can significantly improve the matching efficiency and accuracy in sequential matching. The novelty of this method lies in the coarse-to-fine searching pipeline and a particle filter-based global sampling scheme, that can balance the matching efficiency and accuracy in the long-term navigation task. Moreover, our model works much better than SeqSLAM when the testing sequence has a much smaller scale than the reference sequence. Our experiments demonstrate that the proposed method is efficient in locating short temporary trajectories within long-term reference ones without losing accuracy compared to SeqSLAM.



### SceneFlowFields++: Multi-frame Matching, Visibility Prediction, and Robust Interpolation for Scene Flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1902.10099v2
- **DOI**: 10.1007/s11263-019-01258-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10099v2)
- **Published**: 2019-02-26 18:29:33+00:00
- **Updated**: 2019-10-28 09:27:22+00:00
- **Authors**: René Schuster, Oliver Wasenmüller, Christian Unger, Georg Kuschk, Didier Stricker
- **Comment**: arXiv admin note: text overlap with arXiv:1710.10096
- **Journal**: None
- **Summary**: State-of-the-art scene flow algorithms pursue the conflicting targets of accuracy, run time, and robustness. With the successful concept of pixel-wise matching and sparse-to-dense interpolation, we push the limits of scene flow estimation. Avoiding strong assumptions on the domain or the problem yields a more robust algorithm. This algorithm is fast because we avoid explicit regularization during matching, which allows an efficient computation. Using image information from multiple time steps and explicit visibility prediction based on previous results, we achieve competitive performances on different data sets. Our contributions and results are evaluated in comparative experiments. Overall, we present an accurate scene flow algorithm that is faster and more generic than any individual benchmark leader.



### Unmasking Clever Hans Predictors and Assessing What Machines Really Learn
- **Arxiv ID**: http://arxiv.org/abs/1902.10178v1
- **DOI**: 10.1038/s41467-019-08987-4
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10178v1)
- **Published**: 2019-02-26 19:25:11+00:00
- **Updated**: 2019-02-26 19:25:11+00:00
- **Authors**: Sebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller
- **Comment**: Accepted for publication in Nature Communications
- **Journal**: None
- **Summary**: Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly "intelligent" behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.



### High Flux Passive Imaging with Single-Photon Sensors
- **Arxiv ID**: http://arxiv.org/abs/1902.10190v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.ins-det, I.3.3; I.4.1
- **Links**: [PDF](http://arxiv.org/pdf/1902.10190v2)
- **Published**: 2019-02-26 20:05:42+00:00
- **Updated**: 2019-04-23 22:20:02+00:00
- **Authors**: Atul Ingle, Andreas Velten, Mohit Gupta
- **Comment**: 28 pages, 15 figures, addressed reviewers's comments, fixed some
  errors and typos, official peer reviewed version to appear in IEEE CVPR 2019
- **Journal**: None
- **Summary**: Single-photon avalanche diodes (SPADs) are an emerging technology with a unique capability of capturing individual photons with high timing precision. SPADs are being used in several active imaging systems (e.g., fluorescence lifetime microscopy and LiDAR), albeit mostly limited to low photon flux settings. We propose passive free-running SPAD (PF-SPAD) imaging, an imaging modality that uses SPADs for capturing 2D intensity images with unprecedented dynamic range under ambient lighting, without any active light source. Our key observation is that the precise inter-photon timing measured by a SPAD can be used for estimating scene brightness under ambient lighting conditions, even for very bright scenes. We develop a theoretical model for PF-SPAD imaging, and derive a scene brightness estimator based on the average time of darkness between successive photons detected by a PF-SPAD pixel. Our key insight is that due to the stochastic nature of photon arrivals, this estimator does not suffer from a hard saturation limit. Coupled with high sensitivity at low flux, this enables a PF-SPAD pixel to measure a wide range of scene brightness, from very low to very high, thereby achieving extreme dynamic range. We demonstrate an improvement of over 2 orders of magnitude over conventional sensors by imaging scenes spanning a dynamic range of 1,000,000:1.



### Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPU
- **Arxiv ID**: http://arxiv.org/abs/1902.10194v1
- **DOI**: 10.1109/LRA.2019.2895264
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.10194v1)
- **Published**: 2019-02-26 20:13:14+00:00
- **Updated**: 2019-02-26 20:13:14+00:00
- **Authors**: Georgi Tinchev, Adrian Penate-Sanchez, Maurice Fallon
- **Comment**: Accepted for publication at RA-L/ICRA 2019. More info:
  https://ori.ox.ac.uk/esm-localization
- **Journal**: None
- **Summary**: Localization in challenging, natural environments such as forests or woodlands is an important capability for many applications from guiding a robot navigating along a forest trail to monitoring vegetation growth with handheld sensors. In this work we explore laser-based localization in both urban and natural environments, which is suitable for online applications. We propose a deep learning approach capable of learning meaningful descriptors directly from 3D point clouds by comparing triplets (anchor, positive and negative examples). The approach learns a feature space representation for a set of segmented point clouds that are matched between a current and previous observations. Our learning method is tailored towards loop closure detection resulting in a small model which can be deployed using only a CPU. The proposed learning method would allow the full pipeline to run on robots with limited computational payload such as drones, quadrupeds or UGVs.



### Differentiable Scene Graphs
- **Arxiv ID**: http://arxiv.org/abs/1902.10200v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10200v5)
- **Published**: 2019-02-26 20:22:33+00:00
- **Updated**: 2020-03-14 16:25:32+00:00
- **Authors**: Moshiko Raboh, Roei Herzig, Gal Chechik, Jonathan Berant, Amir Globerson
- **Comment**: Winter Conference on Applications of Computer Vision (WACV), 2020
- **Journal**: None
- **Summary**: Reasoning about complex visual scenes involves perception of entities and their relations. Scene graphs provide a natural representation for reasoning tasks, by assigning labels to both entities (nodes) and relations (edges). Unfortunately, reasoning systems based on SGs are typically trained in a two-step procedure: First, training a model to predict SGs from images; Then, a separate model is created to reason based on predicted SGs. In many domains, it is preferable to train systems jointly in an end-to-end manner, but SGs are not commonly used as intermediate components in visual reasoning systems because being discrete and sparse, scene-graph representations are non-differentiable and difficult to optimize. Here we propose Differentiable Scene Graphs (DSGs), an image representation that is amenable to differentiable end-to-end optimization, and requires supervision only from the downstream tasks. DSGs provide a dense representation for all regions and pairs of regions, and do not spend modelling capacity on areas of the images that do not contain objects or relations of interest. We evaluate our model on the challenging task of identifying referring relationships (RR) in three benchmark datasets, Visual Genome, VRD and CLEVR. We describe a multi-task objective, and train in an end-to-end manner supervised by the downstream RR task. Using DSGs as an intermediate representation leads to new state-of-the-art performance.



### Deep MR Fingerprinting with total-variation and low-rank subspace priors
- **Arxiv ID**: http://arxiv.org/abs/1902.10205v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10205v1)
- **Published**: 2019-02-26 20:33:02+00:00
- **Updated**: 2019-02-26 20:33:02+00:00
- **Authors**: Mohammad Golbabaee, Carolin M. Pirkl, Marion I. Menzel, Guido Buonincontri, Pedro A. Gómez
- **Comment**: None
- **Journal**: International Society for Magnetic Resonance in Medicine,
  Montreal, Canada, 2019
- **Summary**: Deep learning (DL) has recently emerged to address the heavy storage and computation requirements of the baseline dictionary-matching (DM) for Magnetic Resonance Fingerprinting (MRF) reconstruction. Fed with non-iterated back-projected images, the network is unable to fully resolve spatially-correlated corruptions caused from the undersampling artefacts. We propose an accelerated iterative reconstruction to minimize these artefacts before feeding into the network. This is done through a convex regularization that jointly promotes spatio-temporal regularities of the MRF time-series. Except for training, the rest of the parameter estimation pipeline is dictionary-free. We validate the proposed approach on synthetic and in-vivo datasets.



### A Dictionary-Based Generalization of Robust PCA Part II: Applications to Hyperspectral Demixing
- **Arxiv ID**: http://arxiv.org/abs/1902.10238v1
- **DOI**: 10.1109/TSP.2020.2977458
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10238v1)
- **Published**: 2019-02-26 21:42:17+00:00
- **Updated**: 2019-02-26 21:42:17+00:00
- **Authors**: Sirisha Rambhatla, Xingguo Li, Jineng Ren, Jarvis Haupt
- **Comment**: 13 pages; Index Terms - Hyperspectral imaging, Robust-PCA, Dictionary
  Sparse, Matrix Demixing, Target Localization, and Remote Sensing
- **Journal**: None
- **Summary**: We consider the task of localizing targets of interest in a hyperspectral (HS) image based on their spectral signature(s), by posing the problem as two distinct convex demixing task(s). With applications ranging from remote sensing to surveillance, this task of target detection leverages the fact that each material/object possesses its own characteristic spectral response, depending upon its composition. However, since $\textit{signatures}$ of different materials are often correlated, matched filtering-based approaches may not be apply here. To this end, we model a HS image as a superposition of a low-rank component and a dictionary sparse component, wherein the dictionary consists of the $\textit{a priori}$ known characteristic spectral responses of the target we wish to localize, and develop techniques for two different sparsity structures, resulting from different model assumptions. We also present the corresponding recovery guarantees, leveraging our recent theoretical results from a companion paper. Finally, we analyze the performance of the proposed approach via experimental evaluations on real HS datasets for a classification task, and compare its performance with related techniques.



### Target-based Hyperspectral Demixing via Generalized Robust PCA
- **Arxiv ID**: http://arxiv.org/abs/1902.11111v1
- **DOI**: 10.1109/ACSSC.2017.8335372
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.11111v1)
- **Published**: 2019-02-26 21:43:51+00:00
- **Updated**: 2019-02-26 21:43:51+00:00
- **Authors**: Sirisha Rambhatla, Xingguo Li, Jarvis Haupt
- **Comment**: 5 Pages; Index Terms - Hyperspectral imaging, Robust-PCA, Dictionary
  Sparse, Matrix Demixing, Target Localization, and Remote Sensing. arXiv admin
  note: substantial text overlap with arXiv:1902.10238
- **Journal**: 2017 51st Asilomar Conference on Signals, Systems, and Computers
- **Summary**: Localizing targets of interest in a given hyperspectral (HS) image has applications ranging from remote sensing to surveillance. This task of target detection leverages the fact that each material/object possesses its own characteristic spectral response, depending upon its composition. As $\textit{signatures}$ of different materials are often correlated, matched filtering based approaches may not be appropriate in this case. In this work, we present a technique to localize targets of interest based on their spectral signatures. We also present the corresponding recovery guarantees, leveraging our recent theoretical results. To this end, we model a HS image as a superposition of a low-rank component and a dictionary sparse component, wherein the dictionary consists of the $\textit{a priori}$ known characteristic spectral responses of the target we wish to localize. Finally, we analyze the performance of the proposed approach via experimental validation on real HS data for a classification task, and compare it with related techniques.



### TensorMap: Lidar-Based Topological Mapping and Localization via Tensor Decompositions
- **Arxiv ID**: http://arxiv.org/abs/1902.10226v1
- **DOI**: 10.1109/GlobalSIP.2018.8646665
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.10226v1)
- **Published**: 2019-02-26 21:46:20+00:00
- **Updated**: 2019-02-26 21:46:20+00:00
- **Authors**: Sirisha Rambhatla, Nikos D. Sidiropoulos, Jarvis Haupt
- **Comment**: 5 pages; Index Terms - Topological maps, Lidar, Localization of
  Autonomous Vehicles, Orthogonal Tucker Decomposition, and Scan-matching
- **Journal**: 2018 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP)
- **Summary**: We propose a technique to develop (and localize in) topological maps from light detection and ranging (Lidar) data. Localizing an autonomous vehicle with respect to a reference map in real-time is crucial for its safe operation. Owing to the rich information provided by Lidar sensors, these are emerging as a promising choice for this task. However, since a Lidar outputs a large amount of data every fraction of a second, it is progressively harder to process the information in real-time. Consequently, current systems have migrated towards faster alternatives at the expense of accuracy. To overcome this inherent trade-off between latency and accuracy, we propose a technique to develop topological maps from Lidar data using the orthogonal Tucker3 tensor decomposition. Our experimental evaluations demonstrate that in addition to achieving a high compression ratio as compared to full data, the proposed technique, $\textit{TensorMap}$, also accurately detects the position of the vehicle in a graph-based representation of a map. We also analyze the robustness of the proposed technique to Gaussian and translational noise, thus initiating explorations into potential applications of tensor decompositions in Lidar data analysis.



