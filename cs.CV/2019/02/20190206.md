# Arxiv Papers in cs.CV on 2019-02-06
### Semi-supervised learning via Feedforward-Designed Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1902.01980v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.01980v1)
- **Published**: 2019-02-06 00:00:23+00:00
- **Updated**: 2019-02-06 00:00:23+00:00
- **Authors**: Yueru Chen, Yijing Yang, Min Zhang, C. -C. Jay Kuo
- **Comment**: 5 pages, under review of ICIP 2019
- **Journal**: None
- **Summary**: A semi-supervised learning framework using the feedforward-designed convolutional neural networks (FF-CNNs) is proposed for image classification in this work. One unique property of FF-CNNs is that no backpropagation is used in model parameters determination. Since unlabeled data may not always enhance semi-supervised learning, we define an effective quality score and use it to select a subset of unlabeled data in the training process. We conduct experiments on the MNIST, SVHN, and CIFAR-10 datasets, and show that the proposed semi-supervised FF-CNN solution outperforms the CNN trained by backpropagation (BP-CNN) when the amount of labeled data is reduced. Furthermore, we develop an ensemble system that combines the output decision vectors of different semi-supervised FF-CNNs to boost classification accuracy. The ensemble systems can achieve further performance gains on all three benchmarking datasets.



### Bidirectional Inference Networks: A Class of Deep Bayesian Networks for Health Profiling
- **Arxiv ID**: http://arxiv.org/abs/1902.02037v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02037v1)
- **Published**: 2019-02-06 06:10:46+00:00
- **Updated**: 2019-02-06 06:10:46+00:00
- **Authors**: Hao Wang, Chengzhi Mao, Hao He, Mingmin Zhao, Tommi S. Jaakkola, Dina Katabi
- **Comment**: Appeared at AAAI 2019
- **Journal**: None
- **Summary**: We consider the problem of inferring the values of an arbitrary set of variables (e.g., risk of diseases) given other observed variables (e.g., symptoms and diagnosed diseases) and high-dimensional signals (e.g., MRI images or EEG). This is a common problem in healthcare since variables of interest often differ for different patients. Existing methods including Bayesian networks and structured prediction either do not incorporate high-dimensional signals or fail to model conditional dependencies among variables. To address these issues, we propose bidirectional inference networks (BIN), which stich together multiple probabilistic neural networks, each modeling a conditional dependency. Predictions are then made via iteratively updating variables using backpropagation (BP) to maximize corresponding posterior probability. Furthermore, we extend BIN to composite BIN (CBIN), which involves the iterative prediction process in the training stage and improves both accuracy and computational efficiency by adaptively smoothing the optimization landscape. Experiments on synthetic and real-world datasets (a sleep study and a dermatology dataset) show that CBIN is a single model that can achieve state-of-the-art performance and obtain better accuracy in most inference tasks than multiple models each specifically trained for a different task.



### Fooling Neural Network Interpretations via Adversarial Model Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1902.02041v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.02041v3)
- **Published**: 2019-02-06 06:28:09+00:00
- **Updated**: 2019-11-01 00:16:17+00:00
- **Authors**: Juyeon Heo, Sunghwan Joo, Taesup Moon
- **Comment**: None
- **Journal**: NeurIPS 2019, ICCV workshop 2019
- **Summary**: We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.



### DeepIrisNet2: Learning Deep-IrisCodes from Scratch for Segmentation-Robust Visible Wavelength and Near Infrared Iris Recognition
- **Arxiv ID**: http://arxiv.org/abs/1902.05390v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.05390v1)
- **Published**: 2019-02-06 08:07:37+00:00
- **Updated**: 2019-02-06 08:07:37+00:00
- **Authors**: Abhishek Gangwar, Akanksha Joshi, Padmaja Joshi, R. Raghavendra
- **Comment**: 10 pages, 4 Figures
- **Journal**: None
- **Summary**: We first, introduce a deep learning based framework named as DeepIrisNet2 for visible spectrum and NIR Iris representation. The framework can work without classical iris normalization step or very accurate iris segmentation; allowing to work under non-ideal situation. The framework contains spatial transformer layers to handle deformation and supervision branches after certain intermediate layers to mitigate overfitting. In addition, we present a dual CNN iris segmentation pipeline comprising of a iris/pupil bounding boxes detection network and a semantic pixel-wise segmentation network. Furthermore, to get compact templates, we present a strategy to generate binary iris codes using DeepIrisNet2. Since, no ground truth dataset are available for CNN training for iris segmentation, We build large scale hand labeled datasets and make them public; i) iris, pupil bounding boxes, ii) labeled iris texture. The networks are evaluated on challenging ND-IRIS-0405, UBIRIS.v2, MICHE-I, and CASIA v4 Interval datasets. Proposed approach significantly improves the state-of-the-art and achieve outstanding performance surpassing all previous methods.



### Content-based image retrieval system with most relevant features among wavelet and color features
- **Arxiv ID**: http://arxiv.org/abs/1902.02059v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1902.02059v1)
- **Published**: 2019-02-06 08:22:06+00:00
- **Updated**: 2019-02-06 08:22:06+00:00
- **Authors**: Abdolreza Rashno, Elyas Rashno
- **Comment**: None
- **Journal**: None
- **Summary**: Content-based image retrieval (CBIR) has become one of the most important research directions in the domain of digital data management. In this paper, a new feature extraction schema including the norm of low frequency components in wavelet transformation and color features in RGB and HSV domains are proposed as representative feature vector for images in database followed by appropriate similarity measure for each feature type. In CBIR systems, retrieving results are so sensitive to image features. We address this problem with selection of most relevant features among complete feature set by ant colony optimization (ACO)-based feature selection which minimize the number of features as well as maximize F-measure in CBIR system. To evaluate the performance of our proposed CBIR system, it has been compared with three older proposed systems. Results show that the precision and recall of our proposed system are higher than older ones for the majority of image categories in Corel database.



### UrbanFM: Inferring Fine-Grained Urban Flows
- **Arxiv ID**: http://arxiv.org/abs/1902.05377v1
- **DOI**: 10.1145/3292500.3330646
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.05377v1)
- **Published**: 2019-02-06 08:22:18+00:00
- **Updated**: 2019-02-06 08:22:18+00:00
- **Authors**: Yuxuan Liang, Kun Ouyang, Lin Jing, Sijie Ruan, Ye Liu, Junbo Zhang, David S. Rosenblum, Yu Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Urban flow monitoring systems play important roles in smart city efforts around the world. However, the ubiquitous deployment of monitoring devices, such as CCTVs, induces a long-lasting and enormous cost for maintenance and operation. This suggests the need for a technology that can reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity. In this paper, we aim to infer the real-time and fine-grained crowd flows throughout a city based on coarse-grained observations. This task is challenging due to two reasons: the spatial correlations between coarse- and fine-grained urban flows, and the complexities of external impacts. To tackle these issues, we develop a method entitled UrbanFM based on deep neural networks. Our model consists of two major parts: 1) an inference network to generate fine-grained flow distributions from coarse-grained inputs by using a feature extraction module and a novel distributional upsampling module; 2) a general fusion subnet to further boost the performance by considering the influences of different external factors. Extensive experiments on two real-world datasets, namely TaxiBJ and HappyValley, validate the effectiveness and efficiency of our method compared to seven baselines, demonstrating the state-of-the-art performance of our approach on the fine-grained urban flow inference problem.



### Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1902.02067v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02067v3)
- **Published**: 2019-02-06 08:58:37+00:00
- **Updated**: 2020-06-01 05:29:56+00:00
- **Authors**: Derui Wang, Chaoran Li, Sheng Wen, Qing-Long Han, Surya Nepal, Xiangyu Zhang, Yang Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: This paper demonstrates that Non-Maximum Suppression (NMS), which is commonly used in Object Detection (OD) tasks to filter redundant detection results, is no longer secure. Considering that NMS has been an integral part of OD systems, thwarting the functionality of NMS can result in unexpected or even lethal consequences for such systems. In this paper, an adversarial example attack which triggers malfunctioning of NMS in end-to-end OD models is proposed. The attack, namely \texttt{Daedalus}, compresses the dimensions of detection boxes to evade NMS. As a result, the final detection output contains extremely dense false positives. This can be fatal for many OD applications such as autonomous vehicles and surveillance systems. The attack can be generalised to different end-to-end OD models, such that the attack cripples various OD applications. Furthermore, a way to craft robust adversarial examples is developed by using an ensemble of popular detection models as the substitutes. Considering the pervasive nature of model reusing in real-world OD scenarios, Daedalus examples crafted based on an \textit{ensemble of substitutes} can launch attacks without knowing the parameters of the victim models. Experimental results demonstrate that the attack effectively stops NMS from filtering redundant bounding boxes. As the evaluation results suggest, Daedalus increases the false positive rate in detection results to $99.9\%$ and reduces the mean average precision scores to $0$, while maintaining a low cost of distortion on the original inputs. It is also demonstrated that the attack can be practically launched against real-world OD systems via printed posters.



### Fingerprint Recognition under Missing Image Pixels Scenario
- **Arxiv ID**: http://arxiv.org/abs/1902.05389v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05389v1)
- **Published**: 2019-02-06 09:42:17+00:00
- **Updated**: 2019-02-06 09:42:17+00:00
- **Authors**: Dejan Brajovic, Kristina Tomovic, Jovan Radonjic
- **Comment**: Student paper, submitted to The 8th Mediterranean Conference on
  Embedded Computing - MECO'2019
- **Journal**: None
- **Summary**: This work observed the problem of fingerprint image recognition in the case of missing pixels from the original image. The possibility of missing pixels recovery is tested by applying the Compressive Sensing approach. Namely, different percentage of missing pixels is observed and the image reconstruction is done by applying commonly used approach for sparse image reconstruction. The theory is verified by experiments, showing successful image reconstruction and later person identification even if less then 90% of the image pixels is missing.



### GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/1902.02086v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02086v1)
- **Published**: 2019-02-06 09:50:51+00:00
- **Updated**: 2019-02-06 09:50:51+00:00
- **Authors**: Punarjay Chakravarty, Praveen Narayanan, Tom Roussel
- **Comment**: Accepted for ICRA 2019
- **Journal**: None
- **Summary**: We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.



### BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling
- **Arxiv ID**: http://arxiv.org/abs/1902.02102v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1902.02102v3)
- **Published**: 2019-02-06 10:36:57+00:00
- **Updated**: 2019-11-06 16:48:34+00:00
- **Authors**: Lars Maaløe, Marco Fraccaro, Valentin Liévin, Ole Winther
- **Comment**: None
- **Journal**: None
- **Summary**: With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.



### Face Recognition using Compressive Sensing
- **Arxiv ID**: http://arxiv.org/abs/1902.05388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.05388v1)
- **Published**: 2019-02-06 11:21:12+00:00
- **Updated**: 2019-02-06 11:21:12+00:00
- **Authors**: Slavko Kovacevic, Vuko Djaletic, Jelena Vukovic
- **Comment**: Student paper submitted to The 8th Mediterranean Conference on
  Embedded Computing - MECO'2019
- **Journal**: None
- **Summary**: This paper deals with the Compressive Sensing implementation in the Face Recognition problem. Compressive Sensing is new approach in signal processing with a single goal to recover signal from small set of available samples. Compressive Sensing finds its usage in many real applications as it lowers the memory demand and acquisition time, and therefore allows dealing with huge data in the fastest manner. In this paper, the undersampled signal is recovered using the algorithm based on Total Variation minimization. The theory is verified with an experimental results using different percentage of signal samples.



### Progressive Generative Adversarial Networks for Medical Image Super resolution
- **Arxiv ID**: http://arxiv.org/abs/1902.02144v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02144v2)
- **Published**: 2019-02-06 12:38:53+00:00
- **Updated**: 2019-02-18 02:59:05+00:00
- **Authors**: Dwarikanath Mahapatra, Behzad Bozorgtabar
- **Comment**: NA
- **Journal**: None
- **Summary**: Anatomical landmark segmentation and pathology localization are important steps in automated analysis of medical images. They are particularly challenging when the anatomy or pathology is small, as in retinal images and cardiac MRI, or when the image is of low quality due to device acquisition parameters as in magnetic resonance (MR) scanners. We propose an image super-resolution method using progressive generative adversarial networks (P-GAN) that can take as input a low-resolution image and generate a high resolution image of desired scaling factor. The super resolved images can be used for more accurate detection of landmarks and pathology. Our primary contribution is in proposing a multistage model where the output image quality of one stage is progressively improved in the next stage by using a triplet loss function. The triplet loss enables stepwise image quality improvement by using the output of the previous stage as the baseline. This facilitates generation of super resolved images of high scaling factor while maintaining good image quality. Experimental results for image super-resolution show that our proposed multistage P-GAN outperforms competing methods and baseline GAN.



### Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane Representation
- **Arxiv ID**: http://arxiv.org/abs/1902.02166v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02166v2)
- **Published**: 2019-02-06 13:26:03+00:00
- **Updated**: 2019-04-10 12:07:01+00:00
- **Authors**: Yuxin Hou, Arno Solin, Juho Kannala
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents a novel method, MaskMVS, to solve depth estimation for unstructured multi-view image-pose pairs. In the plane-sweep procedure, the depth planes are sampled by histogram matching that ensures covering the depth range of interest. Unlike other plane-sweep methods, we do not rely on a cost metric to explicitly build the cost volume, but instead infer a multiplane mask representation which regularizes the learning. Compared to many previous approaches, we show that our method is lightweight and generalizes well without requiring excessive training. We outperform the current state-of-the-art and show results on the sun3d, scenes11, MVS, and RGBD test data sets.



### Labelling Vertebrae with 2D Reformations of Multidetector CT Images: An Adversarial Approach for Incorporating Prior Knowledge of Spine Anatomy
- **Arxiv ID**: http://arxiv.org/abs/1902.02205v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02205v4)
- **Published**: 2019-02-06 14:34:35+00:00
- **Updated**: 2021-02-26 11:13:14+00:00
- **Authors**: Anjany Sekuboyina, Markus Rempfler, Alexander Valentinitsch, Bjoern H. Menze, Jan S. Kirschke
- **Comment**: Published in Radiology:AI
- **Journal**: None
- **Summary**: Purpose: To use and test a labelling algorithm that operates on two-dimensional (2D) reformations, rather than three-dimensional (3D) data to locate and identify vertebrae.   Methods: We improved the Btrfly Net (described by Sekuboyina et al) that works on sagittal and coronal maximum intensity projections (MIP) and augmented it with two additional components: spine-localization and adversarial a priori-learning. Furthermore, we explored two variants of adversarial training schemes that incorporated the anatomical a priori knowledge into the Btrfly Net. We investigated the superiority of the proposed approach for labelling vertebrae on three datasets: a public benchmarking dataset of 302 CT scans and two in-house datasets with a total of 238 CT scans. We employed Wilcoxon signed-rank test to compute the statistical significance of the improvement in performance observed due to various architectural components in our approach.   Results: On the public dataset, our approach using the described Btrfly(pe-eb) network performed on par with current state-of-the-art methods achieving a statistically significant (p < .001) vertebrae identification rate of 88.5+/-0.2 % and localization distances of less than 7-mm. On the in-house datasets that had a higher inter-scan data variability, we obtained an identification rate of 85.1+/-1.2%.   Conclusion: An identification performance comparable to existing 3D approaches was achieved when labelling vertebrae on 2D MIPs. The performance was further improved using the proposed adversarial training regime that effectively enforced local spine a priori knowledge during training. Lastly, spine-localization increased the generalizability of our approach by homogenizing the content in the MIPs.



### Generative Image Translation for Data Augmentation of Bone Lesion Pathology
- **Arxiv ID**: http://arxiv.org/abs/1902.02248v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1902.02248v1)
- **Published**: 2019-02-06 15:57:39+00:00
- **Updated**: 2019-02-06 15:57:39+00:00
- **Authors**: Anant Gupta, Srivas Venkatesh, Sumit Chopra, Christian Ledig
- **Comment**: None
- **Journal**: None
- **Summary**: Insufficient training data and severe class imbalance are often limiting factors when developing machine learning models for the classification of rare diseases. In this work, we address the problem of classifying bone lesions from X-ray images by increasing the small number of positive samples in the training set. We propose a generative data augmentation approach based on a cycle-consistent generative adversarial network that synthesizes bone lesions on images without pathology. We pose the generative task as an image-patch translation problem that we optimize specifically for distinct bones (humerus, tibia, femur). In experimental results, we confirm that the described method mitigates the class imbalance problem in the binary classification task of bone lesion detection. We show that the augmented training sets enable the training of superior classifiers achieving better performance on a held-out test set. Additionally, we demonstrate the feasibility of transfer learning and apply a generative model that was trained on one body part to another.



### CLEAR: A Consistent Lifting, Embedding, and Alignment Rectification Algorithm for Multi-View Data Association
- **Arxiv ID**: http://arxiv.org/abs/1902.02256v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/1902.02256v3)
- **Published**: 2019-02-06 16:12:57+00:00
- **Updated**: 2020-03-04 23:08:51+00:00
- **Authors**: Kaveh Fathian, Kasra Khosoussi, Yulun Tian, Parker Lusk, Jonathan P. How
- **Comment**: None
- **Journal**: None
- **Summary**: Many robotics applications require alignment and fusion of observations obtained at multiple views to form a global model of the environment. Multi-way data association methods provide a mechanism to improve alignment accuracy of pairwise associations and ensure their consistency. However, existing methods that solve this computationally challenging problem are often too slow for real-time applications. Furthermore, some of the existing techniques can violate the cycle consistency principle, thus drastically reducing the fusion accuracy. This work presents the CLEAR (Consistent Lifting, Embedding, and Alignment Rectification) algorithm to address these issues. By leveraging insights from the multi-way matching and spectral graph clustering literature, CLEAR provides cycle consistent and accurate solutions in a computationally efficient manner. Numerical experiments on both synthetic and real datasets are carried out to demonstrate the scalability and superior performance of our algorithm in real-world problems. This algorithmic framework can provide significant improvement in the accuracy and efficiency of existing discrete assignment problems, which traditionally use pairwise (but potentially inconsistent) correspondences. An implementation of CLEAR is made publicly available online.



### Simultaneous x, y Pixel Estimation and Feature Extraction for Multiple Small Objects in a Scene: A Description of the ALIEN Network
- **Arxiv ID**: http://arxiv.org/abs/1902.05387v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 68T45, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1902.05387v1)
- **Published**: 2019-02-06 16:45:32+00:00
- **Updated**: 2019-02-06 16:45:32+00:00
- **Authors**: Seth Zuckerman, Timothy Klein, Alexander Boxer, Christopher Goldman, Brian Lang
- **Comment**: 6 pages, 4 figures
- **Journal**: None
- **Summary**: We present a deep-learning network that detects multiple small objects (hundreds to thousands) in a scene while simultaneously estimating their x,y pixel locations together with a characteristic feature-set (for instance, target orientation and color). All estimations are performed in a single, forward pass which makes implementing the network fast and efficient. In this paper, we describe the architecture of our network --- nicknamed ALIEN --- and detail its performance when applied to vehicle detection.



### Deep Morphological Simplification Network (MS-Net) for Guided Registration of Brain Magnetic Resonance Images
- **Arxiv ID**: http://arxiv.org/abs/1902.02342v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02342v1)
- **Published**: 2019-02-06 18:58:58+00:00
- **Updated**: 2019-02-06 18:58:58+00:00
- **Authors**: Dongming Wei, Zhengwang Wu, Gang Li, Xiaohuan Cao, Dinggang Shen, Qian Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Objective: Deformable brain MR image registration is challenging due to large inter-subject anatomical variation. For example, the highly complex cortical folding pattern makes it hard to accurately align corresponding cortical structures of individual images. In this paper, we propose a novel deep learning way to simplify the difficult registration problem of brain MR images. Methods: We train a morphological simplification network (MS-Net), which can generate a "simple" image with less anatomical details based on the "complex" input. With MS-Net, the complexity of the fixed image or the moving image under registration can be reduced gradually, thus building an individual (simplification) trajectory represented by MS-Net outputs. Since the generated images at the ends of the two trajectories (of the fixed and moving images) are so simple and very similar in appearance, they are easy to register. Thus, the two trajectories can act as a bridge to link the fixed and the moving images, and guide their registration. Results: Our experiments show that the proposed method can achieve highly accurate registration performance on different datasets (i.e., NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be also easily transferred across diverse image datasets and obtain superior accuracy on surface alignment. Conclusion and Significance: We propose MS-Net as a powerful and flexible tool to simplify brain MR images and their registration. To our knowledge, this is the first work to simplify brain MR image registration by deep learning, instead of estimating deformation field directly.



### Real-time 3D Traffic Cone Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1902.02394v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.02394v2)
- **Published**: 2019-02-06 20:45:30+00:00
- **Updated**: 2019-06-05 21:17:37+00:00
- **Authors**: Ankit Dhall, Dengxin Dai, Luc Van Gool
- **Comment**: IEEE Intelligent Vehicles Symposium (IV'19). arXiv admin note: text
  overlap with arXiv:1809.10548
- **Journal**: None
- **Summary**: Considerable progress has been made in semantic scene understanding of road scenes with monocular cameras. It is, however, mainly related to certain classes such as cars and pedestrians. This work investigates traffic cones, an object class crucial for traffic control in the context of autonomous vehicles. 3D object detection using images from a monocular camera is intrinsically an ill-posed problem. In this work, we leverage the unique structure of traffic cones and propose a pipelined approach to the problem. Specifically, we first detect cones in images by a tailored 2D object detector; then, the spatial arrangement of keypoints on a traffic cone are detected by our deep structural regression network, where the fact that the cross-ratio is projection invariant is leveraged for network regularization; finally, the 3D position of cones is recovered by the classical Perspective n-Point algorithm. Extensive experiments show that our approach can accurately detect traffic cones and estimate their position in the 3D world in real time. The proposed method is also deployed on a real-time, critical system. It runs efficiently on the low-power Jetson TX2, providing accurate 3D position estimates, allowing a race-car to map and drive autonomously on an unseen track indicated by traffic cones. With the help of robust and accurate perception, our race-car won both Formula Student Competitions held in Italy and Germany in 2018, cruising at a top-speed of 54 kmph. Visualization of the complete pipeline, mapping and navigation can be found on our project page.



