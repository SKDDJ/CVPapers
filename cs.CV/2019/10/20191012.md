# Arxiv Papers in cs.CV on 2019-10-12
### Neural Memory Plasticity for Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.05448v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05448v1)
- **Published**: 2019-10-12 00:32:56+00:00
- **Updated**: 2019-10-12 00:32:56+00:00
- **Authors**: Tharindu Fernando, Simon Denman, David Ahmedt-Aristizabal, Sridha Sridharan, Kristin Laurens, Patrick Johnston, Clinton Fookes
- **Comment**: None
- **Journal**: None
- **Summary**: In the domain of machine learning, Neural Memory Networks (NMNs) have recently achieved impressive results in a variety of application areas including visual question answering, trajectory prediction, object tracking, and language modelling. However, we observe that the attention based knowledge retrieval mechanisms used in current NMNs restricts them from achieving their full potential as the attention process retrieves information based on a set of static connection weights. This is suboptimal in a setting where there are vast differences among samples in the data domain; such as anomaly detection where there is no consistent criteria for what constitutes an anomaly. In this paper, we propose a plastic neural memory access mechanism which exploits both static and dynamic connection weights in the memory read, write and output generation procedures. We demonstrate the effectiveness and flexibility of the proposed memory model in three challenging anomaly detection tasks in the medical domain: abnormal EEG identification, MRI tumour type classification and schizophrenia risk detection in children. In all settings, the proposed approach outperforms the current state-of-the-art. Furthermore, we perform an in-depth analysis demonstrating the utility of neural plasticity for the knowledge retrieval process and provide evidence on how the proposed memory model generates sparse yet informative memory outputs.



### MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.05449v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05449v1)
- **Published**: 2019-10-12 00:34:37+00:00
- **Updated**: 2019-10-12 00:34:37+00:00
- **Authors**: Yuning Chai, Benjamin Sapp, Mayank Bansal, Dragomir Anguelov
- **Comment**: Appears in CoRL 2019
- **Journal**: None
- **Summary**: Predicting human behavior is a difficult and crucial task required for motion planning. It is challenging in large part due to the highly uncertain and multi-modal set of possible outcomes in real-world domains such as autonomous driving. Beyond single MAP trajectory prediction, obtaining an accurate probability distribution of the future is an area of active interest. We present MultiPath, which leverages a fixed set of future state-sequence anchors that correspond to modes of the trajectory distribution. At inference, our model predicts a discrete distribution over the anchors and, for each anchor, regresses offsets from anchor waypoints along with uncertainties, yielding a Gaussian mixture at each time step. Our model is efficient, requiring only one forward inference pass to obtain multi-modal future distributions, and the output is parametric, allowing compact communication and analytical probabilistic queries. We show on several datasets that our model achieves more accurate predictions, and compared to sampling baselines, does so with an order of magnitude fewer trajectories.



### Complement Face Forensic Detection and Localization with FacialLandmarks
- **Arxiv ID**: http://arxiv.org/abs/1910.05455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05455v1)
- **Published**: 2019-10-12 01:09:15+00:00
- **Updated**: 2019-10-12 01:09:15+00:00
- **Authors**: Kritaphat Songsri-in, Stefanos Zafeiriou
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, Generative Adversarial Networks (GANs) and image manipulating methods are becoming more powerful and can produce highly realistic face images beyond human recognition which have raised significant concerns regarding the authenticity of digital media. Although there have been some prior works that tackle face forensic classification problem, it is not trivial to estimate edited locations from classification predictions. In this paper, we propose, to the best of our knowledge, the first rigorous face forensic localization dataset, which consists of genuine, generated, and manipulated face images. In particular, the pristine parts contain face images from CelebA and FFHQ datasets. The fake images are generated from various GANs methods, namely DCGANs, LSGANs, BEGANs, WGAN-GP, ProGANs, and StyleGANs. Lastly, the edited subset is generated from StarGAN and SEFCGAN based on free-form masks. In total, the dataset contains about 1.3 million facial images labelled with corresponding binary masks.   Based on the proposed dataset, we demonstrated that explicit adding facial landmarks information in addition to input images improves the performance. In addition, our proposed method consists of two branches and can coherently predict face forensic detection and localization to outperform the previous state-of-the-art techniques on the newly proposed dataset as well as the faceforecsic++ dataset especially on low-quality videos.



### Spoofing and Anti-Spoofing with Wax Figure Faces
- **Arxiv ID**: http://arxiv.org/abs/1910.05457v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05457v1)
- **Published**: 2019-10-12 01:13:06+00:00
- **Updated**: 2019-10-12 01:13:06+00:00
- **Authors**: Shan Jia, Xin Li, Chuanbo Hu, Zhengquan Xu
- **Comment**: None
- **Journal**: None
- **Summary**: We have witnessed rapid advances in both face presentation attack models and presentation attack detection (PAD) in recent years. Compared to widely studied 2D face presentation attacks (e.g. printed photos and video replays), 3D face presentation attacks are more challenging because face recognition systems (FRS) is more easily confused by the 3D characteristics of materials similar to real faces. Existing 3D face spoofing databases, mostly based on 3D facial masks, are restricted to small data size and suffer from poor authenticity due to the difficulty and expense of mask production. In this work, we introduce a wax figure face database (WFFD) as a novel and super-realistic 3D face presentation attack. This database contains 2300 image pairs (totally 4600) and 745 subjects including both real and wax figure faces with high diversity from online collections. On one hand, our experiments have demonstrated the spoofing potential of WFFD on three popular FRSs. On the other hand, we have developed a multi-feature voting scheme for wax figure face detection (anti-spoofing), which combines three discriminative features at the decision level. The proposed detection method was compared against several face PAD approaches and found to outperform other competing methods. Surprisingly, our fusion-based detection method achieves an Average Classification Error Rate (ACER) of 11.73\% on the WFFD database, which is even better than human-based detection.



### Saliency Guided Self-attention Network for Weakly and Semi-supervised Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.05475v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05475v2)
- **Published**: 2019-10-12 03:17:44+00:00
- **Updated**: 2020-01-12 08:24:38+00:00
- **Authors**: Qi Yao, Xiaojin Gong
- **Comment**: 10 pages, 5 figures. Accepted by IEEE ACCESS
- **Journal**: None
- **Summary**: Weakly supervised semantic segmentation (WSSS) using only image-level labels can greatly reduce the annotation cost and therefore has attracted considerable research interest. However, its performance is still inferior to the fully supervised counterparts. To mitigate the performance gap, we propose a saliency guided self-attention network (SGAN) to address the WSSS problem. The introduced self-attention mechanism is able to capture rich and extensive contextual information but may mis-spread attentions to unexpected regions. In order to enable this mechanism to work effectively under weak supervision, we integrate class-agnostic saliency priors into the self-attention mechanism and utilize class-specific attention cues as an additional supervision for SGAN. Our SGAN is able to produce dense and accurate localization cues so that the segmentation performance is boosted. Moreover, by simply replacing the additional supervisions with partially labeled ground-truth, SGAN works effectively for semi-supervised semantic segmentation as well. Experiments on the PASCAL VOC 2012 and COCO datasets show that our approach outperforms all other state-of-the-art methods in both weakly and semi-supervised settings.



### Frustum VoxNet for 3D object detection from RGB-D or Depth images
- **Arxiv ID**: http://arxiv.org/abs/1910.05483v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05483v3)
- **Published**: 2019-10-12 04:06:46+00:00
- **Updated**: 2023-05-25 02:56:37+00:00
- **Authors**: Xiaoke Shen, Ioannis Stamos
- **Comment**: Update for v3: Added 2D detection performance of using RGBDHS as
  input in appendix. page 8, add Acknowledgement. page 10, add Supplementary
  Material. The paper got accepted by 2020 Winter Conference on Applications of
  Computer Vision (WACV '20). The first arxiv version can be found here:
  arXiv:1910.05483
- **Journal**: None
- **Summary**: Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art, achieving a 2 times speedup.



### Combinational Class Activation Maps for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.05518v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05518v2)
- **Published**: 2019-10-12 07:29:59+00:00
- **Updated**: 2019-12-19 07:53:46+00:00
- **Authors**: Seunghan Yang, Yoonhyung Kim, Youngeun Kim, Changick Kim
- **Comment**: The paper was accepted to the IEEE Winter Conference on Applications
  of Computer Vision (WACV'2020)
- **Journal**: None
- **Summary**: Weakly supervised object localization has recently attracted attention since it aims to identify both class labels and locations of objects by using image-level labels. Most previous methods utilize the activation map corresponding to the highest activation source. Exploiting only one activation map of the highest probability class is often biased into limited regions or sometimes even highlights background regions. To resolve these limitations, we propose to use activation maps, named combinational class activation maps (CCAM), which are linear combinations of activation maps from the highest to the lowest probability class. By using CCAM for localization, we suppress background regions to help highlighting foreground objects more accurately. In addition, we design the network architecture to consider spatial relationships for localizing relevant object regions. Specifically, we integrate non-local modules into an existing base network at both low- and high-level layers. Our final model, named non-local combinational class activation maps (NL-CCAM), obtains superior performance compared to previous methods on representative object localization benchmarks including ILSVRC 2016 and CUB-200-2011. Furthermore, we show that the proposed method has a great capability of generalization by visualizing other datasets.



### Template-Instance Loss for Offline Handwritten Chinese Character Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.05545v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05545v1)
- **Published**: 2019-10-12 09:59:17+00:00
- **Updated**: 2019-10-12 09:59:17+00:00
- **Authors**: Yao Xiao, Dan Meng, Cewu Lu, Chi-Keung Tang
- **Comment**: Accepted by ICDAR 2019
- **Journal**: None
- **Summary**: The long-standing challenges for offline handwritten Chinese character recognition (HCCR) are twofold: Chinese characters can be very diverse and complicated while similarly looking, and cursive handwriting (due to increased writing speed and infrequent pen lifting) makes strokes and even characters connected together in a flowing manner. In this paper, we propose the template and instance loss functions for the relevant machine learning tasks in offline handwritten Chinese character recognition. First, the character template is designed to deal with the intrinsic similarities among Chinese characters. Second, the instance loss can reduce category variance according to classification difficulty, giving a large penalty to the outlier instance of handwritten Chinese character. Trained with the new loss functions using our deep network architecture HCCR14Layer model consisting of simple layers, our extensive experiments show that it yields state-of-the-art performance and beyond for offline HCCR.



### Stripe-based and Attribute-aware Network: A Two-Branch Deep Model for Vehicle Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.05549v1
- **DOI**: 10.1088/1361-6501/ab8b81
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05549v1)
- **Published**: 2019-10-12 10:51:35+00:00
- **Updated**: 2019-10-12 10:51:35+00:00
- **Authors**: Jingjing Qian, Wei Jiang, Hao Luo, Hongyan Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (Re-ID) has been attracting increasing interest in the field of computer vision due to the growing utilization of surveillance cameras in public security. However, vehicle Re-ID still suffers a similarity challenge despite the efforts made to solve this problem. This challenge involves distinguishing different instances with nearly identical appearances. In this paper, we propose a novel two-branch stripe-based and attribute-aware deep convolutional neural network (SAN) to learn the efficient feature embedding for vehicle Re-ID task. The two-branch neural network, consisting of stripe-based branch and attribute-aware branches, can adaptively extract the discriminative features from the visual appearance of vehicles. A horizontal average pooling and dimension-reduced convolutional layers are inserted into the stripe-based branch to achieve part-level features. Meanwhile, the attribute-aware branch extracts the global feature under the supervision of vehicle attribute labels to separate the similar vehicle identities with different attribute annotations. Finally, the part-level and global features are concatenated together to form the final descriptor of the input image for vehicle Re-ID. The final descriptor not only can separate vehicles with different attributes but also distinguish vehicle identities with the same attributes. The extensive experiments on both VehicleID and VeRi databases show that the proposed SAN method outperforms other state-of-the-art vehicle Re-ID approaches.



### Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models
- **Arxiv ID**: http://arxiv.org/abs/1910.06699v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.06699v1)
- **Published**: 2019-10-12 11:51:24+00:00
- **Updated**: 2019-10-12 11:51:24+00:00
- **Authors**: César Roberto de Souza, Adrien Gaidon, Yohann Cabon, Naila Murray, Antonio Manuel López
- **Comment**: Pre-print of the article accepted for publication in the Special
  Issue on Generating Realistic Visual Data of Human Behavior of the
  International Journal of Computer Vision (IJCV). arXiv admin note:
  substantial text overlap with arXiv:1612.00881
- **Journal**: None
- **Summary**: Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for "Procedural Human Action Videos". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.



### Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1910.05562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05562v1)
- **Published**: 2019-10-12 13:21:25+00:00
- **Updated**: 2019-10-12 13:21:25+00:00
- **Authors**: Seungmin Lee, Dongwan Kim, Namil Kim, Seong-Gyun Jeong
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch.



### DeepACEv2: Automated Chromosome Enumeration in Metaphase Cell Images Using Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.11091v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11091v5)
- **Published**: 2019-10-12 13:36:32+00:00
- **Updated**: 2020-07-19 12:51:51+00:00
- **Authors**: Li Xiao, Chunlong Luo, Tianqi Yu, Yufan Luo, Manqing Wang, Fuhai Yu, Yinhao Li, Chan Tian, Jie Qiao
- **Comment**: None
- **Journal**: None
- **Summary**: Chromosome enumeration is an essential but tedious procedure in karyotyping analysis. To automate the enumeration process, we develop a chromosome enumeration framework, DeepACEv2, based on the region based object detection scheme. The framework is developed following three steps. Firstly, we take the classical ResNet-101 as the backbone and attach the Feature Pyramid Network (FPN) to the backbone. The FPN takes full advantage of the multiple level features, and we only output the level of feature map that most of the chromosomes are assigned to. Secondly, we enhance the region proposal network's ability by adding a newly proposed Hard Negative Anchors Sampling to extract unapparent but essential information about highly confusing partial chromosomes. Next, to alleviate serious occlusion problems, besides the traditional detection branch, we novelly introduce an isolated Template Module branch to extract unique embeddings of each proposal by utilizing the chromosome's geometric information. The embeddings are further incorporated into the No Maximum Suppression (NMS) procedure to improve the detection of overlapping chromosomes. Finally, we design a Truncated Normalized Repulsion Loss and add it to the loss function to avoid inaccurate localization caused by occlusion. In the newly collected 1375 metaphase images that came from a clinical laboratory, a series of ablation studies validate the effectiveness of each proposed module. Combining them, the proposed DeepACEv2 outperforms all the previous methods, yielding the Whole Correct Ratio(WCR)(%) with respect to images as 71.39, and the Average Error Ratio(AER)(%) with respect to chromosomes as about 1.17.



### Context-Gated Convolution
- **Arxiv ID**: http://arxiv.org/abs/1910.05577v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.05577v4)
- **Published**: 2019-10-12 15:30:18+00:00
- **Updated**: 2020-07-17 16:59:19+00:00
- **Authors**: Xudong Lin, Lin Ma, Wei Liu, Shih-Fu Chang
- **Comment**: ECCV 2020 camera ready version with appendix
- **Journal**: None
- **Summary**: As the basic building block of Convolutional Neural Networks (CNNs), the convolutional layer is designed to extract local patterns and lacks the ability to model global context in its nature. Many efforts have been recently devoted to complementing CNNs with the global modeling ability, especially by a family of works on global feature interaction. In these works, the global context information is incorporated into local features before they are fed into convolutional layers. However, research on neuroscience reveals that the neurons' ability of modifying their functions dynamically according to context is essential for the perceptual tasks, which has been overlooked in most of CNNs. Motivated by this, we propose one novel Context-Gated Convolution (CGC) to explicitly modify the weights of convolutional layers adaptively under the guidance of global context. As such, being aware of the global context, the modulated convolution kernel of our proposed CGC can better extract representative local patterns and compose discriminative features. Moreover, our proposed CGC is lightweight and applicable with modern CNN architectures, and consistently improves the performance of CNNs according to extensive experiments on image classification, action recognition, and machine translation. Our code of this paper is available at https://github.com/XudongLinthu/context-gated-convolution.



### Emotion Generation and Recognition: A StarGAN Approach
- **Arxiv ID**: http://arxiv.org/abs/1910.11090v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11090v1)
- **Published**: 2019-10-12 16:24:46+00:00
- **Updated**: 2019-10-12 16:24:46+00:00
- **Authors**: Aritra Banerjee, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: The main idea of this ISO is to use StarGAN (A type of GAN model) to perform training and testing on an emotion dataset resulting in a emotion recognition which can be generated by the valence arousal score of the 7 basic expressions. We have created an entirely new dataset consisting of 4K videos. This dataset consists of all the basic 7 types of emotions: Happy, Sad, Angry, Surprised, Fear, Disgust, Neutral. We have performed face detection and alignment followed by annotating basic valence arousal values to the frames/images in the dataset depending on the emotions manually. Then the existing StarGAN model is trained on our created dataset after which some manual subjects were chosen to test the efficiency of the trained StarGAN model.



### Open-plan Glare Evaluator (OGE): A Demonstration of a New Glare Prediction Approach Using Machine Learning Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1910.05594v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05594v2)
- **Published**: 2019-10-12 16:33:02+00:00
- **Updated**: 2020-05-11 14:13:22+00:00
- **Authors**: Ayman Wagdy, Veronica Garcia-Hansen, Mohammed Elhenawy, Gillian Isoardi, Robin Drogemuller, Fatma Fathy
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting discomfort glare in open-plan offices is a challenging problem. Although glare research has existed for more than 50 years, all current glare metrics have accuracy limitations, especially in open-plan offices with low lighting levels. Thus, it is crucial to develop a new method to predict glare more accurately. This paper is the first to adopt Machine Learning (ML) approaches in the prediction of glare. This research aims to demonstrate the validity of this approach by comparing the accuracy of the new ML model for open-plan offices (OGE) to the accuracy of the existing glare metrics using local dataset. To utilize and test this approach, Post-Occupancy Evaluation (POE) and High Dynamic Range (HDR) images were collected from 80 occupants (n=80) in four different open-plan offices in Brisbane, Australia. Consequently, various multi-region luminance values, luminance, and glare indices were calculated and examined as input features to train ML models. The accuracy of the ML model was compared to the accuracy of 24 indices which were also evaluated using a Receiver Operating Characteristic (ROC) analysis to identify the best cutoff values (thresholds) for each index in open-plan configurations. Results showed that the ML approach could predict glare with an accuracy of 83.8% (0.80 true positive rate and 0.86 true negative rate), which outperformed the accuracy of the previously developed glare metrics. OGE is applicable for open-plan office situations with low vertical illuminance (200 to 600 lux). However, ML models can be trained with more substantial datasets to achieve global model.



### Facial Expression Recognition Using Human to Animated-Character Expression Translation
- **Arxiv ID**: http://arxiv.org/abs/1910.05595v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05595v1)
- **Published**: 2019-10-12 16:36:54+00:00
- **Updated**: 2019-10-12 16:36:54+00:00
- **Authors**: Kamran Ali, Ilkin Isler, Charles Hughes
- **Comment**: 8 Pages
- **Journal**: None
- **Summary**: Facial expression recognition is a challenging task due to two major problems: the presence of inter-subject variations in facial expression recognition dataset and impure expressions posed by human subjects. In this paper we present a novel Human-to-Animation conditional Generative Adversarial Network (HA-GAN) to overcome these two problems by using many (human faces) to one (animated face) mapping. Specifically, for any given input human expression image, our HA-GAN transfers the expression information from the input image to a fixed animated identity. Stylized animated characters from the Facial Expression Research Group-Database (FERGDB) are used for the generation of fixed identity. By learning this many-to-one identity mapping function using our proposed HA-GAN, the effect of inter-subject variations can be reduced in Facial Expression Recognition(FER). We also argue that the expressions in the generated animated images are pure expressions and since FER is performed on these generated images, the performance of facial expression recognition is improved. Our initial experimental results on the state-of-the-art datasets show that facial expression recognition carried out on the generated animated images using our HA-GAN framework outperforms the baseline deep neural network and produces comparable or even better results than the state-of-the-art methods for facial expression recognition.



### Unsupervised Adversarial Correction of Rigid MR Motion Artifacts
- **Arxiv ID**: http://arxiv.org/abs/1910.05597v1
- **DOI**: 10.1109/ISBI45749.2020.9098570
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05597v1)
- **Published**: 2019-10-12 16:51:08+00:00
- **Updated**: 2019-10-12 16:51:08+00:00
- **Authors**: Karim Armanious, Aastha Tanwar, Sherif Abdulatif, Thomas Küstner, Sergios Gatidis, Bin Yang
- **Comment**: Submitted to IEEE ISBI 2020
- **Journal**: None
- **Summary**: Motion is one of the main sources for artifacts in magnetic resonance (MR) images. It can have significant consequences on the diagnostic quality of the resultant scans. Previously, supervised adversarial approaches have been suggested for the correction of MR motion artifacts. However, these approaches suffer from the limitation of required paired co-registered datasets for training which are often hard or impossible to acquire. Building upon our previous work, we introduce a new adversarial framework with a new generator architecture and loss function for the unsupervised correction of severe rigid motion artifacts in the brain region. Quantitative and qualitative comparisons with other supervised and unsupervised translation approaches showcase the enhanced performance of the introduced framework.



### Facial Emotion Recognition using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.05602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05602v1)
- **Published**: 2019-10-12 17:31:19+00:00
- **Updated**: 2019-10-12 17:31:19+00:00
- **Authors**: Akash Saravanan, Gurudutt Perichetla, Dr. K. S. Gayathri
- **Comment**: AICV '18: International Symposium on Artificial Intelligence and
  Computer Vision. College of Engineering, Guindy. Chennai, India (September
  2018)
- **Journal**: None
- **Summary**: Facial expression recognition is a topic of great interest in most fields from artificial intelligence and gaming to marketing and healthcare. The goal of this paper is to classify images of human faces into one of seven basic emotions. A number of different models were experimented with, including decision trees and neural networks before arriving at a final Convolutional Neural Network (CNN) model. CNNs work better for image recognition tasks since they are able to capture spacial features of the inputs due to their large number of filters. The proposed model consists of six convolutional layers, two max pooling layers and two fully connected layers. Upon tuning of the various hyperparameters, this model achieved a final accuracy of 0.60.



### Cross-Domain Image Classification through Neural-Style Transfer Data Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.05611v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05611v1)
- **Published**: 2019-10-12 18:00:33+00:00
- **Updated**: 2019-10-12 18:00:33+00:00
- **Authors**: Yijie Xu, Arushi Goel
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: In particular, the lack of sufficient amounts of domain-specific data can reduce the accuracy of a classifier. In this paper, we explore the effects of style transfer-based data transformation on the accuracy of a convolutional neural network classifiers in the context of automobile detection under adverse winter weather conditions. The detection of automobiles under highly adverse weather conditions is a difficult task as such conditions present large amounts of noise in each image. The InceptionV2 architecture is trained on a composite dataset, consisting of either normal car image dataset , a mixture of normal and style transferred car images, or a mixture of normal car images and those taken at blizzard conditions, at a ratio of 80:20. All three classifiers are then tested on a dataset of car images taken at blizzard conditions and on vehicle-free snow landscape images. We evaluate and contrast the effectiveness of each classifier upon each dataset, and discuss the strengths and weaknesses of style-transfer based approaches to data augmentation.



### Recent Advances in Imaging Around Corners
- **Arxiv ID**: http://arxiv.org/abs/1910.05613v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05613v1)
- **Published**: 2019-10-12 18:01:52+00:00
- **Updated**: 2019-10-12 18:01:52+00:00
- **Authors**: Tomohiro Maeda, Guy Satat, Tristan Swedish, Lagnojita Sinha, Ramesh Raskar
- **Comment**: None
- **Journal**: None
- **Summary**: Seeing around corners, also known as non-line-of-sight (NLOS) imaging is a computational method to resolve or recover objects hidden around corners. Recent advances in imaging around corners have gained significant interest. This paper reviews different types of existing NLOS imaging techniques and discusses the challenges that need to be addressed, especially for their applications outside of a constrained laboratory environment. Our goal is to introduce this topic to broader research communities as well as provide insights that would lead to further developments in this research area.



### Improve Model Generalization and Robustness to Dataset Bias with Bias-regularized Learning and Domain-guided Augmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.06745v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06745v3)
- **Published**: 2019-10-12 18:15:20+00:00
- **Updated**: 2019-11-13 20:04:08+00:00
- **Authors**: Yundong Zhang, Hang Wu, Huiye Liu, Li Tong, May D Wang
- **Comment**: 9 pages, 5 figures
- **Journal**: None
- **Summary**: Deep Learning has thrived on the emergence of biomedical big data. However, medical datasets acquired at different institutions have inherent bias caused by various confounding factors such as operation policies, machine protocols, treatment preference and etc. As the result, models trained on one dataset, regardless of volume, cannot be confidently utilized for the others. In this study, we investigated model robustness to dataset bias using three large-scale Chest X-ray datasets: first, we assessed the dataset bias using vanilla training baseline; second, we proposed a novel multi-source domain generalization model by (a) designing a new bias-regularized loss function; and (b) synthesizing new data for domain augmentation. We showed that our model significantly outperformed the baseline and other approaches on data from unseen domain in terms of accuracy and various bias measures, without retraining or finetuning. Our method is generally applicable to other biomedical data, providing new algorithms for training models robust to bias for big data analysis and applications. Demo training code is publicly available.



### How are attributes expressed in face DCNNs?
- **Arxiv ID**: http://arxiv.org/abs/1910.05657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05657v1)
- **Published**: 2019-10-12 22:59:48+00:00
- **Updated**: 2019-10-12 22:59:48+00:00
- **Authors**: Prithviraj Dhar, Ankan Bansal, Carlos D. Castillo, Joshua Gleason, P. Jonathon Phillips, Rama Chellappa
- **Comment**: None
- **Journal**: None
- **Summary**: As deep networks become increasingly accurate at recognizing faces, it is vital to understand how these networks process faces. While these networks are solely trained to recognize identities, they also contain face related information such as sex, age, and pose of the face. The networks are not trained to learn these attributes. We introduce expressivity as a measure of how much a feature vector informs us about an attribute, where a feature vector can be from internal or final layers of a network. Expressivity is computed by a second neural network whose inputs are features and attributes. The output of the second neural network approximates the mutual information between feature vectors and an attribute. We investigate the expressivity for two different deep convolutional neural network (DCNN) architectures: a Resnet-101 and an Inception Resnet v2. In the final fully connected layer of the networks, we found the order of expressivity for facial attributes to be Age > Sex > Yaw. Additionally, we studied the changes in the encoding of facial attributes over training iterations. We found that as training progresses, expressivities of yaw, sex, and age decrease. Our technique can be a tool for investigating the sources of bias in a network and a step towards explaining the network's identity decisions.



