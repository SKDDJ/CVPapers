# Arxiv Papers in cs.CV on 2019-10-24
### LUTNet: Learning FPGA Configurations for Highly Efficient Neural Network Inference
- **Arxiv ID**: http://arxiv.org/abs/1910.12625v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12625v2)
- **Published**: 2019-10-24 00:04:56+00:00
- **Updated**: 2020-03-02 23:26:43+00:00
- **Authors**: Erwei Wang, James J. Davis, Peter Y. K. Cheung, George A. Constantinides
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.00938.
  Accepted manuscript uploaded 02/03/20. DOA 01/03/20
- **Journal**: None
- **Summary**: Research has shown that deep neural networks contain significant redundancy, and thus that high classification accuracy can be achieved even when weights and activations are quantized down to binary values. Network binarization on FPGAs greatly increases area efficiency by replacing resource-hungry multipliers with lightweight XNOR gates. However, an FPGA's fundamental building block, the K-LUT, is capable of implementing far more than an XNOR: it can perform any K-input Boolean operation. Inspired by this observation, we propose LUTNet, an end-to-end hardware-software framework for the construction of area-efficient FPGA-based neural network accelerators using the native LUTs as inference operators. We describe the realization of both unrolled and tiled LUTNet architectures, with the latter facilitating smaller, less power-hungry deployment over the former while sacrificing area and energy efficiency along with throughput. For both varieties, we demonstrate that the exploitation of LUT flexibility allows for far heavier pruning than possible in prior works, resulting in significant area savings while achieving comparable accuracy. Against the state-of-the-art binarized neural network implementation, we achieve up to twice the area efficiency for several standard network models when inferencing popular datasets. We also demonstrate that even greater energy efficiency improvements are obtainable.



### Weakly-Supervised Degree of Eye-Closeness Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.10845v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10845v1)
- **Published**: 2019-10-24 00:14:28+00:00
- **Updated**: 2019-10-24 00:14:28+00:00
- **Authors**: Eyasu Mequanint, Shuai Zhang, Bijan Forutanpour, Yingyong Qi, Ning Bi
- **Comment**: None
- **Journal**: None
- **Summary**: Following recent technological advances there is a growing interest in building non-intrusive methods that help us communicate with computing devices. In this regard, accurate information from eye is a promising input medium between a user and computing devices. In this paper we propose a method that captures the degree of eye closeness. Although many methods exist for detection of eyelid openness, they are inherently unable to satisfactorily perform in real world applications. Detailed eye state estimation is more important, in extracting meaningful information, than estimating whether eyes are open or closed. However, learning reliable eye state estimator requires accurate annotations which is cost prohibitive. In this work, we leverage synthetic face images which can be generated via computer graphics rendering techniques and automatically annotated with different levels of eye openness. These synthesized training data images, however, have a domain shift from real-world data. To alleviate this issue, we propose a weakly-supervised method which utilizes the accurate annotation from the synthetic data set, to learn accurate degree of eye openness, and the weakly labeled (open or closed) real world eye data set to control the domain shift. We introduce a data set of 1.3M synthetic face images with detail eye openness and eye gaze information, and 21k real-world images with open/closed annotation. The dataset will be released online upon acceptance. Extensive experiments validate the effectiveness of the proposed approach.



### Handheld Mobile Photography in Very Low Light
- **Arxiv ID**: http://arxiv.org/abs/1910.11336v1
- **DOI**: 10.1145/3355089.3356508
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.11336v1)
- **Published**: 2019-10-24 00:21:08+00:00
- **Updated**: 2019-10-24 00:21:08+00:00
- **Authors**: Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil Karnad, Qiurui He, Jonathan T. Barron, Dillon Sharlet, Ryan Geiss, Samuel W. Hasinoff, Yael Pritch, Marc Levoy
- **Comment**: 22 pages, 27 figures
- **Journal**: ACM Trans. Graph.38, 6, Article 164 (November 2019)
- **Summary**: Taking photographs in low light using a mobile phone is challenging and rarely produces pleasing results. Aside from the physical limits imposed by read noise and photon shot noise, these cameras are typically handheld, have small apertures and sensors, use mass-produced analog electronics that cannot easily be cooled, and are commonly used to photograph subjects that move, like children and pets. In this paper we describe a system for capturing clean, sharp, colorful photographs in light as low as 0.3~lux, where human vision becomes monochromatic and indistinct. To permit handheld photography without flash illumination, we capture, align, and combine multiple frames. Our system employs "motion metering", which uses an estimate of motion magnitudes (whether due to handshake or moving objects) to identify the number of frames and the per-frame exposure times that together minimize both noise and motion blur in a captured burst. We combine these frames using robust alignment and merging techniques that are specialized for high-noise imagery. To ensure accurate colors in such low light, we employ a learning-based auto white balancing algorithm. To prevent the photographs from looking like they were shot in daylight, we use tone mapping techniques inspired by illusionistic painting: increasing contrast, crushing shadows to black, and surrounding the scene with darkness. All of these processes are performed using the limited computational resources of a mobile device. Our system can be used by novice photographers to produce shareable pictures in a few seconds based on a single shutter press, even in environments so dim that humans cannot see clearly.



### Circulant Binary Convolutional Networks: Enhancing the Performance of 1-bit DCNNs with Circulant Back Propagation
- **Arxiv ID**: http://arxiv.org/abs/1910.10853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10853v1)
- **Published**: 2019-10-24 00:24:30+00:00
- **Updated**: 2019-10-24 00:24:30+00:00
- **Authors**: Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, David Doermann
- **Comment**: Published in CVPR2019
- **Journal**: ]Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition. 2019: 2691-2699
- **Summary**: The rapidly decreasing computation and memory cost has recently driven the success of many applications in the field of deep learning. Practical applications of deep learning in resource-limited hardware, such as embedded devices and smart phones, however, remain challenging. For binary convolutional networks, the reason lies in the degraded representation caused by binarizing full-precision filters. To address this problem, we propose new circulant filters (CiFs) and a circulant binary convolution (CBConv) to enhance the capacity of binarized convolutional features via our circulant back propagation (CBP). The CiFs can be easily incorporated into existing deep convolutional neural networks (DCNNs), which leads to new Circulant Binary Convolutional Networks (CBCNs). Extensive experiments confirm that the performance gap between the 1-bit and full-precision DCNNs is minimized by increasing the filter diversity, which further increases the representational ability in our networks. Our experiments on ImageNet show that CBCNs achieve 61.4% top-1 accuracy with ResNet18. Compared to the state-of-the-art such as XNOR, CBCNs can achieve up to 10% higher top-1 accuracy with more powerful representational ability.



### Aggregation Signature for Small Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.10859v1
- **DOI**: 10.1109/TIP.2019.2940477
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10859v1)
- **Published**: 2019-10-24 00:41:13+00:00
- **Updated**: 2019-10-24 00:41:13+00:00
- **Authors**: Chunlei Liu, Wenrui Ding, Jinyu Yang, Vittorio Murino, Baochang Zhang, Jungong Han, Guodong Guo
- **Comment**: IEEE Transactions on Image Processing, 2019
- **Journal**: None
- **Summary**: Small object tracking becomes an increasingly important task, which however has been largely unexplored in computer vision. The great challenges stem from the facts that: 1) small objects show extreme vague and variable appearances, and 2) they tend to be lost easier as compared to normal-sized ones due to the shaking of lens. In this paper, we propose a novel aggregation signature suitable for small object tracking, especially aiming for the challenge of sudden and large drift. We make three-fold contributions in this work. First, technically, we propose a new descriptor, named aggregation signature, based on saliency, able to represent highly distinctive features for small objects. Second, theoretically, we prove that the proposed signature matches the foreground object more accurately with a high probability. Third, experimentally, the aggregation signature achieves a high performance on multiple datasets, outperforming the state-of-the-art methods by large margins. Moreover, we contribute with two newly collected benchmark datasets, i.e., small90 and small112, for visually small object tracking. The datasets will be available in https://github.com/bczhangbczhang/.



### Fast and Differentiable Message Passing on Pairwise Markov Random Fields
- **Arxiv ID**: http://arxiv.org/abs/1910.10892v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10892v3)
- **Published**: 2019-10-24 02:48:11+00:00
- **Updated**: 2020-10-06 05:13:03+00:00
- **Authors**: Zhiwei Xu, Thalaiyasingam Ajanthan, Richard Hartley
- **Comment**: Asian Conference on Computer Vision (ACCV), 2020 (Oral)
- **Journal**: None
- **Summary**: Despite the availability of many Markov Random Field (MRF) optimization algorithms, their widespread usage is currently limited due to imperfect MRF modelling arising from hand-crafted model parameters and the selection of inferior inference algorithm. In addition to differentiability, the two main aspects that enable learning these model parameters are the forward and backward propagation time of the MRF optimization algorithm and its inference capabilities. In this work, we introduce two fast and differentiable message passing algorithms, namely, Iterative Semi-Global Matching Revised (ISGMR) and Parallel Tree-Reweighted Message Passing (TRWP) which are greatly sped up on a GPU by exploiting massive parallelism. Specifically, ISGMR is an iterative and revised version of the standard SGM for general pairwise MRFs with improved optimization effectiveness, and TRWP is a highly parallel version of Sequential TRW (TRWS) for faster optimization. Our experiments on the standard stereo and denoising benchmarks demonstrated that ISGMR and TRWP achieve much lower energies than SGM and Mean-Field (MF), and TRWP is two orders of magnitude faster than TRWS without losing effectiveness in optimization. We further demonstrated the effectiveness of our algorithms on end-to-end learning for semantic segmentation. Notably, our CUDA implementations are at least $7$ and $700$ times faster than PyTorch GPU implementations for forward and backward propagation respectively, enabling efficient end-to-end learning with message passing.



### Anchor Diffusion for Unsupervised Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.10895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10895v1)
- **Published**: 2019-10-24 03:10:07+00:00
- **Updated**: 2019-10-24 03:10:07+00:00
- **Authors**: Zhao Yang, Qiang Wang, Luca Bertinetto, Weiming Hu, Song Bai, Philip H. S. Torr
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approaches tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators of [70], we introduce a technique to establish dense correspondences between pixel embeddings of a reference "anchor" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the ViSal video saliency dataset, showing results competitive with the state of the art.



### Unknown Identity Rejection Loss: Utilizing Unlabeled Data for Face Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.10896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10896v1)
- **Published**: 2019-10-24 03:18:14+00:00
- **Updated**: 2019-10-24 03:18:14+00:00
- **Authors**: Haiming Yu, Yin Fan, Keyu Chen, He Yan, Xiangju Lu, Junhui Liu, Danming Xie
- **Comment**: 8 pages, 2 figures, Workshop paper accepted by Lightweight Face
  Recognition Challenge & Workshop (ICCV 2019)
- **Journal**: None
- **Summary**: Face recognition has advanced considerably with the availability of large-scale labeled datasets. However, how to further improve the performance with the easily accessible unlabeled dataset remains a challenge. In this paper, we propose the novel Unknown Identity Rejection (UIR) loss to utilize the unlabeled data. We categorize identities in unconstrained environment into the known set and the unknown set. The former corresponds to the identities that appear in the labeled training dataset while the latter is its complementary set. Besides training the model to accurately classify the known identities, we also force the model to reject unknown identities provided by the unlabeled dataset via our proposed UIR loss. In order to 'reject' faces of unknown identities, centers of the known identities are forced to keep enough margin from centers of unknown identities which are assumed to be approximated by the features of their samples. By this means, the discriminativeness of the face representations can be enhanced. Experimental results demonstrate that our approach can provide obvious performance improvement by utilizing the unlabeled data.



### Soft Prototyping Camera Designs for Car Detection Based on a Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.10916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10916v1)
- **Published**: 2019-10-24 05:23:07+00:00
- **Updated**: 2019-10-24 05:23:07+00:00
- **Authors**: Zhenyi Liu, Trisha Lian, Joyce Farrell, Brian Wandell
- **Comment**: None
- **Journal**: None
- **Summary**: Imaging systems are increasingly used as input to convolutional neural networks (CNN) for object detection; we would like to design cameras that are optimized for this purpose. It is impractical to build different cameras and then acquire and label the necessary data for every potential camera design; creating software simulations of the camera in context (soft prototyping) is the only realistic approach. We implemented soft-prototyping tools that can quantitatively simulate image radiance and camera designs to create realistic images that are input to a convolutional neural network for car detection. We used these methods to quantify the effect that critical hardware components (pixel size), sensor control (exposure algorithms) and image processing (gamma and demosaicing algorithms) have upon average precision of car detection. We quantify (a) the relationship between pixel size and the ability to detect cars at different distances, (b) the penalty for choosing a poor exposure duration, and (c) the ability of the CNN to perform car detection for a variety of post-acquisition processing algorithms. These results show that the optimal choices for car detection are not constrained by the same metrics used for image quality in consumer photography. It is better to evaluate camera designs for CNN applications using soft prototyping with task-specific metrics rather than consumer photography metrics.



### Knowledge Transfer between Datasets for Learning-based Tissue Microstructure Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.10930v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10930v1)
- **Published**: 2019-10-24 06:25:00+00:00
- **Updated**: 2019-10-24 06:25:00+00:00
- **Authors**: Yu Qin, Yuxing Li, Zhiwen Liu, Chuyang Ye
- **Comment**: 4 pages, 2 figures
- **Journal**: None
- **Summary**: Learning-based approaches, especially those based on deep networks, have enabled high-quality estimation of tissue microstructure from low-quality diffusion magnetic resonance imaging (dMRI) scans, which are acquired with a limited number of diffusion gradients and a relatively poor spatial resolution. These learning-based approaches to tissue microstructure estimation require acquisitions of training dMRI scans with high-quality diffusion signals, which are densely sampled in the q-space and have a high spatial resolution. However, the acquisition of training scans may not be available for all datasets. Therefore, we explore knowledge transfer between different dMRI datasets so that learning-based tissue microstructure estimation can be applied for datasets where training scans are not acquired. Specifically, for a target dataset of interest, where only low-quality diffusion signals are acquired without training scans, we exploit the information in a source dMRI dataset acquired with high-quality diffusion signals. We interpolate the diffusion signals in the source dataset in the q-space using a dictionary-based signal representation, so that the interpolated signals match the acquisition scheme of the target dataset. Then, the interpolated signals are used together with the high-quality tissue microstructure computed from the source dataset to train deep networks that perform tissue microstructure estimation for the target dataset. Experiments were performed on brain dMRI scans with low-quality diffusion signals, where the benefit of the proposed strategy is demonstrated.



### ROBO: Robust, Fully Neural Object Detection for Robot Soccer
- **Arxiv ID**: http://arxiv.org/abs/1910.10949v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.10949v1)
- **Published**: 2019-10-24 07:24:58+00:00
- **Updated**: 2019-10-24 07:24:58+00:00
- **Authors**: Marton Szemenyei, Vladimir Estivill-Castro
- **Comment**: Presented at the 2019 RoboCup Symposium
- **Journal**: None
- **Summary**: Deep Learning has become exceptionally popular in the last few years due to its success in computer vision and other fields of AI. However, deep neural networks are computationally expensive, which limits their application in low power embedded systems, such as mobile robots. In this paper, an efficient neural network architecture is proposed for the problem of detecting relevant objects in robot soccer environments. The ROBO model's increase in efficiency is achieved by exploiting the peculiarities of the environment. Compared to the state-of-the-art Tiny YOLO model, the proposed network provides approximately 35 times decrease in run time, while achieving superior average precision, although at the cost of slightly worse localization accuracy.



### Hierarchical Prototype Learning for Zero-Shot Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.11671v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11671v2)
- **Published**: 2019-10-24 07:26:30+00:00
- **Updated**: 2019-12-10 07:27:03+00:00
- **Authors**: Xingxing Zhang, Shupeng Gui, Zhenfeng Zhu, Yao Zhao, Ji Liu
- **Comment**: This manuscript has been accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Zero-Shot Learning (ZSL) has received extensive attention and successes in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning. Key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary semantic prototypes (e.g., word or attribute vectors). However, the popularly learned projection functions in previous works cannot generalize well due to non-visual components included in semantic prototypes. Besides, the incompleteness of provided prototypes and captured images has less been considered by the state-of-the-art approaches in ZSL. In this paper, we propose a hierarchical prototype learning formulation to provide a systematical solution (named HPL) for zero-shot recognition. Specifically, HPL is able to obtain discriminability on both seen and unseen class domains by learning visual prototypes respectively under the transductive setting. To narrow the gap of two domains, we further learn the interpretable super-prototypes in both visual and semantic spaces. Meanwhile, the two spaces are further bridged by maximizing their structural consistency. This not only facilitates the representativeness of visual prototypes, but also alleviates the loss of information of semantic prototypes. An extensive group of experiments are then carefully designed and presented, demonstrating that HPL obtains remarkably more favorable efficiency and effectiveness, over currently available alternatives under various settings.



### Fast Glare Detection in Document Images
- **Arxiv ID**: http://arxiv.org/abs/1911.05189v1
- **DOI**: 10.1109/ICDARW.2019.60123
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1911.05189v1)
- **Published**: 2019-10-24 09:12:01+00:00
- **Updated**: 2019-10-24 09:12:01+00:00
- **Authors**: Dmitry Rodin, Nikita Orlov
- **Comment**: 4 pages, Workshop on Industrial Applications of Document Analysis and
  Recognition 2019
- **Journal**: None
- **Summary**: Glare is a phenomenon that occurs when the scene has a reflection of a light source or has one in it. This luminescence can hide useful information from the image, making text recognition virtually impossible. In this paper, we propose an approach to detect glare in images taken by users via mobile devices. Our method divides the document into blocks and collects luminance features from the original image and black-white strokes histograms of the binarized image. Finally, glare is detected using a convolutional neural network on the aforementioned histograms and luminance features. The network consists of several feature extraction blocks, one for each type of input, and the detection block, which calculates the resulting glare heatmap based on the output of the extraction part. The proposed solution detects glare with high recall and f-score.



### Adversarial Feature Alignment: Avoid Catastrophic Forgetting in Incremental Task Lifelong Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.10986v1
- **DOI**: 10.1162/neco_a_01232
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10986v1)
- **Published**: 2019-10-24 09:23:02+00:00
- **Updated**: 2019-10-24 09:23:02+00:00
- **Authors**: Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, Lifeng Sun
- **Comment**: None
- **Journal**: Neural Computation, Volume 31, Issue 11, November 2019,
  p.2266-2291
- **Summary**: Human beings are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed as \emph{Catastrophic Forgetting}, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts, e.g. \emph{Lifelong} or \emph{Continual} learning algorithms, have been proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require to store an excessive amount of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this paper, we focus on the incremental multi-task image classification scenario. Inspired by the learning process of human students, where they usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomenons, the proposed method gains even better performance than finetuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracies on new tasks and performance preservation on old tasks.



### ATZSL: Defensive Zero-Shot Recognition in the Presence of Adversaries
- **Arxiv ID**: http://arxiv.org/abs/1910.10994v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10994v2)
- **Published**: 2019-10-24 09:36:11+00:00
- **Updated**: 2019-12-10 08:18:08+00:00
- **Authors**: Xingxing Zhang, Shupeng Gui, Zhenfeng Zhu, Yao Zhao, Ji Liu
- **Comment**: 14 pages, 9 figures, 10 tables, journal
- **Journal**: None
- **Summary**: Zero-shot learning (ZSL) has received extensive attention recently especially in areas of fine-grained object recognition, retrieval, and image captioning. Due to the complete lack of training samples and high requirement of defense transferability, the ZSL model learned is particularly vulnerable against adversarial attacks. Recent work also showed adversarially robust generalization requires more data. This may significantly affect the robustness of ZSL. However, very few efforts have been devoted towards this direction. In this paper, we take an initial attempt, and propose a generic formulation to provide a systematical solution (named ATZSL) for learning a robust ZSL model. It is capable of achieving better generalization on various adversarial objects recognition while only losing a negligible performance on clean images for unseen classes, by casting ZSL into a min-max optimization problem. To address it, we design a defensive relation prediction network, which can bridge the seen and unseen class domains via attributes to generalize prediction and defense strategy. Additionally, our framework can be extended to deal with the poisoned scenario of unseen class attributes. An extensive group of experiments are then presented, demonstrating that ATZSL obtains remarkably more favorable trade-off between model transferability and robustness, over currently available alternatives under various settings.



### Vision-Infused Deep Audio Inpainting
- **Arxiv ID**: http://arxiv.org/abs/1910.10997v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1910.10997v1)
- **Published**: 2019-10-24 09:41:44+00:00
- **Updated**: 2019-10-24 09:41:44+00:00
- **Authors**: Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, Xiaogang Wang
- **Comment**: To appear in ICCV 2019. Code, models, dataset and video results are
  available at the project page:
  https://hangz-nju-cuhk.github.io/projects/AudioInpainting
- **Journal**: None
- **Summary**: Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, \ie synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://hangz-nju-cuhk.github.io/projects/AudioInpainting



### Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison
- **Arxiv ID**: http://arxiv.org/abs/1910.11006v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.MM, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.11006v2)
- **Published**: 2019-10-24 10:04:29+00:00
- **Updated**: 2020-01-21 00:24:44+00:00
- **Authors**: Dongxu Li, Cristian Rodriguez Opazo, Xin Yu, Hongdong Li
- **Comment**: Accepted by WACV2020, First Round, camera-ready version
- **Journal**: None
- **Summary**: Vision-based sign language recognition aims at helping deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge, it is by far the largest public ASL dataset to facilitate word-level sign recognition research.   Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance-based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that models spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at \url{https://dxli94.github.io/WLASL/}.



### A Graph-Based Framework to Bridge Movies and Synopses
- **Arxiv ID**: http://arxiv.org/abs/1910.11009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11009v1)
- **Published**: 2019-10-24 10:07:51+00:00
- **Updated**: 2019-10-24 10:07:51+00:00
- **Authors**: Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei Zhou, Dahua Lin
- **Comment**: Accepted by ICCV 2019 (oral)
- **Journal**: None
- **Summary**: Inspired by the remarkable advances in video analytics, research teams are stepping towards a greater ambition -- movie understanding. However, compared to those activity videos in conventional datasets, movies are significantly different. Generally, movies are much longer and consist of much richer temporal structures. More importantly, the interactions among characters play a central role in expressing the underlying story. To facilitate the efforts along this direction, we construct a dataset called Movie Synopses Associations (MSA) over 327 movies, which provides a synopsis for each movie, together with annotated associations between synopsis paragraphs and movie segments. On top of this dataset, we develop a framework to perform matching between movie segments and synopsis paragraphs. This framework integrates different aspects of a movie, including event dynamics and character interactions, and allows them to be matched with parsed paragraphs, based on a graph-based formulation. Our study shows that the proposed framework remarkably improves the matching accuracy over conventional feature-based methods. It also reveals the importance of narrative structures and character interactions in movie understanding.



### ProLFA: Representative Prototype Selection for Local Feature Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1910.11010v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11010v1)
- **Published**: 2019-10-24 10:08:10+00:00
- **Updated**: 2019-10-24 10:08:10+00:00
- **Authors**: Xingxing Zhang, Zhenfeng Zhu, Yao Zhao
- **Comment**: 9 figures, 5 tables
- **Journal**: None
- **Summary**: Given a set of hand-crafted local features, acquiring a global representation via aggregation is a promising technique to boost computational efficiency and improve task performance. Existing feature aggregation (FA) approaches, including Bag of Words and Fisher Vectors, usually fail to capture the desired information due to their pipeline mode. In this paper, we propose a generic formulation to provide a systematical solution (named ProLFA) to aggregate local descriptors. It is capable of producing compact yet interpretable representations by selecting representative prototypes from numerous descriptors, under relaxed exclusivity constraint. Meanwhile, to strengthen the discriminability of the aggregated representation, we rationally enforce the domain-invariant projection of bundled descriptors along a task-specific direction. Furthermore, ProLFA is also provided with a powerful generalization ability to deal flexibly with the semi-supervised and fully supervised scenarios in local feature aggregation. Experimental results on various descriptors and tasks demonstrate that the proposed ProLFA is considerably superior over currently available alternatives about feature aggregation.



### Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.11012v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11012v2)
- **Published**: 2019-10-24 10:11:20+00:00
- **Updated**: 2020-09-23 05:12:54+00:00
- **Authors**: Xuesong Niu, Hu Han, Shiguang Shan, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Facial action units (AUs) recognition is essential for emotion analysis and has been widely applied in mental state analysis. Existing work on AU recognition usually requires big face dataset with AU labels; however, manual AU annotation requires expertise and can be time-consuming. In this work, we propose a semi-supervised approach for AU recognition utilizing a large number of web face images without AU labels and a relatively small face dataset with AU annotations inspired by the co-training methods. Unlike traditional co-training methods that require provided multi-view features and model re-training, we propose a novel co-training method, namely multi-label co-regularization, for semi-supervised facial AU recognition. Two deep neural networks are utilized to generate multi-view features for both labeled and unlabeled face images, and a multi-view loss is designed to enforce the two feature generators to get conditional independent representations. In order to constrain the prediction consistency of the two views, we further propose a multi-label co-regularization loss by minimizing the distance of the predicted AU probability distributions of two views. In addition, prior knowledge of the relationship between individual AUs is embedded through a graph convolutional network (GCN) for exploiting useful information from the big unlabeled dataset. Experiments on several benchmarks show that the proposed approach can effectively leverage large datasets of face images without AU labels to improve the AU recognition accuracy and outperform the state-of-the-art semi-supervised AU recognition methods.



### Convex Optimisation for Inverse Kinematics
- **Arxiv ID**: http://arxiv.org/abs/1910.11016v1
- **DOI**: 10.1109/3DV.2019.00043
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11016v1)
- **Published**: 2019-10-24 10:25:23+00:00
- **Updated**: 2019-10-24 10:25:23+00:00
- **Authors**: Tarun Yenamandra, Florian Bernard, Jiayi Wang, Franziska Mueller, Christian Theobalt
- **Comment**: None
- **Journal**: None
- **Summary**: We consider the problem of inverse kinematics (IK), where one wants to find the parameters of a given kinematic skeleton that best explain a set of observed 3D joint locations. The kinematic skeleton has a tree structure, where each node is a joint that has an associated geometric transformation that is propagated to all its child nodes. The IK problem has various applications in vision and graphics, for example for tracking or reconstructing articulated objects, such as human hands or bodies. Most commonly, the IK problem is tackled using local optimisation methods. A major downside of these approaches is that, due to the non-convex nature of the problem, such methods are prone to converge to unwanted local optima and therefore require a good initialisation. In this paper we propose a convex optimisation approach for the IK problem based on semidefinite programming, which admits a polynomial-time algorithm that globally solves (a relaxation of) the IK problem. Experimentally, we demonstrate that the proposed method significantly outperforms local optimisation methods using different real-world skeletons.



### Spatiotemporal Tile-based Attention-guided LSTMs for Traffic Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.11030v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11030v3)
- **Published**: 2019-10-24 11:05:22+00:00
- **Updated**: 2020-02-17 22:06:32+00:00
- **Authors**: Tu Nguyen
- **Comment**: Neurips 2019 Traffic4Cast Challenge
- **Journal**: None
- **Summary**: This extended abstract describes our solution for the Traffic4Cast Challenge 2019. The key problem we addressed is to properly model both low-level (pixel based) and high-level spatial information while still preserve the temporal relations among the frames. Our approach is inspired by the recent adoption of convolutional features into a recurrent neural networks such as LSTM to jointly capture the spatio-temporal dependency. While this approach has been proven to surpass the traditional stacked CNNs (using 2D or 3D kernels) in action recognition, we observe suboptimal performance in traffic prediction setting. Therefore, we apply a number of adaptations in the frame encoder-decoder layers and in sampling procedure to better capture the high-resolution trajectories, and to increase the training efficiency.



### Assisting human experts in the interpretation of their visual process: A case study on assessing copper surface adhesive potency
- **Arxiv ID**: http://arxiv.org/abs/1910.11033v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11033v1)
- **Published**: 2019-10-24 11:23:58+00:00
- **Updated**: 2019-10-24 11:23:58+00:00
- **Authors**: Tristan Hascoet, Xuejiao Deng, Kiyoto Tai, Mari Sugiyama, Yuji Adachi, Sachiko Nakamura, Yasuo Ariki, Tomoko Hayashi, Tetusya Takiguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks are often though to lack interpretability due to the distributed nature of their internal representations. In contrast, humans can generally justify, in natural language, for their answer to a visual question with simple common sense reasoning. However, human introspection abilities have their own limits as one often struggles to justify for the recognition process behind our lowest level feature recognition ability: for instance, it is difficult to precisely explain why a given texture seems more characteristic of the surface of a finger nail rather than a plastic bottle. In this paper, we showcase an application in which deep learning models can actually help human experts justify for their own low-level visual recognition process: We study the problem of assessing the adhesive potency of copper sheets from microscopic pictures of their surface. Although highly trained material experts are able to qualitatively assess the surface adhesive potency, they are often unable to precisely justify for their decision process. We present a model that, under careful design considerations, is able to provide visual clues for human experts to understand and justify for their own recognition process. Not only can our model assist human experts in their interpretation of the surface characteristics, we show how this model can be used to test different hypothesis of the copper surface response to different manufacturing processes.



### Learning Multi-Human Optical Flow
- **Arxiv ID**: http://arxiv.org/abs/1910.11667v2
- **DOI**: 10.1007/s11263-019-01279-w
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11667v2)
- **Published**: 2019-10-24 11:44:46+00:00
- **Updated**: 2019-12-04 10:48:45+00:00
- **Authors**: Anurag Ranjan, David T. Hoffmann, Dimitrios Tzionas, Siyu Tang, Javier Romero, Michael J. Black
- **Comment**: arXiv admin note: text overlap with arXiv:1806.05666
- **Journal**: International Journal of Computer Vision (IJCV) 2019
- **Summary**: The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.



### Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments
- **Arxiv ID**: http://arxiv.org/abs/1910.11109v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11109v3)
- **Published**: 2019-10-24 13:48:52+00:00
- **Updated**: 2020-09-13 14:55:16+00:00
- **Authors**: Zhen-Liang Ni, Gui-Bin Bian, Zeng-Guang Hou, Xiao-Hu Zhou, Xiao-Liang Xie, Zhen Li
- **Comment**: Accepted by ICRA2020; Camera ready
- **Journal**: None
- **Summary**: The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960*544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the-art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.



### Depth Camera Based Particle Filter for Robotic Osteotomy Navigation
- **Arxiv ID**: http://arxiv.org/abs/1910.11116v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11116v1)
- **Published**: 2019-10-24 13:53:21+00:00
- **Updated**: 2019-10-24 13:53:21+00:00
- **Authors**: Tim Übelhör, Jonas Gesenhues, Nassim Ayoub, Ali Modabber, Dirk Abel
- **Comment**: 6 pages, submitted to ICRA 2020
- **Journal**: None
- **Summary**: Active surgical robots lack acceptance in clinical practice, because they do not offer the flexibility and usability required for a versatile usage: the systems require a large installation space or a complicated registration step, where the preoperative plan is aligned to the patient and transformed to the base frame of the robot. In this paper, a navigation system for robotic osteotomies is designed, which uses the raw depth images from a camera mounted on the flange of a lightweight robot arm. Consequently, the system does not require any rigid attachment of the robot or fiducials to the bone and the time-consuming registration step is eliminated. Instead, only a coarse initialization is required which improves the usability in surgery. The full six dimensional pose of the iliac crest bone is estimated with a particle filter at a maximum rate of 90 Hz. The presented method is robust against changing lighting conditions, blood or tissue on the bone surface and partial occlusions caused by the surgeons. Proof of the usability in a clinical environment is successfully provided in a corpse study, where surgeons used an augmented reality osteotomy template, which was aligned to bone via the particle filters pose estimates for the resection of transplants from the iliac crest.



### Reversible designs for extreme memory cost reduction of CNN training
- **Arxiv ID**: http://arxiv.org/abs/1910.11127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11127v1)
- **Published**: 2019-10-24 13:59:23+00:00
- **Updated**: 2019-10-24 13:59:23+00:00
- **Authors**: Tristan Hascoet, Quentin Febvre, Yasuo Ariki, Tetsuya Takiguchi
- **Comment**: None
- **Journal**: None
- **Summary**: Training Convolutional Neural Networks (CNN) is a resource intensive task that requires specialized hardware for efficient computation. One of the most limiting bottleneck of CNN training is the memory cost associated with storing the activation values of hidden layers needed for the computation of the weights gradient during the backward pass of the backpropagation algorithm. Recently, reversible architectures have been proposed to reduce the memory cost of training large CNN by reconstructing the input activation values of hidden layers from their output during the backward pass, circumventing the need to accumulate these activations in memory during the forward pass. In this paper, we push this idea to the extreme and analyze reversible network designs yielding minimal training memory footprint. We investigate the propagation of numerical errors in long chains of invertible operations and analyze their effect on training. We introduce the notion of pixel-wise memory cost to characterize the memory footprint of model training, and propose a new model architecture able to efficiently train arbitrarily deep neural networks with a minimum memory cost of 352 bytes per input pixel. This new kind of architecture enables training large neural networks on very limited memory, opening the door for neural network training on embedded devices or non-specialized hardware. For instance, we demonstrate training of our model to 93.3% accuracy on the CIFAR10 dataset within 67 minutes on a low-end Nvidia GTX750 GPU with only 1GB of memory.



### AI in Pursuit of Happiness, Finding Only Sadness: Multi-Modal Facial Emotion Recognition Challenge
- **Arxiv ID**: http://arxiv.org/abs/1911.05187v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05187v1)
- **Published**: 2019-10-24 14:49:39+00:00
- **Updated**: 2019-10-24 14:49:39+00:00
- **Authors**: Carl Norman
- **Comment**: None
- **Journal**: None
- **Summary**: The importance of automated Facial Emotion Recognition (FER) grows the more common human-machine interactions become, which will only continue to increase dramatically with time. A common method to describe human sentiment or feeling is the categorical model the `7 basic emotions', consisting of `Angry', `Disgust', `Fear', `Happiness', `Sadness', `Surprise' and `Neutral'. The `Emotion Recognition in the Wild' (EmotiW) competition is now in its 7th year and has become the standard benchmark for measuring FER performance. The focus of this paper is the EmotiW sub-challenge of classifying videos in the `Acted Facial Expression in the Wild' (AFEW) dataset, consisting of both visual and audio modalities, into one of the above classes. Machine learning has exploded as a research topic in recent years, with advancements in `Deep Learning' a key part of this. Although Deep Learning techniques have been widely applied to the FER task by entrants in previous years, this paper has two main contributions: (i) to apply the latest `state-of-the-art' visual and temporal networks and (ii) exploring various methods of fusing features extracted from the visual and audio elements to enrich the information available to the final model making the prediction. There are a number of complex issues that arise when trying to classify emotions for `in-the-wild' video sequences, which the above two approaches attempt to directly address. There are some positive findings when comparing the results of this paper to past submissions, indicating that further research into the proposed methods and fine-tuning of the models deployed, could result in another step forwards in the field of automated FER.



### RoboNet: Large-Scale Multi-Robot Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11215v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11215v2)
- **Published**: 2019-10-24 15:20:03+00:00
- **Updated**: 2020-01-02 06:26:37+00:00
- **Authors**: Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, Chelsea Finn
- **Comment**: accepted at the Conference on Robot Learning (CoRL) 2019
- **Journal**: None
- **Summary**: Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/



### Data hiding in complex-amplitude modulation using a digital micromirror device
- **Arxiv ID**: http://arxiv.org/abs/1910.11222v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11222v1)
- **Published**: 2019-10-24 15:26:38+00:00
- **Updated**: 2019-10-24 15:26:38+00:00
- **Authors**: Shuming Jiao, Dongfang Zhang, Chonglei Zhang, Yang Gao, Ting Lei, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: A digital micromirror device (DMD) is an amplitude-type spatial light modulator. However, a complex-amplitude light modulation with a DMD can be achieved using the superpixel scheme. In the superpixel scheme, we notice that multiple different DMD local block patterns may correspond to the same complex superpixel value. Based on this inherent encoding redundancy, a large amount of external data can be embedded into the DMD pattern without extra cost. Meanwhile, the original complex light field information carried by the DMD pattern is fully preserved. This proposed scheme is favorable for applications such as secure information transmission and copyright protection.



### When Segmentation is Not Enough: Rectifying Visual-Volume Discordance Through Multisensor Depth-Refined Semantic Segmentation for Food Intake Tracking in Long-Term Care
- **Arxiv ID**: http://arxiv.org/abs/1910.11250v2
- **DOI**: 10.1038/s41598-021-03972-8
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11250v2)
- **Published**: 2019-10-24 15:50:20+00:00
- **Updated**: 2021-03-31 19:56:38+00:00
- **Authors**: Kaylen J Pfisterer, Robert Amelard, Audrey G Chung, Braeden Syrnyk, Alexander MacLean, Heather H Keller, Alexander Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Malnutrition is a multidomain problem affecting 54% of older adults in long-term care (LTC). Monitoring nutritional intake in LTC is laborious and subjective, limiting clinical inference capabilities. Recent advances in automatic image-based food estimation have not yet been evaluated in LTC settings. Here, we describe a fully automatic imaging system for quantifying food intake. We propose a novel deep convolutional encoder-decoder food network with depth-refinement (EDFN-D) using an RGB-D camera for quantifying a plate's remaining food volume relative to reference portions in whole and modified texture foods. We trained and validated the network on the pre-labelled UNIMIB2016 food dataset and tested on our two novel LTC-inspired plate datasets (689 plate images, 36 unique foods). EDFN-D performed comparably to depth-refined graph cut on IOU (0.879 vs. 0.887), with intake errors well below typical 50% (mean percent intake error: -4.2%). We identify how standard segmentation metrics are insufficient due to visual-volume discordance, and include volume disparity analysis to facilitate system trust. This system provides improved transparency, approximates human assessors with enhanced objectivity, accuracy, and precision while avoiding hefty semi-automatic method time requirements. This may help address short-comings currently limiting utility of automated early malnutrition detection in resource-constrained LTC and hospital settings.



### Emotion recognition with 4kresolution database
- **Arxiv ID**: http://arxiv.org/abs/1910.11276v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11276v1)
- **Published**: 2019-10-24 16:44:11+00:00
- **Updated**: 2019-10-24 16:44:11+00:00
- **Authors**: Qian Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: Classifying the human emotion through facial expressions is a big topic in both the Computer Vision and Deep learning fields. Human emotion can be classified as one of the basic emotion types like being angry, happy or dimensional emotion with valence and arousal values. There are a lot of related challenges in this topic, one of the most famous challenges is called the 'Affect-in-the-wild Challenge'(Aff-Wild Challenge). It is the first challenge on the estimation of valence and arousal in-the-wild. This project is an extension of the Aff-wild Challenge. Aff-wild database was created using images with a mean resolution of 607*359, I and Dimitrios sought to find out the performance of the model that is trained on a database that contains4K resolution in-the-wild images. Since there is no existing database to satisfy the requirement, I built this database from scratch with help from Dimitrios and trained neural network models with different hyperparameters on this database. I used network models likeVGG16, AlexNet, ResNet and also some pre-trained models like Ima-geNet VGG. I compared the results of the different network models alongside the results from the Aff-wild database to exploit the optimal model for my database.



### Towards Train-Test Consistency for Semi-supervised Temporal Action Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.11285v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11285v3)
- **Published**: 2019-10-24 17:00:14+00:00
- **Updated**: 2020-03-23 02:56:39+00:00
- **Authors**: Xudong Lin, Zheng Shou, Shih-Fu Chang
- **Comment**: Work in progress
- **Journal**: None
- **Summary**: Recently, Weakly-supervised Temporal Action Localization (WTAL) has been densely studied but there is still a large gap between weakly-supervised models and fully-supervised models. It is practical and intuitive to annotate temporal boundaries of a few examples and utilize them to help WTAL models better detect actions. However, the train-test discrepancy of action localization strategy prevents WTAL models from leveraging semi-supervision for further improvement. At training time, attention or multiple instance learning is used to aggregate predictions of each snippet for video-level classification; at test time, they first obtain action score sequences over time, then truncate segments of scores higher than a fixed threshold, and post-process action segments. The inconsistent strategy makes it hard to explicitly supervise the action localization model with temporal boundary annotations at training time. In this paper, we propose a Train-Test Consistent framework, TTC-Loc. In both training and testing time, our TTC-Loc localizes actions by comparing scores of action classes and predicted threshold, which enables it to be trained with semi-supervision. By fixing the train-test discrepancy, our TTC-Loc significantly outperforms the state-of-the-art performance on THUMOS'14, ActivityNet 1.2 and 1.3 when only video-level labels are provided for training. With full annotations of only one video per class and video-level labels for the other videos, our TTC-Loc further boosts the performance and achieves 33.4\% mAP (IoU threshold 0.5) on THUMOS's 14.



### Identifying Unknown Instances for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1910.11296v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11296v1)
- **Published**: 2019-10-24 17:24:43+00:00
- **Updated**: 2019-10-24 17:24:43+00:00
- **Authors**: Kelvin Wong, Shenlong Wang, Mengye Ren, Ming Liang, Raquel Urtasun
- **Comment**: 3rd Conference on Robot Learning (CoRL 2019)
- **Journal**: None
- **Summary**: In the past few years, we have seen great progress in perception algorithms, particular through the use of deep learning. However, most existing approaches focus on a few categories of interest, which represent only a small fraction of the potential categories that robots need to handle in the real-world. Thus, identifying objects from unknown classes remains a challenging yet crucial task. In this paper, we develop a novel open-set instance segmentation algorithm for point clouds which can segment objects from both known and unknown classes in a holistic way. Our method uses a deep convolutional neural network to project points into a category-agnostic embedding space in which they can be clustered into instances irrespective of their semantics. Experiments on two large-scale self-driving datasets validate the effectiveness of our proposed method.



### Cross-Lingual Vision-Language Navigation
- **Arxiv ID**: http://arxiv.org/abs/1910.11301v3
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11301v3)
- **Published**: 2019-10-24 17:32:38+00:00
- **Updated**: 2020-12-06 02:48:07+00:00
- **Authors**: An Yan, Xin Eric Wang, Jiangtao Feng, Lei Li, William Yang Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Commanding a robot to navigate with natural language instructions is a long-term goal for grounded language understanding and robotics. But the dominant language is English, according to previous studies on vision-language navigation (VLN). To go beyond English and serve people speaking different languages, we collect a bilingual Room-to-Room (BL-R2R) dataset, extending the original benchmark with new Chinese instructions. Based on this newly introduced dataset, we study how an agent can be trained on existing English instructions but navigate effectively with another language under a zero-shot learning scenario. Without any training data of the target language, our model shows competitive results even compared to a model with full access to the target language training data. Moreover, we investigate the transferring ability of our model when given a certain amount of target language training data.



### Controllable Attention for Structured Layered Video Decomposition
- **Arxiv ID**: http://arxiv.org/abs/1910.11306v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11306v1)
- **Published**: 2019-10-24 17:36:40+00:00
- **Updated**: 2019-10-24 17:36:40+00:00
- **Authors**: Jean-Baptiste Alayrac, João Carreira, Relja Arandjelović, Andrew Zisserman
- **Comment**: In ICCV 2019
- **Journal**: None
- **Summary**: The objective of this paper is to be able to separate a video into its natural layers, and to control which of the separated layers to attend to. For example, to be able to separate reflections, transparency or object motion. We make the following three contributions: (i) we introduce a new structured neural network architecture that explicitly incorporates layers (as spatial masks) into its design. This improves separation performance over previous general purpose networks for this task; (ii) we demonstrate that we can augment the architecture to leverage external cues such as audio for controllability and to help disambiguation; and (iii) we experimentally demonstrate the effectiveness of our approach and training procedure with controlled experiments while also showing that the proposed model can be successfully applied to real-word applications such as reflection removal and action recognition in cluttered scenes.



### Progressive Domain Adaptation for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.11319v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11319v1)
- **Published**: 2019-10-24 17:55:04+00:00
- **Updated**: 2019-10-24 17:55:04+00:00
- **Authors**: Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih Hung, Hung-Yu Tseng, Maneesh Singh, Ming-Hsuan Yang
- **Comment**: Accepted in WACV'20. Code and models will be available at
  https://github.com/kevinhkhsu/DA_detection
- **Journal**: None
- **Summary**: Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution. Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal results. In this paper, we propose to bridge the domain gap with an intermediate domain and progressively solve easier adaptation subtasks. This intermediate domain is constructed by translating the source images to mimic the ones in the target domain. To tackle the domain-shift problem, we adopt adversarial learning to align distributions at the feature level. In addition, a weighted task loss is applied to deal with unbalanced image quality in the intermediate domain. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the performance on the target domain.



### TexturePose: Supervising Human Mesh Estimation with Texture Consistency
- **Arxiv ID**: http://arxiv.org/abs/1910.11322v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11322v1)
- **Published**: 2019-10-24 17:55:31+00:00
- **Updated**: 2019-10-24 17:55:31+00:00
- **Authors**: Georgios Pavlakos, Nikos Kolotouros, Kostas Daniilidis
- **Comment**: None
- **Journal**: None
- **Summary**: This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/~pavlakos/projects/texturepose.



### Seeing What a GAN Cannot Generate
- **Arxiv ID**: http://arxiv.org/abs/1910.11626v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11626v1)
- **Published**: 2019-10-24 17:56:04+00:00
- **Updated**: 2019-10-24 17:56:04+00:00
- **Authors**: David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba
- **Comment**: ICCV 2019 oral; http://ganseeing.csail.mit.edu/
- **Journal**: None
- **Summary**: Despite the success of Generative Adversarial Networks (GANs), mode collapse remains a serious issue during GAN training. To date, little work has focused on understanding and quantifying which modes have been dropped by a model. In this work, we visualize mode collapse at both the distribution level and the instance level. First, we deploy a semantic segmentation network to compare the distribution of segmented objects in the generated images with the target distribution in the training set. Differences in statistics reveal object classes that are omitted by a GAN. Second, given the identified omitted object classes, we visualize the GAN's omissions directly. In particular, we compare specific differences between individual photos and their approximate inversions by a GAN. To this end, we relax the problem of inversion and solve the tractable problem of inverting a GAN layer instead of the entire generator. Finally, we use this framework to analyze several recent GANs trained on multiple datasets and identify their typical failure cases.



### Guided Image-to-Image Translation with Bi-Directional Feature Transformation
- **Arxiv ID**: http://arxiv.org/abs/1910.11328v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11328v1)
- **Published**: 2019-10-24 17:58:47+00:00
- **Updated**: 2019-10-24 17:58:47+00:00
- **Authors**: Badour AlBahar, Jia-Bin Huang
- **Comment**: ICCV 2019 Code: https://github.com/vt-vl-lab/Guided-pix2pix Project
  page: https://filebox.ece.vt.edu/~Badour/guided_pix2pix.html
- **Journal**: None
- **Summary**: We address the problem of guided image-to-image translation where we translate an input image into another while respecting the constraints provided by an external, user-provided guidance image. Various conditioning methods for leveraging the given guidance image have been explored, including input concatenation , feature concatenation, and conditional affine transformation of feature activations. All these conditioning mechanisms, however, are uni-directional, i.e., no information flow from the input image back to the guidance. To better utilize the constraints of the guidance image, we present a bi-directional feature transformation (bFT) scheme. We show that our bFT scheme outperforms other conditioning schemes and has comparable results to state-of-the-art methods on different tasks.



### Accurate Layerwise Interpretable Competence Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.11363v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11363v1)
- **Published**: 2019-10-24 18:10:35+00:00
- **Updated**: 2019-10-24 18:10:35+00:00
- **Authors**: Vickram Rajendran, William LeVine
- **Comment**: Proceedings of the 33rd Conference in Neural Information Processing
  Systems (2019), 15 pages
- **Journal**: None
- **Summary**: Estimating machine learning performance 'in the wild' is both an important and unsolved problem. In this paper, we seek to examine, understand, and predict the pointwise competence of classification models. Our contributions are twofold: First, we establish a statistically rigorous definition of competence that generalizes the common notion of classifier confidence; second, we present the ALICE (Accurate Layerwise Interpretable Competence Estimation) Score, a pointwise competence estimator for any classifier. By considering distributional, data, and model uncertainty, ALICE empirically shows accurate competence estimation in common failure situations such as class-imbalanced datasets, out-of-distribution datasets, and poorly trained models. Our contributions allow us to accurately predict the competence of any classification model given any input and error function. We compare our score with state-of-the-art confidence estimators such as model confidence and Trust Score, and show significant improvements in competence prediction over these methods on datasets such as DIGITS, CIFAR10, and CIFAR100.



### Learning eating environments through scene clustering
- **Arxiv ID**: http://arxiv.org/abs/1910.11367v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11367v2)
- **Published**: 2019-10-24 18:16:11+00:00
- **Updated**: 2019-11-10 01:19:56+00:00
- **Authors**: Sri Kalyan Yarlagadda, Sriram Baireddy, David Güera, Carol J. Boushey, Deborah A. Kerr, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: It is well known that dietary habits have a significant influence on health. While many studies have been conducted to understand this relationship, little is known about the relationship between eating environments and health. Yet researchers and health agencies around the world have recognized the eating environment as a promising context for improving diet and health. In this paper, we propose an image clustering method to automatically extract the eating environments from eating occasion images captured during a community dwelling dietary study. Specifically, we are interested in learning how many different environments an individual consumes food in. Our method clusters images by extracting features at both global and local scales using a deep neural network. The variation in the number of clusters and images captured by different individual makes this a very challenging problem. Experimental results show that our method performs significantly better compared to several existing clustering approaches.



### Learning an Uncertainty-Aware Object Detector for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1910.11375v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11375v2)
- **Published**: 2019-10-24 18:52:28+00:00
- **Updated**: 2020-02-03 16:29:08+00:00
- **Authors**: Gregory P. Meyer, Niranjan Thakurdesai
- **Comment**: None
- **Journal**: None
- **Summary**: The capability to detect objects is a core part of autonomous driving. Due to sensor noise and incomplete data, perfectly detecting and localizing every object is infeasible. Therefore, it is important for a detector to provide the amount of uncertainty in each prediction. Providing the autonomous system with reliable uncertainties enables the vehicle to react differently based on the level of uncertainty. Previous work has estimated the uncertainty in a detection by predicting a probability distribution over object bounding boxes. In this work, we propose a method to improve the ability to learn the probability distribution by considering the potential noise in the ground-truth labeled data. Our proposed approach improves not only the accuracy of the learned distribution but also the object detection performance.



### Reconstruction of Undersampled 3D Non-Cartesian Image-Based Navigators for Coronary MRA Using an Unrolled Deep Learning Model
- **Arxiv ID**: http://arxiv.org/abs/1910.11414v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11414v1)
- **Published**: 2019-10-24 20:27:59+00:00
- **Updated**: 2019-10-24 20:27:59+00:00
- **Authors**: Mario O. Malavé, Corey A. Baron, Srivathsan P. Koundinyan, Christopher M. Sandino, Frank Ong, Joseph Y. Cheng, Dwight G. Nishimura
- **Comment**: 34 pages, 5 figures, 1 table, 6 supporting figures, 1 supporting
  table
- **Journal**: None
- **Summary**: Purpose: To rapidly reconstruct undersampled 3D non-Cartesian image-based navigators (iNAVs) using an unrolled deep learning (DL) model for non-rigid motion correction in coronary magnetic resonance angiography (CMRA).   Methods: An unrolled network is trained to reconstruct beat-to-beat 3D iNAVs acquired as part of a CMRA sequence. The unrolled model incorporates a non-uniform FFT operator to perform the data consistency operation, and the regularization term is learned by a convolutional neural network (CNN) based on the proximal gradient descent algorithm. The training set includes 6,000 3D iNAVs acquired from 7 different subjects and 11 scans using a variable-density (VD) cones trajectory. For testing, 3D iNAVs from 4 additional subjects are reconstructed using the unrolled model. To validate reconstruction accuracy, global and localized motion estimates from DL model-based 3D iNAVs are compared with those extracted from 3D iNAVs reconstructed with $\textit{l}_{1}$-ESPIRiT. Then, the high-resolution coronary MRA images motion corrected with autofocusing using the $\textit{l}_{1}$-ESPIRiT and DL model-based 3D iNAVs are assessed for differences.   Results: 3D iNAVs reconstructed using the DL model-based approach and conventional $\textit{l}_{1}$-ESPIRiT generate similar global and localized motion estimates and provide equivalent coronary image quality. Reconstruction with the unrolled network completes in a fraction of the time compared to CPU and GPU implementations of $\textit{l}_{1}$-ESPIRiT (20x and 3x speed increases, respectively).   Conclusion: We have developed a deep neural network architecture to reconstruct undersampled 3D non-Cartesian VD cones iNAVs. Our approach decreases reconstruction time for 3D iNAVs, while preserving the accuracy of non-rigid motion information offered by them for correction.



### Look globally, age locally: Face aging with an attention mechanism
- **Arxiv ID**: http://arxiv.org/abs/1910.12771v1
- **DOI**: 10.1109/ICASSP40776.2020.9054553
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12771v1)
- **Published**: 2019-10-24 21:00:49+00:00
- **Updated**: 2019-10-24 21:00:49+00:00
- **Authors**: Haiping Zhu, Zhizhong Huang, Hongming Shan, Junping Zhang
- **Comment**: arXiv admin note: text overlap with arXiv:1807.09251 by other authors
- **Journal**: IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2020
- **Summary**: Face aging is of great importance for cross-age recognition and entertainment-related applications. Recently, conditional generative adversarial networks (cGANs) have achieved impressive results for face aging. Existing cGANs-based methods usually require a pixel-wise loss to keep the identity and background consistent. However, minimizing the pixel-wise loss between the input and synthesized images likely resulting in a ghosted or blurry face. To address this deficiency, this paper introduces an Attention Conditional GANs (AcGANs) approach for face aging, which utilizes attention mechanism to only alert the regions relevant to face aging. In doing so, the synthesized face can well preserve the background information and personal identity without using the pixel-wise loss, and the ghost artifacts and blurriness can be significantly reduced. Based on the benchmarked dataset Morph, both qualitative and quantitative experiment results demonstrate superior performance over existing algorithms in terms of image quality, personal identity, and age accuracy.



### HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators
- **Arxiv ID**: http://arxiv.org/abs/1910.11432v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11432v1)
- **Published**: 2019-10-24 21:34:29+00:00
- **Updated**: 2019-10-24 21:34:29+00:00
- **Authors**: Chengshu Li, Fei Xia, Roberto Martin-Martin, Silvio Savarese
- **Comment**: Conference on Robot Learning (CoRL) 2019
- **Journal**: None
- **Summary**: Most common navigation tasks in human environments require auxiliary arm interactions, e.g. opening doors, pressing buttons and pushing obstacles away. This type of navigation tasks, which we call Interactive Navigation, requires the use of mobile manipulators: mobile bases with manipulation capabilities. Interactive Navigation tasks are usually long-horizon and composed of heterogeneous phases of pure navigation, pure manipulation, and their combination. Using the wrong part of the embodiment is inefficient and hinders progress. We propose HRL4IN, a novel Hierarchical RL architecture for Interactive Navigation tasks. HRL4IN exploits the exploration benefits of HRL over flat RL for long-horizon tasks thanks to temporally extended commitments towards subgoals. Different from other HRL solutions, HRL4IN handles the heterogeneous nature of the Interactive Navigation task by creating subgoals in different spaces in different phases of the task. Moreover, HRL4IN selects different parts of the embodiment to use for each phase, improving energy efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL algorithm, on Interactive Navigation in two environments - a 2D grid-world environment and a 3D environment with physics simulation. We show that HRL4IN significantly outperforms its baselines in terms of task performance and energy efficiency. More information is available at https://sites.google.com/view/hrl4in.



### Animal Detection in Man-made Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.11443v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11443v2)
- **Published**: 2019-10-24 22:17:24+00:00
- **Updated**: 2020-01-14 18:10:57+00:00
- **Authors**: Abhineet Singh, Marcin Pietrasik, Gabriell Natha, Nehla Ghouaiel, Ken Brizel, Nilanjan Ray
- **Comment**: to appear in to WACV 2020, supplementary:
  [http://webdocs.cs.ualberta.ca/~vis/asingh1/docs/animal_detection_supp.pdf],
  demo: [https://youtu.be/ZkjcP8s0QVQ]
- **Journal**: None
- **Summary**: Automatic detection of animals that have strayed into human inhabited areas has important security and road safety applications. This paper attempts to solve this problem using deep learning techniques from a variety of computer vision fields including object detection, tracking, segmentation and edge detection. Several interesting insights into transfer learning are elicited while adapting models trained on benchmark datasets for real world deployment. Empirical evidence is presented to demonstrate the inability of detectors to generalize from training images of animals in their natural habitats to deployment scenarios of man-made environments. A solution is also proposed using semi-automated synthetic data generation for domain specific training. Code and data used in the experiments are made available to facilitate further work in this domain.



### Unified Multi-scale Feature Abstraction for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.11456v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11456v1)
- **Published**: 2019-10-24 23:19:05+00:00
- **Updated**: 2019-10-24 23:19:05+00:00
- **Authors**: Xi Fang, Bo Du, Sheng Xu, Bradford J. Wood, Pingkun Yan
- **Comment**: Abstract of SPIE Medical Imaging (Oral)
- **Journal**: None
- **Summary**: Automatic medical image segmentation, an essential component of medical image analysis, plays an importantrole in computer-aided diagnosis. For example, locating and segmenting the liver can be very helpful in livercancer diagnosis and treatment. The state-of-the-art models in medical image segmentation are variants ofthe encoder-decoder architecture such as fully convolutional network (FCN) and U-Net.1A major focus ofthe FCN based segmentation methods has been on network structure engineering by incorporating the latestCNN structures such as ResNet2and DenseNet.3In addition to exploring new network structures for efficientlyabstracting high level features, incorporating structures for multi-scale image feature extraction in FCN hashelped to improve performance in segmentation tasks. In this paper, we design a new multi-scale networkarchitecture, which takes multi-scale inputs with dedicated convolutional paths to efficiently combine featuresfrom different scales to better utilize the hierarchical information.



