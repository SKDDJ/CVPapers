# Arxiv Papers in cs.CV on 2019-10-20
### KRNET: Image Denoising with Kernel Regulation Network
- **Arxiv ID**: http://arxiv.org/abs/1910.08867v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08867v1)
- **Published**: 2019-10-20 01:10:14+00:00
- **Updated**: 2019-10-20 01:10:14+00:00
- **Authors**: Peng Liu, Xiaoxiao Zhou, Junyiyang Li, El Basha Mohammad D, Ruogu Fang
- **Comment**: None
- **Journal**: None
- **Summary**: One popular strategy for image denoising is to design a generalized regularization term that is capable of exploring the implicit prior underlying data observation. Convolutional neural networks (CNN) have shown the powerful capability to learn image prior information through a stack of layers defined by a combination of kernels (filters) on the input. However, existing CNN-based methods mainly focus on synthetic gray-scale images. These methods still exhibit low performance when tackling multi-channel color image denoising. In this paper, we optimize CNN regularization capability by developing a kernel regulation module. In particular, we propose a kernel regulation network-block, referred to as KR-block, by integrating the merits of both large and small kernels, that can effectively estimate features in solving image denoising. We build a deep CNN-based denoiser, referred to as KRNET, via concatenating multiple KR-blocks. We evaluate KRNET on additive white Gaussian noise (AWGN), multi-channel (MC) noise, and realistic noise, where KRNET obtains significant performance gains over state-of-the-art methods across a wide spectrum of noise levels.



### Probabilistic Radiomics: Ambiguous Diagnosis with Controllable Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1910.08878v1
- **DOI**: 10.1007/978-3-030-32226-7_73
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08878v1)
- **Published**: 2019-10-20 02:41:07+00:00
- **Updated**: 2019-10-20 02:41:07+00:00
- **Authors**: Jiancheng Yang, Rongyao Fang, Bingbing Ni, Yamin Li, Yi Xu, Linguo Li
- **Comment**: MICCAI 2019 (early accept), with supplementary materials
- **Journal**: None
- **Summary**: Radiomics analysis has achieved great success in recent years. However, conventional Radiomics analysis suffers from insufficiently expressive hand-crafted features. Recently, emerging deep learning techniques, e.g., convolutional neural networks (CNNs), dominate recent research in Computer-Aided Diagnosis (CADx). Unfortunately, as black-box predictors, we argue that CNNs are "diagnosing" voxels (or pixels), rather than lesions; in other words, visual saliency from a trained CNN is not necessarily concentrated on the lesions. On the other hand, classification in clinical applications suffers from inherent ambiguities: radiologists may produce diverse diagnosis on challenging cases. To this end, we propose a controllable and explainable {\em Probabilistic Radiomics} framework, by combining the Radiomics analysis and probabilistic deep learning. In our framework, 3D CNN feature is extracted upon lesion region only, then encoded into lesion representation, by a controllable Non-local Shape Analysis Module (NSAM) based on self-attention. Inspired from variational auto-encoders (VAEs), an Ambiguity PriorNet is used to approximate the ambiguity distribution over human experts. The final diagnosis is obtained by combining the ambiguity prior sample and lesion representation, and the whole network named $DenseSharp^{+}$ is end-to-end trainable. We apply the proposed method on lung nodule diagnosis on LIDC-IDRI database to validate its effectiveness.



### Learning-based real-time method to looking through scattering medium beyond the memory effect
- **Arxiv ID**: http://arxiv.org/abs/1910.11272v2
- **DOI**: 10.1364/OE.383911
- **Categories**: **eess.IV**, cs.CV, physics.optics
- **Links**: [PDF](http://arxiv.org/pdf/1910.11272v2)
- **Published**: 2019-10-20 03:56:26+00:00
- **Updated**: 2019-11-04 07:25:29+00:00
- **Authors**: Enlai Guo, Shuo Zhu, Yan Sun, Lianfa Bai, Jing Han
- **Comment**: 15 pages with 9 figures
- **Journal**: None
- **Summary**: Strong scattering medium brings great difficulties to optical imaging, which is also a problem in medical imaging and many other fields. Optical memory effect makes it possible to image through strong random scattering medium. However, this method also has the limitation of limited angle field-of-view (FOV), which prevents it from being applied in practice. In this paper, a kind of practical convolutional neural network called PDSNet is proposed, which effectively breaks through the limitation of optical memory effect on FOV. Experiments is conducted to prove that the scattered pattern can be reconstructed accurately in real-time by PDSNet, and it is widely applicable to retrieve complex objects of random scales and different scattering media.



### Unsupervised High-Resolution Depth Learning From Videos With Dual Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.08897v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08897v1)
- **Published**: 2019-10-20 04:50:47+00:00
- **Updated**: 2019-10-20 04:50:47+00:00
- **Authors**: Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Unsupervised depth learning takes the appearance difference between a target view and a view synthesized from its adjacent frame as supervisory signal. Since the supervisory signal only comes from images themselves, the resolution of training data significantly impacts the performance. High-resolution images contain more fine-grained details and provide more accurate supervisory signal. However, due to the limitation of memory and computation power, the original images are typically down-sampled during training, which suffers heavy loss of details and disparity accuracy. In order to fully explore the information contained in high-resolution data, we propose a simple yet effective dual networks architecture, which can directly take high-resolution images as input and generate high-resolution and high-accuracy depth map efficiently. We also propose a Self-assembled Attention (SA-Attention) module to handle low-texture region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates that our method achieves state-of-the-art results in the monocular depth estimation task.



### Pavement Image Datasets: A New Benchmark Dataset to Classify and Densify Pavement Distresses
- **Arxiv ID**: http://arxiv.org/abs/1910.11123v2
- **DOI**: 10.1177/0361198120907283
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11123v2)
- **Published**: 2019-10-20 04:55:37+00:00
- **Updated**: 2020-04-28 06:10:33+00:00
- **Authors**: Hamed Majidifard, Peng Jin, Yaw Adu-Gyamfi, William G. Buttlar
- **Comment**: None
- **Journal**: None
- **Summary**: Automated pavement distresses detection using road images remains a challenging topic in the computer vision research community. Recent developments in deep learning has led to considerable research activity directed towards improving the efficacy of automated pavement distress identification and rating. Deep learning models require a large ground truth data set, which is often not readily available in the case of pavements. In this study, a labeled dataset approach is introduced as a first step towards a more robust, easy-to-deploy pavement condition assessment system. The technique is termed herein as the Pavement Image Dataset (PID) method. The dataset consists of images captured from two camera views of an identical pavement segment, i.e., a wide-view and a top-down view. The wide-view images were used to classify the distresses and to train the deep learning frameworks, while the top-down view images allowed calculation of distress density, which will be used in future studies aimed at automated pavement rating. For the wide view group dataset, 7,237 images were manually annotated and distresses classified into nine categories. Images were extracted using the Google Application Programming Interface (API), selecting street-view images using a python-based code developed for this project. The new dataset was evaluated using two mainstream deep learning frameworks: You Only Look Once (YOLO v2) and Faster Region Convolution Neural Network (Faster R-CNN). Accuracy scores using the F1 index were found to be 0.84 for YOLOv2 and 0.65 for the Faster R-CNN model runs; both quite acceptable considering the convenience of utilizing Google maps images.



### Moving Indoor: Unsupervised Video Depth Learning in Challenging Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.08898v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08898v1)
- **Published**: 2019-10-20 04:56:54+00:00
- **Updated**: 2019-10-20 04:56:54+00:00
- **Authors**: Junsheng Zhou, Yuwang Wang, Kaihuai Qin, Wenjun Zeng
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.



### Endowing Deep 3D Models with Rotation Invariance Based on Principal Component Analysis
- **Arxiv ID**: http://arxiv.org/abs/1910.08901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08901v1)
- **Published**: 2019-10-20 05:10:36+00:00
- **Updated**: 2019-10-20 05:10:36+00:00
- **Authors**: Zelin Xiao, Hongxin Lin, Renjie Li, Hongyang Chao, Shengyong Ding
- **Comment**: 8 pages,5 figures
- **Journal**: None
- **Summary**: In this paper, we propose a simple yet effective method to endow deep 3D models with rotation invariance by expressing the coordinates in an intrinsic frame determined by the object shape itself. Key to our approach is to find such an intrinsic frame which should be unique to the identical object shape and consistent across different instances of the same category, e.g. the frame axes of desks should be all roughly along the edges. Interestingly, the principal component analysis exactly provides an effective way to define such a frame, i.e. setting the principal components as the frame axes. As the principal components have direction ambiguity caused by the sign-ambiguity of eigenvector computation, there exist several intrinsic frames for each object. In order to achieve absolute rotation invariance for a deep model, we adopt the coordinates expressed in all intrinsic frames as inputs to obtain multiple output features, which will be further aggregated as a final feature via a self-attention module. Our method is theoretically rotation-invariant and can be flexibly embedded into the current network architectures. Comprehensive experiments demonstrate that our approach can achieve near state-of-the-art performance on rotation-augmented dataset for ModelNet40 classification and outperform other models on SHREC'17 perturbed retrieval task.



### LinesToFacePhoto: Face Photo Generation from Lines with Conditional Self-Attention Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1910.08914v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08914v1)
- **Published**: 2019-10-20 07:05:24+00:00
- **Updated**: 2019-10-20 07:05:24+00:00
- **Authors**: Yuhang Li, Xuejin Chen, Feng Wu, Zheng-Jun Zha
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we explore the task of generating photo-realistic face images from lines. Previous methods based on conditional generative adversarial networks (cGANs) have shown their power to generate visually plausible images when a conditional image and an output image share well-aligned structures. However, these models fail to synthesize face images with a whole set of well-defined structures, e.g. eyes, noses, mouths, etc., especially when the conditional line map lacks one or several parts. To address this problem, we propose a conditional self-attention generative adversarial network (CSAGAN). We introduce a conditional self-attention mechanism to cGANs to capture long-range dependencies between different regions in faces. We also build a multi-scale discriminator. The large-scale discriminator enforces the completeness of global structures and the small-scale discriminator encourages fine details, thereby enhancing the realism of generated face images. We evaluate the proposed model on the CelebA-HD dataset by two perceptual user studies and three quantitative metrics. The experiment results demonstrate that our method generates high-quality facial images while preserving facial structures. Our results outperform state-of-the-art methods both quantitatively and qualitatively.



### Policy Learning for Malaria Control
- **Arxiv ID**: http://arxiv.org/abs/1910.08926v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08926v1)
- **Published**: 2019-10-20 08:19:40+00:00
- **Updated**: 2019-10-20 08:19:40+00:00
- **Authors**: Van Bach Nguyen, Belaid Mohamed Karim, Bao Long Vu, Jörg Schlötterer, Michael Granitzer
- **Comment**: None
- **Journal**: None
- **Summary**: Sequential decision making is a typical problem in reinforcement learning with plenty of algorithms to solve it. However, only a few of them can work effectively with a very small number of observations. In this report, we introduce the progress to learn the policy for Malaria Control as a Reinforcement Learning problem in the KDD Cup Challenge 2019 and propose diverse solutions to deal with the limited observations problem. We apply the Genetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking to find the optimal policy for five years in a row with only 20 episodes/100 evaluations. We evaluate those algorithms and compare their performance with Random Search as a baseline. Among these algorithms, Q-Learning with sequence breaking has been submitted to the challenge and got ranked 7th in KDD Cup.



### Sketch2Code: Transformation of Sketches to UI in Real-time Using Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.08930v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08930v1)
- **Published**: 2019-10-20 08:59:11+00:00
- **Updated**: 2019-10-20 08:59:11+00:00
- **Authors**: Vanita Jain, Piyush Agrawal, Subham Banga, Rishabh Kapoor, Shashwat Gulyani
- **Comment**: None
- **Journal**: None
- **Summary**: User Interface (UI) prototyping is a necessary step in the early stages of application development. Transforming sketches of a Graphical User Interface (UI) into a coded UI application is an uninspired but time-consuming task performed by a UI designer. An automated system that can replace human efforts for straightforward implementation of UI designs will greatly speed up this procedure. The works that propose such a system primarily focus on using UI wireframes as input rather than hand-drawn sketches. In this paper, we put forward a novel approach wherein we employ a Deep Neural Network that is trained on our custom database of such sketches to detect UI elements in the input sketch. Detection of objects in sketches is a peculiar visual recognition task that requires a specific solution that our deep neural network model attempts to provide. The output from the network is a platform-independent UI representation object. The UI representation object is a dictionary of key-value pairs to represent the UI elements recognized along with their properties. This is further consumed by our UI parser which creates code for different platforms. The intrinsic platform-independence allows the model to create a UI prototype for multiple platforms with single training. This two-step approach without the need for two trained models improves over other methods giving time-efficient results (average time: 129 ms) with good accuracy.



### i-RIM applied to the fastMRI challenge
- **Arxiv ID**: http://arxiv.org/abs/1910.08952v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08952v1)
- **Published**: 2019-10-20 11:32:22+00:00
- **Updated**: 2019-10-20 11:32:22+00:00
- **Authors**: Patrick Putzky, Dimitrios Karkalousos, Jonas Teuwen, Nikita Miriakov, Bart Bakker, Matthan Caan, Max Welling
- **Comment**: Abstract submitted to the fastMRI challenge
- **Journal**: None
- **Summary**: We, team AImsterdam, summarize our submission to the fastMRI challenge (Zbontar et al., 2018). Our approach builds on recent advances in invertible learning to infer models as presented in Putzky and Welling (2019). Both, our single-coil and our multi-coil model share the same basic architecture.



### Combining Shape Priors with Conditional Adversarial Networks for Improved Scapula Segmentation in MR images
- **Arxiv ID**: http://arxiv.org/abs/1910.08963v3
- **DOI**: 10.1109/ISBI45749.2020.9098360
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08963v3)
- **Published**: 2019-10-20 12:36:56+00:00
- **Updated**: 2020-01-23 08:36:45+00:00
- **Authors**: Arnaud Boutillon, Bhushan Borotikar, Valérie Burdin, Pierre-Henri Conze
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes an automatic method for scapula bone segmentation from Magnetic Resonance (MR) images using deep learning. The purpose of this work is to incorporate anatomical priors into a conditional adversarial framework, given a limited amount of heterogeneous annotated images. Our approach encourages the segmentation model to follow the global anatomical properties of the underlying anatomy through a learnt non-linear shape representation while the adversarial contribution refines the model by promoting realistic delineations. These contributions are evaluated on a dataset of 15 pediatric shoulder examinations, and compared to state-of-the-art architectures including UNet and recent derivatives. The significant improvements achieved bring new perspectives for the pre-operative management of musculo-skeletal diseases.



### Image Difficulty Curriculum for Generative Adversarial Networks (CuGAN)
- **Arxiv ID**: http://arxiv.org/abs/1910.08967v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08967v2)
- **Published**: 2019-10-20 13:06:26+00:00
- **Updated**: 2019-10-22 22:19:01+00:00
- **Authors**: Petru Soviany, Claudiu Ardei, Radu Tudor Ionescu, Marius Leordeanu
- **Comment**: Accepted at WACV 2020
- **Journal**: None
- **Summary**: Despite the significant advances in recent years, Generative Adversarial Networks (GANs) are still notoriously hard to train. In this paper, we propose three novel curriculum learning strategies for training GANs. All strategies are first based on ranking the training images by their difficulty scores, which are estimated by a state-of-the-art image difficulty predictor. Our first strategy is to divide images into gradually more difficult batches. Our second strategy introduces a novel curriculum loss function for the discriminator that takes into account the difficulty scores of the real images. Our third strategy is based on sampling from an evolving distribution, which favors the easier images during the initial training stages and gradually converges to a uniform distribution, in which samples are equally likely, regardless of difficulty. We compare our curriculum learning strategies with the classic training procedure on two tasks: image generation and image translation. Our experiments indicate that all strategies provide faster convergence and superior results. For example, our best curriculum learning strategy applied on spectrally normalized GANs (SNGANs) fooled human annotators in thinking that generated CIFAR-like images are real in 25.0% of the presented cases, while the SNGANs trained using the classic procedure fooled the annotators in only 18.4% cases. Similarly, in image translation, the human annotators preferred the images produced by the Cycle-consistent GAN (CycleGAN) trained using curriculum learning in 40.5% cases and those produced by CycleGAN based on classic training in only 19.8% cases, 39.7% cases being labeled as ties.



### Identity Document and banknote security forensics: a survey
- **Arxiv ID**: http://arxiv.org/abs/1910.08993v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08993v1)
- **Published**: 2019-10-20 14:35:15+00:00
- **Updated**: 2019-10-20 14:35:15+00:00
- **Authors**: Albert Berenguel Centeno, Oriol Ramos Terrades, Josep Lladós Canet, Cristina Cañero Morales
- **Comment**: 35 pages, 5 figures, 14 tables
- **Journal**: None
- **Summary**: Counterfeiting and piracy are a form of theft that has been steadily growing in recent years. Banknotes and identity documents are two common objects of counterfeiting. Aiming to detect these counterfeits, the present survey covers a wide range of anti-counterfeiting security features, categorizing them into three components: security substrate, security inks and security printing. respectively. From the computer vision perspective, we present works in the literature covering these three categories. Other topics, such as history of counterfeiting, effects on society and document experts, counterfeiter types of attacks, trends among others are covered. Therefore, from non-experienced to professionals in security documents, can be introduced or deepen its knowledge in anti-counterfeiting measures.



### SANet:Superpixel Attention Network for Skin Lesion Attributes Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.08995v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.TO, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/1910.08995v1)
- **Published**: 2019-10-20 14:42:04+00:00
- **Updated**: 2019-10-20 14:42:04+00:00
- **Authors**: Xinzi He, Baiying Lei, Tianfu Wang
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: The accurate detection of lesion attributes is meaningful for both the computeraid diagnosis system and dermatologists decisions. However, unlike lesion segmentation and melenoma classification, there are few deep learning methods and literatures focusing on this task. Currently, the lesion attribute detection still remains challenging due to the extremely unbalanced class distribution and insufficient samples, as well as large intraclass and low interclass variations. To solve these problems, we propose a deep learning framework named superpixel attention network (SANet). Firstly, we segment input images into small regions and shuffle the obtained regions by the random shuttle mechanism (RSM). Secondly, we apply the SANet to capture discriminative features and reconstruct input images. Specifically, SANet contains two sub modules: superpixel average pooling and superpixel at tention module. We introduce a superpixel average pooling to reformulate the superpixel classification problem as a superpixel segmentation problem and a SAMis utilized to focus on discriminative superpixel regions and feature channels. Finally, we design a novel but effective loss, namely global balancing loss to address the serious data imbalance in ISIC 2018 Task 2 lesion attributes detection dataset. The proposed method achieves quite good performance on the ISIC 2018 Task 2 challenge.



### Boosting Network Weight Separability via Feed-Backward Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.09024v2
- **DOI**: 10.1109/ACCESS.2020.3041470
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09024v2)
- **Published**: 2019-10-20 17:04:40+00:00
- **Updated**: 2020-09-14 14:05:15+00:00
- **Authors**: Jongmin Yu, Hyeontaek Oh
- **Comment**: 8 pages, 6 figures
- **Journal**: in IEEE Access, vol. 8, pp. 214923-214931, 2020
- **Summary**: This paper proposes a new evaluation metric and boosting method for weight separability in neural network design. In contrast to general visual recognition methods designed to encourage both intra-class compactness and inter-class separability of latent features, we focus on estimating linear independence of column vectors in weight matrix and improving the separability of weight vectors. To this end, we propose an evaluation metric for weight separability based on semi-orthogonality of a matrix and Frobenius distance, and the feed-backward reconstruction loss which explicitly encourages weight separability between the column vectors in the weight matrix. The experimental results on image classification and face recognition demonstrate that the weight separability boosting via minimization of feed-backward reconstruction loss can improve the visual recognition performance, hence universally boosting the performance on various visual recognition tasks.



### CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer Assisted Interventions
- **Arxiv ID**: http://arxiv.org/abs/1910.09031v1
- **DOI**: 10.1109/JPROC.2019.2946993
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09031v1)
- **Published**: 2019-10-20 17:41:29+00:00
- **Updated**: 2019-10-20 17:41:29+00:00
- **Authors**: Tom Vercauteren, Mathias Unberath, Nicolas Padoy, Nassir Navab
- **Comment**: None
- **Journal**: None
- **Summary**: Data-driven computational approaches have evolved to enable extraction of information from medical images with a reliability, accuracy and speed which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theatres are extremely complex and typically rely on poorly integrated intra-operative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer assisted interventions, we highlight the crucial need to take context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer assisted intervention, or CAI4CAI, arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision making ultimately producing more precise and reliable interventions.



### Image recognition from raw labels collected without annotators
- **Arxiv ID**: http://arxiv.org/abs/1910.09055v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09055v3)
- **Published**: 2019-10-20 19:58:21+00:00
- **Updated**: 2020-02-25 23:11:46+00:00
- **Authors**: Fatih Furkan Yilmaz, Reinhard Heckel
- **Comment**: Version changelog: Added content on ImageNet related experiments;
  Re-structured the document to incorporate the new content
- **Journal**: None
- **Summary**: Image classification problems are typically addressed by first collecting examples with candidate labels, second cleaning the candidate labels manually, and third training a deep neural network on the clean examples. The manual labeling step is often the most expensive one as it requires workers to label millions of images. In this paper we propose to work without any explicitly labeled data by i) directly training the deep neural network on the noisy candidate labels, and ii) early stopping the training to avoid overfitting. With this procedure we exploit an intriguing property of standard overparameterized convolutional neural networks trained with (stochastic) gradient descent: Clean labels are fitted faster than noisy ones. We consider two classification problems, a subset of ImageNet and CIFAR-10. For both, we construct large candidate datasets without any explicit human annotations, that only contain 10%-50% correctly labeled examples per class. We show that training on the candidate examples and regularizing through early stopping gives higher test performance for both problems than when training on the original, clean data. This is possible because the candidate datasets contain a huge number of clean examples, and, as we show in this paper, the noise generated through the label collection process is not nearly as adversarial for learning as the noise generated by randomly flipping labels.



### Zero-Shot Recognition via Optimal Transport
- **Arxiv ID**: http://arxiv.org/abs/1910.09057v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09057v2)
- **Published**: 2019-10-20 20:18:18+00:00
- **Updated**: 2020-12-27 03:52:18+00:00
- **Authors**: Wenlin Wang, Hongteng Xu, Guoyin Wang, Wenqi Wang, Lawrence Carin
- **Comment**: To appear in WACV 2021
- **Journal**: None
- **Summary**: We propose an optimal transport (OT) framework for generalized zero-shot learning (GZSL), seeking to distinguish samples for both seen and unseen classes, with the assist of auxiliary attributes. The discrepancy between features and attributes is minimized by solving an optimal transport problem. {Specifically, we build a conditional generative model to generate features from seen-class attributes, and establish an optimal transport between the distribution of the generated features and that of the real features.} The generative model and the optimal transport are optimized iteratively with an attribute-based regularizer, that further enhances the discriminative power of the generated features. A classifier is learned based on the features generated for both the seen and unseen classes. In addition to generalized zero-shot learning, our framework is also applicable to standard and transductive ZSL problems. Experiments show that our optimal transport-based method outperforms state-of-the-art methods on several benchmark datasets.



### Deep Mouse: An End-to-end Auto-context Refinement Framework for Brain Ventricle and Body Segmentation in Embryonic Mice Ultrasound Volumes
- **Arxiv ID**: http://arxiv.org/abs/1910.09061v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09061v2)
- **Published**: 2019-10-20 20:49:39+00:00
- **Updated**: 2019-10-30 00:53:23+00:00
- **Authors**: Tongda Xu, Ziming Qiu, William Das, Chuiyu Wang, Jack Langerman, Nitin Nair, Orlando Aristizabal, Jonathan Mamou, Daniel H. Turnbull, Jeffrey A. Ketterling, Yao Wang
- **Comment**: Full Paper Submission to ISBI 2020
- **Journal**: None
- **Summary**: High-frequency ultrasound (HFU) is well suited for imaging embryonic mice due to its noninvasive and real-time characteristics. However, manual segmentation of the brain ventricles (BVs) and body requires substantial time and expertise. This work proposes a novel deep learning based end-to-end auto-context refinement framework, consisting of two stages. The first stage produces a low resolution segmentation of the BV and body simultaneously. The resulting probability map for each object (BV or body) is then used to crop a region of interest (ROI) around the target object in both the original image and the probability map to provide context to the refinement segmentation network. Joint training of the two stages provides significant improvement in Dice Similarity Coefficient (DSC) over using only the first stage (0.818 to 0.906 for the BV, and 0.919 to 0.934 for the body). The proposed method significantly reduces the inference time (102.36 to 0.09 s/volume around 1000x faster) while slightly improves the segmentation accuracy over the previous methods using slide-window approaches.



### Structured Prediction Helps 3D Human Motion Modelling
- **Arxiv ID**: http://arxiv.org/abs/1910.09070v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09070v1)
- **Published**: 2019-10-20 22:06:14+00:00
- **Updated**: 2019-10-20 22:06:14+00:00
- **Authors**: Emre Aksan, Manuel Kaufmann, Otmar Hilliges
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Human motion prediction is a challenging and important task in many computer vision application domains. Existing work only implicitly models the spatial structure of the human skeleton. In this paper, we propose a novel approach that decomposes the prediction into individual joints by means of a structured prediction layer that explicitly models the joint dependencies. This is implemented via a hierarchy of small-sized neural networks connected analogously to the kinematic chains in the human body as well as a joint-wise decomposition in the loss function. The proposed layer is agnostic to the underlying network and can be used with existing architectures for motion modelling. Prior work typically leverages the H3.6M dataset. We show that some state-of-the-art techniques do not perform well when trained and tested on AMASS, a recently released dataset 14 times the size of H3.6M. Our experiments indicate that the proposed layer increases the performance of motion forecasting irrespective of the base network, joint-angle representation, and prediction horizon. We furthermore show that the layer also improves motion predictions qualitatively. We make code and models publicly available at https://ait.ethz.ch/projects/2019/spl.



### Peanut Maturity Classification using Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1910.11122v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11122v2)
- **Published**: 2019-10-20 22:51:49+00:00
- **Updated**: 2019-10-25 00:33:46+00:00
- **Authors**: Sheng Zou, Yu-Chien Tseng, Alina Zare, Diane Rowland, Barry Tillman, Seung-Chul Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: Seed maturity in peanut (Arachis hypogaea L.) determines economic return to a producer because of its impact on seed weight (yield), and critically influences seed vigor and other quality characteristics. During seed development, the inner mesocarp layer of the pericarp (hull) transitions in color from white to black as the seed matures. The maturity assessment process involves the removal of the exocarp of the hull and visually categorizing the mesocarp color into varying color classes from immature (white, yellow, orange) to mature (brown, and black). This visual color classification is time consuming because the exocarp must be manually removed. In addition, the visual classification process involves human assessment of colors, which leads to large variability of color classification from observer to observer. A more objective, digital imaging approach to peanut maturity is needed, optimally without the requirement of removal of the hull's exocarp. This study examined the use of a hyperspectral imaging (HSI) process to determine pod maturity with intact pericarps. The HSI method leveraged spectral differences between mature and immature pods within a classification algorithm to identify the mature and immature pods. The results showed a high classification accuracy with consistency using samples from different years and cultivars. In addition, the proposed method was capable of estimating a continuous-valued, pixel-level maturity value for individual peanut pods, allowing for a valuable tool that can be utilized in seed quality research. This new method solves issues of labor intensity and subjective error that all current methods of peanut maturity determination have.



### Looking Ahead: Anticipating Pedestrians Crossing with Future Frames Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.09077v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09077v2)
- **Published**: 2019-10-20 23:12:13+00:00
- **Updated**: 2020-03-16 22:49:17+00:00
- **Authors**: Mohamed Chaabane, Ameni Trabelsi, Nathaniel Blanchard, Ross Beveridge
- **Comment**: None
- **Journal**: WACV 2020
- **Summary**: In this paper, we present an end-to-end future-prediction model that focuses on pedestrian safety. Specifically, our model uses previous video frames, recorded from the perspective of the vehicle, to predict if a pedestrian will cross in front of the vehicle. The long term goal of this work is to design a fully autonomous system that acts and reacts as a defensive human driver would --- predicting future events and reacting to mitigate risk. We focus on pedestrian-vehicle interactions because of the high risk of harm to the pedestrian if their actions are miss-predicted. Our end-to-end model consists of two stages: the first stage is an encoder/decoder network that learns to predict future video frames. The second stage is a deep spatio-temporal network that utilizes the predicted frames of the first stage to predict the pedestrian's future action. Our system achieves state-of-the-art accuracy on pedestrian behavior prediction and future frames prediction on the Joint Attention for Autonomous Driving (JAAD) dataset.



