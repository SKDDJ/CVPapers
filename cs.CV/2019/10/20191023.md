# Arxiv Papers in cs.CV on 2019-10-23
### KnowIT VQA: Answering Knowledge-Based Questions about Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.10706v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.10706v3)
- **Published**: 2019-10-23 01:44:12+00:00
- **Updated**: 2019-12-24 04:13:21+00:00
- **Authors**: Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.



### Using Segmentation Masks in the ICCV 2019 Learning to Drive Challenge
- **Arxiv ID**: http://arxiv.org/abs/1910.10317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10317v1)
- **Published**: 2019-10-23 02:24:28+00:00
- **Updated**: 2019-10-23 02:24:28+00:00
- **Authors**: Antonia Lovjer, Minsu Yeom, Benedikt D. Schifferer, Iddo Drori
- **Comment**: None
- **Journal**: ICCV Autonomous Driving Workshop, 2019
- **Summary**: In this work we predict vehicle speed and steering angle given camera image frames. Our key contribution is using an external pre-trained neural network for segmentation. We augment the raw images with their segmentation masks and mirror images. We ensemble three diverse neural network models (i) a CNN using a single image and its segmentation mask, (ii) a stacked CNN taking as input a sequence of images and segmentation masks, and (iii) a bidirectional GRU, extracting image features using a pre-trained ResNet34, DenseNet121 and our own CNN single image model. We achieve the second best performance for MSE angle and second best performance overall, to win 2nd place in the ICCV Learning to Drive challenge. We make our models and code publicly available.



### Winning the ICCV 2019 Learning to Drive Challenge
- **Arxiv ID**: http://arxiv.org/abs/1910.10318v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10318v1)
- **Published**: 2019-10-23 02:31:18+00:00
- **Updated**: 2019-10-23 02:31:18+00:00
- **Authors**: Michael Diodato, Yu Li, Manik Goyal, Iddo Drori
- **Comment**: None
- **Journal**: ICCV Autonomous Driving Workshop, 2019
- **Summary**: Autonomous driving has a significant impact on society. Predicting vehicle trajectories, specifically, angle and speed, is important for safe and comfortable driving. This work focuses on fusing inputs from camera sensors and visual map data which lead to significant improvement in performance and plays a key role in winning the challenge. We use pre-trained CNN's for processing image frames, a neural network for fusing the image representation with visual map data, and train a sequence model for time series prediction. We demonstrate the best performing MSE angle and best performance overall, to win the ICCV 2019 Learning to Drive challenge. We make our models and code publicly available.



### Class-imbalanced Domain Adaptation: An Empirical Odyssey
- **Arxiv ID**: http://arxiv.org/abs/1910.10320v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10320v2)
- **Published**: 2019-10-23 02:35:46+00:00
- **Updated**: 2020-09-19 07:58:59+00:00
- **Authors**: Shuhan Tan, Xingchao Peng, Kate Saenko
- **Comment**: ECCV 2020 Workshops - TASK-CV 2020
- **Journal**: None
- **Summary**: Unsupervised domain adaptation is a promising way to generalize deep models to novel domains. However, the current literature assumes that the label distribution is domain-invariant and only aligns the feature distributions or vice versa. In this work, we explore the more realistic task of Class-imbalanced Domain Adaptation: How to align feature distributions across domains while the label distributions of the two domains are also different? Taking a practical step towards this problem, we constructed the first benchmark with 22 cross-domain tasks from 6real-image datasets. We conducted comprehensive experiments on 10 recent domain adaptation methods and find most of them are very fragile in the face of coexisting feature and label distribution shift. Towards a better solution, we further proposed a feature and label distribution CO-ALignment (COAL) model with a novel combination of existing ideas. COAL is empirically shown to outperform the most recent domain adaptation methods on our benchmarks. We believe the provided benchmarks, empirical analysis results, and the COAL baseline could stimulate and facilitate future research towards this important problem.



### Region Based Adversarial Synthesis of Facial Action Units
- **Arxiv ID**: http://arxiv.org/abs/1910.10323v1
- **DOI**: 10.1007/978-3-030-37734-2_42
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10323v1)
- **Published**: 2019-10-23 02:43:57+00:00
- **Updated**: 2019-10-23 02:43:57+00:00
- **Authors**: Zhilei Liu, Diyi Liu, Yunpeng Wu
- **Comment**: Accepted by MMM2020
- **Journal**: None
- **Summary**: Facial expression synthesis or editing has recently received increasing attention in the field of affective computing and facial expression modeling. However, most existing facial expression synthesis works are limited in paired training data, low resolution, identity information damaging, and so on. To address those limitations, this paper introduces a novel Action Unit (AU) level facial expression synthesis method called Local Attentive Conditional Generative Adversarial Network (LAC-GAN) based on face action units annotations. Given desired AU labels, LAC-GAN utilizes local AU regional rules to control the status of each AU and attentive mechanism to combine several of them into the whole photo-realistic facial expressions or arbitrary facial expressions. In addition, unpaired training data is utilized in our proposed method to train the manipulation module with the corresponding AU labels, which learns a mapping between a facial expression manifold. Extensive qualitative and quantitative evaluations are conducted on the commonly used BP4D dataset to verify the effectiveness of our proposed AU synthesis method.



### Iterative Distance-Aware Similarity Matrix Convolution with Mutual-Supervised Point Elimination for Efficient Point Cloud Registration
- **Arxiv ID**: http://arxiv.org/abs/1910.10328v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10328v3)
- **Published**: 2019-10-23 03:26:49+00:00
- **Updated**: 2020-08-06 03:08:34+00:00
- **Authors**: Jiahao Li, Changhao Zhang, Ziyao Xu, Hangning Zhou, Chi Zhang
- **Comment**: None
- **Journal**: European Conference on Computer Vision (ECCV) 2020
- **Summary**: In this paper, we propose a novel learning-based pipeline for partially overlapping 3D point cloud registration. The proposed model includes an iterative distance-aware similarity matrix convolution module to incorporate information from both the feature and Euclidean space into the pairwise point matching process. These convolution layers learn to match points based on joint information of the entire geometric features and Euclidean offset for each point pair, overcoming the disadvantage of matching by simply taking the inner product of feature vectors. Furthermore, a two-stage learnable point elimination technique is presented to improve computational efficiency and reduce false positive correspondence pairs. A novel mutual-supervision loss is proposed to train the model without extra annotations of keypoints. The pipeline can be easily integrated with both traditional (e.g. FPFH) and learning-based features. Experiments on partially overlapping and noisy point cloud registration show that our method outperforms the current state-of-the-art, while being more computationally efficient. Code is publicly available at https://github.com/jiahaowork/idam.



### Stain Style Transfer using Transitive Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.10330v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10330v1)
- **Published**: 2019-10-23 03:29:56+00:00
- **Updated**: 2019-10-23 03:29:56+00:00
- **Authors**: Shaojin Cai, Yuyang Xue3 Qinquan Gao, Min Du, Gang Chen, Hejun Zhang, Tong Tong
- **Comment**: MICCAI 2019 MLMIR Workshop, Oral Paper
- **Journal**: None
- **Summary**: Digitized pathological diagnosis has been in increasing demand recently. It is well known that color information is critical to the automatic and visual analysis of pathological slides. However, the color variations due to various factors not only have negative impact on pathologist's diagnosis, but also will reduce the robustness of the algorithms. The factors that cause the color differences are not only in the process of making the slices, but also in the process of digitization. Different strategies have been proposed to alleviate the color variations. Most of such techniques rely on collecting color statistics to perform color matching across images and highly dependent on a reference template slide. Since the pathological slides between hospitals are usually unpaired, these methods do not yield good matching results. In this work, we propose a novel network that we refer to as Transitive Adversarial Networks (TAN) to transfer the color information among slides from different hospitals or centers. It is not necessary for an expert to pick a representative reference slide in the proposed TAN method. We compare the proposed method with the state-of-the-art methods quantitatively and qualitatively. Compared with the state-of-the-art methods, our method yields an improvement of 0.87dB in terms of PSNR, demonstrating the effectiveness of the proposed TAN method in stain style transfer.



### Relation Modeling with Graph Convolutional Networks for Facial Action Unit Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.10334v1
- **DOI**: 10.1007/978-3-030-37734-2_40
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10334v1)
- **Published**: 2019-10-23 03:57:52+00:00
- **Updated**: 2019-10-23 03:57:52+00:00
- **Authors**: Zhilei Liu, Jiahui Dong, Cuicui Zhang, Longbiao Wang, Jianwu Dang
- **Comment**: Accepted by MMM2020
- **Journal**: None
- **Summary**: Most existing AU detection works considering AU relationships are relying on probabilistic graphical models with manually extracted features. This paper proposes an end-to-end deep learning framework for facial AU detection with graph convolutional network (GCN) for AU relation modeling, which has not been explored before. In particular, AU related regions are extracted firstly, latent representations full of AU information are learned through an auto-encoder. Moreover, each latent representation vector is feed into GCN as a node, the connection mode of GCN is determined based on the relationships of AUs. Finally, the assembled features updated through GCN are concatenated for AU detection. Extensive experiments on BP4D and DISFA benchmarks demonstrate that our framework significantly outperforms the state-of-the-art methods for facial AU detection. The proposed framework is also validated through a series of ablation studies.



### Facial Expression Restoration Based on Improved Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.10344v1
- **DOI**: 10.1007/978-3-030-37734-2_43
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10344v1)
- **Published**: 2019-10-23 04:27:40+00:00
- **Updated**: 2019-10-23 04:27:40+00:00
- **Authors**: Zhilei Liu, Le Li, Yunpeng Wu, Cuicui Zhang
- **Comment**: Accepted by MMM2020
- **Journal**: None
- **Summary**: Facial expression analysis in the wild is challenging when the facial image is with low resolution or partial occlusion. Considering the correlations among different facial local regions under different facial expressions, this paper proposes a novel facial expression restoration method based on generative adversarial network by integrating an improved graph convolutional network (IGCN) and region relation modeling block (RRMB). Unlike conventional graph convolutional networks taking vectors as input features, IGCN can use tensors of face patches as inputs. It is better to retain the structure information of face patches. The proposed RRMB is designed to address facial generative tasks including inpainting and super-resolution with facial action units detection, which aims to restore facial expression as the ground-truth. Extensive experiments conducted on BP4D and DISFA benchmarks demonstrate the effectiveness of our proposed method through quantitative and qualitative evaluations.



### Unsupervised Dual Adversarial Learning for Anomaly Detection in Colonoscopy Video Frames
- **Arxiv ID**: http://arxiv.org/abs/1910.10345v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10345v2)
- **Published**: 2019-10-23 04:30:19+00:00
- **Updated**: 2021-02-06 10:42:14+00:00
- **Authors**: Yuyuan Liu, Yu Tian, Gabriel Maicas, Leonardo Z. C. T. Pu, Rajvinder Singh, Johan W. Verjans, Gustavo Carneiro
- **Comment**: Accepted by ISBI 2020
- **Journal**: None
- **Summary**: The automatic detection of frames containing polyps from a colonoscopy video sequence is an important first step for a fully automated colonoscopy analysis tool. Typically, such detection system is built using a large annotated data set of frames with and without polyps, which is expensive to be obtained. In this paper, we introduce a new system that detects frames containing polyps as anomalies from a distribution of frames from exams that do not contain any polyps. The system is trained using a one-class training set consisting of colonoscopy frames without polyps -- such training set is considerably less expensive to obtain, compared to the 2-class data set mentioned above. During inference, the system is only able to reconstruct frames without polyps, and when it tries to reconstruct a frame with polyp, it automatically removes (i.e., photoshop) it from the frame -- the difference between the input and reconstructed frames is used to detect frames with polyps. We name our proposed model as anomaly detection generative adversarial network (ADGAN), comprising a dual GAN with two generators and two discriminators. We show that our proposed approach achieves the state-of-the-art result on this data set, compared with recently proposed anomaly detection systems.



### TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation
- **Arxiv ID**: http://arxiv.org/abs/1911.05186v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG, cs.SD, eess.AS, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05186v1)
- **Published**: 2019-10-23 05:02:15+00:00
- **Updated**: 2019-10-23 05:02:15+00:00
- **Authors**: Wubo Li, Wei Zou, Xiangang Li
- **Comment**: submitted to ICASSP 2020
- **Journal**: None
- **Summary**: Multimodalities provide promising performance than unimodality in most tasks. However, learning the semantic of the representations from multimodalities efficiently is extremely challenging. To tackle this, we propose the Transformer based Cross-modal Translator (TCT) to learn unimodal sequence representations by translating from other related multimodal sequences on a supervised learning method. Combined TCT with Multimodal Transformer Network (MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses multimodality. The proposed method reports new state-of-the-art performance on video-grounded dialogue which indicates representations learned by TCT are more semantics compared to directly use unimodality.



### EdgeAI: A Vision for Deep Learning in IoT Era
- **Arxiv ID**: http://arxiv.org/abs/1910.10356v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10356v1)
- **Published**: 2019-10-23 05:16:32+00:00
- **Updated**: 2019-10-23 05:16:32+00:00
- **Authors**: Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu
- **Comment**: To appear in IEEE Design and Test
- **Journal**: None
- **Summary**: The significant computational requirements of deep learning present a major bottleneck for its large-scale adoption on hardware-constrained IoT-devices. Here, we envision a new paradigm called EdgeAI to address major impediments associated with deploying deep networks at the edge. Specifically, we discuss the existing directions in computation-aware deep learning and describe two new challenges in the IoT era: (1) Data-independent deployment of learning, and (2) Communication-aware distributed inference. We further present new directions from our recent research to alleviate the latter two challenges. Overcoming these challenges is crucial for rapid adoption of learning on IoT-devices in order to truly enable EdgeAI.



### Streaming Networks: Enable A Robust Classification of Noise-Corrupted Images
- **Arxiv ID**: http://arxiv.org/abs/1910.11107v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11107v1)
- **Published**: 2019-10-23 05:32:32+00:00
- **Updated**: 2019-10-23 05:32:32+00:00
- **Authors**: Sergey Tarasenko, Fumihiko Takahashi
- **Comment**: 10 pages, 16 figures
- **Journal**: None
- **Summary**: The convolution neural nets (conv nets) have achieved a state-of-the-art performance in many applications of image and video processing. The most recent studies illustrate that the conv nets are fragile in terms of recognition accuracy to various image distortions such as noise, scaling, rotation, etc. In this study we focus on the problem of robust recognition accuracy of random noise distorted images. A common solution to this problem is either to add a lot of noisy images into a training dataset, which can be very costly, or use sophisticated loss function and denoising techniques. We introduce a novel conv net architecture with multiple streams. Each stream is taking a certain intensity slice of the original image as an input, and stream parameters are trained independently. We call this novel network a "Streaming Net". Our results indicate that Streaming Net outperforms 1-stream conv net (employed as a single stream) and 1-stream wide conv net (employs the same number of filters as Streaming Net) in recognition accuracy of noise-corrupted images, while producing the same or higher recognition accuracy of no noise images in almost all of the tests. Thus, we introduce a new simple method to increase robustness of recognition of noisy images without using data generation or sophisticated training techniques.



### Deep Classification Network for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.10369v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10369v1)
- **Published**: 2019-10-23 05:50:04+00:00
- **Updated**: 2019-10-23 05:50:04+00:00
- **Authors**: Azeez Oluwafemi, Yang Zou, B. V. K. Vijaya Kumar
- **Comment**: None
- **Journal**: None
- **Summary**: Monocular Depth Estimation is usually treated as a supervised and regression problem when it actually is very similar to semantic segmentation task since they both are fundamentally pixel-level classification tasks. We applied depth increments that increases with depth in discretizing depth values and then applied Deeplab v2 and the result was higher accuracy. We were able to achieve a state-of-the-art result on the KITTI dataset and outperformed existing architecture by an 8% margin.



### Semi-supervised Multi-domain Multi-task Training for Metastatic Colon Lymph Node Diagnosis From Abdominal CT
- **Arxiv ID**: http://arxiv.org/abs/1910.10371v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10371v1)
- **Published**: 2019-10-23 05:54:08+00:00
- **Updated**: 2019-10-23 05:54:08+00:00
- **Authors**: Saskia Glaser, Gabriel Maicas, Sergei Bedrikovetski, Tarik Sammour, Gustavo Carneiro
- **Comment**: Under review at ISBI 2020
- **Journal**: None
- **Summary**: The diagnosis of the presence of metastatic lymph nodes from abdominal computed tomography (CT) scans is an essential task performed by radiologists to guide radiation and chemotherapy treatment. State-of-the-art deep learning classifiers trained for this task usually rely on a training set containing CT volumes and their respective image-level (i.e., global) annotation. However, the lack of annotations for the localisation of the regions of interest (ROIs) containing lymph nodes can limit classification accuracy due to the small size of the relevant ROIs in this problem. The use of lymph node ROIs together with global annotations in a multi-task training process has the potential to improve classification accuracy, but the high cost involved in obtaining the ROI annotation for the same samples that have global annotations is a roadblock for this alternative. We address this limitation by introducing a new training strategy from two data sets: one containing the global annotations, and another (publicly available) containing only the lymph node ROI localisation. We term our new strategy semi-supervised multi-domain multi-task training, where the goal is to improve the diagnosis accuracy on the globally annotated data set by incorporating the ROI annotations from a different domain. Using a private data set containing global annotations and a public data set containing lymph node ROI localisation, we show that our proposed training mechanism improves the area under the ROC curve for the classification task compared to several training method baselines.



### Efficient Decoupled Neural Architecture Search by Structure and Operation Sampling
- **Arxiv ID**: http://arxiv.org/abs/1910.10397v1
- **DOI**: 10.1109/ICASSP40776.2020.9053197
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10397v1)
- **Published**: 2019-10-23 08:00:22+00:00
- **Updated**: 2019-10-23 08:00:22+00:00
- **Authors**: Heung-Chang Lee, Do-Guk Kim, Bohyung Han
- **Comment**: None
- **Journal**: IEEE ICASSP 2020
- **Summary**: We propose a novel neural architecture search algorithm via reinforcement learning by decoupling structure and operation search processes. Our approach samples candidate models from the multinomial distribution on the policy vectors defined on the two search spaces independently. The proposed technique improves the efficiency of architecture search process significantly compared to the conventional methods based on reinforcement learning with the RNN controllers while achieving competitive accuracy and model size in target tasks. Our policy vectors are easily interpretable throughout the training procedure, which allows to analyze the search progress and the discovered architectures; the black-box characteristics of the RNN controllers hamper understanding training progress in terms of policy parameter updates. Our experiments demonstrate outstanding performance compared to the state-of-the-art methods with a fraction of search cost.



### Random 2.5D U-net for Fully 3D Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.10398v1
- **DOI**: 10.1007/978-3-030-33327-0_19
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10398v1)
- **Published**: 2019-10-23 08:02:09+00:00
- **Updated**: 2019-10-23 08:02:09+00:00
- **Authors**: Christoph Angermann, Markus Haltmeier
- **Comment**: Submission for joint MICCAI-Workshops on Computing and Visualization
  for Intravascular Imaging and Computer Assisted Stenting (CVII-STENT) 2019
- **Journal**: None
- **Summary**: Convolutional neural networks are state-of-the-art for various segmentation tasks. While for 2D images these networks are also computationally efficient, 3D convolutions have huge storage requirements and therefore, end-to-end training is limited by GPU memory and data size. To overcome this issue, we introduce a network structure for volumetric data without 3D convolution layers. The main idea is to include projections from different directions to transform the volumetric data to a sequence of images, where each image contains information of the full data. We then apply 2D convolutions to these projection images and lift them again to volumetric data using a trainable reconstruction algorithm. The proposed architecture can be applied end-to-end to very large data volumes without cropping or sliding-window techniques. For a tested sparse binary segmentation task, it outperforms already known standard approaches and is more resistant to generation of artefacts.



### INTEL-TAU: A Color Constancy Dataset
- **Arxiv ID**: http://arxiv.org/abs/1910.10404v5
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10404v5)
- **Published**: 2019-10-23 08:21:51+00:00
- **Updated**: 2020-12-23 14:40:01+00:00
- **Authors**: Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Jarno Nikkanen, Moncef Gabbouj
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: In this paper, we describe a new large dataset for illumination estimation. This dataset, called INTEL-TAU, contains 7022 images in total, which makes it the largest available high-resolution dataset for illumination estimation research. The variety of scenes captured using three different camera models, namely Canon 5DSR, Nikon D810, and Sony IMX135, makes the dataset appropriate for evaluating the camera and scene invariance of the different illumination estimation techniques. Privacy masking is done for sensitive information, e.g., faces. Thus, the dataset is coherent with the new General Data Protection Regulation (GDPR). Furthermore, the effect of color shading for mobile images can be evaluated with INTEL-TAU dataset, as both corrected and uncorrected versions of the raw data are provided. Furthermore, this paper benchmarks several color constancy approaches on the proposed dataset.



### Identification of primary angle-closure on AS-OCT images with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.10414v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10414v1)
- **Published**: 2019-10-23 08:47:29+00:00
- **Updated**: 2019-10-23 08:47:29+00:00
- **Authors**: Chenglang Yuan, Cheng Bian, Hongjian Kang, Shu Liang, Kai Ma, Yefeng Zheng
- **Comment**: The third place in angle-closure glaucoma evaluation (AGE) Challenge,
  MICCAI 2019
- **Journal**: None
- **Summary**: Primary angle-closure disease (PACD) is a severe retinal disease, which might cause irreversible vision loss. In clinic, accurate identification of angle-closure and localization of the scleral spur's position on anterior segment optical coherence tomography (AS-OCT) is essential for the diagnosis of PACD. However, manual delineation might confine in low accuracy and low efficiency. In this paper, we propose an efficient and accurate end-to-end architecture for angle-closure classification and scleral spur localization. Specifically, we utilize a revised ResNet152 as our backbone to improve the accuracy of the angle-closure identification. For scleral spur localization, we adopt EfficientNet as encoder because of its powerful feature extraction potential. By combining the skip-connect module and pyramid pooling module, the network is able to collect semantic cues in feature maps from multiple dimensions and scales. Afterward, we propose a novel keypoint registration loss to constrain the model's attention to the intensity and location of the scleral spur area. Several experiments are extensively conducted to evaluate our method on the angle-closure glaucoma evaluation (AGE) Challenge dataset. The results show that our proposed architecture ranks the first place of the classification task on the test dataset and achieves the average Euclidean distance error of 12.00 pixels in the scleral spur localization task.



### Divide-and-Conquer Adversarial Learning for High-Resolution Image and Video Enhancement
- **Arxiv ID**: http://arxiv.org/abs/1910.10455v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10455v1)
- **Published**: 2019-10-23 11:00:51+00:00
- **Updated**: 2019-10-23 11:00:51+00:00
- **Authors**: Zhiwu Huang, Danda Pani Paudel, Guanju Li, Jiqing Wu, Radu Timofte, Luc Van Gool
- **Comment**: None
- **Journal**: None
- **Summary**: This paper introduces a divide-and-conquer inspired adversarial learning (DACAL) approach for photo enhancement. The key idea is to decompose the photo enhancement process into hierarchically multiple sub-problems, which can be better conquered from bottom to up. On the top level, we propose a perception-based division to learn additive and multiplicative components, required to translate a low-quality image or video into its high-quality counterpart. On the intermediate level, we use a frequency-based division with generative adversarial network (GAN) to weakly supervise the photo enhancement process. On the lower level, we design a dimension-based division that enables the GAN model to better approximates the distribution distance on multiple independent one-dimensional data to train the GAN model. While considering all three hierarchies, we develop multiscale and recurrent training approaches to optimize the image and video enhancement process in a weakly-supervised manner. Both quantitative and qualitative results clearly demonstrate that the proposed DACAL achieves the state-of-the-art performance for high-resolution image and video enhancement.



### Deep Learning Supersampled Scanning Transmission Electron Microscopy
- **Arxiv ID**: http://arxiv.org/abs/1910.10467v2
- **DOI**: None
- **Categories**: **eess.IV**, cond-mat.mtrl-sci, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10467v2)
- **Published**: 2019-10-23 11:30:25+00:00
- **Updated**: 2019-10-25 09:39:05+00:00
- **Authors**: Jeffrey M. Ede
- **Comment**: 19 pages, 21 figures, 3 tables
- **Journal**: None
- **Summary**: Compressed sensing can increase resolution, and decrease electron dose and scan time of electron microscope point-scan systems with minimal information loss. Building on a history of successful deep learning applications in compressed sensing, we have developed a two-stage multiscale generative adversarial network to supersample scanning transmission electron micrographs with point-scan coverage reduced to 1/16, 1/25, ..., 1/100 px. We propose a novel non-adversarial learning policy to train a unified generator for multiple coverages and introduce an auxiliary network to homogenize prioritization of training data with varied signal-to-noise ratios. This achieves root mean square errors of 3.23% and 4.54% at 1/16 px and 1/100 px coverage, respectively; within 1% of errors for networks trained for each coverage individually. Detailed error distributions are presented for unified and individual coverage generators, including errors per output pixel. In addition, we present a baseline one-stage network for a single coverage and investigate numerical precision for web serving. Source code, training data, and pretrained models are publicly available at https://github.com/Jeffrey-Ede/DLSS-STEM



### An Analytical Lidar Sensor Model Based on Ray Path Information
- **Arxiv ID**: http://arxiv.org/abs/1910.10469v1
- **DOI**: 10.1109/LRA.2017.2669376
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10469v1)
- **Published**: 2019-10-23 11:32:22+00:00
- **Updated**: 2019-10-23 11:32:22+00:00
- **Authors**: Alexander Schaefer, Lukas Luft, Wolfram Burgard
- **Comment**: 8 pages
- **Journal**: IEEE Robotics and Automation Letters (Volume: 2, Issue: 3, July
  2017)
- **Summary**: Two core competencies of a mobile robot are to build a map of the environment and to estimate its own pose on the basis of this map and incoming sensor readings. To account for the uncertainties in this process, one typically employs probabilistic state estimation approaches combined with a model of the specific sensor. Over the past years, lidar sensors have become a popular choice for mapping and localization. However, many common lidar models perform poorly in unstructured, unpredictable environments, they lack a consistent physical model for both mapping and localization, and they do not exploit all the information the sensor provides, e.g. out-of-range measurements. In this paper, we introduce a consistent physical model that can be applied to mapping as well as to localization. It naturally deals with unstructured environments and makes use of both out-of-range measurements and information about the ray path. The approach can be seen as a generalization of the well-established reflection model, but in addition to counting ray reflections and traversals in a specific map cell, it considers the distances that all rays travel inside this cell. We prove that the resulting map maximizes the data likelihood and demonstrate that our model outperforms state-of-the-art sensor models in extensive real-world experiments.



### A Hierarchical Mixture Density Network
- **Arxiv ID**: http://arxiv.org/abs/1910.13523v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1910.13523v1)
- **Published**: 2019-10-23 11:33:07+00:00
- **Updated**: 2019-10-23 11:33:07+00:00
- **Authors**: Fan Yang, Jaymar Soriano, Takatomi Kubo, Kazushi Ikeda
- **Comment**: 8 pages, 5 figures, conference
- **Journal**: The 24th International Conference on Neural Information
  Processing, 2017
- **Summary**: The relationship among three correlated variables could be very sophisticated, as a result, we may not be able to find their hidden causality and model their relationship explicitly. However, we still can make our best guess for possible mappings among these variables, based on the observed relationship. One of the complicated relationships among three correlated variables could be a two-layer hierarchical many-to-many mapping. In this paper, we proposed a Hierarchical Mixture Density Network (HMDN) to model the two-layer hierarchical many-to-many mapping. We apply HMDN on an indoor positioning problem and show its benefit.



### Neural Ordinary Differential Equations for Semantic Segmentation of Individual Colon Glands
- **Arxiv ID**: http://arxiv.org/abs/1910.10470v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10470v1)
- **Published**: 2019-10-23 11:35:19+00:00
- **Updated**: 2019-10-23 11:35:19+00:00
- **Authors**: Hans Pinckaers, Geert Litjens
- **Comment**: Accepted to 'Medical Imaging meets NeurIPS' workshop at NeurIPS 2019.
  Source code available at:
  https://github.com/DIAGNijmegen/neural-odes-segmentation
- **Journal**: None
- **Summary**: Automated medical image segmentation plays a key role in quantitative research and diagnostics. Convolutional neural networks based on the U-Net architecture are the state-of-the-art. A key disadvantage is the hard-coding of the receptive field size, which requires architecture optimization for each segmentation task. Furthermore, increasing the receptive field results in an increasing number of weights. Recently, Neural Ordinary Differential Equations (NODE) have been proposed, a new type of continuous depth deep neural network. This framework allows for a dynamic receptive field at a fixed memory cost and a smaller amount of parameters. We show on a colon gland segmentation dataset (GlaS) that these NODEs can be used within the U-Net framework to improve segmentation results while reducing memory load and parameter counts.



### Closed-Form Full Map Posteriors for Robot Localization with Lidar Sensors
- **Arxiv ID**: http://arxiv.org/abs/1910.10493v1
- **DOI**: 10.1109/IROS.2017.8206583
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10493v1)
- **Published**: 2019-10-23 11:53:06+00:00
- **Updated**: 2019-10-23 11:53:06+00:00
- **Authors**: Lukas Luft, Alexander Schaefer, Tobias Schubert, Wolfram Burgard
- **Comment**: 7 pages
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems, Vancouver, BC, 2017, pp. 6678-6684
- **Summary**: A popular class of lidar-based grid mapping algorithms computes for each map cell the probability that it reflects an incident laser beam. These algorithms typically determine the map as the set of reflection probabilities that maximizes the likelihood of the underlying laser data and do not compute the full posterior distribution over all possible maps. Thereby, they discard crucial information about the confidence of the estimate. The approach presented in this paper preserves this information by determining the full map posterior. In general, this problem is hard because distributions over real-valued quantities can possess infinitely many dimensions. However, for two state-of-the-art beam-based lidar models, our approach yields closed-form map posteriors that possess only two parameters per cell. Even better, these posteriors come for free, in the sense that they use the same parameters as the traditional approaches, without the need for additional computations. An important use case for grid maps is robot localization, which we formulate as Bayesian filtering based on the closed-form map posterior rather than based on a single map. The resulting measurement likelihoods can also be expressed in closed form. In simulations and extensive real-world experiments, we show that leveraging the full map posterior improves the localization accuracy compared to approaches that use the most likely map.



### DCT Maps: Compact Differentiable Lidar Maps Based on the Cosine Transform
- **Arxiv ID**: http://arxiv.org/abs/1910.11147v1
- **DOI**: 10.1109/LRA.2018.2794602
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11147v1)
- **Published**: 2019-10-23 12:07:12+00:00
- **Updated**: 2019-10-23 12:07:12+00:00
- **Authors**: Alexander Schaefer, Lukas Luft, Wolfram Burgard
- **Comment**: 8 pages
- **Journal**: IEEE Robotics and Automation Letters (Volume: 3, Issue: 2, April
  2018)
- **Summary**: Most robot mapping techniques for lidar sensors tessellate the environment into pixels or voxels and assume uniformity of the environment within them. Although intuitive, this representation entails disadvantages: The resulting grid maps exhibit aliasing effects and are not differentiable. In the present paper, we address these drawbacks by introducing a novel mapping technique that does neither rely on tessellation nor on the assumption of piecewise uniformity of the space, without increasing memory requirements. Instead of representing the map in the position domain, we store the map parameters in the discrete frequency domain and leverage the continuous extension of the inverse discrete cosine transform to convert them to a continuously differentiable scalar field in the position domain, which we call DCT map. A DCT map assigns to each point in space a lidar decay rate, which models the local permeability of the space for laser rays. In this way, the map can describe objects of different laser permeabilities, from completely opaque to completely transparent. DCT maps represent lidar measurements significantly more accurate than grid maps, Gaussian process occupancy maps, and Hilbert maps, all with the same memory requirements, as demonstrated in our real-world experiments.



### Semantic Segmentation of Skin Lesions using a Small Data Set
- **Arxiv ID**: http://arxiv.org/abs/1910.10534v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/1910.10534v1)
- **Published**: 2019-10-23 12:54:35+00:00
- **Updated**: 2019-10-23 12:54:35+00:00
- **Authors**: Beril Sirmacek, Max Kivits
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: Early detection of melanoma is difficult for the human eye but a crucial step towards reducing its death rate. Computerized detection of these melanoma and other skin lesions is necessary. The central research question in this paper is "How to segment skin lesion images using a neural network with low available data?". This question is divided into three sub questions regarding best performing network structure, training data and training method. First theory associated with these questions is discussed. Literature states that U-net CNN structures have excellent performances on the segmentation task, more training data increases network performance and utilizing transfer learning enables networks to generalize to new data better.   To validate these findings in the literature two experiments are conducted. The first experiment trains a network on data sets of different size. The second experiment proposes twelve network structures and trains them on the same data set. The experimental results support the findings in the literature. The FCN16 and FCN32 networks perform best in the accuracy, intersection over union and mean BF1 Score metric. Concluding from these results the skin lesion segmentation network is a fully convolutional structure with a skip architecture and an encoder depth of either one or two. Weights of this network should be initialized using transfer learning from the pre trained VGG16 network. Training data should be cropped to reduce complexity and augmented during training to reduce the likelihood of overfitting.



### A Maximum Likelihood Approach to Extract Finite Planes from 3-D Laser Scans
- **Arxiv ID**: http://arxiv.org/abs/1910.11146v1
- **DOI**: 10.1109/ICRA.2019.8794318
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11146v1)
- **Published**: 2019-10-23 13:06:34+00:00
- **Updated**: 2019-10-23 13:06:34+00:00
- **Authors**: Alexander Schaefer, Johan Vertens, Daniel Büscher, Wolfram Burgard
- **Comment**: None
- **Journal**: International Conference on Robotics and Automation , Montreal,
  QC, Canada, 2019, pp. 72-78
- **Summary**: Whether it is object detection, model reconstruction, laser odometry, or point cloud registration: Plane extraction is a vital component of many robotic systems. In this paper, we propose a strictly probabilistic method to detect finite planes in organized 3-D laser range scans. An agglomerative hierarchical clustering technique, our algorithm builds planes from bottom up, always extending a plane by the point that decreases the measurement likelihood of the scan the least. In contrast to most related methods, which rely on heuristics like orthogonal point-to-plane distance, we leverage the ray path information to compute the measurement likelihood. We evaluate our approach not only on the popular SegComp benchmark, but also provide a challenging synthetic dataset that overcomes SegComp's deficiencies. Both our implementation and the suggested dataset are available at www.github.com/acschaefer/ppe.



### Deep generative model-driven multimodal prostate segmentation in radiotherapy
- **Arxiv ID**: http://arxiv.org/abs/1910.10542v1
- **DOI**: 10.1007/978-3-030-32486-5_15
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10542v1)
- **Published**: 2019-10-23 13:08:12+00:00
- **Updated**: 2019-10-23 13:08:12+00:00
- **Authors**: Kibrom Berihu Girum, Gilles Créhange, Raabid Hussain, Paul Michael Walker, Alain Lalande
- **Comment**: 8 pages, camera ready paper, accepted for Artificial Intelligence in
  Radiation Therapy (AIRT), in conjunction with MICCAI 2019
- **Journal**: None
- **Summary**: Deep learning has shown unprecedented success in a variety of applications, such as computer vision and medical image analysis. However, there is still potential to improve segmentation in multimodal images by embedding prior knowledge via learning-based shape modeling and registration to learn the modality invariant anatomical structure of organs. For example, in radiotherapy automatic prostate segmentation is essential in prostate cancer diagnosis, therapy, and post-therapy assessment from T2-weighted MR or CT images. In this paper, we present a fully automatic deep generative model-driven multimodal prostate segmentation method using convolutional neural network (DGMNet). The novelty of our method comes with its embedded generative neural network for learning-based shape modeling and its ability to adapt for different imaging modalities via learning-based registration. The proposed method includes a multi-task learning framework that combines a convolutional feature extraction and an embedded regression and classification based shape modeling. This enables the network to predict the deformable shape of an organ. We show that generative neural networkbased shape modeling trained on a reliable contrast imaging modality (such as MRI) can be directly applied to low contrast imaging modality (such as CT) to achieve accurate prostate segmentation. The method was evaluated on MRI and CT datasets acquired from different clinical centers with large variations in contrast and scanning protocols. Experimental results reveal that our method can be used to automatically and accurately segment the prostate gland in different imaging modalities.



### A Deep Learning based Pipeline for Efficient Oral Cancer Screening on Whole Slide Images
- **Arxiv ID**: http://arxiv.org/abs/1910.10549v3
- **DOI**: 10.1007/978-3-030-50516-5_22
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10549v3)
- **Published**: 2019-10-23 13:16:14+00:00
- **Updated**: 2020-04-15 10:54:55+00:00
- **Authors**: Jiahao Lu, Nataša Sladoje, Christina Runow Stark, Eva Darai Ramqvist, Jan-Michaél Hirsch, Joakim Lindblad
- **Comment**: Accepted to ICIAR 2020
- **Journal**: In Proceedings of the 17th International Conference on Image
  Analysis and Recognition (ICIAR), LNCS-12132, pp. 249-261, P\'ovoa de Varzim,
  Portugal, June 2020
- **Summary**: Oral cancer incidence is rapidly increasing worldwide. The most important determinant factor in cancer survival is early diagnosis. To facilitate large scale screening, we propose a fully automated pipeline for oral cancer detection on whole slide cytology images. The pipeline consists of fully convolutional regression-based nucleus detection, followed by per-cell focus selection, and CNN based classification. Our novel focus selection step provides fast per-cell focus decisions at human-level accuracy. We demonstrate that the pipeline provides efficient cancer classification of whole slide cytology images, improving over previous results both in terms of accuracy and feasibility. The complete source code is available at https://github.com/MIDA-group/OralScreen.



### Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans
- **Arxiv ID**: http://arxiv.org/abs/1910.10550v1
- **DOI**: 10.1109/ECMR.2019.8870928
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.10550v1)
- **Published**: 2019-10-23 13:21:05+00:00
- **Updated**: 2019-10-23 13:21:05+00:00
- **Authors**: Alexander Schaefer, Daniel Büscher, Johan Vertens, Lukas Luft, Wolfram Burgard
- **Comment**: 9 pages
- **Journal**: European Conference on Mobile Robots, Prague, Czech Republic,
  2019, pp. 1-7
- **Summary**: Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation at www.github.com/acschaefer/polex. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15~months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.



### Domain Bridge for Unpaired Image-to-Image Translation and Unsupervised Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1910.10563v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10563v3)
- **Published**: 2019-10-23 13:51:40+00:00
- **Updated**: 2020-03-14 16:18:04+00:00
- **Authors**: Fabio Pizzati, Raoul de Charette, Michela Zaccaria, Pietro Cerri
- **Comment**: WACV 20 camera ready
- **Journal**: None
- **Summary**: Image-to-image translation architectures may have limited effectiveness in some circumstances. For example, while generating rainy scenarios, they may fail to model typical traits of rain as water drops, and this ultimately impacts the synthetic images realism. With our method, called domain bridge, web-crawled data are exploited to reduce the domain gap, leading to the inclusion of previously ignored elements in the generated images. We make use of a network for clear to rain translation trained with the domain bridge to extend our work to Unsupervised Domain Adaptation (UDA). In that context, we introduce an online multimodal style-sampling strategy, where image translation multimodality is exploited at training time to improve performances. Finally, a novel approach for self-supervised learning is presented, and used to further align the domains. With our contributions, we simultaneously increase the realism of the generated images, while reaching on par performances with respect to the UDA state-of-the-art, with a simpler approach.



### Learning Priors in High-frequency Domain for Inverse Imaging Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.11148v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11148v1)
- **Published**: 2019-10-23 14:15:42+00:00
- **Updated**: 2019-10-23 14:15:42+00:00
- **Authors**: Zhuonan He, Jinjie Zhou, Dong Liang, Yuhao Wang, Qiegen Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Ill-posed inverse problems in imaging remain an active research topic in several decades, with new approaches constantly emerging. Recognizing that the popular dictionary learning and convolutional sparse coding are both essentially modeling the high-frequency component of an image, which convey most of the semantic information such as texture details, in this work we propose a novel multi-profile high-frequency transform-guided denoising autoencoder as prior (HF-DAEP). To achieve this goal, we first extract a set of multi-profile high-frequency components via a specific transformation and add the artificial Gaussian noise to these high-frequency components as training samples. Then, as the high-frequency prior information is learned, we incorporate it into classical iterative reconstruction process by proximal gradient descent technique. Preliminary results on highly under-sampled magnetic resonance imaging and sparse-view computed tomography reconstruction demonstrate that the proposed method can efficiently reconstruct feature details and present advantages over state-of-the-arts.



### Autoencoding with a Classifier System
- **Arxiv ID**: http://arxiv.org/abs/1910.10579v8
- **DOI**: 10.1109/TEVC.2021.3079320
- **Categories**: **cs.NE**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10579v8)
- **Published**: 2019-10-23 14:27:29+00:00
- **Updated**: 2021-05-12 13:20:17+00:00
- **Authors**: Richard J. Preen, Stewart W. Wilson, Larry Bull
- **Comment**: None
- **Journal**: IEEE Transactions on Evolutionary Computation (2021)
- **Summary**: Autoencoders are data-specific compression algorithms learned automatically from examples. The predominant approach has been to construct single large global models that cover the domain. However, training and evaluating models of increasing size comes at the price of additional time and computational cost. Conditional computation, sparsity, and model pruning techniques can reduce these costs while maintaining performance. Learning classifier systems (LCS) are a framework for adaptively subdividing input spaces into an ensemble of simpler local approximations that together cover the domain. LCS perform conditional computation through the use of a population of individual gating/guarding components, each associated with a local approximation. This article explores the use of an LCS to adaptively decompose the input domain into a collection of small autoencoders where local solutions of different complexity may emerge. In addition to benefits in convergence time and computational cost, it is shown possible to reduce code size as well as the resulting decoder computational cost when compared with the global model equivalent.



### SalGaze: Personalizing Gaze Estimation Using Visual Saliency
- **Arxiv ID**: http://arxiv.org/abs/1910.10603v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10603v1)
- **Published**: 2019-10-23 15:11:08+00:00
- **Updated**: 2019-10-23 15:11:08+00:00
- **Authors**: Zhuoqing Chang, Matias Di Martino, Qiang Qiu, Steven Espinosa, Guillermo Sapiro
- **Comment**: Accepted by ICCV 2019 Workshop
- **Journal**: None
- **Summary**: Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods.



### Speech Emotion Recognition via Contrastive Loss under Siamese Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.11174v1
- **DOI**: 10.1145/3267935.3267946
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1910.11174v1)
- **Published**: 2019-10-23 15:43:42+00:00
- **Updated**: 2019-10-23 15:43:42+00:00
- **Authors**: Zheng Lian, Ya Li, Jianhua Tao, Jian Huang
- **Comment**: ASMMC-MMAC 2018 Proceedings of the Joint Workshop of the 4th Workshop
  on Affective Social Multimedia Computing and first Multi-Modal Affective
  Computing of Large-Scale Multimedia Data
- **Journal**: None
- **Summary**: Speech emotion recognition is an important aspect of human-computer interaction. Prior work proposes various end-to-end models to improve the classification performance. However, most of them rely on the cross-entropy loss together with softmax as the supervision component, which does not explicitly encourage discriminative learning of features. In this paper, we introduce the contrastive loss function to encourage intra-class compactness and inter-class separability between learnable features. Furthermore, multiple feature selection methods and pairwise sample selection methods are evaluated. To verify the performance of the proposed system, we conduct experiments on The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database, a common evaluation corpus. Experimental results reveal the advantages of the proposed method, which reaches 62.19% in the weighted accuracy and 63.21% in the unweighted accuracy. It outperforms the baseline system that is optimized without the contrastive loss function with 1.14% and 2.55% in the weighted accuracy and the unweighted accuracy, respectively.



### Expression Analysis Based on Face Regions in Read-world Conditions
- **Arxiv ID**: http://arxiv.org/abs/1911.05188v1
- **DOI**: 10.1007/s11633-019-1176-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.05188v1)
- **Published**: 2019-10-23 16:04:21+00:00
- **Updated**: 2019-10-23 16:04:21+00:00
- **Authors**: Zheng Lian, Ya Li, Jian-Hua Tao, Jian Huang, Ming-Yue Niu
- **Comment**: International Journal of Automation and Computing 2018
- **Journal**: None
- **Summary**: Facial emotion recognition is an essential and important aspect of the field of human-machine interaction. Past research on facial emotion recognition focuses on the laboratory environment. However, it faces many challenges in real-world conditions, i.e., illumination changes, large pose variations and partial or full occlusions. Those challenges lead to different face areas with different degrees of sharpness and completeness. Inspired by this fact, we focus on the authenticity of predictions generated by different <emotion, region> pairs. For example, if only the mouth areas are available and the emotion classifier predicts happiness, then there is a question of how to judge the authenticity of predictions. This problem can be converted into the contribution of different face areas to different emotions. In this paper, we divide the whole face into six areas: nose areas, mouth areas, eyes areas, nose to mouth areas, nose to eyes areas and mouth to eyes areas. To obtain more convincing results, our experiments are conducted on three different databases: facial expression recognition + ( FER+), real-world affective faces database (RAF-DB) and expression in-the-wild (ExpW) dataset. Through analysis of the classification accuracy, the confusion matrix and the class activation map (CAM), we can establish convincing results. To sum up, the contributions of this paper lie in two areas: 1) We visualize concerned areas of human faces in emotion recognition; 2) We analyze the contribution of different face areas to different emotions in real-world conditions through experimental analysis. Our findings can be combined with findings in psychology to promote the understanding of emotional expressions.



### Occlusions for Effective Data Augmentation in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.10651v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10651v2)
- **Published**: 2019-10-23 16:19:22+00:00
- **Updated**: 2019-10-25 15:25:57+00:00
- **Authors**: Ruth Fong, Andrea Vedaldi
- **Comment**: Accepted to 2019 ICCV Workshop on Interpreting and Explaining Visual
  Artificial Intelligence Models (v2: corrected references)
- **Journal**: None
- **Summary**: Deep networks for visual recognition are known to leverage "easy to recognise" portions of objects such as faces and distinctive texture patterns. The lack of a holistic understanding of objects may increase fragility and overfitting. In recent years, several papers have proposed to address this issue by means of occlusions as a form of data augmentation. However, successes have been limited to tasks such as weak localization and model interpretation, but no benefit was demonstrated on image classification on large-scale datasets. In this paper, we show that, by using a simple technique based on batch augmentation, occlusions as data augmentation can result in better performance on ImageNet for high-capacity models (e.g., ResNet50). We also show that varying amounts of occlusions used during training can be used to study the robustness of different neural network architectures.



### Breast Anatomy Enriched Tumor Saliency Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.10652v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10652v1)
- **Published**: 2019-10-23 16:19:26+00:00
- **Updated**: 2019-10-23 16:19:26+00:00
- **Authors**: Fei Xu, Yingtao Zhang, Min Xian, H. D. Cheng, Boyu Zhang, Jianrui Ding, Chunping Ning, Ying Wang
- **Comment**: 4 pages, 6 figures
- **Journal**: None
- **Summary**: Breast cancer investigation is of great significance, and developing tumor detection methodologies is a critical need. However, it is a challenging task for breast ultrasound due to the complicated breast structure and poor quality of the images. In this paper, we propose a novel tumor saliency estimation model guided by enriched breast anatomy knowledge to localize the tumor. Firstly, the breast anatomy layers are generated by a deep neural network. Then we refine the layers by integrating a non-semantic breast anatomy model to solve the problems of incomplete mammary layers. Meanwhile, a new background map generation method weighted by the semantic probability and spatial distance is proposed to improve the performance. The experiment demonstrates that the proposed method with the new background map outperforms four state-of-the-art TSE models with increasing 10% of F_meansure on the BUS public dataset.



### Accurate 6D Object Pose Estimation by Pose Conditioned Mesh Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.10653v1
- **DOI**: 10.1109/ICASSP40776.2020.9053627
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.10653v1)
- **Published**: 2019-10-23 16:23:12+00:00
- **Updated**: 2019-10-23 16:23:12+00:00
- **Authors**: Pedro Castro, Anil Armagan, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: Current 6D object pose methods consist of deep CNN models fully optimized for a single object but with its architecture standardized among objects with different shapes. In contrast to previous works, we explicitly exploit each object's distinct topological information i.e. 3D dense meshes in the pose estimation model, with an automated process and prior to any post-processing refinement stage. In order to achieve this, we propose a learning framework in which a Graph Convolutional Neural Network reconstructs a pose conditioned 3D mesh of the object. A robust estimation of the allocentric orientation is recovered by computing, in a differentiable manner, the Procrustes' alignment between the canonical and reconstructed dense 3D meshes. 6D egocentric pose is then lifted using additional mask and 2D centroid projection estimations. Our method is capable of self validating its pose estimation by measuring the quality of the reconstructed mesh, which is invaluable in real life applications. In our experiments on the LINEMOD, OCCLUSION and YCB-Video benchmarks, the proposed method outperforms state-of-the-arts.



### gradSLAM: Automagically differentiable SLAM
- **Arxiv ID**: http://arxiv.org/abs/1910.10672v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10672v3)
- **Published**: 2019-10-23 17:13:41+00:00
- **Updated**: 2020-11-19 18:53:59+00:00
- **Authors**: Krishna Murthy Jatavallabhula, Soroush Saryazdi, Ganesh Iyer, Liam Paull
- **Comment**: Video: https://youtu.be/2ygtSJTmo08 . Project page and code:
  https://gradslam.github.io This tech report is an extended version of the
  ICRA 2020 paper "gradSLAM: Dense SLAM meets automatic differentiation". The
  first two authors contributed equally
- **Journal**: None
- **Summary**: Blending representation learning approaches with simultaneous localization and mapping (SLAM) systems is an open question, because of their highly modular and complex nature. Functionally, SLAM is an operation that transforms raw sensor inputs into a distribution over the state(s) of the robot and the environment. If this transformation (SLAM) were expressible as a differentiable function, we could leverage task-based error signals to learn representations that optimize task performance. However, several components of a typical dense SLAM system are non-differentiable. In this work, we propose gradSLAM, a methodology for posing SLAM systems as differentiable computational graphs, which unifies gradient-based learning and SLAM. We propose differentiable trust-region optimizers, surface measurement and fusion schemes, and raycasting, without sacrificing accuracy. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.   TL;DR: We leverage the power of automatic differentiation frameworks to make dense SLAM differentiable.



### A Useful Taxonomy for Adversarial Robustness of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.10679v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10679v1)
- **Published**: 2019-10-23 17:33:15+00:00
- **Updated**: 2019-10-23 17:33:15+00:00
- **Authors**: Leslie N. Smith
- **Comment**: NRL Technical Report
- **Journal**: None
- **Summary**: Adversarial attacks and defenses are currently active areas of research for the deep learning community. A recent review paper divided the defense approaches into three categories; gradient masking, robust optimization, and adversarial example detection. We divide gradient masking and robust optimization differently: (1) increasing intra-class compactness and inter-class separation of the feature vectors improves adversarial robustness, and (2) marginalization or removal of non-robust image features also improves adversarial robustness. By reframing these topics differently, we provide a fresh perspective that provides insight into the underlying factors that enable training more robust networks and can help inspire novel solutions. In addition, there are several papers in the literature of adversarial defenses that claim there is a cost for adversarial robustness, or a trade-off between robustness and accuracy but, under this proposed taxonomy, we hypothesis that this is not universal. We follow up on our taxonomy with several challenges to the deep learning research community that builds on the connections and insights in this paper.



### Contrastive Representation Distillation
- **Arxiv ID**: http://arxiv.org/abs/1910.10699v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10699v3)
- **Published**: 2019-10-23 17:59:18+00:00
- **Updated**: 2022-01-24 19:12:34+00:00
- **Authors**: Yonglong Tian, Dilip Krishnan, Phillip Isola
- **Comment**: ICLR 2020. Project Page: http://hobbitlong.github.io/CRD/, Code:
  http://github.com/HobbitLong/RepDistiller. Typo fixed in the newest version
- **Journal**: None
- **Summary**: Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.



### Contextual Imagined Goals for Self-Supervised Robotic Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11670v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11670v1)
- **Published**: 2019-10-23 18:00:18+00:00
- **Updated**: 2019-10-23 18:00:18+00:00
- **Authors**: Ashvin Nair, Shikhar Bahl, Alexander Khazatsky, Vitchyr Pong, Glen Berseth, Sergey Levine
- **Comment**: 12 pages, to be presented at Conference on Robot Learning (CoRL)
  2019. Project website: https://ccrig.github.io/
- **Journal**: None
- **Summary**: While reinforcement learning provides an appealing formalism for learning individual skills, a general-purpose robotic system must be able to master an extensive repertoire of behaviors. Instead of learning a large collection of skills individually, can we instead enable a robot to propose and practice its own behaviors automatically, learning about the affordances and behaviors that it can perform in its environment, such that it can then repurpose this knowledge once a new task is commanded by the user? In this paper, we study this question in the context of self-supervised goal-conditioned reinforcement learning. A central challenge in this learning regime is the problem of goal setting: in order to practice useful skills, the robot must be able to autonomously set goals that are feasible but diverse. When the robot's environment and available objects vary, as they do in most open-world settings, the robot must propose to itself only those goals that it can accomplish in its present setting with the objects that are at hand. Previous work only studies self-supervised goal-conditioned RL in a single-environment setting, where goal proposals come from the robot's past experience or a generative model are sufficient. In more diverse settings, this frequently leads to impossible goals and, as we show experimentally, prevents effective learning. We propose a conditional goal-setting model that aims to propose goals that are feasible from the robot's current state. We demonstrate that this enables self-supervised goal-conditioned off-policy learning with raw image observations in the real world, enabling a robot to manipulate a variety of objects and generalize to new objects that were not seen during training.



### 6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints
- **Arxiv ID**: http://arxiv.org/abs/1910.10750v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.10750v1)
- **Published**: 2019-10-23 18:16:53+00:00
- **Updated**: 2019-10-23 18:16:53+00:00
- **Authors**: Chen Wang, Roberto Martín-Martín, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, Yuke Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real-time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.



### We Know Where We Don't Know: 3D Bayesian CNNs for Credible Geometric Uncertainty
- **Arxiv ID**: http://arxiv.org/abs/1910.10793v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.10793v2)
- **Published**: 2019-10-23 20:07:24+00:00
- **Updated**: 2020-04-02 00:35:12+00:00
- **Authors**: Tyler LaBonte, Carianne Martinez, Scott A. Roberts
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Deep learning has been successfully applied to the segmentation of 3D Computed Tomography (CT) scans. Establishing the credibility of these segmentations requires uncertainty quantification (UQ) to identify untrustworthy predictions. Recent UQ architectures include Monte Carlo dropout networks (MCDNs), which approximate deep Gaussian processes, and Bayesian neural networks (BNNs), which learn the distribution of the weight space. BNNs are advantageous over MCDNs for UQ but are thought to be computationally infeasible in high dimension, and neither architecture has produced interpretable geometric uncertainty maps. We propose a novel 3D Bayesian convolutional neural network (BCNN), the first deep learning method which generates statistically credible geometric uncertainty maps and scales for application to 3D data. We present experimental results on CT scans of graphite electrodes and laser-welded metals and show that our BCNN outperforms an MCDN in recent uncertainty metrics. The geometric uncertainty maps generated by our BCNN capture distributions of sigmoid values that are interpretable as confidence intervals, critical for applications that rely on deep learning for high-consequence decisions. Code available at https://github.com/sandialabs/bcnn.



### Low Shot Learning with Untrained Neural Networks for Imaging Inverse Problems
- **Arxiv ID**: http://arxiv.org/abs/1910.10797v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.10797v1)
- **Published**: 2019-10-23 20:23:22+00:00
- **Updated**: 2019-10-23 20:23:22+00:00
- **Authors**: Oscar Leong, Wesam Sakla
- **Comment**: Deep Inverse NeurIPS 2019 Workshop
- **Journal**: None
- **Summary**: Employing deep neural networks as natural image priors to solve inverse problems either requires large amounts of data to sufficiently train expressive generative models or can succeed with no data via untrained neural networks. However, very few works have considered how to interpolate between these no- to high-data regimes. In particular, how can one use the availability of a small amount of data (even $5-25$ examples) to one's advantage in solving these inverse problems and can a system's performance increase as the amount of data increases as well? In this work, we consider solving linear inverse problems when given a small number of examples of images that are drawn from the same distribution as the image of interest. Comparing to untrained neural networks that use no data, we show how one can pre-train a neural network with a few given examples to improve reconstruction results in compressed sensing and semantic image recovery problems such as colorization. Our approach leads to improved reconstruction as the amount of available data increases and is on par with fully trained generative models, while requiring less than $1 \%$ of the data needed to train a generative model.



### Wasserstein total variation filtering
- **Arxiv ID**: http://arxiv.org/abs/1910.10822v1
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV, stat.AP
- **Links**: [PDF](http://arxiv.org/pdf/1910.10822v1)
- **Published**: 2019-10-23 22:03:53+00:00
- **Updated**: 2019-10-23 22:03:53+00:00
- **Authors**: Erdem Varol, Amin Nejatbakhsh
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we expand upon the theory of trend filtering by introducing the use of the Wasserstein metric as a means to control the amount of spatiotemporal variation in filtered time series data. While trend filtering utilizes regularization to produce signal estimates that are piecewise linear, in the case of $\ell_1$ regularization, or temporally smooth, in the case of $\ell_2$ regularization, it ignores the topology of the spatial distribution of signal. By incorporating the information about the underlying metric space of the pixel layout, the Wasserstein metric is an attractive choice as a regularizer to undercover spatiotemporal trends in time series data. We introduce a globally optimal algorithm for efficiently estimating the filtered signal under a Wasserstein finite differences operator. The efficacy of the proposed algorithm in preserving spatiotemporal trends in time series video is demonstrated in both simulated and fluorescent microscopy videos of the nematode caenorhabditis elegans and compared against standard trend filtering algorithms.



### Semi-Supervised Histology Classification using Deep Multiple Instance Learning and Contrastive Predictive Coding
- **Arxiv ID**: http://arxiv.org/abs/1910.10825v3
- **DOI**: None
- **Categories**: **cs.CV**, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1910.10825v3)
- **Published**: 2019-10-23 22:12:57+00:00
- **Updated**: 2019-11-02 16:41:26+00:00
- **Authors**: Ming Y. Lu, Richard J. Chen, Jingwen Wang, Debora Dillon, Faisal Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks can be trained to perform histology slide classification using weak annotations with multiple instance learning (MIL). However, given the paucity of labeled histology data, direct application of MIL can easily suffer from overfitting and the network is unable to learn rich feature representations due to the weak supervisory signal. We propose to overcome such limitations with a two-stage semi-supervised approach that combines the power of data-efficient self-supervised feature learning via contrastive predictive coding (CPC) and the interpretability and flexibility of regularized attention-based MIL. We apply our two-stage CPC + MIL semi-supervised pipeline to the binary classification of breast cancer histology images. Across five random splits, we report state-of-the-art performance with a mean validation accuracy of 95% and an area under the ROC curve of 0.968. We further evaluate the quality of features learned via CPC relative to simple transfer learning and show that strong classification performance using CPC features can be efficiently leveraged under the MIL framework even with the feature encoder frozen.



