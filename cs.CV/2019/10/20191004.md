# Arxiv Papers in cs.CV on 2019-10-04
### NeurReg: Neural Registration and Its Application to Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.01763v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.01763v1)
- **Published**: 2019-10-04 00:07:22+00:00
- **Updated**: 2019-10-04 00:07:22+00:00
- **Authors**: Wentao Zhu, Andriy Myronenko, Ziyue Xu, Wenqi Li, Holger Roth, Yufang Huang, Fausto Milletari, Daguang Xu
- **Comment**: WACV 2020 first round early accept; supplementary
  https://drive.google.com/file/d/1kzTLQn8cpoQNAYWUDJMtN5HcqhbWbl7G/view?usp=sharing;
  code will be released soon under NVIDIA open source; demos
  https://www.youtube.com/watch?v=GYLD7t7dSAg&t=3s
- **Journal**: None
- **Summary**: Registration is a fundamental task in medical image analysis which can be applied to several tasks including image segmentation, intra-operative tracking, multi-modal image alignment, and motion analysis. Popular registration tools such as ANTs and NiftyReg optimize an objective function for each pair of images from scratch which is time-consuming for large images with complicated deformation. Facilitated by the rapid progress of deep learning, learning-based approaches such as VoxelMorph have been emerging for image registration. These approaches can achieve competitive performance in a fraction of a second on advanced GPUs. In this work, we construct a neural registration framework, called NeurReg, with a hybrid loss of displacement fields and data similarity, which substantially improves the current state-of-the-art of registrations. Within the framework, we simulate various transformations by a registration simulator which generates fixed image and displacement field ground truth for training. Furthermore, we design three segmentation frameworks based on the proposed registration framework: 1) atlas-based segmentation, 2) joint learning of both segmentation and registration tasks, and 3) multi-task learning with atlas-based segmentation as an intermediate feature. Extensive experimental results validate the effectiveness of the proposed NeurReg framework based on various metrics: the endpoint error (EPE) of the predicted displacement field, mean square error (MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dice coefficient, uncertainty estimation, and the interpretability of the segmentation. The proposed NeurReg improves registration accuracy with fast inference speed, which can greatly accelerate related medical image analysis tasks.



### Two Stream Networks for Self-Supervised Ego-Motion Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.01764v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01764v3)
- **Published**: 2019-10-04 00:31:49+00:00
- **Updated**: 2019-11-19 18:10:55+00:00
- **Authors**: Rares Ambrus, Vitor Guizilini, Jie Li, Sudeep Pillai, Adrien Gaidon
- **Comment**: Conference on Robot Learning (CoRL 2019)
- **Journal**: None
- **Summary**: Learning depth and camera ego-motion from raw unlabeled RGB video streams is seeing exciting progress through self-supervision from strong geometric cues. To leverage not only appearance but also scene geometry, we propose a novel self-supervised two-stream network using RGB and inferred depth information for accurate visual odometry. In addition, we introduce a sparsity-inducing data augmentation policy for ego-motion learning that effectively regularizes the pose network to enable stronger generalization performance. As a result, we show that our proposed two-stream pose network achieves state-of-the-art results among learning-based methods on the KITTI odometry benchmark, and is especially suited for self-supervision at scale. Our experiments on a large-scale urban driving dataset of 1 million frames indicate that the performance of our proposed architecture does indeed scale progressively with more data.



### Robust Semi-Supervised Monocular Depth Estimation with Reprojected Distances
- **Arxiv ID**: http://arxiv.org/abs/1910.01765v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01765v3)
- **Published**: 2019-10-04 00:32:20+00:00
- **Updated**: 2019-11-19 17:59:41+00:00
- **Authors**: Vitor Guizilini, Jie Li, Rares Ambrus, Sudeep Pillai, Adrien Gaidon
- **Comment**: Conference on Robot Learning (CoRL 2019)
- **Journal**: None
- **Summary**: Dense depth estimation from a single image is a key problem in computer vision, with exciting applications in a multitude of robotic tasks. Initially viewed as a direct regression problem, requiring annotated labels as supervision at training time, in the past few years a substantial amount of work has been done in self-supervised depth training based on strong geometric cues, both from stereo cameras and more recently from monocular video sequences. In this paper we investigate how these two approaches (supervised & self-supervised) can be effectively combined, so that a depth model can learn to encode true scale from sparse supervision while achieving high fidelity local accuracy by leveraging geometric cues. To this end, we propose a novel supervised loss term that complements the widely used photometric loss, and show how it can be used to train robust semi-supervised monocular depth estimation models. Furthermore, we evaluate how much supervision is actually necessary to train accurate scale-aware monocular depth models, showing that with our proposed framework, very sparse LiDAR information, with as few as 4 beams (less than 100 valid depth values per image), is enough to achieve results competitive with the current state-of-the-art.



### Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing
- **Arxiv ID**: http://arxiv.org/abs/1910.05316v1
- **DOI**: None
- **Categories**: **cs.NI**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05316v1)
- **Published**: 2019-10-04 00:53:44+00:00
- **Updated**: 2019-10-04 00:53:44+00:00
- **Authors**: En Li, Liekang Zeng, Zhi Zhou, Xu Chen
- **Comment**: Accepted by IEEE Transactions on Wireless Communications, Sept 2019
- **Journal**: None
- **Summary**: As a key technology of enabling Artificial Intelligence (AI) applications in 5G era, Deep Neural Networks (DNNs) have quickly attracted widespread attention. However, it is challenging to run computation-intensive DNN-based tasks on mobile devices due to the limited computation resources. What's worse, traditional cloud-assisted DNN inference is heavily hindered by the significant wide-area network latency, leading to poor real-time performance as well as low quality of user experience. To address these challenges, in this paper, we propose Edgent, a framework that leverages edge computing for DNN collaborative inference through device-edge synergy. Edgent exploits two design knobs: (1) DNN partitioning that adaptively partitions computation between device and edge for purpose of coordinating the powerful cloud resource and the proximal edge resource for real-time DNN inference; (2) DNN right-sizing that further reduces computing latency via early exiting inference at an appropriate intermediate DNN layer. In addition, considering the potential network fluctuation in real-world deployment, Edgentis properly design to specialize for both static and dynamic network environment. Specifically, in a static environment where the bandwidth changes slowly, Edgent derives the best configurations with the assist of regression-based prediction models, while in a dynamic environment where the bandwidth varies dramatically, Edgent generates the best execution plan through the online change point detection algorithm that maps the current bandwidth state to the optimal configuration. We implement Edgent prototype based on the Raspberry Pi and the desktop PC and the extensive experimental evaluations demonstrate Edgent's effectiveness in enabling on-demand low-latency edge intelligence.



### Active Learning with Point Supervision for Cost-Effective Panicle Detection in Cereal Crops
- **Arxiv ID**: http://arxiv.org/abs/1910.01789v3
- **DOI**: 10.1186/S13007-020-00575-8
- **Categories**: **cs.CV**, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.01789v3)
- **Published**: 2019-10-04 02:56:43+00:00
- **Updated**: 2020-04-17 07:26:02+00:00
- **Authors**: Akshay L Chandra, Sai Vikas Desai, Vineeth N Balasubramanian, Seishi Ninomiya, Wei Guo
- **Comment**: Accepted as a journal paper at BMC Plant Methods (February 2020)
- **Journal**: None
- **Summary**: Panicle density of cereal crops such as wheat and sorghum is one of the main components for plant breeders and agronomists in understanding the yield of their crops. To phenotype the panicle density effectively, researchers agree there is a significant need for computer vision-based object detection techniques. Especially in recent times, research in deep learning-based object detection shows promising results in various agricultural studies. However, training such systems usually requires a lot of bounding-box labeled data. Since crops vary by both environmental and genetic conditions, acquisition of huge amount of labeled image datasets for each crop is expensive and time-consuming. Thus, to catalyze the widespread usage of automatic object detection for crop phenotyping, a cost-effective method to develop such automated systems is essential. We propose a point supervision based active learning approach for panicle detection in cereal crops. In our approach, the model constantly interacts with a human annotator by iteratively querying the labels for only the most informative images, as opposed to all images in a dataset. Our query method is specifically designed for cereal crops which usually tend to have panicles with low variance in appearance. Our method reduces labeling costs by intelligently leveraging low-cost weak labels (object centers) for picking the most informative images for which strong labels (bounding boxes) are required. We show promising results on two publicly available cereal crop datasets - Sorghum and Wheat. On Sorghum, 6 variants of our proposed method outperform the best baseline method with more than 55% savings in labeling time. Similarly, on Wheat, 3 variants of our proposed methods outperform the best baseline method with more than 50% of savings in labeling time.



### Motion Planning through Demonstration to Deal with Complex Motions in Assembly Process
- **Arxiv ID**: http://arxiv.org/abs/1910.01821v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01821v1)
- **Published**: 2019-10-04 07:25:46+00:00
- **Updated**: 2019-10-04 07:25:46+00:00
- **Authors**: Yan Wang, Kensuke Harada, Weiwei Wan
- **Comment**: 7 pages, 12 figures, 1 table, Humanoids 2019 conference accepted
  paper. Video can be found here: https://www.youtube.com/watch?v=hQxP-YVKMwc
- **Journal**: None
- **Summary**: Complex and skillful motions in actual assembly process are challenging for the robot to generate with existing motion planning approaches, because some key poses during the human assembly can be too skillful for the robot to realize automatically. In order to deal with this problem, this paper develops a motion planning method using skillful motions from demonstration, which can be applied to complete robotic assembly process including complex and skillful motions. In order to demonstrate conveniently without redundant third-party devices, we attach augmented reality (AR) markers to the manipulated object to track and capture poses of the object during the human assembly process, which are employed as key poses to execute motion planning by the planner. Derivative of every key pose serves as criterion to determine the priority of use of key poses in order to accelerate the motion planning. The effectiveness of the presented method is verified through some numerical examples and actual robot experiments.



### Few-Shot Abstract Visual Reasoning With Spectral Features
- **Arxiv ID**: http://arxiv.org/abs/1910.01833v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01833v1)
- **Published**: 2019-10-04 08:15:15+00:00
- **Updated**: 2019-10-04 08:15:15+00:00
- **Authors**: Tanner Bohn, Yining Hu, Charles X. Ling
- **Comment**: 11 pages, 3 figures
- **Journal**: None
- **Summary**: We present an image preprocessing technique capable of improving the performance of few-shot classifiers on abstract visual reasoning tasks. Many visual reasoning tasks with abstract features are easy for humans to learn with few examples but very difficult for computer vision approaches with the same number of samples, despite the ability for deep learning models to learn abstract features. Same-different (SD) problems represent a type of visual reasoning task requiring knowledge of pattern repetition within individual images, and modern computer vision approaches have largely faltered on these classification problems, even when provided with vast amounts of training data. We propose a simple method for solving these problems based on the insight that removing peaks from the amplitude spectrum of an image is capable of emphasizing the unique parts of the image. When combined with several classifiers, our method performs well on the SD SVRT tasks with few-shot learning, improving upon the best comparable results on all tasks, with average absolute accuracy increases nearly 40% for some classifiers. In particular, we find that combining Relational Networks with this image preprocessing approach improves their performance from chance-level to over 90% accuracy on several SD tasks.



### Enriching Visual with Verbal Explanations for Relational Concepts -- Combining LIME with Aleph
- **Arxiv ID**: http://arxiv.org/abs/1910.01837v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01837v1)
- **Published**: 2019-10-04 08:51:41+00:00
- **Updated**: 2019-10-04 08:51:41+00:00
- **Authors**: Johannes Rabold, Hannah Deininger, Michael Siebers, Ute Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing number of deep learning applications, there is a growing demand for explanations. Visual explanations provide information about which parts of an image are relevant for a classifier's decision. However, highlighting of image parts (e.g., an eye) cannot capture the relevance of a specific feature value for a class (e.g., that the eye is wide open). Furthermore, highlighting cannot convey whether the classification depends on the mere presence of parts or on a specific spatial relation between them. Consequently, we present an approach that is capable of explaining a classifier's decision in terms of logic rules obtained by the Inductive Logic Programming system Aleph. The examples and the background knowledge needed for Aleph are based on the explanation generation method LIME. We demonstrate our approach with images of a blocksworld domain. First, we show that our approach is capable of identifying a single relation as important explanatory construct. Afterwards, we present the more complex relational concept of towers. Finally, we show how the generated relational rules can be explicitly related with the input image, resulting in richer explanations.



### SELF: Learning to Filter Noisy Labels with Self-Ensembling
- **Arxiv ID**: http://arxiv.org/abs/1910.01842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01842v1)
- **Published**: 2019-10-04 08:59:54+00:00
- **Updated**: 2019-10-04 08:59:54+00:00
- **Authors**: Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, Thomas Brox
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures.



### Prediction of Human Full-Body Movements with Motion Optimization and Recurrent Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.01843v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01843v2)
- **Published**: 2019-10-04 09:04:56+00:00
- **Updated**: 2020-03-18 10:57:07+00:00
- **Authors**: Philipp Kratzer, Marc Toussaint, Jim Mainprice
- **Comment**: International Conference on Robotics and Automation (ICRA) 2020
- **Journal**: None
- **Summary**: Human movement prediction is difficult as humans naturally exhibit complex behaviors that can change drastically from one environment to the next. In order to alleviate this issue, we propose a prediction framework that decouples short-term prediction, linked to internal body dynamics, and long-term prediction, linked to the environment and task constraints. In this work we investigate encoding short-term dynamics in a recurrent neural network, while we account for environmental constraints, such as obstacle avoidance, using gradient-based trajectory optimization. Experiments on real motion data demonstrate that our framework improves the prediction with respect to state-of-the-art motion prediction methods, as it accounts to beforehand unseen environmental structures. Moreover we demonstrate on an example, how this framework can be used to plan robot trajectories that are optimized to coordinate with a human partner.



### EBBIOT: A Low-complexity Tracking Algorithm for Surveillance in IoVT Using Stationary Neuromorphic Vision Sensors
- **Arxiv ID**: http://arxiv.org/abs/1910.01851v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01851v1)
- **Published**: 2019-10-04 09:50:55+00:00
- **Updated**: 2019-10-04 09:50:55+00:00
- **Authors**: Jyotibdha Acharya, Andres Ussa Caycedo, Vandana Reddy Padala, Rishi Raj Sidhu Singh, Garrick Orchard, Bharath Ramesh, Arindam Basu
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: In this paper, we present EBBIOT-a novel paradigm for object tracking using stationary neuromorphic vision sensors in low-power sensor nodes for the Internet of Video Things (IoVT). Different from fully event based tracking or fully frame based approaches, we propose a mixed approach where we create event-based binary images (EBBI) that can use memory efficient noise filtering algorithms. We exploit the motion triggering aspect of neuromorphic sensors to generate region proposals based on event density counts with >1000X less memory and computes compared to frame based approaches. We also propose a simple overlap based tracker (OT) with prediction based handling of occlusion. Our overall approach requires 7X less memory and 3X less computations than conventional noise filtering and event based mean shift (EBMS) tracking. Finally, we show that our approach results in significantly higher precision and recall compared to EBMS approach as well as Kalman Filter tracker when evaluated over 1.1 hours of traffic recordings at two different locations.



### DELP-DAR System for License Plate Detection and Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.01853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.01853v1)
- **Published**: 2019-10-04 10:02:57+00:00
- **Updated**: 2019-10-04 10:02:57+00:00
- **Authors**: Zied Selmi, Mohamed Ben Halima, Umapada Pal, M. Adel Alimi
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic License Plate detection and Recognition (ALPR) is a quite popular and active research topic in the field of computer vision, image processing and intelligent transport systems. ALPR is used to make detection and recognition processes more robust and efficient in highly complicated environments and backgrounds. Several research investigations are still necessary due to some constraints such as: completeness of numbering systems of countries, different colors, various languages, multiple sizes and varied fonts. For this, we present in this paper an automatic framework for License Plate (LP) detection and recognition from complex scenes. Our framework is based on mask region convolutional neural networks used for LP detection, segmentation and recognition. Although some studies have focused on LP detection, LP recognition, LP segmentation or just two of them, our study uses the maskr-cnn in the three stages. The evaluation of our framework is enhanced by four datasets for different countries and consequently with various languages. In fact, it tested on four datasets including images captured from multiple scenes under numerous conditions such as varied orientation, poor quality images, blurred images and complex environmental backgrounds. Extensive experiments show the robustness and efficiency of our suggested framework in all datasets.



### Stacked Autoencoder Based Deep Random Vector Functional Link Neural Network for Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.01858v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.01858v4)
- **Published**: 2019-10-04 10:25:24+00:00
- **Updated**: 2020-02-13 05:25:33+00:00
- **Authors**: Rakesh Katuwal, P. N. Suganthan
- **Comment**: 29 pages, 17 figures
- **Journal**: None
- **Summary**: Extreme learning machine (ELM), which can be viewed as a variant of Random Vector Functional Link (RVFL) network without the input-output direct connections, has been extensively used to create multi-layer (deep) neural networks. Such networks employ randomization based autoencoders (AE) for unsupervised feature extraction followed by an ELM classifier for final decision making. Each randomization based AE acts as an independent feature extractor and a deep network is obtained by stacking several such AEs. Inspired by the better performance of RVFL over ELM, in this paper, we propose several deep RVFL variants by utilizing the framework of stacked autoencoders. Specifically, we introduce direct connections (feature reuse) from preceding layers to the fore layers of the network as in the original RVFL network. Such connections help to regularize the randomization and also reduce the model complexity. Furthermore, we also introduce denoising criterion, recovering clean inputs from their corrupted versions, in the autoencoders to achieve better higher level representations than the ordinary autoencoders. Extensive experiments on several classification datasets show that our proposed deep networks achieve overall better and faster generalization than the other relevant state-of-the-art deep neural networks.



### A Topological Loss Function for Deep-Learning based Image Segmentation using Persistent Homology
- **Arxiv ID**: http://arxiv.org/abs/1910.01877v2
- **DOI**: 10.1109/TPAMI.2020.3013679
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01877v2)
- **Published**: 2019-10-04 11:18:49+00:00
- **Updated**: 2020-09-18 11:23:51+00:00
- **Authors**: James R. Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A. Zimmer, Julia A. Schnabel, Andrew P. King
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020
- **Journal**: None
- **Summary**: We introduce a method for training neural networks to perform image or volume segmentation in which prior knowledge about the topology of the segmented object can be explicitly provided and then incorporated into the training process. By using the differentiable properties of persistent homology, a concept used in topological data analysis, we can specify the desired topology of segmented objects in terms of their Betti numbers and then drive the proposed segmentations to contain the specified topological features. Importantly this process does not require any ground-truth labels, just prior knowledge of the topology of the structure being segmented. We demonstrate our approach in three experiments. Firstly we create a synthetic task in which handwritten MNIST digits are de-noised, and show that using this kind of topological prior knowledge in the training of the network significantly improves the quality of the de-noised digits. Secondly we perform an experiment in which the task is segmenting the myocardium of the left ventricle from cardiac magnetic resonance images. We show that the incorporation of the prior knowledge of the topology of this anatomy improves the resulting segmentations in terms of both the topological accuracy and the Dice coefficient. Thirdly, we extend the method to 3D volumes and demonstrate its performance on the task of segmenting the placenta from ultrasound data, again showing that incorporating topological priors improves performance on this challenging task. We find that embedding explicit prior knowledge in neural network segmentation tasks is most beneficial when the segmentation task is especially challenging and that it can be used in either a semi-supervised or post-processing context to extract a useful training gradient from images without pixelwise labels.



### Bregman-divergence-guided Legendre exponential dispersion model with finite cumulants (K-LED)
- **Arxiv ID**: http://arxiv.org/abs/1910.03025v1
- **DOI**: None
- **Categories**: **math.ST**, cs.CV, cs.IT, cs.LG, math.IT, math.OC, stat.ML, stat.TH
- **Links**: [PDF](http://arxiv.org/pdf/1910.03025v1)
- **Published**: 2019-10-04 11:24:31+00:00
- **Updated**: 2019-10-04 11:24:31+00:00
- **Authors**: Hyenkyun Woo
- **Comment**: 21pages, 2figures
- **Journal**: None
- **Summary**: Exponential dispersion model is a useful framework in machine learning and statistics. Primarily, thanks to the additive structure of the model, it can be achieved without difficulty to estimate parameters including mean. However, tight conditions on cumulant function, such as analyticity, strict convexity, and steepness, reduce the class of exponential dispersion model. In this work, we present relaxed exponential dispersion model K-LED (Legendre exponential dispersion model with K cumulants). The cumulant function of the proposed model is a convex function of Legendre type having continuous partial derivatives of K-th order on the interior of a convex domain. Most of the K-LED models are developed via Bregman-divergence-guided log-concave density function with coercivity shape constraints. The main advantage of the proposed model is that the first cumulant (or the mean parameter space) of the 1-LED model is easily computed through the extended global optimum property of Bregman divergence. An extended normal distribution is introduced as an example of 1-LED based on Tweedie distribution. On top of that, we present 2-LED satisfying mean-variance relation of quasi-likelihood function. There is an equivalence between a subclass of quasi-likelihood function and a regular 2-LED model, of which the canonical parameter space is open. A typical example is a regular 2-LED model with power variance function, i.e., a variance is in proportion to the power of the mean of observations. This model is equivalent to a subclass of beta-divergence (or a subclass of quasi-likelihood function with power variance function). Furthermore, a new parameterized K-LED model, the cumulant function of which is the convex extended logistic loss function, is proposed. This model includes Bernoulli distribution and Poisson distribution.



### Hyperspectral holography and spectroscopy: computational features of inverse discrete cosine transform
- **Arxiv ID**: http://arxiv.org/abs/1910.03013v1
- **DOI**: None
- **Categories**: **math.NA**, cs.CV, cs.NA
- **Links**: [PDF](http://arxiv.org/pdf/1910.03013v1)
- **Published**: 2019-10-04 12:15:16+00:00
- **Updated**: 2019-10-04 12:15:16+00:00
- **Authors**: Vladimir Katkovnik, Igor Shevkunov, Karen Egiazarian
- **Comment**: 20 pages, 9 figures
- **Journal**: None
- **Summary**: Broadband hyperspectral digital holography and Fourier transform spectroscopy are important instruments in various science and application fields. In the digital hyperspectral holography and spectroscopy the variable of interest are obtained as inverse discrete cosine transforms of observed diffractive intensity patterns. In these notes, we provide a variety of algorithms for the inverse cosine transform with the proofs of perfect spectrum reconstruction, as well as we discuss and illustrate some nontrivial features of these algorithms.



### 4D MRI: Robust sorting of free breathing MRI slices for use in interventional settings
- **Arxiv ID**: http://arxiv.org/abs/1910.01902v2
- **DOI**: 10.1371/journal.pone.0235175
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.01902v2)
- **Published**: 2019-10-04 12:37:31+00:00
- **Updated**: 2020-07-06 14:11:50+00:00
- **Authors**: Gino Gulamhussene, Fabian Joeres, Marko Rak, Maciej Pech, Christian Hansen
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Purpose: We aim to develop a robust 4D MRI method for large FOVs enabling the extraction of irregular respiratory motion that is readily usable with all MRI machines and thus applicable to support a wide range of interventional settings.   Method: We propose a 4D MRI reconstruction method to capture an arbitrary number of breathing states. It uses template updates in navigator slices and search regions for fast and robust vessel cross-section tracking. It captures FOVs of 255 mm x 320 mm x 228 mm at a spatial resolution of 1.82 mm x 1.82 mm x 4mm and temporal resolution of 200ms. A total of 37 4D MRIs of 13 healthy subjects were reconstructed to validate the method. A quantitative evaluation of the reconstruction rate and speed of both the new and baseline method was performed. Additionally, a study with ten radiologists was conducted to assess the subjective reconstruction quality of both methods.   Results: Our results indicate improved mean reconstruction rates compared to the baseline method (79.4\% vs. 45.5\%) and improved mean reconstruction times (24s vs. 73s) per subject. Interventional radiologists perceive the reconstruction quality of our method as higher compared to the baseline (262.5 points vs. 217.5 points, p=0.02).   Conclusions: Template updates are an effective and efficient way to increase 4D MRI reconstruction rates and to achieve better reconstruction quality. Search regions reduce reconstruction time. These improvements increase the applicability of 4D MRI as a base for seamless support of interventional image guidance in percutaneous interventions.



### Revisiting Classical Bagging with Modern Transfer Learning for On-the-fly Disaster Damage Detector
- **Arxiv ID**: http://arxiv.org/abs/1910.01911v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01911v1)
- **Published**: 2019-10-04 12:47:58+00:00
- **Updated**: 2019-10-04 12:47:58+00:00
- **Authors**: Junghoon Seo, Seungwon Lee, Beomsu Kim, Taegyun Jeon
- **Comment**: Accepted at the 2019 NeurIPS Workshop on Artificial Intelligence for
  Humanitarian Assistance and Disaster Response(AI+HADR 2019)
- **Journal**: None
- **Summary**: Automatic post-disaster damage detection using aerial imagery is crucial for quick assessment of damage caused by disaster and development of a recovery plan. The main problem preventing us from creating an applicable model in practice is that damaged (positive) examples we are trying to detect are much harder to obtain than undamaged (negative) examples, especially in short time. In this paper, we revisit the classical bootstrap aggregating approach in the context of modern transfer learning for data-efficient disaster damage detection. Unlike previous classical ensemble learning articles, our work points out the effectiveness of simple bagging in deep transfer learning that has been underestimated in the context of imbalanced classification. Benchmark results on the AIST Building Change Detection dataset show that our approach significantly outperforms existing methodologies, including the recently proposed disentanglement learning.



### Layout-Graph Reasoning for Fashion Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.01923v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9
- **Links**: [PDF](http://arxiv.org/pdf/1910.01923v1)
- **Published**: 2019-10-04 12:59:16+00:00
- **Updated**: 2019-10-04 12:59:16+00:00
- **Authors**: Weijiang Yu, Xiaodan Liang, Ke Gong, Chenhan Jiang, Nong Xiao, Liang Lin
- **Comment**: 9 pages, 5 figures, CVPR2019
- **Journal**: None
- **Summary**: Detecting dense landmarks for diverse clothes, as a fundamental technique for clothes analysis, has attracted increasing research attention due to its huge application potential. However, due to the lack of modeling underlying semantic layout constraints among landmarks, prior works often detect ambiguous and structure-inconsistent landmarks of multiple overlapped clothes in one person. In this paper, we propose to seamlessly enforce structural layout relationships among landmarks on the intermediate representations via multiple stacked layout-graph reasoning layers. We define the layout-graph as a hierarchical structure including a root node, body-part nodes (e.g. upper body, lower body), coarse clothes-part nodes (e.g. collar, sleeve) and leaf landmark nodes (e.g. left-collar, right-collar). Each Layout-Graph Reasoning(LGR) layer aims to map feature representations into structural graph nodes via a Map-to-Node module, performs reasoning over structural graph nodes to achieve global layout coherency via a layout-graph reasoning module, and then maps graph nodes back to enhance feature representations via a Node-to-Map module. The layout-graph reasoning module integrates a graph clustering operation to generate representations of intermediate nodes (bottom-up inference) and then a graph deconvolution operation (top-down inference) over the whole graph. Extensive experiments on two public fashion landmark datasets demonstrate the superiority of our model. Furthermore, to advance the fine-grained fashion landmark research for supporting more comprehensive clothes generation and attribute recognition, we contribute the first Fine-grained Fashion Landmark Dataset (FFLD) containing 200k images annotated with at most 32 key-points for 13 clothes types.



### Generating Relevant Counter-Examples from a Positive Unlabeled Dataset for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.01968v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01968v1)
- **Published**: 2019-10-04 14:33:24+00:00
- **Updated**: 2019-10-04 14:33:24+00:00
- **Authors**: Florent Chiaroni, Ghazaleh Khodabandelou, Mohamed-Cherif Rahal, Nicolas Hueber, Frederic Dufaux
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: With surge of available but unlabeled data, Positive Unlabeled (PU) learning is becoming a thriving challenge. This work deals with this demanding task for which recent GAN-based PU approaches have demonstrated promising results. Generative adversarial Networks (GANs) are not hampered by deterministic bias or need for specific dimensionality. However, existing GAN-based PU approaches also present some drawbacks such as sensitive dependence to prior knowledge, a cumbersome architecture or first-stage overfitting. To settle these issues, we propose to incorporate a biased PU risk within the standard GAN discriminator loss function. In this manner, the discriminator is constrained to request the generator to converge towards the unlabeled samples distribution while diverging from the positive samples distribution. This enables the proposed model, referred to as D-GAN, to exclusively learn the counter-examples distribution without prior knowledge. Experiments demonstrate that our approach outperforms state-of-the-art PU methods without prior by overcoming their issues.



### Farkas layers: don't shift the data, fix the geometry
- **Arxiv ID**: http://arxiv.org/abs/1910.02840v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02840v1)
- **Published**: 2019-10-04 15:24:37+00:00
- **Updated**: 2019-10-04 15:24:37+00:00
- **Authors**: Aram-Alexandre Pooladian, Chris Finlay, Adam M Oberman
- **Comment**: None
- **Journal**: None
- **Summary**: Successfully training deep neural networks often requires either batch normalization, appropriate weight initialization, both of which come with their own challenges. We propose an alternative, geometrically motivated method for training. Using elementary results from linear programming, we introduce Farkas layers: a method that ensures at least one neuron is active at a given layer. Focusing on residual networks with ReLU activation, we empirically demonstrate a significant improvement in training capacity in the absence of batch normalization or methods of initialization across a broad range of network sizes on benchmark datasets.



### Variational Osmosis for Non-linear Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1910.02012v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.NA, math.NA, 62H35, 94A08, 65K10, 35A15
- **Links**: [PDF](http://arxiv.org/pdf/1910.02012v2)
- **Published**: 2019-10-04 16:08:35+00:00
- **Updated**: 2020-03-24 18:42:14+00:00
- **Authors**: Simone Parisotto, Luca Calatroni, Aurélie Bugeau, Nicolas Papadakis, Carola-Bibiane Schönlieb
- **Comment**: 10 pages, 8 figures
- **Journal**: None
- **Summary**: We propose a new variational model for non-linear image fusion. Our approach is based on the use of an osmosis energy term related to the one studied in Vogel et al. (2013) and Weickert et al. (2013) The minimization of the proposed non-convex energy realizes visually plausible image data fusion, invariant to multiplicative brightness changes. On the practical side, it requires minimal supervision and parameter tuning and can encode prior information on the structure of the images to be fused. For the numerical solution of the proposed model, we develop a primal-dual algorithm and we apply the resulting minimization scheme to solve multi-modal face fusion, color transfer and cultural heritage conservation problems. Visual and quantitative comparisons to state-of-the-art approaches prove the out-performance and the flexibility of our method.



### Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.02027v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02027v1)
- **Published**: 2019-10-04 16:39:25+00:00
- **Updated**: 2019-10-04 16:39:25+00:00
- **Authors**: Yunji Kim, Seonghyeon Nam, In Cho, Seon Joo Kim
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: We propose a deep video prediction model conditioned on a single image and an action class. To generate future frames, we first detect keypoints of a moving object and predict future motion as a sequence of keypoints. The input image is then translated following the predicted keypoints sequence to compose future frames. Detecting the keypoints is central to our algorithm, and our method is trained to detect the keypoints of arbitrary objects in an unsupervised manner. Moreover, the detected keypoints of the original videos are used as pseudo-labels to learn the motion of objects. Experimental results show that our method is successfully applied to various datasets without the cost of labeling keypoints in videos. The detected keypoints are similar to human-annotated labels, and prediction results are more realistic compared to the previous methods.



### Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory
- **Arxiv ID**: http://arxiv.org/abs/1910.02029v3
- **DOI**: 10.1007/s11263-020-01374-3
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02029v3)
- **Published**: 2019-10-04 16:44:59+00:00
- **Updated**: 2020-10-22 12:03:18+00:00
- **Authors**: Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool
- **Comment**: Accepted to IJCV 2020, 20 pages, 10 Figures, Demo Video:
  https://people.ee.ethz.ch/~arunv/resources/talk2nav.mp4
- **Journal**: None
- **Summary**: The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions -- one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our project page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html



### Generative Adversarial Networks for Failure Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.02034v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02034v1)
- **Published**: 2019-10-04 16:51:55+00:00
- **Updated**: 2019-10-04 16:51:55+00:00
- **Authors**: Shuai Zheng, Ahmed Farahat, Chetan Gupta
- **Comment**: ECML PKDD 2019 (The European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases, 2019)
- **Journal**: None
- **Summary**: Prognostics and Health Management (PHM) is an emerging engineering discipline which is concerned with the analysis and prediction of equipment health and performance. One of the key challenges in PHM is to accurately predict impending failures in the equipment. In recent years, solutions for failure prediction have evolved from building complex physical models to the use of machine learning algorithms that leverage the data generated by the equipment. However, failure prediction problems pose a set of unique challenges that make direct application of traditional classification and prediction algorithms impractical. These challenges include the highly imbalanced training data, the extremely high cost of collecting more failure samples, and the complexity of the failure patterns. Traditional oversampling techniques will not be able to capture such complexity and accordingly result in overfitting the training data. This paper addresses these challenges by proposing a novel algorithm for failure prediction using Generative Adversarial Networks (GAN-FP). GAN-FP first utilizes two GAN networks to simultaneously generate training samples and build an inference network that can be used to predict failures for new samples. GAN-FP first adopts an infoGAN to generate realistic failure and non-failure samples, and initialize the weights of the first few layers of the inference network. The inference network is then tuned by optimizing a weighted loss objective using only real failure and non-failure samples. The inference network is further tuned using a second GAN whose purpose is to guarantee the consistency between the generated samples and corresponding labels. GAN-FP can be used for other imbalanced classification problems as well.



### Stacked Wasserstein Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1910.02560v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02560v1)
- **Published**: 2019-10-04 17:07:42+00:00
- **Updated**: 2019-10-04 17:07:42+00:00
- **Authors**: Wenju Xu, Shawn Keshmiri, Guanghui Wang
- **Comment**: arXiv admin note: text overlap with arXiv:1902.05581
- **Journal**: None
- **Summary**: Approximating distributions over complicated manifolds, such as natural images, are conceptually attractive. The deep latent variable model, trained using variational autoencoders and generative adversarial networks, is now a key technique for representation learning. However, it is difficult to unify these two models for exact latent-variable inference and parallelize both reconstruction and sampling, partly due to the regularization under the latent variables, to match a simple explicit prior distribution. These approaches are prone to be oversimplified, and can only characterize a few modes of the true distribution. Based on the recently proposed Wasserstein autoencoder (WAE) with a new regularization as an optimal transport. The paper proposes a stacked Wasserstein autoencoder (SWAE) to learn a deep latent variable model. SWAE is a hierarchical model, which relaxes the optimal transport constraints at two stages. At the first stage, the SWAE flexibly learns a representation distribution, i.e., the encoded prior; and at the second stage, the encoded representation distribution is approximated with a latent variable model under the regularization encouraging the latent distribution to match the explicit prior. This model allows us to generate natural textual outputs as well as perform manipulations in the latent space to induce changes in the output space. Both quantitative and qualitative results demonstrate the superior performance of SWAE compared with the state-of-the-art approaches in terms of faithful reconstruction and generation quality.



### Neural Turtle Graphics for Modeling City Road Layouts
- **Arxiv ID**: http://arxiv.org/abs/1910.02055v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02055v1)
- **Published**: 2019-10-04 17:30:00+00:00
- **Updated**: 2019-10-04 17:30:00+00:00
- **Authors**: Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai Wei, Ming-Yu Liu, Antonio Torralba, Sanja Fidler
- **Comment**: ICCV-2019 Oral
- **Journal**: None
- **Summary**: We propose Neural Turtle Graphics (NTG), a novel generative model for spatial graphs, and demonstrate its applications in modeling city road layouts. Specifically, we represent the road layout using a graph where nodes in the graph represent control points and edges in the graph represent road segments. NTG is a sequential generative model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train NTG on Open Street Map data and show that it outperforms existing approaches using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch parts of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.



### Memory efficient brain tumor segmentation using an autoencoder-regularized U-Net
- **Arxiv ID**: http://arxiv.org/abs/1910.02058v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1910.02058v1)
- **Published**: 2019-10-04 17:33:07+00:00
- **Updated**: 2019-10-04 17:33:07+00:00
- **Authors**: Markus Frey, Matthias Nau
- **Comment**: None
- **Journal**: None
- **Summary**: Early diagnosis and accurate segmentation of brain tumors are imperative for successful treatment. Unfortunately, manual segmentation is time consuming, costly and despite extensive human expertise often inaccurate. Here, we present an MRI-based tumor segmentation framework using an autoencoder-regularized 3D-convolutional neural network. We trained the model on manually segmented structural T1, T1ce, T2, and Flair MRI images of 335 patients with tumors of variable severity, size and location. We then tested the model using independent data of 125 patients and successfully segmented brain tumors into three subregions: the tumor core (TC), the enhancing tumor (ET) and the whole tumor (WT). We also explored several data augmentations and preprocessing steps to improve segmentation performance. Importantly, our model was implemented on a single NVIDIA GTX1060 graphics unit and hence optimizes tumor segmentation for widely affordable hardware. In sum, we present a memory-efficient and affordable solution to tumor segmentation to support the accurate diagnostics of oncological brain pathologies.



### Neural Puppet: Generative Layered Cartoon Characters
- **Arxiv ID**: http://arxiv.org/abs/1910.02060v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02060v3)
- **Published**: 2019-10-04 17:40:51+00:00
- **Updated**: 2020-10-12 23:54:09+00:00
- **Authors**: Omid Poursaeed, Vladimir G. Kim, Eli Shechtman, Jun Saito, Serge Belongie
- **Comment**: WACV 2020
- **Journal**: None
- **Summary**: We propose a learning based method for generating new animations of a cartoon character given a few example images. Our method is designed to learn from a traditionally animated sequence, where each frame is drawn by an artist, and thus the input images lack any common structure, correspondences, or labels. We express pose changes as a deformation of a layered 2.5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. This enables us to extract a common low-dimensional structure from a diverse set of character poses. We combine recent advances in differentiable rendering as well as mesh-aware models to successfully align common template even if only a few character images are available during training. In addition to coarse poses, character appearance also varies due to shading, out-of-plane motions, and artistic effects. We capture these subtle changes by applying an image translation network to refine the mesh rendering, providing an end-to-end model to generate new animations of a character with high visual quality. We demonstrate that our generative model can be used to synthesize in-between frames and to create data-driven deformation. Our template fitting procedure outperforms state-of-the-art generic techniques for detecting image correspondences.



### Higher Order Function Networks for View Planning and Multi-View Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.02066v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02066v1)
- **Published**: 2019-10-04 17:46:13+00:00
- **Updated**: 2019-10-04 17:46:13+00:00
- **Authors**: Selim Engin, Eric Mitchell, Daewon Lee, Volkan Isler, Daniel D. Lee
- **Comment**: 7 pages, 6 figures
- **Journal**: None
- **Summary**: We consider the problem of planning views for a robot to acquire images of an object for visual inspection and reconstruction. In contrast to offline methods which require a 3D model of the object as input or online methods which rely on only local measurements, our method uses a neural network which encodes shape information for a large number of objects. We build on recent deep learning methods capable of generating a complete 3D reconstruction of an object from a single image. Specifically, in this work, we extend a recent method which uses Higher Order Functions (HOF) to represent the shape of the object. We present a new generalization of this method to incorporate multiple images as input and establish a connection between visibility and reconstruction quality. This relationship forms the foundation of our view planning method where we compute viewpoints to visually cover the output of the multi-view HOF network with as few images as possible. Experiments indicate that our method provides a good compromise between online and offline methods: Similar to online methods, our method does not require the true object model as input. In terms of number of views, it is much more efficient. In most cases, its performance is comparable to the optimal offline case even on object classes the network has not been trained on.



### Adaptively Denoising Proposal Collection for Weakly Supervised Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.02101v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02101v2)
- **Published**: 2019-10-04 18:42:32+00:00
- **Updated**: 2019-10-19 21:12:53+00:00
- **Authors**: Wenju Xu, Yuanwei Wu, Wenchi Ma, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we address the problem of weakly supervised object localization (WSL), which trains a detection network on the dataset with only image-level annotations. The proposed approach is built on the observation that the proposal set from the training dataset is a collection of background, object parts, and objects. Several strategies are taken to adaptively eliminate the noisy proposals and generate pseudo object-level annotations for the weakly labeled dataset. A multiple instance learning (MIL) algorithm enhanced by mask-out strategy is adopted to collect the class-specific object proposals, which are then utilized to adapt a pre-trained classification network to a detection network. In addition, the detection results from the detection network are re-weighted by jointly considering the detection scores and the overlap ratio of proposals in a proposal subset optimization framework. The optimal proposals work as object-level labels that enable a pseudo-strongly supervised dataset for training the detection network. Consequently, we establish a fully adaptive detection network. Extensive evaluations on the PASCAL VOC 2007 and 2012 datasets demonstrate a significant improvement compared with the state-of-the-art methods.



### Direct Visual-Inertial Odometry with Semi-Dense Mapping
- **Arxiv ID**: http://arxiv.org/abs/1910.02106v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02106v1)
- **Published**: 2019-10-04 18:49:22+00:00
- **Updated**: 2019-10-04 18:49:22+00:00
- **Authors**: Wenju Xu, Dongkyu Choi, Guanghui Wang
- **Comment**: None
- **Journal**: None
- **Summary**: The paper presents a direct visual-inertial odometry system. In particular, a tightly coupled nonlinear optimization based method is proposed by integrating the recent advances in direct dense tracking and Inertial Measurement Unit (IMU) pre-integration, and a factor graph optimization is adapted to estimate the pose of the camera and rebuild a semi-dense map. Two sliding windows are maintained in the proposed approach. The first one, based on Direct Sparse Odometry (DSO), is to estimate the depths of candidate points for mapping and dense visual tracking. In the second one, measurements from the IMU pre-integration and dense visual tracking are fused probabilistically using a tightly-coupled, optimization-based sensor fusion framework. As a result, the IMU pre-integration provides additional constraints to suppress the scale drift induced by the visual odometry. Evaluations on real-world benchmark datasets show that the proposed method achieves competitive results in indoor scenes.



### Tensor-based algorithms for image classification
- **Arxiv ID**: http://arxiv.org/abs/1910.02150v2
- **DOI**: 10.3390/a12110240
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02150v2)
- **Published**: 2019-10-04 21:16:33+00:00
- **Updated**: 2019-11-28 22:31:02+00:00
- **Authors**: Stefan Klus, Patrick Gelß
- **Comment**: None
- **Journal**: Algorithms, 12(11), 240, 2019
- **Summary**: The interest in machine learning with tensor networks has been growing rapidly in recent years. We show that tensor-based methods developed for learning the governing equations of dynamical systems from data can, in the same way, be used for supervised learning problems and propose two novel approaches for image classification. One is a kernel-based reformulation of the previously introduced MANDy (multidimensional approximation of nonlinear dynamics), the other an alternating ridge regression in the tensor-train format. We apply both methods to the MNIST and fashion MNIST data set and show that the approaches are competitive with state-of-the-art neural network-based classifiers.



### SLAM-based Integrity Monitoring Using GPS and Fish-eye Camera
- **Arxiv ID**: http://arxiv.org/abs/1910.02165v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02165v1)
- **Published**: 2019-10-04 23:05:52+00:00
- **Updated**: 2019-10-04 23:05:52+00:00
- **Authors**: Sriramya Bhamidipati, Grace Xingxin Gao
- **Comment**: 32nd International Technical Meeting of the Satellite Division of the
  Institute of Navigation, ION GNSS+ 2019, Miami, FL, Sept 2019
- **Journal**: None
- **Summary**: Urban navigation using GPS and fish-eye camera suffers from multipath effects in GPS measurements and data association errors in pixel intensities across image frames. We propose a Simultaneous Localization and Mapping (SLAM)-based Integrity Monitoring (IM) algorithm to compute the position protection levels while accounting for multiple faults in both GPS and vision. We perform graph optimization using the sequential data of GPS pseudoranges, pixel intensities, vehicle dynamics, and satellite ephemeris to simultaneously localize the vehicle as well as the landmarks, namely GPS satellites and key image pixels in the world frame. We estimate the fault mode vector by analyzing the temporal correlation across the GPS measurement residuals and spatial correlation across the vision intensity residuals. In particular, to detect and isolate the vision faults, we developed a superpixel-based piecewise Random Sample Consensus (RANSAC) technique to perform spatial voting across image pixels. For an estimated fault mode, we compute the protection levels by applying worst-case failure slope analysis to the linearized Graph-SLAM framework. We perform ground vehicle experiments in the semi-urban area of Champaign, IL and have demonstrated the successful detection and isolation of multiple faults. We also validate tighter protection levels and lower localization errors achieved via the proposed algorithm as compared to SLAM-based IM that utilizes only GPS measurements.



