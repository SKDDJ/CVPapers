# Arxiv Papers in cs.CV on 2019-10-27
### Structured Low-Rank Algorithms: Theory, MR Applications, and Links to Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.12162v1
- **DOI**: 10.1109/MSP.2019.2950432
- **Categories**: **cs.CV**, cs.IT, cs.LG, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12162v1)
- **Published**: 2019-10-27 01:46:02+00:00
- **Updated**: 2019-10-27 01:46:02+00:00
- **Authors**: Mathews Jacob, Merry P. Mani, Jong Chul Ye
- **Comment**: Accepted for IEEE Signal Processing Magazine
- **Journal**: None
- **Summary**: In this survey, we provide a detailed review of recent advances in the recovery of continuous domain multidimensional signals from their few non-uniform (multichannel) measurements using structured low-rank matrix completion formulation. This framework is centered on the fundamental duality between the compactness (e.g., sparsity) of the continuous signal and the rank of a structured matrix, whose entries are functions of the signal. This property enables the reformulation of the signal recovery as a low-rank structured matrix completion, which comes with performance guarantees. We will also review fast algorithms that are comparable in complexity to current compressed sensing methods, which enables the application of the framework to large-scale magnetic resonance (MR) recovery problems. The remarkable flexibility of the formulation can be used to exploit signal properties that are difficult to capture by current sparse and low-rank optimization strategies. We demonstrate the utility of the framework in a wide range of MR imaging (MRI) applications, including highly accelerated imaging, calibration-free acquisition, MR artifact correction, and ungated dynamic MRI.



### Adversarial Defense via Local Flatness Regularization
- **Arxiv ID**: http://arxiv.org/abs/1910.12165v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12165v4)
- **Published**: 2019-10-27 02:12:20+00:00
- **Updated**: 2020-12-17 07:19:03+00:00
- **Authors**: Jia Xu, Yiming Li, Yong Jiang, Shu-Tao Xia
- **Comment**: Accepted by the ICIP 2020. The first two authors contributed equally
  to this work
- **Journal**: None
- **Summary**: Adversarial defense is a popular and important research area. Due to its intrinsic mechanism, one of the most straightforward and effective ways of defending attacks is to analyze the property of loss surface in the input space. In this paper, we define the local flatness of the loss surface as the maximum value of the chosen norm of the gradient regarding to the input within a neighborhood centered on the benign sample, and discuss the relationship between the local flatness and adversarial vulnerability. Based on the analysis, we propose a novel defense approach via regularizing the local flatness, dubbed local flatness regularization (LFR). We also demonstrate the effectiveness of the proposed method from other perspectives, such as human visual mechanism, and analyze the relationship between LFR and other related methods theoretically. Experiments are conducted to verify our theory and demonstrate the superiority of the proposed method.



### Traffic Sign Detection and Recognition for Autonomous Driving in Virtual Simulation Environment
- **Arxiv ID**: http://arxiv.org/abs/1911.05626v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05626v1)
- **Published**: 2019-10-27 03:38:10+00:00
- **Updated**: 2019-10-27 03:38:10+00:00
- **Authors**: Meixin Zhu, Jingyun Hu, Ziyuan Pu, Zhiyong Cui, Liangwu Yan, Yinhai Wang
- **Comment**: 8 pages, 12 figures
- **Journal**: None
- **Summary**: This study developed a traffic sign detection and recognition algorithm based on the RetinaNet. Two main aspects were revised to improve the detection of traffic signs: image cropping to address the issue of large image and small traffic signs; and using more anchors with various scales to detect traffic signs with different sizes and shapes. The proposed algorithm was trained and tested in a series of autonomous driving front-view images in a virtual simulation environment. Results show that the algorithm performed extremely well under good illumination and weather conditions. Its drawbacks are that it sometimes failed to detect object under bad weather conditions like snow and failed to distinguish speed limits signs with different limit values.



### Multi-source Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.12181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12181v1)
- **Published**: 2019-10-27 05:04:25+00:00
- **Updated**: 2019-10-27 05:04:25+00:00
- **Authors**: Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches. Our source code is released at: https://github.com/Luodian/MADAN.



### Spot Evasion Attacks: Adversarial Examples for License Plate Recognition Systems with Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.00927v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00927v2)
- **Published**: 2019-10-27 05:34:23+00:00
- **Updated**: 2019-11-28 02:23:19+00:00
- **Authors**: Ya-guan Qian, Dan-feng Ma, Bin Wang, Jun Pan, Jia-min Wang, Jian-hai Chen, Wu-jie Zhou, Jing-sheng Lei
- **Comment**: 26 pages, 16 figures
- **Journal**: None
- **Summary**: Recent studies have shown convolution neural networks (CNNs) for image recognition are vulnerable to evasion attacks with carefully manipulated adversarial examples. Previous work primarily focused on how to generate adversarial examples closed to source images, by introducing pixel-level perturbations into the whole or specific part of images. In this paper, we propose an evasion attack on CNN classifiers in the context of License Plate Recognition (LPR), which adds predetermined perturbations to specific regions of license plate images, simulating some sort of naturally formed spots (such as sludge, etc.). Therefore, the problem is modeled as an optimization process searching for optimal perturbation positions, which is different from previous work that consider pixel values as decision variables. Notice that this is a complex nonlinear optimization problem, and we use a genetic-algorithm based approach to obtain optimal perturbation positions. In experiments, we use the proposed algorithm to generate various adversarial examples in the form of rectangle, circle, ellipse and spots cluster. Experimental results show that these adversarial examples are almost ignored by human eyes, but can fool HyperLPR with high attack success rate over 93%. Therefore, we believe that this kind of spot evasion attacks would pose a great threat to current LPR systems, and needs to be investigated further by the security community.



### Segmenting Ships in Satellite Imagery With Squeeze and Excitation U-Net
- **Arxiv ID**: http://arxiv.org/abs/1910.12206v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12206v1)
- **Published**: 2019-10-27 08:28:51+00:00
- **Updated**: 2019-10-27 08:28:51+00:00
- **Authors**: Venkatesh R, Anand Metha
- **Comment**: None
- **Journal**: None
- **Summary**: The ship-detection task in satellite imagery presents significant obstacles to even the most state of the art segmentation models due to lack of labelled dataset or approaches which are not able to generalize to unseen images. The most common methods for semantic segmentation involve complex two-stage networks or networks which make use of a multi-scale scene parsing module. In this paper, we propose a modified version of the popular U-Net architecture called Squeeze and Excitation U-Net and train it with a loss that helps in directly optimizing the intersection over union (IoU) score. Our method gives comparable performance to other methods while having the additional benefit of being computationally efficient.



### Human Keypoint Detection by Progressive Context Refinement
- **Arxiv ID**: http://arxiv.org/abs/1910.12223v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12223v1)
- **Published**: 2019-10-27 09:57:08+00:00
- **Updated**: 2019-10-27 09:57:08+00:00
- **Authors**: Jing Zhang, Zhe Chen, Dacheng Tao
- **Comment**: Technical Report for "Joint COCO and MapillaryWorkshop at ICCV 2019:
  COCO Keypoint Detection Challenge Track"
- **Journal**: None
- **Summary**: Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination and scale variance of person instances. In this paper, we find that context information plays an important role in addressing these issues, and propose a novel method named progressive context refinement (PCR) for human keypoint detection. First, we devise a simple but effective context-aware module (CAM) that can efficiently integrate spatial and channel context information to aid feature learning for locating hard keypoints. Then, we construct the PCR model by stacking several CAMs sequentially with shortcuts and employ multi-task learning to progressively refine the context information and predictions. Besides, to maximize PCR's potential for the aforementioned hard case inference, we propose a hard-negative person detection mining strategy together with a joint-training strategy by exploiting the unlabeled coco dataset and external dataset. Extensive experiments on the COCO keypoint detection benchmark demonstrate the superiority of PCR over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.



### EdgeFool: An Adversarial Image Enhancement Filter
- **Arxiv ID**: http://arxiv.org/abs/1910.12227v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12227v2)
- **Published**: 2019-10-27 10:16:26+00:00
- **Updated**: 2020-03-05 10:08:19+00:00
- **Authors**: Ali Shahin Shamsabadi, Changjae Oh, Andrea Cavallaro
- **Comment**: None
- **Journal**: Proceedings of the 45th IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP)2020
- **Summary**: Adversarial examples are intentionally perturbed images that mislead classifiers. These images can, however, be easily detected using denoising algorithms, when high-frequency spatial perturbations are used, or can be noticed by humans, when perturbations are large. In this paper, we propose EdgeFool, an adversarial image enhancement filter that learns structure-aware adversarial perturbations. EdgeFool generates adversarial images with perturbations that enhance image details via training a fully convolutional neural network end-to-end with a multi-task loss function. This loss function accounts for both image detail enhancement and class misleading objectives. We evaluate EdgeFool on three classifiers (ResNet-50, ResNet-18 and AlexNet) using two datasets (ImageNet and Private-Places365) and compare it with six adversarial methods (DeepFool, SparseFool, Carlini-Wagner, SemanticAdv, Non-targeted and Private Fast Gradient Sign Methods). Code is available at https://github.com/smartcameras/EdgeFool.git.



### Solving Optimization Problems through Fully Convolutional Networks: an Application to the Travelling Salesman Problem
- **Arxiv ID**: http://arxiv.org/abs/1910.12243v1
- **DOI**: 10.1186/s11671-018-2831-8
- **Categories**: **cs.LG**, cs.CV, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1910.12243v1)
- **Published**: 2019-10-27 11:32:39+00:00
- **Updated**: 2019-10-27 11:32:39+00:00
- **Authors**: Zhengxuan Ling, Xinyu Tao, Yu Zhang, Xi Chen
- **Comment**: 25pages,7figures,research article
- **Journal**: None
- **Summary**: In the new wave of artificial intelligence, deep learning is impacting various industries. As a closely related area, optimization algorithms greatly contribute to the development of deep learning. But the reverse applications are still insufficient. Is there any efficient way to solve certain optimization problem through deep learning? The key is to convert the optimization to a representation suitable for deep learning. In this paper, a traveling salesman problem (TSP) is studied. Considering that deep learning is good at image processing, an image representation method is proposed to transfer a TSP to an image. Based on samples of a 10 city TSP, a fully convolutional network (FCN) is used to learn the mapping from a feasible region to an optimal solution. The training process is analyzed and interpreted through stages. A visualization method is presented to show how a FCN can understand the training task of a TSP. Once the training is completed, no significant effort is required to solve a new TSP and the prediction is obtained on the scale of milliseconds. The results show good performance in finding the global optimal solution. Moreover, the developed FCN model has been demonstrated on TSP's with different city numbers, proving excellent generalization performance.



### Twins Recognition Using Hierarchical Score Level Fusion
- **Arxiv ID**: http://arxiv.org/abs/1911.05625v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1911.05625v1)
- **Published**: 2019-10-27 11:33:16+00:00
- **Updated**: 2019-10-27 11:33:16+00:00
- **Authors**: Cihan Akin, Umit Kacar, Murvet Kirci
- **Comment**: 4 pages, 5 figures, 1 table
- **Journal**: None
- **Summary**: With the development of technology, the usage areas and importance of biometric systems have started to increase. Since the characteristics of each person are different from each other, a single model biometric system can yield successful results. However, because the characteristics of twin people are very close to each other, multiple biometric systems including multiple characteristics of individuals will be more appropriate and will increase the recognition rate. In this study, a multiple biometric recognition system consisting of a combination of multiple algorithms and multiple models was developed to distinguish people from other people and their twins. Ear and voice biometric data were used for the multimodal model and 38 pair of twin ear images and sound recordings were used in the data set. Sound and ear recognition rates were obtained using classical (hand-crafted) and deep learning algorithms. The results obtained were combined with the hierarchical score level fusion method to achieve a success rate of 94.74% in rank-1 and 100% in rank -2.



### Prediction stability as a criterion in active learning
- **Arxiv ID**: http://arxiv.org/abs/1910.12246v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12246v1)
- **Published**: 2019-10-27 12:01:37+00:00
- **Updated**: 2019-10-27 12:01:37+00:00
- **Authors**: Junyu Liu, Xiang Li, Jin Wang, Jiqiang Zhou, Jianxiong Shen
- **Comment**: None
- **Journal**: None
- **Summary**: Recent breakthroughs made by deep learning rely heavily on large number of annotated samples. To overcome this shortcoming, active learning is a possible solution. Beside the previous active learning algorithms that only adopted information after training, we propose a new class of method based on the information during training, named sequential-based method. An specific criterion of active learning called prediction stability is proposed to prove the feasibility of sequential-based methods. Experiments are made on CIFAR-10 and CIFAR-100, and the results indicates that prediction stability is effective and works well on fewer-labeled datasets. Prediction stability reaches the accuracy of traditional acquisition functions like entropy on CIFAR-10, and notably outperforms them on CIFAR-100.



### Smart Hypothesis Generation for Efficient and Robust Room Layout Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.12257v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12257v1)
- **Published**: 2019-10-27 13:10:13+00:00
- **Updated**: 2019-10-27 13:10:13+00:00
- **Authors**: Martin Hirzer, Peter M. Roth, Vincent Lepetit
- **Comment**: Accepted: Winter Conference on Applications of Computer Vision (WACV)
  2020
- **Journal**: None
- **Summary**: We propose a novel method to efficiently estimate the spatial layout of a room from a single monocular RGB image. As existing approaches based on low-level feature extraction, followed by a vanishing point estimation are very slow and often unreliable in realistic scenarios, we build on semantic segmentation of the input image. To obtain better segmentations, we introduce a robust, accurate and very efficient hypothesize-and-test scheme. The key idea is to use three segmentation hypotheses, each based on a different number of visible walls. For each hypothesis, we predict the image locations of the room corners and select the hypothesis for which the layout estimated from the room corners is consistent with the segmentation. We demonstrate the efficiency and robustness of our method on three challenging benchmark datasets, where we significantly outperform the state-of-the-art.



### L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained Visual Categorization
- **Arxiv ID**: http://arxiv.org/abs/1910.12259v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12259v1)
- **Published**: 2019-10-27 13:15:32+00:00
- **Updated**: 2019-10-27 13:15:32+00:00
- **Authors**: Mina Basirat, Peter M. Roth
- **Comment**: Accepted: Winter Conference on Applications of Computer Vision (WACV)
  2020
- **Journal**: None
- **Summary**: Deep neural networks paved the way for significant improvements in image visual categorization during the last years. However, even though the tasks are highly varying, differing in complexity and difficulty, existing solutions mostly build on the same architectural decisions. This also applies to the selection of activation functions (AFs), where most approaches build on Rectified Linear Units (ReLUs). In this paper, however, we show that the choice of a proper AF has a significant impact on the classification accuracy, in particular, if fine, subtle details are of relevance. Therefore, we propose to model the degree of absence and the presence of features via the AF by using piece-wise linear functions, which we refer to as L*ReLU. In this way, we can ensure the required properties, while still inheriting the benefits in terms of computational efficiency from ReLUs. We demonstrate our approach for the task of Fine-grained Visual Categorization (FGVC), running experiments on seven different benchmark datasets. The results do not only demonstrate superior results but also that for different tasks, having different characteristics, different AFs are selected.



### Exploring 3 R's of Long-term Tracking: Re-detection, Recovery and Reliability
- **Arxiv ID**: http://arxiv.org/abs/1910.12273v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12273v1)
- **Published**: 2019-10-27 14:46:08+00:00
- **Updated**: 2019-10-27 14:46:08+00:00
- **Authors**: Shyamgopal Karthik, Abhinav Moudgil, Vineet Gandhi
- **Comment**: None
- **Journal**: None
- **Summary**: Recent works have proposed several long term tracking benchmarks and highlight the importance of moving towards long-duration tracking to bridge the gap with application requirements. The current evaluation methodologies, however, do not focus on several aspects that are crucial in a long term perspective like Re-detection, Recovery, and Reliability. In this paper, we propose novel evaluation strategies for a more in-depth analysis of trackers from a long-term perspective. More specifically, (a) we test re-detection capability of the trackers in the wild by simulating virtual cuts, (b) we investigate the role of chance in the recovery of tracker after failure and (c) we propose a novel metric allowing visual inference on the ability of a tracker to track contiguously (without any failure) at a given accuracy. We present several original insights derived from an extensive set of quantitative and qualitative experiments.



### Hierarchical Clustering with Hard-batch Triplet Loss for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.12278v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12278v2)
- **Published**: 2019-10-27 15:07:57+00:00
- **Updated**: 2020-04-22 07:46:20+00:00
- **Authors**: Kaiwei Zeng
- **Comment**: Accepted by CVPR2020
- **Journal**: None
- **Summary**: For most unsupervised person re-identification (re-ID), people often adopt unsupervised domain adaptation (UDA) method. UDA often train on the labeled source dataset and evaluate on the target dataset, which often focuses on learning differences between the source dataset and the target dataset to improve the generalization of the model. Base on these, we explore how to make use of the similarity of samples to conduct a fully unsupervised method which just trains on the unlabeled target dataset. Concretely, we propose a hierarchical clustering-guided re-ID (HCR) method. We use hierarchical clustering to generate pseudo labels and use these pseudo labels as monitors to conduct the training. In order to exclude hard examples and promote the convergence of the model, We use PK sampling in each iteration, which randomly selects a fixed number of samples from each cluster for training. We evaluate our model on Market-1501, DukeMTMC-reID and MSMT17. Results show that HCR gets the state-of-the-arts and achieves 55.3% mAP on Market-1501 and 46.8% mAP on DukeMTMC-reID. Our code will be released soon.



### Non-Local ConvLSTM for Video Compression Artifact Reduction
- **Arxiv ID**: http://arxiv.org/abs/1910.12286v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12286v1)
- **Published**: 2019-10-27 15:45:41+00:00
- **Updated**: 2019-10-27 15:45:41+00:00
- **Authors**: Yi Xu, Longwen Gao, Kai Tian, Shuigeng Zhou, Huyang Sun
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Video compression artifact reduction aims to recover high-quality videos from low-quality compressed videos. Most existing approaches use a single neighboring frame or a pair of neighboring frames (preceding and/or following the target frame) for this task. Furthermore, as frames of high quality overall may contain low-quality patches, and high-quality patches may exist in frames of low quality overall, current methods focusing on nearby peak-quality frames (PQFs) may miss high-quality details in low-quality frames. To remedy these shortcomings, in this paper we propose a novel end-to-end deep neural network called non-local ConvLSTM (NL-ConvLSTM in short) that exploits multiple consecutive frames. An approximate non-local strategy is introduced in NL-ConvLSTM to capture global motion patterns and trace the spatiotemporal dependency in a video sequence. This approximate strategy makes the non-local module work in a fast and low space-cost way. Our method uses the preceding and following frames of the target frame to generate a residual, from which a higher quality frame is reconstructed. Experiments on two datasets show that NL-ConvLSTM outperforms the existing methods.



### MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.12295v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12295v1)
- **Published**: 2019-10-27 16:24:42+00:00
- **Updated**: 2019-10-27 16:24:42+00:00
- **Authors**: Rongcheng Lin, Jing Xiao, Jianping Fan
- **Comment**: ICCV 2019 YouTube8M workshop
- **Journal**: ICCV 2019
- **Summary**: In this paper, we present and discuss a deep mixture model with online knowledge distillation (MOD) for large-scale video temporal concept localization, which is ranked 3rd in the 3rd YouTube-8M Video Understanding Challenge. Specifically, we find that by enabling knowledge sharing with online distillation, fintuning a mixture model on a smaller dataset can achieve better evaluation performance. Based on this observation, in our final solution, we trained and fintuned 12 NeXtVLAD models in parallel with a 2-layer online distillation structure. The experimental results show that the proposed distillation structure can effectively avoid overfitting and shows superior generalization performance. The code is publicly available at: https://github.com/linrongc/solution_youtube8m_v3



### Multi-Resolution Overlapping Stripes Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.12322v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12322v1)
- **Published**: 2019-10-27 18:38:42+00:00
- **Updated**: 2019-10-27 18:38:42+00:00
- **Authors**: Arda Efe Okay, Manal AlGhamdi, Robert Westendorp, Mohamed Abdel-Mottaleb
- **Comment**: 5 pages
- **Journal**: None
- **Summary**: This paper addresses the person re-identification (PReID) problem by combining global and local information at multiple feature resolutions with different loss functions. Many previous studies address this problem using either part-based features or global features. In case of part-based representation, the spatial correlation between these parts is not considered, while global-based representation are not sensitive to spatial variations. This paper presents a part-based model with a multi-resolution network that uses different level of features. The output of the last two conv blocks is then partitioned horizontally and processed in pairs with overlapping stripes to cover the important information that might lie between parts. We use different loss functions to combine local and global information for classification. Experimental results on a benchmark dataset demonstrate that the presented method outperforms the state-of-the-art methods.



### Leveraging Auxiliary Text for Deep Recognition of Unseen Visual Relationships
- **Arxiv ID**: http://arxiv.org/abs/1910.12324v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12324v1)
- **Published**: 2019-10-27 18:45:04+00:00
- **Updated**: 2019-10-27 18:45:04+00:00
- **Authors**: Gal Sadeh Kenigsfield, Ran El-Yaniv
- **Comment**: None
- **Journal**: None
- **Summary**: One of the most difficult tasks in scene understanding is recognizing interactions between objects in an image. This task is often called visual relationship detection (VRD). We consider the question of whether, given auxiliary textual data in addition to the standard visual data used for training VRD models, VRD performance can be improved. We present a new deep model that can leverage additional textual data. Our model relies on a shared text--image representation of subject-verb-object relationships appearing in the text, and object interactions in images. Our method is the first to enable recognition of visual relationships missing in the visual training data and appearing only in the auxiliary text. We test our approach on two different text sources: text originating in images and text originating in books. We test and validate our approach using two large-scale recognition tasks: VRD and Scene Graph Generation. We show a surprising result: Our approach works better with text originating in books, and outperforms the text originating in images on the task of unseen relationship recognition. It is comparable to the model which utilizes text originating in images on the task of seen relationship recognition.



### GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil MRI Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.12325v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12325v4)
- **Published**: 2019-10-27 19:11:05+00:00
- **Updated**: 2020-03-30 23:33:06+00:00
- **Authors**: Anuroop Sriram, Jure Zbontar, Tullie Murrell, C. Lawrence Zitnick, Aaron Defazio, Daniel K. Sodickson
- **Comment**: None
- **Journal**: None
- **Summary**: Magnetic Resonance Image (MRI) acquisition is an inherently slow process which has spurred the development of two different acceleration methods: acquiring multiple correlated samples simultaneously (parallel imaging) and acquiring fewer samples than necessary for traditional signal processing methods (compressed sensing). Both methods provide complementary approaches to accelerating the speed of MRI acquisition. In this paper, we present a novel method to integrate traditional parallel imaging methods into deep neural networks that is able to generate high quality reconstructions even for high acceleration factors. The proposed method, called GrappaNet, performs progressive reconstruction by first mapping the reconstruction problem to a simpler one that can be solved by a traditional parallel imaging methods using a neural network, followed by an application of a parallel imaging method, and finally fine-tuning the output with another neural network. The entire network can be trained end-to-end. We present experimental results on the recently released fastMRI dataset and show that GrappaNet can generate higher quality reconstructions than competing methods for both $4\times$ and $8\times$ acceleration.



### Weakly Supervised Multi-Task Learning for Cell Detection and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.12326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/1910.12326v1)
- **Published**: 2019-10-27 19:11:39+00:00
- **Updated**: 2019-10-27 19:11:39+00:00
- **Authors**: Alireza Chamanzar, Yao Nie
- **Comment**: None
- **Journal**: None
- **Summary**: Cell detection and segmentation is fundamental for all downstream analysis of digital pathology images. However, obtaining the pixel-level ground truth for single cell segmentation is extremely labor intensive. To overcome this challenge, we developed an end-to-end deep learning algorithm to perform both single cell detection and segmentation using only point labels. This is achieved through the combination of different task orientated point label encoding methods and a multi-task scheduler for training. We apply and validate our algorithm on PMS2 stained colon rectal cancer and tonsil tissue images. Compared to the state-of-the-art, our algorithm shows significant improvement in cell detection and segmentation without increasing the annotation efforts.



### Deep Learning Models for Digital Pathology
- **Arxiv ID**: http://arxiv.org/abs/1910.12329v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.12329v2)
- **Published**: 2019-10-27 19:38:58+00:00
- **Updated**: 2019-10-29 01:35:20+00:00
- **Authors**: Aïcha BenTaieb, Ghassan Hamarneh
- **Comment**: Technical report, Survey, 58 pages, 5 figures
- **Journal**: None
- **Summary**: Histopathology images; microscopy images of stained tissue biopsies contain fundamental prognostic information that forms the foundation of pathological analysis and diagnostic medicine. However, diagnostics from histopathology images generally rely on a visual cognitive assessment of tissue slides which implies an inherent element of interpretation and hence subjectivity. Access to digitized histopathology images enabled the development of computational systems aiming at reducing manual intervention and automating parts of pathologists' workflow. Specifically, applications of deep learning to histopathology image analysis now offer opportunities for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However digitized histopathology tissue slides are unique in a variety of ways and come with their own set of computational challenges. In this survey, we summarize the different challenges facing computational systems for digital pathology and provide a review of state-of-the-art works that developed deep learning-based solutions for the predictive modeling of histopathology images from a detection, stain normalization, segmentation, and tissue classification perspective. We then discuss the challenges facing the validation and integration of such deep learning-based computational systems in clinical workflow and reflect on future opportunities for histopathology derived image measurements and better predictive modeling.



### Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab
- **Arxiv ID**: http://arxiv.org/abs/1910.13824v2
- **DOI**: 10.3929/ethz-b-000388707
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13824v2)
- **Published**: 2019-10-27 20:39:14+00:00
- **Updated**: 2019-11-21 10:10:48+00:00
- **Authors**: Henry Martin, Ye Hong, Dominik Bucher, Christian Rupprecht, René Buffat
- **Comment**: None
- **Journal**: None
- **Summary**: The goal of the IARAI competition traffic4cast was to predict the city-wide traffic status within a 15-minute time window, based on information from the previous hour. The traffic status was given as multi-channel images (one pixel roughly corresponds to 100x100 meters), where one channel indicated the traffic volume, another one the average speed of vehicles, and a third one their rough heading. As part of our work on the competition, we evaluated many different network architectures, analyzed the statistical properties of the given data in detail, and thought about how to transform the problem to be able to take additional spatio-temporal context-information into account, such as the street network, the positions of traffic lights, or the weather. This document summarizes our efforts that led to our best submission, and gives some insights about which other approaches we evaluated, and why they did not work as well as imagined.



### SENSE: a Shared Encoder Network for Scene-flow Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.12361v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12361v1)
- **Published**: 2019-10-27 21:44:00+00:00
- **Updated**: 2019-10-27 21:44:00+00:00
- **Authors**: Huaizu Jiang, Deqing Sun, Varun Jampani, Zhaoyang Lv, Erik Learned-Miller, Jan Kautz
- **Comment**: ICCV 2019 Oral
- **Journal**: International Conference on Computer Vision 2019
- **Summary**: We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.



### The Quo Vadis submission at Traffic4cast 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.12363v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12363v1)
- **Published**: 2019-10-27 21:50:30+00:00
- **Updated**: 2019-10-27 21:50:30+00:00
- **Authors**: Dan Oneata, Cosmin George Alexandru, Marius Stanescu, Octavian Pascu, Alexandru Magan, Adrian Postelnicu, Horia Cucu
- **Comment**: Extended abstract for the Traffic4cast competition from NeurIPS 2019
- **Journal**: None
- **Summary**: We describe the submission of the Quo Vadis team to the Traffic4cast competition, which was organized as part of the NeurIPS 2019 series of challenges. Our system consists of a temporal regression module, implemented as $1\times1$ 2d convolutions, augmented with spatio-temporal biases. We have found that using biases is a straightforward and efficient way to include seasonal patterns and to improve the performance of the temporal regression model. Our implementation obtains a mean squared error of $9.47\times 10^{-3}$ on the test data, placing us on the eight place team-wise. We also present our attempts at incorporating spatial correlations into the model; however, contrary to our expectations, adding this type of auxiliary information did not benefit the main system. Our code is available at https://github.com/danoneata/traffic4cast.



