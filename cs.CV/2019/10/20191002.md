# Arxiv Papers in cs.CV on 2019-10-02
### LIT: Light-field Inference of Transparency for Refractive Object Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.00721v4
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00721v4)
- **Published**: 2019-10-02 00:15:00+00:00
- **Updated**: 2020-03-23 17:22:39+00:00
- **Authors**: Zheming Zhou, Xiaotong Chen, Odest Chadwicke Jenkins
- **Comment**: None
- **Journal**: None
- **Summary**: Translucency is prevalent in everyday scenes. As such, perception of transparent objects is essential for robots to perform manipulation. Compared with texture-rich or texture-less Lambertian objects, transparency induces significant uncertainty on object appearances. Ambiguity can be due to changes in lighting, viewpoint, and backgrounds, each of which brings challenges to existing object pose estimation algorithms. In this work, we propose LIT, a two-stage method for transparent object pose estimation using light-field sensing and photorealistic rendering. LIT employs multiple filters specific to light-field imagery in deep networks to capture transparent material properties, with robust depth and pose estimators based on generative sampling. Along with the LIT algorithm, we introduce the light-field transparent object dataset ProLIT for the tasks of recognition, localization and pose estimation. With respect to this ProLIT dataset, we demonstrate that LIT can outperform both state-of-the-art end-to-end pose estimation methods and a generative pose estimator on transparent objects.



### Comparing Deep Learning Models for Multi-cell Classification in Liquid-based Cervical Cytology Images
- **Arxiv ID**: http://arxiv.org/abs/1910.00722v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.00722v1)
- **Published**: 2019-10-02 00:20:23+00:00
- **Updated**: 2019-10-02 00:20:23+00:00
- **Authors**: Sudhir Sornapudi, G. T. Brown, Zhiyun Xue, Rodney Long, Lisa Allen, Sameer Antani
- **Comment**: AMIA 2019 Annual Symposium, Washington DC
- **Journal**: AMIA Annu Symp Proc. 2019 (2019) 820-827
- **Summary**: Liquid-based cytology (LBC) is a reliable automated technique for the screening of Papanicolaou (Pap) smear data. It is an effective technique for collecting a majority of the cervical cells and aiding cytopathologists in locating abnormal cells. Most methods published in the research literature rely on accurate cell segmentation as a prior, which remains challenging due to a variety of factors, e.g., stain consistency, presence of clustered cells, etc. We propose a method for automatic classification of cervical slide images through generation of labeled cervical patch data and extracting deep hierarchical features by fine-tuning convolution neural networks, as well as a novel graph-based cell detection approach for cellular level evaluation. The results show that the proposed pipeline can classify images of both single cell and overlapping cells. The VGG-19 model is found to be the best at classifying the cervical cytology patch data with 95 % accuracy under precision-recall curve.



### A Pre-defined Sparse Kernel Based Convolution for Deep CNNs
- **Arxiv ID**: http://arxiv.org/abs/1910.00724v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CC
- **Links**: [PDF](http://arxiv.org/pdf/1910.00724v2)
- **Published**: 2019-10-02 00:38:38+00:00
- **Updated**: 2019-10-16 16:31:05+00:00
- **Authors**: Souvik Kundu, Saurav Prakash, Haleh Akrami, Peter A. Beerel, Keith M. Chugg
- **Comment**: 8 pages, 12 figures, Computer vision
- **Journal**: None
- **Summary**: The high demand for computational and storage resources severely impede the deployment of deep convolutional neural networks (CNNs) in limited-resource devices. Recent CNN architectures have proposed reduced complexity versions (e.g. SuffleNet and MobileNet) but at the cost of modest decreases inaccuracy. This paper proposes pSConv, a pre-defined sparse 2D kernel-based convolution, which promises significant improvements in the trade-off between complexity and accuracy for both CNN training and inference. To explore the potential of this approach, we have experimented with two widely accepted datasets, CIFAR-10 and Tiny ImageNet, in sparse variants of both the ResNet18 and VGG16 architectures. Our approach shows a parameter count reduction of up to 4.24x with modest degradation in classification accuracy relative to that of standard CNNs. Our approach outperforms a popular variant of ShuffleNet using a variant of ResNet18 with pSConv having 3x3 kernels with only four of nine elements not fixed at zero. In particular, the parameter count is reduced by 1.7x for CIFAR-10 and 2.29x for Tiny ImageNet with an increased accuracy of ~4%.



### NASS-AI: Towards Digitization of Parliamentary Bills using Document Level Embedding and Bidirectional Long Short-Term Memory
- **Arxiv ID**: http://arxiv.org/abs/1910.04865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, cs.IR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04865v1)
- **Published**: 2019-10-02 00:39:02+00:00
- **Updated**: 2019-10-02 00:39:02+00:00
- **Authors**: Adewale Akinfaderin, Olamilekan Wahab
- **Comment**: Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World
- **Journal**: None
- **Summary**: There has been several reports in the Nigerian and International media about the Senators and House of Representative Members of the Nigerian National Assembly (NASS) being the highest paid in the world. Despite this high-level of parliamentary compensation and a lack of oversight, most of the legislative duties like bills introduced and vote proceedings are shrouded in mystery without an open and annotated corpus. In this paper, we present results from ongoing research on the categorization of bills introduced in the Nigerian parliament since the fourth republic (1999 - 2018). For this task, we employed a multi-step approach which involves extracting text from scanned and embedded pdfs with low to medium quality using Optical Character Recognition (OCR) tools and labeling them into eight categories. We investigate the performance of document level embedding for feature representation of the extracted texts before using a Bidirectional Long Short-Term Memory (Bi-LSTM) for our classifier. The performance was further compared with other feature representation and machine learning techniques. We believe that these results are well-positioned to have a substantial impact on the quest to meet the basic open data charter principles.



### Animating Face using Disentangled Audio Representations
- **Arxiv ID**: http://arxiv.org/abs/1910.00726v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1910.00726v1)
- **Published**: 2019-10-02 00:47:19+00:00
- **Updated**: 2019-10-02 00:47:19+00:00
- **Authors**: Gaurav Mittal, Baoyuan Wang
- **Comment**: Accepted at WACV 2020 (Winter conference on Applications of Computer
  Vision)
- **Journal**: None
- **Summary**: All previous methods for audio-driven talking head generation assume the input audio to be clean with a neutral tone. As we show empirically, one can easily break these systems by simply adding certain background noise to the utterance or changing its emotional tone (to such as sad). To make talking head generation robust to such variations, we propose an explicit audio representation learning framework that disentangles audio sequences into various factors such as phonetic content, emotional tone, background noise and others. We conduct experiments to validate that conditioned on disentangled content representation, the generated mouth movement by our model is significantly more accurate than previous approaches (without disentangled learning) in the presence of noise and emotional variations. We further demonstrate that our framework is compatible with current state-of-the-art approaches by replacing their original audio learning component with ours. To our best knowledge, this is the first work which improves the performance of talking head generation from disentangled audio representation perspective, which is important for many real-world applications.



### Analyzing and Improving Neural Networks by Generating Semantic Counterexamples through Differentiable Rendering
- **Arxiv ID**: http://arxiv.org/abs/1910.00727v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00727v2)
- **Published**: 2019-10-02 00:47:57+00:00
- **Updated**: 2020-07-17 22:08:36+00:00
- **Authors**: Lakshya Jain, Varun Chandrasekaran, Uyeong Jang, Wilson Wu, Andrew Lee, Andy Yan, Steven Chen, Somesh Jha, Sanjit A. Seshia
- **Comment**: None
- **Journal**: None
- **Summary**: Even as deep neural networks (DNNs) have achieved remarkable success on vision-related tasks, their performance is brittle to transformations in the input. Of particular interest are semantic transformations that model changes that have a basis in the physical world, such as rotations, translations, changes in lighting or camera pose. In this paper, we show how differentiable rendering can be utilized to generate images that are informative, yet realistic, and which can be used to analyze DNN performance and improve its robustness through data augmentation. Given a differentiable renderer and a DNN, we show how to use off-the-shelf attacks from adversarial machine learning to generate semantic counterexamples -- images where semantic features are changed as to produce misclassifications or misdetections. We validate our approach on DNNs for image classification and object detection. For classification, we show that semantic counterexamples, when used to augment the dataset, (i) improve generalization performance (ii) enhance robustness to semantic transformations, and (iii) transfer between models. Additionally, in comparison to sampling-based semantic augmentation, our technique generates more informative data in a sample efficient manner.



### Boosting Image Recognition with Non-differentiable Constraints
- **Arxiv ID**: http://arxiv.org/abs/1910.00736v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00736v1)
- **Published**: 2019-10-02 01:21:21+00:00
- **Updated**: 2019-10-02 01:21:21+00:00
- **Authors**: Xuan Li, Yuchen Lu, Peng Xu, Jizong Peng, Christian Desrosiers, Xue Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of image recognition with non-differentiable constraints. A lot of real-life recognition applications require a rich output structure with deterministic constraints that are discrete or modeled by a non-differentiable function. A prime example is recognizing digit sequences, which are restricted by such rules (e.g., \textit{container code detection}, \textit{social insurance number recognition}, etc.). We investigate the usefulness of adding non-differentiable constraints in learning for the task of digit sequence recognition. Toward this goal, we synthesize six different datasets from MNIST and Cropped SVHN, with three discrete rules inspired by real-life protocols. To deal with the non-differentiability of these rules, we propose a reinforcement learning approach based on the policy gradient method. We find that incorporating this rule-based reinforcement can effectively increase the accuracy for all datasets and provide a good inductive bias which improves the model even with limited data. On one of the datasets, MNIST\_Rule2, models trained with rule-based reinforcement increase the accuracy by 4.7\% for 2000 samples and 23.6\% for 500 samples. We further test our model against synthesized adversarial examples, e.g., blocking out digits, and observe that adding our rule-based reinforcement increases the model robustness with a relatively smaller performance drop.



### Emergence of Writing Systems Through Multi-Agent Cooperation
- **Arxiv ID**: http://arxiv.org/abs/1910.00741v1
- **DOI**: 10.1609/aaai.v34i10.7243
- **Categories**: **cs.MA**, cs.CL, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1910.00741v1)
- **Published**: 2019-10-02 01:35:14+00:00
- **Updated**: 2019-10-02 01:35:14+00:00
- **Authors**: Shresth Verma, Joydip Dhar
- **Comment**: Under Review as Student Abstract at AAAI'20
- **Journal**: None
- **Summary**: Learning to communicate is considered an essential task to develop a general AI. While recent literature in language evolution has studied emergent language through discrete or continuous message symbols, there has been little work in the emergence of writing systems in artificial agents. In this paper, we present a referential game setup with two agents, where the mode of communication is a written language system that emerges during the play. We show that the agents can learn to coordinate successfully using this mode of communication. Further, we study how the game rules affect the writing system taxonomy by proposing a consistency metric.



### A Deep Factorization of Style and Structure in Fonts
- **Arxiv ID**: http://arxiv.org/abs/1910.00748v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00748v2)
- **Published**: 2019-10-02 02:24:12+00:00
- **Updated**: 2020-05-16 01:43:18+00:00
- **Authors**: Nikita Srivatsan, Jonathan T. Barron, Dan Klein, Taylor Berg-Kirkpatrick
- **Comment**: EMNLP 2019 (oral)
- **Journal**: None
- **Summary**: We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.



### Inferring and Improving Street Maps with Data-Driven Automation
- **Arxiv ID**: http://arxiv.org/abs/1910.04869v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04869v2)
- **Published**: 2019-10-02 02:30:46+00:00
- **Updated**: 2019-11-06 18:55:08+00:00
- **Authors**: Favyen Bastani, Songtao He, Satvat Jagwani, Edward Park, Sofiane Abbar, Mohammad Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden, Mohammad Amin Sadeghi
- **Comment**: None
- **Journal**: None
- **Summary**: Street maps are a crucial data source that help to inform a wide range of decisions, from navigating a city to disaster relief and urban planning. However, in many parts of the world, street maps are incomplete or lag behind new construction. Editing maps today involves a tedious process of manually tracing and annotating roads, buildings, and other map features.   Over the past decade, many automatic map inference systems have been proposed to automatically extract street map data from satellite imagery, aerial imagery, and GPS trajectory datasets. However, automatic map inference has failed to gain traction in practice due to two key limitations: high error rates (low precision), which manifest in noisy inference outputs, and a lack of end-to-end system design to leverage inferred data to update existing street maps.   At MIT and QCRI, we have developed a number of algorithms and approaches to address these challenges, which we combined into a new system we call Mapster. Mapster is a human-in-the-loop street map editing system that incorporates three components to robustly accelerate the mapping process over traditional tools and workflows: high-precision automatic map inference, data refinement, and machine-assisted map editing.   Through an evaluation on a large-scale dataset including satellite imagery, GPS trajectories, and ground-truth map data in forty cities, we show that Mapster makes automation practical for map editing, and enables the curation of map datasets that are more complete and up-to-date at less cost.



### Joint Learning of Semantic Alignment and Object Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.00754v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00754v1)
- **Published**: 2019-10-02 02:43:47+00:00
- **Updated**: 2019-10-02 02:43:47+00:00
- **Authors**: Sangryul Jeon, Dongbo Min, Seungryong Kim, Kwanghoon Sohn
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several standard benchmarks for semantic matching and landmark detection, including a newly introduced dataset, JLAD, which contains larger number of challenging image pairs than existing datasets.



### GLADAS: Gesture Learning for Advanced Driver Assistance Systems
- **Arxiv ID**: http://arxiv.org/abs/1910.04695v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04695v1)
- **Published**: 2019-10-02 03:55:45+00:00
- **Updated**: 2019-10-02 03:55:45+00:00
- **Authors**: Ethan Shaotran, Jonathan J. Cruz, Vijay Janapa Reddi
- **Comment**: 9 Pages, 7 Figures
- **Journal**: None
- **Summary**: Human-computer interaction (HCI) is crucial for the safety of lives as autonomous vehicles (AVs) become commonplace. Yet, little effort has been put toward ensuring that AVs understand humans on the road. In this paper, we present GLADAS, a simulator-based research platform designed to teach AVs to understand pedestrian hand gestures. GLADAS supports the training, testing, and validation of deep learning-based self-driving car gesture recognition systems. We focus on gestures as they are a primordial (i.e, natural and common) way to interact with cars. To the best of our knowledge, GLADAS is the first system of its kind designed to provide an infrastructure for further research into human-AV interaction. We also develop a hand gesture recognition algorithm for self-driving cars, using GLADAS to evaluate its performance. Our results show that an AV understands human gestures 85.91% of the time, reinforcing the need for further research into human-AV interaction.



### How does topology influence gradient propagation and model performance of deep networks with DenseNet-type skip connections?
- **Arxiv ID**: http://arxiv.org/abs/1910.00780v3
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.00780v3)
- **Published**: 2019-10-02 05:25:47+00:00
- **Updated**: 2021-04-01 00:04:30+00:00
- **Authors**: Kartikeya Bhardwaj, Guihong Li, Radu Marculescu
- **Comment**: Accepted at CVPR 2021
- **Journal**: None
- **Summary**: DenseNets introduce concatenation-type skip connections that achieve state-of-the-art accuracy in several computer vision tasks. In this paper, we reveal that the topology of the concatenation-type skip connections is closely related to the gradient propagation which, in turn, enables a predictable behavior of DNNs' test performance. To this end, we introduce a new metric called NN-Mass to quantify how effectively information flows through DNNs. Moreover, we empirically show that NN-Mass also works for other types of skip connections, e.g., for ResNets, Wide-ResNets (WRNs), and MobileNets, which contain addition-type skip connections (i.e., residuals or inverted residuals). As such, for both DenseNet-like CNNs and ResNets/WRNs/MobileNets, our theoretically grounded NN-Mass can identify models with similar accuracy, despite having significantly different size/compute requirements. Detailed experiments on both synthetic and real datasets (e.g., MNIST, CIFAR-10, CIFAR-100, ImageNet) provide extensive evidence for our insights. Finally, the closed-form equation of our NN-Mass enables us to design significantly compressed DenseNets (for CIFAR-10) and MobileNets (for ImageNet) directly at initialization without time-consuming training and/or searching.



### Global visual localization in LiDAR-maps through shared 2D-3D embedding space
- **Arxiv ID**: http://arxiv.org/abs/1910.04871v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04871v2)
- **Published**: 2019-10-02 09:59:00+00:00
- **Updated**: 2020-03-10 12:14:33+00:00
- **Authors**: Daniele Cattaneo, Matteo Vaghi, Simone Fontana, Augusto Luis Ballardini, Domenico Giorgio Sorrenti
- **Comment**: Accepted for presentation at IEEE ICRA 2020
- **Journal**: None
- **Summary**: Global localization is an important and widely studied problem for many robotic applications. Place recognition approaches can be exploited to solve this task, e.g., in the autonomous driving field. While most vision-based approaches match an image w.r.t. an image database, global visual localization within LiDAR-maps remains fairly unexplored, even though the path toward high definition 3D maps, produced mainly from LiDARs, is clear. In this work we leverage Deep Neural Network (DNN) approaches to create a shared embedding space between images and LiDAR-maps, allowing for image to 3D-LiDAR place recognition. We trained a 2D and a 3D DNN that create embeddings, respectively from images and from point clouds, that are close to each other whether they refer to the same place. An extensive experimental activity is presented to assess the effectiveness of the approach w.r.t. different learning paradigms, network architectures, and loss functions. All the evaluations have been performed using the Oxford Robotcar Dataset, which encompasses a wide range of weather and light conditions.



### Slanted Stixels: A way to represent steep streets
- **Arxiv ID**: http://arxiv.org/abs/1910.01466v1
- **DOI**: 10.1007/s11263-019-01226-9
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01466v1)
- **Published**: 2019-10-02 11:52:01+00:00
- **Updated**: 2019-10-02 11:52:01+00:00
- **Authors**: Daniel Hernandez-Juarez, Lukas Schneider, Pau Cebrian, Antonio Espinosa, David Vazquez, Antonio M. Lopez, Uwe Franke, Marc Pollefeys, Juan C. Moure
- **Comment**: Journal preprint (published in IJCV 2019:
  https://link.springer.com/article/10.1007/s11263-019-01226-9). arXiv admin
  note: text overlap with arXiv:1707.05397
- **Journal**: IJCV 2019
- **Summary**: This work presents and evaluates a novel compact scene representation based on Stixels that infers geometric and semantic information. Our approach overcomes the previous rather restrictive geometric assumptions for Stixels by introducing a novel depth model to account for non-flat roads and slanted objects. Both semantic and depth cues are used jointly to infer the scene representation in a sound global energy minimization formulation.   Furthermore, a novel approximation scheme is introduced in order to significantly reduce the computational complexity of the Stixel algorithm, and then achieve real-time computation capabilities. The idea is to first perform an over-segmentation of the image, discarding the unlikely Stixel cuts, and apply the algorithm only on the remaining Stixel cuts. This work presents a novel over-segmentation strategy based on a Fully Convolutional Network (FCN), which outperforms an approach based on using local extrema of the disparity map.   We evaluate the proposed methods in terms of semantic and geometric accuracy as well as run-time on four publicly available benchmark datasets. Our approach maintains accuracy on flat road scene datasets while improving substantially on a novel non-flat road dataset.



### Object Parsing in Sequences Using CoordConv Gated Recurrent Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.00895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00895v1)
- **Published**: 2019-10-02 11:58:16+00:00
- **Updated**: 2019-10-02 11:58:16+00:00
- **Authors**: Ayush Gaud, Y V S Harish, K Madhava Krishna
- **Comment**: None
- **Journal**: None
- **Summary**: We present a monocular object parsing framework for consistent keypoint localization by capturing temporal correlation on sequential data. In this paper, we propose a novel recurrent network based architecture to model long-range dependencies between intermediate features which are highly useful in tasks like keypoint localization and tracking. We leverage the expressiveness of the popular stacked hourglass architecture and augment it by adopting memory units between intermediate layers of the network with weights shared across stages for video frames. We observe that this weight sharing scheme not only enables us to frame hourglass architecture as a recurrent network but also prove to be highly effective in producing increasingly refined estimates for sequential tasks. Furthermore, we propose a new memory cell, we call CoordConvGRU which learns to selectively preserve spatio-temporal correlation and showcase our results on the keypoint localization task. The experiments show that our approach is able to model the motion dynamics between the frames and significantly outperforms the baseline hourglass network. Even though our network is trained on a synthetically rendered dataset, we observe that with minimal fine tuning on 300 real images we are able to achieve performance at par with various state-of-the-art methods trained with the same level of supervisory inputs. By using a simpler architecture than other methods enables us to run it in real time on a standard GPU which is desirable for such applications. Finally, we make our architectures and 524 annotated sequences of cars from KITTI dataset publicly available.



### Bio-Inspired Foveated Technique for Augmented-Range Vehicle Detection Using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.00944v1
- **DOI**: 10.1109/IJCNN.2019.8851947
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.00944v1)
- **Published**: 2019-10-02 13:37:58+00:00
- **Updated**: 2019-10-02 13:37:58+00:00
- **Authors**: Pedro Azevedo, Sabrina S. Panceri, Rânik Guidolini, Vinicius B. Cardoso, Claudine Badue, Thiago Oliveira-Santos, Alberto F. De Souza
- **Comment**: Paper accepted at IJCNN 2019
- **Journal**: None
- **Summary**: We propose a bio-inspired foveated technique to detect cars in a long range camera view using a deep convolutional neural network (DCNN) for the IARA self-driving car. The DCNN receives as input (i) an image, which is captured by a camera installed on IARA's roof; and (ii) crops of the image, which are centered in the waypoints computed by IARA's path planner and whose sizes increase with the distance from IARA. We employ an overlap filter to discard detections of the same car in different crops of the same image based on the percentage of overlap of detections' bounding boxes. We evaluated the performance of the proposed augmented-range vehicle detection system (ARVDS) using the hardware and software infrastructure available in the IARA self-driving car. Using IARA, we captured thousands of images of real traffic situations containing cars in a long range. Experimental results show that ARVDS increases the Average Precision (AP) of long range car detection from 29.51% (using a single whole image) to 63.15%.



### CNN-based Semantic Segmentation using Level Set Loss
- **Arxiv ID**: http://arxiv.org/abs/1910.00950v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00950v1)
- **Published**: 2019-10-02 13:45:33+00:00
- **Updated**: 2019-10-02 13:45:33+00:00
- **Authors**: Youngeun Kim, Seunghyeon Kim, Taekyung Kim, Changick Kim
- **Comment**: 2019 IEEE Winter Conference on Applications of Computer Vision
  (WACV). IEEE, 2019
- **Journal**: None
- **Summary**: Thesedays, Convolutional Neural Networks are widely used in semantic segmentation. However, since CNN-based segmentation networks produce low-resolution outputs with rich semantic information, it is inevitable that spatial details (e.g., small bjects and fine boundary information) of segmentation results will be lost. To address this problem, motivated by a variational approach to image segmentation (i.e., level set theory), we propose a novel loss function called the level set loss which is designed to refine spatial details of segmentation results. To deal with multiple classes in an image, we first decompose the ground truth into binary images. Note that each binary image consists of background and regions belonging to a class. Then we convert level set functions into class probability maps and calculate the energy for each class. The network is trained to minimize the weighted sum of the level set loss and the cross-entropy loss. The proposed level set loss improves the spatial details of segmentation results in a time and memory efficient way. Furthermore, our experimental results show that the proposed loss function achieves better performance than previous approaches.



### Road scenes analysis in adverse weather conditions by polarization-encoded images and adapted deep learning
- **Arxiv ID**: http://arxiv.org/abs/1910.04870v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04870v1)
- **Published**: 2019-10-02 13:47:46+00:00
- **Updated**: 2019-10-02 13:47:46+00:00
- **Authors**: Rachel Blin, Samia Ainouz, Stéphane Canu, Fabrice Meriaudeau
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection in road scenes is necessary to develop both autonomous vehicles and driving assistance systems. Even if deep neural networks for recognition task have shown great performances using conventional images, they fail to detect objects in road scenes in complex acquisition situations. In contrast, polarization images, characterizing the light wave, can robustly describe important physical properties of the object even under poor illumination or strong reflections. This paper shows how non-conventional polarimetric imaging modality overcomes the classical methods for object detection especially in adverse weather conditions. The efficiency of the proposed method is mostly due to the high power of the polarimetry to discriminate any object by its reflective properties and on the use of deep neural networks for object detection. Our goal by this work, is to prove that polarimetry brings a real added value compared with RGB images for object detection. Experimental results on our own dataset composed of road scene images taken during adverse weather conditions show that polarimetry together with deep learning can improve the state-of-the-art by about 20% to 50% on different detection tasks.



### Deep learning within a priori temporal feature spaces for large-scale dynamic MR image reconstruction: Application to 5-D cardiac MR Multitasking
- **Arxiv ID**: http://arxiv.org/abs/1910.00956v1
- **DOI**: 10.1007/978-3-030-32245-8_55
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00956v1)
- **Published**: 2019-10-02 13:52:38+00:00
- **Updated**: 2019-10-02 13:52:38+00:00
- **Authors**: Yuhua Chen, Jaime L. Shaw, Yibin Xie, Debiao Li, Anthony G. Christodoulou
- **Comment**: Early accepted by MICCAI 2019
- **Journal**: Medical Image Computing and Computer Assisted Intervention 2019 pp
  495-504
- **Summary**: High spatiotemporal resolution dynamic magnetic resonance imaging (MRI) is a powerful clinical tool for imaging moving structures as well as to reveal and quantify other physical and physiological dynamics. The low speed of MRI necessitates acceleration methods such as deep learning reconstruction from under-sampled data. However, the massive size of many dynamic MRI problems prevents deep learning networks from directly exploiting global temporal relationships. In this work, we show that by applying deep neural networks inside a priori calculated temporal feature spaces, we enable deep learning reconstruction with global temporal modeling even for image sequences with >40,000 frames. One proposed variation of our approach using dilated multi-level Densely Connected Network (mDCN) speeds up feature space coordinate calculation by 3000x compared to conventional iterative methods, from 20 minutes to 0.39 seconds. Thus, the combination of low-rank tensor and deep learning models not only makes large-scale dynamic MRI feasible but also practical for routine clinical application.



### Privacy-preserving Federated Brain Tumour Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.00962v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00962v1)
- **Published**: 2019-10-02 14:02:06+00:00
- **Updated**: 2019-10-02 14:02:06+00:00
- **Authors**: Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M. Jorge Cardoso, Andrew Feng
- **Comment**: MICCAI MLMI 2019
- **Journal**: None
- **Summary**: Due to medical data privacy regulations, it is often infeasible to collect and share patient data in a centralised data lake. This poses challenges for training machine learning algorithms, such as deep convolutional networks, which often require large numbers of diverse training examples. Federated learning sidesteps this difficulty by bringing code to the patient data owners and only sharing intermediate model training updates among them. Although a high-accuracy model could be achieved by appropriately aggregating these model updates, the model shared could indirectly leak the local training examples. In this paper, we investigate the feasibility of applying differential-privacy techniques to protect the patient data in a federated learning setup. We implement and evaluate practical federated learning systems for brain tumour segmentation on the BraTS dataset. The experimental results show that there is a trade-off between model performance and privacy protection costs.



### ConfusionFlow: A model-agnostic visualization for temporal analysis of classifier confusion
- **Arxiv ID**: http://arxiv.org/abs/1910.00969v3
- **DOI**: 10.1109/TVCG.2020.3012063
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00969v3)
- **Published**: 2019-10-02 14:18:09+00:00
- **Updated**: 2020-07-02 17:01:55+00:00
- **Authors**: Andreas Hinterreiter, Peter Ruch, Holger Stitz, Martin Ennemoser, Jürgen Bernard, Hendrik Strobelt, Marc Streit
- **Comment**: Changes compared to previous version: Reintroduced NN pruning use
  case; restructured Evaluation section; several additional minor revisions.
  Submitted as Minor Revision to IEEE TVCG on 2020-07-02
- **Journal**: None
- **Summary**: Classifiers are among the most widely used supervised machine learning algorithms. Many classification models exist, and choosing the right one for a given task is difficult. During model selection and debugging, data scientists need to assess classifiers' performances, evaluate their learning behavior over time, and compare different models. Typically, this analysis is based on single-number performance measures such as accuracy. A more detailed evaluation of classifiers is possible by inspecting class errors. The confusion matrix is an established way for visualizing these class errors, but it was not designed with temporal or comparative analysis in mind. More generally, established performance analysis systems do not allow a combined temporal and comparative analysis of class-level information. To address this issue, we propose ConfusionFlow, an interactive, comparative visualization tool that combines the benefits of class confusion matrices with the visualization of performance characteristics over time. ConfusionFlow is model-agnostic and can be used to compare performances for different model types, model architectures, and/or training and test datasets. We demonstrate the usefulness of ConfusionFlow in a case study on instance selection strategies in active learning. We further assess the scalability of ConfusionFlow and present a use case in the context of neural network pruning.



### Learning Continuous 3D Reconstructions for Geometrically Aware Grasping
- **Arxiv ID**: http://arxiv.org/abs/1910.00983v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.00983v2)
- **Published**: 2019-10-02 14:40:16+00:00
- **Updated**: 2020-03-18 04:25:35+00:00
- **Authors**: Mark Van der Merwe, Qingkai Lu, Balakumar Sundaralingam, Martin Matak, Tucker Hermans
- **Comment**: IEEE Conference on Robotics and Automation 2020 (ICRA 2020)
  Camera-Ready. Includes updated experiments from initial submission
- **Journal**: None
- **Summary**: Deep learning has enabled remarkable improvements in grasp synthesis for previously unseen objects from partial object views. However, existing approaches lack the ability to explicitly reason about the full 3D geometry of the object when selecting a grasp, relying on indirect geometric reasoning derived when learning grasp success networks. This abandons explicit geometric reasoning, such as avoiding undesired robot object collisions. We propose to utilize a novel, learned 3D reconstruction to enable geometric awareness in a grasping system. We leverage the structure of the reconstruction network to learn a grasp success classifier which serves as the objective function for a continuous grasp optimization. We additionally explicitly constrain the optimization to avoid undesired contact, directly using the reconstruction. We examine the role of geometry in grasping both in the training of grasp metrics and through 96 robot grasping trials. Our results can be found on https://sites.google.com/view/reconstruction-grasp/.



### Unsupervised Doodling and Painting with Improved SPIRAL
- **Arxiv ID**: http://arxiv.org/abs/1910.01007v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, I.2; I.4
- **Links**: [PDF](http://arxiv.org/pdf/1910.01007v1)
- **Published**: 2019-10-02 15:12:06+00:00
- **Updated**: 2019-10-02 15:12:06+00:00
- **Authors**: John F. J. Mellor, Eunbyung Park, Yaroslav Ganin, Igor Babuschkin, Tejas Kulkarni, Dan Rosenbaum, Andy Ballard, Theophane Weber, Oriol Vinyals, S. M. Ali Eslami
- **Comment**: See https://learning-to-paint.github.io for an interactive version of
  this paper, with videos
- **Journal**: None
- **Summary**: We investigate using reinforcement learning agents as generative models of images (extending arXiv:1804.01118). A generative agent controls a simulated painting environment, and is trained with rewards provided by a discriminator network simultaneously trained to assess the realism of the agent's samples, either unconditional or reconstructions. Compared to prior work, we make a number of improvements to the architectures of the agents and discriminators that lead to intriguing and at times surprising results. We find that when sufficiently constrained, generative agents can learn to produce images with a degree of visual abstraction, despite having only ever seen real photographs (no human brush strokes). And given enough time with the painting environment, they can produce images with considerable realism. These results show that, under the right circumstances, some aspects of human drawing can emerge from simulated embodiment, without the need for external supervision, imitation or social cues. Finally, we note the framework's potential for use in creative applications.



### Empirical evaluation of full-reference image quality metrics on MDID database
- **Arxiv ID**: http://arxiv.org/abs/1910.01050v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01050v2)
- **Published**: 2019-10-02 16:13:31+00:00
- **Updated**: 2019-10-04 18:08:01+00:00
- **Authors**: Domonkos Varga
- **Comment**: None
- **Journal**: None
- **Summary**: In this study, our goal is to give a comprehensive evaluation of 32 state-of-the-art FR-IQA metrics using the recently published MDID. This database contains distorted images derived from a set of reference, pristine images using random types and levels of distortions. Specifically, Gaussian noise, Gaussian blur, contrast change, JPEG noise, and JPEG2000 noise were considered.



### Estimating localized complexity of white-matter wiring with GANs
- **Arxiv ID**: http://arxiv.org/abs/1910.04868v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04868v2)
- **Published**: 2019-10-02 16:45:32+00:00
- **Updated**: 2019-12-01 02:48:04+00:00
- **Authors**: Haraldur T. Hallgrimsson, Richika Sharan, Scott T. Grafton, Ambuj K. Singh
- **Comment**: Three page extended abstract, accepted to Medical Imaging meets
  NeurIPS 2019 workshop
- **Journal**: None
- **Summary**: In-vivo examination of the physical connectivity of axonal projections through the white matter of the human brain is made possible by diffusion weighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers derived scalar metrics such as fractional anisotrophy as proxies for "white matter integrity," and differences of such measures have been observed as significantly correlating with various neurological diagnosis and clinical measures such as executive function, presence of multiple sclerosis, and genetic similarity. The analysis of such voxel measures is confounded in areas of more complicated fiber wiring due to crossing, kissing, and dispersing fibers. Recently, Volz et al. introduced a simple probabilistic measure of the count of distinct fiber populations within a voxel, which was shown to reduce variance in group comparisons. We propose a complementary measure that considers the complexity of a voxel in context of its local region, with an aim to quantify the localized wiring complexity of every part of white matter. This allows, for example, identification of particularly ambiguous regions of the brain for tractographic approaches of modeling global wiring connectivity. Our method builds on recent advances in image inpainting, in which the task is to plausibly fill in a missing region of an image. Our proposed method builds on a Bayesian estimate of heteroscedastic aleatoric uncertainty of a region of white matter by inpainting it from its context. We define the localized wiring complexity of white matter as how accurately and confidently a well-trained model can predict the missing patch. In our results, we observe low aleatoric uncertainty along major neuronal pathways which increases at junctions and towards cortex boundaries. This directly quantifies the difficulty of lesion inpainting of dMRI images at all parts of white matter.



### Deep 3D Pan via adaptive "t-shaped" convolutions with global and local adaptive dilations
- **Arxiv ID**: http://arxiv.org/abs/1910.01089v3
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01089v3)
- **Published**: 2019-10-02 17:09:58+00:00
- **Updated**: 2019-10-21 01:52:19+00:00
- **Authors**: Juan Luis Gonzalez Bello, Munchurl Kim
- **Comment**: Check our video at https://www.youtube.com/watch?v=o0b-e282Rt4
- **Journal**: None
- **Summary**: Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or Deep 3D Pan, with "t-shaped" adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel "t-shaped" adaptive kernel with globally and locally adaptive dilation, which can efficiently incorporate global camera shift into and handle local 3D geometries of the target image's pixels for the synthesis of naturally looking 3D panned views when a 2-D input image is given. Extensive experiments were performed on the KITTI, CityScapes and our VICLAB_STEREO indoors dataset to prove the efficacy of our method. Our monster-net significantly outperforms the state-of-the-art method, SOTA, by a large margin in all metrics of RMSE, PSNR, and SSIM. Our proposed monster-net is capable of reconstructing more reliable image structures in synthesized images with coherent geometry. Moreover, the disparity information that can be extracted from the "t-shaped" kernel is much more reliable than that of the SOTA for the unsupervised monocular depth estimation task, confirming the effectiveness of our method.



### W-Net: A CNN-based Architecture for White Blood Cells Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.01091v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.01091v1)
- **Published**: 2019-10-02 17:13:42+00:00
- **Updated**: 2019-10-02 17:13:42+00:00
- **Authors**: Changhun Jung, Mohammed Abuhamad, Jumabek Alikhanov, Aziz Mohaisen, Kyungja Han, DaeHun Nyang
- **Comment**: None
- **Journal**: None
- **Summary**: Computer-aided methods for analyzing white blood cells (WBC) have become widely popular due to the complexity of the manual process. Recent works have shown highly accurate segmentation and detection of white blood cells from microscopic blood images. However, the classification of the observed cells is still a challenge and highly demanded as the distribution of the five types reflects on the condition of the immune system. This work proposes W-Net, a CNN-based method for WBC classification. We evaluate W-Net on a real-world large-scale dataset, obtained from The Catholic University of Korea, that includes 6,562 real images of the five WBC types. W-Net achieves an average accuracy of 97%.



### OpenVSLAM: A Versatile Visual SLAM Framework
- **Arxiv ID**: http://arxiv.org/abs/1910.01122v3
- **DOI**: 10.1145/3343031.3350539
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.01122v3)
- **Published**: 2019-10-02 18:00:01+00:00
- **Updated**: 2023-04-06 12:34:01+00:00
- **Authors**: Shinya Sumikura, Mikiya Shibuya, Ken Sakurada
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce OpenVSLAM, a visual SLAM framework with high usability and extensibility. Visual SLAM systems are essential for AR devices, autonomous control of robots and drones, etc. However, conventional open-source visual SLAM frameworks are not appropriately designed as libraries called from third-party programs. To overcome this situation, we have developed a novel visual SLAM framework. This software is designed to be easily used and extended. It incorporates several useful features and functions for research and development.



### Review of Learning-based Longitudinal Motion Planning for Autonomous Vehicles: Research Gaps between Self-driving and Traffic Congestion
- **Arxiv ID**: http://arxiv.org/abs/1910.06070v2
- **DOI**: None
- **Categories**: **eess.SP**, cs.CV, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.06070v2)
- **Published**: 2019-10-02 19:19:48+00:00
- **Updated**: 2021-07-17 15:37:49+00:00
- **Authors**: Hao Zhou, Jorge Laval, Anye Zhou, Yu Wang, Wenchao Wu, Zhu Qing, Srinivas Peeta
- **Comment**: submitted to presentation at TRB 2020
- **Journal**: None
- **Summary**: Self-driving technology companies and the research community are accelerating their pace to use machine learning longitudinal motion planning (mMP) for autonomous vehicles (AVs). This paper reviews the current state of the art in mMP, with an exclusive focus on its impact on traffic congestion. We identify the availability of congestion scenarios in current datasets, and summarize the required features for training mMP. For learning methods, we survey the major methods in both imitation learning and non-imitation learning. We also highlight the emerging technologies adopted by some leading AV companies, e.g. Tesla, Waymo, and Comma.ai. We find that: i) the AV industry has been mostly focusing on the long tail problem related to safety and overlooked the impact on traffic congestion, ii) the current public self-driving datasets have not included enough congestion scenarios, and mostly lack the necessary input features/output labels to train mMP, and iii) albeit reinforcement learning (RL) approach can integrate congestion mitigation into the learning goal, the major mMP method adopted by industry is still behavior cloning (BC), whose capability to learn a congestion-mitigating mMP remains to be seen. Based on the review, the study identifies the research gaps in current mMP development. Some suggestions towards congestion mitigation for future mMP studies are proposed: i) enrich data collection to facilitate the congestion learning, ii) incorporate non-imitation learning methods to combine traffic efficiency into a safety-oriented technical route, and iii) integrate domain knowledge from the traditional car following (CF) theory to improve the string stability of mMP.



### A Geometric Approach to Online Streaming Feature Selection
- **Arxiv ID**: http://arxiv.org/abs/1910.01182v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01182v2)
- **Published**: 2019-10-02 19:36:46+00:00
- **Updated**: 2020-03-16 04:49:21+00:00
- **Authors**: Salimeh Yasaei Sekeh, Madan Ravi Ganesh, Shurjo Banerjee, Jason J. Corso, Alfred O. Hero
- **Comment**: 10 page, 5 figures, 4 tables
- **Journal**: None
- **Summary**: Online Streaming Feature Selection (OSFS) is a sequential learning problem where individual features across all samples are made available to algorithms in a streaming fashion. In this work, firstly, we assert that OSFS's main assumption of having data from all the samples available at runtime is unrealistic and introduce a new setting where features and samples are streamed concurrently called OSFS with Streaming Samples (OSFS-SS). Secondly, the primary OSFS method, SAOLA utilizes an unbounded mutual information measure and requires multiple comparison steps between the stored and incoming feature sets to evaluate a feature's importance. We introduce Geometric Online Adaption, an algorithm that requires relatively less feature comparison steps and uses a bounded conditional geometric dependency measure. Our algorithm outperforms several OSFS baselines including SAOLA on a variety of datasets. We also extend SAOLA to work in the OSFS-SS setting and show that GOA continues to achieve the best results. Thirdly, the current paradigm of the OSFS algorithm comparison is flawed. Algorithms are measured by comparing the number of features used and the accuracy obtained by the learner, two properties that are fundamentally at odds with one another. Without fixing a limit on either of these properties, the qualities of features obtained by different algorithms are incomparable. We try to rectify this inconsistency by fixing the maximum number of features available to the learner and comparing algorithms in terms of their accuracy. Additionally, we characterize the behaviour of SAOLA and GOA on feature sets derived from popular deep convolutional featurizers.



### Automatic Group Cohesiveness Detection With Multi-modal Features
- **Arxiv ID**: http://arxiv.org/abs/1910.01197v1
- **DOI**: 10.1145/3340555.3355716
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01197v1)
- **Published**: 2019-10-02 20:04:42+00:00
- **Updated**: 2019-10-02 20:04:42+00:00
- **Authors**: Bin Zhu, Xin Guo, Kenneth Barner, Charles Boncelet
- **Comment**: None
- **Journal**: None
- **Summary**: Group cohesiveness is a compelling and often studied composition in group dynamics and group performance. The enormous number of web images of groups of people can be used to develop an effective method to detect group cohesiveness. This paper introduces an automatic group cohesiveness prediction method for the 7th Emotion Recognition in the Wild (EmotiW 2019) Grand Challenge in the category of Group-based Cohesion Prediction. The task is to predict the cohesive level for a group of people in images. To tackle this problem, a hybrid network including regression models which are separately trained on face features, skeleton features, and scene features is proposed. Predicted regression values, corresponding to each feature, are fused for the final cohesive intensity. Experimental results demonstrate that the proposed hybrid network is effective and makes promising improvements. A mean squared error (MSE) of 0.444 is achieved on the testing sets which outperforms the baseline MSE of 0.5.



### Using Image Priors to Improve Scene Understanding
- **Arxiv ID**: http://arxiv.org/abs/1910.01198v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01198v1)
- **Published**: 2019-10-02 20:08:26+00:00
- **Updated**: 2019-10-02 20:08:26+00:00
- **Authors**: Brigit Schroeder, Hanlin Tang, Alexandre Alahi
- **Comment**: Accepted to Women in Computer Vision (WiCV) Workshop at CVPR 2019
- **Journal**: None
- **Summary**: Semantic segmentation algorithms that can robustly segment objects across multiple camera viewpoints are crucial for assuring navigation and safety in emerging applications such as autonomous driving. Existing algorithms treat each image in isolation, but autonomous vehicles often revisit the same locations or maintain information from the immediate past. We propose a simple yet effective method for leveraging these image priors to improve semantic segmentation of images from sequential driving datasets. We examine several methods to fuse these temporal scene priors, and introduce a prior fusion network that is able to learn how to transfer this information. The prior fusion model improves the accuracy over the non-prior baseline from 69.1% to 73.3% for dynamic classes, and from 88.2% to 89.1% for static classes. Compared to models such as FCN-8, our prior method achieves the same accuracy with 5 times fewer parameters. We used a simple encoder decoder backbone, but this general prior fusion method could be applied to more complex semantic segmentation backbones. We also discuss how structured representations of scenes in the form of a scene graph could be leveraged as priors to further improve scene understanding.



### Embodied Language Grounding with 3D Visual Feature Representations
- **Arxiv ID**: http://arxiv.org/abs/1910.01210v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.01210v3)
- **Published**: 2019-10-02 20:37:27+00:00
- **Updated**: 2021-06-17 21:31:18+00:00
- **Authors**: Mihir Prabhudesai, Hsiao-Yu Fish Tung, Syed Ashar Javed, Maximilian Sieb, Adam W. Harley, Katerina Fragkiadaki
- **Comment**: None
- **Journal**: Conference on Computer Vision and Pattern Recognition. 2020, pp.
  2220-2229
- **Summary**: We propose associating language utterances to 3D visual abstractions of the scene they describe. The 3D visual abstractions are encoded as 3-dimensional visual feature maps. We infer these 3D visual scene feature maps from RGB images of the scene via view prediction: when the generated 3D scene feature map is neurally projected from a camera viewpoint, it should match the corresponding RGB image. We present generative models that condition on the dependency tree of an utterance and generate a corresponding visual 3D feature map as well as reason about its plausibility, and detector models that condition on both the dependency tree of an utterance and a related image and localize the object referents in the 3D feature map inferred from the image. Our model outperforms models of language and vision that associate language with 2D CNN activations or 2D images by a large margin in a variety of tasks, such as, classifying plausibility of utterances, detecting referential expressions, and supplying rewards for trajectory optimization of object placement policies from language instructions. We perform numerous ablations and show the improved performance of our detectors is due to its better generalization across camera viewpoints and lack of object interferences in the inferred 3D feature space, and the improved performance of our generators is due to their ability to spatially reason about objects and their configurations in 3D when mapping from language to scenes.



### Attacking Vision-based Perception in End-to-End Autonomous Driving Models
- **Arxiv ID**: http://arxiv.org/abs/1910.01907v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.RO, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01907v1)
- **Published**: 2019-10-02 20:56:55+00:00
- **Updated**: 2019-10-02 20:56:55+00:00
- **Authors**: Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang
- **Comment**: under review in the Journal of Systems Architecture 2019. arXiv admin
  note: substantial text overlap with arXiv:1903.05157
- **Journal**: None
- **Summary**: Recent advances in machine learning, especially techniques such as deep neural networks, are enabling a range of emerging applications. One such example is autonomous driving, which often relies on deep learning for perception. However, deep learning-based perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images. Nevertheless, the vast majority of such demonstrations focus on perception that is disembodied from end-to-end control. We present novel end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective. We define several objective functions that quantify the success of an attack and develop techniques based on Bayesian Optimization to efficiently traverse the search space of higher dimensional attacks. Additionally, we define a novel class of hijacking attacks, where painted lines on the road cause the driver-less car to follow a target path. Through the use of network deconvolution, we provide insights into the successful attacks, which appear to work by mimicking activations of entirely different scenarios. Our code is available at https://github.com/xz-group/AdverseDrive



### IIITM Face: A Database for Facial Attribute Detection in Constrained and Simulated Unconstrained Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.01219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01219v1)
- **Published**: 2019-10-02 21:03:44+00:00
- **Updated**: 2019-10-02 21:03:44+00:00
- **Authors**: Raj Kuwar Gupta, Shresth Verma, KV Arya, Soumya Agarwal, Prince Gupta
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses the challenges of face attribute detection specifically in the Indian context. While there are numerous face datasets in unconstrained environments, none of them captures emotions in different face orientations. Moreover, there is an under-representation of people of Indian ethnicity in these datasets since they have been scraped from popular search engines. As a result, the performance of state-of-the-art techniques can't be evaluated on Indian faces. In this work, we introduce a new dataset, IIITM Face, for the scientific community to address these challenges. Our dataset includes 107 participants who exhibit 6 emotions in 3 different face orientations. Each of these images is further labelled on attributes like gender, presence of moustache, beard or eyeglasses, clothes worn by the subjects and the density of their hair. Moreover, the images are captured in high resolution with specific background colors which can be easily replaced by cluttered backgrounds to simulate `in the Wild' behaviour. We demonstrate the same by constructing IIITM Face-SUE. Both IIITM Face and IIITM Face-SUE have been benchmarked across key multi-label metrics for the research community to compare their results.



### ROMark: A Robust Watermarking System Using Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1910.01221v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01221v1)
- **Published**: 2019-10-02 21:05:05+00:00
- **Updated**: 2019-10-02 21:05:05+00:00
- **Authors**: Bingyang Wen, Sergul Aydore
- **Comment**: 5 pages, 1 figure, Machine Learning with Guarantees workshop at
  NeurIPS 2019
- **Journal**: None
- **Summary**: The availability and easy access to digital communication increase the risk of copyrighted material piracy. In order to detect illegal use or distribution of data, digital watermarking has been proposed as a suitable tool. It protects the copyright of digital content by embedding imperceptible information into the data in the presence of an adversary. The goal of the adversary is to remove the copyrighted content of the data. Therefore, an efficient watermarking framework must be robust to multiple image-processing operations known as attacks that can alter embedded copyright information. Another line of research \textit{adversarial machine learning} also tackles with similar problems to guarantee robustness to imperceptible perturbations of the input. In this work, we propose to apply robust optimization from adversarial machine learning to improve the robustness of a CNN-based watermarking framework. Our experimental results on the COCO dataset show that the robustness of a watermarking framework can be improved by utilizing robust optimization in training.



### DeepMark: One-Shot Clothing Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.01225v1
- **DOI**: 10.1109/ICCVW.2019.00399
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01225v1)
- **Published**: 2019-10-02 21:18:17+00:00
- **Updated**: 2019-10-02 21:18:17+00:00
- **Authors**: Alexey Sidnev, Alexey Trushkov, Maxim Kazakov, Ivan Korolev, Vladislav Sorokin
- **Comment**: Published in ICCV 2019 Workshop
- **Journal**: None
- **Summary**: The one-shot approach, DeepMark, for fast clothing detection as a modification of a multi-target network, CenterNet, is proposed in the paper. The state-of-the-art accuracy of 0.723 mAP for bounding box detection task and 0.532 mAP for landmark detection task on the DeepFashion2 Challenge dataset were achieved. The proposed architecture can be used effectively on the low-power devices.



### Weakly supervised segmentation from extreme points
- **Arxiv ID**: http://arxiv.org/abs/1910.01236v1
- **DOI**: 10.1007/978-3-030-33642-4_5
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01236v1)
- **Published**: 2019-10-02 21:54:00+00:00
- **Updated**: 2019-10-02 21:54:00+00:00
- **Authors**: Holger Roth, Ling Zhang, Dong Yang, Fausto Milletari, Ziyue Xu, Xiaosong Wang, Daguang Xu
- **Comment**: Accepted at the MICCAI Workshop for Large-scale Annotation of
  Biomedical data and Expert Label Synthesis, Shenzen, China, 2019
- **Journal**: LABELS 2019, HAL-MICCAI 2019, CuRIOUS 2019. Lecture Notes in
  Computer Science, vol 11851. Springer, Cham
- **Summary**: Annotation of medical images has been a major bottleneck for the development of accurate and robust machine learning models. Annotation is costly and time-consuming and typically requires expert knowledge, especially in the medical domain. Here, we propose to use minimal user interaction in the form of extreme point clicks in order to train a segmentation model that can, in turn, be used to speed up the annotation of medical images. We use extreme points in each dimension of a 3D medical image to constrain an initial segmentation based on the random walker algorithm. This segmentation is then used as a weak supervisory signal to train a fully convolutional network that can segment the organ of interest based on the provided user clicks. We show that the network's predictions can be refined through several iterations of training and prediction using the same weakly annotated data. Ultimately, our method has the potential to speed up the generation process of new training datasets for the development of new machine learning and deep learning-based models for, but not exclusively, medical image analysis.



### Learning Dense Wide Baseline Stereo Matching for People
- **Arxiv ID**: http://arxiv.org/abs/1910.01241v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.01241v1)
- **Published**: 2019-10-02 22:24:11+00:00
- **Updated**: 2019-10-02 22:24:11+00:00
- **Authors**: Akin Caliskan, Armin Mustafa, Evren Imre, Adrian Hilton
- **Comment**: To appear in 3D Reconstruction in the Wild Workshop, ICCV 2019
- **Journal**: None
- **Summary**: Existing methods for stereo work on narrow baseline image pairs giving limited performance between wide baseline views. This paper proposes a framework to learn and estimate dense stereo for people from wide baseline image pairs. A synthetic people stereo patch dataset (S2P2) is introduced to learn wide baseline dense stereo matching for people. The proposed framework not only learns human specific features from synthetic data but also exploits pooling layer and data augmentation to adapt to real data. The network learns from the human specific stereo patches from the proposed dataset for wide-baseline stereo estimation. In addition to patch match learning, a stereo constraint is introduced in the framework to solve wide baseline stereo reconstruction of humans. Quantitative and qualitative performance evaluation against state-of-the-art methods of proposed method demonstrates improved wide baseline stereo reconstruction on challenging datasets. We show that it is possible to learn stereo matching from synthetic people dataset and improve performance on real datasets for stereo reconstruction of people from narrow and wide baseline stereo data.



### Cardiac Segmentation of LGE MRI with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1910.01242v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.01242v1)
- **Published**: 2019-10-02 22:26:20+00:00
- **Updated**: 2019-10-02 22:26:20+00:00
- **Authors**: Holger Roth, Wentao Zhu, Dong Yang, Ziyue Xu, Daguang Xu
- **Comment**: Accepted at the MICCAI Workshop Statistical Atlases and Computational
  Modeling of the Heart (STACOM), MS-CMRSeg 2019: Multi-sequence Cardiac MR
  Segmentation Challenge, Shenzen, China
- **Journal**: None
- **Summary**: In this work, we attempt the segmentation of cardiac structures in late gadolinium-enhanced (LGE) magnetic resonance images (MRI) using only minimal supervision in a two-step approach. In the first step, we register a small set of five LGE cardiac magnetic resonance (CMR) images with ground truth labels to a set of 40 target LGE CMR images without annotation. Each manually annotated ground truth provides labels of the myocardium and the left ventricle (LV) and right ventricle (RV) cavities, which are used as atlases. After multi-atlas label fusion by majority voting, we possess noisy labels for each of the targeted LGE images. A second set of manual labels exists for 30 patients of the target LGE CMR images, but are annotated on different MRI sequences (bSSFP and T2-weighted). Again, we use multi-atlas label fusion with a consistency constraint to further refine our noisy labels if additional annotations in other modalities are available for a given patient. In the second step, we train a deep convolutional network for semantic segmentation on the target data while using data augmentation techniques to avoid over-fitting to the noisy labels. After inference and simple post-processing, we achieve our final segmentation for the targeted LGE CMR images, resulting in an average Dice of 0.890, 0.780, and 0.844 for LV cavity, LV myocardium, and RV cavity, respectively.



### Emotion Recognition with Spatial Attention and Temporal Softmax Pooling
- **Arxiv ID**: http://arxiv.org/abs/1910.01254v2
- **DOI**: 10.1007/978-3-030-27202-9_29
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01254v2)
- **Published**: 2019-10-02 23:53:10+00:00
- **Updated**: 2019-10-04 02:52:19+00:00
- **Authors**: Masih Aminbeidokhti, Marco Pedersoli, Patrick Cardinal, Eric Granger
- **Comment**: 9 pages; 2 figures; 2 tables; Best paper award at ICIAR 2019
- **Journal**: None
- **Summary**: Video-based emotion recognition is a challenging task because it requires to distinguish the small deformations of the human face that represent emotions, while being invariant to stronger visual differences due to different identities. State-of-the-art methods normally use complex deep learning models such as recurrent neural networks (RNNs, LSTMs, GRUs), convolutional neural networks (CNNs, C3D, residual networks) and their combination. In this paper, we propose a simpler approach that combines a CNN pre-trained on a public dataset of facial images with (1) a spatial attention mechanism, to localize the most important regions of the face for a given emotion, and (2) temporal softmax pooling, to select the most important frames of the given video. Results on the challenging EmotiW dataset show that this approach can achieve higher accuracy than more complex approaches.



### Distillation $\approx$ Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.01255v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.IT, cs.LG, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/1910.01255v1)
- **Published**: 2019-10-02 23:53:39+00:00
- **Updated**: 2019-10-02 23:53:39+00:00
- **Authors**: Bin Dong, Jikai Hou, Yiping Lu, Zhihua Zhang
- **Comment**: Accepted by NeurIPS 2019 Workshop on Machine Learning with
  Guarantees. Submitted to other places
- **Journal**: None
- **Summary**: Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is "early stopping". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.



