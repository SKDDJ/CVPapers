# Arxiv Papers in cs.CV on 2019-10-29
### Best Practices for Convolutional Neural Networks Applied to Object Recognition in Images
- **Arxiv ID**: http://arxiv.org/abs/1910.13029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13029v1)
- **Published**: 2019-10-29 01:18:15+00:00
- **Updated**: 2019-10-29 01:18:15+00:00
- **Authors**: Anderson de Andrade
- **Comment**: None
- **Journal**: None
- **Summary**: This research project studies the impact of convolutional neural networks (CNN) in image classification tasks. We explore different architectures and training configurations with the use of ReLUs, Nesterov's accelerated gradient, dropout and maxout networks. We work with the CIFAR-10 dataset as part of a Kaggle competition to identify objects in images. Initial results show that CNNs outperform our baseline by acting as invariant feature detectors. Comparisons between different preprocessing procedures show better results for global contrast normalization and ZCA whitening. ReLUs are much faster than tanh units and outperform sigmoids. We provide extensive details about our training hyperparameters, providing intuition for their selection that could help enhance learning in similar situations. We design 4 models of convolutional neural networks that explore characteristics such as depth, number of feature maps, size and overlap of kernels, pooling regions, and different subsampling techniques. Results favor models of moderate depth that use an extensive number of parameters in both convolutional and dense layers. Maxout networks are able to outperform rectifiers on some models but introduce too much noise as the complexity of the fully-connected layers increases. The final discussion explains our results and provides additional techniques that could improve performance.



### Deep Multi-Magnification Networks for Multi-Class Breast Cancer Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.13042v2
- **DOI**: 10.1016/j.compmedimag.2021.101866
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13042v2)
- **Published**: 2019-10-29 02:25:36+00:00
- **Updated**: 2021-01-04 19:05:05+00:00
- **Authors**: David Joon Ho, Dig V. K. Yarlagadda, Timothy M. D'Alfonso, Matthew G. Hanna, Anne Grabenstetter, Peter Ntiamoah, Edi Brogi, Lee K. Tan, Thomas J. Fuchs
- **Comment**: Accepted at Computerized Medical Imaging and Graphics
- **Journal**: None
- **Summary**: Pathologic analysis of surgical excision specimens for breast carcinoma is important to evaluate the completeness of surgical excision and has implications for future treatment. This analysis is performed manually by pathologists reviewing histologic slides prepared from formalin-fixed tissue. In this paper, we present Deep Multi-Magnification Network trained by partial annotation for automated multi-class tissue segmentation by a set of patches from multiple magnifications in digitized whole slide images. Our proposed architecture with multi-encoder, multi-decoder, and multi-concatenation outperforms other single and multi-magnification-based architectures by achieving the highest mean intersection-over-union, and can be used to facilitate pathologists' assessments of breast cancer.



### Converged Deep Framework Assembling Principled Modules for CS-MRI
- **Arxiv ID**: http://arxiv.org/abs/1910.13046v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13046v1)
- **Published**: 2019-10-29 02:30:57+00:00
- **Updated**: 2019-10-29 02:30:57+00:00
- **Authors**: Risheng Liu, Yuxi Zhang, Shichao Cheng, Zhongxuan Luo, Xin Fan
- **Comment**: None
- **Journal**: None
- **Summary**: Compressed Sensing Magnetic Resonance Imaging (CS-MRI) significantly accelerates MR data acquisition at a sampling rate much lower than the Nyquist criterion. A major challenge for CS-MRI lies in solving the severely ill-posed inverse problem to reconstruct aliasing-free MR images from the sparse k-space data. Conventional methods typically optimize an energy function, producing reconstruction of high quality, but their iterative numerical solvers unavoidably bring extremely slow processing. Recent data-driven techniques are able to provide fast restoration by either learning direct prediction to final reconstruction or plugging learned modules into the energy optimizer. Nevertheless, these data-driven predictors cannot guarantee the reconstruction following constraints underlying the regularizers of conventional methods so that the reliability of their reconstruction results are questionable. In this paper, we propose a converged deep framework assembling principled modules for CS-MRI that fuses learning strategy with the iterative solver of a conventional reconstruction energy. This framework embeds an optimal condition checking mechanism, fostering \emph{efficient} and \emph{reliable} reconstruction. We also apply the framework to two practical tasks, \emph{i.e.}, parallel imaging and reconstruction with Rician noise. Extensive experiments on both benchmark and manufacturer-testing images demonstrate that the proposed method reliably converges to the optimal solution more efficiently and accurately than the state-of-the-art in various scenarios.



### Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.13049v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13049v2)
- **Published**: 2019-10-29 02:40:32+00:00
- **Updated**: 2019-12-05 05:35:27+00:00
- **Authors**: Qiming Zhang, Jing Zhang, Wei Liu, Dacheng Tao
- **Comment**: 11 pages, 2 figures, accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However, the different data distributions in the two domains, or \emph{domain shift/discrepancy}, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5$\rightarrow $Cityscapes and SYNTHIA$\rightarrow $Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at \url{https://github.com/RogerZhangzz/CAG_UDA}.



### POIRot: A rotation invariant omni-directional pointnet
- **Arxiv ID**: http://arxiv.org/abs/1910.13050v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13050v2)
- **Published**: 2019-10-29 02:41:19+00:00
- **Updated**: 2019-10-30 02:51:50+00:00
- **Authors**: Liu Yang, Rudrasis Chakraborty, Stella X. Yu
- **Comment**: None
- **Journal**: None
- **Summary**: Point-cloud is an efficient way to represent 3D world. Analysis of point-cloud deals with understanding the underlying 3D geometric structure. But due to the lack of smooth topology, and hence the lack of neighborhood structure, standard correlation can not be directly applied on point-cloud. One of the popular approaches to do point correlation is to partition the point-cloud into voxels and extract features using standard 3D correlation. But this approach suffers from sparsity of point-cloud and hence results in multiple empty voxels. One possible solution to deal with this problem is to learn a MLP to map a point or its local neighborhood to a high dimensional feature space. All these methods suffer from a large number of parameters requirement and are susceptible to random rotations. A popular way to make the model "invariant" to rotations is to use data augmentation techniques with small rotations but the potential drawback includes \item more training samples \item susceptible to large rotations. In this work, we develop a rotation invariant point-cloud segmentation and classification scheme based on the omni-directional camera model (dubbed as {\bf POIRot$^1$}). Our proposed model is rotationally invariant and can preserve geometric shape of a 3D point-cloud. Because of the inherent rotation invariant property, our proposed framework requires fewer number of parameters (please see \cite{Iandola2017SqueezeNetAA} and the references therein for motivation of lean models). Several experiments have been performed to show that our proposed method can beat the state-of-the-art algorithms in classification and part segmentation applications.



### PT-ResNet: Perspective Transformation-Based Residual Network for Semantic Road Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.13055v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13055v1)
- **Published**: 2019-10-29 02:53:46+00:00
- **Updated**: 2019-10-29 02:53:46+00:00
- **Authors**: Rui Fan, Yuan Wang, Lei Qiao, Ruiwen Yao, Peng Han, Weidong Zhang, Ioannis Pitas, Ming Liu
- **Comment**: 5 pages, 5 figures, accepted by 2019 IEEE International Conference on
  Imaging Systems and Techniques (IST)
- **Journal**: None
- **Summary**: Semantic road region segmentation is a high-level task, which paves the way towards road scene understanding. This paper presents a residual network trained for semantic road segmentation. Firstly, we represent the projections of road disparities in the v-disparity map as a linear model, which can be estimated by optimizing the v-disparity map using dynamic programming. This linear model is then utilized to reduce the redundant information in the left and right road images. The right image is also transformed into the left perspective view, which greatly enhances the road surface similarity between the two images. Finally, the processed stereo images and their disparity maps are concatenated to create a set of 3D images, which are then utilized to train our neural network. The experimental results illustrate that our network achieves a maximum F1-measure of approximately 91.19% when analyzing the images from the KITTI road dataset.



### Disentangling the Spatial Structure and Style in Conditional VAE
- **Arxiv ID**: http://arxiv.org/abs/1910.13062v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13062v2)
- **Published**: 2019-10-29 03:14:13+00:00
- **Updated**: 2020-07-15 09:02:56+00:00
- **Authors**: Ziye Zhang, Li Sun, Zhilin Zheng, Qingli Li
- **Comment**: 5 pages, 3 figures
- **Journal**: Published on ICIP 2020
- **Summary**: This paper aims to disentangle the latent space in cVAE into the spatial structure and the style code, which are complementary to each other, with one of them $z_s$ being label relevant and the other $z_u$ irrelevant. The generator is built by a connected encoder-decoder and a label condition mapping network. Depending on whether the label is related with the spatial structure, the output $z_s$ from the condition mapping network is used either as a style code or a spatial structure code. The encoder provides the label irrelevant posterior from which $z_u$ is sampled. The decoder employs $z_s$ and $z_u$ in each layer by adaptive normalization like SPADE or AdaIN. Extensive experiments on two datasets with different types of labels show the effectiveness of our method.



### SID4VAM: A Benchmark Dataset with Synthetic Images for Visual Attention Modeling
- **Arxiv ID**: http://arxiv.org/abs/1910.13066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13066v1)
- **Published**: 2019-10-29 03:29:36+00:00
- **Updated**: 2019-10-29 03:29:36+00:00
- **Authors**: David Berga, Xosé R. Fdez-Vidal, Xavier Otazu, Xosé M. Pardo
- **Comment**: 10 pages, 8 figures, 3 tables, conference paper (ICCV 2019),
  http://openaccess.thecvf.com/content_ICCV_2019/papers/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.pdf
- **Journal**: The IEEE International Conference on Computer Vision (ICCV) 2019
- **Summary**: A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets.



### The Six Fronts of the Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.13076v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13076v1)
- **Published**: 2019-10-29 04:07:00+00:00
- **Updated**: 2019-10-29 04:07:00+00:00
- **Authors**: Alceu Bissoto, Eduardo Valle, Sandra Avila
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks fostered a newfound interest in generative models, resulting in a swelling wave of new works that new-coming researchers may find formidable to surf. In this paper, we intend to help those researchers, by splitting that incoming wave into six "fronts": Architectural Contributions, Conditional Techniques, Normalization and Constraint Contributions, Loss Functions, Image-to-image Translations, and Validation Metrics. The division in fronts organizes literature into approachable blocks, ultimately communicating to the reader how the area is evolving. Previous surveys in the area, which this works also tabulates, focus on a few of those fronts, leaving a gap that we propose to fill with a more integrated, comprehensive overview. Here, instead of an exhaustive survey, we opt for a straightforward review: our target is to be an entry point to this vast literature, and also to be able to update experienced researchers to the newest techniques.



### Learning Rich Image Region Representation for Visual Question Answering
- **Arxiv ID**: http://arxiv.org/abs/1910.13077v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13077v1)
- **Published**: 2019-10-29 04:10:06+00:00
- **Updated**: 2019-10-29 04:10:06+00:00
- **Authors**: Bei Liu, Zhicheng Huang, Zhaoyang Zeng, Zheyu Chen, Jianlong Fu
- **Comment**: Rank 2 in VQA Challenge 2019
- **Journal**: None
- **Summary**: We propose to boost VQA by leveraging more powerful feature extractors by improving the representation ability of both visual and text features and the ensemble of models. For visual feature, some detection techniques are used to improve the detector. For text feature, we adopt BERT as the language model and find that it can significantly improve VQA performance. Our solution won the second place in the VQA Challenge 2019.



### Classification Calibration for Long-tail Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.13081v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13081v3)
- **Published**: 2019-10-29 04:29:25+00:00
- **Updated**: 2020-07-31 00:56:29+00:00
- **Authors**: Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Jun Hao Liew, Sheng Tang, Steven Hoi, Jiashi Feng
- **Comment**: This report presents our winning solution to LVIS 2019 challenge
- **Journal**: None
- **Summary**: Remarkable progress has been made in object instance detection and segmentation in recent years. However, existing state-of-the-art methods are mostly evaluated with fairly balanced and class-limited benchmarks, such as Microsoft COCO dataset [8]. In this report, we investigate the performance drop phenomenon of state-of-the-art two-stage instance segmentation models when processing extreme long-tail training data based on the LVIS [5] dataset, and find a major cause is the inaccurate classification of object proposals. Based on this observation, we propose to calibrate the prediction of classification head to improve recognition performance for the tail classes. Without much additional cost and modification of the detection model architecture, our calibration method improves the performance of the baseline by a large margin on the tail classes. Codes will be available. Importantly, after the submission, we find significant improvement can be further achieved by modifying the calibration head, which we will update later.



### GLIMPS: A Greedy Mixed Integer Approach for Super Robust Matched Subspace Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.13089v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13089v1)
- **Published**: 2019-10-29 05:15:40+00:00
- **Updated**: 2019-10-29 05:15:40+00:00
- **Authors**: Md Mahfuzur Rahman, Daniel Pimentel-Alarcon
- **Comment**: 8 pages, 5 figures, 57th Allerton Conference
- **Journal**: None
- **Summary**: Due to diverse nature of data acquisition and modern applications, many contemporary problems involve high dimensional datum $\x \in \R^\d$ whose entries often lie in a union of subspaces and the goal is to find out which entries of $\x$ match with a particular subspace $\sU$, classically called \emph {matched subspace detection}. Consequently, entries that match with one subspace are considered as inliers w.r.t the subspace while all other entries are considered as outliers. Proportion of outliers relative to each subspace varies based on the degree of coordinates from subspaces. This problem is a combinatorial NP-hard in nature and has been immensely studied in recent years. Existing approaches can solve the problem when outliers are sparse. However, if outliers are abundant or in other words if $\x$ contains coordinates from a fair amount of subspaces, this problem can't be solved with acceptable accuracy or within a reasonable amount of time. This paper proposes a two-stage approach called \emph{Greedy Linear Integer Mixed Programmed Selector} (GLIMPS) for this abundant-outliers setting, which combines a greedy algorithm and mixed integer formulation and can tolerate over 80\% outliers, outperforming the state-of-the-art.



### Style Mixer: Semantic-aware Multi-Style Transfer Network
- **Arxiv ID**: http://arxiv.org/abs/1910.13093v1
- **DOI**: 10.1111/cgf.13853
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13093v1)
- **Published**: 2019-10-29 05:35:34+00:00
- **Updated**: 2019-10-29 05:35:34+00:00
- **Authors**: Zixuan Huang, Jinghuai Zhang, Jing Liao
- **Comment**: Pacific Graphics 2019
- **Journal**: None
- **Summary**: Recent neural style transfer frameworks have obtained astonishing visual quality and flexibility in Single-style Transfer (SST), but little attention has been paid to Multi-style Transfer (MST) which refers to simultaneously transferring multiple styles to the same image. Compared to SST, MST has the potential to create more diverse and visually pleasing stylization results. In this paper, we propose the first MST framework to automatically incorporate multiple styles into one result based on regional semantics. We first improve the existing SST backbone network by introducing a novel multi-level feature fusion module and a patch attention module to achieve better semantic correspondences and preserve richer style details. For MST, we designed a conceptually simple yet effective region-based style fusion module to insert into the backbone. It assigns corresponding styles to content regions based on semantic matching, and then seamlessly combines multiple styles together. Comprehensive evaluations demonstrate that our framework outperforms existing works of SST and MST.



### A Robust Pavement Mapping System Based on Normal-Constrained Stereo Visual Odometry
- **Arxiv ID**: http://arxiv.org/abs/1910.13102v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13102v1)
- **Published**: 2019-10-29 06:17:09+00:00
- **Updated**: 2019-10-29 06:17:09+00:00
- **Authors**: Huaiyang Huang, Rui Fan, Yilong Zhu, Ming Liu, Ioannis Pitas
- **Comment**: 6 pages, 7 figures, 3 tables, accepted for publishing on 2019 IEEE
  International Conference on Imaging Systems and Techniques (IST)
- **Journal**: None
- **Summary**: Pavement condition is crucial for civil infrastructure maintenance. This task usually requires efficient road damage localization, which can be accomplished by the visual odometry system embedded in unmanned aerial vehicles (UAVs). However, the state-of-the-art visual odometry and mapping methods suffer from large drift under the degeneration of the scene structure. To alleviate this issue, we integrate normal constraints into the visual odometry process, which greatly helps to avoid large drift. By parameterizing the normal vector on the tangential plane, the normal factors are coupled with traditional reprojection factors in the pose optimization procedure. The experimental results demonstrate the effectiveness of the proposed system. The overall absolute trajectory error is improved by approximately 20%, which indicates that the estimated trajectory is much more accurate than that obtained using other state-of-the-art methods.



### Discriminant analysis based on projection onto generalized difference subspace
- **Arxiv ID**: http://arxiv.org/abs/1910.13113v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13113v2)
- **Published**: 2019-10-29 06:56:17+00:00
- **Updated**: 2019-10-30 03:19:56+00:00
- **Authors**: Kazuhiro Fukui, Naoya Sogi, Takumi Kobayashi, Jing-Hao Xue, Atsuto Maki
- **Comment**: None
- **Journal**: None
- **Summary**: This paper discusses a new type of discriminant analysis based on the orthogonal projection of data onto a generalized difference subspace (GDS). In our previous work, we have demonstrated that GDS projection works as the quasi-orthogonalization of class subspaces, which is an effective feature extraction for subspace based classifiers. Interestingly, GDS projection also works as a discriminant feature extraction through a similar mechanism to the Fisher discriminant analysis (FDA). A direct proof of the connection between GDS projection and FDA is difficult due to the significant difference in their formulations. To avoid the difficulty, we first introduce geometrical Fisher discriminant analysis (gFDA) based on a simplified Fisher criterion. Our simplified Fisher criterion is derived from a heuristic yet practically plausible principle: the direction of the sample mean vector of a class is in most cases almost equal to that of the first principal component vector of the class, under the condition that the principal component vectors are calculated by applying the principal component analysis (PCA) without data centering. gFDA can work stably even under few samples, bypassing the small sample size (SSS) problem of FDA. Next, we prove that gFDA is equivalent to GDS projection with a small correction term. This equivalence ensures GDS projection to inherit the discriminant ability from FDA via gFDA. Furthermore, to enhance the performances of gFDA and GDS projection, we normalize the projected vectors on the discriminant spaces. Extensive experiments using the extended Yale B+ database and the CMU face database show that gFDA and GDS projection have equivalent or better performance than the original FDA and its extensions.



### An α-Matte Boundary Defocus Model Based Cascaded Network for Multi-focus Image Fusion
- **Arxiv ID**: http://arxiv.org/abs/1910.13136v2
- **DOI**: 10.1109/TIP.2020.3018261
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13136v2)
- **Published**: 2019-10-29 09:02:45+00:00
- **Updated**: 2019-10-30 01:01:13+00:00
- **Authors**: Haoyu Ma, Qingmin Liao, Juncheng Zhang, Shaojun Liu, Jing-Hao Xue
- **Comment**: 10 pages, 8 figures, journal Unfortunately, I cannot spell one of the
  authors' name coorectly
- **Journal**: None
- **Summary**: Capturing an all-in-focus image with a single camera is difficult since the depth of field of the camera is usually limited. An alternative method to obtain the all-in-focus image is to fuse several images focusing at different depths. However, existing multi-focus image fusion methods cannot obtain clear results for areas near the focused/defocused boundary (FDB). In this paper, a novel {\alpha}-matte boundary defocus model is proposed to generate realistic training data with the defocus spread effect precisely modeled, especially for areas near the FDB. Based on this {\alpha}-matte defocus model and the generated data, a cascaded boundary aware convolutional network termed MMF-Net is proposed and trained, aiming to achieve clearer fusion results around the FDB. More specifically, the MMF-Net consists of two cascaded sub-nets for initial fusion and boundary fusion, respectively; these two sub-nets are designed to first obtain a guidance map of FDB and then refine the fusion near the FDB. Experiments demonstrate that with the help of the new {\alpha}-matte boundary defocus model, the proposed MMF-Net outperforms the state-of-the-art methods both qualitatively and quantitatively.



### Concept Saliency Maps to Visualize Relevant Features in Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1910.13140v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13140v1)
- **Published**: 2019-10-29 09:15:25+00:00
- **Updated**: 2019-10-29 09:15:25+00:00
- **Authors**: Lennart Brocki, Neo Christopher Chung
- **Comment**: 18th IEEE International Conference on Machine Learning and
  Applications (ICMLA)
- **Journal**: None
- **Summary**: Evaluating, explaining, and visualizing high-level concepts in generative models, such as variational autoencoders (VAEs), is challenging in part due to a lack of known prediction classes that are required to generate saliency maps in supervised learning. While saliency maps may help identify relevant features (e.g., pixels) in the input for classification tasks of deep neural networks, similar frameworks are understudied in unsupervised learning. Therefore, we introduce a new method of obtaining saliency maps for latent representations of known or novel high-level concepts, often called concept vectors in generative models. Concept scores, analogous to class scores in classification tasks, are defined as dot products between concept vectors and encoded input data, which can be readily used to compute the gradients. The resulting concept saliency maps are shown to highlight input features deemed important for high-level concepts. Our method is applied to the VAE's latent space of CelebA dataset in which known attributes such as "smiles" and "hats" are used to elucidate relevant facial features. Furthermore, our application to spatial transcriptomic (ST) data of a mouse olfactory bulb demonstrates the potential of latent representations of morphological layers and molecular features in advancing our understanding of complex biological systems. By extending the popular method of saliency maps to generative models, the proposed concept saliency maps help improve interpretability of latent variable models in deep learning.   Codes to reproduce and to implement concept saliency maps: https://github.com/lenbrocki/concept-saliency-maps



### Decomposable-Net: Scalable Low-Rank Compression for Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.13141v3
- **DOI**: 10.24963/ijcai.2021/447
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13141v3)
- **Published**: 2019-10-29 09:15:40+00:00
- **Updated**: 2021-09-29 08:34:33+00:00
- **Authors**: Atsushi Yaguchi, Taiji Suzuki, Shuhei Nitta, Yukinobu Sakata, Akiyuki Tanizawa
- **Comment**: 13 pages, 6 figures, 5 tables. Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence (IJCAI-21), pages
  3249-3256
- **Journal**: None
- **Summary**: Compressing DNNs is important for the real-world applications operating on resource-constrained devices. However, we typically observe drastic performance deterioration when changing model size after training is completed. Therefore, retraining is required to resume the performance of the compressed models suitable for different devices. In this paper, we propose Decomposable-Net (the network decomposable in any size), which allows flexible changes to model size without retraining. We decompose weight matrices in the DNNs via singular value decomposition and adjust ranks according to the target model size. Unlike the existing low-rank compression methods that specialize the model to a fixed size, we propose a novel backpropagation scheme that jointly minimizes losses for both of full- and low-rank networks. This enables not only to maintain the performance of a full-rank network {\it without retraining} but also to improve low-rank networks in multiple sizes. Additionally, we introduce a simple criterion for rank selection that effectively suppresses approximation error. In experiments on the ImageNet classification task, Decomposable-Net yields superior accuracy in a wide range of model sizes. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\%$ with $0.27\times$MACs with ResNet-50, compared to Tucker decomposition ($67.4\% / 0.30\times$), Trained Rank Pruning ($70.6\% / 0.28\times$), and universally slimmable networks ($71.4\% / 0.26\times$).



### Results from the Robocademy ITN: Autonomy, Disturbance Rejection and Perception for Advanced Marine Robotics
- **Arxiv ID**: http://arxiv.org/abs/1910.13144v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13144v1)
- **Published**: 2019-10-29 09:28:04+00:00
- **Updated**: 2019-10-29 09:28:04+00:00
- **Authors**: Matias Valdenegro-Toro, Mariela De Lucas Alvarez, Mariia Dmitrieva, Bilal Wehbe, Georgios Salavasidis, Shahab Heshmati-Alamdari, Juan F. Fuentes-Pérez, Veronika Yordanova, Klemen Istenič, Thomas Guerneve
- **Comment**: 19 pages, 20 figures, initial preprint
- **Journal**: None
- **Summary**: Marine and Underwater resources are important part of the economy of many countries. This requires significant financial resources into their construction and maintentance. Robotics is expected to fill this void, by automating and/or removing humans from hostile environments in order to easily perform maintenance tasks. The Robocademy Marie Sklodowska-Curie Initial Training Network was funded by the European Union's FP7 research program in order to train 13 Fellows into world-leading researchers in Marine and Underwater Robotics. The fellows developed guided research into three areas of key importance: Autonomy, Disturbance Rejection, and Perception. This paper presents a summary of the fellows' research in the three action lines. 71 scientific publications were the primary result of this project, with many other publications currently in the pipeline. Most of the fellows have found employment in Europe, which shows the high demand for this kind of experts. We believe the results from this project are already having an impact in the marine robotics industry, as key technologies are being adopted already.



### LeanConvNets: Low-cost Yet Effective Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.13157v2
- **DOI**: 10.1109/JSTSP.2020.2972775
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13157v2)
- **Published**: 2019-10-29 09:51:10+00:00
- **Updated**: 2020-02-12 10:50:12+00:00
- **Authors**: Jonathan Ephrath, Moshe Eliasof, Lars Ruthotto, Eldad Haber, Eran Treister
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have become indispensable for solving machine learning tasks in speech recognition, computer vision, and other areas that involve high-dimensional data. A CNN filters the input feature using a network containing spatial convolution operators with compactly supported stencils. In practice, the input data and the hidden features consist of a large number of channels, which in most CNNs are fully coupled by the convolution operators. This coupling leads to immense computational cost in the training and prediction phase. In this paper, we introduce LeanConvNets that are derived by sparsifying fully-coupled operators in existing CNNs. Our goal is to improve the efficiency of CNNs by reducing the number of weights, floating point operations and latency times, with minimal loss of accuracy. Our lean convolution operators involve tuning parameters that controls the trade-off between the network's accuracy and computational costs. These convolutions can be used in a wide range of existing networks, and we exemplify their use in residual networks (ResNets). Using a range of benchmark problems from image classification and semantic segmentation, we demonstrate that the resulting LeanConvNet's accuracy is close to state-of-the-art networks while being computationally less expensive. In our tests, the lean versions of ResNet in most cases outperform comparable reduced architectures such as MobileNets and ShuffleNets.



### Adversarial Example in Remote Sensing Image Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.13222v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13222v2)
- **Published**: 2019-10-29 12:26:04+00:00
- **Updated**: 2020-03-03 04:39:58+00:00
- **Authors**: Li Chen, Guowei Zhu, Qi Li, Haifeng Li
- **Comment**: 12 pages
- **Journal**: None
- **Summary**: With the wide application of remote sensing technology in various fields, the accuracy and security requirements for remote sensing images (RSIs) recognition are also increasing. In recent years, due to the rapid development of deep learning in the field of image recognition, RSI recognition models based on deep convolution neural networks (CNNs) outperform traditional hand-craft feature techniques. However, CNNs also pose security issues when they show their capability of accurate classification. By adding a very small variation of the adversarial perturbation to the input image, the CNN model can be caused to produce erroneous results with extremely high confidence, and the modification of the image is not perceived by the human eye. This added adversarial perturbation image is called an adversarial example, which poses a serious security problem for systems based on CNN model recognition results. This paper, for the first time, analyzes adversarial example problem of RSI recognition under CNN models. In the experiments, we used different attack algorithms to fool multiple high-accuracy RSI recognition models trained on multiple RSI datasets. The results show that RSI recognition models are also vulnerable to adversarial examples, and the models with different structures trained on the same RSI dataset also have different vulnerabilities. For each RSI dataset, the number of features also affects the vulnerability of the model. Many features are good for defensive adversarial examples. Further, we find that the attacked class of RSI has an attack selectivity property. The misclassification of adversarial examples of the RSIs are related to the similarity of the original classes in the CNN feature space. In addition, adversarial examples in RSI recognition are of great significance for the security of remote sensing applications, showing a huge potential for future research.



### Region-based Convolution Neural Network Approach for Accurate Segmentation of Pelvic Radiograph
- **Arxiv ID**: http://arxiv.org/abs/1910.13231v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13231v2)
- **Published**: 2019-10-29 12:45:21+00:00
- **Updated**: 2019-12-31 12:49:11+00:00
- **Authors**: Ata Jodeiri, Reza A. Zoroofi, Yuta Hiasa, Masaki Takao, Nobuhiko Sugano, Yoshinobu Sato, Yoshito Otake
- **Comment**: Accepted at ICBME 2019
- **Journal**: None
- **Summary**: With the increasing usage of radiograph images as a most common medical imaging system for diagnosis, treatment planning, and clinical studies, it is increasingly becoming a vital factor to use machine learning-based systems to provide reliable information for surgical pre-planning. Segmentation of pelvic bone in radiograph images is a critical preprocessing step for some applications such as automatic pose estimation and disease detection. However, the encoder-decoder style network known as U-Net has demonstrated limited results due to the challenging complexity of the pelvic shapes, especially in severe patients. In this paper, we propose a novel multi-task segmentation method based on Mask R-CNN architecture. For training, the network weights were initialized by large non-medical dataset and fine-tuned with radiograph images. Furthermore, in the training process, augmented data was generated to improve network performance. Our experiments show that Mask R-CNN utilizing multi-task learning, transfer learning, and data augmentation techniques achieve 0.96 DICE coefficient, which significantly outperforms the U-Net. Notably, for a fair comparison, the same transfer learning and data augmentation techniques have been used for U-net training.



### Detecting motorcycle helmet use with deep learning
- **Arxiv ID**: http://arxiv.org/abs/1910.13232v1
- **DOI**: 10.1016/j.aap.2019.105319
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13232v1)
- **Published**: 2019-10-29 12:50:40+00:00
- **Updated**: 2019-10-29 12:50:40+00:00
- **Authors**: Felix Wilhelm Siebert, Hanhe Lin
- **Comment**: None
- **Journal**: Accident Analysis and Prevention 134 (2020) 105319
- **Summary**: The continuous motorization of traffic has led to a sustained increase in the global number of road related fatalities and injuries. To counter this, governments are focusing on enforcing safe and law-abiding behavior in traffic. However, especially in developing countries where the motorcycle is the main form of transportation, there is a lack of comprehensive data on the safety-critical behavioral metric of motorcycle helmet use. This lack of data prohibits targeted enforcement and education campaigns which are crucial for injury prevention. Hence, we have developed an algorithm for the automated registration of motorcycle helmet usage from video data, using a deep learning approach. Based on 91,000 annotated frames of video data, collected at multiple observation sites in 7 cities across the country of Myanmar, we trained our algorithm to detect active motorcycles, the number and position of riders on the motorcycle, as well as their helmet use. An analysis of the algorithm's accuracy on an annotated test data set, and a comparison to available human-registered helmet use data reveals a high accuracy of our approach. Our algorithm registers motorcycle helmet use rates with an accuracy of -4.4% and +2.1% in comparison to a human observer, with minimal training for individual observation sites. Without observation site specific training, the accuracy of helmet use detection decreases slightly, depending on a number of factors. Our approach can be implemented in existing roadside traffic surveillance infrastructure and can facilitate targeted data-driven injury prevention campaigns with real-time speed. Implications of the proposed method, as well as measures that can further improve detection accuracy are discussed.



### Navigation Agents for the Visually Impaired: A Sidewalk Simulator and Experiments
- **Arxiv ID**: http://arxiv.org/abs/1910.13249v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13249v1)
- **Published**: 2019-10-29 13:23:02+00:00
- **Updated**: 2019-10-29 13:23:02+00:00
- **Authors**: Martin Weiss, Simon Chamorro, Roger Girgis, Margaux Luck, Samira E. Kahou, Joseph P. Cohen, Derek Nowrouzezahrai, Doina Precup, Florian Golemo, Chris Pal
- **Comment**: Accepted at CoRL2019. Code & video available at
  https://mweiss17.github.io/SEVN/
- **Journal**: None
- **Summary**: Millions of blind and visually-impaired (BVI) people navigate urban environments every day, using smartphones for high-level path-planning and white canes or guide dogs for local information. However, many BVI people still struggle to travel to new places. In our endeavor to create a navigation assistant for the BVI, we found that existing Reinforcement Learning (RL) environments were unsuitable for the task. This work introduces SEVN, a sidewalk simulation environment and a neural network-based approach to creating a navigation agent. SEVN contains panoramic images with labels for house numbers, doors, and street name signs, and formulations for several navigation tasks. We study the performance of an RL algorithm (PPO) in this setting. Our policy model fuses multi-modal observations in the form of variable resolution images, visible text, and simulated GPS data to navigate to a goal door. We hope that this dataset, simulator, and experimental results will provide a foundation for further research into the creation of agents that can assist members of the BVI community with outdoor navigation.



### Estimating Skin Tone and Effects on Classification Performance in Dermatology Datasets
- **Arxiv ID**: http://arxiv.org/abs/1910.13268v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13268v1)
- **Published**: 2019-10-29 13:48:17+00:00
- **Updated**: 2019-10-29 13:48:17+00:00
- **Authors**: Newton M. Kinyanjui, Timothy Odonga, Celia Cintas, Noel C. F. Codella, Rameswar Panda, Prasanna Sattigeri, Kush R. Varshney
- **Comment**: NeurIPS 2019 Workshop on Fair ML for Health
- **Journal**: None
- **Summary**: Recent advances in computer vision and deep learning have led to breakthroughs in the development of automated skin image analysis. In particular, skin cancer classification models have achieved performance higher than trained expert dermatologists. However, no attempt has been made to evaluate the consistency in performance of machine learning models across populations with varying skin tones. In this paper, we present an approach to estimate skin tone in benchmark skin disease datasets, and investigate whether model performance is dependent on this measure. Specifically, we use individual typology angle (ITA) to approximate skin tone in dermatology datasets. We look at the distribution of ITA values to better understand skin color representation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a collection of dermoscopic images of skin lesions for the detection of skin cancer, and 2) the SD-198 dataset, a collection of clinical images capturing a wide variety of skin diseases. To estimate ITA, we first develop segmentation models to isolate non-diseased areas of skin. We find that the majority of the data in the the two datasets have ITA values between 34.5{\deg} and 48{\deg}, which are associated with lighter skin, and is consistent with under-representation of darker skinned populations in these datasets. We also find no measurable correlation between performance of machine learning model and ITA values, though more comprehensive data is needed for further validation.



### Improving sequence-to-sequence speech recognition training with on-the-fly data augmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.13296v2
- **DOI**: None
- **Categories**: **eess.AS**, cs.CV, cs.LG, cs.SD
- **Links**: [PDF](http://arxiv.org/pdf/1910.13296v2)
- **Published**: 2019-10-29 14:38:22+00:00
- **Updated**: 2020-02-03 08:12:31+00:00
- **Authors**: Thai-Son Nguyen, Sebastian Stueker, Jan Niehues, Alex Waibel
- **Comment**: To appear in ICASSP 2020
- **Journal**: None
- **Summary**: Sequence-to-Sequence (S2S) models recently started to show state-of-the-art performance for automatic speech recognition (ASR). With these large and deep models overfitting remains the largest problem, outweighing performance improvements that can be obtained from better architectures. One solution to the overfitting problem is increasing the amount of available training data and the variety exhibited by the training data with the help of data augmentation. In this paper we examine the influence of three data augmentation methods on the performance of two S2S model architectures. One of the data augmentation method comes from literature, while two other methods are our own development - a time perturbation in the frequency domain and sub-sequence sampling. Our experiments on Switchboard and Fisher data show state-of-the-art performance for S2S models that are trained solely on the speech training data and do not use additional text data.



### Weighted boxes fusion: Ensembling boxes from different object detection models
- **Arxiv ID**: http://arxiv.org/abs/1910.13302v3
- **DOI**: 10.1016/j.imavis.2021.104117
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13302v3)
- **Published**: 2019-10-29 14:45:26+00:00
- **Updated**: 2021-02-06 14:47:37+00:00
- **Authors**: Roman Solovyev, Weimin Wang, Tatiana Gabruseva
- **Comment**: None
- **Journal**: Image and Vision Computing (2021): 104117
- **Summary**: In this work, we present a novel method for combining predictions of object detection models: weighted boxes fusion. Our algorithm utilizes confidence scores of all proposed bounding boxes to constructs the averaged boxes. We tested method on several datasets and evaluated it in the context of the Open Images and COCO Object Detection tracks, achieving top results in these challenges. The source code is publicly available at https://github.com/ZFTurbo/Weighted-Boxes-Fusion



### Distributed and Consistent Multi-Image Feature Matching via QuickMatch
- **Arxiv ID**: http://arxiv.org/abs/1910.13317v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.13317v1)
- **Published**: 2019-10-29 15:26:54+00:00
- **Updated**: 2019-10-29 15:26:54+00:00
- **Authors**: Zachary Serlin, Guang Yang, Brandon Sookraj, Calin Belta, Roberto Tron
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we consider the multi-image object matching problem, extend a centralized solution of the problem to a distributed solution, and present an experimental application of the centralized solution. Multi-image feature matching is a keystone of many applications, including simultaneous localization and mapping, homography, object detection, and structure from motion. We first review the QuickMatch algorithm for multi-image feature matching. We then present a scheme for distributing sets of features across computational units (agents) that largely preserves feature match quality and minimizes communication between agents (avoiding, in particular, the need of flooding all data to all agents). Finally, we show how QuickMatch performs on an object matching test with low quality images. The centralized QuickMatch algorithm is compared to other standard matching algorithms, while the Distributed QuickMatch algorithm is compared to the centralized algorithm in terms of preservation of match consistency. The presented experiment shows that QuickMatch matches features across a large number of images and features in larger numbers and more accurately than standard techniques.



### Semantic Object Accuracy for Generative Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1910.13321v2
- **DOI**: 10.1109/TPAMI.2020.3021209
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.13321v2)
- **Published**: 2019-10-29 15:35:52+00:00
- **Updated**: 2020-06-02 12:25:16+00:00
- **Authors**: Tobias Hinz, Stefan Heinrich, Stefan Wermter
- **Comment**: Added a user study to verify results. Code available at
  https://github.com/tohinz/semantic-object-accuracy-for-generative-text-to-image-synthesis
- **Journal**: TPAMI (Early Access), 2020
- **Summary**: Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g. whether an image generated from "a car driving down the street" contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.



### Resolution-independent meshes of super pixels
- **Arxiv ID**: http://arxiv.org/abs/1910.13323v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13323v2)
- **Published**: 2019-10-29 15:36:10+00:00
- **Updated**: 2019-11-01 11:17:43+00:00
- **Authors**: Vitaliy Kurlin, Philip Smith
- **Comment**: None
- **Journal**: None
- **Summary**: The over-segmentation into superpixels is an important preprocessing step to smartly compress the input size and speed up higher level tasks. A superpixel was traditionally considered as a small cluster of square-based pixels that have similar color intensities and are closely located to each other. In this discrete model the boundaries of superpixels often have irregular zigzags consisting of horizontal or vertical edges from a given pixel grid. However digital images represent a continuous world, hence the following continuous model in the resolution-independent formulation can be more suitable for the reconstruction problem.   Instead of uniting squares in a grid, a resolution-independent superpixel is defined as a polygon that has straight edges with any possible slope at subpixel resolution. The harder continuous version of the over-segmentation problem is to split an image into polygons and find a best (say, constant) color of each polygon so that the resulting colored mesh well approximates the given image. Such a mesh of polygons can be rendered at any higher resolution with all edges kept straight.   We propose a fast conversion of any traditional superpixels into polygons and guarantees that their straight edges do not intersect. The meshes based on the superpixels SEEDS (Superpixels Extracted via Energy-Driven Sampling) and SLIC (Simple Linear Iterative Clustering) are compared with past meshes based on the Line Segment Detector. The experiments on the Berkeley Segmentation Database confirm that the new superpixels have more compact shapes than pixel-based superpixels.



### Machine Learning-Based Analysis of Sperm Videos and Participant Data for Male Fertility Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.13327v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13327v1)
- **Published**: 2019-10-29 15:38:47+00:00
- **Updated**: 2019-10-29 15:38:47+00:00
- **Authors**: Steven A. Hicks, Jorunn M. Andersen, Oliwia Witczak, Vajira Thambawita, Påll Halvorsen, Hugo L. Hammer, Trine B. Haugen, Michael A. Riegler
- **Comment**: Preprint, accepted by Nature Scientific Reports for publication
  24.10.2019
- **Journal**: None
- **Summary**: Methods for automatic analysis of clinical data are usually targeted towards a specific modality and do not make use of all relevant data available. In the field of male human reproduction, clinical and biological data are not used to its fullest potential. Manual evaluation of a semen sample using a microscope is time-consuming and requires extensive training. Furthermore, the validity of manual semen analysis has been questioned due to limited reproducibility, and often high inter-personnel variation. The existing computer-aided sperm analyzer systems are not recommended for routine clinical use due to methodological challenges caused by the consistency of the semen sample. Thus, there is a need for an improved methodology. We use modern and classical machine learning techniques together with a dataset consisting of 85 videos of human semen samples and related participant data to automatically predict sperm motility. Used techniques include simple linear regression and more sophisticated methods using convolutional neural networks. Our results indicate that sperm motility prediction based on deep learning using sperm motility videos is rapid to perform and consistent. The algorithms performed worse when participant data was added. In conclusion, machine learning-based automatic analysis may become a valuable tool in male infertility investigation and research.



### Weakly Supervised Prostate TMA Classification via Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.13328v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, q-bio.TO
- **Links**: [PDF](http://arxiv.org/pdf/1910.13328v2)
- **Published**: 2019-10-29 15:44:20+00:00
- **Updated**: 2019-11-06 14:24:35+00:00
- **Authors**: Jingwen Wang, Richard J. Chen, Ming Y. Lu, Alexander Baras, Faisal Mahmood
- **Comment**: None
- **Journal**: None
- **Summary**: Histology-based grade classification is clinically important for many cancer types in stratifying patients distinct treatment groups. In prostate cancer, the Gleason score is a grading system used to measure the aggressiveness of prostate cancer from the spatial organization of cells and the distribution of glands. However, the subjective interpretation of Gleason score often suffers from large interobserver and intraobserver variability. Previous work in deep learning-based objective Gleason grading requires manual pixel-level annotation. In this work, we propose a weakly-supervised approach for grade classification in tissue micro-arrays (TMA) using graph convolutional networks (GCNs), in which we model the spatial organization of cells as a graph to better capture the proliferation and community structure of tumor cells. As node-level features in our graph representation, we learn the morphometry of each cell using a contrastive predictive coding (CPC)-based self-supervised approach. We demonstrate that on a five-fold cross validation our method can achieve $0.9659\pm0.0096$ AUC using only TMA-level labels. Our method demonstrates a 39.80\% improvement over standard GCNs with texture features and a 29.27% improvement over GCNs with VGG19 features. Our proposed pipeline can be used to objectively stratify low and high risk cases, reducing inter- and intra-observer variability and pathologist workload.



### On the Benefit of Adversarial Training for Monocular Depth Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.13340v1
- **DOI**: 10.1016/j.cviu.2019.102848
- **Categories**: **eess.IV**, cs.CV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.13340v1)
- **Published**: 2019-10-29 15:57:24+00:00
- **Updated**: 2019-10-29 15:57:24+00:00
- **Authors**: Rick Groenendijk, Sezer Karaoglu, Theo Gevers, Thomas Mensink
- **Comment**: 11 pages, 8 tables, 5 figures, accepted at CVIU
- **Journal**: None
- **Summary**: In this paper we address the benefit of adding adversarial training to the task of monocular depth estimation. A model can be trained in a self-supervised setting on stereo pairs of images, where depth (disparities) are an intermediate result in a right-to-left image reconstruction pipeline. For the quality of the image reconstruction and disparity prediction, a combination of different losses is used, including L1 image reconstruction losses and left-right disparity smoothness. These are local pixel-wise losses, while depth prediction requires global consistency. Therefore, we extend the self-supervised network to become a Generative Adversarial Network (GAN), by including a discriminator which should tell apart reconstructed (fake) images from real images. We evaluate Vanilla GANs, LSGANs and Wasserstein GANs in combination with different pixel-wise reconstruction losses. Based on extensive experimental evaluation, we conclude that adversarial training is beneficial if and only if the reconstruction loss is not too constrained. Even though adversarial training seems promising because it promotes global consistency, non-adversarial training outperforms (or is on par with) any method trained with a GAN when a constrained reconstruction loss is used in combination with batch normalisation. Based on the insights of our experimental evaluation we obtain state-of-the art monocular depth estimation results by using batch normalisation and different output scales.



### Sequential image processing methods for improving semantic video segmentation algorithms
- **Arxiv ID**: http://arxiv.org/abs/1910.13348v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.13348v1)
- **Published**: 2019-10-29 16:07:02+00:00
- **Updated**: 2019-10-29 16:07:02+00:00
- **Authors**: Beril Sirmacek, Nicolò Botteghi, Santiago Sanchez Escalonilla Plaza
- **Comment**: 29 pages, original work of the authors
- **Journal**: None
- **Summary**: Recently, semantic video segmentation gained high attention especially for supporting autonomous driving systems. Deep learning methods made it possible to implement real time segmentation and object identification algorithms on videos. However, most of the available approaches process each video frame independently disregarding their sequential relation in time. Therefore their results suddenly miss some of the object segments in some of the frames even if they were detected properly in the earlier frames. Herein we propose two sequential probabilistic video frame analysis approaches to improve the segmentation performance of the existing algorithms. Our experiments show that using the information of the past frames we increase the performance and consistency of the state of the art algorithms.



### Training Set Effect on Super Resolution for Automated Target Recognition
- **Arxiv ID**: http://arxiv.org/abs/1911.07934v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07934v3)
- **Published**: 2019-10-29 16:44:48+00:00
- **Updated**: 2020-02-07 20:02:58+00:00
- **Authors**: Matthew Ciolino, David Noever, Josh Kalin
- **Comment**: 10 pages, 19 figures, 26 references
- **Journal**: None
- **Summary**: Single Image Super Resolution (SISR) is the process of mapping a low-resolution image to a high resolution image. This inherently has applications in remote sensing as a way to increase the spatial resolution in satellite imagery. This suggests a possible improvement to automated target recognition in image classification and object detection. We explore the effect that different training sets have on SISR with the network, Super Resolution Generative Adversarial Network (SRGAN). We train 5 SRGANs on different land-use classes (e.g. agriculture, cities, ports) and test them on the same unseen dataset. We attempt to find the qualitative and quantitative differences in SISR, binary classification, and object detection performance. We find that curated training sets that contain objects in the test ontology perform better on both computer vision tasks while having a complex distribution of images allows object detection models to perform better. However, Super Resolution (SR) might not be beneficial to certain problems and will see a diminishing amount of returns for datasets that are closer to being solved.



### Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1910.13395v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13395v2)
- **Published**: 2019-10-29 16:58:25+00:00
- **Updated**: 2020-03-17 08:55:05+00:00
- **Authors**: Kuan Fang, Yuke Zhu, Animesh Garg, Silvio Savarese, Li Fei-Fei
- **Comment**: CoRL 2019
- **Journal**: None
- **Summary**: The fundamental challenge of planning for multi-step manipulation is to find effective and plausible action sequences that lead to the task goal. We present Cascaded Variational Inference (CAVIN) Planner, a model-based method that hierarchically generates plans by sampling from latent spaces. To facilitate planning over long time horizons, our method learns latent representations that decouple the prediction of high-level effects from the generation of low-level motions through cascaded variational inference. This enables us to model dynamics at two different levels of temporal resolutions for hierarchical planning. We evaluate our approach in three multi-step robotic manipulation tasks in cluttered tabletop environments given high-dimensional observations. Empirical results demonstrate that the proposed method outperforms state-of-the-art model-based methods by strategically interacting with multiple objects.



### Learning to Manipulate Deformable Objects without Demonstrations
- **Arxiv ID**: http://arxiv.org/abs/1910.13439v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13439v2)
- **Published**: 2019-10-29 17:56:56+00:00
- **Updated**: 2020-03-02 21:45:54+00:00
- **Authors**: Yilin Wu, Wilson Yan, Thanard Kurutach, Lerrel Pinto, Pieter Abbeel
- **Comment**: Project website:
  https://sites.google.com/view/alternating-pick-and-place
- **Journal**: None
- **Summary**: In this paper we tackle the problem of deformable object manipulation through model-free visual reinforcement learning (RL). In order to circumvent the sample inefficiency of RL, we propose two key ideas that accelerate learning. First, we propose an iterative pick-place action space that encodes the conditional relationship between picking and placing on deformable objects. The explicit structural encoding enables faster learning under complex object dynamics. Second, instead of jointly learning both the pick and the place locations, we only explicitly learn the placing policy conditioned on random pick points. Then, by selecting the pick point that has Maximal Value under Placing (MVP), we obtain our picking policy. This provides us with an informed picking policy during testing, while using only random pick points during training. Experimentally, this learning framework obtains an order of magnitude faster learning compared to independent action-spaces on our suite of deformable object manipulation tasks with visual RGB observations. Finally, using domain randomization, we transfer our policies to a real PR2 robot for challenging cloth and rope coverage tasks, and demonstrate significant improvements over standard RL techniques on average coverage.



### Deep convolutional neural network application on rooftop detection for aerial image
- **Arxiv ID**: http://arxiv.org/abs/1910.13509v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13509v1)
- **Published**: 2019-10-29 20:04:02+00:00
- **Updated**: 2019-10-29 20:04:02+00:00
- **Authors**: Mengge Chen, Jonathan Li
- **Comment**: 4 pages, two figures
- **Journal**: None
- **Summary**: As one of the most destructive disasters in the world, earthquake causes death, injuries, destruction and enormous damage to the affected area. It is significant to detect buildings after an earthquake in response to reconstruction and damage evaluation. In this research, we proposed an automatic rooftop detection method based on the convolutional neural network (CNN) to extract buildings in the city of Christchurch and tuned hyperparameters to detect small detached houses from the aerial image. The experiment result shows that our approach can effectively and accurately detect and segment buildings and has competitive performance.



### Hybrid Machine Learning Model of Extreme Learning Machine Radial basis function for Breast Cancer Detection and Diagnosis; a Multilayer Fuzzy Expert System
- **Arxiv ID**: http://arxiv.org/abs/1910.13574v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML, 68Q05
- **Links**: [PDF](http://arxiv.org/pdf/1910.13574v1)
- **Published**: 2019-10-29 23:33:23+00:00
- **Updated**: 2019-10-29 23:33:23+00:00
- **Authors**: Sanaz Mojrian, Gergo Pinter, Javad Hassannataj Joloudari, Imre Felde, Narjes Nabipour, Laszlo Nadai, Amir Mosavi
- **Comment**: 7 pages, 5 figures
- **Journal**: None
- **Summary**: Mammography is often used as the most common laboratory method for the detection of breast cancer, yet associated with the high cost and many side effects. Machine learning prediction as an alternative method has shown promising results. This paper presents a method based on a multilayer fuzzy expert system for the detection of breast cancer using an extreme learning machine (ELM) classification model integrated with radial basis function (RBF) kernel called ELM-RBF, considering the Wisconsin dataset. The performance of the proposed model is further compared with a linear-SVM model. The proposed model outperforms the linear-SVM model with RMSE, R2, MAPE equal to 0.1719, 0.9374 and 0.0539, respectively. Furthermore, both models are studied in terms of criteria of accuracy, precision, sensitivity, specificity, validation, true positive rate (TPR), and false-negative rate (FNR). The ELM-RBF model for these criteria presents better performance compared to the SVM model.



### Domain Generalization via Model-Agnostic Learning of Semantic Features
- **Arxiv ID**: http://arxiv.org/abs/1910.13580v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13580v1)
- **Published**: 2019-10-29 23:43:01+00:00
- **Updated**: 2019-10-29 23:43:01+00:00
- **Authors**: Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, Ben Glocker
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge about inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.



