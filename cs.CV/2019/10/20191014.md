# Arxiv Papers in cs.CV on 2019-10-14
### Self-supervised Label Augmentation via Input Transformations
- **Arxiv ID**: http://arxiv.org/abs/1910.05872v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05872v2)
- **Published**: 2019-10-14 00:37:33+00:00
- **Updated**: 2020-06-29 12:10:28+00:00
- **Authors**: Hankook Lee, Sung Ju Hwang, Jinwoo Shin
- **Comment**: Accepted to ICML 2020. Code available at
  https://github.com/hankook/SLA
- **Journal**: None
- **Summary**: Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision of input transformation. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.



### Interpretable Deep Neural Networks for Facial Expression and Dimensional Emotion Recognition in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/1910.05877v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05877v2)
- **Published**: 2019-10-14 01:33:06+00:00
- **Updated**: 2019-12-13 23:17:07+00:00
- **Authors**: Valentin Richer, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: In this project, we created a database with two types of annotations used in the emotion recognition domain : Action Units and Valence Arousal to try to achieve better results than with only one model. The originality of the approach is also based on the type of architecture used to perform the prediction of the emotions : a categorical Generative Adversarial Network. This kind of dual network can generate images based on the pictures from the new dataset thanks to its generative network and decide if an image is fake or real thanks to its discriminative network as well as help to predict the annotations for Action Units and Valence Arousal due to its categorical nature. GANs were trained on the Action Units model only, then the Valence Arousal model only and then on both the Action Units model and Valence Arousal model in order to test different parameters and understand their influence. The generative and discriminative aspects of the GANs have performed interesting results.



### A New Local Transformation Module for Few-shot Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.05886v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05886v2)
- **Published**: 2019-10-14 01:52:21+00:00
- **Updated**: 2019-11-12 01:02:48+00:00
- **Authors**: Yuwei Yang, Fanman Meng, Hongliang Li, Qingbo Wu, Xiaolong Xu, Shuai Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot segmentation segments object regions of new classes with a few of manual annotations. Its key step is to establish the transformation module between support images (annotated images) and query images (unlabeled images), so that the segmentation cues of support images can guide the segmentation of query images. The existing methods form transformation model based on global cues, which however ignores the local cues that are verified in this paper to be very important for the transformation. This paper proposes a new transformation module based on local cues, where the relationship of the local features is used for transformation. To enhance the generalization performance of the network, the relationship matrix is calculated in a high-dimensional metric embedding space based on cosine distance. In addition, to handle the challenging mapping problem from the low-level local relationships to high-level semantic cues, we propose to apply generalized inverse matrix of the annotation matrix of support images to transform the relationship matrix linearly, which is non-parametric and class-agnostic. The result by the matrix transformation can be regarded as an attention map with high-level semantic cues, based on which a transformation module can be built simply.The proposed transformation module is a general module that can be used to replace the transformation module in the existing few-shot segmentation frameworks. We verify the effectiveness of the proposed method on Pascal VOC 2012 dataset. The value of mIoU achieves at 57.0% in 1-shot and 60.6% in 5-shot, which outperforms the state-of-the-art method by 1.6% and 3.5%, respectively.



### TruNet: Short Videos Generation from Long Videos via Story-Preserving Truncation
- **Arxiv ID**: http://arxiv.org/abs/1910.05899v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05899v1)
- **Published**: 2019-10-14 03:06:15+00:00
- **Updated**: 2019-10-14 03:06:15+00:00
- **Authors**: Fan Yang, Xiao Liu, Dongliang He, Chuang Gan, Jian Wang, Chao Li, Fu Li, Shilei Wen
- **Comment**: ICCV intelligent short video workshop
- **Journal**: None
- **Summary**: In this work, we introduce a new problem, named as {\em story-preserving long video truncation}, that requires an algorithm to automatically truncate a long-duration video into multiple short and attractive sub-videos with each one containing an unbroken story. This differs from traditional video highlight detection or video summarization problems in that each sub-video is required to maintain a coherent and integral story, which is becoming particularly important for resource-production video sharing platforms such as Youtube, Facebook, TikTok, Kwai, etc. To address the problem, we collect and annotate a new large video truncation dataset, named as TruNet, which contains 1470 videos with on average 11 short stories per video. With the new dataset, we further develop and train a neural architecture for video truncation that consists of two components: a Boundary Aware Network (BAN) and a Fast-Forward Long Short-Term Memory (FF-LSTM). We first use the BAN to generate high quality temporal proposals by jointly considering frame-level attractiveness and boundaryness. We then apply the FF-LSTM, which tends to capture high-order dependencies among a sequence of frames, to decide whether a temporal proposal is a coherent and integral story. We show that our proposed framework outperforms existing approaches for the story-preserving long video truncation problem in both quantitative measures and user-study. The dataset is available for public academic research usage at https://ai.baidu.com/broad/download.



### Multi-Stage HRNet: Multiple Stage High-Resolution Network for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.05901v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05901v1)
- **Published**: 2019-10-14 03:08:03+00:00
- **Updated**: 2019-10-14 03:08:03+00:00
- **Authors**: Junjie Huang, Zheng Zhu, Guan Huang
- **Comment**: technical report
- **Journal**: None
- **Summary**: Human pose estimation are of importance for visual understanding tasks such as action recognition and human-computer interaction. In this work, we present a Multiple Stage High-Resolution Network (Multi-Stage HRNet) to tackling the problem of multi-person pose estimation in images. Specifically, we follow the top-down pipelines and high-resolution representations are maintained during single-person pose estimation. In addition, multiple stage network and cross stage feature aggregation are adopted to further refine the keypoint position. The resulting approach achieves promising results in COCO datasets. Our single-model-single-scale test configuration obtains 77.1 AP score in test-dev using publicly available training data.



### Density-Aware Convolutional Networks with Context Encoding for Airborne LiDAR Point Cloud Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.05909v1
- **DOI**: 10.1016/j.isprsjprs.2020.05.023
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05909v1)
- **Published**: 2019-10-14 04:01:33+00:00
- **Updated**: 2019-10-14 04:01:33+00:00
- **Authors**: Xiang Li, Mingyang Wang, Congcong Wen, Lingjing Wang, Nan Zhou, Yi Fang
- **Comment**: None
- **Journal**: None
- **Summary**: To better address challenging issues of the irregularity and inhomogeneity inherently present in 3D point clouds, researchers have been shifting their focus from the design of hand-craft point feature towards the learning of 3D point signatures using deep neural networks for 3D point cloud classification. Recent proposed deep learning based point cloud classification methods either apply 2D CNN on projected feature images or apply 1D convolutional layers directly on raw point sets. These methods cannot adequately recognize fine-grained local structures caused by the uneven density distribution of the point cloud data. In this paper, to address this challenging issue, we introduced a density-aware convolution module which uses the point-wise density to re-weight the learnable weights of convolution kernels. The proposed convolution module is able to fully approximate the 3D continuous convolution on unevenly distributed 3D point sets. Based on this convolution module, we further developed a multi-scale fully convolutional neural network with downsampling and upsampling blocks to enable hierarchical point feature learning. In addition, to regularize the global semantic context, we implemented a context encoding module to predict a global context encoding and formulated a context encoding regularizer to enforce the predicted context encoding to be aligned with the ground truth one. The overall network can be trained in an end-to-end fashion with the raw 3D coordinates as well as the height above ground as inputs. Experiments on the International Society for Photogrammetry and Remote Sensing (ISPRS) 3D labeling benchmark demonstrated the superiority of the proposed method for point cloud classification. Our model achieved a new state-of-the-art performance with an average F1 score of 71.2% and improved the performance by a large margin on several categories.



### Vertebrae Detection and Localization in CT with Two-Stage CNNs and Dense Annotations
- **Arxiv ID**: http://arxiv.org/abs/1910.05911v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05911v1)
- **Published**: 2019-10-14 04:12:59+00:00
- **Updated**: 2019-10-14 04:12:59+00:00
- **Authors**: James McCouat, Ben Glocker
- **Comment**: Accept into the MICCAI workshop MSKI 2019
- **Journal**: None
- **Summary**: We propose a new, two-stage approach to the vertebrae centroid detection and localization problem. The first stage detects where the vertebrae appear in the scan using 3D samples, the second identifies the specific vertebrae within that region-of-interest using 2D slices. Our solution utilizes new techniques to improve the accuracy of the algorithm such as a revised approach to dense labelling from sparse centroid annotations and usage of large anisotropic kernels in the base level of a U-net architecture to maximize the receptive field. Our method improves the state-of-the-art's mean localization accuracy by 0.87mm on a publicly available spine CT benchmark.



### Preimplantation Blastomere Boundary Identification in HMC Microscopic Images of Early Stage Human Embryos
- **Arxiv ID**: http://arxiv.org/abs/1910.05972v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.05972v1)
- **Published**: 2019-10-14 08:15:49+00:00
- **Updated**: 2019-10-14 08:15:49+00:00
- **Authors**: Shakiba Kheradmand, Parvaneh Saeedi, Jason Au, John Havelock
- **Comment**: 15 pages
- **Journal**: None
- **Summary**: We present a novel method for identification of the boundary of embryonic cells (blastomeres) in Hoffman Modulation Contrast (HMC) microscopic images that are taken between day one to day three. Identification of boundaries of blastomeres is a challenging task, especially in the cases containing four or more cells. This is because these cells are bundled up tightly inside an embryo's membrane and any 2D image projection of such 3D embryo includes cell overlaps, occlusions, and projection ambiguities. Moreover, human embryos include fragmentation, which does not conform to any specific patterns or shape. Here we developed a model-based iterative approach, in which blastomeres are modeled as ellipses that conform to the local image features, such as edges and normals. In an iterative process, each image feature contributes only to one candidate and is removed upon being associated to a model candidate. We have tested the proposed algorithm on an image dataset comprising of 468 human embryos obtained from different sources. An overall Precision, Sensitivity and Overall Quality (OQ) of 92%, 88% and 83% are achieved.



### An Efficient Tensor Completion Method via New Latent Nuclear Norm
- **Arxiv ID**: http://arxiv.org/abs/1910.05986v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05986v1)
- **Published**: 2019-10-14 08:46:43+00:00
- **Updated**: 2019-10-14 08:46:43+00:00
- **Authors**: Jinshi Yu, Weijun Sun, Yuning Qiu, Shengli Xie
- **Comment**: None
- **Journal**: None
- **Summary**: In tensor completion, the latent nuclear norm is commonly used to induce low-rank structure, while substantially failing to capture the global information due to the utilization of unbalanced unfolding scheme. To overcome this drawback, a new latent nuclear norm equipped with a more balanced unfolding scheme is defined for low-rank regularizer. Moreover, the new latent nuclear norm together with the Frank-Wolfe (FW) algorithm is developed as an efficient completion method by utilizing the sparsity structure of observed tensor. Specifically, both FW linear subproblem and line search only need to access the observed entries, by which we can instead maintain the sparse tensors and a set of small basis matrices during iteration. Most operations are based on sparse tensors, and the closed-form solution of FW linear subproblem can be obtained from rank-one SVD. We theoretically analyze the space-complexity and time-complexity of the proposed method, and show that it is much more efficient over other norm-based completion methods for higher-order tensors. Extensive experimental results of visual-data inpainting demonstrate that the proposed method is able to achieve state-of-the-art performance at smaller costs of time and space, which is very meaningful for the memory-limited equipment in practical applications.



### A unified framework of predicting binary interestingness of images based on discriminant correlation analysis and multiple kernel learning
- **Arxiv ID**: http://arxiv.org/abs/1910.05996v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05996v1)
- **Published**: 2019-10-14 09:03:48+00:00
- **Updated**: 2019-10-14 09:03:48+00:00
- **Authors**: Qiang Sun, Liting Wang, Maohui Li, Longtao Zhang, Yuxiang Yang
- **Comment**: 30 pages, 9 figures, 6 tables
- **Journal**: None
- **Summary**: In the modern content-based image retrieval systems, there is an increasingly interest in constructing a computationally effective model to predict the interestingness of images since the measure of image interestingness could improve the human-centered search satisfaction and the user experience in different applications. In this paper, we propose a unified framework to predict the binary interestingness of images based on discriminant correlation analysis (DCA) and multiple kernel learning (MKL) techniques. More specially, on the one hand, to reduce feature redundancy in describing the interestingness cues of images, the DCA or multi-set discriminant correlation analysis (MDCA) technique is adopted to fuse multiple feature sets of the same type for individual cues by taking into account the class structure among the samples involved to describe the three classical interestingness cues, unusualness,aesthetics as well as general preferences, with three sets of compact and representative features; on the other hand, to make good use of the heterogeneity from the three sets of high-level features for describing the interestingness cues, the SimpleMKL method is employed to enhance the generalization ability of the built model for the task of the binary interestingness classification. Experimental results on the publicly-released interestingness prediction data set have demonstrated the rationality and effectiveness of the proposed framework in the binary prediction of image interestingness where we have conducted several groups of comparative studies across different interestingness feature combinations, different interestingness cues, as well as different feature types for the three interestingness cues.



### Optimization and Manipulation of Contextual Mutual Spaces for Multi-User Virtual and Augmented Reality Interaction
- **Arxiv ID**: http://arxiv.org/abs/1910.05998v2
- **DOI**: 10.1109/VR46266.2020.00055
- **Categories**: **cs.HC**, cs.CV, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.05998v2)
- **Published**: 2019-10-14 09:10:54+00:00
- **Updated**: 2020-02-09 05:36:47+00:00
- **Authors**: Mohammad Keshavarzi, Allen Y. Yang, Woojin Ko, Luisa Caldas
- **Comment**: Accepted at 2020 IEEE Conference on Virtual Reality and 3D User
  Interfaces (VR)
- **Journal**: None
- **Summary**: Spatial computing experiences are physically constrained by the geometry and semantics of the local user environment. This limitation is elevated in remote multi-user interaction scenarios, where finding a common virtual ground physically accessible for all participants becomes challenging. Locating a common accessible virtual ground is difficult for the users themselves, particularly if they are not aware of the spatial properties of other participants. In this paper, we introduce a framework to generate an optimal mutual virtual space for a multi-user interaction setting where remote users' room spaces can have different layout and sizes. The framework further recommends movement of surrounding furniture objects that expand the size of the mutual space with minimal physical effort. Finally, we demonstrate the performance of our solution on real-world datasets and also a real HoloLens application. Results show the proposed algorithm can effectively discover optimal shareable space for multi-user virtual interaction and hence facilitate remote spatial computing communication in various collaborative workflows.



### Real-World Image Datasets for Federated Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11089v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11089v3)
- **Published**: 2019-10-14 09:33:26+00:00
- **Updated**: 2021-01-05 06:31:32+00:00
- **Authors**: Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, Qiang Yang
- **Comment**: This paper is published at the 2nd International Workshop on
  Federated Learning for Data Privacy and Confidentiality, in Conjunction with
  NeurIPS 2019 (FL-NeurIPS 19)
- **Journal**: None
- **Summary**: Federated learning is a new machine learning paradigm which allows data parties to build machine learning models collaboratively while keeping their data secure and private. While research efforts on federated learning have been growing tremendously in the past two years, most existing works still depend on pre-existing public datasets and artificial partitions to simulate data federations due to the lack of high-quality labeled data generated from real-world edge applications. Consequently, advances on benchmark and model evaluations for federated learning have been lagging behind. In this paper, we introduce a real-world image dataset. The dataset contains more than 900 images generated from 26 street cameras and 7 object categories annotated with detailed bounding box. The data distribution is non-IID and unbalanced, reflecting the characteristic real-world federated learning scenarios. Based on this dataset, we implemented two mainstream object detection algorithms (YOLO and Faster R-CNN) and provided an extensive benchmark on model performance, efficiency, and communication in a federated learning setting. Both the dataset and algorithms are made publicly available.



### Multi-view consensus CNN for 3D facial landmark placement
- **Arxiv ID**: http://arxiv.org/abs/1910.06007v1
- **DOI**: 10.1007/978-3-030-20887-5_44
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.06007v1)
- **Published**: 2019-10-14 09:34:29+00:00
- **Updated**: 2019-10-14 09:34:29+00:00
- **Authors**: Rasmus R. Paulsen, Kristine Aavild Juhl, Thilde Marie Haspang, Thomas Hansen, Melanie Ganz, Gudmundur Einarsson
- **Comment**: This is a pre-print of an article published in proceedings of the
  asian conference on computer vision 2018 (LNCS 11361). The final
  authenticated version is available online at:
  https://doi.org/10.1007/978-3-030-20887-5_44
- **Journal**: Proceedings of the asian conference on computer vision 2018.
  Lecture Notes in Computer Science, vol 11361. Springer
- **Summary**: The rapid increase in the availability of accurate 3D scanning devices has moved facial recognition and analysis into the 3D domain. 3D facial landmarks are often used as a simple measure of anatomy and it is crucial to have accurate algorithms for automatic landmark placement. The current state-of-the-art approaches have yet to gain from the dramatic increase in performance reported in human pose tracking and 2D facial landmark placement due to the use of deep convolutional neural networks (CNN). Development of deep learning approaches for 3D meshes has given rise to the new subfield called geometric deep learning, where one topic is the adaptation of meshes for the use of deep CNNs. In this work, we demonstrate how methods derived from geometric deep learning, namely multi-view CNNs, can be combined with recent advances in human pose tracking. The method finds 2D landmark estimates and propagates this information to 3D space, where a consensus method determines the accurate 3D face landmark position. We utilise the method on a standard 3D face dataset and show that it outperforms current methods by a large margin. Further, we demonstrate how models trained on 3D range scans can be used to accurately place anatomical landmarks in magnetic resonance images.



### A Collaborative Framework for High-Definition Mapping
- **Arxiv ID**: http://arxiv.org/abs/1910.06014v2
- **DOI**: 10.1109/ITSC.2019.8917292
- **Categories**: **cs.RO**, cs.CV, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/1910.06014v2)
- **Published**: 2019-10-14 09:46:52+00:00
- **Updated**: 2020-07-28 23:59:21+00:00
- **Authors**: Alexis Stoven-Dubois, Kuntima Kiala Miguel, Aziz Dziri, Bertrand Leroy, Roland Chapuis
- **Comment**: 2019 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: 2019 IEEE Intelligent Transportation Systems Conference (ITSC)
- **Summary**: For connected vehicles to have a substantial effect on road safety, it is required that accurate positions and trajectories can be shared. To this end, all vehicles must be accurately geolocalized in a common frame. This can be achieved by merging GNSS (Global Navigation Satellite System) information and visual observations matched with a map of geo-positioned landmarks. Building such a map remains a challenge, and current solutions are facing strong cost-related limitations.   We present a collaborative framework for high-definition mapping, in which vehicles equipped with standard sensors, such as a GNSS receiver and a mono-visual camera, update a map of geolocalized landmarks. Our system is composed of two processing blocks: the first one is embedded in each vehicle, and aims at geolocalizing the vehicle and the detected feature marks. The second is operated on cloud servers, and uses observations from all the vehicles to compute updates for the map of geo-positioned landmarks. As the map's landmarks are detected and positioned by more and more vehicles, the accuracy of the map increases, eventually converging in probability towards a null error. The landmarks geo-positions are estimated in a stable and scalable way, enabling to provide dynamic map updates in an automatic manner.



### OmniTrack: Real-time detection and tracking of objects, text and logos in video
- **Arxiv ID**: http://arxiv.org/abs/1910.06017v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06017v1)
- **Published**: 2019-10-14 09:52:40+00:00
- **Updated**: 2019-10-14 09:52:40+00:00
- **Authors**: Hannes Fassold, Ridouane Ghermi
- **Comment**: accepted for IEEE ISM Conference, 2019
- **Journal**: None
- **Summary**: The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described. The major steps in the training procedure for the combined detector for text and logo will be presented. We will describe then the OmniTrack algorithm, consisting of the phases preprocessing, feature calculation, prediction, matching and update. Several performance optimizations have been implemented there as well, like doing the object detection and optical flow calculation asynchronously. Experiments show that the proposed algorithm runs in real-time for standard definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.



### Deep Semantic Parsing of Freehand Sketches with Homogeneous Transformation, Soft-Weighted Loss, and Staged Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.06023v2
- **DOI**: 10.1109/TMM.2020.3028466
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06023v2)
- **Published**: 2019-10-14 10:21:44+00:00
- **Updated**: 2020-12-03 01:56:43+00:00
- **Authors**: Ying Zheng, Hongxun Yao, Xiaoshuai Sun
- **Comment**: 13 pages, accepted by IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep framework for part-level semantic parsing of freehand sketches, which makes three main contributions that are experimentally shown to have substantial practical merit. First, we propose a homogeneous transformation method to address the problem of domain adaptation. For the task of sketch parsing, there is no available data of labeled freehand sketches that can be directly used for model training. An alternative solution is to learn from datasets of real image parsing, while the domain adaptation is an inevitable problem. Unlike existing methods that utilize the edge maps of real images to approximate freehand sketches, the proposed homogeneous transformation method transforms the data from domains of real images and freehand sketches into a homogeneous space to minimize the semantic gap. Second, we design a soft-weighted loss function as guidance for the training process, which gives attention to both the ambiguous label boundary and class imbalance. Third, we present a staged learning strategy to improve the parsing performance of the trained model, which takes advantage of the shared information and specific characteristic from different sketch categories. Extensive experimental results demonstrate the effectiveness of the above three methods. Specifically, to evaluate the generalization ability of our homogeneous transformation method, additional experiments for the task of sketch-based image retrieval are conducted on the QMUL FG-SBIR dataset. Finally, by integrating the proposed three methods into a unified framework of deep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the public SketchParse dataset.



### Sketch-Specific Data Augmentation for Freehand Sketch Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.06038v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06038v2)
- **Published**: 2019-10-14 11:15:07+00:00
- **Updated**: 2020-12-03 07:10:49+00:00
- **Authors**: Ying Zheng, Hongxun Yao, Xiaoshuai Sun, Shengping Zhang, Sicheng Zhao, Fatih Porikli
- **Comment**: None
- **Journal**: None
- **Summary**: Sketch recognition remains a significant challenge due to the limited training data and the substantial intra-class variance of freehand sketches for the same object. Conventional methods for this task often rely on the availability of the temporal order of sketch strokes, additional cues acquired from different modalities and supervised augmentation of sketch datasets with real images, which also limit the applicability and feasibility of these methods in real scenarios.   In this paper, we propose a novel sketch-specific data augmentation (SSDA) method that leverages the quantity and quality of the sketches automatically. From the aspect of quantity, we introduce a Bezier pivot based deformation (BPD) strategy to enrich the training data. Towards quality improvement, we present a mean stroke reconstruction (MSR) approach to generate a set of novel types of sketches with smaller intra-class variances. Both of these solutions are unrestricted from any multi-source data and temporal cues of sketches. Furthermore, we show that some recent deep convolutional neural network models that are trained on generic classes of real images can be better choices than most of the elaborate architectures that are designed explicitly for sketch recognition. As SSDA can be integrated with any convolutional neural networks, it has a distinct advantage over the existing methods. Our extensive experimental evaluations demonstrate that the proposed method achieves the state-of-the-art results (84.27%) on the TU-Berlin dataset, outperforming the human performance by a remarkable 11.17% increase. Finally, more experiments show the practical value of our approach for the task of sketch-based image retrieval.



### Encoder-Decoder based CNN and Fully Connected CRFs for Remote Sensed Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.06041v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06041v1)
- **Published**: 2019-10-14 11:22:18+00:00
- **Updated**: 2019-10-14 11:22:18+00:00
- **Authors**: Vikas Agaradahalli Gurumurthy
- **Comment**: None
- **Journal**: None
- **Summary**: With the advancement of remote-sensed imaging large volumes of very high resolution land cover images can now be obtained. Automation of object recognition in these 2D images, however, is still a key issue. High intra-class variance and low inter-class variance in Very High Resolution (VHR) images hamper the accuracy of prediction in object recognition tasks. Most successful techniques in various computer vision tasks recently are based on deep supervised learning. In this work, a deep Convolutional Neural Network (CNN) based on symmetric encoder-decoder architecture with skip connections is employed for the 2D semantic segmentation of most common land cover object classes - impervious surface, buildings, low vegetation, trees and cars. Atrous convolutions are employed to have large receptive field in the proposed CNN model. Further, the CNN outputs are post-processed using Fully Connected Conditional Random Field (FCRF) model to refine the CNN pixel label predictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5% on the ISPRS Vaihingen Dataset.



### Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.06056v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06056v4)
- **Published**: 2019-10-14 11:53:03+00:00
- **Updated**: 2021-08-10 14:15:33+00:00
- **Authors**: Martin Thümmel, Sven Sickert, Joachim Denzler
- **Comment**: 6 pages, 6 figures, IEEE International Workshop on Biometrics and
  Forensics 2021
- **Journal**: None
- **Summary**: The human face has a high potential for biometric identification due to its many individual traits. At the same time, such identification is vulnerable to biometric copies. These presentation attacks pose a great challenge in unsupervised authentication settings. As a countermeasure, we propose a method that automatically analyzes the plausibility of facial behavior based on a sequence of 3D face scans. A compact feature representation measures facial behavior using the temporal curvature change. Finally, we train our method only on genuine faces in an anomaly detection scenario. Our method can detect presentation attacks using elastic 3D masks, bent photographs with eye holes, and monitor replay-attacks. For evaluation, we recorded a challenging database containing such cases using a high-quality 3D sensor. It features 109 4D face scans including eleven different types of presentation attacks. We achieve error rates of 11% and 6% for APCER and BPCER, respectively.



### Structure Preserving Compressive Sensing MRI Reconstruction using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06067v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06067v2)
- **Published**: 2019-10-14 12:08:48+00:00
- **Updated**: 2020-04-26 09:50:15+00:00
- **Authors**: Puneesh Deora, Bhavya Vasudeva, Saumik Bhattacharya, Pyari Mohan Pradhan
- **Comment**: Accepted in IEEE CVPR Workshop on NTIRE 2020
- **Journal**: None
- **Summary**: Compressive sensing magnetic resonance imaging (CS-MRI) accelerates the acquisition of MR images by breaking the Nyquist sampling limit. In this work, a novel generative adversarial network (GAN) based framework for CS-MRI reconstruction is proposed. Leveraging a combination of patch-based discriminator and structural similarity index based loss, our model focuses on preserving high frequency content as well as fine textural details in the reconstructed image. Dense and residual connections have been incorporated in a U-net based generator architecture to allow easier transfer of information as well as variable network length. We show that our algorithm outperforms state-of-the-art methods in terms of quality of reconstruction and robustness to noise. Also, the reconstruction time, which is of the order of milliseconds, makes it highly suitable for real-time clinical use.



### Light Field Synthesis by Training Deep Network in the Refocused Image Domain
- **Arxiv ID**: http://arxiv.org/abs/1910.06072v5
- **DOI**: 10.1109/TIP.2020.2992354
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06072v5)
- **Published**: 2019-10-14 12:13:37+00:00
- **Updated**: 2020-04-29 03:07:29+00:00
- **Authors**: Chang-Le Liu, Kuang-Tsu Shih, Jiun-Woei Huang, Homer H. Chen
- **Comment**: Accepted to IEEE Transactions on Image Processing
- **Journal**: None
- **Summary**: Light field imaging, which captures spatio-angular information of incident light on image sensor, enables many interesting applications like image refocusing and augmented reality. However, due to the limited sensor resolution, a trade-off exists between the spatial and angular resolution. To increase the angular resolution, view synthesis techniques have been adopted to generate new views from existing views. However, traditional learning-based view synthesis mainly considers the image quality of each view of the light field and neglects the quality of the refocused images. In this paper, we propose a new loss function called refocused image error (RIE) to address the issue. The main idea is that the image quality of the synthesized light field should be optimized in the refocused image domain because it is where the light field is perceived. We analyze the behavior of RIL in the spectral domain and test the performance of our approach against previous approaches on both real and software-rendered light field datasets using objective assessment metrics such as MSE, MAE, PSNR, SSIM, and GMSD. Experimental results show that the light field generated by our method results in better refocused images than previous methods.



### ReActNet: Temporal Localization of Repetitive Activities in Real-World Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.06096v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06096v1)
- **Published**: 2019-10-14 12:33:59+00:00
- **Updated**: 2019-10-14 12:33:59+00:00
- **Authors**: Giorgos Karvounas, Iason Oikonomidis, Antonis Argyros
- **Comment**: Accepted for presentation as a regular paper in the Intelligent
  ShortVideo workshop, organized in conjunction with ICCV 2019
- **Journal**: None
- **Summary**: We address the problem of temporal localization of repetitive activities in a video, i.e., the problem of identifying all segments of a video that contain some sort of repetitive or periodic motion. To do so, the proposed method represents a video by the matrix of pairwise frame distances. These distances are computed on frame representations obtained with a convolutional neural network. On top of this representation, we design, implement and evaluate ReActNet, a lightweight convolutional neural network that classifies a given frame as belonging (or not) to a repetitive video segment. An important property of the employed representation is that it can handle repetitive segments of arbitrary number and duration. Furthermore, the proposed training process requires a relatively small number of annotated videos. Our method raises several of the limiting assumptions of existing approaches regarding the contents of the video and the types of the observed repetitive activities. Experimental results on recent, publicly available datasets validate our design choices, verify the generalization potential of ReActNet and demonstrate its superior performance in comparison to the current state of the art.



### What's in my Room? Object Recognition on Indoor Panoramic Images
- **Arxiv ID**: http://arxiv.org/abs/1910.06138v2
- **DOI**: 10.1109/ICRA40945.2020.9197335
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06138v2)
- **Published**: 2019-10-14 13:40:45+00:00
- **Updated**: 2021-02-18 18:04:41+00:00
- **Authors**: Julia Guerrero-Viu, Clara Fernandez-Labrador, Cédric Demonceaux, Jose J. Guerrero
- **Comment**: Project webpage: "https://webdiis.unizar.es/~jguerrer/room_OR/"
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA), pages 567-573
- **Summary**: In the last few years, there has been a growing interest in taking advantage of the 360 panoramic images potential, while managing the new challenges they imply. While several tasks have been improved thanks to the contextual information these images offer, object recognition in indoor scenes still remains a challenging problem that has not been deeply investigated. This paper provides an object recognition system that performs object detection and semantic segmentation tasks by using a deep learning model adapted to match the nature of equirectangular images. From these results, instance segmentation masks are recovered, refined and transformed into 3D bounding boxes that are placed into the 3D model of the room. Quantitative and qualitative results support that our method outperforms the state of the art by a large margin and show a complete understanding of the main objects in indoor scenes.



### Direct Energy-resolving CT Imaging via Energy-integrating CT images using a Unified Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1910.06154v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.06154v1)
- **Published**: 2019-10-14 14:05:17+00:00
- **Updated**: 2019-10-14 14:05:17+00:00
- **Authors**: Lisha Yao, Sui Li, Manman Zhu, Dong Zeng, Zhaoying Bian, Jianhua Ma
- **Comment**: 5 pages, 3 figures, Accepted by MIC/NSS 2019
- **Journal**: None
- **Summary**: Energy-resolving computed tomography (ErCT) has the ability to acquire energy-dependent measurements simultaneously and quantitative material information with improved contrast-to-noise ratio. Meanwhile, ErCT imaging system is usually equipped with an advanced photon counting detector, which is expensive and technically complex. Therefore, clinical ErCT scanners are not yet commercially available, and they are in various stage of completion. This makes the researchers less accessible to the ErCT images. In this work, we investigate to produce ErCT images directly from existing energy-integrating CT (EiCT) images via deep neural network. Specifically, different from other networks that produce ErCT images at one specific energy, this model employs a unified generative adversarial network (uGAN) to concurrently train EiCT datasets and ErCT datasets with different energies and then performs image-to-image translation from existing EiCT images to multiple ErCT image outputs at various energy bins. In this study, the present uGAN generates ErCT images at 70keV, 90keV, 110keV, and 130keV simultaneously from EiCT images at140kVp. We evaluate the present uGAN model on a set of over 1380 CT image slices and show that the present uGAN model can produce promising ErCT estimation results compared with the ground truth qualitatively and quantitatively.



### Mask-Guided Attention Network for Occluded Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.06160v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06160v2)
- **Published**: 2019-10-14 14:13:43+00:00
- **Updated**: 2019-10-15 09:25:52+00:00
- **Authors**: Yanwei Pang, Jin Xie, Muhammad Haris Khan, Rao Muhammad Anwer, Fahad Shahbaz Khan, Ling Shao
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Pedestrian detection relying on deep convolution neural networks has made significant progress. Though promising results have been achieved on standard pedestrians, the performance on heavily occluded pedestrians remains far from satisfactory. The main culprits are intra-class occlusions involving other pedestrians and inter-class occlusions caused by other objects, such as cars and bicycles. These result in a multitude of occlusion patterns. We propose an approach for occluded pedestrian detection with the following contributions. First, we introduce a novel mask-guided attention network that fits naturally into popular pedestrian detection pipelines. Our attention network emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. Second, we empirically demonstrate that coarse-level segmentation annotations provide reasonable approximation to their dense pixel-wise counterparts. Experiments are performed on CityPersons and Caltech datasets. Our approach sets a new state-of-the-art on both datasets. Our approach obtains an absolute gain of 9.5% in log-average miss rate, compared to the best reported results on the heavily occluded (HO) pedestrian set of CityPersons test set. Further, on the HO pedestrian set of Caltech dataset, our method achieves an absolute gain of 5.0% in log-average miss rate, compared to the best reported results. Code and models are available at: https://github.com/Leotju/MGAN.



### KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment
- **Arxiv ID**: http://arxiv.org/abs/1910.06180v2
- **DOI**: 10.1109/TIP.2020.2967829
- **Categories**: **cs.CV**, cs.MM, I.4.9; I.4.m
- **Links**: [PDF](http://arxiv.org/pdf/1910.06180v2)
- **Published**: 2019-10-14 14:38:48+00:00
- **Updated**: 2020-05-27 09:40:51+00:00
- **Authors**: Vlad Hosu, Hanhe Lin, Tamas Sziranyi, Dietmar Saupe
- **Comment**: Published
- **Journal**: Trans. Image Proc. 29 (2020) 4041-4056
- **Summary**: Deep learning methods for image quality assessment (IQA) are limited due to the small size of existing datasets. Extensive datasets require substantial resources both for generating publishable content and annotating it accurately. We present a systematic and scalable approach to creating KonIQ-10k, the largest IQA dataset to date, consisting of 10,073 quality scored images. It is the first in-the-wild database aiming for ecological validity, concerning the authenticity of distortions, the diversity of content, and quality-related indicators. Through the use of crowdsourcing, we obtained 1.2 million reliable quality ratings from 1,459 crowd workers, paving the way for more general IQA models. We propose a novel, deep learning model (KonCept512), to show an excellent generalization beyond the test set (0.921 SROCC), to the current state-of-the-art database LIVE-in-the-Wild (0.825 SROCC). The model derives its core performance from the InceptionResNet architecture, being trained at a higher resolution than previous models (512x384). Correlation analysis shows that KonCept512 performs similar to having 9 subjective scores for each test image.



### Unsupervised Multi-stream Highlight detection for the Game "Honor of Kings"
- **Arxiv ID**: http://arxiv.org/abs/1910.06189v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06189v2)
- **Published**: 2019-10-14 14:58:56+00:00
- **Updated**: 2019-10-22 06:32:02+00:00
- **Authors**: Li Wang, Zixun Sun, Wentao Yao, Hui Zhan, Chengwei Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing popularity of E-sport live, Highlight Flashback has been a critical functionality of live platforms, which aggregates the overall exciting fighting scenes in a few seconds. In this paper, we introduce a novel training strategy without any additional annotation to automatically generate highlights for game video live. Considering that the existing manual edited clips contain more highlights than long game live videos, we perform pair-wise ranking constraints across clips from edited and long live videos. A multi-stream framework is also proposed to fuse spatial, temporal as well as audio features extracted from videos. To evaluate our method, we test on long game live videos with an average length of about 15 minutes. Extensive experimental results on videos demonstrate its satisfying performance on highlights generation and effectiveness by the fusion of three streams.



### Variational Tracking and Prediction with Generative Disentangled State-Space Models
- **Arxiv ID**: http://arxiv.org/abs/1910.06205v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06205v1)
- **Published**: 2019-10-14 15:20:30+00:00
- **Updated**: 2019-10-14 15:20:30+00:00
- **Authors**: Adnan Akhundov, Maximilian Soelch, Justin Bayer, Patrick van der Smagt
- **Comment**: None
- **Journal**: None
- **Summary**: We address tracking and prediction of multiple moving objects in visual data streams as inference and sampling in a disentangled latent state-space model. By encoding objects separately and including explicit position information in the latent state space, we perform tracking via amortized variational Bayesian inference of the respective latent positions. Inference is implemented in a modular neural framework tailored towards our disentangled latent space. Generative and inference model are jointly learned from observations only. Comparing to related prior work, we empirically show that our Markovian state-space assumption enables faithful and much improved long-term prediction well beyond the training horizon. Further, our inference model correctly decomposes frames into objects, even in the presence of occlusions. Tracking performance is increased significantly over prior art.



### Spatial and Colour Opponency in Anatomically Constrained Deep Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.11086v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11086v1)
- **Published**: 2019-10-14 15:28:44+00:00
- **Updated**: 2019-10-14 15:28:44+00:00
- **Authors**: Ethan Harris, Daniela Mihai, Jonathon Hare
- **Comment**: None
- **Journal**: None
- **Summary**: Colour vision has long fascinated scientists, who have sought to understand both the physiology of the mechanics of colour vision and the psychophysics of colour perception. We consider representations of colour in anatomically constrained convolutional deep neural networks. Following ideas from neuroscience, we classify cells in early layers into groups relating to their spectral and spatial functionality. We show the emergence of single and double opponent cells in our networks and characterise how the distribution of these cells changes under the constraint of a retinal bottleneck. Our experiments not only open up a new understanding of how deep networks process spatial and colour information, but also provide new tools to help understand the black box of deep learning. The code for all experiments is avaialable at \url{https://github.com/ecs-vlc/opponency}.



### ICPS-net: An End-to-End RGB-based Indoor Camera Positioning System using deep convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06219v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06219v1)
- **Published**: 2019-10-14 15:43:56+00:00
- **Updated**: 2019-10-14 15:43:56+00:00
- **Authors**: Ali Ghofrani, Rahil Mahdian Toroghi, Sayed Mojtaba Tabatabaie
- **Comment**: 7 pages, 8 figures, pre-print of ICMV2019 accepted paper
- **Journal**: None
- **Summary**: Indoor positioning and navigation inside an area with no GPS-data availability is a challenging problem. There are applications such as augmented reality, autonomous driving, navigation of drones inside tunnels, in which indoor positioning gets crucial. In this paper, a tandem architecture of deep network-based systems, for the first time to our knowledge, is developed to address this problem. This structure is trained on the scene images being obtained through scanning of the desired area segments using photogrammetry. A CNN structure based on EfficientNet is trained as a classifier of the scenes, followed by a MobileNet CNN structure which is trained to perform as a regressor. The proposed system achieves amazingly fine precisions for both Cartesian position and Quaternion information of the camera.



### Wasserstein Distance Guided Cross-Domain Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.07676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07676v1)
- **Published**: 2019-10-14 16:37:09+00:00
- **Updated**: 2019-10-14 16:37:09+00:00
- **Authors**: Jie Su
- **Comment**: 47 pages, Master Thesis
- **Journal**: None
- **Summary**: Domain adaptation aims to generalise a high-performance learner on target domain (non-labelled data) by leveraging the knowledge from source domain (rich labelled data) which comes from a different but related distribution. Assuming the source and target domains data(e.g. images) come from a joint distribution but follow on different marginal distributions, the domain adaptation work aims to infer the joint distribution from the source and target domain to learn the domain invariant features. Therefore, in this study, I extend the existing state-of-the-art approach to solve the domain adaptation problem. In particular, I propose a new approach to infer the joint distribution of images from different distributions, namely Wasserstein Distance Guided Cross-Domain Learning (WDGCDL). WDGCDL applies the Wasserstein distance to estimate the divergence between the source and target distribution which provides good gradient property and promising generalisation bound. Moreover, to tackle the training difficulty of the proposed framework, I propose two different training schemes for stable training. Qualitative results show that this new approach is superior to the existing state-of-the-art methods in the standard domain adaptation benchmark.



### Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks
- **Arxiv ID**: http://arxiv.org/abs/1910.06259v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CR, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06259v4)
- **Published**: 2019-10-14 16:38:03+00:00
- **Updated**: 2020-06-30 12:03:44+00:00
- **Authors**: David Stutz, Matthias Hein, Bernt Schiele
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.



### Real-world adversarial attack on MTCNN face detection system
- **Arxiv ID**: http://arxiv.org/abs/1910.06261v2
- **DOI**: 10.1109/SIBIRCON48586.2019.8958122
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06261v2)
- **Published**: 2019-10-14 16:42:09+00:00
- **Updated**: 2020-04-02 22:29:44+00:00
- **Authors**: Edgar Kaziakhmedov, Klim Kireev, Grigorii Melnikov, Mikhail Pautov, Aleksandr Petiushko
- **Comment**: None
- **Journal**: 2019 International Multi-Conference on Engineering, Computer and
  Information Sciences (SIBIRCON)
- **Summary**: Recent studies proved that deep learning approaches achieve remarkable results on face detection task. On the other hand, the advances gave rise to a new problem associated with the security of the deep convolutional neural network models unveiling potential risks of DCNNs based applications. Even minor input changes in the digital domain can result in the network being fooled. It was shown then that some deep learning-based face detectors are prone to adversarial attacks not only in a digital domain but also in the real world. In the paper, we investigate the security of the well-known cascade CNN face detection system - MTCNN and introduce an easily reproducible and a robust way to attack it. We propose different face attributes printed on an ordinary white and black printer and attached either to the medical face mask or to the face directly. Our approach is capable of breaking the MTCNN detector in a real-world scenario.



### Scale-Equivariant Steerable Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.11093v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11093v2)
- **Published**: 2019-10-14 16:46:34+00:00
- **Updated**: 2020-02-06 14:09:17+00:00
- **Authors**: Ivan Sosnovik, Michał Szmaja, Arnold Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously developed methods for scale equivariance and local scale invariance. We demonstrate state-of-the-art results on MNIST-scale dataset and on STL-10 dataset in the supervised learning setting.



### Organ-based Chronological Age Estimation based on 3D MRI Scans
- **Arxiv ID**: http://arxiv.org/abs/1910.06271v2
- **DOI**: 10.23919/Eusipco47968.2020.9287398
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06271v2)
- **Published**: 2019-10-14 16:55:10+00:00
- **Updated**: 2020-03-03 11:39:16+00:00
- **Authors**: Karim Armanious, Sherif Abdulatif, Anish Rao Bhaktharaguttu, Thomas Küstner, Tobias Hepp, Sergios Gatidis, Bin Yang
- **Comment**: Submitted to IEEE EUSIPCO 2020
- **Journal**: None
- **Summary**: Individuals age differently depending on a multitude of different factors such as lifestyle, medical history and genetics. Often, the global chronological age is not indicative of the true ageing process. An organ-based age estimation would yield a more accurate health state assessment. In this work, we propose a new deep learning architecture for organ-based age estimation based on magnetic resonance images (MRI). The proposed network is a 3D convolutional neural network (CNN) with increased depth and width made possible by the hybrid utilization of inception and fire modules. We apply the proposed framework for the tasks of brain and knee age estimation. Quantitative comparisons against concurrent MR-based regression networks and different 2D and 3D data feeding strategies illustrated the superior performance of the proposed work.



### Distribution-Aware Coordinate Representation for Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.06278v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06278v1)
- **Published**: 2019-10-14 17:03:56+00:00
- **Updated**: 2019-10-14 17:03:56+00:00
- **Authors**: Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu
- **Comment**: Results on the COCO keypoint detection challenge: 78.9% AP on the
  test-dev set (Top-1 in the leaderbord by 12 Oct 2019) and 76.4% AP on the
  test-challenge set. Project page: https://ilovepose.github.io/coco
- **Journal**: None
- **Summary**: While being the de facto standard coordinate representation in human pose estimation, heatmap is never systematically investigated in the literature, to our best knowledge. This work fills this gap by studying the coordinate representation with a particular focus on the heatmap. Interestingly, we found that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for human pose estimation performance, which nevertheless was not recognised before. In light of the discovered importance, we further probe the design limitations of the standard coordinate decoding method widely used by existing methods, and propose a more principled distribution-aware decoding method. Meanwhile, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating accurate heatmap distributions for unbiased model training. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoint (DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves the performance of a variety of state-of-the-art human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO, consistently validating the usefulness and effectiveness of our novel coordinate representation idea.



### Finding New Diagnostic Information for Detecting Glaucoma using Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06302v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06302v2)
- **Published**: 2019-10-14 17:29:17+00:00
- **Updated**: 2020-09-03 02:36:09+00:00
- **Authors**: Erfan Noury, Suria S. Mannil, Robert T. Chang, An Ran Ran, Carol Y. Cheung, Suman S. Thapa, Harsha L. Rao, Srilakshmi Dasari, Mohammed Riyazuddin, Dolly Chang, Sriharsha Nagaraj, Clement C. Tham, Reza Zadeh
- **Comment**: 28 pages, 12 figures, 15 tables, title changed, new authors added
- **Journal**: None
- **Summary**: We describe a new approach to automated Glaucoma detection in 3D Spectral Domain Optical Coherence Tomography (OCT) optic nerve scans. First, we gathered a unique and diverse multi-ethnic dataset of OCT scans consisting of glaucoma and non-glaucomatous cases obtained from four tertiary care eye hospitals located in four different countries. Using this longitudinal data, we achieved state-of-the-art results for automatically detecting Glaucoma from a single raw OCT using a 3D Deep Learning system. These results are close to human doctors in a variety of settings across heterogeneous datasets and scanning environments. To verify correctness and interpretability of the automated categorization, we used saliency maps to find areas of focus for the model. Matching human doctor behavior, the model predictions indeed correlated with the conventional diagnostic parameters in the OCT printouts, such as the retinal nerve fiber layer. We further used our model to find new areas in the 3D data that are presently not being identified as a diagnostic parameter to detect glaucoma by human doctors. Namely, we found that the Lamina Cribrosa (LC) region can be a valuable source of helpful diagnostic information previously unavailable to doctors during routine clinical care because it lacks a quantitative printout. Our model provides such volumetric quantification of this region. We found that even when a majority of the RNFL is removed, the LC region can distinguish glaucoma. This is clinically relevant in high myopes, when the RNFL is already reduced, and thus the LC region may help differentiate glaucoma in this confounding situation. We further generalize this approach to create a new algorithm called DiagFind that provides a recipe for finding new diagnostic information in medical imagery that may have been previously unusable by doctors.



### Deep learning for Aerosol Forecasting
- **Arxiv ID**: http://arxiv.org/abs/1910.06789v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, physics.ao-ph, physics.data-an, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06789v1)
- **Published**: 2019-10-14 17:35:08+00:00
- **Updated**: 2019-10-14 17:35:08+00:00
- **Authors**: Caleb Hoyne, S. Karthik Mukkavilli, David Meger
- **Comment**: Machine Learning and the Physical Sciences Workshop at the 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada
- **Journal**: None
- **Summary**: Reanalysis datasets combining numerical physics models and limited observations to generate a synthesised estimate of variables in an Earth system, are prone to biases against ground truth. Biases identified with the NASA Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2) aerosol optical depth (AOD) dataset, against the Aerosol Robotic Network (AERONET) ground measurements in previous studies, motivated the development of a deep learning based AOD prediction model globally. This study combines a convolutional neural network (CNN) with MERRA-2, tested against all AERONET sites. The new hybrid CNN-based model provides better estimates validated versus AERONET ground truth, than only using MERRA-2 reanalysis.



### Dynamic Attention Networks for Task Oriented Grounding
- **Arxiv ID**: http://arxiv.org/abs/1910.06315v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06315v1)
- **Published**: 2019-10-14 17:57:21+00:00
- **Updated**: 2019-10-14 17:57:21+00:00
- **Authors**: Soumik Dasgupta, Badri N. Patro, Vinay P. Namboodiri
- **Comment**: Accepted ICCV 2019 Workshop
- **Journal**: None
- **Summary**: In order to successfully perform tasks specified by natural language instructions, an artificial agent operating in a visual world needs to map words, concepts, and actions from the instruction to visual elements in its environment. This association is termed as Task-Oriented Grounding. In this work, we propose a novel Dynamic Attention Network architecture for the efficient multi-modal fusion of text and visual representations which can generate a robust definition of state for the policy learner. Our model assumes no prior knowledge from visual and textual domains and is an end to end trainable. For a 3D visual world where the observation changes continuously, the attention on the visual elements tends to be highly co-related from a one-time step to the next. We term this as "Dynamic Attention". In this work, we show that Dynamic Attention helps in achieving grounding and also aids in the policy learning objective. Since most practical robotic applications take place in the real world where the observation space is continuous, our framework can be used as a generalized multi-modal fusion unit for robotic control through natural language. We show the effectiveness of using 1D convolution over Gated Attention Hadamard product on the rate of convergence of the network. We demonstrate that the cell-state of a Long Short Term Memory (LSTM) is a natural choice for modeling Dynamic Attention and shows through visualization that the generated attention is very close to how humans tend to focus on the environment.



### NeurVPS: Neural Vanishing Point Scanning via Conic Convolution
- **Arxiv ID**: http://arxiv.org/abs/1910.06316v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06316v3)
- **Published**: 2019-10-14 17:58:23+00:00
- **Updated**: 2021-03-25 03:03:02+00:00
- **Authors**: Yichao Zhou, Haozhi Qi, Jingwei Huang, Yi Ma
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: We present a simple yet effective end-to-end trainable deep network with geometry-inspired convolutional operators for detecting vanishing points in images. Traditional convolutional neural networks rely on aggregating edge features and do not have mechanisms to directly exploit the geometric properties of vanishing points as the intersections of parallel lines. In this work, we identify a canonical conic space in which the neural network can effectively compute the global geometric information of vanishing points locally, and we propose a novel operator named conic convolution that can be implemented as regular convolutions in this space. This new operator explicitly enforces feature extractions and aggregations along the structural lines and yet has the same number of parameters as the regular 2D convolution. Our extensive experiments on both synthetic and real-world datasets show that the proposed operator significantly improves the performance of vanishing point detection over traditional methods. The code and dataset have been made publicly available at https://github.com/zhou13/neurvps.



### Building Information Modeling and Classification by Visual Learning At A City Scale
- **Arxiv ID**: http://arxiv.org/abs/1910.06391v2
- **DOI**: 10.5281/zenodo.3996808
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06391v2)
- **Published**: 2019-10-14 19:44:25+00:00
- **Updated**: 2020-07-21 00:59:50+00:00
- **Authors**: Qian Yu, Chaofeng Wang, Barbaros Cetiner, Stella X. Yu, Frank Mckenna, Ertugrul Taciroglu, Kincho H. Law
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Journal**: None
- **Summary**: In this paper, we provide two case studies to demonstrate how artificial intelligence can empower civil engineering. In the first case, a machine learning-assisted framework, BRAILS, is proposed for city-scale building information modeling. Building information modeling (BIM) is an efficient way of describing buildings, which is essential to architecture, engineering, and construction. Our proposed framework employs deep learning technique to extract visual information of buildings from satellite/street view images. Further, a novel machine learning (ML)-based statistical tool, SURF, is proposed to discover the spatial patterns in building metadata.   The second case focuses on the task of soft-story building classification. Soft-story buildings are a type of buildings prone to collapse during a moderate or severe earthquake. Hence, identifying and retrofitting such buildings is vital in the current earthquake preparedness efforts. For this task, we propose an automated deep learning-based procedure for identifying soft-story buildings from street view images at a regional scale. We also create a large-scale building image database and a semi-automated image labeling approach that effectively annotates new database entries. Through extensive computational experiments, we demonstrate the effectiveness of the proposed method.



### FireNet: Real-time Segmentation of Fire Perimeter from Aerial Video
- **Arxiv ID**: http://arxiv.org/abs/1910.06407v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06407v1)
- **Published**: 2019-10-14 20:19:15+00:00
- **Updated**: 2019-10-14 20:19:15+00:00
- **Authors**: Jigar Doshi, Dominic Garcia, Cliff Massey, Pablo Llueca, Nicolas Borensztein, Michael Baird, Matthew Cook, Devaki Raj
- **Comment**: Published at NeurIPS 2019; Workshop on Artificial Intelligence for
  Humanitarian Assistance and Disaster Response(AI+HADR 2019)
- **Journal**: None
- **Summary**: In this paper, we share our approach to real-time segmentation of fire perimeter from aerial full-motion infrared video. We start by describing the problem from a humanitarian aid and disaster response perspective. Specifically, we explain the importance of the problem, how it is currently resolved, and how our machine learning approach improves it. To test our models we annotate a large-scale dataset of 400,000 frames with guidance from domain experts. Finally, we share our approach currently deployed in production with inference speed of 20 frames per second and an accuracy of 92 (F1 Score).



### Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position
- **Arxiv ID**: http://arxiv.org/abs/1910.06425v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06425v1)
- **Published**: 2019-10-14 21:17:52+00:00
- **Updated**: 2019-10-14 21:17:52+00:00
- **Authors**: Haonan Peng, Xingjian Yang, Yun-Hsuan Su, Blake Hannaford
- **Comment**: 6 pages, 10 figures, ICRA2020(under review)
- **Journal**: None
- **Summary**: Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calculation. Since the reported end effector position of surgical robots like RAVEN-II is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker.



### Tell-the-difference: Fine-grained Visual Descriptor via a Discriminating Referee
- **Arxiv ID**: http://arxiv.org/abs/1910.06426v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.06426v1)
- **Published**: 2019-10-14 21:21:03+00:00
- **Updated**: 2019-10-14 21:21:03+00:00
- **Authors**: Shuangjie Xu, Feng Xu, Yu Cheng, Pan Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we investigate a novel problem of telling the difference between image pairs in natural language. Compared to previous approaches for single image captioning, it is challenging to fetch linguistic representation from two independent visual information. To this end, we have proposed an effective encoder-decoder caption framework based on Hyper Convolution Net. In addition, a series of novel feature fusing techniques for pairwise visual information fusing are introduced and a discriminating referee is proposed to evaluate the pipeline. Because of the lack of appropriate datasets to support this task, we have collected and annotated a large new dataset with Amazon Mechanical Turk (AMT) for generating captions in a pairwise manner (with 14764 images and 26710 image pairs in total). The dataset is the first one on the relative difference caption task that provides descriptions in free language. We evaluate the effectiveness of our model on two datasets in the field and it outperforms the state-of-the-art approach by a large margin.



### Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06428v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06428v1)
- **Published**: 2019-10-14 21:22:54+00:00
- **Updated**: 2019-10-14 21:22:54+00:00
- **Authors**: Bairavi Venkatesh, Tosha Shah, Antong Chen, Soheil Ghafurian
- **Comment**: None
- **Journal**: None
- **Summary**: It is common for pathologists to annotate specific regions of the tissue, such as tumor, directly on the glass slide with markers. Although this practice was helpful prior to the advent of histology whole slide digitization, it often occludes important details which are increasingly relevant to immuno-oncology due to recent advancements in digital pathology imaging techniques. The current work uses a generative adversarial network with cycle loss to remove these annotations while still maintaining the underlying structure of the tissue by solving an image-to-image translation problem. We train our network on up to 300 whole slide images with marker inks and show that 70% of the corrected image patches are indistinguishable from originally uncontaminated image tissue to a human expert. This portion increases 97% when we replace the human expert with a deep residual network. We demonstrated the fidelity of the method to the original image by calculating the correlation between image gradient magnitudes. We observed a revival of up to 94,000 nuclei per slide in our dataset, the majority of which were located on tissue border.



### Building Damage Detection in Satellite Imagery Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06444v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06444v1)
- **Published**: 2019-10-14 22:03:49+00:00
- **Updated**: 2019-10-14 22:03:49+00:00
- **Authors**: Joseph Z. Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, Valeriya Zaytseva
- **Comment**: None
- **Journal**: None
- **Summary**: In all types of disasters, from earthquakes to armed conflicts, aid workers need accurate and timely data such as damage to buildings and population displacement to mount an effective response. Remote sensing provides this data at an unprecedented scale, but extracting operationalizable information from satellite images is slow and labor-intensive. In this work, we use machine learning to automate the detection of building damage in satellite imagery. We compare the performance of four different convolutional neural network models in detecting damaged buildings in the 2010 Haiti earthquake. We also quantify how well the models will generalize to future disasters by training and testing models on different disaster events.



