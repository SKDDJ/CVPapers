# Arxiv Papers in cs.CV on 2019-10-18
### Normal Estimation for 3D Point Clouds via Local Plane Constraint and Multi-scale Selection
- **Arxiv ID**: http://arxiv.org/abs/1910.08537v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08537v1)
- **Published**: 2019-10-18 00:17:42+00:00
- **Updated**: 2019-10-18 00:17:42+00:00
- **Authors**: Jun Zhou, Hua Huang, Bin Liu, Xiuping Liu
- **Comment**: arXiv admin note: text overlap with arXiv:1710.04954,
  arXiv:1904.07172, arXiv:1812.00709 by other authors
- **Journal**: None
- **Summary**: In this paper, we propose a normal estimation method for unstructured 3D point clouds. In this method, a feature constraint mechanism called Local Plane Features Constraint (LPFC) is used and then a multi-scale selection strategy is introduced. The LPEC can be used in a single-scale point network architecture for a more stable normal estimation of the unstructured 3D point clouds. In particular, it can partly overcome the influence of noise on a large sampling scale compared to the other methods which only use regression loss for normal estimation. For more details, a subnetwork is built after point-wise features extracted layers of the network and it gives more constraints to each point of the local patch via a binary classifier in the end. Then we use multi-task optimization to train the normal estimation and local plane classification tasks simultaneously.Also, to integrate the advantages of multi-scale results, a scale selection strategy is adopted, which is a data-driven approach for selecting the optimal scale around each point and encourages subnetwork specialization. Specifically, we employed a subnetwork called Scale Estimation Network to extract scale weight information from multi-scale features. More analysis is given about the relations between noise levels, local boundary, and scales in the experiment. These relationships can be a better guide to choosing particular scales for a particular model. Besides, the experimental result shows that our network can distinguish the points on the fitting plane accurately and this can be used to guide the normal estimation and our multi-scale method can improve the results well. Compared to some state-of-the-art surface normal estimators, our method is robust to noise and can achieve competitive results.



### Bilinear Constraint based ADMM for Mixed Poisson-Gaussian Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/1910.08206v2
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08206v2)
- **Published**: 2019-10-18 00:38:50+00:00
- **Updated**: 2020-01-27 20:16:42+00:00
- **Authors**: Jie Zhang, Yuping Duan, Yue Lu, Michael K. Ng, Huibin Chang
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose new operator-splitting algorithms for the total variation regularized infimal convolution (TV-IC) model [4] in order to remove mixed Poisson-Gaussian(MPG) noise. In the existing splitting algorithm for TV-IC, an inner loop by Newton method had to be adopted for one nonlinear optimization subproblem, which increased the computation cost per outer loop. By introducing a new bilinear constraint and applying the alternating direction method of multipliers (ADMM), all subproblems of the proposed algorithms named as BCA (short for Bilinear Constraint based ADMM algorithm) and BCAf(short for a variant of BCA with fully splitting form) can be very efficiently solved; especially for the proposed BCAf, they can be calculated without any inner iterations. Under mild conditions, the convergence of the proposed BCA is investigated. Numerically, compared to existing primal-dual algorithms for the TV-IC model, the proposed algorithms, with fewer tunable parameters, converge much faster and produce comparable results meanwhile.



### Unsupervised Multi-Task Feature Learning on Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1910.08207v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08207v1)
- **Published**: 2019-10-18 00:43:29+00:00
- **Updated**: 2019-10-18 00:43:29+00:00
- **Authors**: Kaveh Hassani, Mike Haley
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%.



### A Deep Learning-based Framework for the Detection of Schools of Herring in Echograms
- **Arxiv ID**: http://arxiv.org/abs/1910.08215v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08215v1)
- **Published**: 2019-10-18 01:12:46+00:00
- **Updated**: 2019-10-18 01:12:46+00:00
- **Authors**: Alireza Rezvanifar, Tunai Porto Marques, Melissa Cote, Alexandra Branzan Albu, Alex Slonimer, Thomas Tolhurst, Kaan Ersahin, Todd Mudge, Stephane Gauthier
- **Comment**: Accepted to NeurIPS 2019 workshop on Tackling Climate Change with
  Machine Learning, Vancouver, Canada
- **Journal**: None
- **Summary**: Tracking the abundance of underwater species is crucial for understanding the effects of climate change on marine ecosystems. Biologists typically monitor underwater sites with echosounders and visualize data as 2D images (echograms); they interpret these data manually or semi-automatically, which is time-consuming and prone to inconsistencies. This paper proposes a deep learning framework for the automatic detection of schools of herring from echograms. Experiments demonstrated that our approach outperforms a traditional machine learning algorithm using hand-crafted features. Our framework could easily be expanded to detect more species of interest to sustainable fisheries.



### Toward 3D Object Reconstruction from Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/1910.08223v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08223v2)
- **Published**: 2019-10-18 01:53:39+00:00
- **Updated**: 2019-10-21 11:43:43+00:00
- **Authors**: Haozhe Xie, Hongxun Yao, Shangchen Zhou, Shengping Zhang, Xiaoshuai Sun, Wenxiu Sun
- **Comment**: None
- **Journal**: None
- **Summary**: Inferring the 3D shape of an object from an RGB image has shown impressive results, however, existing methods rely primarily on recognizing the most similar 3D model from the training set to solve the problem. These methods suffer from poor generalization and may lead to low-quality reconstructions for unseen objects. Nowadays, stereo cameras are pervasive in emerging devices such as dual-lens smartphones and robots, which enables the use of the two-view nature of stereo images to explore the 3D structure and thus improve the reconstruction performance. In this paper, we propose a new deep learning framework for reconstructing the 3D shape of an object from a pair of stereo images, which reasons about the 3D structure of the object by taking bidirectional disparities and feature correspondences between the two views into account. Besides, we present a large-scale synthetic benchmarking dataset, namely StereoShapeNet, containing 1,052,976 pairs of stereo images rendered from ShapeNet along with the corresponding bidirectional depth and disparity maps. Experimental results on the StereoShapeNet benchmark demonstrate that the proposed framework outperforms the state-of-the-art methods.



### Adversarial T-shirt! Evading Person Detectors in A Physical World
- **Arxiv ID**: http://arxiv.org/abs/1910.11099v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11099v3)
- **Published**: 2019-10-18 02:20:17+00:00
- **Updated**: 2020-07-06 03:06:26+00:00
- **Authors**: Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin
- **Comment**: None
- **Journal**: None
- **Summary**: It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decisionmakers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to-rigid objects such as T-shirts. We show that the proposed method achieves74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.



### Spatially-Aware Graph Neural Networks for Relational Behavior Forecasting from Sensor Data
- **Arxiv ID**: http://arxiv.org/abs/1910.08233v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1910.08233v1)
- **Published**: 2019-10-18 03:14:10+00:00
- **Updated**: 2019-10-18 03:14:10+00:00
- **Authors**: Sergio Casas, Cole Gulino, Renjie Liao, Raquel Urtasun
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of relational behavior forecasting from sensor data. Towards this goal, we propose a novel spatially-aware graph neural network (SpAGNN) that models the interactions between agents in the scene. Specifically, we exploit a convolutional neural network to detect the actors and compute their initial states. A graph neural network then iteratively updates the actor states via a message passing process. Inspired by Gaussian belief propagation, we design the messages to be spatially-transformed parameters of the output distributions from neighboring agents. Our model is fully differentiable, thus enabling end-to-end training. Importantly, our probabilistic predictions can model uncertainty at the trajectory level. We demonstrate the effectiveness of our approach by achieving significant improvements over the state-of-the-art on two real-world self-driving datasets: ATG4D and nuScenes.



### Mirror Descent View for Neural Network Quantization
- **Arxiv ID**: http://arxiv.org/abs/1910.08237v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08237v3)
- **Published**: 2019-10-18 03:19:21+00:00
- **Updated**: 2021-03-02 05:13:00+00:00
- **Authors**: Thalaiyasingam Ajanthan, Kartik Gupta, Philip H. S. Torr, Richard Hartley, Puneet K. Dokania
- **Comment**: This paper was accepted at AISTATS 2021
- **Journal**: None
- **Summary**: Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. It is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we present a numerically stable implementation of MD that requires storing an additional set of auxiliary variables (unconstrained), and show that it is strikingly analogous to the Straight Through Estimator (STE) based method which is typically viewed as a "trick" to avoid vanishing gradients issue. Our experiments on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets with VGG-16, ResNet-18, and MobileNetV2 architectures show that our MD variants obtain quantized networks with state-of-the-art performance. Code is available at https://github.com/kartikgupta-at-anu/md-bnn.



### Investigating Task-driven Latent Feasibility for Nonconvex Image Modeling
- **Arxiv ID**: http://arxiv.org/abs/1910.08242v3
- **DOI**: 10.1109/TIP.2020.3004733
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08242v3)
- **Published**: 2019-10-18 03:36:00+00:00
- **Updated**: 2020-07-02 02:52:51+00:00
- **Authors**: Risheng Liu, Pan Mu, Jian Chen, Xin Fan, Zhongxuan Luo
- **Comment**: 11 pages, Accepted at IEEE TIP
- **Journal**: None
- **Summary**: Properly modeling latent image distributions plays an important role in a variety of image-related vision problems. Most exiting approaches aim to formulate this problem as optimization models (e.g., Maximum A Posterior, MAP) with handcrafted priors. In recent years, different CNN modules are also considered as deep priors to regularize the image modeling process. However, these explicit regularization techniques require deep understandings on the problem and elaborately mathematical skills. In this work, we provide a new perspective, named Task-driven Latent Feasibility (TLF), to incorporate specific task information to narrow down the solution space for the optimization-based image modeling problem. Thanks to the flexibility of TLF, both designed and trained constraints can be embedded into the optimization process. By introducing control mechanisms based on the monotonicity and boundedness conditions, we can also strictly prove the convergence of our proposed inference process. We demonstrate that different types of image modeling problems, such as image deblurring and rain streaks removals, can all be appropriately addressed within our TLF framework. Extensive experiments also verify the theoretical results and show the advantages of our method against existing state-of-the-art approaches.



### AFO-TAD: Anchor-free One-Stage Detector for Temporal Action Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.08250v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08250v1)
- **Published**: 2019-10-18 03:57:05+00:00
- **Updated**: 2019-10-18 03:57:05+00:00
- **Authors**: Yiping Tang, Chuang Niu, Minghao Dong, Shenghan Ren, Jimin Liang
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal action detection is a fundamental yet challenging task in video understanding. Many of the state-of-the-art methods predict the boundaries of action instances based on predetermined anchors akin to the two-dimensional object detection detectors. However, it is hard to detect all the action instances with predetermined temporal scales because the durations of instances in untrimmed videos can vary from few seconds to several minutes. In this paper, we propose a novel action detection architecture named anchor-free one-stage temporal action detector (AFO-TAD). AFO-TAD achieves better performance for detecting action instances with arbitrary lengths and high temporal resolution, which can be attributed to two aspects. First, we design a receptive field adaption module which dynamically adjusts the receptive field for precise action detection. Second, AFO-TAD directly predicts the categories and boundaries at every temporal locations without predetermined anchors. Extensive experiments show that AFO-TAD improves the state-of-the-art performance on THUMOS'14.



### Eye in the Sky: Drone-Based Object Tracking and 3D Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.08259v1
- **DOI**: 10.1145/3343031.3350933
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08259v1)
- **Published**: 2019-10-18 04:44:44+00:00
- **Updated**: 2019-10-18 04:44:44+00:00
- **Authors**: Haotian Zhang, Gaoang Wang, Zhichao Lei, Jenq-Neng Hwang
- **Comment**: Accepted to ACMMM2019
- **Journal**: None
- **Summary**: Drones, or general UAVs, equipped with a single camera have been widely deployed to a broad range of applications, such as aerial photography, fast goods delivery and most importantly, surveillance. Despite the great progress achieved in computer vision algorithms, these algorithms are not usually optimized for dealing with images or video sequences acquired by drones, due to various challenges such as occlusion, fast camera motion and pose variation. In this paper, a drone-based multi-object tracking and 3D localization scheme is proposed based on the deep learning based object detection. We first combine a multi-object tracking method called TrackletNet Tracker (TNT) which utilizes temporal and appearance information to track detected objects located on the ground for UAV applications. Then, we are also able to localize the tracked ground objects based on the group plane estimated from the Multi-View Stereo technique. The system deployed on the drone can not only detect and track the objects in a scene, but can also localize their 3D coordinates in meters with respect to the drone camera. The experiments have proved our tracker can reliably handle most of the detected objects captured by drones and achieve favorable 3D localization performance when compared with the state-of-the-art methods.



### BOBBY2: Buffer Based Robust High-Speed Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.08263v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08263v1)
- **Published**: 2019-10-18 05:10:33+00:00
- **Updated**: 2019-10-18 05:10:33+00:00
- **Authors**: Keifer Lee, Jun Jet Tai, Swee King Phang
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, a novel high-speed single object tracker that is robust against non-semantic distractor exemplars is introduced; dubbed BOBBY2. It incorporates a novel exemplar buffer module that sparsely caches the target's appearance across time, enabling it to adapt to potential target deformation. As for training, an augmented ImageNet-VID dataset was used in conjunction with the one cycle policy, enabling it to reach convergence with less than 2 epoch worth of data. For validation, the model was benchmarked on the GOT-10k dataset and on an additional small, albeit challenging custom UAV dataset collected with the TU-3 UAV. We demonstrate that the exemplar buffer is capable of providing redundancies in case of unintended target drifts, a desirable trait in any middle to long term tracking. Even when the buffer is predominantly filled with distractors instead of valid exemplars, BOBBY2 is capable of maintaining a near-optimal level of accuracy. BOBBY2 manages to achieve a very competitive result on the GOT-10k dataset and to a lesser degree on the challenging custom TU-3 dataset, without fine-tuning, demonstrating its generalizability. In terms of speed, BOBBY2 utilizes a stripped down AlexNet as feature extractor with 63% less parameters than a vanilla AlexNet, thus being able to run at a competitive 85 FPS.



### Semi-supervised Learning using Adversarial Training with Good and Bad Samples
- **Arxiv ID**: http://arxiv.org/abs/1910.08540v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08540v1)
- **Published**: 2019-10-18 05:47:08+00:00
- **Updated**: 2019-10-18 05:47:08+00:00
- **Authors**: Wenyuan Li, Zichen Wang, Yuguang Yue, Jiayun Li, William Speier, Mingyuan Zhou, Corey W. Arnold
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we investigate semi-supervised learning (SSL) for image classification using adversarial training. Previous results have illustrated that generative adversarial networks (GANs) can be used for multiple purposes. Triple-GAN, which aims to jointly optimize model components by incorporating three players, generates suitable image-label pairs to compensate for the lack of labeled data in SSL with improved benchmark performance. Conversely, Bad (or complementary) GAN, optimizes generation to produce complementary data-label pairs and force a classifier's decision boundary to lie between data manifolds. Although it generally outperforms Triple-GAN, Bad GAN is highly sensitive to the amount of labeled data used for training. Unifying these two approaches, we present unified-GAN (UGAN), a novel framework that enables a classifier to simultaneously learn from both good and bad samples through adversarial training. We perform extensive experiments on various datasets and demonstrate that UGAN: 1) achieves state-of-the-art performance among other deep generative models, and 2) is robust to variations in the amount of labeled data used for training.



### PointRNN: Point Recurrent Neural Network for Moving Point Cloud Processing
- **Arxiv ID**: http://arxiv.org/abs/1910.08287v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08287v2)
- **Published**: 2019-10-18 07:21:37+00:00
- **Updated**: 2019-11-24 12:26:29+00:00
- **Authors**: Hehe Fan, Yi Yang
- **Comment**: technical report
- **Journal**: None
- **Summary**: In this paper, we introduce a Point Recurrent Neural Network (PointRNN) for moving point cloud processing. At each time step, PointRNN takes point coordinates $\boldsymbol{P} \in \mathbb{R}^{n \times 3}$ and point features $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ as input ($n$ and $d$ denote the number of points and the number of feature channels, respectively). The state of PointRNN is composed of point coordinates $\boldsymbol{P}$ and point states $\boldsymbol{S} \in \mathbb{R}^{n \times d'}$ ($d'$ denotes the number of state channels). Similarly, the output of PointRNN is composed of $\boldsymbol{P}$ and new point features $\boldsymbol{Y} \in \mathbb{R}^{n \times d''}$ ($d''$ denotes the number of new feature channels). Since point clouds are orderless, point features and states from two time steps can not be directly operated. Therefore, a point-based spatiotemporally-local correlation is adopted to aggregate point features and states according to point coordinates. We further propose two variants of PointRNN, i.e., Point Gated Recurrent Unit (PointGRU) and Point Long Short-Term Memory (PointLSTM). We apply PointRNN, PointGRU and PointLSTM to moving point cloud prediction, which aims to predict the future trajectories of points in a set given their history movements. Experimental results show that PointRNN, PointGRU and PointLSTM are able to produce correct predictions on both synthetic and real-world datasets, demonstrating their ability to model point cloud sequences. The code has been released at \url{https://github.com/hehefan/PointRNN}.



### Diversity in Fashion Recommendation using Semantic Parsing
- **Arxiv ID**: http://arxiv.org/abs/1910.08292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1910.08292v1)
- **Published**: 2019-10-18 07:47:41+00:00
- **Updated**: 2019-10-18 07:47:41+00:00
- **Authors**: Sagar Verma, Sukhad Anand, Chetan Arora, Atul Rai
- **Comment**: 5 pages, ICIP2018, code:
  https://github.com/sagarverma/fashion_recommendation_stlstm
- **Journal**: None
- **Summary**: Developing recommendation system for fashion images is challenging due to the inherent ambiguity associated with what criterion a user is looking at. Suggesting multiple images where each output image is similar to the query image on the basis of a different feature or part is one way to mitigate the problem. Existing works for fashion recommendation have used Siamese or Triplet network to learn features between a similar pair and a similar-dissimilar triplet respectively. However, these methods do not provide basic information such as, how two clothing images are similar, or which parts present in the two images make them similar. In this paper, we propose to recommend images by explicitly learning and exploiting part based similarity. We propose a novel approach of learning discriminative features from weakly-supervised data by using visual attention over the parts and a texture encoding network. We show that the learned features surpass the state-of-the-art in retrieval task on DeepFashion dataset. We then use the proposed model to recommend fashion images having an explicit variation with respect to similarity of any of the parts.



### Development of a hand pose recognition system on an embedded computer using CNNs
- **Arxiv ID**: http://arxiv.org/abs/1910.11100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11100v1)
- **Published**: 2019-10-18 08:09:23+00:00
- **Updated**: 2019-10-18 08:09:23+00:00
- **Authors**: Dennis Núñez Fernández
- **Comment**: LatinX in AI Research at NeurIPS 2019
- **Journal**: None
- **Summary**: Demand of hand pose recognition systems are growing in the last years in technologies like human-machine interfaces. This work suggests an approach for hand pose recognition in embedded computers using hand tracking and CNNs. Results show a fast time response with an accuracy of 94.50% and low power consumption.



### Attention Mechanism Enhanced Kernel Prediction Networks for Denoising of Burst Images
- **Arxiv ID**: http://arxiv.org/abs/1910.08313v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08313v2)
- **Published**: 2019-10-18 09:06:53+00:00
- **Updated**: 2020-01-29 06:03:09+00:00
- **Authors**: Bin Zhang, Shenyao Jin, Yili Xia, Yongming Huang, Zixiang Xiong
- **Comment**: accepted by ICASSP 2020
- **Journal**: None
- **Summary**: Deep learning based image denoising methods have been extensively investigated. In this paper, attention mechanism enhanced kernel prediction networks (AME-KPNs) are proposed for burst image denoising, in which, nearly cost-free attention modules are adopted to first refine the feature maps and to further make a full use of the inter-frame and intra-frame redundancies within the whole image burst. The proposed AME-KPNs output per-pixel spatially-adaptive kernels, residual maps and corresponding weight maps, in which, the predicted kernels roughly restore clean pixels at their corresponding locations via an adaptive convolution operation, and subsequently, residuals are weighted and summed to compensate the limited receptive field of predicted kernels. Simulations and real-world experiments are conducted to illustrate the robustness of the proposed AME-KPNs in burst image denoising.



### Multimodal Image Super-resolution via Deep Unfolding with Side Information
- **Arxiv ID**: http://arxiv.org/abs/1910.08320v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08320v1)
- **Published**: 2019-10-18 09:32:24+00:00
- **Updated**: 2019-10-18 09:32:24+00:00
- **Authors**: Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis, Nikos Deligiannis
- **Comment**: 5 pages, 5 figures, 3 tables, EUSIPCO 2019
- **Journal**: None
- **Summary**: Deep learning methods have been successfully applied to various computer vision tasks. However, existing neural network architectures do not per se incorporate domain knowledge about the addressed problem, thus, understanding what the model has learned is an open research topic. In this paper, we rely on the unfolding of an iterative algorithm for sparse approximation with side information, and design a deep learning architecture for multimodal image super-resolution that incorporates sparse priors and effectively utilizes information from another image modality. We develop two deep models performing reconstruction of a high-resolution image of a target image modality from its low-resolution variant with the aid of a high-resolution image from a second modality. We apply the proposed models to super-resolve near-infrared images using as side information high-resolution RGB\ images. Experimental results demonstrate the superior performance of the proposed models against state-of-the-art methods including unimodal and multimodal approaches.



### OpenDenoising: an Extensible Benchmark for Building Comparative Studies of Image Denoisers
- **Arxiv ID**: http://arxiv.org/abs/1910.08328v1
- **DOI**: 10.1109/ICASSP40776.2020.9053937
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08328v1)
- **Published**: 2019-10-18 09:51:38+00:00
- **Updated**: 2019-10-18 09:51:38+00:00
- **Authors**: Florian Lemarchand, Eduardo Fernandes Montesuma, Maxime Pelcat, Erwan Nogues
- **Comment**: None
- **Journal**: None
- **Summary**: Image denoising has recently taken a leap forward due to machine learning. However, image denoisers, both expert-based and learning-based, are mostly tested on well-behaved generated noises (usually Gaussian) rather than on real-life noises, making performance comparisons difficult in real-world conditions. This is especially true for learning-based denoisers which performance depends on training data. Hence, choosing which method to use for a specific denoising problem is difficult.   This paper proposes a comparative study of existing denoisers, as well as an extensible open tool that makes it possible to reproduce and extend the study. MWCNN is shown to outperform other methods when trained for a real-world image interception noise, and additionally is the second least compute hungry of the tested methods. To evaluate the robustness of conclusions, three test sets are compared. A Kendall's Tau correlation of only 60% is obtained on methods ranking between noise types, demonstrating the need for a benchmarking tool.



### KerCNNs: biologically inspired lateral connections for classification of corrupted images
- **Arxiv ID**: http://arxiv.org/abs/1910.08336v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1910.08336v1)
- **Published**: 2019-10-18 10:31:06+00:00
- **Updated**: 2019-10-18 10:31:06+00:00
- **Authors**: Noemi Montobbio, Laurent Bonnasse-Gahot, Giovanna Citti, Alessandro Sarti
- **Comment**: None
- **Journal**: None
- **Summary**: The state of the art in many computer vision tasks is represented by Convolutional Neural Networks (CNNs). Although their hierarchical organization and local feature extraction are inspired by the structure of primate visual systems, the lack of lateral connections in such architectures critically distinguishes their analysis from biological object processing. The idea of enriching CNNs with recurrent lateral connections of convolutional type has been put into practice in recent years, in the form of learned recurrent kernels with no geometrical constraints. In the present work, we introduce biologically plausible lateral kernels encoding a notion of correlation between the feedforward filters of a CNN: at each layer, the associated kernel acts as a transition kernel on the space of activations. The lateral kernels are defined in terms of the filters, thus providing a parameter-free approach to assess the geometry of horizontal connections based on the feedforward structure. We then test this new architecture, which we call KerCNN, on a generalization task related to global shape analysis and pattern completion: once trained for performing basic image classification, the network is evaluated on corrupted testing images. The image perturbations examined are designed to undermine the recognition of the images via local features, thus requiring an integration of context information - which in biological vision is critically linked to lateral connectivity. Our KerCNNs turn out to be far more stable than CNNs and recurrent CNNs to such degradations, thus validating this biologically inspired approach to reinforce object recognition under challenging conditions.



### Automatic Data Augmentation by Learning the Deterministic Policy
- **Arxiv ID**: http://arxiv.org/abs/1910.08343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08343v2)
- **Published**: 2019-10-18 11:22:32+00:00
- **Updated**: 2019-10-22 01:04:59+00:00
- **Authors**: Yinghuan Shi, Tiexin Qin, Yong Liu, Jiwen Lu, Yang Gao, Dinggang Shen
- **Comment**: Sorry for withdrawing our paper, there exists a mistake in the
  experiment, and we will reupload once we have fixed it
- **Journal**: None
- **Summary**: Aiming to produce sufficient and diverse training samples, data augmentation has been demonstrated for its effectiveness in training deep models. Regarding that the criterion of the best augmentation is challenging to define, we in this paper present a novel learning-based augmentation method termed as DeepAugNet, which formulates the final augmented data as a collection of several sequentially augmented subsets. Specifically, the current augmented subset is required to maximize the performance improvement compared with the last augmented subset by learning the deterministic augmentation policy using deep reinforcement learning. By introducing an unified optimization goal, DeepAugNet intends to combine the data augmentation and the deep model training in an end-to-end training manner which is realized by simultaneously training a hybrid architecture of dueling deep Q-learning algorithm and a surrogate deep model. We extensively evaluated our proposed DeepAugNet on various benchmark datasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature. Compared with the current state-of-the-arts, our method can achieve a significant improvement in small-scale datasets, and a comparable performance in large-scale datasets. Code will be available soon.



### SDCNet: Smoothed Dense-Convolution Network for Restoring Low-Dose Cerebral CT Perfusion
- **Arxiv ID**: http://arxiv.org/abs/1910.08364v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08364v1)
- **Published**: 2019-10-18 12:12:42+00:00
- **Updated**: 2019-10-18 12:12:42+00:00
- **Authors**: Peng Liu, Ruogu Fang
- **Comment**: None
- **Journal**: The IEEE International Symposium on Biomedical Imaging (ISBI 2018)
- **Summary**: With substantial public concerns on potential cancer risks and health hazards caused by the accumulated radiation exposure in medical imaging, reducing radiation dose in X-ray based medical imaging such as Computed Tomography Perfusion (CTP) has raised significant research interests. In this paper, we embrace the deep Convolutional Neural Networks (CNN) based approaches and introduce Smoothed Dense-Convolution Neural Network (SDCNet) to recover high-dose quality CTP images from low-dose ones. SDCNet is composed of sub-network blocks cascaded by skip-connections to infer the noise (differentials) from paired low/high-dose CT scans. SDCNet can effectively remove the noise in real low-dose CT scans and enhance the quality of medical images. We evaluate the proposed architecture on thousands of CT perfusion frames for both reconstructed image denoising and perfusion map quantification including cerebral blood flow (CBF) and cerebral blood volume (CBV). SDCNet achieves high performance in both visual and quantitative results with promising computational efficiency, comparing favorably with state-of-the-art approaches. \textit{The code is available at \url{https://github.com/cswin/RC-Nets}}.



### Image Deconvolution with Deep Image and Kernel Priors
- **Arxiv ID**: http://arxiv.org/abs/1910.08386v1
- **DOI**: 10.1109/ICCVW.2019.00127
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08386v1)
- **Published**: 2019-10-18 12:44:31+00:00
- **Updated**: 2019-10-18 12:44:31+00:00
- **Authors**: Zhunxuan Wang, Zipei Wang, Qiqi Li, Hakan Bilen
- **Comment**: In Proceedings of the 2019 IEEE International Conference on Computer
  Vision Workshops (ICCVW)
- **Journal**: None
- **Summary**: Image deconvolution is the process of recovering convolutional degraded images, which is always a hard inverse problem because of its mathematically ill-posed property. On the success of the recently proposed deep image prior (DIP), we build an image deconvolution model with deep image and kernel priors (DIKP). DIP is a learning-free representation which uses neural net structures to express image prior information, and it showed great success in many energy-based models, e.g. denoising, super-resolution, inpainting. Instead, our DIKP model uses such priors in image deconvolution to model not only images but also kernels, combining the ideas of traditional learning-free deconvolution methods with neural nets. In this paper, we show that DIKP improve the performance of learning-free image deconvolution, and we experimentally demonstrate this on the standard benchmark of six standard test images in terms of PSNR and visual effects.



### A novel centroid update approach for clustering-based superpixel methods and superpixel-based edge detection
- **Arxiv ID**: http://arxiv.org/abs/1910.08439v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08439v2)
- **Published**: 2019-10-18 14:31:52+00:00
- **Updated**: 2020-05-23 05:39:21+00:00
- **Authors**: Houwang Zhang, Chong Wu, Le Zhang, Hanying Zheng
- **Comment**: This paper has been accepted by ICIP2020
- **Journal**: None
- **Summary**: Superpixel is widely used in image processing. And among the methods for superpixel generation, clustering-based methods have a high speed and a good performance at the same time. However, most clustering-based superpixel methods are sensitive to noise. To solve these problems, in this paper, we first analyze the features of noise. Then according to the statistical features of noise, we propose a novel centroid update approach to enhance the robustness of clustering-based superpixel methods. Besides, we propose a novel superpixel-based edge detection method. The experiments on BSD500 dataset show that our approach can significantly enhance the performance of clustering-based superpixel methods in noisy environment. Moreover, we also show that our proposed edge detection method outperforms other classical methods.



### Towards Learning Cross-Modal Perception-Trace Models
- **Arxiv ID**: http://arxiv.org/abs/1910.08549v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.AI, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08549v1)
- **Published**: 2019-10-18 15:20:38+00:00
- **Updated**: 2019-10-18 15:20:38+00:00
- **Authors**: Achim Rettinger, Viktoria Bogdanova, Philipp Niemann
- **Comment**: None
- **Journal**: None
- **Summary**: Representation learning is a key element of state-of-the-art deep learning approaches. It enables to transform raw data into structured vector space embeddings. Such embeddings are able to capture the distributional semantics of their context, e.g. by word windows on natural language sentences, graph walks on knowledge graphs or convolutions on images. So far, this context is manually defined, resulting in heuristics which are solely optimized for computational performance on certain tasks like link-prediction. However, such heuristic models of context are fundamentally different to how humans capture information. For instance, when reading a multi-modal webpage (i) humans do not perceive all parts of a document equally: Some words and parts of images are skipped, others are revisited several times which makes the perception trace highly non-sequential; (ii) humans construct meaning from a document's content by shifting their attention between text and image, among other things, guided by layout and design elements. In this paper we empirically investigate the difference between human perception and context heuristics of basic embedding models. We conduct eye tracking experiments to capture the underlying characteristics of human perception of media documents containing a mixture of text and images. Based on that, we devise a prototypical computational perception-trace model, called CMPM. We evaluate empirically how CMPM can improve a basic skip-gram embedding approach. Our results suggest, that even with a basic human-inspired computational perception model, there is a huge potential for improving embeddings since such a model does inherently capture multiple modalities, as well as layout and design elements.



### Illumination-Based Data Augmentation for Robust Background Subtraction
- **Arxiv ID**: http://arxiv.org/abs/1910.08470v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.08470v1)
- **Published**: 2019-10-18 15:28:59+00:00
- **Updated**: 2019-10-18 15:28:59+00:00
- **Authors**: Dimitrios Sakkos, Hubert P. H. Shum, Edmond S. L. Ho
- **Comment**: SKIMA 2019 - Best Paper Award
- **Journal**: None
- **Summary**: A core challenge in background subtraction (BGS) is handling videos with sudden illumination changes in consecutive frames. In this paper, we tackle the problem from a data point-of-view using data augmentation. Our method performs data augmentation that not only creates endless data on the fly, but also features semantic transformations of illumination which enhance the generalisation of the model. It successfully simulates flashes and shadows by applying the Euclidean distance transform over a binary mask that is randomly generated. Such data allows us to effectively train an illumination-invariant deep learning model for BGS. Experimental results demonstrate the contribution of the synthetics in the ability of the models to perform BGS even when significant illumination changes take place. The source code of the project is made publicly available at https://github.com/dksakkos/illumination_augmentation.



### Understanding Deep Networks via Extremal Perturbations and Smooth Masks
- **Arxiv ID**: http://arxiv.org/abs/1910.08485v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08485v1)
- **Published**: 2019-10-18 16:02:01+00:00
- **Updated**: 2019-10-18 16:02:01+00:00
- **Authors**: Ruth Fong, Mandela Patrick, Andrea Vedaldi
- **Comment**: Accepted at ICCV 2019 as oral; supp mat at
  http://ruthcfong.github.io/files/fong19_extremal_supps.pdf
- **Journal**: None
- **Summary**: The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.



### Single and Cross-Dimensional Feature Detection and Description: An Evaluation
- **Arxiv ID**: http://arxiv.org/abs/1910.08515v1
- **DOI**: 10.1049/iet-ipr.2019.1523
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08515v1)
- **Published**: 2019-10-18 17:21:53+00:00
- **Updated**: 2019-10-18 17:21:53+00:00
- **Authors**: Odysseas Kechagias-Stamatis, Nabil Aouf, Mark A. Richardson
- **Comment**: None
- **Journal**: None
- **Summary**: Three-dimensional local feature detection and description techniques are widely used for object registration and recognition applications. Although several evaluations of 3D local feature detection and description methods have already been published, these are constrained in a single dimensional scheme, i.e. either 3D or 2D methods that are applied onto multiple projections of the 3D data. However, cross-dimensional (mixed 2D and 3D) feature detection and description has yet to be investigated. Here, we evaluated the performance of both single and cross-dimensional feature detection and description methods on several 3D datasets and demonstrated the superiority of cross-dimensional over single-dimensional schemes.



### Texture Bias Of CNNs Limits Few-Shot Classification Performance
- **Arxiv ID**: http://arxiv.org/abs/1910.08519v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08519v1)
- **Published**: 2019-10-18 17:30:11+00:00
- **Updated**: 2019-10-18 17:30:11+00:00
- **Authors**: Sam Ringer, Will Williams, Tom Ash, Remi Francis, David MacLeod
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate image classification given small amounts of labelled data (few-shot classification) remains an open problem in computer vision. In this work we examine how the known texture bias of Convolutional Neural Networks (CNNs) affects few-shot classification performance. Although texture bias can help in standard image classification, in this work we show it significantly harms few-shot classification performance. After correcting this bias we demonstrate state-of-the-art performance on the competitive miniImageNet task using a method far simpler than the current best performing few-shot learning approaches.



### Fast Local Planning and Mapping in Unknown Off-Road Terrain
- **Arxiv ID**: http://arxiv.org/abs/1910.08521v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08521v1)
- **Published**: 2019-10-18 17:33:43+00:00
- **Updated**: 2019-10-18 17:33:43+00:00
- **Authors**: Timothy Overbye, Srikanth Saripalli
- **Comment**: 7 pages, 14 figures
- **Journal**: None
- **Summary**: In this paper, we present a fast, on-line mapping and planning solution for operation in unknown, off-road, environments. We combine obstacle detection along with a terrain gradient map to make simple and adaptable cost map. This map can be created and updated at 10 Hz. An A* planner finds optimal paths over the map. Finally, we take multiple samples over the control input space and do a kinematic forward simulation to generated feasible trajectories. Then the most optimal trajectory, as determined by the cost map and proximity to A* path, is chosen and sent to the controller. Our method allows real time operation at rates of 30 Hz. We demonstrate the efficiency of our method in various off-road terrain at high speed.



### Deep Learning for Whole Slide Image Analysis: An Overview
- **Arxiv ID**: http://arxiv.org/abs/1910.11097v1
- **DOI**: 10.3389/fmed.2019.00264
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11097v1)
- **Published**: 2019-10-18 17:53:42+00:00
- **Updated**: 2019-10-18 17:53:42+00:00
- **Authors**: Neofytos Dimitriou, Ognjen Arandjelović, Peter D Caie
- **Comment**: None
- **Journal**: None
- **Summary**: The widespread adoption of whole slide imaging has increased the demand for effective and efficient gigapixel image analysis. Deep learning is at the forefront of computer vision, showcasing significant improvements over previous methodologies on visual understanding. However, whole slide images have billions of pixels and suffer from high morphological heterogeneity as well as from different types of artefacts. Collectively, these impede the conventional use of deep learning. For the clinical translation of deep learning solutions to become a reality, these challenges need to be addressed. In this paper, we review work on the interdisciplinary attempt of training deep neural networks using whole slide images, and highlight the different ideas underlying these methodologies.



### Generative Adversarial Networks And Domain Adaptation For Training Data Independent Image Registration
- **Arxiv ID**: http://arxiv.org/abs/1910.08593v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08593v2)
- **Published**: 2019-10-18 19:21:17+00:00
- **Updated**: 2020-03-26 20:21:25+00:00
- **Authors**: Dwarikanath Mahapatra
- **Comment**: None
- **Journal**: None
- **Summary**: Medical image registration is an important task in automated analysis of multi-modal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However,DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining.To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest Xray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation



### SurReal: Complex-Valued Learning as Principled Transformations on a Scaling and Rotation Manifold
- **Arxiv ID**: http://arxiv.org/abs/1910.11334v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11334v3)
- **Published**: 2019-10-18 20:36:29+00:00
- **Updated**: 2020-11-06 19:05:06+00:00
- **Authors**: Rudrasis Chakraborty, Yifei Xing, Stella Yu
- **Comment**: 12 pages, accepted to TNNLS journal
- **Journal**: None
- **Summary**: Complex-valued data is ubiquitous in signal and image processing applications, and complex-valued representations in deep learning have appealing theoretical properties. While these aspects have long been recognized, complex-valued deep learning continues to lag far behind its real-valued counterpart.   We propose a principled geometric approach to complex-valued deep learning. Complex-valued data could often be subject to arbitrary complex-valued scaling; as a result, real and imaginary components could co-vary. Instead of treating complex values as two independent channels of real values, we recognize their underlying geometry: We model the space of complex numbers as a product manifold of non-zero scaling and planar rotations. Arbitrary complex-valued scaling naturally becomes a group of transitive actions on this manifold.   We propose to extend the property instead of the form of real-valued functions to the complex domain. We define convolution as weighted Fr\'echet mean on the manifold that is equivariant to the group of scaling/rotation actions, and define distance transform on the manifold that is invariant to the action group. The manifold perspective also allows us to define nonlinear activation functions such as tangent ReLU and G-transport, as well as residual connections on the manifold-valued data.   We dub our model SurReal, as our experiments on MSTAR and RadioML deliver high performance with only a fractional size of real-valued and complex-valued baseline models.



### Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?
- **Arxiv ID**: http://arxiv.org/abs/1910.08640v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08640v2)
- **Published**: 2019-10-18 22:02:41+00:00
- **Updated**: 2019-10-23 16:02:06+00:00
- **Authors**: Simran Kaur, Jeremy Cohen, Zachary C. Lipton
- **Comment**: To appear in the "Science Meets Engineering of Deep Learning"
  Workshop at NeurIPS 2019
- **Journal**: None
- **Summary**: For a standard convolutional neural network, optimizing over the input pixels to maximize the score of some target class will generally produce a grainy-looking version of the original image. However, Santurkar et al. (2019) demonstrated that for adversarially-trained neural networks, this optimization produces images that uncannily resemble the target class. In this paper, we show that these "perceptually-aligned gradients" also occur under randomized smoothing, an alternative means of constructing adversarially-robust classifiers. Our finding supports the hypothesis that perceptually-aligned gradients may be a general property of robust classifiers. We hope that our results will inspire research aimed at explaining this link between perceptually-aligned gradients and adversarial robustness.



### Intracranial Hemorrhage Segmentation Using Deep Convolutional Model
- **Arxiv ID**: http://arxiv.org/abs/1910.08643v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08643v2)
- **Published**: 2019-10-18 22:08:05+00:00
- **Updated**: 2019-11-15 12:59:47+00:00
- **Authors**: Murtadha D. Hssayeni, M. S., Muayad S. Croock, Ph. D., Aymen Al-Ani, Ph. D., Hassan Falah Al-khafaji, M. D., Zakaria A. Yahya, M. D., Behnaz Ghoraani, Ph. D
- **Comment**: None
- **Journal**: None
- **Summary**: Traumatic brain injuries could cause intracranial hemorrhage (ICH). ICH could lead to disability or death if it is not accurately diagnosed and treated in a time-sensitive procedure. The current clinical protocol to diagnose ICH is examining Computerized Tomography (CT) scans by radiologists to detect ICH and localize its regions. However, this process relies heavily on the availability of an experienced radiologist. In this paper, we designed a study protocol to collect a dataset of 82 CT scans of subjects with traumatic brain injury. Later, the ICH regions were manually delineated in each slice by a consensus decision of two radiologists. Recently, fully convolutional networks (FCN) have shown to be successful in medical image segmentation. We developed a deep FCN, called U-Net, to segment the ICH regions from the CT scans in a fully automated manner. The method achieved a Dice coefficient of 0.31 for the ICH segmentation based on 5-fold cross-validation. The dataset is publicly available online at PhysioNet repository for future analysis and comparison.



### Toward Metrics for Differentiating Out-of-Distribution Sets
- **Arxiv ID**: http://arxiv.org/abs/1910.08650v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08650v3)
- **Published**: 2019-10-18 22:26:49+00:00
- **Updated**: 2020-11-19 16:15:21+00:00
- **Authors**: Mahdieh Abbasi, Changjian Shui, Arezoo Rajabi, Christian Gagne, Rakesh Bobba
- **Comment**: Workshop on Safety and Robustness in Decision Making, NeurIPS 2019
- **Journal**: ECAI 2020 : 24th European Conference on Artificial Intelligence
- **Summary**: Vanilla CNNs, as uncalibrated classifiers, suffer from classifying out-of-distribution (OOD) samples nearly as confidently as in-distribution samples. To tackle this challenge, some recent works have demonstrated the gains of leveraging available OOD sets for training end-to-end calibrated CNNs. However, a critical question remains unanswered in these works: how to differentiate OOD sets for selecting the most effective one(s) that induce training such CNNs with high detection rates on unseen OOD sets? To address this pivotal question, we provide a criterion based on generalization errors of Augmented-CNN, a vanilla CNN with an added extra class employed for rejection, on in-distribution and unseen OOD sets. However, selecting the most effective OOD set by directly optimizing this criterion incurs a huge computational cost. Instead, we propose three novel computationally-efficient metrics for differentiating between OOD sets according to their "protection" level of in-distribution sub-manifolds. We empirically verify that the most protective OOD sets -- selected according to our metrics -- lead to A-CNNs with significantly lower generalization errors than the A-CNNs trained on the least protective ones. We also empirically show the effectiveness of a protective OOD set for training well-generalized confidence-calibrated vanilla CNNs. These results confirm that 1) all OOD sets are not equally effective for training well-performing end-to-end models (i.e., A-CNNs and calibrated CNNs) for OOD detection tasks and 2) the protection level of OOD sets is a viable factor for recognizing the most effective one. Finally, across the image classification tasks, we exhibit A-CNN trained on the most protective OOD set can also detect black-box FGS adversarial examples as their distance (measured by our metrics) is becoming larger from the protected sub-manifolds.



