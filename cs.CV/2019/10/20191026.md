# Arxiv Papers in cs.CV on 2019-10-26
### Driving Datasets Literature Review
- **Arxiv ID**: http://arxiv.org/abs/1910.11968v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11968v1)
- **Published**: 2019-10-26 00:28:39+00:00
- **Updated**: 2019-10-26 00:28:39+00:00
- **Authors**: Charles-Éric Noël Laflamme, François Pomerleau, Philippe Giguère
- **Comment**: None
- **Journal**: None
- **Summary**: This report is a survey of the different autonomous driving datasets which have been published up to date. The first section introduces the many sensor types used in autonomous driving datasets. The second section investigates the calibration and synchronization procedure required to generate accurate data. The third section describes the diverse driving tasks explored by the datasets. Finally, the fourth section provides comprehensive lists of datasets, mainly in the form of tables.



### Cross-Channel Intragroup Sparsity Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11971v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11971v2)
- **Published**: 2019-10-26 01:03:01+00:00
- **Updated**: 2020-06-12 05:29:47+00:00
- **Authors**: Zhilin Yu, Chao Wang, Xin Wang, Qing Wu, Yong Zhao, Xundong Wu
- **Comment**: None
- **Journal**: None
- **Summary**: Modern deep neural networks rely on overparameterization to achieve state-of-the-art generalization. But overparameterized models are computationally expensive. Network pruning is often employed to obtain less demanding models for deployment. Fine-grained pruning removes individual weights in parameter tensors and can achieve a high model compression ratio with little accuracy degradation. However, it introduces irregularity into the computing dataflow and often does not yield improved model inference efficiency in practice. Coarse-grained model pruning, while realizing satisfactory inference speedup through removal of network weights in groups, e.g. an entire filter, often lead to significant accuracy degradation. This work introduces the cross-channel intragroup (CCI) sparsity structure, which can prevent the inference inefficiency of fine-grained pruning while maintaining outstanding model performance. We then present a novel training algorithm designed to perform well under the constraint imposed by the CCI-Sparsity. Through a series of comparative experiments we show that our proposed CCI-Sparsity structure and the corresponding pruning algorithm outperform prior art in inference efficiency by a substantial margin given suited hardware acceleration in the future.



### Novel Co-variant Feature Point Matching Based on Gaussian Mixture Model
- **Arxiv ID**: http://arxiv.org/abs/1910.11981v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11981v1)
- **Published**: 2019-10-26 02:31:02+00:00
- **Updated**: 2019-10-26 02:31:02+00:00
- **Authors**: Liang Shen, Jiahua Zhu, Chongyi Fan, Xiaotao Huang, Tian Jin
- **Comment**: arXiv admin note: text overlap with arXiv:0905.2635 by other authors
- **Journal**: None
- **Summary**: The feature frame is a key idea of feature matching problem between two images. However, most of the traditional matching methods only simply employ the spatial location information (the coordinates), which ignores the shape and orientation information of the local feature. Such additional information can be obtained along with coordinates using general co-variant detectors such as DOG, Hessian, Harris-Affine and MSER. In this paper, we develop a novel method considering all the feature center position coordinates, the local feature shape and orientation information based on Gaussian Mixture Model for co-variant feature matching. We proposed three sub-versions in our method for solving the matching problem in different conditions: rigid, affine and non-rigid, respectively, which all optimized by expectation maximization algorithm. Due to the effective utilization of the additional shape and orientation information, the proposed model can significantly improve the performance in terms of convergence speed and recall. Besides, it is more robust to the outliers.



### Diagnosis of Pediatric Obstructive Sleep Apnea via Face Classification with Persistent Homology and Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1911.05628v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML, 62P10
- **Links**: [PDF](http://arxiv.org/pdf/1911.05628v1)
- **Published**: 2019-10-26 03:43:44+00:00
- **Updated**: 2019-10-26 03:43:44+00:00
- **Authors**: Milad Kiaee, Adam B Kashlak, Jisu Kim, Giseon Heo
- **Comment**: 23 pages, 9 figures
- **Journal**: None
- **Summary**: Obstructive sleep apnea is a serious condition causing a litany of health problems especially in the pediatric population. However, this chronic condition can be treated if diagnosis is possible. The gold standard for diagnosis is an overnight sleep study, which is often unobtainable by many potentially suffering from this condition. Hence, we attempt to develop a fast non-invasive diagnostic tool by training a classifier on 2D and 3D facial images of a patient to recognize facial features associated with obstructive sleep apnea. In this comparative study, we consider both persistent homology and geometric shape analysis from the field of computational topology as well as convolutional neural networks, a powerful method from deep learning whose success in image and specifically facial recognition has already been demonstrated by computer scientists.



### Learning Disentangled Representation for Robust Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.12003v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12003v2)
- **Published**: 2019-10-26 05:52:11+00:00
- **Updated**: 2019-11-01 07:00:05+00:00
- **Authors**: Chanho Eom, Bumsub Ham
- **Comment**: None
- **Journal**: NeurIPS 2019
- **Summary**: We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose), which requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed \emph{identity shuffle GAN} (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.



### Fair Generative Modeling via Weak Supervision
- **Arxiv ID**: http://arxiv.org/abs/1910.12008v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12008v2)
- **Published**: 2019-10-26 06:40:03+00:00
- **Updated**: 2020-06-30 11:11:27+00:00
- **Authors**: Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon
- **Comment**: First two authors contributed equally
- **Journal**: None
- **Summary**: Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.



### Dense Dilated Network with Probability Regularized Walk for Vessel Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.12010v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12010v1)
- **Published**: 2019-10-26 06:44:11+00:00
- **Updated**: 2019-10-26 06:44:11+00:00
- **Authors**: Lei Mou, Li Chen, Jun Cheng, Zaiwang Gu, Yitian Zhao, Jiang Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The detection of retinal vessel is of great importance in the diagnosis and treatment of many ocular diseases. Many methods have been proposed for vessel detection. However, most of the algorithms neglect the connectivity of the vessels, which plays an important role in the diagnosis. In this paper, we propose a novel method for retinal vessel detection. The proposed method includes a dense dilated network to get an initial detection of the vessels and a probability regularized walk algorithm to address the fracture issue in the initial detection. The dense dilated network integrates newly proposed dense dilated feature extraction blocks into an encoder-decoder structure to extract and accumulate features at different scales. A multiscale Dice loss function is adopted to train the network. To improve the connectivity of the segmented vessels, we also introduce a probability regularized walk algorithm to connect the broken vessels. The proposed method has been applied on three public data sets: DRIVE, STARE and CHASE_DB1. The results show that the proposed method outperforms the state-of-the-art methods in accuracy, sensitivity, specificity and also are under receiver operating characteristic curve.



### Diverse Video Captioning Through Latent Variable Expansion
- **Arxiv ID**: http://arxiv.org/abs/1910.12019v6
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12019v6)
- **Published**: 2019-10-26 08:34:20+00:00
- **Updated**: 2021-06-15 14:50:14+00:00
- **Authors**: Huanhou Xiao, Jinglun Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Automatically describing video content with text description is challenging but important task, which has been attracting a lot of attention in computer vision community. Previous works mainly strive for the accuracy of the generated sentences, while ignoring the sentences diversity, which is inconsistent with human behavior. In this paper, we aim to caption each video with multiple descriptions and propose a novel framework. Concretely, for a given video, the intermediate latent variables of conventional encode-decode process are utilized as input to the conditional generative adversarial network (CGAN) with the purpose of generating diverse sentences. We adopt different Convolutional Neural Networks (CNNs) as our generator that produces descriptions conditioned on latent variables and discriminator that assesses the quality of generated sentences. Simultaneously, a novel DCE metric is designed to assess the diverse captions. We evaluate our method on the benchmark datasets, where it demonstrates its ability to generate diverse descriptions and achieves superior results against other state-of-the-art methods.



### Deep learning on edge: extracting field boundaries from satellite images with a convolutional neural network
- **Arxiv ID**: http://arxiv.org/abs/1910.12023v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12023v2)
- **Published**: 2019-10-26 08:46:24+00:00
- **Updated**: 2020-02-03 06:13:29+00:00
- **Authors**: François Waldner, Foivos I. Diakogiannis
- **Comment**: None
- **Journal**: None
- **Summary**: Applications of digital agricultural services often require either farmers or their advisers to provide digital records of their field boundaries. Automatic extraction of field boundaries from satellite imagery would reduce the reliance on manual input of these records which is time consuming and error-prone, and would underpin the provision of remote products and services. The lack of current field boundary data sets seems to indicate low uptake of existing methods,presumably because of expensive image preprocessing requirements and local, often arbitrary, tuning. In this paper, we address the problem of field boundary extraction from satellite images as a multitask semantic segmentation problem. We used ResUNet-a, a deep convolutional neural network with a fully connected UNet backbone that features dilated convolutions and conditioned inference, to assign three labels to each pixel: 1) the probability of belonging to a field; 2) the probability of being part of a boundary; and 3) the distance to the closest boundary. These labels can then be combined to obtain closed field boundaries. Using a single composite image from Sentinel-2, the model was highly accurate in mapping field extent, field boundaries, and, consequently, individual fields. Replacing the monthly composite with a single-date image close to the compositing period only marginally decreased accuracy. We then showed in a series of experiments that our model generalised well across resolutions, sensors, space and time without recalibration. Building consensus by averaging model predictions from at least four images acquired across the season is the key to coping with the temporal variations of accuracy. By minimising image preprocessing requirements and replacing local arbitrary decisions by data-driven ones, our approach is expected to facilitate the extraction of individual crop fields at scale.



### SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT Image Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.12024v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, eess.SP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12024v1)
- **Published**: 2019-10-26 09:04:45+00:00
- **Updated**: 2019-10-26 09:04:45+00:00
- **Authors**: Zhipeng Li, Siqi Ye, Yong Long, Saiprasad Ravishankar
- **Comment**: Accepted to International Conference on Computer Vision (ICCV) -
  Learning for Computational Imaging (LCI) Workshop, 2019
- **Journal**: None
- **Summary**: Recent years have witnessed growing interest in machine learning-based models and techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can typically be categorized into supervised learning methods and unsupervised or model-based learning methods. Supervised learning methods have recently shown success in image restoration tasks. However, they often rely on large training sets. Model-based learning methods such as dictionary or transform learning do not require large or paired training sets and often have good generalization properties, since they learn general properties of CT image sets. Recent works have shown the promising reconstruction performance of methods such as PWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches into a learned union of transforms. In this paper, we propose a new Supervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image reconstruction that combines the benefits of supervised learning methods and (unsupervised) transform learning-based methods such as PWLS-ULTRA that involve highly image-adaptive clustering. The SUPER model consists of several layers, each of which includes a deep network learned in a supervised manner and an unsupervised iterative method that involves image-adaptive components. The SUPER reconstruction algorithms are learned in a greedy manner from training data. The proposed SUPER learning methods dramatically outperform both the constituent supervised learning-based networks and iterative algorithms for LDCT, and use much fewer iterations in the iterative reconstruction modules.



### Consistency Regularization for Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.12027v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12027v2)
- **Published**: 2019-10-26 09:06:03+00:00
- **Updated**: 2020-02-18 21:43:54+00:00
- **Authors**: Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee
- **Comment**: ICLR2020
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.



### Blood Vessel Detection using Modified Multiscale MF-FDOG Filters for Diabetic Retinopathy
- **Arxiv ID**: http://arxiv.org/abs/1910.12028v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12028v1)
- **Published**: 2019-10-26 09:17:10+00:00
- **Updated**: 2019-10-26 09:17:10+00:00
- **Authors**: Debojyoti Mallick, Kundan Kumar, Sumanshu Agarwal
- **Comment**: 5 Pages, 7 Figures, ICAML2019
- **Journal**: None
- **Summary**: Blindness in diabetic patients caused by retinopathy (characterized by an increase in the diameter and new branches of the blood vessels inside the retina) is a grave concern. Many efforts have been made for the early detection of the disease using various image processing techniques on retinal images. However, most of the methods are plagued with the false detection of the blood vessel pixels. Given that, here, we propose a modified matched filter with the first derivative of Gaussian. The method uses the top-hat transform and contrast limited histogram equalization. Further, we segment the modified multiscale matched filter response by using a binary threshold obtained from the first derivative of Gaussian. The method was assessed on a publicly available database (DRIVE database). As anticipated, the proposed method provides a higher accuracy compared to the literature. Moreover, a lesser false detection from the existing matched filters and its variants have been observed.



### PoseLifter: Absolute 3D human pose lifting network from a single noisy 2D human pose
- **Arxiv ID**: http://arxiv.org/abs/1910.12029v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12029v2)
- **Published**: 2019-10-26 09:18:30+00:00
- **Updated**: 2020-03-13 14:46:07+00:00
- **Authors**: Ju Yong Chang, Gyeongsik Moon, Kyoung Mu Lee
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents a new network (i.e., PoseLifter) that can lift a 2D human pose to an absolute 3D pose in a camera coordinate system. The proposed network estimates the absolute 3D location of a target subject and generates an improved 3D relative pose estimation compared with existing pose-lifting methods. Using the PoseLifter with a 2D pose estimator in a cascade fashion can estimate a 3D human pose from a single RGB image. In this case, we empirically prove that using realistic 2D poses synthesized with the real error distribution of 2D body joints considerably improves the performance of our PoseLifter. The proposed method is applied to public datasets to achieve state-of-the-art 2D-to-3D pose lifting and 3D human pose estimation.



### HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.12032v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12032v1)
- **Published**: 2019-10-26 09:29:54+00:00
- **Updated**: 2019-10-26 09:29:54+00:00
- **Authors**: Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu
- **Comment**: 10 pages, 6 figures, to be presented at ICCV 2019
- **Journal**: None
- **Summary**: Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network (ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with "in-the-wild" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.



### Region Mutual Information Loss for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.12037v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12037v1)
- **Published**: 2019-10-26 09:39:36+00:00
- **Updated**: 2019-10-26 09:39:36+00:00
- **Authors**: Shuai Zhao, Yang Wang, Zheng Yang, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic segmentation is a fundamental problem in computer vision. It is considered as a pixel-wise classification problem in practice, and most segmentation models use a pixel-wise loss as their optimization riterion. However, the pixel-wise loss ignores the dependencies between pixels in an image. Several ways to exploit the relationship between pixels have been investigated, \eg, conditional random fields (CRF) and pixel affinity based methods. Nevertheless, these methods usually require additional model branches, large extra memories, or more inference time. In this paper, we develop a region mutual information (RMI) loss to model the dependencies among pixels more simply and efficiently. In contrast to the pixel-wise loss which treats the pixels as independent samples, RMI uses one pixel and its neighbour pixels to represent this pixel. Then for each pixel in an image, we get a multi-dimensional point that encodes the relationship between pixels, and the image is cast into a multi-dimensional distribution of these high-dimensional points. The prediction and ground truth thus can achieve high order consistency through maximizing the mutual information (MI) between their multi-dimensional distributions. Moreover, as the actual value of the MI is hard to calculate, we derive a lower bound of the MI and maximize the lower bound to maximize the real value of the MI. RMI only requires a few extra computational resources in the training stage, and there is no overhead during testing. Experimental results demonstrate that RMI can achieve substantial and consistent improvements in performance on PASCAL VOC 2012 and CamVid datasets. The code is available at https://github.com/ZJULearning/RMI.



### Learning an Efficient Network for Large-Scale Hierarchical Object Detection with Data Imbalance: 3rd Place Solution to Open Images Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.12044v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12044v1)
- **Published**: 2019-10-26 10:31:35+00:00
- **Updated**: 2019-10-26 10:31:35+00:00
- **Authors**: Xingyuan Bu, Junran Peng, Changbao Wang, Cunjun Yu, Guoliang Cao
- **Comment**: 6 pages, 3 figures, ICCV 2019 Open Images Workshop
- **Journal**: None
- **Summary**: This report details our solution to the Google AI Open Images Challenge 2019 Object Detection Track. Based on our detailed analysis on the Open Images dataset, it is found that there are four typical features: large-scale, hierarchical tag system, severe annotation incompleteness and data imbalance. Considering these characteristics, many strategies are employed, including larger backbone, distributed softmax loss, class-aware sampling, expert model, and heavier classifier. In virtue of these effective strategies, our best single model could achieve a mAP of 61.90. After ensemble, the final mAP is boosted to 67.17 in the public leaderboard and 64.21 in the private leaderboard, which earns 3rd place in the Open Images Challenge 2019.



### Deep Learning for Hyperspectral Image Classification: An Overview
- **Arxiv ID**: http://arxiv.org/abs/1910.12861v1
- **DOI**: 10.1109/TGRS.2019.2907932
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12861v1)
- **Published**: 2019-10-26 11:50:27+00:00
- **Updated**: 2019-10-26 11:50:27+00:00
- **Authors**: Shutao Li, Weiwei Song, Leyuan Fang, Yushi Chen, Pedram Ghamisi, Jón Atli Benediktsson
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no.
  9, pp. 6690-6709, Sep. 2019
- **Summary**: Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework which divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral-spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.



### A Preliminary Study on Optimal Placement of Cameras
- **Arxiv ID**: http://arxiv.org/abs/1910.12053v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12053v1)
- **Published**: 2019-10-26 12:06:44+00:00
- **Updated**: 2019-10-26 12:06:44+00:00
- **Authors**: Lin Xu
- **Comment**: 6 pages, 8 figures
- **Journal**: None
- **Summary**: This paper primarily focuses on figuring out the best array of cameras, or visual sensors, so that such a placement enables the maximum utilization of these visual sensors. Maximizing the utilization of these cameras can convert to another problem that is simpler for the formulation, that is, maximizing the total coverage with these cameras. To solve the problem, the coverage problem is first defined subject to the capabilities and limits of cameras. Then, poses of cameras are analyzed for the best arrangement.



### ETNet: Error Transition Network for Arbitrary Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1910.12056v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.12056v2)
- **Published**: 2019-10-26 12:49:00+00:00
- **Updated**: 2019-10-29 17:04:01+00:00
- **Authors**: Chunjin Song, Zhijie Wu, Yang Zhou, Minglun Gong, Hui Huang
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Numerous valuable efforts have been devoted to achieving arbitrary style transfer since the seminal work of Gatys et al. However, existing state-of-the-art approaches often generate insufficiently stylized results under challenging cases. We believe a fundamental reason is that these approaches try to generate the stylized result in a single shot and hence fail to fully satisfy the constraints on semantic structures in the content images and style patterns in the style images. Inspired by the works on error-correction, instead, we propose a self-correcting model to predict what is wrong with the current stylization and refine it accordingly in an iterative manner. For each refinement, we transit the error features across both the spatial and scale domain and invert the processed features into a residual image, with a network we call Error Transition Network (ETNet). The proposed model improves over the state-of-the-art methods with better semantic structures and more adaptive style pattern details. Various qualitative and quantitative experiments show that the key concept of both progressive strategy and error-correction leads to better results. Code and models are available at https://github.com/zhijieW94/ETNet.



### MAP-Net: Multi Attending Path Neural Network for Building Footprint Extraction from Remote Sensed Imagery
- **Arxiv ID**: http://arxiv.org/abs/1910.12060v2
- **DOI**: 10.1109/TGRS.2020.3026051
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12060v2)
- **Published**: 2019-10-26 13:09:17+00:00
- **Updated**: 2020-09-30 07:05:59+00:00
- **Authors**: Qing Zhu, Cheng Liao, Han Hu, Xiaoming Mei, Haifeng Li
- **Comment**: 13 pages, 10 figures
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing 2020 IEEE
  Transactions on Geoscience and Remote Sensing 2020 IEEE Transactions on
  Geoscience and Remote Sensing 2020
- **Summary**: Accurately and efficiently extracting building footprints from a wide range of remote sensed imagery remains a challenge due to their complex structure, variety of scales and diverse appearances. Existing convolutional neural network (CNN)-based building extraction methods are complained that they cannot detect the tiny buildings because the spatial information of CNN feature maps are lost during repeated pooling operations of the CNN, and the large buildings still have inaccurate segmentation edges. Moreover, features extracted by a CNN are always partial which restricted by the size of the respective field, and large-scale buildings with low texture are always discontinuous and holey when extracted. This paper proposes a novel multi attending path neural network (MAP-Net) for accurately extracting multiscale building footprints and precise boundaries. MAP-Net learns spatial localization-preserved multiscale features through a multi-parallel path in which each stage is gradually generated to extract high-level semantic features with fixed resolution. Then, an attention module adaptively squeezes channel-wise features from each path for optimization, and a pyramid spatial pooling module captures global dependency for refining discontinuous building footprints. Experimental results show that MAP-Net outperforms state-of-the-art (SOTA) algorithms in boundary localization accuracy as well as continuity of large buildings. Specifically, our method achieved 0.68\%, 1.74\%, 1.46\% precision, and 1.50\%, 1.53\%, 0.82\% IoU score improvement without increasing computational complexity compared with the latest HRNetv2 on the Urban 3D, Deep Globe and WHU datasets, respectively. The TensorFlow implementation is available at https://github.com/lehaifeng/MAPNet.



### A Soft STAPLE Algorithm Combined with Anatomical Knowledge
- **Arxiv ID**: http://arxiv.org/abs/1910.12077v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12077v1)
- **Published**: 2019-10-26 14:38:27+00:00
- **Updated**: 2019-10-26 14:38:27+00:00
- **Authors**: Eytan Kats, Jacob Goldberger, Hayit Greenspan
- **Comment**: International Conference on Medical Image Computing and Computer
  Assisted Intervention (MICCAI) 2019
- **Journal**: None
- **Summary**: Supervised machine learning algorithms, especially in the medical domain, are affected by considerable ambiguity in expert markings. In this study we address the case where the experts' opinion is obtained as a distribution over the possible values. We propose a soft version of the STAPLE algorithm for experts' markings fusion that can handle soft values. The algorithm was applied to obtain consensus from soft Multiple Sclerosis (MS) segmentation masks. Soft MS segmentations are constructed from manual binary delineations by including lesion surrounding voxels in the segmentation mask with a reduced confidence weight. We suggest that these voxels contain additional anatomical information about the lesion structure. The fused masks are utilized as ground truth mask to train a Fully Convolutional Neural Network (FCNN). The proposed method was evaluated on the MICCAI 2016 challenge dataset, and yields improved precision-recall tradeoff and a higher average Dice similarity coefficient.



### Wavelets to the Rescue: Improving Sample Quality of Latent Variable Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1911.05627v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05627v1)
- **Published**: 2019-10-26 15:16:05+00:00
- **Updated**: 2019-10-26 15:16:05+00:00
- **Authors**: Prashnna K Gyawali, Rudra Saha, Linwei Wang, VSR Veeravasarapu, Maneesh Singh
- **Comment**: None
- **Journal**: None
- **Summary**: Variational Autoencoders (VAE) are probabilistic deep generative models underpinned by elegant theory, stable training processes, and meaningful manifold representations. However, they produce blurry images due to a lack of explicit emphasis over high-frequency textural details of the images, and the difficulty to directly model the complex joint probability distribution over the high-dimensional image space. In this work, we approach these two challenges with a novel wavelet space VAE that uses the decoder to model the images in the wavelet coefficient space. This enables the VAE to emphasize over high-frequency components within an image obtained via wavelet decomposition. Additionally, by decomposing the complex function of generating high-dimensional images into inverse wavelet transformation and generation of wavelet coefficients, the latter becomes simpler to model by the VAE. We empirically validate that deep generative models operating in the wavelet space can generate images of higher quality than the image (RGB) space counterparts. Quantitatively, on benchmark natural image datasets, we achieve consistently better FID scores than VAE based architectures and competitive FID scores with a variety of GAN models for the same architectural and experimental setup. Furthermore, the proposed wavelet-based generative model retains desirable attributes like disentangled and informative latent representation without losing the quality in the generated samples.



### FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.12100v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12100v1)
- **Published**: 2019-10-26 17:00:11+00:00
- **Updated**: 2019-10-26 17:00:11+00:00
- **Authors**: Keqiang Sun, Wayne Wu, Tinghao Liu, Shuo Yang, Quan Wang, Qiang Zhou, Zuochang Ye, Chen Qian
- **Comment**: Accepted to ICCV 2019. Project page:
  https://keqiangsun.github.io/projects/FAB/FAB.html
- **Journal**: None
- **Summary**: Recently, facial landmark detection algorithms have achieved remarkable performance on static images. However, these algorithms are neither accurate nor stable in motion-blurred videos. The missing of structure information makes it difficult for state-of-the-art facial landmark detection algorithms to yield good results. In this paper, we propose a framework named FAB that takes advantage of structure consistency in the temporal dimension for facial landmark detection in motion-blurred videos. A structure predictor is proposed to predict the missing face structural information temporally, which serves as a geometry prior. This allows our framework to work as a virtuous circle. On one hand, the geometry prior helps our structure-aware deblurring network generates high quality deblurred images which lead to better landmark detection results. On the other hand, better landmark detection results help structure predictor generate better geometry prior for the next frame. Moreover, it is a flexible video-based framework that can incorporate any static image-based methods to provide a performance boost on video datasets. Extensive experiments on Blurred-300VW, the proposed Real-world Motion Blur (RWMB) datasets and 300VW demonstrate the superior performance to the state-of-the-art methods. Datasets and models will be publicly available at https://keqiangsun.github.io/projects/FAB/FAB.html.



### Estimation of Pelvic Sagittal Inclination from Anteroposterior Radiograph Using Convolutional Neural Networks: Proof-of-Concept Study
- **Arxiv ID**: http://arxiv.org/abs/1910.12122v1
- **DOI**: 10.29007/w6t7
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12122v1)
- **Published**: 2019-10-26 19:27:18+00:00
- **Updated**: 2019-10-26 19:27:18+00:00
- **Authors**: Ata Jodeiri, Yoshito Otake, Reza A. Zoroofi, Yuta Hiasa, Masaki Takao, Keisuke Uemura, Nobuhiko Sugano, Yoshinobu Sato
- **Comment**: Best Technical Paper Award Winner of CAOS 2018
  (https://www.caos-international.org/award-paper.php)
- **Journal**: None
- **Summary**: Alignment of the bones in standing position provides useful information in surgical planning. In total hip arthroplasty (THA), pelvic sagittal inclination (PSI) angle in the standing position is an important factor in planning of cup alignment and has been estimated mainly from radiographs. Previous methods for PSI estimation used a patient-specific CT to create digitally reconstructed radiographs (DRRs) and compare them with the radiograph to estimate relative position between the pelvis and the x-ray detector. In this study, we developed a method that estimates PSI angle from a single anteroposterior radiograph using two convolutional neural networks (CNNs) without requiring the patient-specific CT, which reduces radiation exposure of the patient and opens up the possibility of application in a larger number of hospitals where CT is not acquired in a routine protocol.



### Exploiting GAN Internal Capacity for High-Quality Reconstruction of Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1911.05630v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.05630v1)
- **Published**: 2019-10-26 22:07:24+00:00
- **Updated**: 2019-10-26 22:07:24+00:00
- **Authors**: Marcos Pividori, Guillermo L. Grinblat, Lucas C. Uzal
- **Comment**: This preprint is the result of the work done for the undergraduate
  dissertation of M. Pividori supervised by L.C. Uzal and G.L. Grinblat, and
  presented in July 2019
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have demonstrated impressive results in modeling the distribution of natural images, learning latent representations that capture semantic variations in an unsupervised basis. Beyond the generation of novel samples, it is of special interest to exploit the ability of the GAN generator to model the natural image manifold and hence generate credible changes when manipulating images. However, this line of work is conditioned by the quality of the reconstruction. Until now, only inversion to the latent space has been considered, we propose to exploit the representation in intermediate layers of the generator, and we show that this leads to increased capacity. In particular, we observe that the representation after the first dense layer, present in all state-of-the-art GAN models, is expressive enough to represent natural images with high visual fidelity. It is possible to interpolate around these images obtaining a sequence of new plausible synthetic images that cannot be generated from the latent space. Finally, as an example of potential applications that arise from this inversion mechanism, we show preliminary results in exploiting the learned representation in the attention map of the generator to obtain an unsupervised segmentation of natural images.



