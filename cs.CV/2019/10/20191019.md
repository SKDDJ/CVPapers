# Arxiv Papers in cs.CV on 2019-10-19
### Sensor fusion using EMG and vision for hand gesture classification in mobile applications
- **Arxiv ID**: http://arxiv.org/abs/1910.11126v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1910.11126v1)
- **Published**: 2019-10-19 00:02:48+00:00
- **Updated**: 2019-10-19 00:02:48+00:00
- **Authors**: Enea Ceolini, Gemma Taverni, Lyes Khacef, Melika Payvand, Elisa Donati
- **Comment**: None
- **Journal**: None
- **Summary**: The discrimination of human gestures using wearable solutions is extremely important as a supporting technique for assisted living, healthcare of the elderly and neurorehabilitation. This paper presents a mobile electromyography (EMG) analysis framework to be an auxiliary component in physiotherapy sessions or as a feedback for neuroprosthesis calibration. We implemented a framework that allows the integration of multisensors, EMG and visual information, to perform sensor fusion and to improve the accuracy of hand gesture recognition tasks. In particular, we used an event-based camera adapted to run on the limited computational resources of mobile phones. We introduced a new publicly available dataset of sensor fusion for hand gesture recognition recorded from 10 subjects and used it to train the recognition models offline. We compare the online results of the hand gesture recognition using the fusion approach with the individual sensors with an improvement in the accuracy of 13% and 11%, for EMG and vision respectively, reaching 85%.



### NASIB: Neural Architecture Search withIn Budget
- **Arxiv ID**: http://arxiv.org/abs/1910.08665v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08665v1)
- **Published**: 2019-10-19 00:12:39+00:00
- **Updated**: 2019-10-19 00:12:39+00:00
- **Authors**: Abhishek Singh, Anubhav Garg, Jinan Zhou, Shiv Ram Dubey, Debo Dutta
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Architecture Search (NAS) represents a class of methods to generate the optimal neural network architecture and typically iterate over candidate architectures till convergence over some particular metric like validation loss. They are constrained by the available computation resources, especially in enterprise environments. In this paper, we propose a new approach for NAS, called NASIB, which adapts and attunes to the computation resources (budget) available by varying the exploration vs. exploitation trade-off. We reduce the expert bias by searching over an augmented search space induced by Superkernels. The proposed method can provide the architecture search useful for different computation resources and different domains beyond image classification of natural images where we lack bespoke architecture motifs and domain expertise. We show, on CIFAR10, that itis possible to search over a space that comprises of 12x more candidate operations than the traditional prior art in just 1.5 GPU days, while reaching close to state of the art accuracy. While our method searches over an exponentially larger search space, it could lead to novel architectures that require lesser domain expertise, compared to the majority of the existing methods.



### SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.08681v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08681v4)
- **Published**: 2019-10-19 02:35:38+00:00
- **Updated**: 2020-07-22 05:25:14+00:00
- **Authors**: Qing Guo, Xiaofei Xie, Felix Juefei-Xu, Lei Ma, Zhongguo Li, Wanli Xue, Wei Feng, Yang Liu
- **Comment**: 18 pages, 5 figures. This paper has been accepted to ECCV2020
- **Journal**: None
- **Summary**: Adversarial attacks of deep neural networks have been intensively studied on image, audio, natural language, patch, and pixel classification tasks. Nevertheless, as a typical, while important real-world application, the adversarial attacks of online video object tracking that traces an object's moving trajectory instead of its category are rarely explored. In this paper, we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). To this end, we first propose a \textit{spatial-aware} basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C&W, and comprehensively analyze the attacking performance. We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) real-time trackers require the attack to satisfy a certain level of efficiency. To address these challenges, we further propose the spatial-aware online incremental attack (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible. In addition, as an optimization-based method, SPARK quickly converges to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks. The in-depth evaluation on state-of-the-art trackers (i.e., SiamRPN++ with AlexNet, MobileNetv2, and ResNet-50, and SiamDW) on OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and transferability of SPARK in misleading the trackers under both UA and TA with minor perturbations.



### Real-Time Lip Sync for Live 2D Animation
- **Arxiv ID**: http://arxiv.org/abs/1910.08685v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.HC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08685v1)
- **Published**: 2019-10-19 03:12:26+00:00
- **Updated**: 2019-10-19 03:12:26+00:00
- **Authors**: Deepali Aneja, Wilmot Li
- **Comment**: None
- **Journal**: None
- **Summary**: The emergence of commercial tools for real-time performance-based 2D animation has enabled 2D characters to appear on live broadcasts and streaming platforms. A key requirement for live animation is fast and accurate lip sync that allows characters to respond naturally to other actors or the audience through the voice of a human performer. In this work, we present a deep learning based interactive system that automatically generates live lip sync for layered 2D characters using a Long Short Term Memory (LSTM) model. Our system takes streaming audio as input and produces viseme sequences with less than 200ms of latency (including processing time). Our contributions include specific design decisions for our feature definition and LSTM configuration that provide a small but useful amount of lookahead to produce accurate lip sync. We also describe a data augmentation procedure that allows us to achieve good results with a very small amount of hand-animated training data (13-20 minutes). Extensive human judgement experiments show that our results are preferred over several competing methods, including those that only support offline (non-live) processing. Video summary and supplementary results at GitHub link: https://github.com/deepalianeja/CharacterLipSync2D



### Fast Portrait Segmentation with Highly Light-weight Network
- **Arxiv ID**: http://arxiv.org/abs/1910.08695v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08695v4)
- **Published**: 2019-10-19 03:48:36+00:00
- **Updated**: 2020-05-30 12:09:38+00:00
- **Authors**: Yuezun Li, Ao Luo, Siwei Lyu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we describe a fast and light-weight portrait segmentation method based on a new highly light-weight backbone (HLB) architecture. The core element of HLB is a bottleneck-based factorized block (BFB) that has much fewer parameters than existing alternatives while keeping good learning capacity. Consequently, the HLB-based portrait segmentation method can run faster than the existing methods yet retaining the competitive accuracy performance with state-of-the-arts. Experiments conducted on two benchmark datasets demonstrate the effectiveness and efficiency of our method.



### Attention Guided Metal Artifact Correction in MRI using Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.08705v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08705v1)
- **Published**: 2019-10-19 04:51:35+00:00
- **Updated**: 2019-10-19 04:51:35+00:00
- **Authors**: Jee Won Kim, Kinam Kwon, Byungjai Kim, HyunWook Park
- **Comment**: 6 pages, 5 figures
- **Journal**: ICCV 2019 Workshop on Interpreting and Explaining Visual
  Artificial Intelligence Models
- **Summary**: An attention guided scheme for metal artifact correction in MRI using deep neural network is proposed in this paper. The inputs of the networks are two distorted images obtained with dual-polarity readout gradients. With MR image generation module and the additional data consistency loss to the previous work [1], the network is trained to estimate the frequency-shift map, off-resonance map, and attention map. The attention map helps to produce better distortion-corrected images by weighting on more relevant distortion-corrected images where two distortion-corrected images are produced with half of the frequency-shift maps. In this paper, we observed that in a real MRI environment, two distorted images obtained with opposite polarities of readout gradient showed artifacts in a different region. Therefore, we proved that using the attention map was important in that it reduced the residual ripple and pile-up artifacts near metallic implants.



### Correlation Maximized Structural Similarity Loss for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.08711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08711v1)
- **Published**: 2019-10-19 06:33:11+00:00
- **Updated**: 2019-10-19 06:33:11+00:00
- **Authors**: Shuai Zhao, Boxi Wu, Wenqing Chu, Yao Hu, Deng Cai
- **Comment**: None
- **Journal**: None
- **Summary**: Most semantic segmentation models treat semantic segmentation as a pixel-wise classification task and use a pixel-wise classification error as their optimization criterions. However, the pixel-wise error ignores the strong dependencies among the pixels in an image, which limits the performance of the model. Several ways to incorporate the structure information of the objects have been investigated, \eg, conditional random fields (CRF), image structure priors based methods, and generative adversarial network (GAN). Nevertheless, these methods usually require extra model branches or additional memories, and some of them show limited improvements. In contrast, we propose a simple yet effective structural similarity loss (SSL) to encode the structure information of the objects, which only requires a few additional computational resources in the training phase. Inspired by the widely-used structural similarity (SSIM) index in image quality assessment, we use the linear correlation between two images to quantify their structural similarity. And the goal of the proposed SSL is to pay more attention to the positions, whose associated predictions lead to a low degree of linear correlation between two corresponding regions in the ground truth map and the predicted map. Thus the model can achieve a strong structural similarity between the two maps through minimizing the SSL over the whole map. The experimental results demonstrate that our method can achieve substantial and consistent improvements in performance on the PASCAL VOC 2012 and Cityscapes datasets. The code will be released soon.



### MixModule: Mixed CNN Kernel Module for Medical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.08728v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08728v2)
- **Published**: 2019-10-19 09:06:21+00:00
- **Updated**: 2020-02-26 03:22:59+00:00
- **Authors**: Henry H. Yu, Xue Feng, Hao Sun, Ziwen Wang
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have been successfully applied to medical image classification, segmentation, and related tasks. Among the many CNNs architectures, U-Net and its improved versions based are widely used and achieve state-of-the-art performance these years. These improved architectures focus on structural improvements and the size of the convolution kernel is generally fixed. In this paper, we propose a module that combines the benefits of multiple kernel sizes and we apply the proposed module to U-Net and its variants. We test our module on three segmentation benchmark datasets and experimental results show significant improvement.



### Coordinated Joint Multimodal Embeddings for Generalized Audio-Visual Zeroshot Classification and Retrieval of Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.08732v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1910.08732v1)
- **Published**: 2019-10-19 09:39:28+00:00
- **Updated**: 2019-10-19 09:39:28+00:00
- **Authors**: Kranti Kumar Parida, Neeraj Matiyali, Tanaya Guha, Gaurav Sharma
- **Comment**: To appear in WACV 2020, Project Page:
  https://cse.iitk.ac.in/users/kranti/avzsl.html
- **Journal**: None
- **Summary**: We present an audio-visual multimodal approach for the task of zeroshot learning (ZSL) for classification and retrieval of videos. ZSL has been studied extensively in the recent past but has primarily been limited to visual modality and to images. We demonstrate that both audio and visual modalities are important for ZSL for videos. Since a dataset to study the task is currently not available, we also construct an appropriate multimodal dataset with 33 classes containing 156,416 videos, from an existing large scale audio event dataset. We empirically show that the performance improves by adding audio modality for both tasks of zeroshot classification and retrieval, when using multimodal extensions of embedding learning methods. We also propose a novel method to predict the `dominant' modality using a jointly learned modality attention network. We learn the attention in a semi-supervised setting and thus do not require any additional explicit labelling for the modalities. We provide qualitative validation of the modality specific attention, which also successfully generalizes to unseen test classes.



### Tracking-Assisted Segmentation of Biological Cells
- **Arxiv ID**: http://arxiv.org/abs/1910.08735v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.08735v1)
- **Published**: 2019-10-19 09:49:01+00:00
- **Updated**: 2019-10-19 09:49:01+00:00
- **Authors**: Deepak K. Gupta, Nathan de Bruijn, Andreas Panteli, Efstratios Gavves
- **Comment**: Accepted in NeurIPS2019, Medical Imaging meets NeurIPS workshop
- **Journal**: None
- **Summary**: U-Net and its variants have been demonstrated to work sufficiently well in biological cell tracking and segmentation. However, these methods still suffer in the presence of complex processes such as collision of cells, mitosis and apoptosis. In this paper, we augment U-Net with Siamese matching-based tracking and propose to track individual nuclei over time. By modelling the behavioural pattern of the cells, we achieve improved segmentation and tracking performances through a re-segmentation procedure. Our preliminary investigations on the Fluo-N2DH-SIM+ and Fluo-N2DH-GOWT1 datasets demonstrate that absolute improvements of up to 3.8 % and 3.4% can be obtained in segmentation and tracking accuracy, respectively.



### Utilising Low Complexity CNNs to Lift Non-Local Redundancies in Video Coding
- **Arxiv ID**: http://arxiv.org/abs/1910.08737v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08737v2)
- **Published**: 2019-10-19 10:09:39+00:00
- **Updated**: 2020-04-28 05:53:52+00:00
- **Authors**: Jan P. Klopp, Liang-Gee Chen, Shao-Yi Chien
- **Comment**: 13 pages, 3 figures
- **Journal**: None
- **Summary**: Digital media is ubiquitous and produced in ever-growing quantities. This necessitates a constant evolution of compression techniques, especially for video, in order to maintain efficient storage and transmission. In this work, we aim at exploiting non-local redundancies in video data that remain difficult to erase for conventional video codecs. We design convolutional neural networks with a particular emphasis on low memory and computational footprint. The parameters of those networks are trained on the fly, at encoding time, to predict the residual signal from the decoded video signal. After the training process has converged, the parameters are compressed and signalled as part of the code of the underlying video codec. The method can be applied to any existing video codec to increase coding gains while its low computational footprint allows for an application under resource-constrained conditions. Building on top of High Efficiency Video Coding, we achieve coding gains similar to those of pretrained denoising CNNs while only requiring about 1% of their computational complexity. Through extensive experiments, we provide insights into the effectiveness of our network design decisions. In addition, we demonstrate that our algorithm delivers stable performance under conditions met in practical video compression: our algorithm performs without significant performance loss on very long random access segments (up to 256 frames) and with moderate performance drops can even be applied to single frames in high-resolution low delay settings.



### Component Attention Guided Face Super-Resolution Network: CAGFace
- **Arxiv ID**: http://arxiv.org/abs/1910.08761v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08761v1)
- **Published**: 2019-10-19 12:28:14+00:00
- **Updated**: 2019-10-19 12:28:14+00:00
- **Authors**: Ratheesh Kalarot, Tao Li, Fatih Porikli
- **Comment**: Submitted to WACV 2020
- **Journal**: None
- **Summary**: To make the best use of the underlying structure of faces, the collective information through face datasets and the intermediate estimates during the upsampling process, here we introduce a fully convolutional multi-stage neural network for 4$\times$ super-resolution for face images. We implicitly impose facial component-wise attention maps using a segmentation network to allow our network to focus on face-inherent patterns. Each stage of our network is composed of a stem layer, a residual backbone, and spatial upsampling layers. We recurrently apply stages to reconstruct an intermediate image, and then reuse its space-to-depth converted versions to bootstrap and enhance image quality progressively. Our experiments show that our face super-resolution method achieves quantitatively superior and perceptually pleasing results in comparison to state of the art.



### Automated Composition of Picture-Synched Music Soundtracks for Movies
- **Arxiv ID**: http://arxiv.org/abs/1910.08773v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08773v1)
- **Published**: 2019-10-19 13:51:57+00:00
- **Updated**: 2019-10-19 13:51:57+00:00
- **Authors**: Vansh Dassani, Jon Bird, Dave Cliff
- **Comment**: To be presented at the 16th ACM SIGGRAPH European Conference on
  Visual Media Production. London, England: 17th-18th December 2019. 10 pages,
  9 figures
- **Journal**: None
- **Summary**: We describe the implementation of and early results from a system that automatically composes picture-synched musical soundtracks for videos and movies. We use the phrase "picture-synched" to mean that the structure of the automatically composed music is determined by visual events in the input movie, i.e. the final music is synchronised to visual events and features such as cut transitions or within-shot key-frame events. Our system combines automated video analysis and computer-generated music-composition techniques to create unique soundtracks in response to the video input, and can be thought of as an initial step in creating a computerised replacement for a human composer writing music to fit the picture-locked edit of a movie. Working only from the video information in the movie, key features are extracted from the input video, using video analysis techniques, which are then fed into a machine-learning-based music generation tool, to compose a piece of music from scratch. The resulting soundtrack is tied to video features, such as scene transition markers and scene-level energy values, and is unique to the input video. Although the system we describe here is only a preliminary proof-of-concept, user evaluations of the output of the system have been positive.



### Facial Emotion Recognition Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11113v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11113v1)
- **Published**: 2019-10-19 14:54:11+00:00
- **Updated**: 2019-10-19 14:54:11+00:00
- **Authors**: Ching-Da Wu, Li-Heng Chen
- **Comment**: 5 pages, 7 figures, 4 tables
- **Journal**: None
- **Summary**: We aim to construct a system that captures real-world facial images through the front camera on a laptop. The system is capable of processing/recognizing the captured image and predict a result in real-time. In this system, we exploit the power of deep learning technique to learn a facial emotion recognition (FER) model based on a set of labeled facial images. Finally, experiments are conducted to evaluate our model using largely used public database.



### SpatialFlow: Bridging All Tasks for Panoptic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.08787v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08787v3)
- **Published**: 2019-10-19 15:16:27+00:00
- **Updated**: 2020-08-27 09:21:09+00:00
- **Authors**: Qiang Chen, Anda Cheng, Xiangyu He, Peisong Wang, Jian Cheng
- **Comment**: Accepted to IEEE TCSVT
- **Journal**: None
- **Summary**: Object location is fundamental to panoptic segmentation as it is related to all things and stuff in the image scene. Knowing the locations of objects in the image provides clues for segmenting and helps the network better understand the scene. How to integrate object location in both thing and stuff segmentation is a crucial problem. In this paper, we propose spatial information flows to achieve this objective. The flows can bridge all sub-tasks in panoptic segmentation by delivering the object's spatial context from the box regression task to others. More importantly, we design four parallel sub-networks to get a preferable adaptation of object spatial information in sub-tasks. Upon the sub-networks and the flows, we present a location-aware and unified framework for panoptic segmentation, denoted as SpatialFlow. We perform a detailed ablation study on each component and conduct extensive experiments to prove the effectiveness of SpatialFlow. Furthermore, we achieve state-of-the-art results, which are $47.9$ PQ and $62.5$ PQ respectively on MS-COCO and Cityscapes panoptic benchmarks. Code will be available at https://github.com/chensnathan/SpatialFlow.



### LEt-SNE: A Hybrid Approach To Data Embedding and Visualization of Hyperspectral Imagery
- **Arxiv ID**: http://arxiv.org/abs/1910.08790v2
- **DOI**: 10.1109/ICASSP40776.2020.9053924
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08790v2)
- **Published**: 2019-10-19 15:45:15+00:00
- **Updated**: 2020-02-08 17:02:12+00:00
- **Authors**: Megh Shukla, Biplab Banerjee, Krishna Mohan Buddhiraju
- **Comment**: Accepted, ICASSP 2020
- **Journal**: None
- **Summary**: Hyperspectral Imagery (and Remote Sensing in general) captured from UAVs or satellites are highly voluminous in nature due to the large spatial extent and wavelengths captured by them. Since analyzing these images requires a huge amount of computational time and power, various dimensionality reduction techniques have been used for feature reduction. Some popular techniques among these falter when applied to Hyperspectral Imagery due to the famed curse of dimensionality. In this paper, we propose a novel approach, LEt-SNE, which combines graph based algorithms like t-SNE and Laplacian Eigenmaps into a model parameterized by a shallow feed forward network. We introduce a new term, Compression Factor, that enables our method to combat the curse of dimensionality. The proposed algorithm is suitable for manifold visualization and sample clustering with labelled or unlabelled data. We demonstrate that our method is competitive with current state-of-the-art methods on hyperspectral remote sensing datasets in public domain.



### Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.08811v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.08811v1)
- **Published**: 2019-10-19 17:56:43+00:00
- **Updated**: 2019-10-19 17:56:43+00:00
- **Authors**: Juil Sock, Guillermo Garcia-Hernando, Tae-Kyun Kim
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we explore how a strategic selection of camera movements can facilitate the task of 6D multi-object pose estimation in cluttered scenarios while respecting real-world constraints important in robotics and augmented reality applications, such as time and distance traveled. In the proposed framework, a set of multiple object hypotheses is given to an agent, which is inferred by an object pose estimator and subsequently spatio-temporally selected by a fusion function that makes use of a verification score that circumvents the need of ground-truth annotations. The agent reasons about these hypotheses, directing its attention to the object which it is most uncertain about, moving the camera towards such an object. Unlike previous works that propose short-sighted policies, our agent is trained in simulated scenarios using reinforcement learning, attempting to learn the camera moves that produce the most accurate object poses hypotheses for a given temporal and spatial budget, without the need of viewpoints rendering during inference. Our experiments show that the proposed approach successfully estimates the 6D object pose of a stack of objects in both challenging cluttered synthetic and real scenarios, showing superior performance compared to strong baselines.



### Deep Parametric Indoor Lighting Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.08812v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08812v1)
- **Published**: 2019-10-19 17:57:01+00:00
- **Updated**: 2019-10-19 17:57:01+00:00
- **Authors**: Marc-André Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian Gagné, Jean-François Lalonde
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method to estimate lighting from a single image of an indoor scene. Previous work has used an environment map representation that does not account for the localized nature of indoor lighting. Instead, we represent lighting as a set of discrete 3D lights with geometric and photometric parameters. We train a deep neural network to regress these parameters from a single image, on a dataset of environment maps annotated with depth. We propose a differentiable layer to convert these parameters to an environment map to compute our loss; this bypasses the challenge of establishing correspondences between estimated and ground truth lights. We demonstrate, via quantitative and qualitative evaluations, that our representation and training scheme lead to more accurate results compared to previous work, while allowing for more realistic 3D object compositing with spatially-varying lighting.



### NormGrad: Finding the Pixels that Matter for Training
- **Arxiv ID**: http://arxiv.org/abs/1910.08823v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08823v1)
- **Published**: 2019-10-19 19:16:20+00:00
- **Updated**: 2019-10-19 19:16:20+00:00
- **Authors**: Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, Hakan Bilen, Andrea Vedaldi
- **Comment**: None
- **Journal**: None
- **Summary**: The different families of saliency methods, either based on contrastive signals, closed-form formulas mixing gradients with activations or on perturbation masks, all focus on which parts of an image are responsible for the model's inference. In this paper, we are rather interested by the locations of an image that contribute to the model's training. First, we propose a principled attribution method that we extract from the summation formula used to compute the gradient of the weights for a 1x1 convolutional layer. The resulting formula is fast to compute and can used throughout the network, allowing us to efficiently produce fined-grained importance maps. We will show how to extend it in order to compute saliency maps at any targeted point within the network. Secondly, to make the attribution really specific to the training of the model, we introduce a meta-learning approach for saliency methods by considering an inner optimisation step within the loss. This way, we do not aim at identifying the parts of an image that contribute to the model's output but rather the locations that are responsible for the good training of the model on this image. Conversely, we also show that a similar meta-learning approach can be used to extract the adversarial locations which can lead to the degradation of the model.



### ProxIQA: A Proxy Approach to Perceptual Optimization of Learned Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1910.08845v2
- **DOI**: 10.1109/TIP.2020.3036752
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08845v2)
- **Published**: 2019-10-19 21:07:33+00:00
- **Updated**: 2020-10-29 15:48:02+00:00
- **Authors**: Li-Heng Chen, Christos G. Bampis, Zhi Li, Andrey Norkin, Alan C. Bovik
- **Comment**: 12 pages, 12 figures, 5 tables
- **Journal**: None
- **Summary**: The use of $\ell_p$ $(p=1,2)$ norms has largely dominated the measurement of loss in neural networks due to their simplicity and analytical properties. However, when used to assess the loss of visual information, these simple norms are not very consistent with human perception. Here, we describe a different "proximal" approach to optimize image analysis networks against quantitative perceptual models. Specifically, we construct a proxy network, broadly termed ProxIQA, which mimics the perceptual model while serving as a loss layer of the network. We experimentally demonstrate how this optimization framework can be applied to train an end-to-end optimized image compression network. By building on top of an existing deep image compression model, we are able to demonstrate a bitrate reduction of as much as $31\%$ over MSE optimization, given a specified perceptual quality (VMAF) level.



### Image Restoration Using Deep Regulated Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.08853v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.08853v1)
- **Published**: 2019-10-19 22:30:23+00:00
- **Updated**: 2019-10-19 22:30:23+00:00
- **Authors**: Peng Liu, Xiaoxiao Zhou, Junyi Yang, El Basha Mohammad D, Ruogu Fang
- **Comment**: None
- **Journal**: None
- **Summary**: While the depth of convolutional neural networks has attracted substantial attention in the deep learning research, the width of these networks has recently received greater interest. The width of networks, defined as the size of the receptive fields and the density of the channels, has demonstrated crucial importance in low-level vision tasks such as image denoising and restoration. However, the limited generalization ability, due to the increased width of networks, creates a bottleneck in designing wider networks. In this paper, we propose the Deep Regulated Convolutional Network (RC-Net), a deep network composed of regulated sub-network blocks cascaded by skip-connections, to overcome this bottleneck. Specifically, the Regulated Convolution block (RC-block), featured by a combination of large and small convolution filters, balances the effectiveness of prominent feature extraction and the generalization ability of the network. RC-Nets have several compelling advantages: they embrace diversified features through large-small filter combinations, alleviate the hazy boundary and blurred details in image denoising and super-resolution problems, and stabilize the learning process. Our proposed RC-Nets outperform state-of-the-art approaches with significant performance gains in various image restoration tasks while demonstrating promising generalization ability. The code is available at https://github.com/cswin/RC-Nets.



### The Deepfake Detection Challenge (DFDC) Preview Dataset
- **Arxiv ID**: http://arxiv.org/abs/1910.08854v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1910.08854v2)
- **Published**: 2019-10-19 22:35:52+00:00
- **Updated**: 2019-10-23 18:47:35+00:00
- **Authors**: Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, Cristian Canton Ferrer
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we introduce a preview of the Deepfakes Detection Challenge (DFDC) dataset consisting of 5K videos featuring two facial modification algorithms. A data collection campaign has been carried out where participating actors have entered into an agreement to the use and manipulation of their likenesses in our creation of the dataset. Diversity in several axes (gender, skin-tone, age, etc.) has been considered and actors recorded videos with arbitrary backgrounds thus bringing visual variability. Finally, a set of specific metrics to evaluate the performance have been defined and two existing models for detecting deepfakes have been tested to provide a reference performance baseline. The DFDC dataset preview can be downloaded at: deepfakedetectionchallenge.ai



