# Arxiv Papers in cs.CV on 2019-10-08
### ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.03151v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03151v4)
- **Published**: 2019-10-08 01:14:26+00:00
- **Updated**: 2020-04-07 13:53:51+00:00
- **Authors**: Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, Qinghua Hu
- **Comment**: Accepted to CVPR 2020; Project Page:
  https://github.com/BangguWu/ECANet
- **Journal**: None
- **Summary**: Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via $1D$ convolution. Furthermore, we develop a method to adaptively select kernel size of $1D$ convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.



### GetNet: Get Target Area for Image Pairing
- **Arxiv ID**: http://arxiv.org/abs/1910.03152v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03152v1)
- **Published**: 2019-10-08 01:18:33+00:00
- **Updated**: 2019-10-08 01:18:33+00:00
- **Authors**: Henry H. Yu, Jiang Liu, Hao Sun, Ziwen Wang, Haotian Zhang
- **Comment**: Accepted by Image and Vision Computing New Zealand (IVCNZ 2019)
- **Journal**: None
- **Summary**: Image pairing is an important research task in the field of computer vision. And finding image pairs containing objects of the same category is the basis of many tasks such as tracking and person re-identification, etc., and it is also the focus of our research. Existing traditional methods and deep learning-based methods have some degree of defects in speed or accuracy. In this paper, we made improvements on the Siamese network and proposed GetNet. The proposed method GetNet combines STN and Siamese network to get the target area first and then perform subsequent processing. Experiments show that our method achieves competitive results in speed and accuracy.



### Model-based Behavioral Cloning with Future Image Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.03157v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03157v1)
- **Published**: 2019-10-08 01:32:54+00:00
- **Updated**: 2019-10-08 01:32:54+00:00
- **Authors**: Alan Wu, AJ Piergiovanni, Michael S. Ryoo
- **Comment**: None
- **Journal**: None
- **Summary**: We present a visual imitation learning framework that enables learning of robot action policies solely based on expert samples without any robot trials. Robot exploration and on-policy trials in a real-world environment could often be expensive/dangerous. We present a new approach to address this problem by learning a future scene prediction model solely on a collection of expert trajectories consisting of unlabeled example videos and actions, and by enabling generalized action cloning using future image similarity. The robot learns to visually predict the consequences of taking an action, and obtains the policy by evaluating how similar the predicted future image is to an expert image. We develop a stochastic action-conditioned convolutional autoencoder, and present how we take advantage of future images for robot learning. We conduct experiments in simulated and real-life environments using a ground mobility robot with and without obstacles, and compare our models to multiple baseline methods.



### xYOLO: A Model For Real-Time Object Detection In Humanoid Soccer On Low-End Hardware
- **Arxiv ID**: http://arxiv.org/abs/1910.03159v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03159v1)
- **Published**: 2019-10-08 01:33:52+00:00
- **Updated**: 2019-10-08 01:33:52+00:00
- **Authors**: Daniel Barry, Munir Shah, Merel Keijsers, Humayun Khan, Banon Hopman
- **Comment**: 6 pages, 5 figures
- **Journal**: None
- **Summary**: With the emergence of onboard vision processing for areas such as the internet of things (IoT), edge computing and autonomous robots, there is increasing demand for computationally efficient convolutional neural network (CNN) models to perform real-time object detection on resource constraints hardware devices. Tiny-YOLO is generally considered as one of the faster object detectors for low-end devices and is the basis for our work. Our experiments on this network have shown that Tiny-YOLO can achieve 0.14 frames per second(FPS) on the Raspberry Pi 3 B, which is too slow for soccer playing autonomous humanoid robots detecting goal and ball objects. In this paper we propose an adaptation to the YOLO CNN model named xYOLO, that can achieve object detection at a speed of 9.66 FPS on the Raspberry Pi 3 B. This is achieved by trading an acceptable amount of accuracy, making the network approximately 70 times faster than Tiny-YOLO. Greater inference speed-ups were also achieved on a desktop CPU and GPU. Additionally we contribute an annotated Darknet dataset for goal and ball detection.



### Deep Multiphase Level Set for Scene Parsing
- **Arxiv ID**: http://arxiv.org/abs/1910.03166v2
- **DOI**: 10.1109/TIP.2019.2957915
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03166v2)
- **Published**: 2019-10-08 01:58:24+00:00
- **Updated**: 2019-10-13 02:53:07+00:00
- **Authors**: Pingping Zhang, Wei Liu, Yinjie Lei, Hongyu Wang, Huchuan Lu
- **Comment**: 12 pages, 8 tables, 8 figures
- **Journal**: None
- **Summary**: Recently, Fully Convolutional Network (FCN) seems to be the go-to architecture for image segmentation, including semantic scene parsing. However, it is difficult for a generic FCN to discriminate pixels around the object boundaries, thus FCN based methods may output parsing results with inaccurate boundaries. Meanwhile, level set based active contours are superior to the boundary estimation due to the sub-pixel accuracy that they achieve. However, they are quite sensitive to initial settings. To address these limitations, in this paper we propose a novel Deep Multiphase Level Set (DMLS) method for semantic scene parsing, which efficiently incorporates multiphase level sets into deep neural networks. The proposed method consists of three modules, i.e., recurrent FCNs, adaptive multiphase level set, and deeply supervised learning. More specifically, recurrent FCNs learn multi-level representations of input images with different contexts. Adaptive multiphase level set drives the discriminative contour for each semantic class, which makes use of the advantages of both global and local information. In each time-step of the recurrent FCNs, deeply supervised learning is incorporated for model training. Extensive experiments on three public benchmarks have shown that our proposed method achieves new state-of-the-art performances.



### Sky pixel detection in outdoor imagery using an adaptive algorithm and machine learning
- **Arxiv ID**: http://arxiv.org/abs/1910.03182v2
- **DOI**: 10.1016/j.uclim.2019.100572
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03182v2)
- **Published**: 2019-10-08 02:45:38+00:00
- **Updated**: 2019-12-09 23:01:34+00:00
- **Authors**: Kerry A. Nice, Jasper S. Wijnands, Ariane Middel, Jingcheng Wang, Yiming Qiu, Nan Zhao, Jason Thompson, Gideon D. P. A. Aschwanden, Haifeng Zhao, Mark Stevenson
- **Comment**: This revision accepted in Urban Climate. 17 pages, 12 figures
- **Journal**: None
- **Summary**: Computer vision techniques enable automated detection of sky pixels in outdoor imagery. In urban climate, sky detection is an important first step in gathering information about urban morphology and sky view factors. However, obtaining accurate results remains challenging and becomes even more complex using imagery captured under a variety of lighting and weather conditions.   To address this problem, we present a new sky pixel detection system demonstrated to produce accurate results using a wide range of outdoor imagery types. Images are processed using a selection of mean-shift segmentation, K-means clustering, and Sobel filters to mark sky pixels in the scene. The algorithm for a specific image is chosen by a convolutional neural network, trained with 25,000 images from the Skyfinder data set, reaching 82% accuracy for the top three classes. This selection step allows the sky marking to follow an adaptive process and to use different techniques and parameters to best suit a particular image. An evaluation of fourteen different techniques and parameter sets shows that no single technique can perform with high accuracy across varied Skyfinder and Google Street View data sets. However, by using our adaptive process, large increases in accuracy are observed. The resulting system is shown to perform better than other published techniques.



### Dynamic Mode Decomposition based feature for Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.03188v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03188v1)
- **Published**: 2019-10-08 03:09:42+00:00
- **Updated**: 2019-10-08 03:09:42+00:00
- **Authors**: Rahul-Vigneswaran K, Sachin-Kumar S, Neethu Mohan, Soman KP
- **Comment**: Selected for Spotlight presentation at TENCON 2019
- **Journal**: None
- **Summary**: Irrespective of the fact that Machine learning has produced groundbreaking results, it demands an enormous amount of data in order to perform so. Even though data production has been in its all-time high, almost all the data is unlabelled, hence making them unsuitable for training the algorithms. This paper proposes a novel method of extracting the features using Dynamic Mode Decomposition (DMD). The experiment is performed using data samples from Imagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink approach (RKS). The results have shown that DMD features with RKS give competing results.



### Level set image segmentation with velocity term learned from data with applications to lung nodule segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.03191v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03191v3)
- **Published**: 2019-10-08 03:13:17+00:00
- **Updated**: 2021-01-19 01:42:22+00:00
- **Authors**: Matthew C Hancock, Jerry F Magnan
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: Lung nodule segmentation, i.e., the algorithmic delineation of the lung nodule surface, is a fundamental component of computational nodule analysis pipelines. We propose a new method for segmentation that is a machine learning based extension of current approaches, using labeled image examples to improve its accuracy.   Approach: We introduce an extension of the standard level set image segmentation method where the velocity function is learned from data via machine learning regression methods, rather than a priori designed. Instead, the method employs a set of features to learn a velocity function that guides the level set evolution from initialization.   Results: We apply the method to image volumes of lung nodules from CT scans in the publicly available LIDC dataset, obtaining an average intersection over union score of 0.7185($\pm$0.1114), which is competitive with other methods. We analyze segmentation performance by anatomical and appearance-based categories of the nodules, finding that the method performs better for isolated nodules with well-defined margins. We find that the segmentation performance for nodules in more complex surroundings and having more complex CT appearance is improved with the addition of combined global-local features.   Conclusions: The level set machine learning segmentation approach proposed herein is competitive with current methods. It provides accurate lung nodule segmentation results in a variety of anatomical contexts.



### A Study on Wrist Identification for Forensic Investigation
- **Arxiv ID**: http://arxiv.org/abs/1910.03213v1
- **DOI**: 10.1016/j.imavis.2019.05.005
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03213v1)
- **Published**: 2019-10-08 05:04:11+00:00
- **Updated**: 2019-10-08 05:04:11+00:00
- **Authors**: Wojciech Michal Matkowski, Frodo Kin Sun Chan, Adams Wai Kin Kong
- **Comment**: None
- **Journal**: Image and Vision Computing, vol. 88, August 2019, pp 96-112
- **Summary**: Criminal and victim identification based on crime scene images is an important part of forensic investigation. Criminals usually avoid identification by covering their faces and tattoos in the evidence images, which are taken in uncontrolled environments. Existing identification methods, which make use of biometric traits, such as vein, skin mark, height, skin color, weight, race, etc., are considered for solving this problem. The soft biometric traits, including skin color, gender, height, weight and race, provide useful information but not distinctive enough. Veins and skin marks are limited to high resolution images and some body sites may neither have enough skin marks nor clear veins. Terrorists and rioters tend to expose their wrists in a gesture of triumph, greeting or salute, while paedophiles usually show them when touching victims. However, wrists were neglected by the biometric community for forensic applications. In this paper, a wrist identification algorithm, which includes skin segmentation, key point localization, image to template alignment, large feature set extraction, and classification, is proposed. The proposed algorithm is evaluated on NTU-Wrist-Image-Database-v1, which consists of 3945 images from 731 different wrists, including 205 pairs of wrist images collected from the Internet, taken under uneven illuminations with different poses and resolutions. The experimental results show that wrist is a useful clue for criminal and victim identification. Keywords: biometrics, criminal and victim identification, forensics, wrist.



### The 'Paris-end' of town? Urban typology through machine learning
- **Arxiv ID**: http://arxiv.org/abs/1910.03220v1
- **DOI**: 10.3390/urbansci4020027
- **Categories**: **cs.CV**, cs.CY
- **Links**: [PDF](http://arxiv.org/pdf/1910.03220v1)
- **Published**: 2019-10-08 05:26:33+00:00
- **Updated**: 2019-10-08 05:26:33+00:00
- **Authors**: Kerry A. Nice, Jason Thompson, Jasper S. Wijnands, Gideon D. P. A. Aschwanden, Mark Stevenson
- **Comment**: None
- **Journal**: None
- **Summary**: The confluence of recent advances in availability of geospatial information, computing power, and artificial intelligence offers new opportunities to understand how and where our cities differ or are alike. Departing from a traditional `top-down' analysis of urban design features, this project analyses millions of images of urban form (consisting of street view, satellite imagery, and street maps) to find shared characteristics. A (novel) neural network-based framework is trained with imagery from the largest 1692 cities in the world and the resulting models are used to compare within-city locations from Melbourne and Sydney to determine the closest connections between these areas and their international comparators. This work demonstrates a new, consistent, and objective method to begin to understand the relationship between cities and their health, transport, and environmental consequences of their design. The results show specific advantages and disadvantages using each type of imagery. Neural networks trained with map imagery will be highly influenced by the mix of roads, public transport, and green and blue space as well as the structure of these elements. The colours of natural and built features stand out as dominant characteristics in satellite imagery. The use of street view imagery will emphasise the features of a human scaled visual geography of streetscapes. Finally, and perhaps most importantly, this research also answers the age-old question, ``Is there really a `Paris-end' to your city?''.



### Identifying Candidate Spaces for Advert Implantation
- **Arxiv ID**: http://arxiv.org/abs/1910.03227v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03227v1)
- **Published**: 2019-10-08 06:12:53+00:00
- **Updated**: 2019-10-08 06:12:53+00:00
- **Authors**: Soumyabrata Dev, Hossein Javidnia, Murhaf Hossari, Matthew Nicholson, Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, François Pitié
- **Comment**: Published in Proc. IEEE 7th International Conference on Computer
  Science and Network Technology, 2019
- **Journal**: None
- **Summary**: Virtual advertising is an important and promising feature in the area of online advertising. It involves integrating adverts onto live or recorded videos for product placements and targeted advertisements. Such integration of adverts is primarily done by video editors in the post-production stage, which is cumbersome and time-consuming. Therefore, it is important to automatically identify candidate spaces in a video frame, wherein new adverts can be implanted. The candidate space should match the scene perspective, and also have a high quality of experience according to human subjective judgment. In this paper, we propose the use of a bespoke neural net that can assist the video editors in identifying candidate spaces. We benchmark our approach against several deep-learning architectures on a large-scale image dataset of candidate spaces of outdoor scenes. Our work is the first of its kind in this area of multimedia and augmented reality applications, and achieves the best results.



### Meta Module Network for Compositional Visual Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1910.03230v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03230v5)
- **Published**: 2019-10-08 06:28:24+00:00
- **Updated**: 2020-11-08 02:52:51+00:00
- **Authors**: Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, Jingjing Liu
- **Comment**: Accepted to WACV 21 (Oral)
- **Journal**: None
- **Summary**: Neural Module Network (NMN) exhibits strong interpretability and compositionality thanks to its handcrafted neural modules with explicit multi-hop reasoning capability. However, most NMNs suffer from two critical drawbacks: 1) scalability: customized module for specific function renders it impractical when scaling up to a larger set of functions in complex tasks; 2) generalizability: rigid pre-defined module inventory makes it difficult to generalize to unseen functions in new tasks/domains. To design a more powerful NMN architecture for practical use, we propose Meta Module Network (MMN) centered on a novel meta module, which can take in function recipes and morph into diverse instance modules dynamically. The instance modules are then woven into an execution graph for complex visual reasoning, inheriting the strong explainability and compositionality of NMN. With such a flexible instantiation mechanism, the parameters of instance modules are inherited from the central meta module, retaining the same model complexity as the function set grows, which promises better scalability. Meanwhile, as functions are encoded into the embedding space, unseen functions can be readily represented based on its structural similarity with previously observed ones, which ensures better generalizability. Experiments on GQA and CLEVR datasets validate the superiority of MMN over state-of-the-art NMN designs. Synthetic experiments on held-out unseen functions from GQA dataset also demonstrate the strong generalizability of MMN. Our code and model are released in Github https://github.com/wenhuchen/Meta-Module-Network.



### Metric Pose Estimation for Human-Machine Interaction Using Monocular Vision
- **Arxiv ID**: http://arxiv.org/abs/1910.03239v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03239v1)
- **Published**: 2019-10-08 07:00:05+00:00
- **Updated**: 2019-10-08 07:00:05+00:00
- **Authors**: Christoph Heindl, Markus Ikeda, Gernot Stübl, Andreas Pichler, Josef Scharinger
- **Comment**: IROS 2019, Factory of the Future
- **Journal**: None
- **Summary**: The rapid growth of collaborative robotics in production requires new automation technologies that take human and machine equally into account. In this work, we describe a monocular camera based system to detect human-machine interactions from a bird's-eye perspective. Our system predicts poses of humans and robots from a single wide-angle color image. Even though our approach works on 2D color input, we lift the majority of detections to a metric 3D space. Our system merges pose information with predefined virtual sensors to coordinate human-machine interactions. We demonstrate the advantages of our system in three use cases.



### Semi Few-Shot Attribute Translation
- **Arxiv ID**: http://arxiv.org/abs/1910.03240v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03240v2)
- **Published**: 2019-10-08 07:02:19+00:00
- **Updated**: 2019-10-16 15:59:54+00:00
- **Authors**: Ricard Durall, Franz-Josef Pfreundt, Janis Keuper
- **Comment**: arXiv admin note: text overlap with arXiv:1904.04232,
  arXiv:1901.02199 by other authors
- **Journal**: None
- **Summary**: Recent studies have shown remarkable success in image-to-image translation for attribute transfer applications. However, most of existing approaches are based on deep learning and require an abundant amount of labeled data to produce good results, therefore limiting their applicability. In the same vein, recent advances in meta-learning have led to successful implementations with limited available data, allowing so-called few-shot learning.   In this paper, we address this limitation of supervised methods, by proposing a novel approach based on GANs. These are trained in a meta-training manner, which allows them to perform image-to-image translations using just a few labeled samples from a new target class. This work empirically demonstrates the potential of training a GAN for few shot image-to-image translation on hair color attribute synthesis tasks, opening the door to further research on generative transfer learning.



### Self-Paced Deep Regression Forests for Facial Age Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.03244v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03244v5)
- **Published**: 2019-10-08 07:08:45+00:00
- **Updated**: 2020-04-17 11:36:39+00:00
- **Authors**: Shijie Ai, Lili Pan, Yazhou Ren
- **Comment**: 7 pages, 5 figures, 2 tables
- **Journal**: None
- **Summary**: Facial age estimation is an important and challenging problem in computer vision. Existing approaches usually employ deep neural networks (DNNs) to fit the mapping from facial features to age, even though there exist some noisy and confusing samples. We argue that it is more desirable to distinguish noisy and confusing facial images from regular ones, and alleviate the interference arising from them. To this end, we propose self-paced deep regression forests (SP-DRFs) -- a gradual learning DNNs framework for age estimation. As the model is learned gradually, from simplicity to complexity, it tends to emphasize more on reliable samples and avoid bad local minima. Moreover, the proposed capped-likelihood function helps to exclude noisy samples in training, rendering our SP-DRFs significantly more robust. We demonstrate the efficacy of SP-DRFs on Morph II and FG-NET datasets, where our model achieves state-of-the-art performance.



### Eyenet: Attention based Convolutional Encoder-Decoder Network for Eye Region Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.03274v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03274v1)
- **Published**: 2019-10-08 08:43:27+00:00
- **Updated**: 2019-10-08 08:43:27+00:00
- **Authors**: Priya Kansal, Sabari Nathan
- **Comment**: To be appear in ICCVW 2019
- **Journal**: None
- **Summary**: With the immersive development in the field of augmented and virtual reality, accurate and speedy eye-tracking is required. Facebook Research has organized a challenge, named OpenEDS Semantic Segmentation challenge for per-pixel segmentation of the key eye regions: the sclera, the iris, the pupil, and everything else (background). There are two constraints set for the participants viz MIOU and the computational complexity of the model. More recently, researchers have achieved quite a good result using the convolutional neural networks (CNN) in segmenting eyeregions. However, the environmental challenges involved in this task such as low resolution, blur, unusual glint and, illumination, off-angles, off-axis, use of glasses and different color of iris region hinder the accuracy of segmentation. To address the challenges in eye segmentation, the present work proposes a robust and computationally efficient attention-based convolutional encoder-decoder network for segmenting all the eye regions. Our model, named EyeNet, includes modified residual units as the backbone, two types of attention blocks and multi-scale supervision for segmenting the aforesaid four eye regions. Our proposed model achieved a total score of 0.974(EDS Evaluation metric) on test data, which demonstrates superior results compared to the baseline methods.



### Defective samples simulation through Neural Style Transfer for automatic surface defect segment
- **Arxiv ID**: http://arxiv.org/abs/1910.03334v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03334v1)
- **Published**: 2019-10-08 10:55:19+00:00
- **Updated**: 2019-10-08 10:55:19+00:00
- **Authors**: Taoran Wei, Danhua Cao, Xingru Jiang, Caiyun Zheng, Lizhe Liu
- **Comment**: To be published in 2019 International Conference on Optical
  Instrument and Technology (OIT 2019)
- **Journal**: None
- **Summary**: Owing to the lack of defect samples in industrial product quality inspection, trained segmentation model tends to overfit when applied online. To address this problem, we propose a defect sample simulation algorithm based on neural style transfer. The simulation algorithm requires only a small number of defect samples for training, and can efficiently generate simulation samples for next-step segmentation task. In our work, we introduce a masked histogram matching module to maintain color consistency of the generated area and the true defect. To preserve the texture consistency with the surrounding pixels, we take the fast style transfer algorithm to blend the generated area into the background. At the same time, we also use the histogram loss to further improve the quality of the generated image. Besides, we propose a novel structure of segment net to make it more suitable for defect segmentation task. We train the segment net with the real defect samples and the generated simulation samples separately on the button datasets. The results show that the F1 score of the model trained with only the generated simulation samples reaches 0.80, which is better than the real sample result.



### Graph-based Spatial-temporal Feature Learning for Neuromorphic Vision Sensing
- **Arxiv ID**: http://arxiv.org/abs/1910.03579v2
- **DOI**: 10.1109/TIP.2020.3023597
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03579v2)
- **Published**: 2019-10-08 10:55:57+00:00
- **Updated**: 2019-11-11 16:19:35+00:00
- **Authors**: Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze, Yiannis Andreopoulos
- **Comment**: 16 pages, 5 figures. This work is a journal extension of our ICCV'19
  paper arXiv:1908.06648
- **Journal**: None
- **Summary**: Neuromorphic vision sensing (NVS)\ devices represent visual information as sequences of asynchronous discrete events (a.k.a., "spikes") in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, feature representation for NVS is far behind its APS-based counterparts, resulting in lower performance in high-level computer vision tasks. To fully utilize its sparse and asynchronous nature, we propose a compact graph representation for NVS, which allows for end-to-end learning with graph convolution neural networks. We couple this with a novel end-to-end feature learning framework that accommodates both appearance-based and motion-based tasks. The core of our framework comprises a spatial feature learning module, which utilizes residual-graph convolutional neural networks (RG-CNN), for end-to-end learning of appearance-based features directly from graphs. We extend this with our proposed Graph2Grid block and temporal feature learning module for efficiently modelling temporal dependencies over multiple graphs and a long temporal extent. We show how our framework can be configured for object classification, action recognition and action similarity labeling. Importantly, our approach preserves the spatial and temporal coherence of spike events, while requiring less computation and memory. The experimental validation shows that our proposed framework outperforms all recent methods on standard datasets. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we introduce, evaluate and make available the American Sign Language letters (ASL-DVS), as well as human action dataset (UCF101-DVS, HMDB51-DVS and ASLAN-DVS).



### Improving Map Re-localization with Deep 'Movable' Objects Segmentation on 3D LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1910.03336v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03336v1)
- **Published**: 2019-10-08 11:11:27+00:00
- **Updated**: 2019-10-08 11:11:27+00:00
- **Authors**: Victor Vaquero, Kai Fischer, Francesc Moreno-Noguer, Alberto Sanfeliu, Stefan Milz
- **Comment**: None
- **Journal**: None
- **Summary**: Localization and Mapping is an essential component to enable Autonomous Vehicles navigation, and requires an accuracy exceeding that of commercial GPS-based systems. Current odometry and mapping algorithms are able to provide this accurate information. However, the lack of robustness of these algorithms against dynamic obstacles and environmental changes, even for short time periods, forces the generation of new maps on every session without taking advantage of previously obtained ones. In this paper we propose the use of a deep learning architecture to segment movable objects from 3D LiDAR point clouds in order to obtain longer-lasting 3D maps. This will in turn allow for better, faster and more accurate re-localization and trajectoy estimation on subsequent days. We show the effectiveness of our approach in a very dynamic and cluttered scenario, a supermarket parking lot. For that, we record several sequences on different days and compare localization errors with and without our movable objects segmentation method. Results show that we are able to accurately re-locate over a filtered map, consistently reducing trajectory errors between an average of 35.1% with respect to a non-filtered map version and of 47.9% with respect to a standalone map created on the current session.



### Modulated Self-attention Convolutional Network for VQA
- **Arxiv ID**: http://arxiv.org/abs/1910.03343v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03343v2)
- **Published**: 2019-10-08 11:28:38+00:00
- **Updated**: 2019-10-31 16:59:23+00:00
- **Authors**: Jean-Benoit Delbrouck, Antoine Maiorca, Nathan Hubens, Stéphane Dupont
- **Comment**: Accepted at NeurIPS 2019 workshop: ViGIL
- **Journal**: None
- **Summary**: As new data-sets for real-world visual reasoning and compositional question answering are emerging, it might be needed to use the visual feature extraction as a end-to-end process during training. This small contribution aims to suggest new ideas to improve the visual processing of traditional convolutional network for visual question answering (VQA). In this paper, we propose to modulate by a linguistic input a CNN augmented with self-attention. We show encouraging relative improvements for future research in this direction.



### Improved Res2Net model for Person re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.04061v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04061v2)
- **Published**: 2019-10-08 12:12:11+00:00
- **Updated**: 2020-02-24 10:24:25+00:00
- **Authors**: Zongjing Cao, Hyo Jong Lee
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: Person re-identification has become a very popular research topic in the computer vision community owing to its numerous applications and growing importance in visual surveillance. Person re-identification remains challenging due to occlusion, illumination and significant intra-class variations across different cameras. In this paper, we propose a multi-task network base on an improved Res2Net model that simultaneously computes the identification loss and verification loss of two pedestrian images. Given a pair of pedestrian images, the system predicts the identities of the two input images and whether they belong to the same identity. In order to obtain deeper feature information of pedestrians, we propose to use the latest Res2Net model for feature extraction of each input image. Experiments on several large-scale person re-identification benchmark datasets demonstrate the accuracy of our approach. For example, rank-1 accuracies are 83.18% (+1.38) and 93.14% (+0.84) for the DukeMTMC and Market-1501 datasets, respectively. The proposed method shows encouraging improvements compared with state-of-the-art methods.



### Refining 6D Object Pose Predictions using Abstract Render-and-Compare
- **Arxiv ID**: http://arxiv.org/abs/1910.03412v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03412v1)
- **Published**: 2019-10-08 14:17:39+00:00
- **Updated**: 2019-10-08 14:17:39+00:00
- **Authors**: Arul Selvam Periyasamy, Max Schwarz, Sven Behnke
- **Comment**: Accepted for IEEE-RAS International Conference on Humanoid Robots,
  Toronto, Canada, to appear October 2019
- **Journal**: None
- **Summary**: Robotic systems often require precise scene analysis capabilities, especially in unstructured, cluttered situations, as occurring in human-made environments. While current deep-learning based methods yield good estimates of object poses, they often struggle with large amounts of occlusion and do not take inter-object effects into account. Vision as inverse graphics is a promising concept for detailed scene analysis. A key element for this idea is a method for inferring scene parameter updates from the rasterized 2D scene. However, the rasterization process is notoriously difficult to invert, both due to the projection and occlusion process, but also due to secondary effects such as lighting or reflections. We propose to remove the latter from the process by mapping the rasterized image into an abstract feature space learned in a self-supervised way from pixel correspondences. Using only a light-weight inverse rendering module, this allows us to refine 6D object pose estimations in highly cluttered scenes by optimizing a simple pixel-wise difference in the abstract image representation. We evaluate our approach on the challenging YCB-Video dataset, where it yields large improvements and demonstrates a large basin of attraction towards the correct object poses.



### TraffickCam: Explainable Image Matching For Sex Trafficking Investigations
- **Arxiv ID**: http://arxiv.org/abs/1910.03455v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03455v1)
- **Published**: 2019-10-08 15:24:30+00:00
- **Updated**: 2019-10-08 15:24:30+00:00
- **Authors**: Abby Stylianou, Richard Souvenir, Robert Pless
- **Comment**: Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA
- **Journal**: None
- **Summary**: Investigations of sex trafficking sometimes have access to photographs of victims in hotel rooms. These images directly link victims to places, which can help verify where victims have been trafficked or where traffickers might operate in the future. Current machine learning approaches give promising results in image search to find the matching hotel. This paper explores approaches to make this end-to-end system better support government and law enforcement requirements, including improved performance, visualization approaches that explain what parts of the image led to a match, and infrastructure to support exporting the results of a query.



### Directional Adversarial Training for Cost Sensitive Deep Learning Classification Applications
- **Arxiv ID**: http://arxiv.org/abs/1910.03468v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03468v1)
- **Published**: 2019-10-08 15:40:09+00:00
- **Updated**: 2019-10-08 15:40:09+00:00
- **Authors**: Matteo Terzi, Gian Antonio Susto, Pratik Chaudhari
- **Comment**: None
- **Journal**: None
- **Summary**: In many real-world applications of Machine Learning it is of paramount importance not only to provide accurate predictions, but also to ensure certain levels of robustness. Adversarial Training is a training procedure aiming at providing models that are robust to worst-case perturbations around predefined points. Unfortunately, one of the main issues in adversarial training is that robustness w.r.t. gradient-based attackers is always achieved at the cost of prediction accuracy. In this paper, a new algorithm, called Wasserstein Projected Gradient Descent (WPGD), for adversarial training is proposed. WPGD provides a simple way to obtain cost-sensitive robustness, resulting in a finer control of the robustness-accuracy trade-off. Moreover, WPGD solves an optimal transport problem on the output space of the network and it can efficiently discover directions where robustness is required, allowing to control the directional trade-off between accuracy and robustness. The proposed WPGD is validated in this work on image recognition tasks with different benchmark datasets and architectures. Moreover, real world-like datasets are often unbalanced: this paper shows that when dealing with such type of datasets, the performance of adversarial training are mainly affected in term of standard accuracy.



### Observer Dependent Lossy Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1910.03472v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03472v2)
- **Published**: 2019-10-08 15:43:29+00:00
- **Updated**: 2020-11-02 10:11:58+00:00
- **Authors**: Maurice Weber, Cedric Renggli, Helmut Grabner, Ce Zhang
- **Comment**: @German Conference on Pattern Recognition (DAGM GCPR 2020)
- **Journal**: None
- **Summary**: Deep neural networks have recently advanced the state-of-the-art in image compression and surpassed many traditional compression algorithms. The training of such networks involves carefully trading off entropy of the latent representation against reconstruction quality. The term quality crucially depends on the observer of the images which, in the vast majority of literature, is assumed to be human. In this paper, we aim to go beyond this notion of compression quality and look at human visual perception and image classification simultaneously. To that end, we use a family of loss functions that allows to optimize deep image compression depending on the observer and to interpolate between human perceived visual quality and classification accuracy, enabling a more unified view on image compression. Our extensive experiments show that using perceptual loss functions to train a compression system preserves classification accuracy much better than traditional codecs such as BPG without requiring retraining of classifiers on compressed images. For example, compressing ImageNet to 0.25 bpp reduces Inception-ResNet classification accuracy by only 2%. At the same time, when using a human friendly loss function, the same compression system achieves competitive performance in terms of MS-SSIM. By combining these two objective functions, we show that there is a pronounced trade-off in compression quality between the human visual system and classification accuracy.



### Learning event representations for temporal segmentation of image sequences by dynamic graph embedding
- **Arxiv ID**: http://arxiv.org/abs/1910.03483v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03483v3)
- **Published**: 2019-10-08 15:48:50+00:00
- **Updated**: 2020-12-10 11:03:37+00:00
- **Authors**: Mariella Dimiccoli, Herwig Wendt
- **Comment**: Accepted in IEEE Transactions on Image Processing, 2020. To appear
- **Journal**: None
- **Summary**: Recently, self-supervised learning has proved to be effective to learn representations of events suitable for temporal segmentation in image sequences, where events are understood as sets of temporally adjacent images that are semantically perceived as a whole. However, although this approach does not require expensive manual annotations, it is data hungry and suffers from domain adaptation problems. As an alternative, in this work, we propose a novel approach for learning event representations named Dynamic Graph Embedding (DGE). The assumption underlying our model is that a sequence of images can be represented by a graph that encodes both semantic and temporal similarity. The key novelty of DGE is to learn jointly the graph and its graph embedding. At its core, DGE works by iterating over two steps: 1) updating the graph representing the semantic and temporal similarity of the data based on the current data representation, and 2) updating the data representation to take into account the current data graph structure. The main advantage of DGE over state-of-the-art self-supervised approaches is that it does not require any training set, but instead learns iteratively from the data itself a low-dimensional embedding that reflects their temporal and semantic similarity. Experimental results on two benchmark datasets of real image sequences captured at regular time intervals demonstrate that the proposed DGE leads to event representations effective for temporal segmentation. In particular, it achieves robust temporal segmentation on the EDUBSeg and EDUBSeg-Desc benchmark datasets, outperforming the state of the art. Additional experiments on two Human Motion Segmentation benchmark datasets demonstrate the generalization capabilities of the proposed DGE.



### Real-time processing of high-resolution video and 3D model-based tracking for remote towers
- **Arxiv ID**: http://arxiv.org/abs/1910.03517v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03517v2)
- **Published**: 2019-10-08 16:22:29+00:00
- **Updated**: 2020-01-15 12:41:52+00:00
- **Authors**: Oliver J. D. Barrowclough, Sverre Briseid, Georg Muntingh, Torbjørn Viksand
- **Comment**: None
- **Journal**: None
- **Summary**: High quality video data is a core component in emerging remote tower operations as it inherently contains a huge amount of information on which an air traffic controller can base decisions. Various digital technologies also have the potential to exploit this data to bring enhancements, including tracking ground movements by relating events in the video view to their positions in 3D space. The total resolution of remote tower setups with multiple cameras often exceeds 25 million RGB pixels and is captured at 30 frames per second or more. It is thus a challenge to efficiently process all the data in such a way as to provide relevant real-time enhancements to the controller. In this paper we discuss how a number of improvements can be implemented efficiently on a single workstation by decoupling processes and utilizing hardware for parallel computing. We also highlight how decoupling the processes in this way increases resilience of the software solution in the sense that failure of a single component does not impair the function of the other components.



### Predicting Auction Price of Vehicle License Plate with Deep Residual Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.04879v1
- **DOI**: 10.1007/978-3-030-26142-9_16
- **Categories**: **cs.CV**, cs.LG, econ.GN, q-fin.EC
- **Links**: [PDF](http://arxiv.org/pdf/1910.04879v1)
- **Published**: 2019-10-08 16:38:06+00:00
- **Updated**: 2019-10-08 16:38:06+00:00
- **Authors**: Vinci Chow
- **Comment**: None
- **Journal**: Trends and Applications in Knowledge Discovery and Data Mining.
  PAKDD 2019. Lecture Notes in Computer Science, vol 11607. Springer, Cham
- **Summary**: Due to superstition, license plates with desirable combinations of characters are highly sought after in China, fetching prices that can reach into the millions in government-held auctions. Despite the high stakes involved, there has been essentially no attempt to provide price estimates for license plates. We present an end-to-end neural network model that simultaneously predict the auction price, gives the distribution of prices and produces latent feature vectors. While both types of neural network architectures we consider outperform simpler machine learning methods, convolutional networks outperform recurrent networks for comparable training time or model complexity. The resulting model powers our online price estimator and search engine.



### Multi-Source Domain Adaptation and Semi-Supervised Domain Adaptation with Focus on Visual Domain Adaptation Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.03548v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03548v2)
- **Published**: 2019-10-08 17:17:35+00:00
- **Updated**: 2019-10-14 05:28:07+00:00
- **Authors**: Yingwei Pan, Yehao Li, Qi Cai, Yang Chen, Ting Yao
- **Comment**: Rank 1 in Multi-Source Domain Adaptation of Visual Domain Adaptation
  Challenge (VisDA-2019). Source code of each task:
  https://github.com/Panda-Peter/visda2019-multisource and
  https://github.com/Panda-Peter/visda2019-semisupervised
- **Journal**: None
- **Summary**: This notebook paper presents an overview and comparative analysis of our systems designed for the following two tasks in Visual Domain Adaptation Challenge (VisDA-2019): multi-source domain adaptation and semi-supervised domain adaptation.   Multi-Source Domain Adaptation: We investigate both pixel-level and feature-level adaptation for multi-source domain adaptation task, i.e., directly hallucinating labeled target sample via CycleGAN and learning domain-invariant feature representations through self-learning. Moreover, the mechanism of fusing features from different backbones is further studied to facilitate the learning of domain-invariant classifiers. Source code and pre-trained models are available at \url{https://github.com/Panda-Peter/visda2019-multisource}.   Semi-Supervised Domain Adaptation: For this task, we adopt a standard self-learning framework to construct a classifier based on the labeled source and target data, and generate the pseudo labels for unlabeled target data. These target data with pseudo labels are then exploited to re-training the classifier in a following iteration. Furthermore, a prototype-based classification module is additionally utilized to strengthen the predictions. Source code and pre-trained models are available at \url{https://github.com/Panda-Peter/visda2019-semisupervised}.



### When Does Self-supervision Improve Few-shot Learning?
- **Arxiv ID**: http://arxiv.org/abs/1910.03560v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03560v2)
- **Published**: 2019-10-08 17:47:14+00:00
- **Updated**: 2020-07-30 06:08:35+00:00
- **Authors**: Jong-Chyi Su, Subhransu Maji, Bharath Hariharan
- **Comment**: ECCV 2020 camera ready. This is an updated version of "Boosting
  Supervision with Self-Supervision for Few-shot Learning" arXiv:1906.07079
- **Journal**: None
- **Summary**: We investigate the role of self-supervised learning (SSL) in the context of few-shot learning. Although recent research has shown the benefits of SSL on large unlabeled datasets, its utility on small datasets is relatively unexplored. We find that SSL reduces the relative error rate of few-shot meta-learners by 4%-27%, even when the datasets are small and only utilizing images within the datasets. The improvements are greater when the training set is smaller or the task is more challenging. Although the benefits of SSL may increase with larger training sets, we observe that SSL can hurt the performance when the distributions of images used for meta-learning and SSL are different. We conduct a systematic study by varying the degree of domain shift and analyzing the performance of several meta-learners on a multitude of domains. Based on this analysis we present a technique that automatically selects images for SSL from a large, generic pool of unlabeled images for a given dataset that provides further improvements.



### Deep Network Classification by Scattering and Homotopy Dictionary Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.03561v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03561v3)
- **Published**: 2019-10-08 17:47:44+00:00
- **Updated**: 2020-02-20 17:32:42+00:00
- **Authors**: John Zarka, Louis Thiry, Tomás Angles, Stéphane Mallat
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet.



### Object-centric Forward Modeling for Model Predictive Control
- **Arxiv ID**: http://arxiv.org/abs/1910.03568v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03568v1)
- **Published**: 2019-10-08 17:58:10+00:00
- **Updated**: 2019-10-08 17:58:10+00:00
- **Authors**: Yufei Ye, Dhiraj Gandhi, Abhinav Gupta, Shubham Tulsiani
- **Comment**: None
- **Journal**: None
- **Summary**: We present an approach to learn an object-centric forward model, and show that this allows us to plan for sequences of actions to achieve distant desired goals. We propose to model a scene as a collection of objects, each with an explicit spatial location and implicit visual feature, and learn to model the effects of actions using random interaction data. Our model allows capturing the robot-object and object-object interactions, and leads to more sample-efficient and accurate predictions. We show that this learned model can be leveraged to search for action sequences that lead to desired goal configurations, and that in conjunction with a learned correction module, this allows for robust closed loop execution. We present experiments both in simulation and the real world, and show that our approach improves over alternate implicit or pixel-space forward models. Please see our project page (https://judyye.github.io/ocmpc/) for result videos.



### A Deep Learning Framework for Detection of Targets in Thermal Images to Improve Firefighting
- **Arxiv ID**: http://arxiv.org/abs/1910.03617v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03617v3)
- **Published**: 2019-10-08 18:08:42+00:00
- **Updated**: 2020-04-17 21:54:44+00:00
- **Authors**: Manish Bhattarai, Manel Martínez-Ramón
- **Comment**: None
- **Journal**: None
- **Summary**: Intelligent detection and processing capabilities can be instrumental to improving the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. The objective of this research is to create an automated system that is capable of real-time, intelligent object detection and recognition and facilitates the improved situational awareness of firefighters during an emergency response. We have explored state of the art machine/deep learning techniques to achieve this objective. The goal for this work is to enhance the situational awareness of firefighters by effectively exploiting the information gathered from infrared cameras carried by firefighters. To accomplish this, we use a trained deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real time. In the midst of those critical circumstances created by structure fire, this system is able to accurately inform the decision making process of firefighters with real-time up-to-date scene information by extracting, processing, and analyzing crucial information. With the new information produced by the framework, firefighters are able to make more informed inferences about the circumstances for their safe navigation through such hazardous and potentially catastrophic environments.



### SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations
- **Arxiv ID**: http://arxiv.org/abs/1910.03624v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03624v1)
- **Published**: 2019-10-08 18:22:21+00:00
- **Updated**: 2019-10-08 18:22:21+00:00
- **Authors**: Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of $\ell_p$-bounded and $\ell_p$-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations. Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples.



### Prose for a Painting
- **Arxiv ID**: http://arxiv.org/abs/1910.03634v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03634v1)
- **Published**: 2019-10-08 18:39:49+00:00
- **Updated**: 2019-10-08 18:39:49+00:00
- **Authors**: Prerna Kashyap, Samrat Phatale, Iddo Drori
- **Comment**: None
- **Journal**: ICCV Workshop on Closing the Loop Between Vision and Language,
  2019
- **Summary**: Painting captions are often dry and simplistic which motivates us to describe a painting creatively in the style of Shakespearean prose. This is a difficult problem, since there does not exist a large supervised dataset from paintings to Shakespearean prose. Our solution is to use an intermediate English poem description of the painting and then apply language style transfer which results in Shakespearean prose describing the painting. We rate our results by human evaluation on a Likert scale, and evaluate the quality of language style transfer using BLEU score as a function of prose length. We demonstrate the applicability and limitations of our approach by generating Shakespearean prose for famous paintings. We make our models and code publicly available.



### Bregman Proximal Framework for Deep Linear Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.03638v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV, cs.IR, cs.LG, 90C26, 26B25, 90C30, 49M27, 47J25, 65K05, 65F22
- **Links**: [PDF](http://arxiv.org/pdf/1910.03638v1)
- **Published**: 2019-10-08 18:45:34+00:00
- **Updated**: 2019-10-08 18:45:34+00:00
- **Authors**: Mahesh Chandra Mukkamala, Felix Westerkamp, Emanuel Laude, Daniel Cremers, Peter Ochs
- **Comment**: 34 pages, 54 images
- **Journal**: None
- **Summary**: A typical assumption for the analysis of first order optimization methods is the Lipschitz continuity of the gradient of the objective function. However, for many practical applications this assumption is violated, including loss functions in deep learning. To overcome this issue, certain extensions based on generalized proximity measures known as Bregman distances were introduced. This initiated the development of the Bregman proximal gradient (BPG) algorithm and an inertial variant (momentum based) CoCaIn BPG, which however rely on problem dependent Bregman distances. In this paper, we develop Bregman distances for using BPG methods to train Deep Linear Neural Networks. The main implications of our results are strong convergence guarantees for these algorithms. We also propose several strategies for their efficient implementation, for example, closed form updates and a closed form expression for the inertial parameter of CoCaIn BPG. Moreover, the BPG method requires neither diminishing step sizes nor line search, unlike its corresponding Euclidean version. We numerically illustrate the competitiveness of the proposed methods compared to existing state of the art schemes.



### REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs
- **Arxiv ID**: http://arxiv.org/abs/1910.03667v1
- **DOI**: 10.1016/j.media.2019.101570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03667v1)
- **Published**: 2019-10-08 20:20:43+00:00
- **Updated**: 2019-10-08 20:20:43+00:00
- **Authors**: José Ignacio Orlando, Huazhu Fu, João Barbossa Breda, Karel van Keer, Deepti R. Bathula, Andrés Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo Lee, Joonseok Lee, Xiaoxiao Li, Peng Liu, Shuai Lu, Balamurali Murugesan, Valery Naranjo, Sai Samarth R. Phaye, Sharath M. Shankaranarayana, Apoorva Sikka, Jaemin Son, Anton van den Hengel, Shujun Wang, Junyan Wu, Zifeng Wu, Guanghui Xu, Yongli Xu, Pengshuai Yin, Fei Li, Xiulan Zhang, Yanwu Xu, Xiulan Zhang, Hrvoje Bogunović
- **Comment**: Accepted for publication in Medical Image Analysis
- **Journal**: None
- **Summary**: Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (\url{https://refuge.grand-challenge.org}), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.



### Representation Learning with Statistical Independence to Mitigate Bias
- **Arxiv ID**: http://arxiv.org/abs/1910.03676v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03676v4)
- **Published**: 2019-10-08 20:33:58+00:00
- **Updated**: 2020-11-20 17:57:38+00:00
- **Authors**: Ehsan Adeli, Qingyu Zhao, Adolf Pfefferbaum, Edith V. Sullivan, Li Fei-Fei, Juan Carlos Niebles, Kilian M. Pohl
- **Comment**: WACV 2021
- **Journal**: None
- **Summary**: Presence of bias (in datasets or tasks) is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in recent years. Such challenges range from spurious associations between variables in medical studies to the bias of race in gender or face recognition systems. Controlling for all types of biases in the dataset curation stage is cumbersome and sometimes impossible. The alternative is to use the available data and build models incorporating fair representation learning. In this paper, we propose such a model based on adversarial training with two competing objectives to learn features that have (1) maximum discriminative power with respect to the task and (2) minimal statistical mean dependence with the protected (bias) variable(s). Our approach does so by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and the learned features. We apply our method to synthetic data, medical images (containing task bias), and a dataset for gender classification (containing dataset bias). Our results show that the learned features by our method not only result in superior prediction performance but also are unbiased. The code is available at https://github.com/QingyuZhao/BR-Net/.



### NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.03695v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.4; I.2.1
- **Links**: [PDF](http://arxiv.org/pdf/1910.03695v1)
- **Published**: 2019-10-08 21:16:28+00:00
- **Updated**: 2019-10-08 21:16:28+00:00
- **Authors**: Sehyun Chun, Nima Hamidi Ghalehjegh, Joseph B. Choi, Chris W. Schwarz, John G. Gaspar, Daniel V. McGehee, Stephen S. Baek
- **Comment**: None
- **Journal**: None
- **Summary**: A new convolutional neural network (CNN) architecture for 2D driver/passenger pose estimation and seat belt detection is proposed in this paper. The new architecture is more nimble and thus more suitable for in-vehicle monitoring tasks compared to other generic pose estimation algorithms. The new architecture, named NADS-Net, utilizes the feature pyramid network (FPN) backbone with multiple detection heads to achieve the optimal performance for driver/passenger state detection tasks. The new architecture is validated on a new data set containing video clips of 100 drivers in 50 driving sessions that are collected for this study. The detection performance is analyzed under different demographic, appearance, and illumination conditions. The results presented in this paper may provide meaningful insights for the autonomous driving research community and automotive industry for future algorithm development and data collection.



