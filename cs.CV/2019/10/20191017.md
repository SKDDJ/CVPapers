# Arxiv Papers in cs.CV on 2019-10-17
### Cross Attention Network for Few-shot Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.07677v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07677v1)
- **Published**: 2019-10-17 01:45:58+00:00
- **Updated**: 2019-10-17 01:45:58+00:00
- **Authors**: Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen
- **Comment**: 12 pages, 4 figures. NeurIPS 2019 (Accepted)
- **Journal**: None
- **Summary**: Few-shot classification aims to recognize unlabeled samples from unseen classes given only few labeled samples. The unseen classes and low-data problem make few-shot classification very challenging. Many existing approaches extracted features from labeled and unlabeled samples independently, as a result, the features are not discriminative enough. In this work, we propose a novel Cross Attention Network to address the challenging problems in few-shot classification. Firstly, Cross Attention Module is introduced to deal with the problem of unseen classes. The module generates cross attention maps for each pair of class feature and query sample feature so as to highlight the target object regions, making the extracted feature more discriminative. Secondly, a transductive inference algorithm is proposed to alleviate the low-data problem, which iteratively utilizes the unlabeled query set to augment the support set, thereby making the class features more representative. Extensive experiments on two benchmarks show our method is a simple, effective and computationally efficient framework and outperforms the state-of-the-arts.



### A Parametric Perceptual Deficit Modeling and Diagnostics Framework for Retina Damage using Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/1910.07688v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07688v1)
- **Published**: 2019-10-17 02:54:26+00:00
- **Updated**: 2019-10-17 02:54:26+00:00
- **Authors**: Prithul Aniruddha, Nasif Zaman, Alireza Tavakkoli, Stewart Zuckerbrod
- **Comment**: None
- **Journal**: LNSC. 11845, pp. 1-12 (2019)
- **Summary**: Age-related Macular Degeneration (AMD) is a progressive visual impairment affecting millions of individuals. Since there is no current treatment for the disease, the only means of improving the lives of individuals suffering from the disease is via assistive technologies. In this paper we propose a novel and effective methodology to accurately generate a parametric model for the perceptual deficit caused by the physiological deterioration of a patient's retina due to AMD. Based on the parameters of the model, a mechanism is developed to simulate the patient's perception as a result of the disease. This simulation can effectively deliver the perceptual impact and its progression to the patient's eye doctor. In addition, we propose a mixed-reality apparatus and interface to allow the patient recover functional vision and to compensate for the perceptual loss caused by the physiological damage. The results obtained by the proposed approach show the superiority of our framework over the state-of-the-art low-vision systems.



### Deep Contextual Attention for Human-Object Interaction Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.07721v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07721v1)
- **Published**: 2019-10-17 05:44:46+00:00
- **Updated**: 2019-10-17 05:44:46+00:00
- **Authors**: Tiancai Wang, Rao Muhammad Anwer, Muhammad Haris Khan, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao, Jorma Laaksonen
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: Human-object interaction detection is an important and relatively new class of visual relationship detection tasks, essential for deeper scene understanding. Most existing approaches decompose the problem into object localization and interaction recognition. Despite showing progress, these approaches only rely on the appearances of humans and objects and overlook the available context information, crucial for capturing subtle interactions between them. We propose a contextual attention framework for human-object interaction detection. Our approach leverages context by learning contextually-aware appearance features for human and object instances. The proposed attention module then adaptively selects relevant instance-centric context information to highlight image regions likely to contain human-object interactions. Experiments are performed on three benchmarks: V-COCO, HICO-DET and HCVRD. Our approach outperforms the state-of-the-art on all datasets. On the V-COCO dataset, our method achieves a relative gain of 4.4% in terms of role mean average precision ($mAP_{role}$), compared to the existing best approach.



### Deformable Kernel Networks for Joint Image Filtering
- **Arxiv ID**: http://arxiv.org/abs/1910.08373v3
- **DOI**: 10.1007/s11263-020-01386-z
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08373v3)
- **Published**: 2019-10-17 06:55:40+00:00
- **Updated**: 2020-10-21 01:55:17+00:00
- **Authors**: Beomjun Kim, Jean Ponce, Bumsub Ham
- **Comment**: International Journal of Computer Vision (2020). arXiv admin note:
  substantial text overlap with arXiv:1903.11286
- **Journal**: None
- **Summary**: Joint image filters are used to transfer structural details from a guidance picture used as a prior to a target image, in tasks such as enhancing spatial resolution and suppressing noise. Previous methods based on convolutional neural networks (CNNs) combine nonlinear activations of spatially-invariant kernels to estimate structural details and regress the filtering result. In this paper, we instead learn explicitly sparse and spatially-variant kernels. We propose a CNN architecture and its efficient implementation, called the deformable kernel network (DKN), that outputs sets of neighbors and the corresponding weights adaptively for each pixel. The filtering result is then computed as a weighted average. We also propose a fast version of DKN that runs about seventeen times faster for an image of size 640 x 480. We demonstrate the effectiveness and flexibility of our models on the tasks of depth map upsampling, saliency map upsampling, cross-modality image restoration, texture removal, and semantic segmentation. In particular, we show that the weighted averaging process with sparsely sampled 3 x 3 kernels outperforms the state of the art by a significant margin in all cases.



### Detecting intracranial aneurysm rupture from 3D surfaces using a novel GraphNet approach
- **Arxiv ID**: http://arxiv.org/abs/1910.08375v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08375v1)
- **Published**: 2019-10-17 07:15:41+00:00
- **Updated**: 2019-10-17 07:15:41+00:00
- **Authors**: Z. Ma, L. Song, X. Feng, G. Yang, W. Zhu, J. Liu, Y. Zhang, X. Yang, Y. Yin
- **Comment**: Submitted to ISBI 2020
- **Journal**: None
- **Summary**: Intracranial aneurysm (IA) is a life-threatening blood spot in human's brain if it ruptures and causes cerebral hemorrhage. It is challenging to detect whether an IA has ruptured from medical images. In this paper, we propose a novel graph based neural network named GraphNet to detect IA rupture from 3D surface data. GraphNet is based on graph convolution network (GCN) and is designed for graph-level classification and node-level segmentation. The network uses GCN blocks to extract surface local features and pools to global features. 1250 patient data including 385 ruptured and 865 unruptured IAs were collected from clinic for experiments. The performance on randomly selected 234 test patient data was reported. The experiment with the proposed GraphNet achieved accuracy of 0.82, area-under-curve (AUC) of receiver operating characteristic (ROC) curve 0.82 in the classification task, significantly outperforming the baseline approach without using graph based networks. The segmentation output of the model achieved mean graph-node-based dice coefficient (DSC) score 0.88.



### Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale Denoising Score Matching
- **Arxiv ID**: http://arxiv.org/abs/1910.07762v2
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07762v2)
- **Published**: 2019-10-17 08:21:17+00:00
- **Updated**: 2019-12-20 04:58:04+00:00
- **Authors**: Zengyi Li, Yubei Chen, Friedrich T. Sommer
- **Comment**: None
- **Journal**: None
- **Summary**: Energy-Based Models (EBMs) assign unnormalized log-probability to data samples. This functionality has a variety of applications, such as sample synthesis, data denoising, sample restoration, outlier detection, Bayesian reasoning, and many more. But training of EBMs using standard maximum likelihood is extremely slow because it requires sampling from the model distribution. Score matching potentially alleviates this problem. In particular, denoising score matching \citep{vincent2011connection} has been successfully used to train EBMs. Using noisy data samples with one fixed noise level, these models learn fast and yield good results in data denoising \citep{saremi2019neural}. However, demonstrations of such models in high quality sample synthesis of high dimensional data were lacking. Recently, \citet{song2019generative} have shown that a generative model trained by denoising score matching accomplishes excellent sample synthesis, when trained with data samples corrupted with multiple levels of noise. Here we provide analysis and empirical evidence showing that training with multiple noise levels is necessary when the data dimension is high. Leveraging this insight, we propose a novel EBM trained with multi-scale denoising score matching. Our model exhibits data generation performance comparable to state-of-the-art techniques such as GANs, and sets a new baseline for EBMs. The proposed model also provides density information and performs well in an image inpainting task.



### Making Third Person Techniques Recognize First-Person Actions in Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.07766v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07766v1)
- **Published**: 2019-10-17 08:26:23+00:00
- **Updated**: 2019-10-17 08:26:23+00:00
- **Authors**: Sagar Verma, Pravin Nagar, Divam Gupta, Chetan Arora
- **Comment**: 5 pages, ICIP2018,
  code:https://github.com/sagarverma/ego_action_recognition
- **Journal**: None
- **Summary**: We focus on first-person action recognition from egocentric videos. Unlike third person domain, researchers have divided first-person actions into two categories: involving hand-object interactions and the ones without, and developed separate techniques for the two action categories. Further, it has been argued that traditional cues used for third person action recognition do not suffice, and egocentric specific features, such as head motion and handled objects have been used for such actions. Unlike the state-of-the-art approaches, we show that a regular two stream Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) architecture, having separate streams for objects and motion, can generalize to all categories of first-person actions. The proposed approach unifies the feature learned by all action categories, making the proposed architecture much more practical. In an important observation, we note that the size of the objects visible in the egocentric videos is much smaller. We show that the performance of the proposed model improves after cropping and resizing frames to make the size of objects comparable to the size of ImageNet's objects. Our experiments on the standard datasets: GTEA, EGTEA Gaze+, HUJI, ADL, UTE, and Kitchen, proves that our model significantly outperforms various state-of-the-art techniques.



### On the Risk of Cancelable Biometrics
- **Arxiv ID**: http://arxiv.org/abs/1910.07770v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07770v4)
- **Published**: 2019-10-17 08:37:59+00:00
- **Updated**: 2022-09-29 04:24:07+00:00
- **Authors**: Xingbo Dong, Jaewoo Park, Zhe Jin, Andrew Beng Jin Teoh, Massimo Tistarelli, KokSheik Wong
- **Comment**: None
- **Journal**: None
- **Summary**: Cancelable biometrics (CB) employs an irreversible transformation to convert the biometric features into transformed templates while preserving the relative distance between two templates for security and privacy protection. However, distance preservation invites unexpected security issues such as pre-image attacks, which are often neglected.This paper presents a generalized pre-image attack method and its extension version that operates on practical CB systems. We theoretically reveal that distance preservation property is a vulnerability source in the CB schemes. We then propose an empirical information leakage estimation algorithm to access the pre-image attack risk of the CB schemes. The experiments conducted with six CB schemes designed for the face, iris and fingerprint, demonstrate that the risks originating from the distance computed from two transformed templates significantly compromise the security of CB schemes. Our work reveals the potential risk of existing CB systems theoretically and experimentally.



### Detecting Urban Changes with Recurrent Neural Networks from Multitemporal Sentinel-2 Data
- **Arxiv ID**: http://arxiv.org/abs/1910.07778v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07778v1)
- **Published**: 2019-10-17 09:15:43+00:00
- **Updated**: 2019-10-17 09:15:43+00:00
- **Authors**: Maria Papadomanolaki, Sagar Verma, Maria Vakalopoulou, Siddharth Gupta, Konstantinos Karantzalos
- **Comment**: 4 pages, IGARSS2019
- **Journal**: None
- **Summary**: \begin{abstract} The advent of multitemporal high resolution data, like the Copernicus Sentinel-2, has enhanced significantly the potential of monitoring the earth's surface and environmental dynamics. In this paper, we present a novel deep learning framework for urban change detection which combines state-of-the-art fully convolutional networks (similar to U-Net) for feature representation and powerful recurrent networks (such as LSTMs) for temporal modeling. We report our results on the recently publicly available bi-temporal Onera Satellite Change Detection (OSCD) Sentinel-2 dataset, enhancing the temporal information with additional images of the same region on different dates. Moreover, we evaluate the performance of the recurrent networks as well as the use of the additional dates on the unseen test-set using an ensemble cross-validation strategy. All the developed models during the validation phase have scored an overall accuracy of more than 95%, while the use of LSTMs and further temporal information, boost the F1 rate of the change class by an additional 1.5%.



### NAMF: A Non-local Adaptive Mean Filter for Salt-and-Pepper Noise Removal
- **Arxiv ID**: http://arxiv.org/abs/1910.07787v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07787v2)
- **Published**: 2019-10-17 09:31:07+00:00
- **Updated**: 2020-07-27 15:48:33+00:00
- **Authors**: Houwang Zhang, Yuan Zhu, Hanying Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, a novel algorithm called a non-local adaptive mean filter (NAMF) for removing salt-and-pepper (SAP) noise from corrupted images is presented. We employ an efficient window detector with adaptive size to detect the noise, the noisy pixel will be replaced by the combination of its neighboring pixels, and finally we use a SAP noise based non-local mean filter to reconstruct the intensity values of noisy pixels. Extensive experimental results demonstrate that NAMF can obtain better performance in terms of quality for restoring images at all levels of SAP noise.



### Organ At Risk Segmentation with Multiple Modality
- **Arxiv ID**: http://arxiv.org/abs/1910.07800v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07800v1)
- **Published**: 2019-10-17 10:01:11+00:00
- **Updated**: 2019-10-17 10:01:11+00:00
- **Authors**: Kuan-Lun Tseng, Winston Hsu, Chun-ting Wu, Ya-Fang Shih, Fan-Yun Sun
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of image segmentation in computer vision, biomedical image segmentation have achieved remarkable progress on brain tumor segmentation and Organ At Risk (OAR) segmentation. However, most of the research only uses single modality such as Computed Tomography (CT) scans while in real world scenario doctors often use multiple modalities to get more accurate result. To better leverage different modalities, we have collected a large dataset consists of 136 cases with CT and MR images which diagnosed with nasopharyngeal cancer. In this paper, we propose to use Generative Adversarial Network to perform CT to MR transformation to synthesize MR images instead of aligning two modalities. The synthesized MR can be jointly trained with CT to achieve better performance. In addition, we use instance segmentation model to extend the OAR segmentation task to segment both organs and tumor region. The collected dataset will be made public soon.



### Introducing Hann windows for reducing edge-effects in patch-based image segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.07831v1
- **DOI**: 10.1371/journal.pone.0229839
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07831v1)
- **Published**: 2019-10-17 11:39:16+00:00
- **Updated**: 2019-10-17 11:39:16+00:00
- **Authors**: Nicolas Pielawski, Carolina Wählby
- **Comment**: None
- **Journal**: None
- **Summary**: There is a limitation in the size of an image that can be processed using computationally demanding methods such as e.g. Convolutional Neural Networks (CNNs). Some imaging modalities - notably biological and medical - can result in images up to a few gigapixels in size, meaning that they have to be divided into smaller parts, or patches, for processing. However, when performing image segmentation, this may lead to undesirable artefacts, such as edge effects in the final re-combined image. We introduce windowing methods from signal processing to effectively reduce such edge effects. With the assumption that the central part of an image patch often holds richer contextual information than its sides and corners, we reconstruct the prediction by overlapping patches that are being weighted depending on 2-dimensional windows. We compare the results of four different windows: Hann, Bartlett-Hann, Triangular and a recently proposed window by Cui et al., and show that the cosine-based Hann window achieves the best improvement as measured by the Structural Similarity Index (SSIM). The proposed windowing method can be used together with any CNN model for segmentation without any modification and significantly improves network predictions.



### Can I teach a robot to replicate a line art
- **Arxiv ID**: http://arxiv.org/abs/1910.07860v1
- **DOI**: 10.1109/WACV45572.2020.9093434
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07860v1)
- **Published**: 2019-10-17 12:40:15+00:00
- **Updated**: 2019-10-17 12:40:15+00:00
- **Authors**: Raghav Brahmadesam Venkataramaiyer, Subham Kumar, Vinay P. Namboodiri
- **Comment**: 9 pages, Accepted for the 2020 Winter Conference on Applications of
  Computer Vision (WACV '20); Supplementary Video: https://youtu.be/nMt5Dw04XhY
- **Journal**: None
- **Summary**: Line art is arguably one of the fundamental and versatile modes of expression. We propose a pipeline for a robot to look at a grayscale line art and redraw it. The key novel elements of our pipeline are: a) we propose a novel task of mimicking line drawings, b) to solve the pipeline we modify the Quick-draw dataset to obtain supervised training for converting a line drawing into a series of strokes c) we propose a multi-stage segmentation and graph interpretation pipeline for solving the problem. The resultant method has also been deployed on a CNC plotter as well as a robotic arm. We have trained several variations of the proposed methods and evaluate these on a dataset obtained from Quick-draw. Through the best methods we observe an accuracy of around 98% for this task, which is a significant improvement over the baseline architecture we adapted from. This therefore allows for deployment of the method on robots for replicating line art in a reliable manner. We also show that while the rule-based vectorization methods do suffice for simple drawings, it fails for more complicated sketches, unlike our method which generalizes well to more complicated distributions.



### Go with the Flow: Perception-refined Physics Simulation
- **Arxiv ID**: http://arxiv.org/abs/1910.07861v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07861v1)
- **Published**: 2019-10-17 12:41:34+00:00
- **Updated**: 2019-10-17 12:41:34+00:00
- **Authors**: Tom F. H. Runia, Kirill Gavrilyuk, Cees G. M. Snoek, Arnold W. M. Smeulders
- **Comment**: None
- **Journal**: None
- **Summary**: For many of the physical phenomena around us, we have developed sophisticated models explaining their behavior. Nevertheless, inferring specifics from visual observations is challenging due to the high number of causally underlying physical parameters -- including material properties and external forces. This paper addresses the problem of inferring such latent physical properties from observations. Our solution is an iterative refinement procedure with simulation at its core. The algorithm gradually updates the physical model parameters by running a simulation of the observed phenomenon and comparing the current simulation to a real-world observation. The physical similarity is computed using an embedding function that maps physically similar examples to nearby points. As a tangible example, we concentrate on flags curling in the wind -- a seemingly simple phenomenon but physically highly involved. Based on its underlying physical model and visual manifestation, we propose an instantiation of the embedding function. For this mapping, modeled as a deep network, we introduce a spectral decomposition layer that decomposes a video volume into its temporal spectral power and corresponding frequencies. In experiments, we demonstrate our method's ability to recover intrinsic and extrinsic physical parameters from both simulated and real-world video.



### A New Three-stage Curriculum Learning Approach to Deep Network Based Liver Tumor Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.07895v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07895v1)
- **Published**: 2019-10-17 13:29:42+00:00
- **Updated**: 2019-10-17 13:29:42+00:00
- **Authors**: Huiyu Li, Xiabi Liu, Said Boumaraf, Weihua Liu, Xiaopeng Gong, Xiaohong Ma
- **Comment**: 5 pages, 3 figures, 1 table, conference
- **Journal**: None
- **Summary**: Automatic segmentation of liver tumors in medical images is crucial for the computer-aided diagnosis and therapy. It is a challenging task, since the tumors are notoriously small against the background voxels. This paper proposes a new three-stage curriculum learning approach for training deep networks to tackle this small object segmentation problem. The learning in the first stage is performed on the whole input to obtain an initial deep network for tumor segmenta-tion. Then the second stage of learning focuses the strength-ening of tumor specific features by continuing training the network on the tumor patches. Finally, we retrain the net-work on the whole input in the third stage, in order that the tumor specific features and the global context can be inte-grated ideally under the segmentation objective. Benefitting from the proposed learning approach, we only need to em-ploy one single network to segment the tumors directly. We evaluated our approach on the 2017 MICCAI Liver Tumor Segmentation challenge dataset. In the experiments, our approach exhibits significant improvement compared with the commonly used cascaded counterpart.



### Vatex Video Captioning Challenge 2020: Multi-View Features and Hybrid Reward Strategies for Video Captioning
- **Arxiv ID**: http://arxiv.org/abs/1910.11102v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.11102v4)
- **Published**: 2019-10-17 13:52:49+00:00
- **Updated**: 2020-06-24 03:42:09+00:00
- **Authors**: Xinxin Zhu, Longteng Guo, Peng Yao, Shichen Lu, Wei Liu, Jing Liu
- **Comment**: 4 pages,2 figure
- **Journal**: None
- **Summary**: This report describes our solution for the VATEX Captioning Challenge 2020, which requires generating descriptions for the videos in both English and Chinese languages. We identified three crucial factors that improve the performance, namely: multi-view features, hybrid reward, and diverse ensemble. Based on our method of VATEX 2019 challenge, we achieved significant improvements this year with more advanced model architectures, combination of appearance and motion features, and careful hyper-parameters tuning. Our method achieves very competitive results on both of the Chinese and English video captioning tracks.



### Self-supervised 3D Shape and Viewpoint Estimation from Single Images for Robotics
- **Arxiv ID**: http://arxiv.org/abs/1910.07948v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07948v1)
- **Published**: 2019-10-17 14:55:21+00:00
- **Updated**: 2019-10-17 14:55:21+00:00
- **Authors**: Oier Mees, Maxim Tatarchenko, Thomas Brox, Wolfram Burgard
- **Comment**: Accepted at the 2019 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS). Video at
  https://www.youtube.com/watch?v=oQgHG9JdMP4
- **Journal**: None
- **Summary**: We present a convolutional neural network for joint 3D shape prediction and viewpoint estimation from a single input image. During training, our network gets the learning signal from a silhouette of an object in the input image - a form of self-supervision. It does not require ground truth data for 3D shapes and the viewpoints. Because it relies on such a weak form of supervision, our approach can easily be applied to real-world data. We demonstrate that our method produces reasonable qualitative and quantitative results on natural images for both shape estimation and viewpoint prediction. Unlike previous approaches, our method does not require multiple views of the same object instance in the dataset, which significantly expands the applicability in practical robotics scenarios. We showcase it by using the hallucinated shapes to improve the performance on the task of grasping real-world objects both in simulation and with a PR2 robot.



### Convolutional Character Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.07954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07954v1)
- **Published**: 2019-10-17 15:01:00+00:00
- **Updated**: 2019-10-17 15:01:00+00:00
- **Authors**: Linjie Xing, Zhi Tian, Weilin Huang, Matthew R. Scott
- **Comment**: To appear in ICCV 2019
- **Journal**: None
- **Summary**: Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33%->71.08% (with generic lexicon) on ICDAR 2015, and 54.0%->69.23% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet.



### Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control
- **Arxiv ID**: http://arxiv.org/abs/1910.07972v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07972v3)
- **Published**: 2019-10-17 15:33:03+00:00
- **Updated**: 2020-07-08 15:44:10+00:00
- **Authors**: Lukas Hermann, Max Argus, Andreas Eitel, Artemij Amiranashvili, Wolfram Burgard, Thomas Brox
- **Comment**: Accepted at the 2020 IEEE International Conference on Robotics and
  Automation (ICRA). Project page see
  https://lmb.informatik.uni-freiburg.de/projects/curriculum/
- **Journal**: None
- **Summary**: We propose Adaptive Curriculum Generation from Demonstrations (ACGD) for reinforcement learning in the presence of sparse rewards. Rather than designing shaped reward functions, ACGD adaptively sets the appropriate task difficulty for the learner by controlling where to sample from the demonstration trajectories and which set of simulation parameters to use. We show that training vision-based control policies in simulation while gradually increasing the difficulty of the task via ACGD improves the policy transfer to the real world. The degree of domain randomization is also gradually increased through the task difficulty. We demonstrate zero-shot transfer for two real-world manipulation tasks: pick-and-stow and block stacking. A video showing the results can be found at https://lmb.informatik.uni-freiburg.de/projects/curriculum/



### Discrete Residual Flow for Probabilistic Pedestrian Behavior Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.08041v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.08041v1)
- **Published**: 2019-10-17 17:10:28+00:00
- **Updated**: 2019-10-17 17:10:28+00:00
- **Authors**: Ajay Jain, Sergio Casas, Renjie Liao, Yuwen Xiong, Song Feng, Sean Segal, Raquel Urtasun
- **Comment**: CoRL 2019
- **Journal**: None
- **Summary**: Self-driving vehicles plan around both static and dynamic objects, applying predictive models of behavior to estimate future locations of the objects in the environment. However, future behavior is inherently uncertain, and models of motion that produce deterministic outputs are limited to short timescales. Particularly difficult is the prediction of human behavior. In this work, we propose the discrete residual flow network (DRF-Net), a convolutional neural network for human motion prediction that captures the uncertainty inherent in long-range motion forecasting. In particular, our learned network effectively captures multimodal posteriors over future human motion by predicting and updating a discretized distribution over spatial locations. We compare our model against several strong competitors and show that our model outperforms all baselines.



### Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets
- **Arxiv ID**: http://arxiv.org/abs/1910.08051v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08051v1)
- **Published**: 2019-10-17 17:24:22+00:00
- **Updated**: 2019-10-17 17:24:22+00:00
- **Authors**: Yogesh Balaji, Tom Goldstein, Judy Hoffman
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial training is by far the most successful strategy for improving robustness of neural networks to adversarial attacks. Despite its success as a defense mechanism, adversarial training fails to generalize well to unperturbed test set. We hypothesize that this poor generalization is a consequence of adversarial training with uniform perturbation radius around every training sample. Samples close to decision boundary can be morphed into a different class under a small perturbation budget, and enforcing large margins around these samples produce poor decision boundaries that generalize poorly. Motivated by this hypothesis, we propose instance adaptive adversarial training -- a technique that enforces sample-specific perturbation margins around every training sample. We show that using our approach, test accuracy on unperturbed samples improve with a marginal drop in robustness. Extensive experiments on CIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our proposed approach.



### Video Person Re-Identification using Learned Clip Similarity Aggregation
- **Arxiv ID**: http://arxiv.org/abs/1910.08055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08055v1)
- **Published**: 2019-10-17 17:34:27+00:00
- **Updated**: 2019-10-17 17:34:27+00:00
- **Authors**: Neeraj Matiyali, Gaurav Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.



### Meta-learning for fast classifier adaptation to new users of Signature Verification systems
- **Arxiv ID**: http://arxiv.org/abs/1910.08060v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08060v1)
- **Published**: 2019-10-17 17:46:03+00:00
- **Updated**: 2019-10-17 17:46:03+00:00
- **Authors**: Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira
- **Comment**: Accepted for the IEEE Transactions on Information Forensics and
  Security
- **Journal**: None
- **Summary**: Offline Handwritten Signature verification presents a challenging Pattern Recognition problem, where only knowledge of the positive class is available for training. While classifiers have access to a few genuine signatures for training, during generalization they also need to discriminate forgeries. This is particularly challenging for skilled forgeries, where a forger practices imitating the user's signature, and often is able to create forgeries visually close to the original signatures. Most work in the literature address this issue by training for a surrogate objective: discriminating genuine signatures of a user and random forgeries (signatures from other users). In this work, we propose a solution for this problem based on meta-learning, where there are two levels of learning: a task-level (where a task is to learn a classifier for a given user) and a meta-level (learning across tasks). In particular, the meta-learner guides the adaptation (learning) of a classifier for each user, which is a lightweight operation that only requires genuine signatures. The meta-learning procedure learns what is common for the classification across different users. In a scenario where skilled forgeries from a subset of users are available, the meta-learner can guide classifiers to be discriminative of skilled forgeries even if the classifiers themselves do not use skilled forgeries for learning. Experiments conducted on the GPDS-960 dataset show improved performance compared to Writer-Independent systems, and achieve results comparable to state-of-the-art Writer-Dependent systems in the regime of few samples per user (5 reference signatures).



### Context-Aware Saliency Detection for Image Retargeting Using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.08071v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.08071v1)
- **Published**: 2019-10-17 17:59:46+00:00
- **Updated**: 2019-10-17 17:59:46+00:00
- **Authors**: Mahdi Ahmadi, Nader Karimi, Shadrokh Samavi
- **Comment**: 20 pages, 19 figures
- **Journal**: None
- **Summary**: Image retargeting is the task of making images capable of being displayed on screens with different sizes. This work should be done so that high-level visual information and low-level features such as texture remain as intact as possible to the human visual system, while the output image may have different dimensions. Thus, simple methods such as scaling and cropping are not adequate for this purpose. In recent years, researchers have tried to improve the existing retargeting methods and introduce new ones. However, a specific method cannot be utilized to retarget all types of images. In other words, different images require different retargeting methods. Image retargeting has a close relationship to image saliency detection, which is relatively a new image processing task. Earlier saliency detection methods were based on local and global but low-level image information. These methods are called bottom-up methods. On the other hand, newer approaches are top-down and mixed methods that consider the high level and semantic information of the image too. In this paper, we introduce the proposed methods in both saliency detection and retargeting. For the saliency detection, the use of image context and semantic segmentation are examined, and a novel mixed bottom-up, and top-down saliency detection method is introduced. After saliency detection, a modified version of an existing retargeting method is utilized for retargeting the images. The results suggest that the proposed image retargeting pipeline has excellent performance compared to other tested methods. Also, the subjective evaluations on the Pascal dataset can be used as a retargeting quality assessment dataset for further research.



### Mapper Based Classifier
- **Arxiv ID**: http://arxiv.org/abs/1910.08103v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08103v2)
- **Published**: 2019-10-17 18:28:01+00:00
- **Updated**: 2019-10-21 15:32:25+00:00
- **Authors**: Jacek Cyranka, Alexander Georges, David Meyer
- **Comment**: 12 pages, accepted to IEEE ICMLA 2019
- **Journal**: None
- **Summary**: Topological data analysis aims to extract topological quantities from data, which tend to focus on the broader global structure of the data rather than local information. The Mapper method, specifically, generalizes clustering methods to identify significant global mathematical structures, which are out of reach of many other approaches. We propose a classifier based on applying the Mapper algorithm to data projected onto a latent space. We obtain the latent space by using PCA or autoencoders. Notably, a classifier based on the Mapper method is immune to any gradient based attack, and improves robustness over traditional CNNs (convolutional neural networks). We report theoretical justification and some numerical experiments that confirm our claims.



### Enforcing Linearity in DNN succours Robustness and Adversarial Image Generation
- **Arxiv ID**: http://arxiv.org/abs/1910.08108v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08108v2)
- **Published**: 2019-10-17 18:38:40+00:00
- **Updated**: 2019-10-21 17:00:50+00:00
- **Authors**: Anindya Sarkar, Nikhil Kumar Gupta, Raghu Iyengar
- **Comment**: Adversarial Machine Learning
- **Journal**: None
- **Summary**: Recent studies on the adversarial vulnerability of neural networks have shown that models trained with the objective of minimizing an upper bound on the worst-case loss over all possible adversarial perturbations improve robustness against adversarial attacks. Beside exploiting adversarial training framework, we show that by enforcing a Deep Neural Network (DNN) to be linear in transformed input and feature space improves robustness significantly. We also demonstrate that by augmenting the objective function with Local Lipschitz regularizer boost robustness of the model further. Our method outperforms most sophisticated adversarial training methods and achieves state of the art adversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also propose a novel adversarial image generation method by leveraging Inverse Representation Learning and Linearity aspect of an adversarially trained deep neural network classifier.



### A Dataset of Multi-Illumination Images in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1910.08131v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08131v1)
- **Published**: 2019-10-17 19:49:11+00:00
- **Updated**: 2019-10-17 19:49:11+00:00
- **Authors**: Lukas Murmann, Michael Gharbi, Miika Aittala, Fredo Durand
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Collections of images under a single, uncontrolled illumination have enabled the rapid advancement of core computer vision tasks like classification, detection, and segmentation. But even with modern learning techniques, many inverse problems involving lighting and material understanding remain too severely ill-posed to be solved with single-illumination datasets. To fill this gap, we introduce a new multi-illumination dataset of more than 1000 real scenes, each captured under 25 lighting conditions. We demonstrate the richness of this dataset by training state-of-the-art models for three challenging applications: single-image illumination estimation, image relighting, and mixed-illuminant white balance.



### RPBA -- Robust Parallel Bundle Adjustment Based on Covariance Information
- **Arxiv ID**: http://arxiv.org/abs/1910.08138v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08138v1)
- **Published**: 2019-10-17 20:00:32+00:00
- **Updated**: 2019-10-17 20:00:32+00:00
- **Authors**: Helmut Mayer
- **Comment**: None
- **Journal**: None
- **Summary**: A core component of all Structure from Motion (SfM) approaches is bundle adjustment. As the latter is a computational bottleneck for larger blocks, parallel bundle adjustment has become an active area of research. Particularly, consensus-based optimization methods have been shown to be suitable for this task. We have extended them using covariance information derived by the adjustment of individual three-dimensional (3D) points, i.e., "triangulation" or "intersection". This does not only lead to a much better convergence behavior, but also avoids fiddling with the penalty parameter of standard consensus-based approaches. The corresponding novel approach can also be seen as a variant of resection / intersection schemes, where we adjust during intersection a number of sub-blocks directly related to the number of threads available on a computer each containing a fraction of the cameras of the block. We show that our novel approach is suitable for robust parallel bundle adjustment and demonstrate its capabilities in comparison to the basic consensus-based approach as well as a state-of-the-art parallel implementation of bundle adjustment. Code for our novel approach is available on GitHub: https://github.com/helmayer/RPBA



### Deep Sub-Ensembles for Fast Uncertainty Estimation in Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.08168v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.08168v2)
- **Published**: 2019-10-17 21:07:40+00:00
- **Updated**: 2019-11-29 12:23:45+00:00
- **Authors**: Matias Valdenegro-Toro
- **Comment**: 7 pages, 8 figures, Bayesian Deep Learning Workshop 2019 @ NeurIPS
  2019, camera ready
- **Journal**: None
- **Summary**: Fast estimates of model uncertainty are required for many robust robotics applications. Deep Ensembles provides state of the art uncertainty without requiring Bayesian methods, but still it is computationally expensive. In this paper we propose deep sub-ensembles, an approximation to deep ensembles where the core idea is to ensemble only the layers close to the output, and not the whole model. With ResNet-20 on the CIFAR10 dataset, we obtain 1.5-2.5 speedup over a Deep Ensemble, with a small increase in error and NLL, and similarly up to 5-15 speedup with a VGG-like network on the SVHN dataset. Our results show that this idea enables a trade-off between error and uncertainty quality versus computational performance.



### Deep Weakly-Supervised Domain Adaptation for Pain Localization in Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.08173v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08173v2)
- **Published**: 2019-10-17 21:32:06+00:00
- **Updated**: 2020-03-09 17:40:42+00:00
- **Authors**: Gnana Praveen R, Eric Granger, Patrick Cardinal
- **Comment**: Accepted in FG 2020
- **Journal**: None
- **Summary**: Automatic pain assessment has an important potential diagnostic value for populations that are incapable of articulating their pain experiences. As one of the dominating nonverbal channels for eliciting pain expression events, facial expressions has been widely investigated for estimating the pain intensity of individual. However, using state-of-the-art deep learning (DL) models in real-world pain estimation applications poses several challenges related to the subjective variations of facial expressions, operational capture conditions, and lack of representative training videos with labels. Given the cost of annotating intensity levels for every video frame, we propose a weakly-supervised domain adaptation (WSDA) technique that allows for training 3D CNNs for spatio-temporal pain intensity estimation using weakly labeled videos, where labels are provided on a periodic basis. In particular, WSDA integrates multiple instance learning into an adversarial deep domain adaptation framework to train an Inflated 3D-CNN (I3D) model such that it can accurately estimate pain intensities in the target operational domain. The training process relies on weak target loss, along with domain loss and source loss for domain adaptation of the I3D model. Experimental results obtained using labeled source domain RECOLA videos and weakly-labeled target domain UNBC-McMaster videos indicate that the proposed deep WSDA approach can achieve significantly higher level of sequence (bag)-level and frame (instance)-level pain localization accuracy than related state-of-the-art approaches.



### LanCe: A Comprehensive and Lightweight CNN Defense Methodology against Physical Adversarial Attacks on Embedded Multimedia Applications
- **Arxiv ID**: http://arxiv.org/abs/1910.08536v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.08536v1)
- **Published**: 2019-10-17 21:38:11+00:00
- **Updated**: 2019-10-17 21:38:11+00:00
- **Authors**: Zirui Xu, Fuxun Yu, Xiang Chen
- **Comment**: 6 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1905.08790
- **Journal**: None
- **Summary**: Recently, adversarial attacks can be applied to the physical world, causing practical issues to various Convolutional Neural Networks (CNNs) powered applications. Most existing physical adversarial attack defense works only focus on eliminating explicit perturbation patterns from inputs, ignoring interpretation to CNN's intrinsic vulnerability. Therefore, they lack the expected versatility to different attacks and thereby depend on considerable data processing costs. In this paper, we propose LanCe -- a comprehensive and lightweight CNN defense methodology against different physical adversarial attacks. By interpreting CNN's vulnerability, we find that non-semantic adversarial perturbations can activate CNN with significantly abnormal activations and even overwhelm other semantic input patterns' activations. We improve the CNN recognition process by adding a self-verification stage to detect the potential adversarial input with only one CNN inference cost. Based on the detection result, we further propose a data recovery methodology to defend the physical adversarial attacks. We apply such defense methodology into both image and audio CNN recognition scenarios and analyze the computational complexity for each scenario, respectively. Experiments show that our methodology can achieve an average 91% successful rate for attack detection and 89% accuracy recovery. Moreover, it is at most 3x faster compared with the state-of-the-art defense methods, making it feasible to resource-constrained embedded systems, such as mobile devices.



