# Arxiv Papers in cs.CV on 2019-10-05
### Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely Annotated 3D CT Images
- **Arxiv ID**: http://arxiv.org/abs/1910.02175v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02175v3)
- **Published**: 2019-10-05 00:01:28+00:00
- **Updated**: 2019-10-21 20:56:02+00:00
- **Authors**: Deepta Rajan, David Beymer, Shafiqul Abedin, Ehsan Dehghan
- **Comment**: 2019 NeurIPS ML4H (Proceedings of Machine Learning Research)
- **Journal**: None
- **Summary**: Pulmonary embolisms (PE) are known to be one of the leading causes for cardiac-related mortality. Due to inherent variabilities in how PE manifests and the cumbersome nature of manual diagnosis, there is growing interest in leveraging AI tools for detecting PE. In this paper, we build a two-stage detection pipeline that is accurate, computationally efficient, robust to variations in PE types and kernels used for CT reconstruction, and most importantly, does not require dense annotations. Given the challenges in acquiring expert annotations in large-scale datasets, our approach produces state-of-the-art results with very sparse emboli contours (at 10mm slice spacing), while using models with significantly lower number of parameters. We achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of highly severe PEs. Using a large, real-world dataset characterized by complex PE types and patients from multiple hospitals, we present an elaborate empirical study and provide guidelines for designing highly generalizable pipelines.



### To React or not to React: End-to-End Visual Pose Forecasting for Personalized Avatar during Dyadic Conversations
- **Arxiv ID**: http://arxiv.org/abs/1910.02181v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1910.02181v1)
- **Published**: 2019-10-05 00:19:36+00:00
- **Updated**: 2019-10-05 00:19:36+00:00
- **Authors**: Chaitanya Ahuja, Shugao Ma, Louis-Philippe Morency, Yaser Sheikh
- **Comment**: None
- **Journal**: None
- **Summary**: Non verbal behaviours such as gestures, facial expressions, body posture, and para-linguistic cues have been shown to complement or clarify verbal messages. Hence to improve telepresence, in form of an avatar, it is important to model these behaviours, especially in dyadic interactions. Creating such personalized avatars not only requires to model intrapersonal dynamics between a avatar's speech and their body pose, but it also needs to model interpersonal dynamics with the interlocutor present in the conversation. In this paper, we introduce a neural architecture named Dyadic Residual-Attention Model (DRAM), which integrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using selective attention to generate sequences of body pose conditioned on audio and body pose of the interlocutor and audio of the human operating the avatar. We evaluate our proposed model on dyadic conversational data consisting of pose and audio of both participants, confirming the importance of adaptive attention between monadic and dyadic dynamics when predicting avatar pose. We also conduct a user study to analyze judgments of human observers. Our results confirm that the generated body pose is more natural, models intrapersonal dynamics and interpersonal dynamics better than non-adaptive monadic/dyadic models.



### Kornia: an Open Source Differentiable Computer Vision Library for PyTorch
- **Arxiv ID**: http://arxiv.org/abs/1910.02190v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02190v2)
- **Published**: 2019-10-05 01:29:54+00:00
- **Updated**: 2019-10-09 11:55:48+00:00
- **Authors**: Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, Gary Bradski
- **Comment**: Updated adversarial attack example
- **Journal**: None
- **Summary**: This work presents Kornia -- an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.



### A Paired Sparse Representation Model for Robust Face Recognition from a Single Sample
- **Arxiv ID**: http://arxiv.org/abs/1910.02192v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02192v1)
- **Published**: 2019-10-05 01:58:45+00:00
- **Updated**: 2019-10-05 01:58:45+00:00
- **Authors**: Fania Mokhayeri, Eric Granger
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intra-class variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations, non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model, reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem. Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person.



### Early Estimation of User's Intention of Tele-Operation Using Object Affordance and Hand Motion in a Dual First-Person Vision
- **Arxiv ID**: http://arxiv.org/abs/1910.02201v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02201v1)
- **Published**: 2019-10-05 03:21:54+00:00
- **Updated**: 2019-10-05 03:21:54+00:00
- **Authors**: Motoki Kojima, Jun Miura
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes a method of estimating the intention of a user's motion in a robot tele-operation scenario. One of the issues in tele-operation is latency, which occurs due to various reasons such as a slow robot motion and a narrow communication channel. An effective way of reducing the latency is to estimate the human intention of motions and to move the robot proactively. To enable a reliable early intention estimation, we use both hand motion and object affordances in a dual first-person vision (robot and user) with an HMD. Experimental results in an object pickup scenario show the effectiveness of the method.



### Dilated Convolutional Neural Networks for Sequential Manifold-valued Data
- **Arxiv ID**: http://arxiv.org/abs/1910.02206v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02206v1)
- **Published**: 2019-10-05 04:09:37+00:00
- **Updated**: 2019-10-05 04:09:37+00:00
- **Authors**: Xingjian Zhen, Rudrasis Chakraborty, Nicholas Vogt, Barbara B. Bendlin, Vikas Singh
- **Comment**: None
- **Journal**: ICCV 2019
- **Summary**: Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and the algorithms. Motivated by neuroimaging applications, we study formulations where the data are {\em sequential manifold-valued measurements}. This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Fr\'{e}chet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy.



### Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.02212v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02212v1)
- **Published**: 2019-10-05 05:29:03+00:00
- **Updated**: 2019-10-05 05:29:03+00:00
- **Authors**: Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng Wang, Qi Tian
- **Comment**: submitted to IEEE-TPAMI
- **Journal**: None
- **Summary**: 3D skeleton-based action recognition and motion prediction are two essential problems of human activity understanding. In many previous works: 1) they studied two tasks separately, neglecting internal correlations; 2) they did not capture sufficient relations inside the body. To address these issues, we propose a symbiotic model to handle two tasks jointly; and we propose two scales of graphs to explicitly capture relations among body-joints and body-parts. Together, we propose symbiotic graph neural networks, which contain a backbone, an action-recognition head, and a motion-prediction head. Two heads are trained jointly and enhance each other. For the backbone, we propose multi-branch multi-scale graph convolution networks to extract spatial and temporal features. The multi-scale graph convolution networks are based on joint-scale and part-scale graphs. The joint-scale graphs contain actional graphs, capturing action-based relations, and structural graphs, capturing physical constraints. The part-scale graphs integrate body-joints to form specific parts, representing high-level relations. Moreover, dual bone-based graphs and networks are proposed to learn complementary features. We conduct extensive experiments for skeleton-based action recognition and motion prediction with four datasets, NTU-RGB+D, Kinetics, Human3.6M, and CMU Mocap. Experiments show that our symbiotic graph neural networks achieve better performances on both tasks compared to the state-of-the-art methods.



### Colored Transparent Object Matting from a Single Image Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.02222v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02222v1)
- **Published**: 2019-10-05 06:45:33+00:00
- **Updated**: 2019-10-05 06:45:33+00:00
- **Authors**: Jamal Ahmed Rahim, Kwan-Yee Kenneth Wong
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a deep learning based method for colored transparent object matting from a single image. Existing approaches for transparent object matting often require multiple images and long processing times, which greatly hinder their applications on real-world transparent objects. The recently proposed TOM-Net can produce a matte for a colorless transparent object from a single image in a single fast feed-forward pass. In this paper, we extend TOM-Net to handle colored transparent object by modeling the intrinsic color of a transparent object with a color filter. We formulate the problem of colored transparent object matting as simultaneously estimating an object mask, a color filter, and a refractive flow field from a single image, and present a deep learning framework for learning this task. We create a large-scale synthetic dataset for training our network. We also capture a real dataset for evaluation. Experiments on both synthetic and real datasets show promising results, which demonstrate the effectiveness of our method.



### Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.02224v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02224v1)
- **Published**: 2019-10-05 07:07:53+00:00
- **Updated**: 2019-10-05 07:07:53+00:00
- **Authors**: Limeng Qiao, Yemin Shi, Jia Li, Yaowei Wang, Tiejun Huang, Yonghong Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot learning, which aims at extracting new concepts rapidly from extremely few examples of novel classes, has been featured into the meta-learning paradigm recently. Yet, the key challenge of how to learn a generalizable classifier with the capability of adapting to specific tasks with severely limited data still remains in this domain. To this end, we propose a Transductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot learning, by integrating the meta-learning paradigm with both deep metric learning and transductive inference. With exploring the pairwise constraints and regularization prior within each task, we explicitly formulate the adaptation procedure into a standard semi-definite programming problem. By solving the problem with its closed-form solution on the fly with the setup of transduction, our approach efficiently tailors an episodic-wise metric for each task to adapt all features from a shared task-agnostic embedding space into a more discriminative task-specific metric space. Moreover, we further leverage an attention-based bi-directional similarity strategy for extracting the more robust relationship between queries and prototypes. Extensive experiments on three benchmark datasets show that our framework is superior to other existing approaches and achieves the state-of-the-art performance in the few-shot literature.



### Cascaded Volumetric Convolutional Network for Kidney Tumor Segmentation from CT volumes
- **Arxiv ID**: http://arxiv.org/abs/1910.02235v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02235v2)
- **Published**: 2019-10-05 08:44:25+00:00
- **Updated**: 2020-05-04 09:22:58+00:00
- **Authors**: Yao Zhang, Yixin Wang, Feng Hou, Jiawei Yang, Guangwei Xiong, Jiang Tian, Cheng Zhong
- **Comment**: None
- **Journal**: None
- **Summary**: Automated segmentation of kidney and tumor from 3D CT scans is necessary for the diagnosis, monitoring, and treatment planning of the disease. In this paper, we describe a two-stage framework for kidney and tumor segmentation based on 3D fully convolutional network (FCN). The first stage preliminarily locate the kidney and cut off the irrelevant background to reduce class imbalance and computation cost. Then the second stage precisely segment the kidney and tumor on the cropped patch. The proposed method ranks the 4th place out of 105 competitive teams in MICCAI 2019 KiTS Challenge with a Composite Dice of 90.24%.



### Self-supervised Feature Learning for 3D Medical Images by Playing a Rubik's Cube
- **Arxiv ID**: http://arxiv.org/abs/1910.02241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02241v1)
- **Published**: 2019-10-05 10:02:13+00:00
- **Updated**: 2019-10-05 10:02:13+00:00
- **Authors**: Xinrui Zhuang, Yuexiang Li, Yifan Hu, Kai Ma, Yujiu Yang, Yefeng Zheng
- **Comment**: MICCAI 2019
- **Journal**: None
- **Summary**: Witnessed the development of deep learning, increasing number of studies try to build computer aided diagnosis systems for 3D volumetric medical data. However, as the annotations of 3D medical data are difficult to acquire, the number of annotated 3D medical images is often not enough to well train the deep learning networks. The self-supervised learning deeply exploiting the information of raw data is one of the potential solutions to loose the requirement of training data. In this paper, we propose a self-supervised learning framework for the volumetric medical images. A novel proxy task, i.e., Rubik's cube recovery, is formulated to pre-train 3D neural networks. The proxy task involves two operations, i.e., cube rearrangement and cube rotation, which enforce networks to learn translational and rotational invariant features from raw 3D data. Compared to the train-from-scratch strategy, fine-tuning from the pre-trained network leads to a better accuracy on various tasks, e.g., brain hemorrhage classification and brain tumor segmentation. We show that our self-supervised learning approach can substantially boost the accuracies of 3D deep learning networks on the volumetric medical datasets without using extra data. To our best knowledge, this is the first work focusing on the self-supervised learning of 3D neural networks.



### Yet another but more efficient black-box adversarial attack: tiling and evolution strategies
- **Arxiv ID**: http://arxiv.org/abs/1910.02244v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.02244v2)
- **Published**: 2019-10-05 10:36:47+00:00
- **Updated**: 2019-11-21 10:48:51+00:00
- **Authors**: Laurent Meunier, Jamal Atif, Olivier Teytaud
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\ell_\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario. Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\%$ of success rate against InceptionV3 classifier with $630$ queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.



### Object Segmentation Tracking from Generic Video Cues
- **Arxiv ID**: http://arxiv.org/abs/1910.02258v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02258v3)
- **Published**: 2019-10-05 12:47:23+00:00
- **Updated**: 2020-08-14 17:22:19+00:00
- **Authors**: Amirhossein Kardoost, Sabine Müller, Joachim Weickert, Margret Keuper
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a light-weight variational framework for online tracking of object segmentations in videos based on optical flow and image boundaries. While high-end computer vision methods on this task rely on sequence specific training of dedicated CNN architectures, we show the potential of a variational model, based on generic video information from motion and color. Such cues are usually required for tasks such as robot navigation or grasp estimation. We leverage them directly for video object segmentation and thus provide accurate segmentations at potentially very low extra cost. Our simple method can provide competitive results compared to the costly CNN-based methods with parameter tuning. Furthermore, we show that our approach can be combined with state-of-the-art CNN-based segmentations in order to improve over their respective results. We evaluate our method on the datasets DAVIS 16,17 and SegTrack v2.



### A Deep Learning System That Generates Quantitative CT Reports for Diagnosing Pulmonary Tuberculosis
- **Arxiv ID**: http://arxiv.org/abs/1910.02285v1
- **DOI**: 10.1007/s10489-020-02051-1
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02285v1)
- **Published**: 2019-10-05 15:55:25+00:00
- **Updated**: 2019-10-05 15:55:25+00:00
- **Authors**: Wei Wu, Xukun Li, Peng Du, Guanjing Lang, Min Xu, Kaijin Xu, Lanjuan Li
- **Comment**: None
- **Journal**: None
- **Summary**: We developed a deep learning model-based system to automatically generate a quantitative Computed Tomography (CT) diagnostic report for Pulmonary Tuberculosis (PTB) cases.501 CT imaging datasets from 223 patients with active PTB were collected, and another 501 cases from a healthy population served as negative samples.2884 lesions of PTB were carefully labeled and classified manually by professional radiologists.Three state-of-the-art 3D convolution neural network (CNN) models were trained and evaluated in the inspection of PTB CT images. Transfer learning method was also utilized during this process. The best model was selected to annotate the spatial location of lesions and classify them into miliary, infiltrative, caseous, tuberculoma and cavitary types simultaneously.Then the Noisy-Or Bayesian function was used to generate an overall infection probability.Finally, a quantitative diagnostic report was exported.The results showed that the recall and precision rates, from the perspective of a single lesion region of PTB, were 85.9% and 89.2% respectively. The overall recall and precision rates,from the perspective of one PTB case, were 98.7% and 93.7%, respectively. Moreover, the precision rate of the PTB lesion type classification was 90.9%.The new method might serve as an effective reference for decision making by clinical doctors.



### ExpertMatcher: Automating ML Model Selection for Users in Resource Constrained Countries
- **Arxiv ID**: http://arxiv.org/abs/1910.02312v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC
- **Links**: [PDF](http://arxiv.org/pdf/1910.02312v1)
- **Published**: 2019-10-05 18:58:59+00:00
- **Updated**: 2019-10-05 18:58:59+00:00
- **Authors**: Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang, Jayashree Kalpathy-Cramer, Ramesh Raskar
- **Comment**: In NeurIPS Workshop on Machine learning for the Developing World
  (ML4D)
- **Journal**: None
- **Summary**: In this work we introduce ExpertMatcher, a method for automating deep learning model selection using autoencoders. Specifically, we are interested in performing inference on data sources that are distributed across many clients using pretrained expert ML networks on a centralized server. The ExpertMatcher assigns the most relevant model(s) in the central server given the client's data representation. This allows resource-constrained clients in developing countries to utilize the most relevant ML models for their given task without having to evaluate the performance of each ML model. The method is generic and can be beneficial in any setup where there are local clients and numerous centralized expert ML models.



### Covariance-free Partial Least Squares: An Incremental Dimensionality Reduction Method
- **Arxiv ID**: http://arxiv.org/abs/1910.02319v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02319v2)
- **Published**: 2019-10-05 19:45:50+00:00
- **Updated**: 2020-11-10 14:23:13+00:00
- **Authors**: Artur Jordao, Maiko Lie, Victor Hugo Cunha de Melo, William Robson Schwartz
- **Comment**: Accepted for publication at Winter Conference on Applications of
  Computer Vision (WACV) 2021
- **Journal**: None
- **Summary**: Dimensionality reduction plays an important role in computer vision problems since it reduces computational cost and is often capable of yielding more discriminative data representation. In this context, Partial Least Squares (PLS) has presented notable results in tasks such as image classification and neural network optimization. However, PLS is infeasible on large datasets, such as ImageNet, because it requires all the data to be in memory in advance, which is often impractical due to hardware limitations. Additionally, this requirement prevents us from employing PLS on streaming applications where the data are being continuously generated. Motivated by this, we propose a novel incremental PLS, named Covariance-free Incremental Partial Least Squares (CIPLS), which learns a low-dimensional representation of the data using a single sample at a time. In contrast to other state-of-the-art approaches, instead of adopting a partially-discriminative or SGD-based model, we extend Nonlinear Iterative Partial Least Squares (NIPALS) -- the standard algorithm used to compute PLS -- for incremental processing. Among the advantages of this approach are the preservation of discriminative information across all components, the possibility of employing its score matrices for feature selection, and its computational efficiency. We validate CIPLS on face verification and image classification tasks, where it outperforms several other incremental dimensionality reduction techniques. In the context of feature selection, CIPLS achieves comparable results when compared to state-of-the-art techniques.



### Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation
- **Arxiv ID**: http://arxiv.org/abs/1910.02334v1
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02334v1)
- **Published**: 2019-10-05 22:05:43+00:00
- **Updated**: 2019-10-05 22:05:43+00:00
- **Authors**: Benet Oriol Sabat, Cristian Canton Ferrer, Xavier Giro-i-Nieto
- **Comment**: AI for Social Good Workshop at NeurIPS 2019 (short paper)
- **Journal**: None
- **Summary**: This work addresses the challenge of hate speech detection in Internet memes, and attempts using visual information to automatically detect hate speech, unlike any previous work of our knowledge. Memes are pixel-based multimedia documents that contain photos or illustrations together with phrases which, when combined, usually adopt a funny meaning. However, hate memes are also used to spread hate through social networks, so their automatic detection would help reduce their harmful societal impact. Our results indicate that the model can learn to detect some of the memes, but that the task is far from being solved with this simple architecture. While previous work focuses on linguistic hate speech, our experiments indicate how the visual modality can be much more informative for hate speech detection than the linguistic one in memes. In our experiments, we built a dataset of 5,020 memes to train and evaluate a multi-layer perceptron over the visual and language representations, whether independently or fused. The source code and mode and models are available https://github.com/imatge-upc/hate-speech-detection .



