# Arxiv Papers in cs.CV on 2019-10-06
### AdvSPADE: Realistic Unrestricted Attacks for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.02354v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02354v3)
- **Published**: 2019-10-06 02:16:18+00:00
- **Updated**: 2019-11-19 02:38:18+00:00
- **Authors**: Guangyu Shen, Chengzhi Mao, Junfeng Yang, Baishakhi Ray
- **Comment**: None
- **Journal**: None
- **Summary**: Due to the inherent robustness of segmentation models, traditional norm-bounded attack methods show limited effect on such type of models. In this paper, we focus on generating unrestricted adversarial examples for semantic segmentation models. We demonstrate a simple and effective method to generate unrestricted adversarial examples using conditional generative adversarial networks (CGAN) without any hand-crafted metric. The na\"ive implementation of CGAN, however, yields inferior image quality and low attack success rate. Instead, we leverage the SPADE (Spatially-adaptive denormalization) structure with an additional loss item to generate effective adversarial attacks in a single step. We validate our approach on the popular Cityscapes and ADE20K datasets, and demonstrate that our synthetic adversarial examples are not only realistic, but also improve the attack success rate by up to 41.0\% compared with the state of the art adversarial attack methods including PGD.



### Transforming the output of GANs by fine-tuning them with features from different datasets
- **Arxiv ID**: http://arxiv.org/abs/1910.02411v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02411v1)
- **Published**: 2019-10-06 10:29:11+00:00
- **Updated**: 2019-10-06 10:29:11+00:00
- **Authors**: Terence Broad, Mick Grierson
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we present a method for fine-tuning pre-trained GANs with features from different datasets, resulting in the transformation of the output distribution into a new distribution with novel characteristics. The weights of the generator are updated using the weighted sum of the losses from a cross-dataset classifier and the frozen weights of the pre-trained discriminator. We discuss details of the technical implementation and share some of the visual results from this training process.



### Deep learning-based development of personalized human head model with non-uniform conductivity for brain stimulation
- **Arxiv ID**: http://arxiv.org/abs/1910.02420v2
- **DOI**: 10.1109/TMI.2020.2969682
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02420v2)
- **Published**: 2019-10-06 11:33:13+00:00
- **Updated**: 2020-01-23 06:00:57+00:00
- **Authors**: Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata
- **Comment**: None
- **Journal**: IEEE Transactions on Medical Imaging, 2020
- **Summary**: Electromagnetic stimulation of the human brain is a key tool for the neurophysiological characterization and diagnosis of several neurological disorders. Transcranial magnetic stimulation (TMS) is one procedure that is commonly used clinically. However, personalized TMS requires a pipeline for accurate head model generation to provide target-specific stimulation. This process includes intensive segmentation of several head tissues based on magnetic resonance imaging (MRI), which has significant potential for segmentation error, especially for low-contrast tissues. Additionally, a uniform electrical conductivity is assigned to each tissue in the model, which is an unrealistic assumption based on conventional volume conductor modeling. This paper proposes a novel approach to the automatic estimation of electric conductivity in the human head for volume conductor models without anatomical segmentation. A convolutional neural network is designed to estimate personalized electrical conductivity values based on anatomical information obtained from T1- and T2-weighted MRI scans. This approach can avoid the time-consuming process of tissue segmentation and maximize the advantages of position-dependent conductivity assignment based on water content values estimated from MRI intensity values. The computational results of the proposed approach provide similar but smoother electric field results for the brain when compared to conventional approaches.



### Structured Object-Aware Physics Prediction for Video Modeling and Planning
- **Arxiv ID**: http://arxiv.org/abs/1910.02425v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02425v2)
- **Published**: 2019-10-06 11:48:26+00:00
- **Updated**: 2020-02-12 09:38:20+00:00
- **Authors**: Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting
- **Comment**: Published as a conference paper at 2020 International Conference for
  Learning Representations
- **Journal**: None
- **Summary**: When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects.



### Weighted Clustering Ensemble: A Review
- **Arxiv ID**: http://arxiv.org/abs/1910.02433v2
- **DOI**: 10.1016/j.patcog.2021.108428
- **Categories**: **cs.CV**, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02433v2)
- **Published**: 2019-10-06 12:16:29+00:00
- **Updated**: 2021-11-23 15:14:19+00:00
- **Authors**: Mimi Zhang
- **Comment**: None
- **Journal**: Pattern Recognition, 2022
- **Summary**: Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods. Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.



### Joint Stereo Video Deblurring, Scene Flow Estimation and Moving Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.02442v1
- **DOI**: 10.1109/TIP.2019.2945867
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02442v1)
- **Published**: 2019-10-06 12:55:47+00:00
- **Updated**: 2019-10-06 12:55:47+00:00
- **Authors**: Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli, Quan Pan
- **Comment**: Accepted by IEEE Transactions on Image Processing 2019. arXiv admin
  note: text overlap with arXiv:1704.03273
- **Journal**: None
- **Summary**: Stereo videos for the dynamic scenes often show unpleasant blurred effects due to the camera motion and the multiple moving objects with large depth variations. Given consecutive blurred stereo video frames, we aim to recover the latent clean images, estimate the 3D scene flow and segment the multiple moving objects. These three tasks have been previously addressed separately, which fail to exploit the internal connections among these tasks and cannot achieve optimality. In this paper, we propose to jointly solve these three tasks in a unified framework by exploiting their intrinsic connections. To this end, we represent the dynamic scenes with the piece-wise planar model, which exploits the local structure of the scene and expresses various dynamic scenes. Under our model, these three tasks are naturally connected and expressed as the parameter estimation of 3D scene structure and camera motion (structure and motion for the dynamic scenes). By exploiting the blur model constraint, the moving objects and the 3D scene structure, we reach an energy minimization formulation for joint deblurring, scene flow and segmentation. We evaluate our approach extensively on both synthetic datasets and publicly available real datasets with fast-moving objects, camera motion, uncontrolled lighting conditions and shadows. Experimental results demonstrate that our method can achieve significant improvement in stereo video deblurring, scene flow estimation and moving object segmentation, over state-of-the-art methods.



### Enhanced Human-Machine Interaction by Combining Proximity Sensing with Global Perception
- **Arxiv ID**: http://arxiv.org/abs/1910.02445v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02445v3)
- **Published**: 2019-10-06 13:17:57+00:00
- **Updated**: 2019-10-16 05:49:43+00:00
- **Authors**: Christoph Heindl, Markus Ikeda, Gernot St√ºbl, Andreas Pichler, Josef Scharinger
- **Comment**: IROS 2019 / 2nd Workshop on Proximity Perception
- **Journal**: None
- **Summary**: The raise of collaborative robotics has led to wide range of sensor technologies to detect human-machine interactions: at short distances, proximity sensors detect nontactile gestures virtually occlusion-free, while at medium distances, active depth sensors are frequently used to infer human intentions. We describe an optical system for large workspaces to capture human pose based on a single panoramic color camera. Despite the two-dimensional input, our system is able to predict metric 3D pose information over larger field of views than would be possible with active depth measurement cameras. We merge posture context with proximity perception to reduce occlusions and improve accuracy at long distances. We demonstrate the capabilities of our system in two use cases involving multiple humans and robots.



### Fully Convolutional Networks for Chip-wise Defect Detection Employing Photoluminescence Images
- **Arxiv ID**: http://arxiv.org/abs/1910.02451v1
- **DOI**: 10.1007/s10845-020-01563-4
- **Categories**: **eess.IV**, cs.CV, J.2; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1910.02451v1)
- **Published**: 2019-10-06 13:49:03+00:00
- **Updated**: 2019-10-06 13:49:03+00:00
- **Authors**: Maike Lorena Stern, Martin Schellenberger
- **Comment**: 14 pages, 12 figures
- **Journal**: J Intell Manuf (2020) 1-14
- **Summary**: Efficient quality control is inevitable in the manufacturing of light-emitting diodes (LEDs). Because defective LED chips may be traced back to different causes, a time and cost-intensive electrical and optical contact measurement is employed. Fast photoluminescence measurements, on the other hand, are commonly used to detect wafer separation damages but also hold the potential to enable an efficient detection of all kinds of defective LED chips. On a photoluminescence image, every pixel corresponds to an LED chip's brightness after photoexcitation, revealing performance information. But due to unevenly distributed brightness values and varying defect patterns, photoluminescence images are not yet employed for a comprehensive defect detection. In this work, we show that fully convolutional networks can be used for chip-wise defect detection, trained on a small data-set of photoluminescence images. Pixel-wise labels allow us to classify each and every chip as defective or not. Being measurement-based, labels are easy to procure and our experiments show that existing discrepancies between training images and labels do not hinder network training. Using weighted loss calculation, we were able to equalize our highly unbalanced class categories. Due to the consistent use of skip connections and residual shortcuts, our network is able to predict a variety of structures, from extensive defect clusters up to single defective LED chips.



### Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping
- **Arxiv ID**: http://arxiv.org/abs/1910.02490v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02490v3)
- **Published**: 2019-10-06 18:26:25+00:00
- **Updated**: 2020-03-04 04:53:48+00:00
- **Authors**: Antoni Rosinol, Marcus Abate, Yun Chang, Luca Carlone
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.



### REMIND Your Neural Network to Prevent Catastrophic Forgetting
- **Arxiv ID**: http://arxiv.org/abs/1910.02509v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.02509v3)
- **Published**: 2019-10-06 19:48:23+00:00
- **Updated**: 2020-07-13 17:10:44+00:00
- **Authors**: Tyler L. Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, Christopher Kanan
- **Comment**: To appear in the European Conference on Computer Vision (ECCV-2020)
- **Journal**: None
- **Summary**: People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA).



### Joint Image and Depth Estimation with Mask-Based Lensless Cameras
- **Arxiv ID**: http://arxiv.org/abs/1910.02526v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02526v2)
- **Published**: 2019-10-06 21:12:07+00:00
- **Updated**: 2020-06-19 04:15:51+00:00
- **Authors**: Yucheng Zheng, M. Salman Asif
- **Comment**: Added new experiments with camera prototype
- **Journal**: None
- **Summary**: Mask-based lensless cameras replace the lens of a conventional camera with a custom mask. These cameras can potentially be very thin and even flexible. Recently, it has been demonstrated that such mask-based cameras can recover light intensity and depth information of a scene. Existing depth recovery algorithms either assume that the scene consists of a small number of depth planes or solve a sparse recovery problem over a large 3D volume. Both these approaches fail to recover the scenes with large depth variations. In this paper, we propose a new approach for depth estimation based on an alternating gradient descent algorithm that jointly estimates a continuous depth map and light distribution of the unknown scene from its lensless measurements. We present simulation results on image and depth reconstruction for a variety of 3D test scenes. A comparison between the proposed algorithm and other method shows that our algorithm is more robust for natural scenes with a large range of depths. We built a prototype lensless camera and present experimental results for reconstruction of intensity and depth maps of different real objects.



### 3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera
- **Arxiv ID**: http://arxiv.org/abs/1910.02527v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02527v1)
- **Published**: 2019-10-06 21:13:02+00:00
- **Updated**: 2019-10-06 21:13:02+00:00
- **Authors**: Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R. Zamir, Martin Fischer, Jitendra Malik, Silvio Savarese
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, texture, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, and other attributes), rooms (e.g., scene category, volume, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities.   However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations.



### Improving One-shot NAS by Suppressing the Posterior Fading
- **Arxiv ID**: http://arxiv.org/abs/1910.02543v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02543v1)
- **Published**: 2019-10-06 22:31:37+00:00
- **Updated**: 2019-10-06 22:31:37+00:00
- **Authors**: Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie Yan, Wanli Ouyang
- **Comment**: None
- **Journal**: None
- **Summary**: There is a growing interest in automated neural architecture search (NAS). To improve the efficiency of NAS, previous approaches adopt weight sharing method to force all models share the same set of weights. However, it has been observed that a model performing better with shared weights does not necessarily perform better when trained alone. In this paper, we analyse existing weight sharing one-shot NAS approaches from a Bayesian point of view and identify the posterior fading problem, which compromises the effectiveness of shared weights. To alleviate this problem, we present a practical approach to guide the parameter posterior towards its true distribution. Moreover, a hard latency constraint is introduced during the search so that the desired latency can be achieved. The resulted method, namely Posterior Convergent NAS (PC-NAS), achieves state-of-the-art performance under standard GPU latency constraint on ImageNet. In our small search space, our model PC-NAS-S attains 76.8 % top-1 accuracy, 2.1% higher than MobileNetV2 (1.4x) with the same latency. When adopted to the large search space, PC-NAS-L achieves 78.1 % top-1 accuracy within 11ms. The discovered architecture also transfers well to other computer vision applications such as object detection and person re-identification.



### ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1910.02550v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02550v2)
- **Published**: 2019-10-06 23:22:56+00:00
- **Updated**: 2019-10-14 17:29:36+00:00
- **Authors**: Shreeyak S. Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy Zeng, Shuran Song
- **Comment**: Project Website: https://sites.google.com/view/cleargrasp, 13 pages,
  13 figures, submitted to ICRA
- **Journal**: None
- **Summary**: Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp -- a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials available on the project website: https://sites.google.com/view/cleargrasp



