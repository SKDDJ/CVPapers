# Arxiv Papers in cs.CV on 2019-10-21
### Semantics for Global and Local Interpretation of Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.09085v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09085v1)
- **Published**: 2019-10-21 00:00:17+00:00
- **Updated**: 2019-10-21 00:00:17+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: Deep neural networks (DNNs) with high expressiveness have achieved state-of-the-art performance in many tasks. However, their distributed feature representations are difficult to interpret semantically. In this work, human-interpretable semantic concepts are associated with vectors in feature space. The association process is mathematically formulated as an optimization problem. The semantic vectors obtained from the optimal solution are applied to interpret deep neural networks globally and locally. The global interpretations are useful to understand the knowledge learned by DNNs. The interpretation of local behaviors can help to understand individual decisions made by DNNs better. The empirical experiments demonstrate how to use identified semantics to interpret the existing DNNs.



### Contextual Prediction Difference Analysis for Explaining Individual Image Classifications
- **Arxiv ID**: http://arxiv.org/abs/1910.09086v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09086v2)
- **Published**: 2019-10-21 00:04:22+00:00
- **Updated**: 2020-06-06 00:41:19+00:00
- **Authors**: Jindong Gu, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: Much effort has been devoted to understanding the decisions of deep neural networks in recent years. A number of model-aware saliency methods were proposed to explain individual classification decisions by creating saliency maps. However, they are not applicable when the parameters and the gradients of the underlying models are unavailable. Recently, model-agnostic methods have also received attention. As one of them, \textit{Prediction Difference Analysis} (PDA), a probabilistic sound methodology, was proposed. In this work, we first show that PDA can suffer from saturated classifiers. The saturation phenomenon of classifiers exists widely in current neural network-based classifiers. To explain the decisions of saturated classifiers better, we further propose Contextual PDA, which runs hundreds of times faster than PDA. The experiments show the superiority of our method by explaining image classifications of the state-of-the-art deep convolutional neural networks.



### A game method for improving the interpretability of convolution neural network
- **Arxiv ID**: http://arxiv.org/abs/1910.09090v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09090v1)
- **Published**: 2019-10-21 00:32:40+00:00
- **Updated**: 2019-10-21 00:32:40+00:00
- **Authors**: Jinwei Zhao, Qizhou Wang, Fuqiang Zhang, Wanli Qiu, Yufei Wang, Yu Liu, Guo Xie, Weigang Ma, Bin Wang, Xinhong Hei
- **Comment**: None
- **Journal**: None
- **Summary**: Real artificial intelligence always has been focused on by many machine learning researchers, especially in the area of deep learning. However deep neural network is hard to be understood and explained, and sometimes, even metaphysics. The reason is, we believe that: the network is essentially a perceptual model. Therefore, we believe that in order to complete complex intelligent activities from simple perception, it is necessary to con-struct another interpretable logical network to form accurate and reasonable responses and explanations to external things. Researchers like Bolei Zhou and Quanshi Zhang have found many explanatory rules for deep feature extraction aimed at the feature extraction stage of convolution neural network. However, although researchers like Marco Gori have also made great efforts to improve the interpretability of the fully connected layers of the network, the problem is also very difficult. This paper firstly analyzes its reason. Then a method of constructing logical network based on the fully connected layers and extracting logical relation between input and output of the layers is proposed. The game process between perceptual learning and logical abstract cognitive learning is implemented to improve the interpretable performance of deep learning process and deep learning model. The benefits of our approach are illustrated on benchmark data sets and in real-world experiments.



### Self-supervised classification of dynamic obstacles using the temporal information provided by videos
- **Arxiv ID**: http://arxiv.org/abs/1910.09094v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09094v2)
- **Published**: 2019-10-21 00:48:14+00:00
- **Updated**: 2020-06-07 18:41:56+00:00
- **Authors**: Sid Ali Hamideche, Florent Chiaroni, Mohamed-Cherif Rahal
- **Comment**: None
- **Journal**: None
- **Summary**: Nowadays, autonomous driving systems can detect, segment, and classify the surrounding obstacles using a monocular camera. However, state-of-the-art methods solving these tasks generally perform a fully supervised learning process and require a large amount of training labeled data. On another note, some self-supervised learning approaches can deal with detection and segmentation of dynamic obstacles using the temporal information available in video sequences. In this work, we propose to classify the detected obstacles depending on their motion pattern. We present a novel self-supervised framework consisting of learning offline clusters from temporal patch sequences and considering these clusters as labeled sets to train a real-time image classifier. The presented model outperforms state-of-the-art unsupervised image classification methods on large-scale diverse driving video dataset BDD100K.



### Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without Fully-Sampled Data
- **Arxiv ID**: http://arxiv.org/abs/1910.09116v1
- **DOI**: 10.1109/ISBI45749.2020.9098514
- **Categories**: **eess.IV**, cs.CV, cs.LG, eess.SP, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.09116v1)
- **Published**: 2019-10-21 02:20:15+00:00
- **Updated**: 2019-10-21 02:20:15+00:00
- **Authors**: Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Steen Moeller, Jutta Ellermann, Kâmil Uǧurbil, Mehmet Akçakaya
- **Comment**: 5 Pages, 5 Figures
- **Journal**: Proceedings of IEEE ISBI, 2020
- **Summary**: Deep learning (DL) has emerged as a tool for improving accelerated MRI reconstruction. A common strategy among DL methods is the physics-based approach, where a regularized iterative algorithm alternating between data consistency and a regularizer is unrolled for a finite number of iterations. This unrolled network is then trained end-to-end in a supervised manner, using fully-sampled data as ground truth for the network output. However, in a number of scenarios, it is difficult to obtain fully-sampled datasets, due to physiological constraints such as organ motion or physical constraints such as signal decay. In this work, we tackle this issue and propose a self-supervised learning strategy that enables physics-based DL reconstruction without fully-sampled data. Our approach is to divide the acquired sub-sampled points for each scan into training and validation subsets. During training, data consistency is enforced over the training subset, while the validation subset is used to define the loss function. Results show that the proposed self-supervised learning method successfully reconstructs images without fully-sampled data, performing similarly to the supervised approach that is trained with fully-sampled references. This has implications for physics-based inverse problem approaches for other settings, where fully-sampled data is not available or possible to acquire.



### Generative Hierarchical Models for Parts, Objects, and Scenes
- **Arxiv ID**: http://arxiv.org/abs/1910.09119v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09119v1)
- **Published**: 2019-10-21 02:28:16+00:00
- **Updated**: 2019-10-21 02:28:16+00:00
- **Authors**: Fei Deng, Zhuo Zhi, Sungjin Ahn
- **Comment**: None
- **Journal**: None
- **Summary**: Compositional structures between parts and objects are inherent in natural scenes. Modeling such compositional hierarchies via unsupervised learning can bring various benefits such as interpretability and transferability, which are important in many downstream tasks. In this paper, we propose the first deep latent variable model, called RICH, for learning Representation of Interpretable Compositional Hierarchies. At the core of RICH is a latent scene graph representation that organizes the entities of a scene into a tree structure according to their compositional relationships. During inference, taking top-down approach, RICH is able to use higher-level representation to guide lower-level decomposition. This avoids the difficult problem of routing between parts and objects that is faced by bottom-up approaches. In experiments on images containing multiple objects with different part compositions, we demonstrate that RICH is able to learn the latent compositional hierarchy and generate imaginary scenes.



### Enforcing Reasoning in Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1910.11124v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11124v2)
- **Published**: 2019-10-21 02:33:18+00:00
- **Updated**: 2019-12-27 10:09:58+00:00
- **Authors**: Hammad A. Ayyubi, Md. Mehrab Tanjim, David J. Kriegman
- **Comment**: None
- **Journal**: None
- **Summary**: The task of Visual Commonsense Reasoning is extremely challenging in the sense that the model has to not only be able to answer a question given an image, but also be able to learn to reason. The baselines introduced in this task are quite limiting because two networks are trained for predicting answers and rationales separately. Question and image is used as input to train answer prediction network while question, image and correct answer are used as input in the rationale prediction network. As rationale is conditioned on the correct answer, it is based on the assumption that we can solve Visual Question Answering task without any error - which is over ambitious. Moreover, such an approach makes both answer and rationale prediction two completely independent VQA tasks rendering cognition task meaningless. In this paper, we seek to address these issues by proposing an end-to-end trainable model which considers both answers and their reasons jointly. Specifically, we first predict the answer for the question and then use the chosen answer to predict the rationale. However, a trivial design of such a model becomes non-differentiable which makes it difficult to train. We solve this issue by proposing four approaches - softmax, gumbel-softmax, reinforcement learning based sampling and direct cross entropy against all pairs of answers and rationales. We demonstrate through experiments that our model performs competitively against current state-of-the-art. We conclude with an analysis of presented approaches and discuss avenues for further work.



### Perception-Distortion Trade-off with Restricted Boltzmann Machines
- **Arxiv ID**: http://arxiv.org/abs/1910.09122v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09122v1)
- **Published**: 2019-10-21 02:39:28+00:00
- **Updated**: 2019-10-21 02:39:28+00:00
- **Authors**: Chris Cannella, Jie Ding, Mohammadreza Soltani, Vahid Tarokh
- **Comment**: 5 pages, 1 figure
- **Journal**: None
- **Summary**: In this work, we introduce a new procedure for applying Restricted Boltzmann Machines (RBMs) to missing data inference tasks, based on linearization of the effective energy function governing the distribution of observations. We compare the performance of our proposed procedure with those obtained using existing reconstruction procedures trained on incomplete data. We place these performance comparisons within the context of the perception-distortion trade-off observed in other data reconstruction tasks, which has, until now, remained unexplored in tasks relying on incomplete training data.



### Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09134v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.09134v3)
- **Published**: 2019-10-21 03:32:17+00:00
- **Updated**: 2022-04-18 19:44:03+00:00
- **Authors**: Jiaying Lu, Xin Ye, Yi Ren, Yezhou Yang
- **Comment**: None
- **Journal**: CVPR'2022 Workshop on Open-Domain Retrieval Under a Multi-Modal
  Setting
- **Summary**: Multiple-choice VQA has drawn increasing attention from researchers and end-users recently. As the demand for automatically constructing large-scale multiple-choice VQA data grows, we introduce a novel task called textual Distractors Generation for VQA (DG-VQA) focusing on generating challenging yet meaningful distractors given the context image, question, and correct answer. The DG-VQA task aims at generating distractors without ground-truth training samples since such resources are rarely available. To tackle the DG-VQA unsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework that utilizes pre-trained VQA models as an alternative knowledge base to guide the distractor generation process. In Gobbet, a pre-trained VQA model serves as the environment in RL setting to provide feedback for the input multi-modal query, while a neural distractor generator serves as the agent to take actions accordingly. We propose to use existing VQA models' performance degradation as indicators of the quality of generated distractors. On the other hand, we show the utility of generated distractors through data augmentation experiments, since robustness is more and more important when AI models apply to unpredictable open-domain scenarios or security-sensitive applications. We further conduct a manual case study on the factors why distractors generated by Gobbet can fool existing models.



### DwNet: Dense warp-based network for pose-guided human video generation
- **Arxiv ID**: http://arxiv.org/abs/1910.09139v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09139v1)
- **Published**: 2019-10-21 03:56:51+00:00
- **Updated**: 2019-10-21 03:56:51+00:00
- **Authors**: Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, Leonid Sigal
- **Comment**: Accepted to BMVC 2019
- **Journal**: None
- **Summary**: Generation of realistic high-resolution videos of human subjects is a challenging and important task in computer vision. In this paper, we focus on human motion transfer - generation of a video depicting a particular subject, observed in a single image, performing a series of motions exemplified by an auxiliary (driving) video. Our GAN-based architecture, DwNet, leverages dense intermediate pose-guided representation and refinement process to warp the required subject appearance, in the form of the texture, from a source image into a desired pose. Temporal consistency is maintained by further conditioning the decoding process within a GAN on the previously generated frame. In this way a video is generated in an iterative and recurrent fashion. We illustrate the efficacy of our approach by showing state-of-the-art quantitative and qualitative performance on two benchmark datasets: TaiChi and Fashion Modeling. The latter is collected by us and will be made publicly available to the community.



### MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences
- **Arxiv ID**: http://arxiv.org/abs/1910.09165v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.09165v2)
- **Published**: 2019-10-21 06:31:48+00:00
- **Updated**: 2020-02-08 00:22:14+00:00
- **Authors**: Xingyu Liu, Mengyuan Yan, Jeannette Bohg
- **Comment**: ICCV 2019 (Oral)
- **Journal**: None
- **Summary**: Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called $MeteorNet$ for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.



### Mining GOLD Samples for Conditional GANs
- **Arxiv ID**: http://arxiv.org/abs/1910.09170v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09170v1)
- **Published**: 2019-10-21 06:49:32+00:00
- **Updated**: 2019-10-21 06:49:32+00:00
- **Authors**: Sangwoo Mo, Chiheon Kim, Sungwoong Kim, Minsu Cho, Jinwoo Shin
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Conditional generative adversarial networks (cGANs) have gained a considerable attention in recent years due to its class-wise controllability and superior quality for complex generation tasks. We introduce a simple yet effective approach to improving cGANs by measuring the discrepancy between the data distribution and the model distribution on given samples. The proposed measure, coined the gap of log-densities (GOLD), provides an effective self-diagnosis for cGANs while being efficienty computed from the discriminator. We propose three applications of the GOLD: example re-weighting, rejection sampling, and active learning, which improve the training, inference, and data selection of cGANs, respectively. Our experimental results demonstrate that the proposed methods outperform corresponding baselines for all three applications on different image datasets.



### Hadamard Codebook Based Deep Hashing
- **Arxiv ID**: http://arxiv.org/abs/1910.09182v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1910.09182v1)
- **Published**: 2019-10-21 07:33:42+00:00
- **Updated**: 2019-10-21 07:33:42+00:00
- **Authors**: Shen Chen, Liujuan Cao, Mingbao Lin, Yan Wang, Xiaoshuai Sun, Chenglin Wu, Jingfei Qiu, Rongrong Ji
- **Comment**: 8 pages, 7 figures, conference
- **Journal**: None
- **Summary**: As an approximate nearest neighbor search technique, hashing has been widely applied in large-scale image retrieval due to its excellent efficiency. Most supervised deep hashing methods have similar loss designs with embedding learning, while quantizing the continuous high-dim feature into compact binary space. We argue that the existing deep hashing schemes are defective in two issues that seriously affect the performance, i.e., bit independence and bit balance. The former refers to hash codes of different classes should be independent of each other, while the latter means each bit should have a balanced distribution of +1s and -1s. In this paper, we propose a novel supervised deep hashing method, termed Hadamard Codebook based Deep Hashing (HCDH), which solves the above two problems in a unified formulation. Specifically, we utilize an off-the-shelf algorithm to generate a binary Hadamard codebook to satisfy the requirement of bit independence and bit balance, which subsequently serves as the desired outputs of the hash functions learning. We also introduce a projection matrix to solve the inconsistency between the order of Hadamard matrix and the number of classes. Besides, the proposed HCDH further exploits the supervised labels by constructing a classifier on top of the outputs of hash functions. Extensive experiments demonstrate that HCDH can yield discriminative and balanced binary codes, which well outperforms many state-of-the-arts on three widely-used benchmarks.



### Exploring Simple and Transferable Recognition-Aware Image Processing
- **Arxiv ID**: http://arxiv.org/abs/1910.09185v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09185v4)
- **Published**: 2019-10-21 07:36:15+00:00
- **Updated**: 2022-09-10 23:28:03+00:00
- **Authors**: Zhuang Liu, Hung-Ju Wang, Tinghui Zhou, Zhiqiang Shen, Bingyi Kang, Evan Shelhamer, Trevor Darrell
- **Comment**: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI)
- **Journal**: None
- **Summary**: Recent progress in image recognition has stimulated the deployment of vision systems at an unprecedented scale. As a result, visual data are now often consumed not only by humans but also by machines. Existing image processing methods only optimize for better human perception, yet the resulting images may not be accurately recognized by machines. This can be undesirable, e.g., the images can be improperly handled by search engines or recommendation systems. In this work, we examine simple approaches to improve machine recognition of processed images: optimizing the recognition loss directly on the image processing network or through an intermediate input transformation model. Interestingly, the processing model's ability to enhance recognition quality can transfer when evaluated on models of different architectures, recognized categories, tasks and training datasets. This makes the methods applicable even when we do not have the knowledge of future recognition models, e.g., when uploading processed images to the Internet. We conduct experiments on multiple image processing tasks paired with ImageNet classification and PASCAL VOC detection as recognition tasks. With these simple yet effective methods, substantial accuracy gain can be achieved with strong transferability and minimal image quality loss. Through a user study we further show that the accuracy gain can transfer to a black-box cloud model. Finally, we try to explain this transferability phenomenon by demonstrating the similarities of different models' decision boundaries. Code is available at https://github.com/liuzhuang13/Transferable_RA .



### Attribute-aware Pedestrian Detection in a Crowd
- **Arxiv ID**: http://arxiv.org/abs/1910.09188v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09188v2)
- **Published**: 2019-10-21 07:44:26+00:00
- **Updated**: 2019-12-23 02:58:29+00:00
- **Authors**: Jialiang Zhang, Lixiang Lin, Yang Li, Yun-chen Chen, Jianke Zhu, Yao Hu, Steven C. H. Hoi
- **Comment**: None
- **Journal**: None
- **Summary**: Pedestrian detection is an initial step to perform outdoor scene analysis, which plays an essential role in many real-world applications. Although having enjoyed the merits of deep learning frameworks from the generic object detectors, pedestrian detection is still a very challenging task due to heavy occlusion and highly crowded group. Generally, the conventional detectors are unable to differentiate individuals from each other effectively under such a dense environment. To tackle this critical problem, we propose an attribute-aware pedestrian detector to explicitly model people's semantic attributes in a high-level feature detection fashion. Besides the typical semantic features, center position, target's scale and offset, we introduce a pedestrian-oriented attribute feature to encode the high-level semantic differences among the crowd. Moreover, a novel attribute-feature-based Non-Maximum Suppression~(NMS) is proposed to distinguish the person from a highly overlapped group by adaptively rejecting the false-positive results in a very crowd settings. Furthermore, a novel ground truth target is designed to alleviate the difficulties caused by the attribute configuration and extremely class imbalance issues during training. Finally, we evaluate our proposed attribute-aware pedestrian detector on two benchmark datasets including CityPersons and CrowdHuman. The experimental results show that our approach outperforms state-of-the-art methods at a large margin on pedestrian detection.



### Automatic Lumbar Spinal CT Image Segmentation with a Dual Densely Connected U-Net
- **Arxiv ID**: http://arxiv.org/abs/1910.09198v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09198v2)
- **Published**: 2019-10-21 08:20:51+00:00
- **Updated**: 2020-02-04 10:07:05+00:00
- **Authors**: He Tang, Xiaobing Pei, Shilong Huang, Xin Li, Chao Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The clinical treatment of degenerative and developmental lumbar spinal stenosis (LSS) is different. Computed tomography (CT) is helpful in distinguishing degenerative and developmental LSS due to its advantage in imaging of osseous and calcified tissues. However, boundaries of the vertebral body, spinal canal and dural sac have low contrast and hard to identify in a CT image, so the diagnosis depends heavily on the knowledge of expert surgeons and radiologists. In this paper, we develop an automatic lumbar spinal CT image segmentation method to assist LSS diagnosis. The main contributions of this paper are the following: 1) a new lumbar spinal CT image dataset is constructed that contains 2393 axial CT images collected from 279 patients, with the ground truth of pixel-level segmentation labels; 2) a dual densely connected U-shaped neural network (DDU-Net) is used to segment the spinal canal, dural sac and vertebral body in an end-to-end manner; 3) DDU-Net is capable of segmenting tissues with large scale-variant, inconspicuous edges (e.g., spinal canal) and extremely small size (e.g., dural sac); and 4) DDU-Net is practical, requiring no image preprocessing such as contrast enhancement, registration and denoising, and the running time reaches 12 FPS. In the experiment, we achieve state-of-the-art performance on the lumbar spinal image segmentation task. We expect that the technique will increase both radiology workflow efficiency and the perceived value of radiology reports for referring clinicians and patients.



### Analysis and a Solution of Momentarily Missed Detection for Anchor-based Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1910.09212v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09212v2)
- **Published**: 2019-10-21 08:57:37+00:00
- **Updated**: 2020-01-16 15:05:08+00:00
- **Authors**: Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani
- **Comment**: Accepted to WACV 2020, 9 pages
- **Journal**: None
- **Summary**: The employment of convolutional neural networks has led to significant performance improvement on the task of object detection. However, when applying existing detectors to continuous frames in a video, we often encounter momentary miss-detection of objects, that is, objects are undetected exceptionally at a few frames, although they are correctly detected at all other frames. In this paper, we analyze the mechanism of how such miss-detection occurs. For the most popular class of detectors that are based on anchor boxes, we show the followings: i) besides apparent causes such as motion blur, occlusions, background clutters, etc., the majority of remaining miss-detection can be explained by an improper behavior of the detectors at boundaries of the anchor boxes; and ii) this can be rectified by improving the way of choosing positive samples from candidate anchor boxes when training the detectors.



### Decoupling Representation and Classifier for Long-Tailed Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.09217v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09217v2)
- **Published**: 2019-10-21 09:03:19+00:00
- **Updated**: 2020-02-19 15:51:25+00:00
- **Authors**: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis
- **Comment**: None
- **Journal**: Published as a conference paper at ICLR 2020
- **Summary**: The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at https://github.com/facebookresearch/classifier-balancing.



### ipA-MedGAN: Inpainting of Arbitrary Regions in Medical Imaging
- **Arxiv ID**: http://arxiv.org/abs/1910.09230v2
- **DOI**: 10.1109/ICIP40778.2020.9191207
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09230v2)
- **Published**: 2019-10-21 09:34:13+00:00
- **Updated**: 2020-01-30 16:33:17+00:00
- **Authors**: Karim Armanious, Vijeth Kumar, Sherif Abdulatif, Tobias Hepp, Sergios Gatidis, Bin Yang
- **Comment**: Submitted to IEEE ICIP 2020
- **Journal**: None
- **Summary**: Local deformations in medical modalities are common phenomena due to a multitude of factors such as metallic implants or limited field of views in magnetic resonance imaging (MRI). Completion of the missing or distorted regions is of special interest for automatic image analysis frameworks to enhance post-processing tasks such as segmentation or classification. In this work, we propose a new generative framework for medical image inpainting, titled ipA-MedGAN. It bypasses the limitations of previous frameworks by enabling inpainting of arbitrary shaped regions without a prior localization of the regions of interest. Thorough qualitative and quantitative comparisons with other inpainting and translational approaches have illustrated the superior performance of the proposed framework for the task of brain MR inpainting.



### CNN based Extraction of Panels/Characters from Bengali Comic Book Page Images
- **Arxiv ID**: http://arxiv.org/abs/1910.09233v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09233v1)
- **Published**: 2019-10-21 09:37:46+00:00
- **Updated**: 2019-10-21 09:37:46+00:00
- **Authors**: Arpita Dutta, Samit Biswas
- **Comment**: 6 pages, 3 tables and 3 figures. Accepted at GREC 2019 in conjunction
  with ICDAR 2019
- **Journal**: None
- **Summary**: Peoples nowadays prefer to use digital gadgets like cameras or mobile phones for capturing documents. Automatic extraction of panels/characters from the images of a comic document is challenging due to the wide variety of drawing styles adopted by writers, beneficial for readers to read them on mobile devices at any time and useful for automatic digitization. Most of the methods for localization of panel/character rely on the connected component analysis or page background mask and are applicable only for a limited comic dataset. This work proposes a panel/character localization architecture based on the features of YOLO and CNN for extraction of both panels and characters from comic book images. The method achieved remarkable results on Bengali Comic Book Image dataset (BCBId) consisting of total $4130$ images, developed by us as well as on a variety of publicly available comic datasets in other languages, i.e. eBDtheque, Manga 109 and DCM dataset.



### Learning a Generic Adaptive Wavelet Shrinkage Function for Denoising
- **Arxiv ID**: http://arxiv.org/abs/1910.09234v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09234v3)
- **Published**: 2019-10-21 09:38:19+00:00
- **Updated**: 2020-04-14 08:30:31+00:00
- **Authors**: Tobias Alt, Joachim Weickert
- **Comment**: None
- **Journal**: None
- **Summary**: The rise of machine learning in image processing has created a gap between trainable data-driven and classical model-driven approaches: While learning-based models often show superior performance, classical ones are often more transparent. To reduce this gap, we introduce a generic wavelet shrinkage function for denoising which is adaptive to both the wavelet scales as well as the noise standard deviation. It is inferred from trained results of a tightly parametrised function which is inherited from nonlinear diffusion. Our proposed shrinkage function is smooth and compact while only using two parameters. In contrast to many existing shrinkage functions, it is able to enhance image structures by amplifying wavelet coefficients. Experiments show that it outperforms classical shrinkage functions by a significant margin.



### Batch Face Alignment using a Low-rank GAN
- **Arxiv ID**: http://arxiv.org/abs/1910.09244v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09244v1)
- **Published**: 2019-10-21 10:00:40+00:00
- **Updated**: 2019-10-21 10:00:40+00:00
- **Authors**: Jiabo Huang, Xiaohua Xie, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: This paper studies the problem of aligning a set of face images of the same individual into a normalized image while removing the outliers like partial occlusion, extreme facial expression as well as significant illumination variation. Our model seeks an optimal image domain transformation such that the matrix of misaligned images can be decomposed as the sum of a sparse matrix of noise and a rank-one matrix of aligned images. The image transformation is learned in an unsupervised manner, which means that ground-truth aligned images are unnecessary for our model. Specifically, we make use of the remarkable non-linear transforming ability of generative adversarial network(GAN) and guide it with low-rank generation as well as sparse noise constraint to achieve the face alignment. We verify the efficacy of the proposed model with extensive experiments on real-world face databases, demonstrating higher accuracy and efficiency than existing methods.



### MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09308v1
- **DOI**: 10.1186/s12880-020-00543-7
- **Categories**: **eess.IV**, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.09308v1)
- **Published**: 2019-10-21 12:43:25+00:00
- **Updated**: 2019-10-21 12:43:25+00:00
- **Authors**: Dominik Müller, Frank Kramer
- **Comment**: Open-source Python framework available on GitHub
  (https://github.com/frankkramer-lab/MIScnn) and PyPI (miscnn). 11 pages
- **Journal**: M\"uller, D., Kramer, F. MIScnn: a framework for medical image
  segmentation with convolutional neural networks and deep learning. BMC Med
  Imaging 21, 12 (2021)
- **Summary**: The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn. The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization. Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model. With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: https://github.com/frankkramer-lab/MIScnn.



### Directed-Weighting Group Lasso for Eltwise Blocked CNN Pruning
- **Arxiv ID**: http://arxiv.org/abs/1910.09318v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09318v1)
- **Published**: 2019-10-21 12:51:11+00:00
- **Updated**: 2019-10-21 12:51:11+00:00
- **Authors**: Ke Zhan, Shimiao Jiang, Yu Bai, Yi Li, Xu Liu, Zhuoran Xu
- **Comment**: None
- **Journal**: Proceedings of the British Machine Vision Conference (BMVC), 2019
- **Summary**: Eltwise layer is a commonly used structure in the multi-branch deep learning network. In a filter-wise pruning procedure, due to the specific operation of the eltwise layer, all its previous convolutional layers should vote for which filters by index should be pruned. Since only an intersection of the voted filters is pruned, the compression rate is limited. This work proposes a method called Directed-Weighting Group Lasso (DWGL), which enforces an index-wise incremental (directed) coefficient on the filterlevel group lasso items, so that the low index filters getting high activation tend to be kept while the high index ones tend to be pruned. When using DWGL, much fewer filters are retained during the voting process and the compression rate can be boosted. The paper test the proposed method on the ResNet series networks. On CIFAR-10, it achieved a 75.34% compression rate on ResNet-56 with a 0.94% error increment, and a 52.06% compression rate on ResNet-20 with a 0.72% error increment. On ImageNet, it achieved a 53% compression rate with ResNet-50 with a 0.6% error increment, speeding up the network by 2.23 times. Furthermore, it achieved a 75% compression rate on ResNet-50 with a 1.2% error increment, speeding up the network by 4 times.



### A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1910.09399v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09399v1)
- **Published**: 2019-10-21 14:23:14+00:00
- **Updated**: 2019-10-21 14:23:14+00:00
- **Authors**: Jorge Agnese, Jonathan Herrera, Haicheng Tao, Xingquan Zhu
- **Comment**: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery.
  2019
- **Journal**: None
- **Summary**: Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised deep learning methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. In this paper, we review the most recent development in the text-to-image synthesis research domain. Our survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNN). After that, we propose a taxonomy to summarize GAN based text-to-image synthesis into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We elaborate the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games) etc. The survey will conclude with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in the text-to-image synthesis domain.



### Hyperspectral Image Classification Based on Adaptive Sparse Deep Network
- **Arxiv ID**: http://arxiv.org/abs/1910.09405v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09405v1)
- **Published**: 2019-10-21 14:31:33+00:00
- **Updated**: 2019-10-21 14:31:33+00:00
- **Authors**: Jingwen Yan, Zixin Xie, Jingyao Chen, Yinan Liu, Lei Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse model is widely used in hyperspectral image classification.However, different of sparsity and regularization parameters has great influence on the classification results.In this paper, a novel adaptive sparse deep network based on deep architecture is proposed, which can construct the optimal sparse representation and regularization parameters by deep network.Firstly, a data flow graph is designed to represent each update iteration based on Alternating Direction Method of Multipliers (ADMM) algorithm.Forward network and Back-Propagation network are deduced.All parameters are updated by gradient descent in Back-Propagation.Then we proposed an Adaptive Sparse Deep Network.Comparing with several traditional classifiers or other algorithm for sparse model, experiment results indicate that our method achieves great improvement in HSI classification.



### Modeling Disease Progression In Retinal OCTs With Longitudinal Self-Supervised Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09420v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09420v3)
- **Published**: 2019-10-21 14:53:37+00:00
- **Updated**: 2019-10-24 07:30:39+00:00
- **Authors**: Antoine Rivail, Ursula Schmidt-Erfurth, Wolf-Dieter Vogl, Sebastian M. Waldstein, Sophie Riedl, Christoph Grechenig, Zhichao Wu, Hrvoje Bogunović
- **Comment**: Accepted for publication in the MICCAI 2019 PRIME workshop
- **Journal**: None
- **Summary**: Longitudinal imaging is capable of capturing the static ana\-to\-mi\-cal structures and the dynamic changes of the morphology resulting from aging or disease progression. Self-supervised learning allows to learn new representation from available large unlabelled data without any expert knowledge. We propose a deep learning self-supervised approach to model disease progression from longitudinal retinal optical coherence tomography (OCT). Our self-supervised model takes benefit from a generic time-related task, by learning to estimate the time interval between pairs of scans acquired from the same patient. This task is (i) easy to implement, (ii) allows to use irregularly sampled data, (iii) is tolerant to poor registration, and (iv) does not rely on additional annotations. This novel method learns a representation that focuses on progression specific information only, which can be transferred to other types of longitudinal problems. We transfer the learnt representation to a clinically highly relevant task of predicting the onset of an advanced stage of age-related macular degeneration within a given time interval based on a single OCT scan. The boost in prediction accuracy, in comparison to a network learned from scratch or transferred from traditional tasks, demonstrates that our pretrained self-supervised representation learns a clinically meaningful information.



### Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video
- **Arxiv ID**: http://arxiv.org/abs/1910.09430v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.09430v2)
- **Published**: 2019-10-21 15:06:03+00:00
- **Updated**: 2020-02-06 16:28:34+00:00
- **Authors**: Oier Mees, Markus Merklinger, Gabriel Kalweit, Wolfram Burgard
- **Comment**: Accepted at the 2020 IEEE International Conference on Robotics and
  Automation (ICRA). Video at https://www.youtube.com/watch?v=z8gG1k9kSqA
  Project page at http://robotskills.cs.uni-freiburg.de
- **Journal**: None
- **Summary**: Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de



### KuroNet: Pre-Modern Japanese Kuzushiji Character Recognition with Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.09433v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09433v1)
- **Published**: 2019-10-21 15:09:13+00:00
- **Updated**: 2019-10-21 15:09:13+00:00
- **Authors**: Tarin Clanuwat, Alex Lamb, Asanobu Kitamoto
- **Comment**: International Conference on Document Recognition (ICDAR) 2019 [oral]
- **Journal**: None
- **Summary**: Kuzushiji, a cursive writing style, had been used in Japan for over a thousand years starting from the 8th century. Over 3 millions books on a diverse array of topics, such as literature, science, mathematics and even cooking are preserved. However, following a change to the Japanese writing system in 1900, Kuzushiji has not been included in regular school curricula. Therefore, most Japanese natives nowadays cannot read books written or printed just 150 years ago. Museums and libraries have invested a great deal of effort into creating digital copies of these historical documents as a safeguard against fires, earthquakes and tsunamis. The result has been datasets with hundreds of millions of photographs of historical documents which can only be read by a small number of specially trained experts. Thus there has been a great deal of interest in using Machine Learning to automatically recognize these historical texts and transcribe them into modern Japanese characters. Nevertheless, several challenges in Kuzushiji recognition have made the performance of existing systems extremely poor. To tackle these challenges, we propose KuroNet, a new end-to-end model which jointly recognizes an entire page of text by using a residual U-Net architecture which predicts the location and identity of all characters given a page of text (without any pre-processing). This allows the model to handle long range context, large vocabularies, and non-standardized character layouts. We demonstrate that our system is able to successfully recognize a large fraction of pre-modern Japanese documents, but also explore areas where our system is limited and suggest directions for future work.



### Improving Style Transfer with Calibrated Metrics
- **Arxiv ID**: http://arxiv.org/abs/1910.09447v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09447v2)
- **Published**: 2019-10-21 15:26:05+00:00
- **Updated**: 2020-02-13 17:26:52+00:00
- **Authors**: Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, Chuhang Zou, David Forsyth
- **Comment**: updated conference camera ready version. arXiv admin note: text
  overlap with arXiv:1804.00118
- **Journal**: None
- **Summary**: Style transfer methods produce a transferred image which is a rendering of a content image in the manner of a style image. We seek to understand how to improve style transfer.   To do so requires quantitative evaluation procedures, but the current evaluation is qualitative, mostly involving user studies. We describe a novel quantitative evaluation procedure. Our procedure relies on two statistics: the Effectiveness (E) statistic measures the extent that a given style has been transferred to the target, and the Coherence (C) statistic measures the extent to which the original image's content is preserved. Our statistics are calibrated to human preference: targets with larger values of E (resp C) will reliably be preferred by human subjects in comparisons of style (resp. content).   We use these statistics to investigate the relative performance of a number of Neural Style Transfer(NST) methods, revealing several intriguing properties. Admissible methods lie on a Pareto frontier (i.e. improving E reduces C or vice versa). Three methods are admissible: Universal style transfer produces very good C but weak E; modifying the optimization used for Gatys' loss produces a method with strong E and strong C; and a modified cross-layer method has slightly better E at strong cost in C. While the histogram loss improves the E statistics of Gatys' method, it does not make the method admissible. Surprisingly, style weights have relatively little effect in improving EC scores, and most variability in the transfer is explained by the style itself (meaning experimenters can be misguided by selecting styles).



### Tree-gated Deep Mixture-of-Experts For Pose-robust Face Alignment
- **Arxiv ID**: http://arxiv.org/abs/1910.09450v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09450v1)
- **Published**: 2019-10-21 15:30:20+00:00
- **Updated**: 2019-10-21 15:30:20+00:00
- **Authors**: Estephe Arnaud, Arnaud Dapogny, Kevin Bailly
- **Comment**: None
- **Journal**: None
- **Summary**: Face alignment consists of aligning a shape model on a face image. It is an active domain in computer vision as it is a preprocessing for a number of face analysis and synthesis applications. Current state-of-the-art methods already perform well on "easy" datasets, with moderate head pose variations, but may not be robust for "in-the-wild" data with poses up to 90{\deg}. In order to increase robustness to an ensemble of factors of variations (e.g. head pose or occlusions), a given layer (e.g. a regressor or an upstream CNN layer) can be replaced by a Mixture of Experts (MoE) layer that uses an ensemble of experts instead of a single one. The weights of this mixture can be learned as gating functions to jointly learn the experts and the corresponding weights. In this paper, we propose to use tree-structured gates which allows a hierarchical weighting of the experts (Tree-MoE). We investigate the use of Tree-MoE layers in different contexts in the frame of face alignment with cascaded regression, firstly for emphasizing relevant, more specialized feature extractors depending of a high-level semantic information such as head pose (Pose-Tree-MoE), and secondly as an overall more robust regression layer. We perform extensive experiments on several challenging face alignment datasets, demonstrating that our approach outperforms the state-of-the-art methods.



### Depth-wise Decomposition for Accelerating Separable Convolutions in Efficient Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.09455v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09455v1)
- **Published**: 2019-10-21 15:37:53+00:00
- **Updated**: 2019-10-21 15:37:53+00:00
- **Authors**: Yihui He, Jianing Qian, Jianren Wang
- **Comment**: CVPR 2019 workshop, Efficient Deep Learning for Computer Vision
- **Journal**: None
- **Summary**: Very deep convolutional neural networks (CNNs) have been firmly established as the primary methods for many computer vision tasks. However, most state-of-the-art CNNs are large, which results in high inference latency. Recently, depth-wise separable convolution has been proposed for image recognition tasks on computationally limited platforms such as robotics and self-driving cars. Though it is much faster than its counterpart, regular convolution, accuracy is sacrificed. In this paper, we propose a novel decomposition approach based on SVD, namely depth-wise decomposition, for expanding regular convolutions into depthwise separable convolutions while maintaining high accuracy. We show our approach can be further generalized to the multi-channel and multi-layer cases, based on Generalized Singular Value Decomposition (GSVD) [59]. We conduct thorough experiments with the latest ShuffleNet V2 model [47] on both random synthesized dataset and a large-scale image recognition dataset: ImageNet [10]. Our approach outperforms channel decomposition [73] on all datasets. More importantly, our approach improves the Top-1 accuracy of ShuffleNet V2 by ~2%.



### Improving Vehicle Re-Identification using CNN Latent Spaces: Metrics Comparison and Track-to-track Extension
- **Arxiv ID**: http://arxiv.org/abs/1910.09458v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09458v2)
- **Published**: 2019-10-21 15:41:59+00:00
- **Updated**: 2020-09-26 08:39:58+00:00
- **Authors**: Geoffrey Roman-Jimenez, Patrice Guyot, Thierry Malon, Sylvie Chambon, Vincent Charvillat, Alain Crouzil, André Péninou, Julien Pinquier, Florence Sedes, Christine Sénac
- **Comment**: This paper is a postprint of a paper submitted to and accepted for
  publication in the journal IET Computer Vision and is subject to Institution
  of Engineering and Technology Copyright. The copy of record is available at
  the IET Digital Library
- **Journal**: None
- **Summary**: This paper addresses the problem of vehicle re-identification using distance comparison of images in CNN latent spaces.   Firstly, we study the impact of the distance metrics, comparing performances obtained with different metrics: the minimal Euclidean distance (MED), the minimal cosine distance (MCD), and the residue of the sparse coding reconstruction (RSCR). These metrics are applied using features extracted from five different CNN architectures, namely ResNet18, AlexNet, VGG16, InceptionV3 and DenseNet201. We use the specific vehicle re-identification dataset VeRi to fine-tune these CNNs and evaluate results. In overall, independently of the CNN used, MCD outperforms MED, commonly used in the literature. These results are confirmed on other vehicle retrieval datasets. Secondly, we extend the state-of-the-art image-to-track process (I2TP) to a track-to-track process (T2TP). The three distance metrics are extended to measure distance between tracks, enabling T2TP. We compared T2TP with I2TP using the same CNN models. Results show that T2TP outperforms I2TP for MCD and RSCR. T2TP combining DenseNet201 and MCD-based metrics exhibits the best performances, outperforming the state-of-the-art I2TP-based models. Finally, experiments highlight two main results: i) the impact of metric choice in vehicle re-identification, and ii) T2TP improves the performances compared to I2TP, especially when coupled with MCD-based metrics.



### Object landmark discovery through unsupervised adaptation
- **Arxiv ID**: http://arxiv.org/abs/1910.09469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09469v1)
- **Published**: 2019-10-21 15:58:57+00:00
- **Updated**: 2019-10-21 15:58:57+00:00
- **Authors**: Enrique Sanchez, Georgios Tzimiropoulos
- **Comment**: NeurIPS 2019. Code is available
  https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019
- **Journal**: None
- **Summary**: This paper proposes a method to ease the unsupervised learning of object landmark detectors. Similarly to previous methods, our approach is fully unsupervised in a sense that it does not require or make any use of annotated landmarks for the target object category. Contrary to previous works, we do however assume that a landmark detector, which has already learned a structured representation for a given object category in a fully supervised manner, is available. Under this setting, our main idea boils down to adapting the given pre-trained network to the target object categories in a fully unsupervised manner. To this end, our method uses the pre-trained network as a core which remains frozen and does not get updated during training, and learns, in an unsupervised manner, only a projection matrix to perform the adaptation to the target categories. By building upon an existing structured representation learned in a supervised manner, the optimization problem solved by our method is much more constrained with significantly less parameters to learn which seems to be important for the case of unsupervised learning. We show that our method surpasses fully unsupervised techniques trained from scratch as well as a strong baseline based on fine-tuning, and produces state-of-the-art results on several datasets. Code can be found at https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 .



### Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation
- **Arxiv ID**: http://arxiv.org/abs/1910.09470v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09470v1)
- **Published**: 2019-10-21 16:00:53+00:00
- **Updated**: 2019-10-21 16:00:53+00:00
- **Authors**: Rae Jeong, Yusuf Aytar, David Khosid, Yuxiang Zhou, Jackie Kay, Thomas Lampe, Konstantinos Bousmalis, Francesco Nori
- **Comment**: None
- **Journal**: None
- **Summary**: Collecting and automatically obtaining reward signals from real robotic visual data for the purposes of training reinforcement learning algorithms can be quite challenging and time-consuming. Methods for utilizing unlabeled data can have a huge potential to further accelerate robotic learning. We consider here the problem of performing manipulation tasks from pixels. In such tasks, choosing an appropriate state representation is crucial for planning and control. This is even more relevant with real images where noise, occlusions and resolution affect the accuracy and reliability of state estimation. In this work, we learn a latent state representation implicitly with deep reinforcement learning in simulation, and then adapt it to the real domain using unlabeled real robot data. We propose to do so by optimizing sequence-based self supervised objectives. These exploit the temporal nature of robot experience, and can be common in both the simulated and real domains, without assuming any alignment of underlying states in simulated and unlabeled real images. We propose Contrastive Forward Dynamics loss, which combines dynamics model learning with time-contrastive techniques. The learned state representation that results from our methods can be used to robustly solve a manipulation task in simulation and to successfully transfer the learned skill on a real system. We demonstrate the effectiveness of our approaches by training a vision-based reinforcement learning agent for cube stacking. Agents trained with our method, using only 5 hours of unlabeled real robot data for adaptation, shows a clear improvement over domain randomization, and standard visual domain adaptation techniques for sim-to-real transfer.



### Relative Interior Rule in Block-Coordinate Minimization
- **Arxiv ID**: http://arxiv.org/abs/1910.09488v1
- **DOI**: None
- **Categories**: **math.OC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09488v1)
- **Published**: 2019-10-21 16:25:11+00:00
- **Updated**: 2019-10-21 16:25:11+00:00
- **Authors**: Tomáš Werner, Daniel Průša
- **Comment**: None
- **Journal**: None
- **Summary**: (Block-)coordinate minimization is an iterative optimization method which in every iteration finds a global minimum of the objective over a variable or a subset of variables, while keeping the remaining variables constant. While for some problems, coordinate minimization converges to a global minimum (e.g., convex differentiable objective), for general (non-differentiable) convex problems this may not be the case. Despite this drawback, (block-)coordinate minimization can be an acceptable option for large-scale non-differentiable convex problems; an example is methods to solve the linear programming relaxation of the discrete energy minimization problem (MAP inference in graphical models). When block-coordinate minimization is applied to a general convex problem, in every iteration the minimizer over the current coordinate block need not be unique and therefore a single minimizer must be chosen. We propose that this minimizer be chosen from the relative interior of the set of all minimizers over the current block. We show that this rule is not worse, in a certain precise sense, than any other rule.



### S4NN: temporal backpropagation for spiking neural networks with one spike per neuron
- **Arxiv ID**: http://arxiv.org/abs/1910.09495v4
- **DOI**: 10.1142/S0129065720500276
- **Categories**: **cs.NE**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1910.09495v4)
- **Published**: 2019-10-21 16:39:42+00:00
- **Updated**: 2020-06-13 10:33:19+00:00
- **Authors**: Saeed Reza Kheradpisheh, Timothée Masquelier
- **Comment**: None
- **Journal**: International Journal of Neural Systems 2020
- **Summary**: We propose a new supervised learning rule for multilayer spiking neural networks (SNNs) that use a form of temporal coding known as rank-order-coding. With this coding scheme, all neurons fire exactly one spike per stimulus, but the firing order carries information. In particular, in the readout layer, the first neuron to fire determines the class of the stimulus. We derive a new learning rule for this sort of network, named S4NN, akin to traditional error backpropagation, yet based on latencies. We show how approximated error gradients can be computed backward in a feedforward network with any number of layers. This approach reaches state-of-the-art performance with supervised multi fully-connected layer SNNs: test accuracy of 97.4% for the MNIST dataset, and 99.2% for the Caltech Face/Motorbike dataset. Yet, the neuron model that we use, non-leaky integrate-and-fire, is much simpler than the one used in all previous works. The source codes of the proposed S4NN are publicly available at https://github.com/SRKH/S4NN.



### Multi-Resolution Weak Supervision for Sequential Data
- **Arxiv ID**: http://arxiv.org/abs/1910.09505v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09505v1)
- **Published**: 2019-10-21 16:48:18+00:00
- **Updated**: 2019-10-21 16:48:18+00:00
- **Authors**: Frederic Sala, Paroma Varma, Jason Fries, Daniel Y. Fu, Shiori Sagawa, Saelig Khattar, Ashwini Ramamoorthy, Ke Xiao, Kayvon Fatahalian, James Priest, Christopher Ré
- **Comment**: NeurIPS 2019 (Conference on Neural Information Processing Systems)
- **Journal**: None
- **Summary**: Since manually labeling training data is slow and expensive, recent industrial and scientific research efforts have turned to weaker or noisier forms of supervision sources. However, existing weak supervision approaches fail to model multi-resolution sources for sequential data, like video, that can assign labels to individual elements or collections of elements in a sequence. A key challenge in weak supervision is estimating the unknown accuracies and correlations of these sources without using labeled data. Multi-resolution sources exacerbate this challenge due to complex correlations and sample complexity that scales in the length of the sequence. We propose Dugong, the first framework to model multi-resolution weak supervision sources with complex correlations to assign probabilistic labels to training data. Theoretically, we prove that Dugong, under mild conditions, can uniquely recover the unobserved accuracy and correlation parameters and use parameter sharing to improve sample complexity. Our method assigns clinician-validated labels to population-scale biomedical video repositories, helping outperform traditional supervision by 36.8 F1 points and addressing a key use case where machine learning has been severely limited by the lack of expert labeled data. On average, Dugong improves over traditional supervision by 16.0 F1 points and existing weak supervision approaches by 24.2 F1 points across several video and sensor classification tasks.



### Spectral Characterization of functional MRI data on voxel-resolution cortical graphs
- **Arxiv ID**: http://arxiv.org/abs/1910.09507v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, eess.SP
- **Links**: [PDF](http://arxiv.org/pdf/1910.09507v3)
- **Published**: 2019-10-21 16:54:45+00:00
- **Updated**: 2020-05-11 00:22:34+00:00
- **Authors**: Hamid Behjat, Martin Larsson
- **Comment**: Fixed two typos in the equations; (1) definition of L in section 2.1,
  paragraph 1. (2) signal de-meaning and normalization in section 2.4,
  paragraph 1
- **Journal**: None
- **Summary**: The human cortical layer exhibits a convoluted morphology that is unique to each individual. Conventional volumetric fMRI processing schemes take for granted the rich information provided by the underlying anatomy. We present a method to study fMRI data on subject-specific cerebral hemisphere cortex (CHC) graphs, which encode the cortical morphology at the resolution of voxels in 3-D. We study graph spectral energy metrics associated to fMRI data of 100 subjects from the Human Connectome Project database, across seven tasks. Experimental results signify the strength of CHC graphs' Laplacian eigenvector bases in capturing subtle spatial patterns specific to different functional loads as well as experimental conditions within each task.



### Cascaded Generation of High-quality Color Visible Face Images from Thermal Captures
- **Arxiv ID**: http://arxiv.org/abs/1910.09524v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.09524v1)
- **Published**: 2019-10-21 17:37:44+00:00
- **Updated**: 2019-10-21 17:37:44+00:00
- **Authors**: Naser Damer, Fadi Boutros, Khawla Mallat, Florian Kirchbuchner, Jean-Luc Dugelay, Arjan Kuijper
- **Comment**: None
- **Journal**: None
- **Summary**: Generating visible-like face images from thermal images is essential to perform manual and automatic cross-spectrum face recognition. We successfully propose a solution based on cascaded refinement network that, unlike previous works, produces high quality generated color images without the need for face alignment, large databases, data augmentation, polarimetric sensors, computationally-intense training, or unrealistic restriction on the generated resolution. The training of our solution is based on the contextual loss, making it inherently scale (face area) and rotation invariant. We present generated image samples of unknown individuals under different poses and occlusion conditions.We also prove the high similarity in image quality between ground-truth images and generated ones by comparing seven quality metrics. We compare our results with two state-of-the-art approaches proving the superiority of our proposed approach.



### Icentia11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery
- **Arxiv ID**: http://arxiv.org/abs/1910.09570v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, eess.SP, stat.AP, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.09570v1)
- **Published**: 2019-10-21 18:02:36+00:00
- **Updated**: 2019-10-21 18:02:36+00:00
- **Authors**: Shawn Tan, Guillaume Androz, Ahmad Chamseddine, Pierre Fecteau, Aaron Courville, Yoshua Bengio, Joseph Paul Cohen
- **Comment**: Under Review
- **Journal**: None
- **Summary**: We release the largest public ECG dataset of continuous raw signals for representation learning containing 11 thousand patients and 2 billion labelled beats. Our goal is to enable semi-supervised ECG models to be made as well as to discover unknown subtypes of arrhythmia and anomalous ECG signal events. To this end, we propose an unsupervised representation learning task, evaluated in a semi-supervised fashion. We provide a set of baselines for different feature extractors that can be built upon. Additionally, we perform qualitative evaluations on results from PCA embeddings, where we identify some clustering of known subtypes indicating the potential for representation learning in arrhythmia sub-type discovery.



### Designovel's system description for Fashion-IQ challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.11119v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11119v1)
- **Published**: 2019-10-21 18:06:26+00:00
- **Updated**: 2019-10-21 18:06:26+00:00
- **Authors**: Jianri Li, Jae-whan Lee, Woo-sang Song, Ki-young Shin, Byung-hyun Go
- **Comment**: None
- **Journal**: None
- **Summary**: This paper describes Designovel's systems which are submitted to the Fashion IQ Challenge 2019. Goal of the challenge is building an image retrieval system where input query is a candidate image plus two text phrases describe user's feedback about visual differences between the candidate image and the search target. We built the systems by combining methods from recent work on deep metric learning, multi-modal retrieval and natual language processing. First, we encode both candidate and target images with CNNs into high-level representations, and encode text descriptions to a single text vector using Transformer-based encoder. Then we compose candidate image vector and text representation into a single vector which is exptected to be biased toward target image vector. Finally, we compute cosine similarities between composed vector and encoded vectors of whole dataset, and rank them in desceding order to get ranked list. We experimented with Fashion IQ 2019 dataset in various settings of hyperparameters, achieved 39.12% average recall by a single model and 43.67% average recall by an ensemble of 16 models on test dataset.



### Shallow Art: Art Extension Through Simple Machine Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11118v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11118v1)
- **Published**: 2019-10-21 18:47:06+00:00
- **Updated**: 2019-10-21 18:47:06+00:00
- **Authors**: Kyle Robinson, Dan Brown
- **Comment**: 5 pages, 9 figures, presented at the 10th International Conference on
  Computational Creativity (ICCC 2019)
- **Journal**: Proceedings of the 10th International Conference on Computational
  Creativity (2019) 316-320 [ISBN: 978-989-54160-1-1]
- **Summary**: Shallow Art presents, implements, and tests the use of simple single-output classification and regression models for the purpose of art generation. Various machine learning algorithms are trained on collections of computer generated images, artworks from Vincent van Gogh, and artworks from Rembrandt van Rijn. These models are then provided half of an image and asked to complete the missing side. The resulting images are displayed, and we explore implications for computational creativity.



### Volterra Neural Networks (VNNs)
- **Arxiv ID**: http://arxiv.org/abs/1910.09616v5
- **DOI**: 10.1609/aaai.v34i07.6870
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09616v5)
- **Published**: 2019-10-21 19:22:38+00:00
- **Updated**: 2023-06-15 16:06:40+00:00
- **Authors**: Siddharth Roheda, Hamid Krim
- **Comment**: Conference Version published in AAAI 2020. Journal version under
  review at JMLR
- **Journal**: None
- **Summary**: The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video sequence for action recognition. The proposed approach is evaluated on UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform state of the art CNN approaches.



### GANspection
- **Arxiv ID**: http://arxiv.org/abs/1910.09638v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09638v1)
- **Published**: 2019-10-21 20:26:54+00:00
- **Updated**: 2019-10-21 20:26:54+00:00
- **Authors**: Hammad A. Ayyubi
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.



### The SWAX Benchmark: Attacking Biometric Systems with Wax Figures
- **Arxiv ID**: http://arxiv.org/abs/1910.09642v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09642v1)
- **Published**: 2019-10-21 20:40:54+00:00
- **Updated**: 2019-10-21 20:40:54+00:00
- **Authors**: Rafael Henrique Vareto, Araceli Marcia Sandanha, William Robson Schwartz
- **Comment**: None
- **Journal**: None
- **Summary**: A face spoofing attack occurs when an intruder attempts to impersonate someone who carries a gainful authentication clearance. It is a trending topic due to the increasing demand for biometric authentication on mobile devices, high-security areas, among others. This work introduces a new database named Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. Experiments performed with baseline methods show that despite the progress in recent years, advanced spoofing methods are still vulnerable to high-quality violation attempts.



### CPWC: Contextual Point Wise Convolution for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.09643v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.09643v2)
- **Published**: 2019-10-21 20:41:22+00:00
- **Updated**: 2020-02-06 11:57:36+00:00
- **Authors**: Pratik Mazumder, Pravendra Singh, Vinay Namboodiri
- **Comment**: Accepted in ICASSP 2020
- **Journal**: None
- **Summary**: Convolutional layers are a major driving force behind the successes of deep learning. Pointwise convolution (PWC) is a 1x1 convolutional filter that is primarily used for parameter reduction. However, the PWC ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of PWC has a significant impact on the network performance. We propose an alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We experimentally show that our design results in significant improvement in the performance of the network for classification as well as detection.



### Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight
- **Arxiv ID**: http://arxiv.org/abs/1910.09664v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CL, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.09664v1)
- **Published**: 2019-10-21 21:19:33+00:00
- **Updated**: 2019-10-21 21:19:33+00:00
- **Authors**: Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A. Knepper, Yoav Artzi
- **Comment**: Conference on Robot Learning (CoRL) 2019
- **Journal**: None
- **Summary**: We propose a joint simulation and real-world learning framework for mapping navigation instructions and raw first-person observations to continuous control. Our model estimates the need for environment exploration, predicts the likelihood of visiting environment positions during execution, and controls the agent to both explore and visit high-likelihood positions. We introduce Supervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both simulation and real environments without requiring autonomous flight in the physical environment during training, and combines supervised learning for predicting positions to visit and reinforcement learning for continuous control. We evaluate our approach on a natural language instruction-following task with a physical quadcopter, and demonstrate effective execution and exploration behavior.



