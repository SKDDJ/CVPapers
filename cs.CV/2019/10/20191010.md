# Arxiv Papers in cs.CV on 2019-10-10
### Practical License Plate Recognition in Unconstrained Surveillance Systems with Adversarial Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1910.04324v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04324v1)
- **Published**: 2019-10-10 01:37:21+00:00
- **Updated**: 2019-10-10 01:37:21+00:00
- **Authors**: Younkwan Lee, Jiwon Jun, Yoojin Hong, Moongu Jeon
- **Comment**: Accepted at VISAPP, 2019
- **Journal**: None
- **Summary**: Although most current license plate (LP) recognition applications have been significantly advanced, they are still limited to ideal environments where training data are carefully annotated with constrained scenes. In this paper, we propose a novel license plate recognition method to handle unconstrained real world traffic scenes. To overcome these difficulties, we use adversarial super-resolution (SR), and one-stage character segmentation and recognition. Combined with a deep convolutional network based on VGG-net, our method provides simple but reasonable training procedure. Moreover, we introduce GIST-LP, a challenging LP dataset where image samples are effectively collected from unconstrained surveillance scenes. Experimental results on AOLP and GIST-LP dataset illustrate that our method, without any scene-specific adaptation, outperforms current LP recognition approaches in accuracy and provides visual enhancement in our SR results that are easier to understand than original data.



### Unconstrained Road Marking Recognition with Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.04326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04326v1)
- **Published**: 2019-10-10 01:53:50+00:00
- **Updated**: 2019-10-10 01:53:50+00:00
- **Authors**: Younkwan Lee, Juhyun Lee, Yoojin Hong, YeongMin Ko, Moongu Jeon
- **Comment**: Accepted at IEEE Intelligent Vehicles Symposium (IV), 2019
- **Journal**: None
- **Summary**: Recent road marking recognition has achieved great success in the past few years along with the rapid development of deep learning. Although considerable advances have been made, they are often over-dependent on unrepresentative datasets and constrained conditions. In this paper, to overcome these drawbacks, we propose an alternative method that achieves higher accuracy and generates high-quality samples as data augmentation. With the following two major contributions: 1) The proposed deblurring network can successfully recover a clean road marking from a blurred one by adopting generative adversarial networks (GAN). 2) The proposed data augmentation method, based on mutual information, can preserve and learn semantic context from the given dataset. We construct and train a class-conditional GAN to increase the size of training set, which makes it suitable to recognize target. The experimental results have shown that our proposed framework generates deblurred clean samples from blurry ones, and outperforms other methods even with unconstrained road marking datasets.



### Agent with Warm Start and Active Termination for Plane Localization in 3D Ultrasound
- **Arxiv ID**: http://arxiv.org/abs/1910.04331v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04331v1)
- **Published**: 2019-10-10 02:21:52+00:00
- **Updated**: 2019-10-10 02:21:52+00:00
- **Authors**: Haoran Dou, Xin Yang, Jikuan Qian, Wufeng Xue, Hao Qin, Xu Wang, Lequan Yu, Shujun Wang, Yi Xiong, Pheng-Ann Heng, Dong Ni
- **Comment**: 9 pages, 5 figures, 1 table. Accepted by MICCAI 2019 (oral)
- **Journal**: None
- **Summary**: Standard plane localization is crucial for ultrasound (US) diagnosis. In prenatal US, dozens of standard planes are manually acquired with a 2D probe. It is time-consuming and operator-dependent. In comparison, 3D US containing multiple standard planes in one shot has the inherent advantages of less user-dependency and more efficiency. However, manual plane localization in US volume is challenging due to the huge search space and large fetal posture variation. In this study, we propose a novel reinforcement learning (RL) framework to automatically localize fetal brain standard planes in 3D US. Our contribution is two-fold. First, we equip the RL framework with a landmark-aware alignment module to provide warm start and strong spatial bounds for the agent actions, thus ensuring its effectiveness. Second, instead of passively and empirically terminating the agent inference, we propose a recurrent neural network based strategy for active termination of the agent's interaction procedure. This improves both the accuracy and efficiency of the localization system. Extensively validated on our in-house large dataset, our approach achieves the accuracy of 3.4mm/9.6{\deg} and 2.7mm/9.1{\deg} for the transcerebellar and transthalamic plane localization, respectively. Ourproposed RL framework is general and has the potential to improve the efficiency and standardization of US scanning.



### CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.04335v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04335v2)
- **Published**: 2019-10-10 02:34:34+00:00
- **Updated**: 2020-03-02 10:24:13+00:00
- **Authors**: Marvin Chanc√°n, Michael Milford
- **Comment**: Preprint version of article accepted to ICRA 2020
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.



### Visual Understanding of Multiple Attributes Learning Model of X-Ray Scattering Images
- **Arxiv ID**: http://arxiv.org/abs/1910.04357v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.HC, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04357v1)
- **Published**: 2019-10-10 03:51:58+00:00
- **Updated**: 2019-10-10 03:51:58+00:00
- **Authors**: Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin Yager, Wei Xu
- **Comment**: 5 pages, 2 figures, ICCV conference co-held XAIC workshop 2019
- **Journal**: None
- **Summary**: This extended abstract presents a visualization system, which is designed for domain scientists to visually understand their deep learning model of extracting multiple attributes in x-ray scattering images. The system focuses on studying the model behaviors related to multiple structural attributes. It allows users to explore the images in the feature space, the classification output of different attributes, with respect to the actual attributes labelled by domain scientists. Abundant interactions allow users to flexibly select instance images, their clusters, and compare them visually in details. Two preliminary case studies demonstrate its functionalities and usefulness.



### A Generative Approach Towards Improved Robotic Detection of Marine Litter
- **Arxiv ID**: http://arxiv.org/abs/1910.04754v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04754v1)
- **Published**: 2019-10-10 05:28:47+00:00
- **Updated**: 2019-10-10 05:28:47+00:00
- **Authors**: Jungseok Hong, Michael Fulton, Junaed Sattar
- **Comment**: None
- **Journal**: None
- **Summary**: This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects "good quality" images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.



### Measuring robustness of Visual SLAM
- **Arxiv ID**: http://arxiv.org/abs/1910.04755v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04755v1)
- **Published**: 2019-10-10 06:12:44+00:00
- **Updated**: 2019-10-10 06:12:44+00:00
- **Authors**: David Prokhorov, Dmitry Zhukov, Olga Barinova, Anna Vorontsova, Anton Konushin
- **Comment**: None
- **Journal**: None
- **Summary**: Simultaneous localization and mapping (SLAM) is an essential component of robotic systems. In this work we perform a feasibility study of RGB-D SLAM for the task of indoor robot navigation. Recent visual SLAM methods, e.g. ORBSLAM2 \cite{mur2017orb}, demonstrate really impressive accuracy, but the experiments in the papers are usually conducted on just a few sequences, that makes it difficult to reason about the robustness of the methods. Another problem is that all available RGB-D datasets contain the trajectories with very complex camera motions. In this work we extensively evaluate ORBSLAM2 to better understand the state-of-the-art. First, we conduct experiments on the popular publicly available datasets for RGB-D SLAM across the conventional metrics. We perform statistical analysis of the results and find correlations between the metrics and the attributes of the trajectories. Then, we introduce a new large and diverse HomeRobot dataset where we model the motions of a simple home robot. Our dataset is created using physically-based rendering with realistic lighting and contains the scenes composed by human designers. It includes thousands of sequences, that is two orders of magnitude greater than in previous works. We find that while in many cases the accuracy of SLAM is very good, the robustness is still an issue.



### Adaptive and Azimuth-Aware Fusion Network of Multimodal Local Features for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.04392v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04392v2)
- **Published**: 2019-10-10 07:07:01+00:00
- **Updated**: 2020-06-03 10:13:08+00:00
- **Authors**: Yonglin Tian, Kunfeng Wang, Yuang Wang, Yulin Tian, Zilei Wang, Fei-Yue Wang
- **Comment**: Accepted by Neurocomputing
- **Journal**: None
- **Summary**: This paper focuses on the construction of stronger local features and the effective fusion of image and LiDAR data. We adopt different modalities of LiDAR data to generate richer features and present an adaptive and azimuth-aware network to aggregate local features from image, bird's eye view maps and point cloud. Our network mainly consists of three subnetworks: ground plane estimation network, region proposal network and adaptive fusion network. The ground plane estimation network extracts features of point cloud and predicts the parameters of a plane which are used for generating abundant 3D anchors. The region proposal network generates features of image and bird's eye view maps to output region proposals. To integrate heterogeneous image and point cloud features, the adaptive fusion network explicitly adjusts the intensity of multiple local features and achieves the orientation consistency between image and LiDAR data by introduce an azimuth-aware fusion module. Experiments are conducted on KITTI dataset and the results validate the advantages of our aggregation of multimodal local features and the adaptive fusion network.



### On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention
- **Arxiv ID**: http://arxiv.org/abs/1910.04396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04396v1)
- **Published**: 2019-10-10 07:20:54+00:00
- **Updated**: 2019-10-10 07:20:54+00:00
- **Authors**: Junyeop Lee, Sungrae Park, Jeonghun Baek, Seong Joon Oh, Seonghyeon Kim, Hwalsuk Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Scene text recognition (STR) is the task of recognizing character sequences in natural scenes. While there have been great advances in STR methods, current methods still fail to recognize texts in arbitrary shapes, such as heavily curved or rotated texts, which are abundant in daily life (e.g. restaurant signs, product labels, company logos, etc). This paper introduces a novel architecture to recognizing texts of arbitrary shapes, named Self-Attention Text Recognition Network (SATRN), which is inspired by the Transformer. SATRN utilizes the self-attention mechanism to describe two-dimensional (2D) spatial dependencies of characters in a scene text image. Exploiting the full-graph propagation of self-attention, SATRN can recognize texts with arbitrary arrangements and large inter-character spacing. As a result, SATRN outperforms existing STR models by a large margin of 5.7 pp on average in "irregular text" benchmarks. We provide empirical analyses that illustrate the inner mechanisms and the extent to which the model is applicable (e.g. rotated and multi-line text). We will open-source the code.



### BitNet: Learning-Based Bit-Depth Expansion
- **Arxiv ID**: http://arxiv.org/abs/1910.04397v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04397v1)
- **Published**: 2019-10-10 07:21:19+00:00
- **Updated**: 2019-10-10 07:21:19+00:00
- **Authors**: Junyoung Byun, Kyujin Shim, Changick Kim
- **Comment**: Accepted by ACCV 2018, Authors Byun and Shim contributed equally
- **Journal**: None
- **Summary**: Bit-depth is the number of bits for each color channel of a pixel in an image. Although many modern displays support unprecedented higher bit-depth to show more realistic and natural colors with a high dynamic range, most media sources are still in bit-depth of 8 or lower. Since insufficient bit-depth may generate annoying false contours or lose detailed visual appearance, bit-depth expansion (BDE) from low bit-depth (LBD) images to high bit-depth (HBD) images becomes more and more important. In this paper, we adopt a learning-based approach for BDE and propose a novel CNN-based bit-depth expansion network (BitNet) that can effectively remove false contours and restore visual details at the same time. We have carefully designed our BitNet based on an encoder-decoder architecture with dilated convolutions and a novel multi-scale feature integration. We have performed various experiments with four different datasets including MIT-Adobe FiveK, Kodak, ESPL v2, and TESTIMAGES, and our proposed BitNet has achieved state-of-the-art performance in terms of PSNR and SSIM among other existing BDE methods and famous CNN-based image processing networks. Unlike previous methods that separately process each color channel, we treat all RGB channels at once and have greatly improved color restoration. In addition, our network has shown the fastest computational speed in near real-time.



### Sentiment Analysis from Images of Natural Disasters
- **Arxiv ID**: http://arxiv.org/abs/1910.04416v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.04416v1)
- **Published**: 2019-10-10 07:59:11+00:00
- **Updated**: 2019-10-10 07:59:11+00:00
- **Authors**: Syed Zohaib, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha
- **Comment**: None
- **Journal**: None
- **Summary**: Social media have been widely exploited to detect and gather relevant information about opinions and events. However, the relevance of the information is very subjective and rather depends on the application and the end-users. In this article, we tackle a specific facet of social media data processing, namely the sentiment analysis of disaster-related images by considering people's opinions, attitudes, feelings and emotions. We analyze how visual sentiment analysis can improve the results for the end-users/beneficiaries in terms of mining information from social media. We also identify the challenges and related applications, which could help defining a benchmark for future research efforts in visual sentiment analysis.



### Breathing deformation model -- application to multi-resolution abdominal MRI
- **Arxiv ID**: http://arxiv.org/abs/1910.04456v1
- **DOI**: 10.1109/EMBC.2019.885770
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04456v1)
- **Published**: 2019-10-10 09:53:43+00:00
- **Updated**: 2019-10-10 09:53:43+00:00
- **Authors**: Chompunuch Sarasaen, Soumick Chatterjee, Mario Breitkopf, Domenico Iuso, Georg Rose, Oliver Speck
- **Comment**: 2019 41st Annual International Conference of the IEEE Engineering in
  Medicine and Biology Society (EMBC)
- **Journal**: None
- **Summary**: Dynamic MRI is a technique of acquiring a series of images continuously to follow the physiological changes over time. However, such fast imaging results in low resolution images. In this work, abdominal deformation model computed from dynamic low resolution images have been applied to high resolution image, acquired previously, to generate dynamic high resolution MRI. Dynamic low resolution images were simulated into different breathing phases (inhale and exhale). Then, the image registration between breathing time points was performed using the B-spline SyN deformable model and using cross-correlation as a similarity metric. The deformation model between different breathing phases were estimated from highly undersampled data. This deformation model was then applied to the high resolution images to obtain high resolution images of different breathing phases. The results indicated that the deformation model could be computed from relatively very low resolution images.



### Searching for A Robust Neural Architecture in Four GPU Hours
- **Arxiv ID**: http://arxiv.org/abs/1910.04465v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04465v2)
- **Published**: 2019-10-10 10:14:48+00:00
- **Updated**: 2019-10-16 05:43:51+00:00
- **Authors**: Xuanyi Dong, Yi Yang
- **Comment**: Minor modifications to the CVPR 2019 camera-ready version (add code
  link)
- **Journal**: None
- **Summary**: Conventional neural architecture search (NAS) approaches are based on reinforcement learning or evolutionary strategy, which take more than 3000 GPU hours to find a good model on CIFAR-10. We propose an efficient NAS approach learning to search by gradient descent. Our approach represents the search space as a directed acyclic graph (DAG). This DAG contains billions of sub-graphs, each of which indicates a kind of neural architecture. To avoid traversing all the possibilities of the sub-graphs, we develop a differentiable sampler over the DAG. This sampler is learnable and optimized by the validation loss after training the sampled architecture. In this way, our approach can be trained in an end-to-end fashion by gradient descent, named Gradient-based search using Differentiable Architecture Sampler (GDAS). In experiments, we can finish one searching procedure in four GPU hours on CIFAR-10, and the discovered model obtains a test error of 2.82\% with only 2.5M parameters, which is on par with the state-of-the-art. Code is publicly available on GitHub: https://github.com/D-X-Y/NAS-Projects.



### Multi-Stage Pathological Image Classification using Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.04473v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04473v1)
- **Published**: 2019-10-10 10:45:22+00:00
- **Updated**: 2019-10-10 10:45:22+00:00
- **Authors**: Shusuke Takahama, Yusuke Kurose, Yusuke Mukuta, Hiroyuki Abe, Masashi Fukayama, Akihiko Yoshizawa, Masanobu Kitagawa, Tatsuya Harada
- **Comment**: Accepted to ICCV2019. ICCV paper version
- **Journal**: None
- **Summary**: Histopathological image analysis is an essential process for the discovery of diseases such as cancer. However, it is challenging to train CNN on whole slide images (WSIs) of gigapixel resolution considering the available memory capacity. Most of the previous works divide high resolution WSIs into small image patches and separately input them into the model to classify it as a tumor or a normal tissue. However, patch-based classification uses only patch-scale local information but ignores the relationship between neighboring patches. If we consider the relationship of neighboring patches and global features, we can improve the classification performance. In this paper, we propose a new model structure combining the patch-based classification model and whole slide-scale segmentation model in order to improve the prediction performance of automatic pathological diagnosis. We extract patch features from the classification model and input them into the segmentation model to obtain a whole slide tumor probability heatmap. The classification model considers patch-scale local features, and the segmentation model can take global information into account. We also propose a new optimization method that retains gradient information and trains the model partially for end-to-end learning with limited GPU memory capacity. We apply our method to the tumor/normal prediction on WSIs and the classification performance is improved compared with the conventional patch-based method.



### Image Super-Resolution via Attention based Back Projection Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.04476v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04476v1)
- **Published**: 2019-10-10 10:49:20+00:00
- **Updated**: 2019-10-10 10:49:20+00:00
- **Authors**: Zhi-Song Liu, Li-Wen Wang, Chu-Tak Li, Wan-Chi Siu, Yui-Lam Chan
- **Comment**: 9 pages, 7 figures, ABPN
- **Journal**: IEEE International Conference on Computer Vision 2019
- **Summary**: Deep learning based image Super-Resolution (SR) has shown rapid development due to its ability of big data digestion. Generally, deeper and wider networks can extract richer feature maps and generate SR images with remarkable quality. However, the more complex network we have, the more time consumption is required for practical applications. It is important to have a simplified network for efficient image SR. In this paper, we propose an Attention based Back Projection Network (ABPN) for image super-resolution. Similar to some recent works, we believe that the back projection mechanism can be further developed for SR. Enhanced back projection blocks are suggested to iteratively update low- and high-resolution feature residues. Inspired by recent studies on attention models, we propose a Spatial Attention Block (SAB) to learn the cross-correlation across features at different layers. Based on the assumption that a good SR image should be close to the original LR image after down-sampling. We propose a Refined Back Projection Block (RBPB) for final reconstruction. Extensive experiments on some public and AIM2019 Image Super-Resolution Challenge datasets show that the proposed ABPN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.



### Semi-Supervised Variational Autoencoder for Survival Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.04488v1
- **DOI**: 10.1007/978-3-030-46643-5_12
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04488v1)
- **Published**: 2019-10-10 11:31:10+00:00
- **Updated**: 2019-10-10 11:31:10+00:00
- **Authors**: Sveinn P√°lsson, Stefano Cerri, Andrea Dittadi, Koen Van Leemput
- **Comment**: Published in the pre-conference proceeding of "2019 International
  MICCAI BraTS Challenge"
- **Journal**: None
- **Summary**: In this paper we propose a semi-supervised variational autoencoder for classification of overall survival groups from tumor segmentation masks. The model can use the output of any tumor segmentation algorithm, removing all assumptions on the scanning platform and the specific type of pulse sequences used, thereby increasing its generalization properties. Due to its semi-supervised nature, the method can learn to classify survival time by using a relatively small number of labeled subjects. We validate our model on the publicly available dataset from the Multimodal Brain Tumor Segmentation Challenge (BraTS) 2019.



### Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.04562v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04562v1)
- **Published**: 2019-10-10 13:44:18+00:00
- **Updated**: 2019-10-10 13:44:18+00:00
- **Authors**: Chufeng Tang, Lu Sheng, Zhaoxiang Zhang, Xiaolin Hu
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K.



### Machine Learning with Multi-Site Imaging Data: An Empirical Study on the Impact of Scanner Effects
- **Arxiv ID**: http://arxiv.org/abs/1910.04597v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1910.04597v1)
- **Published**: 2019-10-10 14:24:42+00:00
- **Updated**: 2019-10-10 14:24:42+00:00
- **Authors**: Ben Glocker, Robert Robinson, Daniel C. Castro, Qi Dou, Ender Konukoglu
- **Comment**: Presented at the Medical Imaging meets NeurIPS Workshop 2019
- **Journal**: None
- **Summary**: This is an empirical study to investigate the impact of scanner effects when using machine learning on multi-site neuroimaging data. We utilize structural T1-weighted brain MRI obtained from two different studies, Cam-CAN and UK Biobank. For the purpose of our investigation, we construct a dataset consisting of brain scans from 592 age- and sex-matched individuals, 296 subjects from each original study. Our results demonstrate that even after careful pre-processing with state-of-the-art neuroimaging pipelines a classifier can easily distinguish between the origin of the data with very high accuracy. Our analysis on the example application of sex classification suggests that current approaches to harmonize data are unable to remove scanner-specific bias leading to overly optimistic performance estimates and poor generalization. We conclude that multi-site data harmonization remains an open challenge and particular care needs to be taken when using such data with advanced machine learning methods for predictive modelling.



### A cost-effective method for improving and re-purposing large, pre-trained GANs by fine-tuning their class-embeddings
- **Arxiv ID**: http://arxiv.org/abs/1910.04760v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04760v4)
- **Published**: 2019-10-10 15:18:28+00:00
- **Updated**: 2020-10-08 21:46:34+00:00
- **Authors**: Qi Li, Long Mai, Michael A. Alcorn, Anh Nguyen
- **Comment**: Preprint
- **Journal**: None
- **Summary**: Large, pre-trained generative models have been increasingly popular and useful to both the research and wider communities. Specifically, BigGANs a class-conditional Generative Adversarial Networks trained on ImageNet---achieved excellent, state-of-the-art capability in generating realistic photos. However, fine-tuning or training BigGANs from scratch is practically impossible for most researchers and engineers because (1) GAN training is often unstable and suffering from mode-collapse; and (2) the training requires a significant amount of computation, 256 Google TPUs for 2 days or 8xV100 GPUs for 15 days. Importantly, many pre-trained generative models both in NLP and image domains were found to contain biases that are harmful to society. Thus, we need computationally-feasible methods for modifying and re-purposing these huge, pre-trained models for downstream tasks. In this paper, we propose a cost-effective optimization method for improving and re-purposing BigGANs by fine-tuning only the class-embedding layer. We show the effectiveness of our model-editing approach in three tasks: (1) significantly improving the realism and diversity of samples of complete mode-collapse classes; (2) re-purposing ImageNet BigGANs for generating images for Places365; and (3) de-biasing or improving the sample diversity for selected ImageNet classes.



### Visual Indeterminacy in GAN Art
- **Arxiv ID**: http://arxiv.org/abs/1910.04639v3
- **DOI**: 10.1162/leon_a_01930
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.04639v3)
- **Published**: 2019-10-10 15:19:11+00:00
- **Updated**: 2020-03-27 03:23:11+00:00
- **Authors**: Aaron Hertzmann
- **Comment**: Leonardo / SIGGRAPH 2020 Art Papers
- **Journal**: Leonardo, Volume 53, Issue 4, August 2020
- **Summary**: This paper explores visual indeterminacy as a description for artwork created with Generative Adversarial Networks (GANs). Visual indeterminacy describes images which appear to depict real scenes, but, on closer examination, defy coherent spatial interpretation. GAN models seem to be predisposed to producing indeterminate images, and indeterminacy is a key feature of much modern representational art, as well as most GAN art. It is hypothesized that indeterminacy is a consequence of a powerful-but-imperfect image synthesis model that must combine general classes of objects, scenes, and textures.



### Cross-modal knowledge distillation for action recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.04641v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04641v1)
- **Published**: 2019-10-10 15:22:53+00:00
- **Updated**: 2019-10-10 15:22:53+00:00
- **Authors**: Fida Mohammad Thoker, Juergen Gall
- **Comment**: Published in: 2019 IEEE International Conference on Image Processing
  (ICIP)
- **Journal**: None
- **Summary**: In this work, we address the problem how a network for action recognition that has been trained on a modality like RGB videos can be adapted to recognize actions for another modality like sequences of 3D human poses. To this end, we extract the knowledge of the trained teacher network for the source modality and transfer it to a small ensemble of student networks for the target modality. For the cross-modal knowledge distillation, we do not require any annotated data. Instead we use pairs of sequences of both modalities as supervision, which are straightforward to acquire. In contrast to previous works for knowledge distillation that use a KL-loss, we show that the cross-entropy loss together with mutual learning of a small ensemble of student networks performs better. In fact, the proposed approach for cross-modal knowledge distillation nearly achieves the accuracy of a student network trained with full supervision.



### AlignNet-3D: Fast Point Cloud Registration of Partially Observed Objects
- **Arxiv ID**: http://arxiv.org/abs/1910.04668v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.04668v1)
- **Published**: 2019-10-10 16:15:45+00:00
- **Updated**: 2019-10-10 16:15:45+00:00
- **Authors**: Johannes Gro√ü, Aljosa Osep, Bastian Leibe
- **Comment**: Presented at 3DV'19
- **Journal**: None
- **Summary**: Methods tackling multi-object tracking need to estimate the number of targets in the sensing area as well as to estimate their continuous state. While the majority of existing methods focus on data association, precise state (3D pose) estimation is often only coarsely estimated by approximating targets with centroids or (3D) bounding boxes. However, in automotive scenarios, motion perception of surrounding agents is critical and inaccuracies in the vehicle close-range can have catastrophic consequences. In this work, we focus on precise 3D track state estimation and propose a learning-based approach for object-centric relative motion estimation of partially observed objects. Instead of approximating targets with their centroids, our approach is capable of utilizing noisy 3D point segments of objects to estimate their motion. To that end, we propose a simple, yet effective and efficient network, \method, that learns to align point clouds. Our evaluation on two different datasets demonstrates that our method outperforms computationally expensive, global 3D registration methods while being significantly more efficient. We make our data, code, and models available at https://www.vision.rwth-aachen.de/page/alignnet.



### Non-contact Infant Sleep Apnea Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.04725v1
- **DOI**: 10.1109/ICIIS47346.2019.9063269
- **Categories**: **eess.SP**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04725v1)
- **Published**: 2019-10-10 17:34:41+00:00
- **Updated**: 2019-10-10 17:34:41+00:00
- **Authors**: Gihan Jayatilaka, Harshana Weligampola, Suren Sritharan, Pankayraj Pathmanathan, Roshan Ragel, Isuru Nawinne
- **Comment**: Gihan Jayatilaka, Harshana Weligampola and Suren Sritharan are
  equally contributing authors
- **Journal**: None
- **Summary**: Sleep apnea is a breathing disorder where a person repeatedly stops breathing in sleep. Early detection is crucial for infants because it might bring long term adversities. The existing accurate detection mechanism (pulse oximetry) is a skin contact measurement. The existing non-contact mechanisms (acoustics, video processing) are not accurate enough. This paper presents a novel algorithm for the detection of sleep apnea with video processing. The solution is non-contact, accurate and lightweight enough to run on a single board computer. The paper discusses the accuracy of the algorithm on real data, advantages of the new algorithm, its limitations and suggests future improvements.



### MetaPix: Few-Shot Video Retargeting
- **Arxiv ID**: http://arxiv.org/abs/1910.04742v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04742v2)
- **Published**: 2019-10-10 17:51:44+00:00
- **Updated**: 2020-03-24 21:09:45+00:00
- **Authors**: Jessica Lee, Deva Ramanan, Rohit Girdhar
- **Comment**: Short version accepted to NeurIPS'19 MetaLearn Workshop. Full version
  accepted to ICLR 2020. Webpage: https://imjal.github.io/MetaPix/
- **Journal**: None
- **Summary**: We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt - or personalize - a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.



### CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1910.04744v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04744v2)
- **Published**: 2019-10-10 17:52:19+00:00
- **Updated**: 2020-04-05 03:39:21+00:00
- **Authors**: Rohit Girdhar, Deva Ramanan
- **Comment**: ICLR 2020 (oral). Webpage/code/data:
  https://rohitgirdhar.github.io/CATER
- **Journal**: None
- **Summary**: Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures.



### Referring Expression Object Segmentation with Caption-Aware Consistency
- **Arxiv ID**: http://arxiv.org/abs/1910.04748v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04748v1)
- **Published**: 2019-10-10 17:55:00+00:00
- **Updated**: 2019-10-10 17:55:00+00:00
- **Authors**: Yi-Wen Chen, Yi-Hsuan Tsai, Tiantian Wang, Yen-Yu Lin, Ming-Hsuan Yang
- **Comment**: Accepted in BMVC'19, project page at
  https://github.com/wenz116/lang2seg
- **Journal**: None
- **Summary**: Referring expressions are natural language descriptions that identify a particular object within a scene and are widely used in our daily conversations. In this work, we focus on segmenting the object in an image specified by a referring expression. To this end, we propose an end-to-end trainable comprehension network that consists of the language and visual encoders to extract feature representations from both domains. We introduce the spatial-aware dynamic filters to transfer knowledge from text to image, and effectively capture the spatial information of the specified object. To better communicate between the language and visual modules, we employ a caption generation network that takes features shared across both domains as input, and improves both representations via a consistency that enforces the generated sentence to be similar to the given referring expression. We evaluate the proposed framework on two referring expression datasets and show that our method performs favorably against the state-of-the-art algorithms.



### Panoptic-DeepLab
- **Arxiv ID**: http://arxiv.org/abs/1910.04751v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04751v3)
- **Published**: 2019-10-10 17:57:19+00:00
- **Updated**: 2019-10-24 01:13:12+00:00
- **Authors**: Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, Liang-Chieh Chen
- **Comment**: This work is presented at ICCV 2019 Joint COCO and Mapillary
  Recognition Challenge Workshop
- **Journal**: None
- **Summary**: We present Panoptic-DeepLab, a bottom-up and single-shot approach for panoptic segmentation. Our Panoptic-DeepLab is conceptually simple and delivers state-of-the-art results. In particular, we adopt the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. Our single Panoptic-DeepLab sets the new state-of-art at all three Cityscapes benchmarks, reaching 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set, and advances results on the other challenging Mapillary Vistas.



### Combining Geometric and Topological Information for Boundary Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.04778v3
- **DOI**: 10.1109/BigData52589.2021.9672024
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04778v3)
- **Published**: 2019-10-10 18:00:10+00:00
- **Updated**: 2021-01-02 23:27:16+00:00
- **Authors**: Hengrui Luo, Justin Strait
- **Comment**: 38 pages with appendices, 15 figures
- **Journal**: 2021 IEEE International Conference on Big Data (Big Data)
- **Summary**: A fundamental problem in computer vision is boundary estimation, where the goal is to delineate the boundary of objects in an image. In this paper, we propose a method which jointly incorporates geometric and topological information within an image to simultaneously estimate boundaries for objects within images with more complex topologies. We use a topological clustering-based method to assist initialization of the Bayesian active contour model. This combines pixel clustering, boundary smoothness, and potential prior shape information to produce an estimated object boundary. Active contour methods are knownto be extremely sensitive to algorithm initialization, relying on the user to provide a reasonable starting curve to the algorithm. In the presence of images featuring objects with complex topological structures, such as objects with holes or multiple objects, the user must initialize separate curves for each boundary of interest. Our proposed topologically-guided method can provide an interpretable, smart initialization in these settings, freeing up the user from potential pitfalls associated with objects of complex topological structure. We provide a detailed simulation study comparing our initialization to boundary estimates obtained from standard segmentation algorithms. The method is demonstrated on artificial image datasets from computer vision, as well as real-world applications to skin lesion and neural cellular images, for which multiple topological features can be identified.



### Unsupervised video summarization framework using keyframe extraction and video skimming
- **Arxiv ID**: http://arxiv.org/abs/1910.04792v2
- **DOI**: 10.1109/ICCCA49541.2020.9250764
- **Categories**: **cs.IR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04792v2)
- **Published**: 2019-10-10 18:14:48+00:00
- **Updated**: 2020-06-30 18:27:25+00:00
- **Authors**: Shruti Jadon, Mahmood Jasim
- **Comment**: 5 pages, 3 figures. Technical Report
- **Journal**: 2020 IEEE 5th International Conference on Computing Communication
  and Automation (ICCCA)
- **Summary**: Video is one of the robust sources of information and the consumption of online and offline videos has reached an unprecedented level in the last few years. A fundamental challenge of extracting information from videos is a viewer has to go through the complete video to understand the context, as opposed to an image where the viewer can extract information from a single frame. Apart from context understanding, it almost impossible to create a universal summarized video for everyone, as everyone has their own bias of keyframe, e.g; In a soccer game, a coach person might consider those frames which consist of information on player placement, techniques, etc; however, a person with less knowledge about a soccer game, will focus more on frames which consist of goals and score-board. Therefore, if we were to tackle problem video summarization through a supervised learning path, it will require extensive personalized labeling of data. In this paper, we attempt to solve video summarization through unsupervised learning by employing traditional vision-based algorithmic methodologies for accurate feature extraction from video frames. We have also proposed a deep learning-based feature extraction followed by multiple clustering methods to find an effective way of summarizing a video by interesting key-frame extraction. We have compared the performance of these approaches on the SumMe dataset and showcased that using deep learning-based feature extraction has been proven to perform better in case of dynamic viewpoint videos.



### Dynamic Spectral Residual Superpixels
- **Arxiv ID**: http://arxiv.org/abs/1910.04794v2
- **DOI**: 10.1016/j.patcog.2020.107705
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04794v2)
- **Published**: 2019-10-10 18:15:30+00:00
- **Updated**: 2021-04-02 17:01:11+00:00
- **Authors**: Jianchao Zhang, Angelica I. Aviles-Rivero, Daniel Heydecker, Xiaosheng Zhuang, Raymond Chan, Carola-Bibiane Sch√∂nlieb
- **Comment**: None
- **Journal**: Zhang, J., Aviles-Rivero, A. I., Heydecker, D., Zhuang, X., Chan,
  R., & Schonlieb, C. B. (2021). Dynamic spectral residual superpixels. Pattern
  Recognition, 112, 107705
- **Summary**: We consider the problem of segmenting an image into superpixels in the context of $k$-means clustering, in which we wish to decompose an image into local, homogeneous regions corresponding to the underlying objects. Our novel approach builds upon the widely used Simple Linear Iterative Clustering (SLIC), and incorporate a measure of objects' structure based on the spectral residual of an image. Based on this combination, we propose a modified initialisation scheme and search metric, which helps keeps fine-details. This combination leads to better adherence to object boundaries, while preventing unnecessary segmentation of large, uniform areas, while remaining computationally tractable in comparison to other methods. We demonstrate through numerical and visual experiments that our approach outperforms the state-of-the-art techniques.



### CompareNet: Anatomical Segmentation Network with Deep Non-local Label Fusion
- **Arxiv ID**: http://arxiv.org/abs/1910.04797v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04797v1)
- **Published**: 2019-10-10 18:23:38+00:00
- **Updated**: 2019-10-10 18:23:38+00:00
- **Authors**: Yuan Liang, Weinan Song, J. P. Dym, Kun Wang, Lei He
- **Comment**: None
- **Journal**: None
- **Summary**: Label propagation is a popular technique for anatomical segmentation. In this work, we propose a novel deep framework for label propagation based on non-local label fusion. Our framework, named CompareNet, incorporates subnets for both extracting discriminating features, and learning the similarity measure, which lead to accurate segmentation. We also introduce the voxel-wise classification as an unary potential to the label fusion function, for alleviating the search failure issue of the existing non-local fusion strategies. Moreover, CompareNet is end-to-end trainable, and all the parameters are learnt together for the optimal performance. By evaluating CompareNet on two public datasets IBSRv2 and MICCAI 2012 for brain segmentation, we show it outperforms state-of-the-art methods in accuracy, while being robust to pathologies.



### ErrorNet: Learning error representations from limited data to improve vascular segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.04814v4
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04814v4)
- **Published**: 2019-10-10 18:54:15+00:00
- **Updated**: 2020-02-02 00:40:05+00:00
- **Authors**: Nima Tajbakhsh, Brian Lai, Shilpa Ananth, Xiaowei Ding
- **Comment**: Accepted in ISBI 2019. The supplementary material is only available
  in the arxiv version of our paper
- **Journal**: None
- **Summary**: Deep convolutional neural networks have proved effective in segmenting lesions and anatomies in various medical imaging modalities. However, in the presence of small sample size and domain shift problems, these models often produce masks with non-intuitive segmentation mistakes. In this paper, we propose a segmentation framework called ErrorNet, which learns to correct these segmentation mistakes through the repeated process of injecting systematic segmentation errors to the segmentation result based on a learned shape prior, followed by attempting to predict the injected error. During inference, ErrorNet corrects the segmentation mistakes by adding the predicted error map to the initial segmentation result. ErrorNet has advantages over alternatives based on domain adaptation or CRF-based post processing, because it requires neither domain-specific parameter tuning nor any data from the target domains. We have evaluated ErrorNet using five public datasets for the task of retinal vessel segmentation. The selected datasets differ in size and patient population, allowing us to evaluate the effectiveness of ErrorNet in handling small sample size and domain shift problems. Our experiments demonstrate that ErrorNet outperforms a base segmentation model, a CRF-based post processing scheme, and a domain adaptation method, with a greater performance gain in the presence of the aforementioned dataset limitations.



### Self Driving RC Car using Behavioral Cloning
- **Arxiv ID**: http://arxiv.org/abs/1910.06734v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1910.06734v1)
- **Published**: 2019-10-10 19:03:37+00:00
- **Updated**: 2019-10-10 19:03:37+00:00
- **Authors**: Aliasgar Haji, Priyam Shah, Srinivas Bijoor
- **Comment**: 4 pages, 8 figures
- **Journal**: None
- **Summary**: Self Driving Car technology is a vehicle that guides itself without human conduction. The first truly autonomous cars appeared in the 1980s with projects funded by DARPA( Defense Advance Research Project Agency ). Since then a lot has changed with the improvements in the fields of Computer Vision and Machine Learning. We have used the concept of behavioral cloning to convert a normal RC model car into an autonomous car using Deep Learning technology



### Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities
- **Arxiv ID**: http://arxiv.org/abs/1910.04887v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04887v1)
- **Published**: 2019-10-10 21:46:26+00:00
- **Updated**: 2019-10-10 21:46:26+00:00
- **Authors**: Samuel Sharpe, Jin Yan, Fan Wu, Iddo Drori
- **Comment**: None
- **Journal**: CVPR Language and Vision Workshop, 2019
- **Summary**: We present a new task of query auto-completion for estimating instance probabilities. We complete a user query prefix conditioned upon an image. Given the complete query, we fine tune a BERT embedding for estimating probabilities of a broad set of instances. The resulting instance probabilities are used for selection while being agnostic to the segmentation or attention mechanism. Our results demonstrate that auto-completion using both language and vision performs better than using only language, and that fine tuning a BERT embedding allows to efficiently rank instances in the image. In the spirit of reproducible research we make our data, models, and code available.



### Coloring the Black Box: Visualizing neural network behavior with a self-introspective model
- **Arxiv ID**: http://arxiv.org/abs/1910.04903v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, 68T10, 68T37, 68T99, I.2
- **Links**: [PDF](http://arxiv.org/pdf/1910.04903v2)
- **Published**: 2019-10-10 23:02:12+00:00
- **Updated**: 2019-10-15 10:41:04+00:00
- **Authors**: Arturo Pardo, Jos√© A. Guti√©rrez-Guti√©rrez, Jos√© Miguel L√≥pez-Higuera, Brian W. Pogue, Olga M. Conde
- **Comment**: 16 pages, 7 figures
- **Journal**: None
- **Summary**: The following work presents how autoencoding all the possible hidden activations of a network for a given problem can provide insight about its structure, behavior, and vulnerabilities. The method, termed self-introspection, can show that a trained model showcases similar activation patterns (albeit randomly distributed due to initialization) when shown data belonging to the same category, and classification errors occur in fringe areas where the activations are not as clearly defined, suggesting some form of random, slowly varying, implicit encoding occurring within deep networks, that can be observed with this representation. Additionally, obtaining a low-dimensional representation of all the activations allows for (1) real-time model evaluation in the context of a multiclass classification problem, (2) the rearrangement of all hidden layers by their relevance in obtaining a specific output, and (3) the obtainment of a framework where studying possible counter-measures to noise and adversarial attacks is possible. Self-introspection can show how damaged input data can modify the hidden activations, producing an erroneous response. A few illustrative are implemented for feedforward and convolutional models and the MNIST and CIFAR-10 datasets, showcasing its capabilities as a model evaluation framework.



