# Arxiv Papers in cs.CV on 2019-10-31
### Automatic Cobb Angle Detection using Vertebra Detector and Vertebra Corners Regression
- **Arxiv ID**: http://arxiv.org/abs/1910.14202v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14202v1)
- **Published**: 2019-10-31 01:20:28+00:00
- **Updated**: 2019-10-31 01:20:28+00:00
- **Authors**: Bidur Khanal, Lavsen Dahal, Prashant Adhikari, Bishesh Khanal
- **Comment**: Accepted to MICCAI 2019 CSI Workshop & Challenge: Computational
  Methods and Clinical Applications for Spine Imaging
- **Journal**: None
- **Summary**: Correct evaluation and treatment of Scoliosis require accurate estimation of spinal curvature. Current gold standard is to manually estimate Cobb Angles in spinal X-ray images which is time consuming and has high inter-rater variability. We propose an automatic method with a novel framework that first detects vertebrae as objects followed by a landmark detector that estimates the 4 landmark corners of each vertebra separately. Cobb Angles are calculated using the slope of each vertebra obtained from the predicted landmarks. For inference on test data, we perform pre and post processings that include cropping, outlier rejection and smoothing of the predicted landmarks. The results were assessed in AASCE MICCAI challenge 2019 which showed a promise with a SMAPE score of 25.69 on the challenge test set.



### Multi-defect microscopy image restoration under limited data conditions
- **Arxiv ID**: http://arxiv.org/abs/1910.14207v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14207v2)
- **Published**: 2019-10-31 01:55:01+00:00
- **Updated**: 2020-11-30 20:14:19+00:00
- **Authors**: Anastasia Razdaibiedina, Jeevaa Velayutham, Miti Modi
- **Comment**: NeurIPS 2019 Medical Imaging workhop
- **Journal**: None
- **Summary**: Deep learning methods are becoming widely used for restoration of defects associated with fluorescence microscopy imaging. One of the major challenges in application of such methods is the availability of training data. In this work, we propose a unified method for reconstruction of multi-defect fluorescence microscopy images when training data is limited. Our approach consists of two stages: first, we perform data augmentation using Generative Adversarial Network (GAN) with conditional instance normalization (CIN); second, we train a conditional GAN (cGAN) on paired ground-truth and defected images to perform restoration. The experiments on three common types of imaging defects with different amounts of training data show that the proposed method gives comparable results or outperforms CARE, deblurGAN and CycleGAN in restored image quality when available data is limited.



### Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder
- **Arxiv ID**: http://arxiv.org/abs/1910.14208v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.14208v2)
- **Published**: 2019-10-31 01:56:33+00:00
- **Updated**: 2020-01-14 19:21:02+00:00
- **Authors**: Jialin Wu, Raymond J. Mooney
- **Comment**: None
- **Journal**: None
- **Summary**: Most RNN-based image captioning models receive supervision on the output words to mimic human captions. Therefore, the hidden states can only receive noisy gradient signals via layers of back-propagation through time, leading to less accurate generated captions. Consequently, we propose a novel framework, Hidden State Guidance (HSG), that matches the hidden states in the caption decoder to those in a teacher decoder trained on an easier task of autoencoding the captions conditioned on the image. During training with the REINFORCE algorithm, the conventional rewards are sentence-based evaluation metrics equally distributed to each generated word, no matter their relevance. HSG provides a word-level reward that helps the model learn better hidden representations. Experimental results demonstrate that HSG clearly outperforms various state-of-the-art caption decoders using either raw images or detected objects as inputs.



### A Near-Optimal Gradient Flow for Learning Neural Energy-Based Models
- **Arxiv ID**: http://arxiv.org/abs/1910.14216v7
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14216v7)
- **Published**: 2019-10-31 02:26:20+00:00
- **Updated**: 2023-04-28 16:03:06+00:00
- **Authors**: Yang Wu, Pengxu Wei, Liang Lin
- **Comment**: 42 pages
- **Journal**: None
- **Summary**: In this paper, we propose a novel numerical scheme to optimize the gradient flows for learning energy-based models (EBMs). From a perspective of physical simulation, we redefine the problem of approximating the gradient flow utilizing optimal transport (i.e. Wasserstein) metric. In EBMs, the learning process of stepwise sampling and estimating data distribution performs the functional gradient of minimizing the global relative entropy between the current and target real distribution, which can be treated as dynamic particles moving from disorder to target manifold. Previous learning schemes mainly minimize the entropy concerning the consecutive time KL divergence in each learning step. However, they are prone to being stuck in the local KL divergence by projecting non-smooth information within smooth manifold, which is against the optimal transport principle. To solve this problem, we derive a second-order Wasserstein gradient flow of the global relative entropy from Fokker-Planck equation. Compared with existing schemes, Wasserstein gradient flow is a smoother and near-optimal numerical scheme to approximate real data densities. We also derive this near-proximal scheme and provide its numerical computation equations. Our extensive experiments demonstrate the practical superiority and potentials of our proposed scheme on fitting complex distributions and generating high-quality, high-dimensional data with neural EBMs.



### S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes
- **Arxiv ID**: http://arxiv.org/abs/1910.14218v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14218v1)
- **Published**: 2019-10-31 02:29:57+00:00
- **Updated**: 2019-10-31 02:29:57+00:00
- **Authors**: Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, Hao Su
- **Comment**: Accepted at the Conference on Robot Learning (CoRL) 2019. Project
  webpage is available here: https://sites.google.com/view/s4ggrapsing
- **Journal**: None
- **Summary**: Grasping is among the most fundamental and long-lasting problems in robotics study. This paper studies the problem of 6-DoF(degree of freedom) grasping by a parallel gripper in a cluttered scene captured using a commodity depth sensor from a single viewpoint. We address the problem in a learning-based framework. At the high level, we rely on a single-shot grasp proposal network, trained with synthetic data and tested in real-world scenarios. Our single-shot neural network architecture can predict amodal grasp proposal efficiently and effectively. Our training data synthesis pipeline can generate scenes of complex object configuration and leverage an innovative gripper contact model to create dense and high-quality grasp annotations. Experiments in synthetic and real environments have demonstrated that the proposed approach can outperform state-of-the-arts by a large margin.



### Distilling Pixel-Wise Feature Similarities for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.14226v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14226v1)
- **Published**: 2019-10-31 02:59:51+00:00
- **Updated**: 2019-10-31 02:59:51+00:00
- **Authors**: Yuhu Shan
- **Comment**: None
- **Journal**: None
- **Summary**: Among the neural network compression techniques, knowledge distillation is an effective one which forces a simpler student network to mimic the output of a larger teacher network. However, most of such model distillation methods focus on the image-level classification task. Directly adapting these methods to the task of semantic segmentation only brings marginal improvements. In this paper, we propose a simple, yet effective knowledge representation referred to as pixel-wise feature similarities (PFS) to tackle the challenging distillation problem of semantic segmentation. The developed PFS encodes spatial structural information for each pixel location of the high-level convolutional features, which helps guide the distillation process in an easier way. Furthermore, a novel weighted pixel-level soft prediction imitation approach is proposed to enable the student network to selectively mimic the teacher network's output, according to their pixel-wise knowledge-gaps. Extensive experiments are conducted on the challenging datasets of Pascal VOC 2012, ADE20K and Pascal Context. Our approach brings significant performance improvements compared to several strong baselines and achieves new state-of-the-art results.



### Dynamic Regularizer with an Informative Prior
- **Arxiv ID**: http://arxiv.org/abs/1910.14241v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14241v1)
- **Published**: 2019-10-31 03:40:03+00:00
- **Updated**: 2019-10-31 03:40:03+00:00
- **Authors**: Avinash Kori, Manik Sharma
- **Comment**: None
- **Journal**: None
- **Summary**: Regularization methods, specifically those which directly alter weights like $L_1$ and $L_2$, are an integral part of many learning algorithms. Both the regularizers mentioned above are formulated by assuming certain priors in the parameter space and these assumptions, in some cases, induce sparsity in the parameter space. Regularizers help in transferring beliefs one has on the dataset or the parameter space by introducing adequate terms in the loss function. Any kind of formulation represents a specific set of beliefs: $L_1$ regularization conveys that the parameter space should be sparse whereas $L_2$ regularization conveys that the parameter space should be bounded and continuous. These regularizers in turn leverage certain priors to express these inherent beliefs. A better understanding of how the prior affects the behavior of the parameters and how the priors can be updated based on the dataset can contribute greatly in improving the generalization capabilities of a function estimator. In this work, we introduce a weakly informative prior and then further extend it to an informative prior in order to formulate a regularization penalty, which shows better results in terms of inducing sparsity experimentally, when compared to regularizers based only on Gaussian and Laplacian priors. Experimentally, we verify that a regularizer based on an adapted prior improves the generalization capabilities of any network. We illustrate the performance of the proposed method on the MNIST and CIFAR-10 datasets.



### Cross-Domain Face Synthesis using a Controllable GAN
- **Arxiv ID**: http://arxiv.org/abs/1910.14247v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14247v1)
- **Published**: 2019-10-31 04:16:10+00:00
- **Updated**: 2019-10-31 04:16:10+00:00
- **Authors**: Fania Mokhayeri, Kaveh Kamali, Eric Granger
- **Comment**: None
- **Journal**: Winter Conference on Applications of Computer Vision (WACV 2020)
- **Summary**: The performance of face recognition (FR) systems applied in video surveillance has been shown to improve when the design data is augmented through synthetic face generation. This is true, for instance, with pair-wise matchers (e.g., deep Siamese networks) that typically rely on a reference gallery with one still image per individual. However, generating synthetic images in the source domain may not improve the performance during operations due to the domain shift w.r.t. the target domain. Moreover, despite the emergence of Generative Adversarial Networks (GANs) for realistic synthetic generation, it is often difficult to control the conditions under which synthetic faces are generated. In this paper, a cross-domain face synthesis approach is proposed that integrates a new Controllable GAN (C-GAN). It employs an off-the-shelf 3D face model as a simulator to generate face images under various poses. The simulated images and noise are input to the C-GAN for realism refinement which employs an additional adversarial game as a third player to preserve the identity and specific facial attributes of the refined images. This allows generating realistic synthetic face images that reflects capture conditions in the target domain while controlling the GAN output to generate faces under desired pose conditions. Experiments were performed using videos from the Chokepoint and COX-S2V datasets, and a deep Siamese network for FR with a single reference still per person. Results indicate that the proposed approach can provide a higher level of accuracy compared to the current state-of-the-art approaches for synthetic data augmentation.



### A Review of methods for Textureless Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.14255v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14255v1)
- **Published**: 2019-10-31 04:39:30+00:00
- **Updated**: 2019-10-31 04:39:30+00:00
- **Authors**: Frincy Clement, Kirtan Shah, Dhara Pancholi
- **Comment**: 25 pages
- **Journal**: None
- **Summary**: Textureless object recognition has become a significant task in Computer Vision with the advent of Robotics and its applications in manufacturing sector. It has been very challenging to get good performance because of its lack of discriminative features and reflectance properties. Hence, the approaches used for textured objects cannot be applied for textureless objects. A lot of work has been done in the last 20 years, especially in the recent 5 years after the TLess and other textureless dataset were introduced. In our research, we plan to combine image processing techniques (for feature enhancement) along with deep learning techniques (for object recognition). Here we present an overview of the various existing work in the field of textureless object recognition, which can be broadly classified into View-based, Feature-based and Shape-based. We have also added a review of few of the research papers submitted at the International Conference on Smart Multimedia, 2018. Index terms: Computer Vision, Textureless object detection, Textureless object recognition, Feature-based, Edge detection, Deep Learning



### A Self Validation Network for Object-Level Human Attention Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.14260v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14260v2)
- **Published**: 2019-10-31 04:56:43+00:00
- **Updated**: 2019-12-13 19:06:13+00:00
- **Authors**: Zehua Zhang, Chen Yu, David Crandall
- **Comment**: Accepted by NeurIPS 2019
- **Journal**: None
- **Summary**: Due to the foveated nature of the human vision system, people can focus their visual attention on a small region of their visual field at a time, which usually contains only a single object. Estimating this object of attention in first-person (egocentric) videos is useful for many human-centered real-world applications such as augmented reality applications and driver assistance systems. A straightforward solution for this problem is to pick the object whose bounding box is hit by the gaze, where eye gaze point estimation is obtained from a traditional eye gaze estimator and object candidates are generated from an off-the-shelf object detector. However, such an approach can fail because it addresses the where and the what problems separately, despite that they are highly related, chicken-and-egg problems. In this paper, we propose a novel unified model that incorporates both spatial and temporal evidence in identifying as well as locating the attended object in firstperson videos. It introduces a novel Self Validation Module that enforces and leverages consistency of the where and the what concepts. We evaluate on two public datasets, demonstrating that Self Validation Module significantly benefits both training and testing and that our model outperforms the state-of-the-art.



### Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.14303v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14303v1)
- **Published**: 2019-10-31 08:33:25+00:00
- **Updated**: 2019-10-31 08:33:25+00:00
- **Authors**: Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, Wenwu Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Temporal sentence grounding in videos aims to detect and localize one target video segment, which semantically corresponds to a given sentence. Existing methods mainly tackle this task via matching and aligning semantics between a sentence and candidate video segments, while neglect the fact that the sentence information plays an important role in temporally correlating and composing the described contents in videos. In this paper, we propose a novel semantic conditioned dynamic modulation (SCDM) mechanism, which relies on the sentence semantics to modulate the temporal convolution operations for better correlating and composing the sentence related video contents over time. More importantly, the proposed SCDM performs dynamically with respect to the diverse video contents so as to establish a more precise matching relationship between sentence and video, thereby improving the temporal grounding accuracy. Extensive experiments on three public datasets demonstrate that our proposed model outperforms the state-of-the-arts with clear margins, illustrating the ability of SCDM to better associate and localize relevant video contents for temporal sentence grounding. Our code for this paper is available at https://github.com/yytzsy/SCDM .



### Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images
- **Arxiv ID**: http://arxiv.org/abs/1911.07937v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.07937v2)
- **Published**: 2019-10-31 09:14:28+00:00
- **Updated**: 2019-12-02 16:19:18+00:00
- **Authors**: Talip Ucar
- **Comment**: 10 pages, 15 figures. In the second version of the paper, a link to a
  demo site is added under Figure-12
- **Journal**: None
- **Summary**: Using generative models for Inverse Graphics is an active area of research. However, most works focus on developing models for supervised and semi-supervised methods. In this paper, we study the problem of unsupervised learning of 3D geometry from single images. Our approach is to use a generative model that produces 2-D images as projections of a latent 3D voxel grid, which we train either as a variational auto-encoder or using adversarial methods. Our contributions are as follows: First, we show how to recover 3D shape and pose from general datasets such as MNIST, and MNIST Fashion in good quality. Second, we compare the shapes learned using adversarial and variational methods. Adversarial approach gives denser 3D shapes. Third, we explore the idea of modelling the pose of an object as uniform distribution to recover 3D shape from a single image. Our experiment with the CelebA dataset \cite{liu2015faceattributes} proves that we can recover complete 3D shape from a single image when the object is symmetric along one, or more axis whilst results obtained using ModelNet40 \cite{wu20153d} show the potential side-effects, in which the model learns 3D shapes such that it can render the same image from any viewpoint. Forth, we present a general end-to-end approach to learning 3D shapes from single images in a completely unsupervised fashion by modelling the factors of variation such as azimuth as independent latent variables. Our method makes no assumptions about the dataset, and can work with synthetic as well as real images (i.e. unsupervised in true sense). We present our results, by training the model using the $\mu$-VAE objective \cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST Fashion, CelebA and six categories of ModelNet40. The model is able to learn 3D shapes and the pose in qood quality and leverages information learned across all datasets.



### On the Proof of Fixed-Point Convergence for Plug-and-Play ADMM
- **Arxiv ID**: http://arxiv.org/abs/1910.14325v1
- **DOI**: 10.1109/LSP.2019.2950611
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14325v1)
- **Published**: 2019-10-31 09:19:06+00:00
- **Updated**: 2019-10-31 09:19:06+00:00
- **Authors**: Ruturaj G. Gavaskar, Kunal N. Chaudhury
- **Comment**: Accepted in IEEE Signal Processing Letters
- **Journal**: None
- **Summary**: In most state-of-the-art image restoration methods, the sum of a data-fidelity and a regularization term is optimized using an iterative algorithm such as ADMM (alternating direction method of multipliers). In recent years, the possibility of using denoisers for regularization has been explored in several works. A popular approach is to formally replace the proximal operator within the ADMM framework with some powerful denoiser. However, since most state-of-the-art denoisers cannot be posed as a proximal operator, one cannot guarantee the convergence of these so-called plug-and-play (PnP) algorithms. In fact, the theoretical convergence of PnP algorithms is an active research topic. In this letter, we consider the result of Chan et al. (IEEE TCI, 2017), where fixed-point convergence of an ADMM-based PnP algorithm was established for a class of denoisers. We argue that the original proof is incomplete, since convergence is not analyzed for one of the three possible cases outlined in the paper. Moreover, we explain why the argument for the other cases does not apply in this case. We give a different analysis to fill this gap, which firmly establishes the original convergence theorem.



### Weakly Supervised Tracklet Person Re-Identification by Deep Feature-wise Mutual Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.14333v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14333v1)
- **Published**: 2019-10-31 09:37:19+00:00
- **Updated**: 2019-10-31 09:37:19+00:00
- **Authors**: Zhirui Chen, Jianheng Li, Wei-Shi Zheng
- **Comment**: None
- **Journal**: None
- **Summary**: The scalability problem caused by the difficulty in annotating Person Re-identification(Re-ID) datasets has become a crucial bottleneck in the development of Re-ID.To address this problem, many unsupervised Re-ID methods have recently been proposed.Nevertheless, most of these models require transfer from another auxiliary fully supervised dataset, which is still expensive to obtain.In this work, we propose a Re-ID model based on Weakly Supervised Tracklets(WST) data from various camera views, which can be inexpensively acquired by combining the fragmented tracklets of the same person in the same camera view over a period of time.We formulate our weakly supervised tracklets Re-ID model by a novel method, named deep feature-wise mutual learning(DFML), which consists of Mutual Learning on Feature Extractors (MLFE) and Mutual Learning on Feature Classifiers (MLFC).We propose MLFE by leveraging two feature extractors to learn from each other to extract more robust and discriminative features.On the other hand, we propose MLFC by adapting discriminative features from various camera views to each classifier. Extensive experiments demonstrate the superiority of our proposed DFML over the state-of-the-art unsupervised models and even some supervised models on three Re-ID benchmark datasets.



### Image-Guided Depth Upsampling via Hessian and TV Priors
- **Arxiv ID**: http://arxiv.org/abs/1910.14377v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14377v1)
- **Published**: 2019-10-31 11:19:22+00:00
- **Updated**: 2019-10-31 11:19:22+00:00
- **Authors**: Alireza Ahrabian, Joao F. C. Mota, Andrew M. Wallace
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method that combines sparse depth (LiDAR) measurements with an intensity image and to produce a dense high-resolution depth image. As there are few, but accurate, depth measurements from the scene, our method infers the remaining depth values by incorporating information from the intensity image, namely the magnitudes and directions of the identified edges, and by assuming that the scene is composed mostly of flat surfaces. Such inference is achieved by solving a convex optimisation problem with properly weighted regularisers that are based on the `1-norm (specifically, on total variation). We solve the resulting problem with a computationally efficient ADMM-based algorithm. Using the SYNTHIA and KITTI datasets, our experiments show that the proposed method achieves a depth reconstruction performance comparable to or better than other model-based methods.



### LiDAR-Flow: Dense Scene Flow Estimation from Sparse LiDAR and Stereo Images
- **Arxiv ID**: http://arxiv.org/abs/1910.14453v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.14453v2)
- **Published**: 2019-10-31 13:22:19+00:00
- **Updated**: 2019-12-13 16:35:50+00:00
- **Authors**: Ramy Battrawy, René Schuster, Oliver Wasenmüller, Qing Rao, Didier Stricker
- **Comment**: Accepted in IROS19
- **Journal**: None
- **Summary**: We propose a new approach called LiDAR-Flow to robustly estimate a dense scene flow by fusing a sparse LiDAR with stereo images. We take the advantage of the high accuracy of LiDAR to resolve the lack of information in some regions of stereo images due to textureless objects, shadows, ill-conditioned light environment and many more. Additionally, this fusion can overcome the difficulty of matching unstructured 3D points between LiDAR-only scans. Our LiDAR-Flow approach consists of three main steps; each of them exploits LiDAR measurements. First, we build strong seeds from LiDAR to enhance the robustness of matches between stereo images. The imagery part seeks the motion matches and increases the density of scene flow estimation. Then, a consistency check employs LiDAR seeds to remove the possible mismatches. Finally, LiDAR measurements constraint the edge-preserving interpolation method to fill the remaining gaps. In our evaluation we investigate the individual processing steps of our LiDAR-Flow approach and demonstrate the superior performance compared to image-only approach.



### Continual Unsupervised Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.14481v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14481v1)
- **Published**: 2019-10-31 14:18:45+00:00
- **Updated**: 2019-10-31 14:18:45+00:00
- **Authors**: Dushyant Rao, Francesco Visin, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu, Raia Hadsell
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.



### Towards vision-based robotic skins: a data-driven, multi-camera tactile sensor
- **Arxiv ID**: http://arxiv.org/abs/1910.14526v3
- **DOI**: 10.1109/RoboSoft48309.2020.9116060
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14526v3)
- **Published**: 2019-10-31 15:10:20+00:00
- **Updated**: 2020-06-23 12:52:47+00:00
- **Authors**: Camill Trueeb, Carmelo Sferrazza, Raffaello D'Andrea
- **Comment**: Accompanying video: https://youtu.be/lbavqAlKl98
- **Journal**: Proceedings of the 2020 3rd IEEE International Conference on Soft
  Robotics (RoboSoft), pp. 333-338
- **Summary**: This paper describes the design of a multi-camera optical tactile sensor that provides information about the contact force distribution applied to its soft surface. This information is contained in the motion of spherical particles spread within the surface, which deforms when subject to force. The small embedded cameras capture images of the different particle patterns that are then mapped to the three-dimensional contact force distribution through a machine learning architecture. The design proposed in this paper exhibits a larger contact surface and a thinner structure than most of the existing camera-based tactile sensors, without the use of additional reflecting components such as mirrors. A modular implementation of the learning architecture is discussed that facilitates the scalability to larger surfaces such as robotic skins.



### AQUALOC: An Underwater Dataset for Visual-Inertial-Pressure Localization
- **Arxiv ID**: http://arxiv.org/abs/1910.14532v1
- **DOI**: 10.1177/0278364919883346
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.14532v1)
- **Published**: 2019-10-31 15:28:08+00:00
- **Updated**: 2019-10-31 15:28:08+00:00
- **Authors**: Maxime Ferrera, Vincent Creuze, Julien Moras, Pauline Trouvé-Peloux
- **Comment**: The International Journal of Robotics Research, SAGE Publications,
  2019
- **Journal**: None
- **Summary**: We present a new dataset, dedicated to the development of simultaneous localization and mapping methods for underwater vehicles navigating close to the seabed. The data sequences composing this dataset are recorded in three different environments: a harbor at a depth of a few meters, a first archaeological site at a depth of 270 meters and a second site at a depth of 380 meters. The data acquisition is performed using Remotely Operated Vehicles equipped with a monocular monochromatic camera, a low-cost inertial measurement unit, a pressure sensor and a computing unit, all embedded in a single enclosure. The sensors' measurements are recorded synchronously on the computing unit and seventeen sequences have been created from all the acquired data. These sequences are made available in the form of ROS bags and as raw data. For each sequence, a trajectory has also been computed offline using a Structure-from-Motion library in order to allow the comparison with real-time localization methods. With the release of this dataset, we wish to provide data difficult to acquire and to encourage the development of vision-based localization methods dedicated to the underwater environment. The dataset can be downloaded from: http://www.lirmm.fr/aqualoc/



### On the Interaction Between Deep Detectors and Siamese Trackers in Video Surveillance
- **Arxiv ID**: http://arxiv.org/abs/1910.14552v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14552v1)
- **Published**: 2019-10-31 15:52:51+00:00
- **Updated**: 2019-10-31 15:52:51+00:00
- **Authors**: Madhu Kiran, Vivek Tiwari, Le Thanh Nguyen-Meidine, Eric Granger
- **Comment**: Presented in AVSS-2019 Conference
- **Journal**: None
- **Summary**: Visual object tracking is an important function in many real-time video surveillance applications, such as localization and spatio-temporal recognition of persons. In real-world applications, an object detector and tracker must interact on a periodic basis to discover new objects, and thereby to initiate tracks. Periodic interactions with the detector can also allow the tracker to validate and/or update its object template with new bounding boxes. However, bounding boxes provided by a state-of-the-art detector are noisy, due to changes in appearance, background and occlusion, which can cause the tracker to drift. Moreover, CNN-based detectors can provide a high level of accuracy at the expense of computational complexity, so interactions should be minimized for real-time applications.   In this paper, a new approach is proposed to manage detector-tracker interactions for trackers from the Siamese-FC family. By integrating a change detection mechanism into a deep Siamese-FC tracker, its template can be adapted in response to changes in a target's appearance that lead to drifts during tracking. An abrupt change detection triggers an update of tracker template using the bounding box produced by the detector, while in the case of a gradual change, the detector is used to update an evolving set of templates for robust matching.   Experiments were performed using state-of-the-art Siamese-FC trackers and the YOLOv3 detector on a subset of videos from the OTB-100 dataset that mimic video surveillance scenarios. Results highlight the importance for reliable VOT of using accurate detectors. They also indicate that our adaptive Siamese trackers are robust to noisy object detections, and can significantly improve the performance of Siamese-FC tracking.



### Visual Appearance Based Person Retrieval in Unconstrained Environment Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.14565v1
- **DOI**: 10.1016/j.imavis.2019.10.002
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14565v1)
- **Published**: 2019-10-31 16:20:33+00:00
- **Updated**: 2019-10-31 16:20:33+00:00
- **Authors**: Hiren Galiyawala, Mehul S Raval, Shivansh Dave
- **Comment**: 11 pages
- **Journal**: Image and Vision Computing, 2019
- **Summary**: Visual appearance-based person retrieval is a challenging problem in surveillance. It uses attributes like height, cloth color, cloth type and gender to describe a human. Such attributes are known as soft biometrics. This paper proposes person retrieval from surveillance video using height, torso cloth type, torso cloth color and gender. The approach introduces an adaptive torso patch extraction and bounding box regression to improve the retrieval. The algorithm uses fine-tuned Mask R-CNN and DenseNet-169 for person detection and attribute classification respectively. The performance is analyzed on AVSS 2018 challenge II dataset and it achieves 11.35% improvement over state-of-the-art based on average Intersection over Union measure.



### Conditional Denoising of Remote Sensing Imagery Using Cycle-Consistent Deep Generative Models
- **Arxiv ID**: http://arxiv.org/abs/1910.14567v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14567v1)
- **Published**: 2019-10-31 16:24:30+00:00
- **Updated**: 2019-10-31 16:24:30+00:00
- **Authors**: Michael Zotov, Jevgenij Gamper
- **Comment**: Accepted NeurIPS AI for Social Good, 14 December 2019
- **Journal**: None
- **Summary**: The potential of using remote sensing imagery for environmental modelling and for providing real time support to humanitarian operations such as hurricane relief efforts is well established. These applications are substantially affected by missing data due to non-structural noise such as clouds, shadows and other atmospheric effects. In this work we probe the potential of applying a cycle-consistent latent variable deep generative model (DGM) for denoising cloudy Sentinel-2 observations conditioned on the information in cloud penetrating bands. We adapt the recently proposed Fr\'{e}chet Distance metric to remote sensing images for evaluating performance of the generator, demonstrate the potential of DGMs for conditional denoising, and discuss future directions as well as the limitations of DGMs in Earth science and humanitarian applications.



### Solving NMF with smoothness and sparsity constraints using PALM
- **Arxiv ID**: http://arxiv.org/abs/1910.14576v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14576v2)
- **Published**: 2019-10-31 16:29:59+00:00
- **Updated**: 2021-03-18 09:04:43+00:00
- **Authors**: Raimon Fabregat, Nelly Pustelnik, Paulo Gonçalves, Pierre Borgnat
- **Comment**: None
- **Journal**: None
- **Summary**: Non-negative matrix factorization is a problem of dimensionality reduction and source separation of data that has been widely used in many fields since it was studied in depth in 1999 by Lee and Seung, including in compression of data, document clustering, processing of audio spectrograms and astronomy. In this work we have adapted a minimization scheme for convex functions with non-differentiable constraints called PALM to solve the NMF problem with solutions that can be smooth and/or sparse, two properties frequently desired.



### Very high resolution Airborne PolSAR Image Classification using Convolutional Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.14578v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14578v2)
- **Published**: 2019-10-31 16:32:10+00:00
- **Updated**: 2020-04-09 21:13:23+00:00
- **Authors**: Minh-Tan Pham, Sébastien Lefèvre
- **Comment**: 5 pages, accepted in EUSAR 2020
- **Journal**: None
- **Summary**: In this work, we exploit convolutional neural networks (CNNs) for the classification of very high resolution (VHR) polarimetric SAR (PolSAR) data. Due to the significant appearance of heterogeneous textures within these data, not only polarimetric features but also structural tensors are exploited to feed CNN models. For deep networks, we use the SegNet model for semantic segmentation, which corresponds to pixelwise classification in remote sensing. Our experiments on the airborne F-SAR data show that for VHR PolSAR images, SegNet could provide high accuracy for the classification task; and introducing structural tensors together with polarimetric features as inputs could help the network to focus more on geometrical information to significantly improve the classification performance.



### Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods
- **Arxiv ID**: http://arxiv.org/abs/1910.14594v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML, 62M45, 68T45, 62H35, 65D18, 68U10, I.2.6; I.5.1; G.3
- **Links**: [PDF](http://arxiv.org/pdf/1910.14594v2)
- **Published**: 2019-10-31 16:47:46+00:00
- **Updated**: 2021-11-22 18:00:12+00:00
- **Authors**: Luca Della Libera, Vladimir Golkov, Yue Zhu, Arman Mielke, Daniel Cremers
- **Comment**: Improved Definition 1, improved and merged Sections 3.3-3.4, minor
  additional changes
- **Journal**: None
- **Summary**: Convolutional networks are successful due to their equivariance/invariance under translations. However, rotatable data such as images, volumes, shapes, or point clouds require processing with equivariance/invariance under rotations in cases where the rotational orientation of the coordinate system does not affect the meaning of the data (e.g. object classification). On the other hand, estimation/processing of rotations is necessary in cases where rotations are important (e.g. motion estimation). There has been recent progress in methods and theory in all these regards. Here we provide an overview of existing methods, both for 2D and 3D rotations (and translations), and identify commonalities and links between them.



### Can adversarial training learn image captioning ?
- **Arxiv ID**: http://arxiv.org/abs/1910.14609v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14609v1)
- **Published**: 2019-10-31 16:59:14+00:00
- **Updated**: 2019-10-31 16:59:14+00:00
- **Authors**: Jean-Benoit Delbrouck, Bastien Vanderplaetse, Stéphane Dupont
- **Comment**: Accepted to NeurIPS 2019 ViGiL workshop
- **Journal**: None
- **Summary**: Recently, generative adversarial networks (GAN) have gathered a lot of interest. Their efficiency in generating unseen samples of high quality, especially images, has improved over the years. In the field of Natural Language Generation (NLG), the use of the adversarial setting to generate meaningful sentences has shown to be difficult for two reasons: the lack of existing architectures to produce realistic sentences and the lack of evaluation tools. In this paper, we propose an adversarial architecture related to the conditional GAN (cGAN) that generates sentences according to a given image (also called image captioning). This attempt is the first that uses no pre-training or reinforcement methods. We also explain why our experiment settings can be safely evaluated and interpreted for further works.



### Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators
- **Arxiv ID**: http://arxiv.org/abs/1910.14634v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14634v2)
- **Published**: 2019-10-31 17:22:00+00:00
- **Updated**: 2020-02-23 01:49:25+00:00
- **Authors**: Reinhard Heckel, Mahdi Soltanolkotabi
- **Comment**: final ICRL version; simplifications in the proof
- **Journal**: International Conference on Learning Representations (ICLR) 2020
- **Summary**: Convolutional Neural Networks (CNNs) have emerged as highly successful tools for image generation, recovery, and restoration. A major contributing factor to this success is that convolutional networks impose strong prior assumptions about natural images. A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional generator to the corrupted image. While this over-parameterized network can fit the corrupted image perfectly, surprisingly after a few iterations of gradient descent it generates an almost uncorrupted image. This intriguing phenomenon enables state-of-the-art CNN-based denoising and regularization of other inverse problems. In this paper, we attribute this effect to a particular architectural choice of convolutional networks, namely convolutions with fixed interpolating filters. We then formally characterize the dynamics of fitting a two-layer convolutional generator to a noisy signal and prove that early-stopped gradient descent denoises/regularizes. Our proof relies on showing that convolutional generators fit the structured part of an image significantly faster than the corrupted portion.



### Does deep learning always outperform simple linear regression in optical imaging?
- **Arxiv ID**: http://arxiv.org/abs/1911.00353v2
- **DOI**: 10.1364/OE.382319
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00353v2)
- **Published**: 2019-10-31 17:26:49+00:00
- **Updated**: 2020-01-17 10:27:57+00:00
- **Authors**: Shuming Jiao, Yang Gao, Jun Feng, Ting Lei, Xiaocong Yuan
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has been extensively applied in many optical imaging applications in recent years. Despite the success, the limitations and drawbacks of deep learning in optical imaging have been seldom investigated. In this work, we show that conventional linear-regression-based methods can outperform the previously proposed deep learning approaches for two black-box optical imaging problems in some extent. Deep learning demonstrates its weakness especially when the number of training samples is small. The advantages and disadvantages of linear-regression-based methods and deep learning are analyzed and compared. Since many optical systems are essentially linear, a deep learning network containing many nonlinearity functions sometimes may not be the most suitable option.



### Enhancing Certifiable Robustness via a Deep Model Ensemble
- **Arxiv ID**: http://arxiv.org/abs/1910.14655v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14655v1)
- **Published**: 2019-10-31 17:48:33+00:00
- **Updated**: 2019-10-31 17:48:33+00:00
- **Authors**: Huan Zhang, Minhao Cheng, Cho-Jui Hsieh
- **Comment**: This is an extended version of ICLR 2019 Safe Machine Learning
  Workshop (SafeML) paper, "RobBoost: A provable approach to boost the
  robustness of deep model ensemble". May 6, 2019, New Orleans, LA, USA
- **Journal**: None
- **Summary**: We propose an algorithm to enhance certified robustness of a deep model ensemble by optimally weighting each base model. Unlike previous works on using ensembles to empirically improve robustness, our algorithm is based on optimizing a guaranteed robustness certificate of neural networks. Our proposed ensemble framework with certified robustness, RobBoost, formulates the optimal model selection and weighting task as an optimization problem on a lower bound of classification margin, which can be efficiently solved using coordinate descent. Experiments show that our algorithm can form a more robust ensemble than naively averaging all available models using robustly trained MNIST or CIFAR base models. Additionally, our ensemble typically has better accuracy on clean (unperturbed) data. RobBoost allows us to further improve certified robustness and clean accuracy by creating an ensemble of already certified models.



### Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/1910.14667v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1910.14667v2)
- **Published**: 2019-10-31 17:56:29+00:00
- **Updated**: 2020-07-22 17:59:49+00:00
- **Authors**: Zuxuan Wu, Ser-Nam Lim, Larry Davis, Tom Goldstein
- **Comment**: ECCV 2020
- **Journal**: None
- **Summary**: We present a systematic study of adversarial attacks on state-of-the-art object detection frameworks. Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Through extensive experiments, we benchmark the effectiveness of adversarially trained patches under both white-box and black-box settings, and quantify transferability of attacks between datasets, object classes, and detector models. Finally, we present a detailed study of physical world attacks using printed posters and wearable clothes, and rigorously quantify the performance of such attacks with different metrics.



### TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines
- **Arxiv ID**: http://arxiv.org/abs/1910.14671v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14671v2)
- **Published**: 2019-10-31 17:59:57+00:00
- **Updated**: 2020-01-09 15:55:26+00:00
- **Authors**: Jingxiang Lin, Unnat Jain, Alexander G. Schwing
- **Comment**: Accepted to NeurIPS 2019. Project page:
  https://deanplayerljx.github.io/tabvcr
- **Journal**: None
- **Summary**: Reasoning is an important ability that we learn from a very early age. Yet, reasoning is extremely hard for algorithms. Despite impressive recent progress that has been reported on tasks that necessitate reasoning, such as visual question answering and visual dialog, models often exploit biases in datasets. To develop models with better reasoning abilities, recently, the new visual commonsense reasoning (VCR) task has been introduced. Not only do models have to answer questions, but also do they have to provide a reason for the given answer. The proposed baseline achieved compelling results, leveraging a meticulously designed model composed of LSTM modules and attention nets. Here we show that a much simpler model obtained by ablating and pruning the existing intricate baseline can perform better with half the number of trainable parameters. By associating visual features with attribute information and better text to image grounding, we obtain further improvements for our simpler & effective baseline, TAB-VCR. We show that this approach results in a 5.3%, 4.4% and 6.5% absolute improvement over the previous state-of-the-art on question answering, answer justification and holistic VCR.



### Co-Generation with GANs using AIS based HMC
- **Arxiv ID**: http://arxiv.org/abs/1910.14673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.14673v1)
- **Published**: 2019-10-31 17:59:59+00:00
- **Updated**: 2019-10-31 17:59:59+00:00
- **Authors**: Tiantian Fang, Alexander G. Schwing
- **Comment**: Accepted to NeurIPS 2019
- **Journal**: None
- **Summary**: Inferring the most likely configuration for a subset of variables of a joint distribution given the remaining ones - which we refer to as co-generation - is an important challenge that is computationally demanding for all but the simplest settings. This task has received a considerable amount of attention, particularly for classical ways of modeling distributions like structured prediction. In contrast, almost nothing is known about this task when considering recently proposed techniques for modeling high-dimensional distributions, particularly generative adversarial nets (GANs). Therefore, in this paper, we study the occurring challenges for co-generation with GANs. To address those challenges we develop an annealed importance sampling based Hamiltonian Monte Carlo co-generation algorithm. The presented approach significantly outperforms classical gradient based methods on a synthetic and on the CelebA and LSUN datasets.



### PIC: Permutation Invariant Critic for Multi-Agent Deep Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1911.00025v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00025v1)
- **Published**: 2019-10-31 18:04:42+00:00
- **Updated**: 2019-10-31 18:04:42+00:00
- **Authors**: Iou-Jen Liu, Raymond A. Yeh, Alexander G. Schwing
- **Comment**: Accepted to CORL2019
- **Journal**: None
- **Summary**: Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent's perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren't permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a 'permutation invariant critic' (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15% to 50% on the challenging multi-agent particle environment (MPE).



### Chirality Nets for Human Pose Regression
- **Arxiv ID**: http://arxiv.org/abs/1911.00029v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00029v1)
- **Published**: 2019-10-31 18:09:05+00:00
- **Updated**: 2019-10-31 18:09:05+00:00
- **Authors**: Raymond A. Yeh, Yuan-Ting Hu, Alexander G. Schwing
- **Comment**: Accepted to NeurIPS2019
- **Journal**: None
- **Summary**: We propose Chirality Nets, a family of deep nets that is equivariant to the "chirality transform," i.e., the transformation to create a chiral pair. Through parameter sharing, odd and even symmetry, we propose and prove variants of standard building blocks of deep nets that satisfy the equivariance property, including fully connected layers, convolutional layers, batch-normalization, and LSTM/GRU cells. The proposed layers lead to a more data efficient representation and a reduction in computation by exploiting symmetry. We evaluate chirality nets on the task of human pose regression, which naturally exploits the left/right mirroring of the human body. We study three pose regression tasks: 3D pose estimation from video, 2D pose forecasting, and skeleton based activity recognition. Our approach achieves/matches state-of-the-art results, with more significant gains on small datasets and limited-data settings.



### Deep learning assessment of breast terminal duct lobular unit involution: towards automated prediction of breast cancer risk
- **Arxiv ID**: http://arxiv.org/abs/1911.00036v1
- **DOI**: 10.1371/journal.pone.0231653
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00036v1)
- **Published**: 2019-10-31 18:12:44+00:00
- **Updated**: 2019-10-31 18:12:44+00:00
- **Authors**: Suzanne C Wetstein, Allison M Onken, Christina Luffman, Gabrielle M Baker, Michael E Pyle, Kevin H Kensler, Ying Liu, Bart Bakker, Ruud Vlutters, Marinus B van Leeuwen, Laura C Collins, Stuart J Schnitt, Josien PW Pluim, Rulla M Tamimi, Yujing J Heng, Mitko Veta
- **Comment**: None
- **Journal**: None
- **Summary**: Terminal ductal lobular unit (TDLU) involution is the regression of milk-producing structures in the breast. Women with less TDLU involution are more likely to develop breast cancer. A major bottleneck in studying TDLU involution in large cohort studies is the need for labor-intensive manual assessment of TDLUs. We developed a computational pathology solution to automatically capture TDLU involution measures. Whole slide images (WSIs) of benign breast biopsies were obtained from the Nurses' Health Study (NHS). A first set of 92 WSIs was annotated for TDLUs, acini and adipose tissue to train deep convolutional neural network (CNN) models for detection of acini, and segmentation of TDLUs and adipose tissue. These networks were integrated into a single computational method to capture TDLU involution measures including number of TDLUs per tissue area, median TDLU span and median number of acini per TDLU. We validated our method on 40 additional WSIs by comparing with manually acquired measures. Our CNN models detected acini with an F1 score of 0.73$\pm$0.09, and segmented TDLUs and adipose tissue with Dice scores of 0.86$\pm$0.11 and 0.86$\pm$0.04, respectively. The inter-observer ICC scores for manual assessments on 40 WSIs of number of TDLUs per tissue area, median TDLU span, and median acini count per TDLU were 0.71, 95% CI [0.51, 0.83], 0.81, 95% CI [0.67, 0.90], and 0.73, 95% CI [0.54, 0.85], respectively. Intra-observer reliability was evaluated on 10/40 WSIs with ICC scores of >0.8. Inter-observer ICC scores between automated results and the mean of the two observers were: 0.80, 95% CI [0.63, 0.90] for number of TDLUs per tissue area, 0.57, 95% CI [0.19, 0.77] for median TDLU span, and 0.80, 95% CI [0.62, 0.89] for median acini count per TDLU. TDLU involution measures evaluated by manual and automated assessment were inversely associated with age and menopausal status.



### SignCol: Open-Source Software for Collecting Sign Language Gestures
- **Arxiv ID**: http://arxiv.org/abs/1911.00071v1
- **DOI**: 10.1109/ICSESS.2018.8663952
- **Categories**: **cs.HC**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00071v1)
- **Published**: 2019-10-31 19:36:18+00:00
- **Updated**: 2019-10-31 19:36:18+00:00
- **Authors**: Mohammad Eslami, Mahdi Karami, Sedigheh Eslami, Solale Tabarestani, Farah Torkamani-Azar, Christoph Meinel
- **Comment**: The paper is presented at ICSESS conference but the published version
  by them on the IEEE Xplore is impaired and the quality of figures is
  inappropriate!! This is the preprint version which had appropriate format and
  figures
- **Journal**: None
- **Summary**: Sign(ed) languages use gestures, such as hand or head movements, for communication. Sign language recognition is an assistive technology for individuals with hearing disability and its goal is to improve such individuals' life quality by facilitating their social involvement. Since sign languages are vastly varied in alphabets, as known as signs, a sign recognition software should be capable of handling eight different types of sign combinations, e.g. numbers, letters, words and sentences. Due to the intrinsic complexity and diversity of symbolic gestures, recognition algorithms need a comprehensive visual dataset to learn by. In this paper, we describe the design and implementation of a Microsoft Kinect-based open source software, called SignCol, for capturing and saving the gestures used in sign languages. Our work supports a multi-language database and reports the recorded items statistics. SignCol can capture and store colored(RGB) frames, depth frames, infrared frames, body index frames, coordinate mapped color-body frames, skeleton information of each frame and camera parameters simultaneously.



### Text-to-image synthesis method evaluation based on visual patterns
- **Arxiv ID**: http://arxiv.org/abs/1911.00077v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00077v1)
- **Published**: 2019-10-31 19:50:42+00:00
- **Updated**: 2019-10-31 19:50:42+00:00
- **Authors**: William Lund Sommer, Alexandros Iosifidis
- **Comment**: 11 pages, including supplementary material
- **Journal**: None
- **Summary**: A commonly used evaluation metric for text-to-image synthesis is the Inception score (IS) \cite{inceptionscore}, which has been shown to be a quality metric that correlates well with human judgment. However, IS does not reveal properties of the generated images indicating the ability of a text-to-image synthesis method to correctly convey semantics of the input text descriptions. In this paper, we introduce an evaluation metric and a visual evaluation method allowing for the simultaneous estimation of the realism, variety and semantic accuracy of generated images. The proposed method uses a pre-trained Inception network \cite{inceptionnet} to produce high dimensional representations for both real and generated images. These image representations are then visualized in a $2$-dimensional feature space defined by the t-distributed Stochastic Neighbor Embedding (t-SNE) \cite{tsne}. Visual concepts are determined by clustering the real image representations, and are subsequently used to evaluate the similarity of the generated images to the real ones by classifying them to the closest visual concept. The resulting classification accuracy is shown to be a effective gauge for the semantic accuracy of text-to-image synthesis methods.



### Automatic Prostate Zonal Segmentation Using Fully Convolutional Network with Feature Pyramid Attention
- **Arxiv ID**: http://arxiv.org/abs/1911.00127v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00127v1)
- **Published**: 2019-10-31 22:00:30+00:00
- **Updated**: 2019-10-31 22:00:30+00:00
- **Authors**: Yongkai Liu, Guang Yang, Sohrab Afshari Mirak, Melina Hosseiny, Afshin Azadikhah, Xinran Zhong, Robert E. Reiter, Yeejin Lee, Steven Raman, Kyunghyun Sung
- **Comment**: Has been accepted by IEEE Access
- **Journal**: None
- **Summary**: Our main objective is to develop a novel deep learning-based algorithm for automatic segmentation of prostate zone and to evaluate the proposed algorithm on an additional independent testing data in comparison with inter-reader consistency between two experts. With IRB approval and HIPAA compliance, we designed a novel convolutional neural network (CNN) for automatic segmentation of the prostatic transition zone (TZ) and peripheral zone (PZ) on T2-weighted (T2w) MRI. The total study cohort included 359 patients from two sources; 313 from a deidentified publicly available dataset (SPIE-AAPM-NCI PROSTATEX challenge) and 46 from a large U.S. tertiary referral center with 3T MRI (external testing dataset (ETD)). The TZ and PZ contours were manually annotated by research fellows, supervised by genitourinary (GU) radiologists. The model was developed using 250 patients and tested internally using the remaining 63 patients from the PROSTATEX (internal testing dataset (ITD)) and tested again (n=46) externally using the ETD. The Dice Similarity Coefficient (DSC) was used to evaluate the segmentation performance. DSCs for PZ and TZ were 0.74 and 0.86 in the ITD respectively. In the ETD, DSCs for PZ and TZ were 0.74 and 0.792, respectively. The inter-reader consistency (Expert 2 vs. Expert 1) were 0.71 (PZ) and 0.75 (TZ). This novel DL algorithm enabled automatic segmentation of PZ and TZ with high accuracy on both ITD and ETD without a performance difference for PZ and less than 10% TZ difference. In the ETD, the proposed method can be comparable to experts in the segmentation of prostate zones.



### Modified U-Net (mU-Net) with Incorporation of Object-Dependent High Level Features for Improved Liver and Liver-Tumor Segmentation in CT Images
- **Arxiv ID**: http://arxiv.org/abs/1911.00140v1
- **DOI**: 10.1109/TMI.2019.2948320
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.00140v1)
- **Published**: 2019-10-31 22:42:53+00:00
- **Updated**: 2019-10-31 22:42:53+00:00
- **Authors**: Hyunseok Seo, Charles Huang, Maxime Bassenne, Ruoxiu Xiao, Lei Xing
- **Comment**: Accept for publication at IEEE Transactions on Medical Imaging
- **Journal**: None
- **Summary**: Segmentation of livers and liver tumors is one of the most important steps in radiation therapy of hepatocellular carcinoma. The segmentation task is often done manually, making it tedious, labor intensive, and subject to intra-/inter- operator variations. While various algorithms for delineating organ-at-risks (OARs) and tumor targets have been proposed, automatic segmentation of livers and liver tumors remains intractable due to their low tissue contrast with respect to the surrounding organs and their deformable shape in CT images. The U-Net has gained increasing popularity recently for image analysis tasks and has shown promising results. Conventional U-Net architectures, however, suffer from three major drawbacks. To cope with these problems, we added a residual path with deconvolution and activation operations to the skip connection of the U-Net to avoid duplication of low resolution information of features. In the case of small object inputs, features in the skip connection are not incorporated with features in the residual path. Furthermore, the proposed architecture has additional convolution layers in the skip connection in order to extract high level global features of small object inputs as well as high level features of high resolution edge information of large object inputs. Efficacy of the modified U-Net (mU-Net) was demonstrated using the public dataset of Liver tumor segmentation (LiTS) challenge 2017. The proposed mU-Net outperformed existing state-of-art networks.



### Multivariate Medians for Image and Shape Analysis
- **Arxiv ID**: http://arxiv.org/abs/1911.00143v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, stat.ME, I.4.3; G.1.6; G.3
- **Links**: [PDF](http://arxiv.org/pdf/1911.00143v2)
- **Published**: 2019-10-31 22:54:31+00:00
- **Updated**: 2021-06-25 16:37:57+00:00
- **Authors**: Martin Welk
- **Comment**: Minor corrections, one additional reference
- **Journal**: None
- **Summary**: Having been studied since long by statisticians, multivariate median concepts found their way into the image processing literature in the course of the last decades, being used to construct robust and efficient denoising filters for multivariate images such as colour images but also matrix-valued images. Based on the similarities between image and geometric data as results of the sampling of continuous physical quantities, it can be expected that the understanding of multivariate median filters for images provides a starting point for the development of shape processing techniques. This paper presents an overview of multivariate median concepts relevant for image and shape processing. It focusses on their mathematical principles and discusses important properties especially in the context of image processing.



### Predicting the Politics of an Image Using Webly Supervised Data
- **Arxiv ID**: http://arxiv.org/abs/1911.00147v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1911.00147v1)
- **Published**: 2019-10-31 23:11:21+00:00
- **Updated**: 2019-10-31 23:11:21+00:00
- **Authors**: Christopher Thomas, Adriana Kovashka
- **Comment**: None
- **Journal**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Summary**: The news media shape public opinion, and often, the visual bias they contain is evident for human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image's political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose a two-stage method to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach facilitates learning and outperforms several strong baselines. We also present extensive qualitative results demonstrating the nuances of the data.



