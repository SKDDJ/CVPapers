# Arxiv Papers in cs.CV on 2019-10-25
### Heterogeneous Graph Learning for Visual Commonsense Reasoning
- **Arxiv ID**: http://arxiv.org/abs/1910.11475v1
- **DOI**: None
- **Categories**: **cs.CV**, 68T01
- **Links**: [PDF](http://arxiv.org/pdf/1910.11475v1)
- **Published**: 2019-10-25 01:04:46+00:00
- **Updated**: 2019-10-25 01:04:46+00:00
- **Authors**: Weijiang Yu, Jingwen Zhou, Weihao Yu, Xiaodan Liang, Nong Xiao
- **Comment**: 11 pages, 5 figures
- **Journal**: None
- **Summary**: Visual commonsense reasoning task aims at leading the research field into solving cognition-level reasoning with the ability of predicting correct answers and meanwhile providing convincing reasoning paths, resulting in three sub-tasks i.e., Q->A, QA->R and Q->AR. It poses great challenges over the proper semantic alignment between vision and linguistic domains and knowledge reasoning to generate persuasive reasoning paths. Existing works either resort to a powerful end-to-end network that cannot produce interpretable reasoning paths or solely explore intra-relationship of visual objects (homogeneous graph) while ignoring the cross-domain semantic alignment among visual concepts and linguistic words. In this paper, we propose a new Heterogeneous Graph Learning (HGL) framework for seamlessly integrating the intra-graph and inter-graph reasoning in order to bridge vision and language domain. Our HGL consists of a primal vision-to-answer heterogeneous graph (VAHG) module and a dual question-to-answer heterogeneous graph (QAHG) module to interactively refine reasoning paths for semantic agreement. Moreover, our HGL integrates a contextual voting module to exploit a long-range visual context for better global reasoning. Experiments on the large-scale Visual Commonsense Reasoning benchmark demonstrate the superior performance of our proposed modules on three tasks (improving 5% accuracy on Q->A, 3.5% on QA->R, 5.8% on Q->AR)



### Multimodal Image Outpainting With Regularized Normalized Diversification
- **Arxiv ID**: http://arxiv.org/abs/1910.11481v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11481v1)
- **Published**: 2019-10-25 01:24:45+00:00
- **Updated**: 2019-10-25 01:24:45+00:00
- **Authors**: Lingzhi Zhang, Jiancong Wang, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we study the problem of generating a set ofrealistic and diverse backgrounds when given only a smallforeground region. We refer to this task as image outpaint-ing. The technical challenge of this task is to synthesize notonly plausible but also diverse image outputs. Traditionalgenerative adversarial networks suffer from mode collapse.While recent approaches propose to maximize orpreserve the pairwise distance between generated sampleswith respect to their latent distance, they do not explicitlyprevent the diverse samples of different conditional inputsfrom collapsing. Therefore, we propose a new regulariza-tion method to encourage diverse sampling in conditionalsynthesis. In addition, we propose a feature pyramid dis-criminator to improve the image quality. Our experimen-tal results show that our model can produce more diverseimages without sacrificing visual quality compared to state-of-the-arts approaches in both the CelebA face dataset and the Cityscape scene dataset.



### Human Action Recognition Using Deep Multilevel Multimodal (M2) Fusion of Depth and Inertial Sensors
- **Arxiv ID**: http://arxiv.org/abs/1910.11482v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11482v1)
- **Published**: 2019-10-25 01:29:58+00:00
- **Updated**: 2019-10-25 01:29:58+00:00
- **Authors**: Zeeshan Ahmad, Naimul Khan
- **Comment**: 10 pages, 13 figures
- **Journal**: None
- **Summary**: Multimodal fusion frameworks for Human Action Recognition (HAR) using depth and inertial sensor data have been proposed over the years. In most of the existing works, fusion is performed at a single level (feature level or decision level), missing the opportunity to fuse rich mid-level features necessary for better classification. To address this shortcoming, in this paper, we propose three novel deep multilevel multimodal fusion frameworks to capitalize on different fusion strategies at various stages and to leverage the superiority of multilevel fusion. At input, we transform the depth data into depth images called sequential front view images (SFIs) and inertial sensor data into signal images. Each input modality, depth and inertial, is further made multimodal by taking convolution with the Prewitt filter. Creating "modality within modality" enables further complementary and discriminative feature extraction through Convolutional Neural Networks (CNNs). CNNs are trained on input images of each modality to learn low-level, high-level and complex features. Learned features are extracted and fused at different stages of the proposed frameworks to combine discriminative and complementary information. These highly informative features are served as input to a multi-class Support Vector Machine (SVM). We evaluate the proposed frameworks on three publicly available multimodal HAR datasets, namely, UTD Multimodal Human Action Dataset (MHAD), Berkeley MHAD, and UTD-MHAD Kinect V2. Experimental results show the supremacy of the proposed fusion frameworks over existing methods.



### Causal inference for climate change events from satellite image time series using computer vision and deep learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11492v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11492v1)
- **Published**: 2019-10-25 02:16:15+00:00
- **Updated**: 2019-10-25 02:16:15+00:00
- **Authors**: Vikas Ramachandra
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a method for causal inference using satellite image time series, in order to determine the treatment effects of interventions which impact climate change, such as deforestation. Simply put, the aim is to quantify the 'before versus after' effect of climate related human driven interventions, such as urbanization; as well as natural disasters, such as hurricanes and forest fires. As a concrete example, we focus on quantifying forest tree cover change/ deforestation due to human led causes. The proposed method involves the following steps. First, we uae computer vision and machine learning/deep learning techniques to detect and quantify forest tree coverage levels over time, at every time epoch. We then look at this time series to identify changepoints. Next, we estimate the expected (forest tree cover) values using a Bayesian structural causal model and projecting/forecasting the counterfactual. This is compared to the values actually observed post intervention, and the difference in the two values gives us the effect of the intervention (as compared to the non intervention scenario, i.e. what would have possibly happened without the intervention). As a specific use case, we analyze deforestation levels before and after the hyperinflation event (intervention) in Brazil (which ended in 1993-94), for the Amazon rainforest region, around Rondonia, Brazil. For this deforestation use case, using our causal inference framework can help causally attribute change/reduction in forest tree cover and increasing deforestation rates due to human activities at various points in time.



### Deep Image Blending
- **Arxiv ID**: http://arxiv.org/abs/1910.11495v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11495v1)
- **Published**: 2019-10-25 02:23:36+00:00
- **Updated**: 2019-10-25 02:23:36+00:00
- **Authors**: Lingzhi Zhang, Tarmily Wen, Jianbo Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Image composition is an important operation to create visual content. Among image composition tasks, image blending aims to seamlessly blend an object from a source image onto a target image with lightly mask adjustment. A popular approach is Poisson image blending, which enforces the gradient domain smoothness in the composite image. However, this approach only considers the boundary pixels of target image, and thus can not adapt to texture of target image. In addition, the colors of the target image often seep through the original source object too much causing a significant loss of content of the source object. We propose a Poisson blending loss that achieves the same purpose of Poisson image blending. In addition, we jointly optimize the proposed Poisson blending loss as well as the style and content loss computed from a deep network, and reconstruct the blending region by iteratively updating the pixels using the L-BFGS solver. In the blending image, we not only smooth out gradient domain of the blending boundary but also add consistent texture into the blending region. User studies show that our method outperforms strong baselines as well as state-of-the-art approaches when placing objects onto both paintings and real-world images.



### Toward an Automatic System for Computer-Aided Assessment in Facial Palsy
- **Arxiv ID**: http://arxiv.org/abs/1910.11497v1
- **DOI**: 10.1089/fpsam.2019.29000.gua.
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11497v1)
- **Published**: 2019-10-25 02:28:22+00:00
- **Updated**: 2019-10-25 02:28:22+00:00
- **Authors**: Diego L. Guarin, Yana Yunusova, Babak Taati, Joseph R Dusseldorp, Suresh Mohan, Joana Tavares, Martinus M. van Veen, Emily Fortier, Tessa A. Hadlock, Nate Jowett
- **Comment**: 21 pages, 4 figures, 1 table
- **Journal**: None
- **Summary**: Importance: Machine learning (ML) approaches to facial landmark localization carry great clinical potential for quantitative assessment of facial function as they enable high-throughput automated quantification of relevant facial metrics from photographs. However, translation from research settings to clinical applications requires important improvements. Objective: To develop an ML algorithm for accurate facial landmarks localization in photographs of facial palsy patients, and use it as part of an automated computer-aided diagnosis system. Design, Setting, and Participants: Facial landmarks were manually localized in portrait photographs of eight expressions obtained from 200 facial palsy patients and 10 controls. A novel ML model for automated facial landmark localization was trained using this disease-specific database. Model output was compared to manual annotations and the output of a model trained using a larger database consisting only of healthy subjects. Model accuracy was evaluated by the normalized root mean square error (NRMSE) between algorithms' prediction and manual annotations. Results: Publicly available algorithms provide poor results when applied to patients compared to healthy controls (NRMSE, 8.56 +/- 2.16 vs. 7.09 +/- 2.34, p << 0.01). We found significant improvement in facial landmark localization accuracy for the clinical population when using a model trained with a relatively small number patients' photographs (1440) compared to a model trained using several thousand more images of healthy faces (NRMSE, 6.03 +/- 2.43 vs. 8.56 +/- 2.16, p << 0.01). Conclusions: Retraining a landmark detection model with a small number of clinical images significantly improved landmark detection performance in frontal view photographs of the clinical population. These results represent the first steps towards an automatic system for computer-aided assessment in facial palsy.



### A comparable study: Intrinsic difficulties of practical plant diagnosis from wide-angle images
- **Arxiv ID**: http://arxiv.org/abs/1910.11506v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11506v2)
- **Published**: 2019-10-25 03:03:34+00:00
- **Updated**: 2019-11-22 10:46:39+00:00
- **Authors**: Katsumasa Suwa, Quan Huu Cap, Ryunosuke Kotani, Hiroyuki Uga, Satoshi Kagiwada, Hitoshi Iyatomi
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Practical automated detection and diagnosis of plant disease from wide-angle images (i.e. in-field images containing multiple leaves using a fixed-position camera) is a very important application for large-scale farm management, in view of the need to ensure global food security. However, developing automated systems for disease diagnosis is often difficult, because labeling a reliable wide-angle disease dataset from actual field images is very laborious. In addition, the potential similarities between the training and test data lead to a serious problem of model overfitting. In this paper, we investigate changes in performance when applying disease diagnosis systems to different scenarios involving wide-angle cucumber test data captured on real farms, and propose an effective diagnostic strategy. We show that leading object recognition techniques such as SSD and Faster R-CNN achieve excellent end-to-end disease diagnostic performance only for a test dataset that is collected from the same population as the training dataset (with F1-score of 81.5% - 84.1% for diagnosed cases of disease), but their performance markedly deteriorates for a completely different test dataset (with F1-score of 4.4 - 6.2%). In contrast, our proposed two-stage systems using independent leaf detection and leaf diagnosis stages attain a promising disease diagnostic performance that is more than six times higher than end-to-end systems (with F1-score of 33.4 - 38.9%) on an unseen target dataset. We also confirm the efficiency of our proposal based on visual assessment, concluding that a two-stage model is a suitable and reasonable choice for practical applications.



### Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait
- **Arxiv ID**: http://arxiv.org/abs/1910.11509v4
- **DOI**: 10.1016/j.eswa.2019.113075
- **Categories**: **cs.LG**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11509v4)
- **Published**: 2019-10-25 03:14:54+00:00
- **Updated**: 2020-05-16 18:50:52+00:00
- **Authors**: Imanne El Maachi, Guillaume-Alexandre Bilodeau, Wassim Bouachir
- **Comment**: Source code available at
  https://github.com/imanneelmaachi/Parkinson-disease-detection-and-severity-prediction-from-gait
- **Journal**: Expert Systems with Applications, 113075 (2019)
- **Summary**: Diagnosing Parkinson's disease is a complex task that requires the evaluation of several motor and non-motor symptoms. During diagnosis, gait abnormalities are among the important symptoms that physicians should consider. However, gait evaluation is challenging and relies on the expertise and subjectivity of clinicians. In this context, the use of an intelligent gait analysis algorithm may assist physicians in order to facilitate the diagnosis process. This paper proposes a novel intelligent Parkinson detection system based on deep learning techniques to analyze gait information. We used 1D convolutional neural network (1D-Convnet) to build a Deep Neural Network (DNN) classifier. The proposed model processes 18 1D-signals coming from foot sensors measuring the vertical ground reaction force (VGRF). The first part of the network consists of 18 parallel 1D-Convnet corresponding to system inputs. The second part is a fully connected network that connects the concatenated outputs of the 1D-Convnets to obtain a final classification. We tested our algorithm in Parkinson's detection and in the prediction of the severity of the disease with the Unified Parkinson's Disease Rating Scale (UPDRS). Our experiments demonstrate the high efficiency of the proposed method in the detection of Parkinson disease based on gait data. The proposed algorithm achieved an accuracy of 98.7 %. To our knowledge, this is the state-of-the-start performance in Parkinson's gait recognition. Furthermore, we achieved an accuracy of 85.3 % in Parkinson's severity prediction. To the best of our knowledge, this is the first algorithm to perform a severity prediction based on the UPDRS. Our results show that the model is able to learn intrinsic characteristics from gait data and to generalize to unseen subjects, which could be helpful in a clinical diagnosis.



### RhythmNet: End-to-end Heart Rate Estimation from Face via Spatial-temporal Representation
- **Arxiv ID**: http://arxiv.org/abs/1910.11515v2
- **DOI**: 10.1109/TIP.2019.2947204
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11515v2)
- **Published**: 2019-10-25 04:03:41+00:00
- **Updated**: 2019-11-04 06:23:47+00:00
- **Authors**: Xuesong Niu, Shiguang Shan, Hu Han, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: Heart rate (HR) is an important physiological signal that reflects the physical and emotional status of a person. Traditional HR measurements usually rely on contact monitors, which may cause inconvenience and discomfort. Recently, some methods have been proposed for remote HR estimation from face videos; however, most of them focus on well-controlled scenarios, their generalization ability into less-constrained scenarios (e.g., with head movement, and bad illumination) are not known. At the same time, lacking large-scale HR databases has limited the use of deep models for remote HR estimation. In this paper, we propose an end-to-end RhythmNet for remote HR estimation from the face. In RyhthmNet, we use a spatial-temporal representation encoding the HR signals from multiple ROI volumes as its input. Then the spatial-temporal representations are fed into a convolutional network for HR estimation. We also take into account the relationship of adjacent HR measurements from a video sequence via Gated Recurrent Unit (GRU) and achieves efficient HR measurement. In addition, we build a large-scale multi-modal HR database (named as VIPL-HR, available at 'http://vipl.ict.ac.cn/view_database.php?id=15'), which contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Our VIPL-HR database contains various variations such as head movements, illumination variations, and acquisition device changes, replicating a less-constrained scenario for HR estimation. The proposed approach outperforms the state-of-the-art methods on both the public-domain and our VIPL-HR databases.



### Team PFDet's Methods for Open Images Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.11534v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11534v1)
- **Published**: 2019-10-25 05:28:36+00:00
- **Updated**: 2019-10-25 05:28:36+00:00
- **Authors**: Yusuke Niitani, Toru Ogawa, Shuji Suzuki, Takuya Akiba, Tommi Kerola, Kohei Ozaki, Shotaro Sano
- **Comment**: None
- **Journal**: None
- **Summary**: We present the instance segmentation and the object detection method used by team PFDet for Open Images Challenge 2019. We tackle a massive dataset size, huge class imbalance and federated annotations. Using this method, the team PFDet achieved 3rd and 4th place in the instance segmentation and the object detection track, respectively.



### TRB: A Novel Triplet Representation for Understanding 2D Human Body
- **Arxiv ID**: http://arxiv.org/abs/1910.11535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11535v1)
- **Published**: 2019-10-25 05:35:21+00:00
- **Updated**: 2019-10-25 05:35:21+00:00
- **Authors**: Haodong Duan, KwanYee Lin, Sheng Jin, Wentao Liu, Chen Qian, Wanli Ouyang
- **Comment**: Accepted by ICCV2019
- **Journal**: None
- **Summary**: Human pose and shape are two important components of 2D human body. However, how to efficiently represent both of them in images is still an open question. In this paper, we propose the Triplet Representation for Body (TRB) -- a compact 2D human body representation, with skeleton keypoints capturing human pose information and contour keypoints containing human shape information. TRB not only preserves the flexibility of skeleton keypoint representation, but also contains rich pose and human shape information. Therefore, it promises broader application areas, such as human shape editing and conditional image generation. We further introduce the challenging problem of TRB estimation, where joint learning of human pose and shape is required. We construct several large-scale TRB estimation datasets, based on popular 2D pose datasets: LSP, MPII, COCO. To effectively solve TRB estimation, we propose a two-branch network (TRB-net) with three novel techniques, namely X-structure (Xs), Directional Convolution (DC) and Pairwise Mapping (PM), to enforce multi-level message passing for joint feature learning. We evaluate our proposed TRB-net and several leading approaches on our proposed TRB datasets, and demonstrate the superiority of our method through extensive evaluations.



### An End-to-End Foreground-Aware Network for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.11547v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11547v2)
- **Published**: 2019-10-25 06:43:19+00:00
- **Updated**: 2021-03-06 09:42:43+00:00
- **Authors**: Yiheng Liu, Wengang Zhou, Jianzhuang Liu, Guojun Qi, Qi Tian, Houqiang Li
- **Comment**: Accepted to IEEE Transactions on Image Processing (TIP), 2021
- **Journal**: None
- **Summary**: Person re-identification is a crucial task of identifying pedestrians of interest across multiple surveillance camera views. In person re-identification, a pedestrian is usually represented with features extracted from a rectangular image region that inevitably contains the scene background, which incurs ambiguity to distinguish different pedestrians and degrades the accuracy. To this end, we propose an end-to-end foreground-aware network to discriminate foreground from background by learning a soft mask for person re-identification. In our method, in addition to the pedestrian ID as supervision for foreground, we introduce the camera ID of each pedestrian image for background modeling. The foreground branch and the background branch are optimized collaboratively. By presenting a target attention loss, the pedestrian features extracted from the foreground branch become more insensitive to the backgrounds, which greatly reduces the negative impacts of changing backgrounds on matching an identical across different camera views. Notably, in contrast to existing methods, our approach does not require any additional dataset to train a human landmark detector or a segmentation model for locating the background regions. The experimental results conducted on three challenging datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17, demonstrate the effectiveness of our approach.



### Progressive Unsupervised Person Re-identification by Tracklet Association with Spatio-Temporal Regularization
- **Arxiv ID**: http://arxiv.org/abs/1910.11560v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11560v1)
- **Published**: 2019-10-25 07:49:03+00:00
- **Updated**: 2019-10-25 07:49:03+00:00
- **Authors**: Qiaokang Xie, Wengang Zhou, Guo-Jun Qi, Qi Tian, Houqiang Li
- **Comment**: 12 pages, 10 figures
- **Journal**: None
- **Summary**: Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model.



### Metric Classification Network in Actual Face Recognition Scene
- **Arxiv ID**: http://arxiv.org/abs/1910.11563v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11563v1)
- **Published**: 2019-10-25 07:58:10+00:00
- **Updated**: 2019-10-25 07:58:10+00:00
- **Authors**: Jian Li, Yan Wang, Xiubao Zhang, Weihong Deng, Haifeng Shen
- **Comment**: arXiv admin note: text overlap with arXiv:1504.03641 by other authors
- **Journal**: None
- **Summary**: In order to make facial features more discriminative, some new models have recently been proposed. However, almost all of these models use the traditional face verification method, where the cosine operation is performed using the features of the bottleneck layer output. However, each of these models needs to change a threshold each time it is operated on a different test set. This is very inappropriate for application in real-world scenarios. In this paper, we train a validation classifier to normalize the decision threshold, which means that the result can be obtained directly without replacing the threshold. We refer to our model as validation classifier, which achieves best result on the structure consisting of one convolution layer and six fully connected layers. To test our approach, we conduct extensive experiments on Labeled Face in the Wild (LFW) and Youtube Faces (YTF), and the relative error reduction is 25.37% and 26.60% than traditional method respectively. These experiments confirm the effectiveness of validation classifier on face recognition task.



### CrevNet: Conditionally Reversible Video Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.11577v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11577v1)
- **Published**: 2019-10-25 08:59:32+00:00
- **Updated**: 2019-10-25 08:59:32+00:00
- **Authors**: Wei Yu, Yichao Lu, Steve Easterbrook, Sanja Fidler
- **Comment**: None
- **Journal**: None
- **Summary**: Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational efficiency.



### A Simple Dynamic Learning Rate Tuning Algorithm For Automated Training of DNNs
- **Arxiv ID**: http://arxiv.org/abs/1910.11605v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11605v1)
- **Published**: 2019-10-25 10:23:12+00:00
- **Updated**: 2019-10-25 10:23:12+00:00
- **Authors**: Koyel Mukherjee, Alind Khare, Ashish Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Training neural networks on image datasets generally require extensive experimentation to find the optimal learning rate regime. Especially, for the cases of adversarial training or for training a newly synthesized model, one would not know the best learning rate regime beforehand. We propose an automated algorithm for determining the learning rate trajectory, that works across datasets and models for both natural and adversarial training, without requiring any dataset/model specific tuning. It is a stand-alone, parameterless, adaptive approach with no computational overhead. We theoretically discuss the algorithm's convergence behavior. We empirically validate our algorithm extensively. Our results show that our proposed approach \emph{consistently} achieves top-level accuracy compared to SOTA baselines in the literature in natural as well as adversarial training.



### Effectiveness of random deep feature selection for securing image manipulation detectors against adversarial examples
- **Arxiv ID**: http://arxiv.org/abs/1910.12392v2
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12392v2)
- **Published**: 2019-10-25 10:33:32+00:00
- **Updated**: 2019-12-26 11:37:28+00:00
- **Authors**: Mauro Barni, Ehsan Nowroozi, Benedetta Tondi, Bowen Zhang
- **Comment**: Submitted to the ICASSP conference to be held in 2020, Barcelona,
  Spain
- **Journal**: None
- **Summary**: We investigate if the random feature selection approach proposed in [1] to improve the robustness of forensic detectors to targeted attacks, can be extended to detectors based on deep learning features. In particular, we study the transferability of adversarial examples targeting an original CNN image manipulation detector to other detectors (a fully connected neural network and a linear SVM) that rely on a random subset of the features extracted from the flatten layer of the original network. The results we got by considering three image manipulation detection tasks (resizing, median filtering and adaptive histogram equalization), two original network architectures and three classes of attacks, show that feature randomization helps to hinder attack transferability, even if, in some cases, simply changing the architecture of the detector, or even retraining the detector is enough to prevent the transferability of the attacks.



### Fast Hardware-Aware Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1910.11609v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11609v3)
- **Published**: 2019-10-25 10:40:57+00:00
- **Updated**: 2020-04-20 02:05:46+00:00
- **Authors**: Li Lyna Zhang, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, Yunxin Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Designing accurate and efficient convolutional neural architectures for vast amount of hardware is challenging because hardware designs are complex and diverse. This paper addresses the hardware diversity challenge in Neural Architecture Search (NAS). Unlike previous approaches that apply search algorithms on a small, human-designed search space without considering hardware diversity, we propose HURRICANE that explores the automatic hardware-aware search over a much larger search space and a two-stage search algorithm, to efficiently generate tailored models for different types of hardware. Extensive experiments on ImageNet demonstrate that our algorithm outperforms state-of-the-art hardware-aware NAS methods under the same latency constraint on three types of hardware. Moreover, the discovered architectures achieve much lower latency and higher accuracy than current state-of-the-art efficient models. Remarkably, HURRICANE achieves a 76.67% top-1 accuracy on ImageNet with a inference latency of only 16.5 ms for DSP, which is a 3.47% higher accuracy and a 6.35x inference speedup than FBNet-iPhoneX, respectively. For VPU, we achieve a 0.53% higher top-1 accuracy than Proxyless-mobile with a 1.49x speedup. Even for well-studied mobile CPU, we achieve a 1.63% higher top-1 accuracy than FBNet-iPhoneX with a comparable inference latency. HURRICANE also reduces the training time by 30.4% compared to SPOS.



### Learning to Localize Temporal Events in Large-scale Video Data
- **Arxiv ID**: http://arxiv.org/abs/1910.11631v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11631v1)
- **Published**: 2019-10-25 11:40:29+00:00
- **Updated**: 2019-10-25 11:40:29+00:00
- **Authors**: Mikel Bober-Irizar, Miha Skalic, David Austin
- **Comment**: ICCV 2019, 3rd Youtube-8M Workshop
- **Journal**: None
- **Summary**: We address temporal localization of events in large-scale video data, in the context of the Youtube-8M Segments dataset. This emerging field within video recognition can enable applications to identify the precise time a specified event occurs in a video, which has broad implications for video search. To address this we present two separate approaches: (1) a gradient boosted decision tree model on a crafted dataset and (2) a combination of deep learning models based on frame-level data, video-level data, and a localization model. The combinations of these two approaches achieved 5th place in the 3rd Youtube-8M video recognition challenge.



### Mixing realities for sketch retrieval in Virtual Reality
- **Arxiv ID**: http://arxiv.org/abs/1910.11637v2
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11637v2)
- **Published**: 2019-10-25 11:52:25+00:00
- **Updated**: 2019-11-05 09:58:24+00:00
- **Authors**: Daniele Giunchi, Stuart james, Donald Degraen, Anthony Steed
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Drawing tools for Virtual Reality (VR) enable users to model 3D designs from within the virtual environment itself. These tools employ sketching and sculpting techniques known from desktop-based interfaces and apply them to hand-based controller interaction. While these techniques allow for mid-air sketching of basic shapes, it remains difficult for users to create detailed and comprehensive 3D models. In our work, we focus on supporting the user in designing the virtual environment around them by enhancing sketch-based interfaces with a supporting system for interactive model retrieval. Through sketching, an immersed user can query a database containing detailed 3D models and replace them into the virtual environment. To understand supportive sketching within a virtual environment, we compare different methods of sketch interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet. %using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and 3D mid-air sketching. Our results show that 3D mid-air sketching is considered to be a more intuitive method to search a collection of models while the addition of physical devices creates confusion due to the complications of their inclusion within a virtual environment. While we pose our work as a retrieval problem for 3D models of chairs, our results can be extrapolated to other sketching tasks for virtual environments.



### Reducing Domain Gap by Reducing Style Bias
- **Arxiv ID**: http://arxiv.org/abs/1910.11645v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11645v4)
- **Published**: 2019-10-25 12:19:11+00:00
- **Updated**: 2021-04-01 02:35:47+00:00
- **Authors**: Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, Donggeun Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) often fail to maintain their performance when they confront new test domains, which is known as the problem of domain shift. Recent studies suggest that one of the main causes of this problem is CNNs' strong inductive bias towards image styles (i.e. textures) which are sensitive to domain changes, rather than contents (i.e. shapes). Inspired by this, we propose to reduce the intrinsic style bias of CNNs to close the gap between domains. Our Style-Agnostic Networks (SagNets) disentangle style encodings from class categories to prevent style biased predictions and focus more on the contents. Extensive experiments show that our method effectively reduces the style bias and makes the model more robust under domain shift. It achieves remarkable performance improvements in a wide range of cross-domain tasks including domain generalization, unsupervised domain adaptation, and semi-supervised domain adaptation on multiple datasets.



### Attend to the Difference: Cross-Modality Person Re-identification via Contrastive Correlation
- **Arxiv ID**: http://arxiv.org/abs/1910.11656v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11656v2)
- **Published**: 2019-10-25 12:36:47+00:00
- **Updated**: 2021-12-01 17:40:36+00:00
- **Authors**: Shizhou Zhang, Yifei Yang, Peng Wang, Guoqiang Liang, Xiuwei Zhang, Yanning Zhang
- **Comment**: The paper is accepted by TIP
- **Journal**: None
- **Summary**: The problem of cross-modality person re-identification has been receiving increasing attention recently, due to its practical significance. Motivated by the fact that human usually attend to the difference when they compare two similar objects, we propose a dual-path cross-modality feature learning framework which preserves intrinsic spatial strictures and attends to the difference of input cross-modality image pairs. Our framework is composed by two main components: a Dual-path Spatial-structure-preserving Common Space Network (DSCSN) and a Contrastive Correlation Network (CCN). The former embeds cross-modality images into a common 3D tensor space without losing spatial structures, while the latter extracts contrastive features by dynamically comparing input image pairs. Note that the representations generated for the input RGB and Infrared images are mutually dependant to each other. We conduct extensive experiments on two public available RGB-IR ReID datasets, SYSU-MM01 and RegDB, and our proposed method outperforms state-of-the-art algorithms by a large margin with both full and simplified evaluation modes.



### Learning Task-Oriented Grasping from Human Activity Datasets
- **Arxiv ID**: http://arxiv.org/abs/1910.11669v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11669v2)
- **Published**: 2019-10-25 12:52:40+00:00
- **Updated**: 2020-05-21 08:06:53+00:00
- **Authors**: Mia Kokic, Danica Kragic, Jeannette Bohg
- **Comment**: None
- **Journal**: IEEE Robotics and Automation Letters 5 (2020) 3352-3359
- **Summary**: We propose to leverage a real-world, human activity RGB dataset to teach a robot Task-Oriented Grasping (TOG). We develop a model that takes as input an RGB image and outputs a hand pose and configuration as well as an object pose and a shape. We follow the insight that jointly estimating hand and object poses increases accuracy compared to estimating these quantities independently of each other. Given the trained model, we process an RGB dataset to automatically obtain the data to train a TOG model. This model takes as input an object point cloud and outputs a suitable region for task-specific grasping. Our ablation study shows that training an object pose predictor with the hand pose information (and vice versa) is better than training without this information. Furthermore, our results on a real-world dataset show the applicability and competitiveness of our method over state-of-the-art. Experiments with a robot demonstrate that our method can allow a robot to preform TOG on novel objects.



### ALET (Automated Labeling of Equipment and Tools): A Dataset, a Baseline and a Usecase for Tool Detection in the Wild
- **Arxiv ID**: http://arxiv.org/abs/1910.11713v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11713v3)
- **Published**: 2019-10-25 13:29:10+00:00
- **Updated**: 2020-12-13 15:31:44+00:00
- **Authors**: Fatih Can Kurnaz, Burak Hocaoğlu, Mert Kaan Yılmaz, İdil Sülo, Sinan Kalkan
- **Comment**: 7 pages, 4 figures
- **Journal**: None
- **Summary**: Robots collaborating with humans in realistic environments will need to be able to detect the tools that can be used and manipulated. However, there is no available dataset or study that addresses this challenge in real settings. In this paper, we fill this gap by providing an extensive dataset (METU-ALET) for detecting farming, gardening, office, stonemasonry, vehicle, woodworking and workshop tools. The scenes correspond to sophisticated environments with or without humans using the tools. The scenes we consider introduce several challenges for object detection, including the small scale of the tools, their articulated nature, occlusion, inter-class invariance, etc. Moreover, we train and compare several state of the art deep object detectors (including Faster R-CNN, Cascade R-CNN, RepPoint and RetinaNet) on our dataset. We observe that the detectors have difficulty in detecting especially small-scale tools or tools that are visually similar to parts of other tools. This in turn supports the importance of our dataset and paper. With the dataset, the code and the trained models, our work provides a basis for further research into tools and their use in robotics applications.



### Self-supervised Moving Vehicle Tracking with Stereo Sound
- **Arxiv ID**: http://arxiv.org/abs/1910.11760v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.SD, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1910.11760v1)
- **Published**: 2019-10-25 14:28:55+00:00
- **Updated**: 2019-10-25 14:28:55+00:00
- **Authors**: Chuang Gan, Hang Zhao, Peihao Chen, David Cox, Antonio Torralba
- **Comment**: To appear at ICCV 2019. Project page:
  http://sound-track.csail.mit.edu
- **Journal**: None
- **Summary**: Humans are able to localize objects in the environment using both visual and auditory cues, integrating information from multiple modalities into a common reference frame. We introduce a system that can leverage unlabeled audio-visual data to learn to localize objects (moving vehicles) in a visual reference frame, purely using stereo sound at inference time. Since it is labor-intensive to manually annotate the correspondences between audio and object bounding boxes, we achieve this goal by using the co-occurrence of visual and audio streams in unlabeled videos as a form of self-supervision, without resorting to the collection of ground-truth annotations. In particular, we propose a framework that consists of a vision "teacher" network and a stereo-sound "student" network. During training, knowledge embodied in a well-established visual vehicle detection model is transferred to the audio domain using unlabeled videos as a bridge. At test time, the stereo-sound student network can work independently to perform object localization us-ing just stereo audio and camera meta-data, without any visual input. Experimental results on a newly collected Au-ditory Vehicle Tracking dataset verify that our proposed approach outperforms several baseline approaches. We also demonstrate that our cross-modal auditory localization approach can assist in the visual localization of moving vehicles under poor lighting conditions.



### Gated Multi-layer Convolutional Feature Extraction Network for Robust Pedestrian Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.11761v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11761v2)
- **Published**: 2019-10-25 14:28:58+00:00
- **Updated**: 2019-12-18 15:39:23+00:00
- **Authors**: Tianrui Liu, Jun-Jie Huang, Tianhong Dai, Guangyu Ren, Tania Stathaki
- **Comment**: None
- **Journal**: International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP) 2020
- **Summary**: Pedestrian detection methods have been significantly improved with the development of deep convolutional neural networks. Nevertheless, robustly detecting pedestrians with a large variant on sizes and with occlusions remains a challenging problem. In this paper, we propose a gated multi-layer convolutional feature extraction method which can adaptively generate discriminative features for candidate pedestrian regions. The proposed gated feature extraction framework consists of squeeze units, gate units and a concatenation layer which perform feature dimension squeezing, feature elements manipulation and convolutional features combination from multiple CNN layers, respectively. We proposed two different gate models which can manipulate the regional feature maps in a channel-wise selection manner and a spatial-wise selection manner, respectively. Experiments on the challenging CityPersons dataset demonstrate the effectiveness of the proposed method, especially on detecting those small-size and occluded pedestrians.



### ClsGAN: Selective Attribute Editing Model Based On Classification Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11764v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11764v2)
- **Published**: 2019-10-25 14:32:21+00:00
- **Updated**: 2020-07-29 12:14:18+00:00
- **Authors**: Liu Ying, Heng Fan, Fuchuan Ni, Jinhai Xiang
- **Comment**: None
- **Journal**: None
- **Summary**: Attribution editing has achieved remarkable progress in recent years owing to the encoder-decoder structure and generative adversarial network (GAN). However, it remains challenging in generating high-quality images with accurate attribute transformation. Attacking these problems, the work proposes a novel selective attribute editing model based on classification adversarial network (referred to as ClsGAN) that shows good balance between attribute transfer accuracy and photo-realistic images. Considering that the editing images are prone to be affected by original attribute due to skip-connection in encoder-decoder structure, an upper convolution residual network (referred to as Tr-resnet) is presented to selectively extract information from the source image and target label. In addition, to further improve the transfer accuracy of generated images, an attribute adversarial classifier (referred to as Atta-cls) is introduced to guide the generator from the perspective of attribute through learning the defects of attribute transfer images. Experimental results on CelebA demonstrate that our ClsGAN performs favorably against state-of-the-art approaches in image quality and transfer accuracy. Moreover, ablation studies are also designed to verify the great performance of Tr-resnet and Atta-cls.



### DR$\vert$GRADUATE: uncertainty-aware deep learning-based diabetic retinopathy grading in eye fundus images
- **Arxiv ID**: http://arxiv.org/abs/1910.11777v2
- **DOI**: 10.1016/j.media.2020.101715
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11777v2)
- **Published**: 2019-10-25 14:56:15+00:00
- **Updated**: 2020-05-29 14:55:20+00:00
- **Authors**: Teresa Araújo, Guilherme Aresta, Luís Mendonça, Susana Penas, Carolina Maia, Ângela Carneiro, Ana Maria Mendonça, Aurélio Campilho
- **Comment**: Published at Medical Image Analysis (Elsevier). Publication licensed
  under the Creative Commons CC-BY-NC-ND 4.0 license
  https://creativecommons.org/licenses/by-nc-nd/4.0/. Figures are compressed
  due to file size constraints
- **Journal**: Medical Image Analysis, Volume 63, July 2020, 101715
- **Summary**: Diabetic retinopathy (DR) grading is crucial in determining the adequate treatment and follow up of patients, but the screening process can be tiresome and prone to errors. Deep learning approaches have shown promising performance as computer-aided diagnosis(CAD) systems, but their black-box behaviour hinders the clinical application. We propose DR$\vert$GRADUATE, a novel deep learning-based DR grading CAD system that supports its decision by providing a medically interpretable explanation and an estimation of how uncertain that prediction is, allowing the ophthalmologist to measure how much that decision should be trusted. We designed DR$\vert$GRADUATE taking into account the ordinal nature of the DR grading problem. A novel Gaussian-sampling approach built upon a Multiple Instance Learning framework allow DR$\vert$GRADUATE to infer an image grade associated with an explanation map and a prediction uncertainty while being trained only with image-wise labels. DR$\vert$GRADUATE was trained on the Kaggle training set and evaluated across multiple datasets. In DR grading, a quadratic-weighted Cohen's kappa (QWK) between 0.71 and 0.84 was achieved in five different datasets. We show that high QWK values occur for images with low prediction uncertainty, thus indicating that this uncertainty is a valid measure of the predictions' quality. Further, bad quality images are generally associated with higher uncertainties, showing that images not suitable for diagnosis indeed lead to less trustworthy predictions. Additionally, tests on unfamiliar medical image data types suggest that DR$\vert$GRADUATE allows outlier detection. The attention maps generally highlight regions of interest for diagnosis. These results show the great potential of DR$\vert$GRADUATE as a second-opinion system in DR severity grading.



### Prediction of gaze direction using Convolutional Neural Networks for Autism diagnosis
- **Arxiv ID**: http://arxiv.org/abs/1911.05629v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.05629v1)
- **Published**: 2019-10-25 15:06:56+00:00
- **Updated**: 2019-10-25 15:06:56+00:00
- **Authors**: Dennis Núñez-Fernández, Franklin Porras-Barrientos, Macarena Vittet-Mondoñedo, Robert H. Gilman, Mirko Zimic
- **Comment**: LatinX in AI Research at NeurIPS 2019
- **Journal**: None
- **Summary**: Autism is a developmental disorder that affects social interaction and communication of children. The gold standard diagnostic tools are very difficult to use and time consuming. However, diagnostic could be deduced from child gaze preferences by looking a video with social and abstract scenes. In this work, we propose an algorithm based on convolutional neural networks to predict gaze direction for a fast and effective autism diagnosis. Early results show that our algorithm achieves real-time response and robust high accuracy for prediction of gaze direction.



### Self-supervised Learning of Detailed 3D Face Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.11791v2
- **DOI**: 10.1109/TIP.2020.3017347
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11791v2)
- **Published**: 2019-10-25 15:16:20+00:00
- **Updated**: 2020-09-02 03:58:23+00:00
- **Authors**: Yajing Chen, Fanzi Wu, Zeyu Wang, Yibing Song, Yonggen Ling, Linchao Bao
- **Comment**: Accepted by IEEE Transactions on Image Processing (TIP)
- **Journal**: None
- **Summary**: In this paper, we present an end-to-end learning framework for detailed 3D face reconstruction from a single image. Our approach uses a 3DMM-based coarse model and a displacement map in UV-space to represent a 3D face. Unlike previous work addressing the problem, our learning framework does not require supervision of surrogate ground-truth 3D models computed with traditional approaches. Instead, we utilize the input image itself as supervision during learning. In the first stage, we combine a photometric loss and a facial perceptual loss between the input face and the rendered face, to regress a 3DMM-based coarse model. In the second stage, both the input image and the regressed texture of the coarse model are unwrapped into UV-space, and then sent through an image-toimage translation network to predict a displacement map in UVspace. The displacement map and the coarse model are used to render a final detailed face, which again can be compared with the original input image to serve as a photometric loss for the second stage. The advantage of learning displacement map in UV-space is that face alignment can be explicitly done during the unwrapping, thus facial details are easier to learn from large amount of data. Extensive experiments demonstrate the superiority of the proposed method over previous work.



### JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception of Humans in Built Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.11792v4
- **DOI**: 10.1109/TPAMI.2021.3070543
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11792v4)
- **Published**: 2019-10-25 15:16:40+00:00
- **Updated**: 2021-04-24 07:09:03+00:00
- **Authors**: Roberto Martín-Martín, Mihir Patel, Hamid Rezatofighi, Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian, Silvio Savarese
- **Comment**: None
- **Journal**: None
- **Summary**: We present JRDB, a novel egocentric dataset collected from our social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360$^\circ$ RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360$^\circ$ spherical image from a fisheye camera and encoder values from the robot's wheels. Our dataset incorporates data from traditionally underrepresented scenes such as indoor environments and pedestrian areas, all from the ego-perspective of the robot, both stationary and navigating. The dataset has been annotated with over 2.3 million bounding boxes spread over 5 individual cameras and 1.8 million associated 3D cuboids around all people in the scenes totaling over 3500 time consistent trajectories. Together with our dataset and the annotations, we launch a benchmark and metrics for 2D and 3D person detection and tracking. With this dataset, which we plan on extending with further types of annotation in the future, we hope to provide a new source of data and a test-bench for research in the areas of egocentric robot vision, autonomous navigation, and all perceptual tasks around social robotics in human environments.



### Real-time Memory Efficient Large-pose Face Alignment via Deep Evolutionary Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11818v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11818v2)
- **Published**: 2019-10-25 16:00:05+00:00
- **Updated**: 2019-11-01 15:31:34+00:00
- **Authors**: Bin Sun, Ming Shao, Siyu Xia, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: There is an urgent need to apply face alignment in a memory-efficient and real-time manner due to the recent explosion of face recognition applications. However, impact factors such as large pose variation and computational inefficiency, still hinder its broad implementation. To this end, we propose a computationally efficient deep evolutionary model integrated with 3D Diffusion Heap Maps (DHM). First, we introduce a sparse 3D DHM to assist the initial modeling process under extreme pose conditions. Afterward, a simple and effective CNN feature is extracted and fed to Recurrent Neural Network (RNN) for evolutionary learning. To accelerate the model, we propose an efficient network structure to accelerate the evolutionary learning process through a factorization strategy. Extensive experiments on three popular alignment databases demonstrate the advantage of the proposed models over the state-of-the-art, especially under large-pose conditions. Notably, the computational speed of our model is 6 times faster than the state-of-the-art on CPU and 14 times on GPU. We also discuss and analyze the limitations of our models and future research work.



### An End-to-End Network for Co-Saliency Detection in One Single Image
- **Arxiv ID**: http://arxiv.org/abs/1910.11819v2
- **DOI**: 10.1007/s11432-022-3686-1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11819v2)
- **Published**: 2019-10-25 16:00:44+00:00
- **Updated**: 2023-02-15 15:17:28+00:00
- **Authors**: Yuanhao Yue, Qin Zou, Hongkai Yu, Qian Wang, Zhongyuan Wang, Song Wang
- **Comment**: None
- **Journal**: SCIENCE CHINA Information Sciences, 2023
- **Summary**: Co-saliency detection within a single image is a common vision problem that has received little attention and has not yet been well addressed. Existing methods often used a bottom-up strategy to infer co-saliency in an image in which salient regions are firstly detected using visual primitives such as color and shape and then grouped and merged into a co-saliency map. However, co-saliency is intrinsically perceived complexly with bottom-up and top-down strategies combined in human vision. To address this problem, this study proposes a novel end-to-end trainable network comprising a backbone net and two branch nets. The backbone net uses ground-truth masks as top-down guidance for saliency prediction, whereas the two branch nets construct triplet proposals for regional feature mapping and clustering, which drives the network to be bottom-up sensitive to co-salient regions. We construct a new dataset of 2,019 natural images with co-saliency in each image to evaluate the proposed method. Experimental results show that the proposed method achieves state-of-the-art accuracy with a running speed of 28 fps.



### Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters
- **Arxiv ID**: http://arxiv.org/abs/1910.11831v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11831v5)
- **Published**: 2019-10-25 16:31:25+00:00
- **Updated**: 2020-05-04 10:19:18+00:00
- **Authors**: Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian
- **Comment**: 22 pages, 12 figures, submitted to ICML 2020, updated experiments on
  Penn Treebank
- **Journal**: None
- **Summary**: DARTS is a popular algorithm for neural architecture search (NAS). Despite its great advantage in search efficiency, DARTS often suffers weak stability, which reflects in the large variation among individual trials as well as the sensitivity to the hyper-parameters of the search process. This paper owes such instability to an optimization gap between the super-network and its sub-networks, namely, improving the validation accuracy of the super-network does not necessarily lead to a higher expectation on the performance of the sampled sub-networks. Then, we point out that the gap is due to the inaccurate estimation of the architectural gradients, based on which we propose an amended estimation method. Mathematically, our method guarantees a bounded error from the true gradients while the original estimation does not. Our approach bridges the gap from two aspects, namely, amending the estimation on the architectural gradients, and unifying the hyper-parameter settings in the search and re-training stages. Experiments on CIFAR10 and ImageNet demonstrate that our approach largely improves search stability and, more importantly, enables DARTS-based approaches to explore much larger search spaces that have not been investigated before.



### Portable system for the prediction of anemia based on the ocular conjunctiva using Artificial Intelligence
- **Arxiv ID**: http://arxiv.org/abs/1910.12399v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12399v1)
- **Published**: 2019-10-25 16:44:03+00:00
- **Updated**: 2019-10-25 16:44:03+00:00
- **Authors**: Bryan Saldivar-Espinoza, Dennis Núñez-Fernández, Franklin Porras-Barrientos, Alicia Alva-Mantari, Lisa Suzanne Leslie, Mirko Zimic
- **Comment**: LatinX in AI Research at NeurIPS 2019
- **Journal**: None
- **Summary**: Anemia is a major health burden worldwide. Examining the hemoglobin level of blood is an important way to achieve the diagnosis of anemia, but it requires blood drawing and a blood test. In this work we propose a non-invasive, fast, and cost-effective screening test for iron-deficiency anemia in Peruvian young children. Our initial results show promising evidence for detecting conjunctival pallor anemia and Artificial Intelligence techniques with photos taken with a popular smartphone.



### BlenderProc
- **Arxiv ID**: http://arxiv.org/abs/1911.01911v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1911.01911v1)
- **Published**: 2019-10-25 16:53:12+00:00
- **Updated**: 2019-10-25 16:53:12+00:00
- **Authors**: Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: BlenderProc is a modular procedural pipeline, which helps in generating real looking images for the training of convolutional neural networks. These can be used in a variety of use cases including segmentation, depth, normal and pose estimation and many others. A key feature of our extension of blender is the simple to use modular pipeline, which was designed to be easily extendable. By offering standard modules, which cover a variety of scenarios, we provide a starting point on which new modules can be created.



### Learning to Track Any Object
- **Arxiv ID**: http://arxiv.org/abs/1910.11844v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.11844v1)
- **Published**: 2019-10-25 16:58:42+00:00
- **Updated**: 2019-10-25 16:58:42+00:00
- **Authors**: Achal Dave, Pavel Tokmakov, Cordelia Schmid, Deva Ramanan
- **Comment**: To be presented at the Holistic Video Understanding workshop at ICCV
- **Journal**: None
- **Summary**: Object tracking can be formulated as "finding the right object in a video". We observe that recent approaches for class-agnostic tracking tend to focus on the "finding" part, but largely overlook the "object" part of the task, essentially doing a template matching over a frame in a sliding-window. In contrast, class-specific trackers heavily rely on object priors in the form of category-specific object detectors. In this work, we re-purpose category-specific appearance models into a generic objectness prior. Our approach converts a category-specific object detector into a category-agnostic, object-specific detector (i.e. a tracker) efficiently, on the fly. Moreover, at test time the same network can be applied to detection and tracking, resulting in a unified approach for the two tasks. We achieve state-of-the-art results on two recent large-scale tracking benchmarks (OxUvA and GOT, using external data). By simply adding a mask prediction branch, our approach is able to produce instance segmentation masks for the tracked object. Despite only using box-level information on the first frame, our method outputs high-quality masks, as evaluated on the DAVIS '17 video object segmentation benchmark.



### LPRNet: Lightweight Deep Network by Low-rank Pointwise Residual Convolution
- **Arxiv ID**: http://arxiv.org/abs/1910.11853v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.11853v3)
- **Published**: 2019-10-25 17:23:05+00:00
- **Updated**: 2019-11-14 22:44:48+00:00
- **Authors**: Bin Sun, Jun Li, Ming Shao, Yun Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning has become popular in recent years primarily due to the powerful computing device such as GPUs. However, deploying these deep models to end-user devices, smart phones, or embedded systems with limited resources is challenging. To reduce the computation and memory costs, we propose a novel lightweight deep learning module by low-rank pointwise residual (LPR) convolution, called LPRNet. Essentially, LPR aims at using low-rank approximation in pointwise convolution to further reduce the module size, while keeping depthwise convolutions as the residual module to rectify the LPR module. This is critical when the low-rankness undermines the convolution process. We embody our design by replacing modules of identical input-output dimension in MobileNet and ShuffleNetv2. Experiments on visual recognition tasks including image classification and face alignment on popular benchmarks show that our LPRNet achieves competitive performance but with significant reduction of Flops and memory cost compared to the state-of-the-art deep models focusing on model compression.



### Noisier2Noise: Learning to Denoise from Unpaired Noisy Data
- **Arxiv ID**: http://arxiv.org/abs/1910.11908v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11908v1)
- **Published**: 2019-10-25 19:30:38+00:00
- **Updated**: 2019-10-25 19:30:38+00:00
- **Authors**: Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady
- **Comment**: None
- **Journal**: None
- **Summary**: We present a method for training a neural network to perform image denoising without access to clean training examples or access to paired noisy training examples. Our method requires only a single noisy realization of each training example and a statistical model of the noise distribution, and is applicable to a wide variety of noise models, including spatially structured noise. Our model produces results which are competitive with other learned methods which require richer training data, and outperforms traditional non-learned denoising methods. We present derivations of our method for arbitrary additive noise, an improvement specific to Gaussian additive noise, and an extension to multiplicative Bernoulli noise.



### Automatic Reminiscence Therapy for Dementia
- **Arxiv ID**: http://arxiv.org/abs/1910.11949v2
- **DOI**: None
- **Categories**: **cs.MM**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11949v2)
- **Published**: 2019-10-25 21:47:52+00:00
- **Updated**: 2021-01-19 12:26:15+00:00
- **Authors**: Mariona Caros, Maite Garolera, Petia Radeva, Xavier Giro-i-Nieto
- **Comment**: MSc thesis at TelecomBCN, Universitat Politecnica de Catalunya 2019
- **Journal**: None
- **Summary**: With people living longer than ever, the number of cases with dementia such as Alzheimer's disease increases steadily. It affects more than 46 million people worldwide, and it is estimated that in 2050 more than 100 million will be affected. While there are not effective treatments for these terminal diseases, therapies such as reminiscence, that stimulate memories from the past are recommended. Currently, reminiscence therapy takes place in care homes and is guided by a therapist or a carer. In this work, we present an AI-based solution to automatize the reminiscence therapy, which consists in a dialogue system that uses photos as input to generate questions. We run a usability case study with patients diagnosed of mild cognitive impairment that shows they found the system very entertaining and challenging. Overall, this paper presents how reminiscence therapy can be automatized by using machine learning, and deployed to smartphones and laptops, making the therapy more accessible to every person affected by dementia.



### Data Augmentation for Skin Lesion using Self-Attention based Progressive Generative Adversarial Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11960v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.11960v1)
- **Published**: 2019-10-25 23:15:36+00:00
- **Updated**: 2019-10-25 23:15:36+00:00
- **Authors**: Ibrahim Saad Ali, Mamdouh Farouk Mohamed, Yousef Bassyouni Mahdy
- **Comment**: None
- **Journal**: None
- **Summary**: Deep Neural Networks (DNNs) show a significant impact on medical imaging. One significant problem with adopting DNNs for skin cancer classification is that the class frequencies in the existing datasets are imbalanced. This problem hinders the training of robust and well-generalizing models. Data Augmentation addresses this by using existing data more effectively. However, standard data augmentation implementations are manually designed and produce only limited reasonably alternative data. Instead, Generative Adversarial Networks (GANs) is utilized to generate a much broader set of augmentations. This paper proposes a novel enhancement for the progressive generative adversarial networks (PGAN) using self-attention mechanism. Self-attention mechanism is used to directly model the long-range dependencies in the feature maps. Accordingly, self-attention complements PGAN to generate fine-grained samples that comprise clinically-meaningful information. Moreover, the stabilization technique was applied to the enhanced generative model. To train the generative models, ISIC 2018 skin lesion challenge dataset was used to synthesize highly realistic skin lesion samples for boosting further the classification result. We achieve an accuracy of 70.1% which is 2.8% better than the non-augmented one of 67.3%.



