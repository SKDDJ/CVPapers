# Arxiv Papers in cs.CV on 2019-10-07
### MASTER: Multi-Aspect Non-local Network for Scene Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.02562v3
- **DOI**: 10.1016/j.patcog.2021.107980
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02562v3)
- **Published**: 2019-10-07 00:31:01+00:00
- **Updated**: 2021-04-11 06:21:38+00:00
- **Authors**: Ning Lu, Wenwen Yu, Xianbiao Qi, Yihao Chen, Ping Gong, Rong Xiao, Xiang Bai
- **Comment**: Accepted by Pattern Recognition. Ning Lu and Wenwen Yu are co-first
  authors
- **Journal**: None
- **Summary**: Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a RNN-based encoder-decoder architecture. However, such methods suffer from attention-drift problem because high similarity among encoded features leads to attention confusion under the RNN-based local attention mechanism. Moreover, RNN-based methods have low efficiency due to poor parallelization. To overcome these problems, we propose the MASTER, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our MASTER on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.



### Action-conditioned Benchmarking of Robotic Video Prediction Models: a Comparative Study
- **Arxiv ID**: http://arxiv.org/abs/1910.02564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02564v1)
- **Published**: 2019-10-07 00:39:17+00:00
- **Updated**: 2019-10-07 00:39:17+00:00
- **Authors**: Manuel Serra Nunes, Atabak Dehban, Plinio Moreno, Jos√© Santos-Victor
- **Comment**: None
- **Journal**: None
- **Summary**: A defining characteristic of intelligent systems is the ability to make action decisions based on the anticipated outcomes. Video prediction systems have been demonstrated as a solution for predicting how the future will unfold visually, and thus, many models have been proposed that are capable of predicting future frames based on a history of observed frames~(and sometimes robot actions). However, a comprehensive method for determining the fitness of different video prediction models at guiding the selection of actions is yet to be developed. Current metrics assess video prediction models based on human perception of frame quality. In contrast, we argue that if these systems are to be used to guide action, necessarily, the actions the robot performs should be encoded in the predicted frames. In this paper, we are proposing a new metric to compare different video prediction models based on this argument. More specifically, we propose an action inference system and quantitatively rank different models based on how well we can infer the robot actions from the predicted frames. Our extensive experiments show that models with high perceptual scores can perform poorly in the proposed action inference tests and thus, may not be suitable options to be used in robot planning systems.



### FastEstimator: A Deep Learning Library for Fast Prototyping and Productization
- **Arxiv ID**: http://arxiv.org/abs/1910.04875v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04875v2)
- **Published**: 2019-10-07 01:01:27+00:00
- **Updated**: 2019-11-18 19:20:13+00:00
- **Authors**: Xiaomeng Dong, Junpyo Hong, Hsi-Ming Chang, Michael Potter, Aritra Chowdhury, Purujit Bahl, Vivek Soni, Yun-Chan Tsai, Rajesh Tamada, Gaurav Kumar, Caroline Favart, V. Ratna Saripalli, Gopal Avinash
- **Comment**: None
- **Journal**: None
- **Summary**: As the complexity of state-of-the-art deep learning models increases by the month, implementation, interpretation, and traceability become ever-more-burdensome challenges for AI practitioners around the world. Several AI frameworks have risen in an effort to stem this tide, but the steady advance of the field has begun to test the bounds of their flexibility, expressiveness, and ease of use. To address these concerns, we introduce a radically flexible high-level open source deep learning framework for both research and industry. We introduce FastEstimator.



### A Novel Technique of Noninvasive Hemoglobin Level Measurement Using HSV Value of Fingertip Image
- **Arxiv ID**: http://arxiv.org/abs/1910.02579v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02579v1)
- **Published**: 2019-10-07 02:17:03+00:00
- **Updated**: 2019-10-07 02:17:03+00:00
- **Authors**: Md Kamrul Hasan, Nazmus Sakib, Joshua Field, Richard R. Love, Sheikh I. Ahamed
- **Comment**: None
- **Journal**: None
- **Summary**: Over the last decade, smartphones have changed radically to support us with mHealth technology, cloud computing, and machine learning algorithm. Having its multifaceted facilities, we present a novel smartphone-based noninvasive hemoglobin (Hb) level prediction model by analyzing hue, saturation and value (HSV) of a fingertip video. Here, we collect 60 videos of 60 subjects from two different locations: Blood Center of Wisconsin, USA and AmaderGram, Bangladesh. We extract red, green, and blue (RGB) pixel intensities of selected images of those videos captured by the smartphone camera with flash on. Then we convert RGB values of selected video frames of a fingertip video into HSV color space and we generate histogram values of these HSV pixel intensities. We average these histogram values of a fingertip video and consider as an observation against the gold standard Hb concentration. We generate two input feature matrices based on observation of two different data sets. Partial Least Squares (PLS) algorithm is applied on the input feature matrix. We observe R2=0.95 in both data sets through our research. We analyze our data using Python OpenCV, Matlab, and R statistics tool.



### Unsupervised Image Super-Resolution with an Indirect Supervised Path
- **Arxiv ID**: http://arxiv.org/abs/1910.02593v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02593v2)
- **Published**: 2019-10-07 03:34:49+00:00
- **Updated**: 2019-10-13 13:18:52+00:00
- **Authors**: Zhen Han, Enyan Dai, Xu Jia, Xiaoying Ren, Shuaijun Chen, Chunjing Xu, Jianzhuang Liu, Qi Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The task of single image super-resolution (SISR) aims at reconstructing a high-resolution (HR) image from a low-resolution (LR) image. Although significant progress has been made by deep learning models, they are trained on synthetic paired data in a supervised way and do not perform well on real data. There are several attempts that directly apply unsupervised image translation models to address such a problem. However, unsupervised low-level vision problem poses more challenge on the accuracy of translation. In this work,we propose a novel framework which is composed of two stages: 1) unsupervised image translation between real LR images and synthetic LR images; 2) supervised super-resolution from approximated real LR images to HR images. It takes the synthetic LR images as a bridge and creates an indirect supervised path from real LR images to HR images. Any existed deep learning based image super-resolution model can be integrated into the second stage of the proposed framework for further improvement. In addition it shows great flexibility in balancing between distortion and perceptual quality under unsupervised setting. The proposed method is evaluated on both NTIRE 2017 and 2018 challenge datasets and achieves favorable performance against supervised methods.



### Human Action Sequence Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.02602v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02602v1)
- **Published**: 2019-10-07 04:27:01+00:00
- **Updated**: 2019-10-07 04:27:01+00:00
- **Authors**: Yan Bin Ng, Basura Fernando
- **Comment**: None
- **Journal**: None
- **Summary**: This paper classifies human action sequences from videos using a machine translation model. In contrast to classical human action classification which outputs a set of actions, our method output a sequence of action in the chronological order of the actions performed by the human. Therefore our method is evaluated using sequential performance measures such as Bilingual Evaluation Understudy (BLEU) scores. Action sequence classification has many applications such as learning from demonstration, action segmentation, detection, localization and video captioning. Furthermore, we use our model that is trained to output action sequences to solve downstream tasks; such as video captioning and action localization. We obtain state of the art results for video captioning in challenging Charades dataset obtaining BLEU-4 score of 34.8 and METEOR score of 33.6 outperforming previous state-of-the-art of 18.8 and 19.5 respectively. Similarly, on ActivityNet captioning, we obtain excellent results in-terms of ROUGE (20.24) and CIDER (37.58) scores. For action localization, without using any explicit start/end action annotations, our method obtains localization performance of 22.2 mAP outperforming prior fully supervised methods.



### CrowdFix: An Eyetracking Dataset of Real Life Crowd Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.02618v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02618v2)
- **Published**: 2019-10-07 05:43:49+00:00
- **Updated**: 2019-10-09 07:40:19+00:00
- **Authors**: Memoona Tahira, Sobas Mehboob, Anis U. Rahman, Omar Arif
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Understanding human visual attention and saliency is an integral part of vision research. In this context, there is an ever-present need for fresh and diverse benchmark datasets, particularly for insight into special use cases like crowded scenes. We contribute to this end by: (1) reviewing the dynamics behind saliency and crowds. (2) using eye tracking to create a dynamic human eye fixation dataset over a new set of crowd videos gathered from the Internet. The videos are annotated into three distinct density levels. (3) Finally, we evaluate state-of-the-art saliency models on our dataset to identify possible improvements for the design and creation of a more robust saliency model.



### Meta-Transfer Learning through Hard Tasks
- **Arxiv ID**: http://arxiv.org/abs/1910.03648v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03648v1)
- **Published**: 2019-10-07 06:05:18+00:00
- **Updated**: 2019-10-07 06:05:18+00:00
- **Authors**: Qianru Sun, Yaoyao Liu, Zhaozheng Chen, Tat-Seng Chua, Bernt Schiele
- **Comment**: An extended version of a paper published in CVPR2019. Under review.
  arXiv admin note: substantial text overlap with arXiv:1812.02391
- **Journal**: None
- **Summary**: Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, typical meta-learning models use shallow neural networks, thus limiting its effectiveness. In order to achieve top performance, some recent works tried to use the DNNs pre-trained on large-scale datasets but mostly in straight-forward manners, e.g., (1) taking their weights as a warm start of meta-training, and (2) freezing their convolutional layers as the feature extractor of base-learners. In this paper, we propose a novel approach called meta-transfer learning (MTL) which learns to transfer the weights of a deep NN for few-shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum that further boosts the learning efficiency of MTL. We conduct few-shot learning experiments and report top performance for five-class few-shot recognition tasks on three challenging benchmarks: miniImageNet, tieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related works validate that our MTL approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.



### Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.02624v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02624v3)
- **Published**: 2019-10-07 06:28:16+00:00
- **Updated**: 2020-04-24 08:22:51+00:00
- **Authors**: Weifeng Ge, Sheng Guo, Weilin Huang, Matthew R. Scott
- **Comment**: Rectifiy some typos in Arxiv title
- **Journal**: None
- **Summary**: Weakly-supervised instance segmentation aims to detect and segment object instances precisely, given imagelevel labels only. Unlike previous methods which are composed of multiple offline stages, we propose Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) that progressively transform image-level labels to pixel-wise labels in a coarse-to-fine manner. We design four cascaded modules including multi-label classification, object detection, instance refinement and instance segmentation, which are implemented sequentially by sharing the same backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy that generalizes labels from high-level images to low-level pixels gradually with increasing accuracy. In addition, we design a proposal calibration module to explore the ability of classification networks to find key pixels that identify object parts, which serves as a post validation strategy running in the inverse order. We evaluate the efficiency of our Label-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007 and 2012. Experimental results show that Label-PEnet outperforms the state-of-the-art algorithms by a clear margin, and obtains comparable performance even with the fully-supervised approaches.



### Softmax Is Not an Artificial Trick: An Information-Theoretic View of Softmax in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.02629v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02629v3)
- **Published**: 2019-10-07 06:46:06+00:00
- **Updated**: 2019-10-15 05:59:37+00:00
- **Authors**: Zhenyue Qin, Dongwoo Kim
- **Comment**: Withdrawn due to Zhenyue Qin uploading the manuscript without consent
  of the other authors
- **Journal**: None
- **Summary**: Despite great popularity of applying softmax to map the non-normalised outputs of a neural network to a probability distribution over predicting classes, this normalised exponential transformation still seems to be artificial. A theoretic framework that incorporates softmax as an intrinsic component is still lacking. In this paper, we view neural networks embedding softmax from an information-theoretic perspective. Under this view, we can naturally and mathematically derive log-softmax as an inherent component in a neural network for evaluating the conditional mutual information between network output vectors and labels given an input datum. We show that training deterministic neural networks through maximising log-softmax is equivalent to enlarging the conditional mutual information, i.e., feeding label information into network outputs. We also generalise our informative-theoretic perspective to neural networks with stochasticity and derive information upper and lower bounds of log-softmax. In theory, such an information-theoretic view offers rationality support for embedding softmax in neural networks; in practice, we eventually demonstrate a computer vision application example of how to employ our information-theoretic view to filter out targeted objects on images.



### Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization
- **Arxiv ID**: http://arxiv.org/abs/1910.02653v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.DC, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02653v3)
- **Published**: 2019-10-07 07:54:06+00:00
- **Updated**: 2020-05-14 17:46:43+00:00
- **Authors**: Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica, Joseph E. Gonzalez
- **Comment**: In Proceedings of 3rd Conference Machine Learning and Systems 2020
  (MLSys 2020)
- **Journal**: None
- **Summary**: We formalize the problem of trading-off DNN training time and memory requirements as the tensor rematerialization optimization problem, a generalization of prior checkpointing strategies. We introduce Checkmate, a system that solves for optimal rematerialization schedules in reasonable times (under an hour) using off-the-shelf MILP solvers or near-optimal schedules with an approximation algorithm, then uses these schedules to accelerate millions of training iterations. Our method scales to complex, realistic architectures and is hardware-aware through the use of accelerator-specific, profile-based cost models. In addition to reducing training cost, Checkmate enables real-world networks to be trained with up to 5.1x larger input sizes. Checkmate is an open-source project, available at https://github.com/parasj/checkmate.



### Multi-label Detection and Classification of Red Blood Cells in Microscopic Images
- **Arxiv ID**: http://arxiv.org/abs/1910.02672v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02672v2)
- **Published**: 2019-10-07 08:40:49+00:00
- **Updated**: 2019-12-14 12:44:04+00:00
- **Authors**: Wei Qiu, Jiaming Guo, Xiang Li, Mengjia Xu, Mo Zhang, Ning Guo, Quanzheng Li
- **Comment**: Wei Qiu, Jiaming Guo and Xiang Li contributed equally
- **Journal**: None
- **Summary**: Cell detection and cell type classification from biomedical images play an important role for high-throughput imaging and various clinical application. While classification of single cell sample can be performed with standard computer vision and machine learning methods, analysis of multi-label samples (region containing congregating cells) is more challenging, as separation of individual cells can be difficult (e.g. touching cells) or even impossible (e.g. overlapping cells). As multi-instance images are common in analyzing Red Blood Cell (RBC) for Sickle Cell Disease (SCD) diagnosis, we develop and implement a multi-instance cell detection and classification framework to address this challenge. The framework firstly trains a region proposal model based on Region-based Convolutional Network (RCNN) to obtain bounding-boxes of regions potentially containing single or multiple cells from input microscopic images, which are extracted as image patches. High-level image features are then calculated from image patches through a pre-trained Convolutional Neural Network (CNN) with ResNet-50 structure. Using these image features inputs, six networks are then trained to make multi-label prediction of whether a given patch contains cells belonging to a specific cell type. As the six networks are trained with image patches consisting of both individual cells and touching/overlapping cells, they can effectively recognize cell types that are presented in multi-instance image samples. Finally, for the purpose of SCD testing, we train another machine learning classifier to predict whether the given image patch contains abnormal cell type based on outputs from the six networks. Testing result of the proposed framework shows that it can achieve good performance in automatic cell detection and classification.



### From Google Maps to a Fine-Grained Catalog of Street trees
- **Arxiv ID**: http://arxiv.org/abs/1910.02675v1
- **DOI**: 10.1016/j.isprsjprs.2017.11.008
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02675v1)
- **Published**: 2019-10-07 08:52:50+00:00
- **Updated**: 2019-10-07 08:52:50+00:00
- **Authors**: Steve Branson, Jan Dirk Wegner, David Hall, Nico Lang, Konrad Schindler, Pietro Perona
- **Comment**: None
- **Journal**: ISPRS Journal of Photogrammetry and Remote Sensing, Volume 135,
  January 2018, Pages 13-30
- **Summary**: Up-to-date catalogs of the urban tree population are important for municipalities to monitor and improve quality of life in cities. Despite much research on automation of tree mapping, mainly relying on dedicated airborne LiDAR or hyperspectral campaigns, trees are still mostly mapped manually in practice. We present a fully automated tree detection and species recognition pipeline to process thousands of trees within a few hours using publicly available aerial and street view images of Google MapsTM. These data provide rich information (viewpoints, scales) from global tree shapes to bark textures. Our work-flow is built around a supervised classification that automatically learns the most discriminative features from thousands of trees and corresponding, public tree inventory data. In addition, we introduce a change tracker to keep urban tree inventories up-to-date. Changes of individual trees are recognized at city-scale by comparing street-level images of the same tree location at two different times. Drawing on recent advances in computer vision and machine learning, we apply convolutional neural networks (CNN) for all classification tasks. We propose the following pipeline: download all available panoramas and overhead images of an area of interest, detect trees per image and combine multi-view detections in a probabilistic framework, adding prior knowledge; recognize fine-grained species of detected trees. In a later, separate module, track trees over time and identify the type of change. We believe this is the first work to exploit publicly available image data for fine-grained tree mapping at city-scale, respectively over many thousands of trees. Experiments in the city of Pasadena, California, USA show that we can detect > 70% of the street trees, assign correct species to > 80% for 40 different species, and correctly detect and classify changes in > 90% of the cases.



### Hierarchical stochastic neighbor embedding as a tool for visualizing the encoding capability of magnetic resonance fingerprinting dictionaries
- **Arxiv ID**: http://arxiv.org/abs/1910.02696v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02696v1)
- **Published**: 2019-10-07 09:48:38+00:00
- **Updated**: 2019-10-07 09:48:38+00:00
- **Authors**: Kirsten Koolstra, Peter B√∂rnert, Boudewijn Lelieveldt, Andrew Webb, Oleh Dzyubachyk
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: In Magnetic Resonance Fingerprinting (MRF) the quality of the estimated parameter maps depends on the encoding capability of the variable flip angle train. In this work we show how the dimensionality reduction technique Hierarchical Stochastic Neighbor Embedding (HSNE) can be used to obtain insight into the encoding capability of different MRF sequences. Embedding high-dimensional MRF dictionaries into a lower-dimensional space and visualizing them with colors, being a surrogate for location in low-dimensional space, provides a comprehensive overview of particular dictionaries and, in addition, enables comparison of different sequences. Dictionaries for various sequences and sequence lengths were compared to each other, and the effect of transmit field variations on the encoding capability was assessed. Clear differences in encoding capability were observed between different sequences, and HSNE results accurately reflect those obtained from an MRF matching simulation.



### Noise as Domain Shift: Denoising Medical Images by Unpaired Image Translation
- **Arxiv ID**: http://arxiv.org/abs/1910.02702v1
- **DOI**: 10.1007/978-3-030-33391-1_1
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02702v1)
- **Published**: 2019-10-07 10:16:31+00:00
- **Updated**: 2019-10-07 10:16:31+00:00
- **Authors**: Ilja Manakov, Markus Rohm, Christoph Kern, Benedikt Schworm, Karsten Kortuem, Volker Tresp
- **Comment**: None
- **Journal**: None
- **Summary**: We cast the problem of image denoising as a domain translation problem between high and low noise domains. By modifying the cycleGAN model, we are able to learn a mapping between these domains on unpaired retinal optical coherence tomography images. In quantitative measurements and a qualitative evaluation by ophthalmologists, we show how this approach outperforms other established methods. The results indicate that the network differentiates subtle changes in the level of noise in the image. Further investigation of the model's feature maps reveals that it has learned to distinguish retinal layers and other distinct regions of the images.



### Push it to the Limit: Discover Edge-Cases in Image Data with Autoencoders
- **Arxiv ID**: http://arxiv.org/abs/1910.02713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02713v1)
- **Published**: 2019-10-07 10:36:40+00:00
- **Updated**: 2019-10-07 10:36:40+00:00
- **Authors**: Ilja Manakov, Volker Tresp
- **Comment**: Accepted as a workshop paper at MEDNeurIPS 2019
- **Journal**: None
- **Summary**: In this paper, we focus on the problem of identifying semantic factors of variation in large image datasets. By training a convolutional Autoencoder on the image data, we create encodings, which describe each datapoint at a higher level of abstraction than pixel-space. We then apply Principal Component Analysis to the encodings to disentangle the factors of variation in the data. Sorting the dataset according to the values of individual principal components, we find that samples at the high and low ends of the distribution often share specific semantic characteristics. We refer to these groups of samples as semantic groups. When applied to real-world data, this method can help discover unwanted edge-cases.



### Continual Learning in Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.02718v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02718v2)
- **Published**: 2019-10-07 10:52:14+00:00
- **Updated**: 2019-10-18 09:48:14+00:00
- **Authors**: Rahaf Aljundi
- **Comment**: PhD Thesis, Supervisor: Tinne Tuytelaars
- **Journal**: None
- **Summary**: Artificial neural networks have exceeded human-level performance in accomplishing several individual tasks (e.g. voice recognition, object recognition, and video games). However, such success remains modest compared to human intelligence that can learn and perform an unlimited number of tasks. Humans' ability of learning and accumulating knowledge over their lifetime is an essential aspect of their intelligence. Continual machine learning aims at a higher level of machine intelligence through providing the artificial agents with the ability to learn online from a non-stationary and never-ending stream of data. A key component of such a never-ending learning process is to overcome the catastrophic forgetting of previously seen data, a problem that neural networks are well known to suffer from. The work described in this thesis has been dedicated to the investigation of continual learning and solutions to mitigate the forgetting phenomena in neural networks. To approach the continual learning problem, we first assume a task incremental setting where tasks are received one at a time and data from previous tasks are not stored. Since the task incremental setting can't be assumed in all continual learning scenarios, we also study the more general online continual setting. We consider an infinite stream of data drawn from a non-stationary distribution with a supervisory or self-supervisory training signal. The proposed methods in this thesis have tackled important aspects of continual learning. They were evaluated on different benchmarks and over various learning sequences. Advances in the state of the art of continual learning have been shown and challenges for bringing continual learning into application were critically identified.



### An Interactive Control Approach to 3D Shape Reconstruction
- **Arxiv ID**: http://arxiv.org/abs/1910.02738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1910.02738v1)
- **Published**: 2019-10-07 11:45:55+00:00
- **Updated**: 2019-10-07 11:45:55+00:00
- **Authors**: Bipul Islam, Ji Liu, Anthony Yezzi, Romeil Sandhu
- **Comment**: None
- **Journal**: None
- **Summary**: The ability to accurately reconstruct the 3D facets of a scene is one of the key problems in robotic vision. However, even with recent advances with machine learning, there is no high-fidelity universal 3D reconstruction method for this optimization problem as schemes often cater to specific image modalities and are often biased by scene abnormalities. Simply put, there always remains an information gap due to the dynamic nature of real-world scenarios. To this end, we demonstrate a feedback control framework which invokes operator inputs (also prone to errors) in order to augment existing reconstruction schemes. For proof-of-concept, we choose a classical region-based stereoscopic reconstruction approach and show how an ill-posed model can be augmented with operator input to be much more robust to scene artifacts. We provide necessary conditions for stability via Lyapunov analysis and perhaps more importantly, we show that the stability depends on a notion of absolute curvature. Mathematically, this aligns with previous work that has shown Ricci curvature as proxy for functional robustness of dynamical networked systems. We conclude with results that show how our method can improve standalone reconstruction schemes.



### Deep Neural Network Compression for Image Classification and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.02747v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02747v1)
- **Published**: 2019-10-07 12:19:37+00:00
- **Updated**: 2019-10-07 12:19:37+00:00
- **Authors**: Georgios Tzelepis, Ahraz Asif, Saimir Baci, Selcuk Cavdar, Eren Erdal Aksoy
- **Comment**: The first two authors contributed equally to this work
- **Journal**: None
- **Summary**: Neural networks have been notorious for being computationally expensive. This is mainly because neural networks are often over-parametrized and most likely have redundant nodes or layers as they are getting deeper and wider. Their demand for hardware resources prohibits their extensive use in embedded devices and puts restrictions on tasks like real-time image classification or object detection. In this work, we propose a network-agnostic model compression method infused with a novel dynamical clustering approach to reduce the computational cost and memory footprint of deep neural networks. We evaluated our new compression method on five different state-of-the-art image classification and object detection networks. In classification networks, we pruned about 95% of network parameters. In advanced detection networks such as YOLOv3, our proposed compression method managed to reduce the model parameters up to 59.70% which yielded 110X less memory without sacrificing much in accuracy.



### ViP: Video Platform for PyTorch
- **Arxiv ID**: http://arxiv.org/abs/1910.02793v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02793v1)
- **Published**: 2019-10-07 13:49:50+00:00
- **Updated**: 2019-10-07 13:49:50+00:00
- **Authors**: Madan Ravi Ganesh, Eric Hofesmann, Nathan Louis, Jason Corso
- **Comment**: None
- **Journal**: None
- **Summary**: This work presents the Video Platform for PyTorch (ViP), a deep learning-based framework designed to handle and extend to any problem domain based on videos. ViP supports (1) a single unified interface applicable to all video problem domains, (2) quick prototyping of video models, (3) executing large-batch operations with reduced memory consumption, and (4) easy and reproducible experimental setups. ViP's core functionality is built with flexibility and modularity in mind to allow for smooth data flow between different parts of the platform and benchmarking against existing methods. In providing a software platform that supports multiple video-based problem domains, we allow for more cross-pollination of models, ideas and stronger generalization in the video understanding research community.



### Learning De-biased Representations with Biased Representations
- **Arxiv ID**: http://arxiv.org/abs/1910.02806v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02806v3)
- **Published**: 2019-10-07 14:11:13+00:00
- **Updated**: 2020-06-30 11:51:02+00:00
- **Authors**: Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, Seong Joon Oh
- **Comment**: Accepted to ICML 2020. Code available at
  https://github.com/clovaai/rebias
- **Journal**: None
- **Summary**: Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https://github.com/clovaai/rebias.



### Learning Navigation by Visual Localization and Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.02818v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02818v1)
- **Published**: 2019-10-07 14:31:38+00:00
- **Updated**: 2019-10-07 14:31:38+00:00
- **Authors**: Iulia Paraicu, Marius Leordeanu
- **Comment**: Submitted to ICRA 2020
- **Journal**: None
- **Summary**: When driving, people make decisions based on current traffic as well as their desired route. They have a mental map of known routes and are often able to navigate without needing directions. Current self-driving models improve their performances when using additional GPS information. Here we aim to push forward self-driving research and perform route planning even in the absence of GPS. Our system learns to predict in real-time vehicle's current location and future trajectory, as a function of time, on a known map, given only the raw video stream and the intended destination. The GPS signal is available only at training time, with training data annotation being fully automatic. Different from other published models, we predict the vehicle's trajectory for up to seven seconds ahead, from which complete steering, speed and acceleration information can be derived for the entire time span. Trajectories capture navigational information on multiple levels, from instant steering commands that depend on present traffic and obstacles ahead, to longer-term navigation decisions, towards a specific destination. We collect our dataset with a regular car and a smartphone that records video and GPS streams. The GPS data is used to derive ground-truth supervision labels and create an analytical representation of the traversed map. In tests, our system outperforms published methods on visual localization and steering and gives accurate navigation assistance between any two known locations.



### FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/1910.04076v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04076v4)
- **Published**: 2019-10-07 14:51:38+00:00
- **Updated**: 2020-10-06 19:29:03+00:00
- **Authors**: Varun Ravi Kumar, Sandesh Athni Hiremath, Stefan Milz, Christian Witt, Clement Pinnard, Senthil Yogamani, Patrick Mader
- **Comment**: Minor fixes added after ICRA 2020 camera ready submission. ICRA 2020
  presentation video - https://www.youtube.com/watch?v=qAsdpHP5e8c&t
- **Journal**: None
- **Summary**: Fisheye cameras are commonly used in applications like autonomous driving and surveillance to provide a large field of view ($>180^{\circ}$). However, they come at the cost of strong non-linear distortions which require more complex algorithms. In this paper, we explore Euclidean distance estimation on fisheye cameras for automotive scenes. Obtaining accurate and dense depth supervision is difficult in practice, but self-supervised learning approaches show promising results and could potentially overcome the problem. We present a novel self-supervised scale-aware framework for learning Euclidean distance and ego-motion from raw monocular fisheye videos without applying rectification. While it is possible to perform piece-wise linear approximation of fisheye projection surface and apply standard rectilinear models, it has its own set of issues like re-sampling distortion and discontinuities in transition regions. To encourage further research in this area, we will release our dataset as part of the WoodScape project \cite{yogamani2019woodscape}. We further evaluated the proposed algorithm on the KITTI dataset and obtained state-of-the-art results comparable to other self-supervised monocular methods. Qualitative results on an unseen fisheye video demonstrate impressive performance https://youtu.be/Sgq1WzoOmXg.



### DeshadowGAN: A Deep Learning Approach to Remove Shadows from Optical Coherence Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1910.02844v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.02844v1)
- **Published**: 2019-10-07 15:11:45+00:00
- **Updated**: 2019-10-07 15:11:45+00:00
- **Authors**: Haris Cheong, Sripad Krishna Devalla, Tan Hung Pham, Zhang Liang, Tin Aung Tun, Xiaofei Wang, Shamira Perera, Leopold Schmetterer, Aung Tin, Craig Boote, Alexandre H. Thiery, Michael J. A. Girard
- **Comment**: None
- **Journal**: None
- **Summary**: Purpose: To remove retinal shadows from optical coherence tomography (OCT) images of the optic nerve head(ONH).   Methods:2328 OCT images acquired through the center of the ONH using a Spectralis OCT machine for both eyes of 13 subjects were used to train a generative adversarial network (GAN) using a custom loss function. Image quality was assessed qualitatively (for artifacts) and quantitatively using the intralayer contrast: a measure of shadow visibility ranging from 0 (shadow-free) to 1 (strong shadow) and compared to compensated images. This was computed in the Retinal Nerve Fiber Layer (RNFL), the Inner Plexiform Layer (IPL), the Photoreceptor layer (PR) and the Retinal Pigment Epithelium (RPE) layers.   Results: Output images had improved intralayer contrast in all ONH tissue layers. On average the intralayer contrast decreased by 33.7$\pm$6.81%, 28.8$\pm$10.4%, 35.9$\pm$13.0%, and43.0$\pm$19.5%for the RNFL, IPL, PR, and RPE layers respectively, indicating successful shadow removal across all depths. This compared to 70.3$\pm$22.7%, 33.9$\pm$11.5%, 47.0$\pm$11.2%, 26.7$\pm$19.0%for compensation. Output images were also free from artifacts commonly observed with compensation.   Conclusions: DeshadowGAN significantly corrected blood vessel shadows in OCT images of the ONH. Our algorithm may be considered as a pre-processing step to improve the performance of a wide range of algorithms including those currently being used for OCT image segmentation, denoising, and classification.   Translational Relevance: DeshadowGAN could be integrated to existing OCT devices to improve the diagnosis and prognosis of ocular pathologies.



### A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1910.02923v2
- **DOI**: 10.1016/j.media.2021.102062
- **Categories**: **cs.LG**, cs.CV, cs.HC, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.02923v2)
- **Published**: 2019-10-07 17:24:33+00:00
- **Updated**: 2021-05-05 12:07:48+00:00
- **Authors**: Samuel Budd, Emma C Robinson, Bernhard Kainz
- **Comment**: Medical Image Analysis Volume 71 2021
  https://doi.org/10.1016/j.media.2021.102062
- **Journal**: Medical Image Analysis, Volume 71, 2021, 102062, ISSN 1361-8415
- **Summary**: Fully automatic deep learning has become the state-of-the-art technique for many tasks including image acquisition, analysis and interpretation, and for the extraction of clinically useful information for computer-aided detection, diagnosis, treatment planning, intervention and therapy. However, the unique challenges posed by medical image analysis suggest that retaining a human end user in any deep learning enabled system will be beneficial. In this review we investigate the role that humans might play in the development and deployment of deep learning enabled diagnostic applications and focus on techniques that will retain a significant input from a human end user. Human-in-the-Loop computing is an area that we see as increasingly important in future research due to the safety-critical nature of working in the medical domain. We evaluate four key areas that we consider vital for deep learning in the clinical practice: (1) Active Learning to choose the best data to annotate for optimal model performance; (2) Interaction with model outputs - using iterative feedback to steer models to optima for a given prediction and offering meaningful ways to interpret and respond to predictions; (3) Practical considerations - developing full scale applications and the key considerations that need to be made before deployment; (4) Future Prospective and Unanswered Questions - knowledge gaps and related research fields that will benefit human-in-the-loop computing as they evolve. We offer our opinions on the most promising directions of research and how various aspects of each area might be unified towards common goals.



### Multi-Modal Machine Learning for Flood Detection in News, Social Media and Satellite Sequences
- **Arxiv ID**: http://arxiv.org/abs/1910.02932v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.02932v1)
- **Published**: 2019-10-07 17:49:59+00:00
- **Updated**: 2019-10-07 17:49:59+00:00
- **Authors**: Kashif Ahmad, Konstantin Pogorelov, Mohib Ullah, Michael Riegler, Nicola Conci, Johannes Langguth, Ala Al-Fuqaha
- **Comment**: None
- **Journal**: MediaEval 2019
- **Summary**: In this paper we present our methods for the MediaEval 2019 Mul-timedia Satellite Task, which is aiming to extract complementaryinformation associated with adverse events from Social Media andsatellites. For the first challenge, we propose a framework jointly uti-lizing colour, object and scene-level information to predict whetherthe topic of an article containing an image is a flood event or not.Visual features are combined using early and late fusion techniquesachieving an average F1-score of82.63,82.40,81.40and76.77. Forthe multi-modal flood level estimation, we rely on both visualand textual information achieving an average F1-score of58.48and46.03, respectively. Finally, for the flooding detection in time-based satellite image sequences we used a combination of classicalcomputer-vision and machine learning approaches achieving anaverage F1-score of58.82%



### Automated Enriched Medical Concept Generation for Chest X-ray Images
- **Arxiv ID**: http://arxiv.org/abs/1910.02935v1
- **DOI**: 10.1007/978-3-030-33850-3_10
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.02935v1)
- **Published**: 2019-10-07 17:52:37+00:00
- **Updated**: 2019-10-07 17:52:37+00:00
- **Authors**: Aydan Gasimova
- **Comment**: MICCAI ML-CDS Workshop 2019
- **Journal**: None
- **Summary**: Decision support tools that rely on supervised learning require large amounts of expert annotations. Using past radiological reports obtained from hospital archiving systems has many advantages as training data above manual single-class labels: they are expert annotations available in large quantities, covering a population-representative variety of pathologies, and they provide additional context to pathology diagnoses, such as anatomical location and severity. Learning to auto-generate such reports from images present many challenges such as the difficulty in representing and generating long, unstructured textual information, accounting for spelling errors and repetition/redundancy, and the inconsistency across different annotators. We therefore propose to first learn visually-informative medical concepts from raw reports, and, using the concept predictions as image annotations, learn to auto-generate structured reports directly from images. We validate our approach on the OpenI [2] chest x-ray dataset, which consists of frontal and lateral views of chest x-ray images, their corresponding raw textual reports and manual medical subject heading (MeSH ) annotations made by radiologists.



### Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation
- **Arxiv ID**: http://arxiv.org/abs/1910.02940v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02940v2)
- **Published**: 2019-10-07 17:58:10+00:00
- **Updated**: 2020-02-12 07:10:24+00:00
- **Authors**: Hang Gao, Xizhou Zhu, Steve Lin, Jifeng Dai
- **Comment**: Project page:
  http://people.eecs.berkeley.edu/~hangg/deformable-kernels/. Accepted as
  conference paper in ICLR 2020. First two authors contributed equally
- **Journal**: None
- **Summary**: Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the "effective" receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime.   In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.



### SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability
- **Arxiv ID**: http://arxiv.org/abs/1910.02974v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.02974v3)
- **Published**: 2019-10-07 18:03:14+00:00
- **Updated**: 2020-03-09 14:35:47+00:00
- **Authors**: Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
- **Comment**: ICRA 2020
- **Journal**: None
- **Summary**: The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.



### Leveraging Vision Reconstruction Pipelines for Satellite Imagery
- **Arxiv ID**: http://arxiv.org/abs/1910.02989v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.02989v2)
- **Published**: 2019-10-07 18:14:28+00:00
- **Updated**: 2019-10-16 15:23:32+00:00
- **Authors**: Kai Zhang, Jin Sun, Noah Snavely
- **Comment**: Project Page: https://kai-46.github.io/VisSat/
- **Journal**: None
- **Summary**: Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.



### Rekall: Specifying Video Events using Compositions of Spatiotemporal Labels
- **Arxiv ID**: http://arxiv.org/abs/1910.02993v1
- **DOI**: None
- **Categories**: **cs.DB**, cs.CL, cs.CV, cs.IR
- **Links**: [PDF](http://arxiv.org/pdf/1910.02993v1)
- **Published**: 2019-10-07 18:18:37+00:00
- **Updated**: 2019-10-07 18:18:37+00:00
- **Authors**: Daniel Y. Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang, Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher R√©, Kayvon Fatahalian
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world video analysis applications require the ability to identify domain-specific events in video, such as interviews and commercials in TV news broadcasts, or action sequences in film. Unfortunately, pre-trained models to detect all the events of interest in video may not exist, and training new models from scratch can be costly and labor-intensive. In this paper, we explore the utility of specifying new events in video in a more traditional manner: by writing queries that compose outputs of existing, pre-trained models. To write these queries, we have developed Rekall, a library that exposes a data model and programming model for compositional video event specification. Rekall represents video annotations from different sources (object detectors, transcripts, etc.) as spatiotemporal labels associated with continuous volumes of spacetime in a video, and provides operators for composing labels into queries that model new video events. We demonstrate the use of Rekall in analyzing video from cable TV news broadcasts, films, static-camera vehicular video streams, and commercial autonomous vehicle logs. In these efforts, domain experts were able to quickly (in a few hours to a day) author queries that enabled the accurate detection of new events (on par with, and in some cases much more accurate than, learned approaches) and to rapidly retrieve video clips for human-in-the-loop tasks such as video content curation and training data curation. Finally, in a user study, novice users of Rekall were able to author queries to retrieve new events in video given just one hour of query development time.



### Bit Efficient Quantization for Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.04877v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.PF
- **Links**: [PDF](http://arxiv.org/pdf/1910.04877v1)
- **Published**: 2019-10-07 18:43:12+00:00
- **Updated**: 2019-10-07 18:43:12+00:00
- **Authors**: Prateeth Nayak, David Zhang, Sek Chai
- **Comment**: EMC2 - NeurIPS workshop 2019, #latentai
- **Journal**: None
- **Summary**: Quantization for deep neural networks have afforded models for edge devices that use less on-board memory and enable efficient low-power inference. In this paper, we present a comparison of model-parameter driven quantization approaches that can achieve as low as 3-bit precision without affecting accuracy. The post-training quantization approaches are data-free, and the resulting weight values are closely tied to the dataset distribution on which the model has converged to optimality. We show quantization results for a number of state-of-art deep neural networks (DNN) using large dataset like ImageNet. To better analyze quantization results, we describe the overall range and local sparsity of values afforded through various quantization schemes. We show the methods to lower bit-precision beyond quantization limits with object class clustering.



### CeliacNet: Celiac Disease Severity Diagnosis on Duodenal Histopathological Images Using Deep Residual Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.03084v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, q-bio.QM, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.03084v1)
- **Published**: 2019-10-07 21:06:41+00:00
- **Updated**: 2019-10-07 21:06:41+00:00
- **Authors**: Rasoul Sali, Lubaina Ehsan, Kamran Kowsari, Marium Khan, Christopher A. Moskaluk, Sana Syed, Donald E. Brown
- **Comment**: accepted at IEEE International Conference on Bioinformatics and
  Biomedicine (IEEE BIBM 2019)
- **Journal**: None
- **Summary**: Celiac Disease (CD) is a chronic autoimmune disease that affects the small intestine in genetically predisposed children and adults. Gluten exposure triggers an inflammatory cascade which leads to compromised intestinal barrier function. If this enteropathy is unrecognized, this can lead to anemia, decreased bone density, and, in longstanding cases, intestinal cancer. The prevalence of the disorder is 1% in the United States. An intestinal (duodenal) biopsy is considered the "gold standard" for diagnosis. The mild CD might go unnoticed due to non-specific clinical symptoms or mild histologic features. In our current work, we trained a model based on deep residual networks to diagnose CD severity using a histological scoring system called the modified Marsh score. The proposed model was evaluated using an independent set of 120 whole slide images from 15 CD patients and achieved an AUC greater than 0.96 in all classes. These results demonstrate the diagnostic power of the proposed model for CD severity classification using histological images.



### ATFaceGAN: Single Face Image Restoration and Recognition from Atmospheric Turbulence
- **Arxiv ID**: http://arxiv.org/abs/1910.03119v2
- **DOI**: 10.1109/FG47880.2020.00012
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03119v2)
- **Published**: 2019-10-07 22:44:32+00:00
- **Updated**: 2020-05-08 04:56:39+00:00
- **Authors**: Chun Pong Lau, Hossein Souri, Rama Chellappa
- **Comment**: 8 pages, 7 figures
- **Journal**: None
- **Summary**: Image degradation due to atmospheric turbulence is common while capturing images at long ranges. To mitigate the degradation due to turbulence which includes deformation and blur, we propose a generative single frame restoration algorithm which disentangles the blur and deformation due to turbulence and reconstructs a restored image. The disentanglement is achieved by decomposing the distortion due to turbulence into blur and deformation components using deblur generator and deformation correction generator respectively. Two paths of restoration are implemented to regularize the disentanglement and generate two restored images from one degraded image. A fusion function combines the features of the restored images to reconstruct a sharp image with rich details. Adversarial and perceptual losses are added to reconstruct a sharp image and suppress the artifacts respectively. Extensive experiments demonstrate the effectiveness of the proposed restoration algorithm, which achieves satisfactory performance in face restoration and face recognition.



### Improvements to Target-Based 3D LiDAR to Camera Calibration
- **Arxiv ID**: http://arxiv.org/abs/1910.03126v3
- **DOI**: 10.1109/ACCESS.2020.3010734
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03126v3)
- **Published**: 2019-10-07 23:03:16+00:00
- **Updated**: 2020-07-18 15:07:13+00:00
- **Authors**: Jiunn-Kai Huang, Jessy W. Grizzle
- **Comment**: None
- **Journal**: IEEE Access, vol. 8, 2020, pp. 134101-134110
- **Summary**: The homogeneous transformation between a LiDAR and monocular camera is required for sensor fusion tasks, such as SLAM. While determining such a transformation is not considered glamorous in any sense of the word, it is nonetheless crucial for many modern autonomous systems. Indeed, an error of a few degrees in rotation or a few percent in translation can lead to 20 cm translation errors at a distance of 5 m when overlaying a LiDAR image on a camera image. The biggest impediments to determining the transformation accurately are the relative sparsity of LiDAR point clouds and systematic errors in their distance measurements. This paper proposes (1) the use of targets of known dimension and geometry to ameliorate target pose estimation in face of the quantization and systematic errors inherent in a LiDAR image of a target, and (2) a fitting method for the LiDAR to monocular camera transformation that fundamentally assumes the camera image data is the most accurate information in one's possession.



### DexPilot: Vision Based Teleoperation of Dexterous Robotic Hand-Arm System
- **Arxiv ID**: http://arxiv.org/abs/1910.03135v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03135v2)
- **Published**: 2019-10-07 23:43:32+00:00
- **Updated**: 2019-10-14 20:58:22+00:00
- **Authors**: Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff, Dieter Fox
- **Comment**: 17 pages, first version of DexPilot
- **Journal**: None
- **Summary**: Teleoperation offers the possibility of imparting robotic systems with sophisticated reasoning skills, intuition, and creativity to perform tasks. However, current teleoperation solutions for high degree-of-actuation (DoA), multi-fingered robots are generally cost-prohibitive, while low-cost offerings usually provide reduced degrees of control. Herein, a low-cost, vision based teleoperation system, DexPilot, was developed that allows for complete control over the full 23 DoA robotic system by merely observing the bare human hand. DexPilot enables operators to carry out a variety of complex manipulation tasks that go beyond simple pick-and-place operations. This allows for collection of high dimensional, multi-modality, state-action data that can be leveraged in the future to learn sensorimotor policies for challenging manipulation tasks. The system performance was measured through speed and reliability metrics across two human demonstrators on a variety of tasks. The videos of the experiments can be found at https://sites.google.com/view/dex-pilot.



