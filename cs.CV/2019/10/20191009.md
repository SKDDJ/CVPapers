# Arxiv Papers in cs.CV on 2019-10-09
### Large-scale Gastric Cancer Screening and Localization Using Multi-task Deep Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.03729v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03729v3)
- **Published**: 2019-10-09 00:27:02+00:00
- **Updated**: 2020-09-19 23:45:16+00:00
- **Authors**: Hong Yu, Xiaofan Zhang, Lingjun Song, Liren Jiang, Xiaodi Huang, Wen Chen, Chenbin Zhang, Jiahui Li, Jiji Yang, Zhiqiang Hu, Qi Duan, Wanyuan Chen, Xianglei He, Jinshuang Fan, Weihai Jiang, Li Zhang, Chengmin Qiu, Minmin Gu, Weiwei Sun, Yangqiong Zhang, Guangyin Peng, Weiwei Shen, Guohui Fu
- **Comment**: under minor revision
- **Journal**: None
- **Summary**: Gastric cancer is one of the most common cancers, which ranks third among the leading causes of cancer death. Biopsy of gastric mucosa is a standard procedure in gastric cancer screening test. However, manual pathological inspection is labor-intensive and time-consuming. Besides, it is challenging for an automated algorithm to locate the small lesion regions in the gigapixel whole-slide image and make the decision correctly.To tackle these issues, we collected large-scale whole-slide image dataset with detailed lesion region annotation and designed a whole-slide image analyzing framework consisting of 3 networks which could not only determine the screening result but also present the suspicious areas to the pathologist for reference. Experiments demonstrated that our proposed framework achieves sensitivity of 97.05% and specificity of 92.72% in screening task and Dice coefficient of 0.8331 in segmentation task. Furthermore, we tested our best model in real-world scenario on 10,315 whole-slide images collected from 4 medical centers.



### ExpertMatcher: Automating ML Model Selection for Clients using Hidden Representations
- **Arxiv ID**: http://arxiv.org/abs/1910.03731v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03731v1)
- **Published**: 2019-10-09 00:42:16+00:00
- **Updated**: 2019-10-09 00:42:16+00:00
- **Authors**: Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang, Jayashree Kalpathy-Cramer, Ramesh Raskar
- **Comment**: In NeurIPS Workshop on Robust AI in Financial Services: Data,
  Fairness, Explainability, Trustworthiness, and Privacy, 2019
- **Journal**: None
- **Summary**: Recently, there has been the development of Split Learning, a framework for distributed computation where model components are split between the client and server (Vepakomma et al., 2018b). As Split Learning scales to include many different model components, there needs to be a method of matching client-side model components with the best server-side model components. A solution to this problem was introduced in the ExpertMatcher (Sharma et al., 2019) framework, which uses autoencoders to match raw data to models. In this work, we propose an extension of ExpertMatcher, where matching can be performed without the need to share the client's raw data representation. The technique is applicable to situations where there are local clients and centralized expert ML models, but the sharing of raw data is constrained.



### Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objects by Harnessing Environmental Fixtures
- **Arxiv ID**: http://arxiv.org/abs/1910.03781v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03781v2)
- **Published**: 2019-10-09 04:08:03+00:00
- **Updated**: 2021-04-03 01:32:21+00:00
- **Authors**: Hengyue Liang, Xibai Lou, Yang Yang, Changhyun Choi
- **Comment**: To appear on ICRA21 Xi'an
- **Journal**: None
- **Summary**: This paper introduces a challenging object grasping task and proposes a self-supervised learning approach. The goal of the task is to grasp an object which is not feasible with a single parallel gripper, but only with harnessing environment fixtures (e.g., walls, furniture, heavy objects). This Slide-to-Wall grasping task assumes no prior knowledge except the partial observation of a target object. Hence the robot should learn an effective policy given a scene observation that may include the target object, environmental fixtures, and any other disturbing objects. We formulate the problem as visual affordances learning for which Target-Oriented Deep Q-Network (TO-DQN) is proposed to efficiently learn visual affordance maps (i.e., Q-maps) to guide robot actions. Since the training necessitates robot's exploration and collision with the fixtures, TO-DQN is first trained safely with a simulated robot manipulator and then applied to a real robot. We empirically show that TO-DQN can learn to solve the task in different environment settings in simulation and outperforms a standard and a variant of Deep Q-Network (DQN) in terms of training efficiency and robustness. The testing performance in both simulation and real-robot experiments shows that the policy trained by TO-DQN achieves comparable performance to humans.



### Exploring Hate Speech Detection in Multimodal Publications
- **Arxiv ID**: http://arxiv.org/abs/1910.03814v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1910.03814v1)
- **Published**: 2019-10-09 06:53:39+00:00
- **Updated**: 2019-10-09 06:53:39+00:00
- **Authors**: Raul Gomez, Jaume Gibert, Lluis Gomez, Dimosthenis Karatzas
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we target the problem of hate speech detection in multimodal publications formed by a text and an image. We gather and annotate a large scale dataset from Twitter, MMHS150K, and propose different models that jointly analyze textual and visual information for hate speech detection, comparing them with unimodal detection. We provide quantitative and qualitative results and analyze the challenges of the proposed task. We find that, even though images are useful for the hate speech detection task, current multimodal models cannot outperform models analyzing only text. We discuss why and open the field and the dataset for further research.



### Trained Rank Pruning for Efficient Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.04576v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04576v4)
- **Published**: 2019-10-09 07:27:33+00:00
- **Updated**: 2020-01-23 21:02:36+00:00
- **Authors**: Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Wenrui Dai, Yingyong Qi, Yiran Chen, Weiyao Lin, Hongkai Xiong
- **Comment**: overlap with arXiv:1812.02402, in order to merge the two submissions
  such that withdraw this version
- **Journal**: None
- **Summary**: To accelerate DNNs inference, low-rank approximation has been widely adopted because of its solid theoretical rationale and efficient implementations. Several previous works attempted to directly approximate a pre-trained model by low-rank decomposition; however, small approximation errors in parameters can ripple over a large prediction loss. Apparently, it is not optimal to separate low-rank approximation from training. Unlike previous works, this paper integrates low rank approximation and regularization into the training process. We propose Trained Rank Pruning (TRP), which alternates between low rank approximation and training. TRP maintains the capacity of the original network while imposing low-rank constraints during training. A nuclear regularization optimized by stochastic sub-gradient descent is utilized to further promote low rank in TRP. Networks trained with TRP has a low-rank structure in nature, and is approximated with negligible performance loss, thus eliminating fine-tuning after low rank approximation. The proposed method is comprehensively evaluated on CIFAR-10 and ImageNet, outperforming previous compression counterparts using low rank approximation. Our code is available at: https://github.com/yuhuixu1993/Trained-Rank-Pruning.



### Gradient Information Guided Deraining with A Novel Network and Adversarial Training
- **Arxiv ID**: http://arxiv.org/abs/1910.03839v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03839v1)
- **Published**: 2019-10-09 08:27:09+00:00
- **Updated**: 2019-10-09 08:27:09+00:00
- **Authors**: Yinglong Wang, Haokui Zhang, Yu Liu, Qinfeng Shi, Bing Zeng
- **Comment**: 12 pages, 9 figures, 4 tables
- **Journal**: None
- **Summary**: In recent years, deep learning based methods have made significant progress in rain-removing. However, the existing methods usually do not have good generalization ability, which leads to the fact that almost all of existing methods have a satisfied performance on removing a specific type of rain streaks, but may have a relatively poor performance on other types of rain streaks. In this paper, aiming at removing multiple types of rain streaks from single images, we propose a novel deraining framework (GRASPP-GAN), which has better generalization capacity. Specifically, a modified ResNet-18 which extracts the deep features of rainy images and a revised ASPP structure which adapts to the various shapes and sizes of rain streaks are composed together to form the backbone of our deraining network. Taking the more prominent characteristics of rain streaks in the gradient domain into consideration, a gradient loss is introduced to help to supervise our deraining training process, for which, a Sobel convolution layer is built to extract the gradient information flexibly. To further boost the performance, an adversarial learning scheme is employed for the first time to train the proposed network. Extensive experiments on both real-world and synthetic datasets demonstrate that our method outperforms the state-of-the-art deraining methods quantitatively and qualitatively. In addition, without any modifications, our proposed framework also achieves good visual performance on dehazing.



### View Confusion Feature Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.03849v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03849v1)
- **Published**: 2019-10-09 08:55:03+00:00
- **Updated**: 2019-10-09 08:55:03+00:00
- **Authors**: Fangyi Liu, Lei Zhang
- **Comment**: accepted by ICCV2019
- **Journal**: None
- **Summary**: Person re-identification is an important task in video surveillance that aims to associate people across camera views at different locations and time. View variability is always a challenging problem seriously degrading person re-identification performance. Most of the existing methods either focus on how to learn view invariant feature or how to combine view-wise features. In this paper, we mainly focus on how to learn view-invariant features by getting rid of view specific information through a view confusion learning mechanism. Specifically, we propose an end-toend trainable framework, called View Confusion Feature Learning (VCFL), for person Re-ID across cameras. To the best of our knowledge, VCFL is originally proposed to learn view-invariant identity-wise features, and it is a kind of combination of view-generic and view-specific methods. Classifiers and feature centers are utilized to achieve view confusion. Furthermore, we extract sift-guided features by using bag-of-words model to help supervise the training of deep networks and enhance the view invariance of features. In experiments, our approach is validated on three benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show the superiority of the proposed method over several state-of-the-art approaches



### Learning deep forest with multi-scale Local Binary Pattern features for face anti-spoofing
- **Arxiv ID**: http://arxiv.org/abs/1910.03850v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03850v1)
- **Published**: 2019-10-09 08:57:10+00:00
- **Updated**: 2019-10-09 08:57:10+00:00
- **Authors**: Rizhao Cai, Changsheng Chen
- **Comment**: 8 pages, 6 figures, submitted to Pattern Recognition Letter in
  December 2018
- **Journal**: None
- **Summary**: Face Anti-Spoofing (FAS) is significant for the security of face recognition systems. Convolutional Neural Networks (CNNs) have been introduced to the field of the FAS and have achieved competitive performance. However, CNN-based methods are vulnerable to the adversarial attack. Attackers could generate adversarial-spoofing examples to circumvent a CNN-based face liveness detector. Studies about the transferability of the adversarial attack reveal that utilizing handcrafted feature-based methods could improve security in a system-level. Therefore, handcrafted feature-based methods are worth our exploration. In this paper, we introduce the deep forest, which is proposed as an alternative towards CNNs by Zhou et al., in the problem of the FAS. To the best of our knowledge, this is the first attempt at exploiting the deep forest in the problem of FAS. Moreover, we propose to re-devise the representation constructing by using LBP descriptors rather than the Grained-Scanning Mechanism in the original scheme. Our method achieves competitive results. On the benchmark database IDIAP REPLAY-ATTACK, 0\% Equal Error Rate (EER) is achieved. This work provides a competitive option in a fusing scheme for improving system-level security and offers important ideas to those who want to explore methods besides CNNs.



### Semantic-aware Image Deblurring
- **Arxiv ID**: http://arxiv.org/abs/1910.03853v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03853v1)
- **Published**: 2019-10-09 08:59:45+00:00
- **Updated**: 2019-10-09 08:59:45+00:00
- **Authors**: Fuhai Chen, Rongrong Ji, Chengpeng Dai, Xiaoshuai Sun, Chia-Wen Lin, Jiayi Ji, Baochang Zhang, Feiyue Huang, Liujuan Cao
- **Comment**: None
- **Journal**: None
- **Summary**: Image deblurring has achieved exciting progress in recent years. However, traditional methods fail to deblur severely blurred images, where semantic contents appears ambiguously. In this paper, we conduct image deblurring guided by the semantic contents inferred from image captioning. Specially, we propose a novel Structured-Spatial Semantic Embedding model for image deblurring (termed S3E-Deblur), which introduces a novel Structured-Spatial Semantic tree model (S3-tree) to bridge two basic tasks in computer vision: image deblurring (ImD) and image captioning (ImC). In particular, S3-tree captures and represents the semantic contents in structured spatial features in ImC, and then embeds the spatial features of the tree nodes into GAN based ImD. Co-training on S3-tree, ImC, and ImD is conducted to optimize the overall model in a multi-task end-to-end manner. Extensive experiments on severely blurred MSCOCO and GoPro datasets demonstrate the significant superiority of S3E-Deblur compared to the state-of-the-arts on both ImD and ImC tasks.



### Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.03858v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03858v1)
- **Published**: 2019-10-09 09:06:49+00:00
- **Updated**: 2019-10-09 09:06:49+00:00
- **Authors**: Zhijie Fang, Antonio M. López
- **Comment**: Paper accepted by IEEE Trans. on Intelligent Transportation Systems.
  arXiv admin note: substantial text overlap with arXiv:1807.10580
- **Journal**: None
- **Summary**: Anticipating the intentions of vulnerable road users (VRUs) such as pedestrians and cyclists is critical for performing safe and comfortable driving maneuvers. This is the case for human driving and, thus, should be taken into account by systems providing any level of driving assistance, from advanced driver assistant systems (ADAS) to fully autonomous vehicles (AVs). In this paper, we show how the latest advances on monocular vision-based human pose estimation, i.e. those relying on deep Convolutional Neural Networks (CNNs), enable to recognize the intentions of such VRUs. In the case of cyclists, we assume that they follow traffic rules to indicate future maneuvers with arm signals. In the case of pedestrians, no indications can be assumed. Instead, we hypothesize that the walking pattern of a pedestrian allows to determine if he/she has the intention of crossing the road in the path of the ego-vehicle, so that the ego-vehicle must maneuver accordingly (e.g. slowing down or stopping). In this paper, we show how the same methodology can be used for recognizing pedestrians and cyclists' intentions. For pedestrians, we perform experiments on the JAAD dataset. For cyclists, we did not found an analogous dataset, thus, we created our own one by acquiring and annotating videos which we share with the research community. Overall, the proposed pipeline provides new state-of-the-art results on the intention recognition of VRUs.



### FastSurfer -- A fast and accurate deep learning based neuroimaging pipeline
- **Arxiv ID**: http://arxiv.org/abs/1910.03866v4
- **DOI**: 10.1016/j.neuroimage.2020.117012
- **Categories**: **eess.IV**, cs.CV, q-bio.NC
- **Links**: [PDF](http://arxiv.org/pdf/1910.03866v4)
- **Published**: 2019-10-09 09:41:14+00:00
- **Updated**: 2020-05-29 13:45:00+00:00
- **Authors**: Leonie Henschel, Sailesh Conjeti, Santiago Estrada, Kersten Diers, Bruce Fischl, Martin Reuter
- **Comment**: Submitted to NeuroImage
- **Journal**: None
- **Summary**: Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, replicating FreeSurfer's anatomical segmentation including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole brain segmentation into 95 classes. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and sub-cortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (in under 1 minute) and surface-based thickness analysis (within only around 1h runtime). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and high sensitivity to group differences in dementia.



### SNIDER: Single Noisy Image Denoising and Rectification for Improving License Plate Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.03876v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03876v1)
- **Published**: 2019-10-09 10:01:07+00:00
- **Updated**: 2019-10-09 10:01:07+00:00
- **Authors**: Younkwan Lee, Juhyun Lee, Hoyeon Ahn, Moongu Jeon
- **Comment**: accepted to ICCV 2019 Workshop
- **Journal**: None
- **Summary**: In this paper, we present an algorithm for real-world license plate recognition (LPR) from a low-quality image. Our method is built upon a framework that includes denoising and rectification, and each task is conducted by Convolutional Neural Networks. Existing denoising and rectification have been treated separately as a single network in previous research. In contrast to the previous work, we here propose an end-to-end trainable network for image recovery, Single Noisy Image DEnoising and Rectification (SNIDER), which focuses on solving both the problems jointly. It overcomes those obstacles by designing a novel network to address the denoising and rectification jointly. Moreover, we propose a way to leverage optimization with the auxiliary tasks for multi-task fitting and novel training losses. Extensive experiments on two challenging LPR datasets demonstrate the effectiveness of our proposed method in recovering the high-quality license plate image from the low-quality one and show that the the proposed method outperforms other state-of-the-art methods.



### Fast Panoptic Segmentation Network
- **Arxiv ID**: http://arxiv.org/abs/1910.03892v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03892v1)
- **Published**: 2019-10-09 10:41:28+00:00
- **Updated**: 2019-10-09 10:41:28+00:00
- **Authors**: Daan de Geus, Panagiotis Meletis, Gijs Dubbelman
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present an end-to-end network for fast panoptic segmentation. This network, called Fast Panoptic Segmentation Network (FPSNet), does not require computationally costly instance mask predictions or merging heuristics. This is achieved by casting the panoptic task into a custom dense pixel-wise classification task, which assigns a class label or an instance id to each pixel. We evaluate FPSNet on the Cityscapes and Pascal VOC datasets, and find that FPSNet is faster than existing panoptic segmentation methods, while achieving better or similar panoptic segmentation performance. On the Cityscapes validation set, we achieve a Panoptic Quality score of 55.1%, at prediction times of 114 milliseconds for images with a resolution of 1024x2048 pixels. For lower resolutions of the Cityscapes dataset and for the Pascal VOC dataset, FPSNet runs at 22 and 35 frames per second, respectively.



### MixMatch Domain Adaptaion: Prize-winning solution for both tracks of VisDA 2019 challenge
- **Arxiv ID**: http://arxiv.org/abs/1910.03903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03903v1)
- **Published**: 2019-10-09 11:22:59+00:00
- **Updated**: 2019-10-09 11:22:59+00:00
- **Authors**: Danila Rukhovich, Danil Galeev
- **Comment**: accepted at TASK-CV 2019 at ICCV
- **Journal**: None
- **Summary**: We present a domain adaptation (DA) system that can be used in multi-source and semi-supervised settings. Using the proposed method we achieved 2nd place on multi-source track and 3rd place on semi-supervised track of the VisDA 2019 challenge (http://ai.bu.edu/visda-2019/). The source code of the method is available at https://github.com/filaPro/visda2019.



### A Semi-Supervised Maximum Margin Metric Learning Approach for Small Scale Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.03905v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03905v1)
- **Published**: 2019-10-09 11:26:30+00:00
- **Updated**: 2019-10-09 11:26:30+00:00
- **Authors**: T M Feroz Ali, Subhasis Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: In video surveillance, person re-identification is the task of searching person images in non-overlapping cameras. Though supervised methods for person re-identification have attained impressive performance, obtaining large scale cross-view labeled training data is very expensive. However, unlabelled data is available in abundance. In this paper, we propose a semi-supervised metric learning approach that can utilize information in unlabelled data with the help of a few labelled training samples. We also address the small sample size problem that inherently occurs due to the few labeled training data. Our method learns a discriminative space where within class samples collapse to singular points, achieving the least within class variance, and then use a maximum margin criterion over a high dimensional kernel space to maximally separate the distinct class samples. A maximum margin criterion with two levels of high dimensional mappings to kernel space is used to obtain better cross-view discrimination of the identities. Cross-view affinity learning with reciprocal nearest neighbor constraints is used to mine new pseudo-classes from the unlabelled data and update the distance metric iteratively. We attain state-of-the-art performance on four challenging datasets with a large margin.



### Skin Lesion Classification Using Ensembles of Multi-Resolution EfficientNets with Meta Data
- **Arxiv ID**: http://arxiv.org/abs/1910.03910v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03910v1)
- **Published**: 2019-10-09 11:36:45+00:00
- **Updated**: 2019-10-09 11:36:45+00:00
- **Authors**: Nils Gessert, Maximilian Nielsen, Mohsin Shaikh, René Werner, Alexander Schlaefer
- **Comment**: First place at the ISIC 2019 Skin Lesion Classification Challenge
  https://challenge2019.isic-archive.com/leaderboard.html
- **Journal**: None
- **Summary**: In this paper, we describe our method for the ISIC 2019 Skin Lesion Classification Challenge. The challenge comes with two tasks. For task 1, skin lesions have to be classified based on dermoscopic images. For task 2, dermoscopic images and additional patient meta data have to be used. A diverse dataset of 25000 images was provided for training, containing images from eight classes. The final test set contains an additional, unknown class. We address this challenging problem with a simple, data driven approach by including external data with skin lesions types that are not present in the training set. Furthermore, multi-class skin lesion classification comes with the problem of severe class imbalance. We try to overcome this problem by using loss balancing. Also, the dataset contains images with very different resolutions. We take care of this property by considering different model input resolutions and different cropping strategies. To incorporate meta data such as age, anatomical site, and sex, we use an additional dense neural network and fuse its features with the CNN. We aggregate all our models with an ensembling strategy where we search for the optimal subset of models. Our best ensemble achieves a balanced accuracy of 74.2% using five-fold cross-validation. On the official test set our method is ranked first for both tasks with a balanced accuracy of 63.6% for task 1 and 63.4% for task 2.



### Learning to Generalize One Sample at a Time with Self-Supervision
- **Arxiv ID**: http://arxiv.org/abs/1910.03915v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03915v3)
- **Published**: 2019-10-09 11:58:29+00:00
- **Updated**: 2019-10-11 14:11:10+00:00
- **Authors**: Antonio D'Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi
- **Comment**: Submitted to ICCV 2019
- **Journal**: None
- **Summary**: Although deep networks have significantly increased the performance of visual recognition methods, it is still challenging to achieve the robustness across visual domains that is necessary for real-world applications. To tackle this issue, research on domain adaptation and generalization has flourished over the last decade. An important aspect to consider when assessing the work done in the literature so far is the amount of data annotation necessary for training each approach, both at the source and target level. In this paper we argue that the data annotation overload should be minimal, as it is costly. Hence, we propose to use self-supervised learning to achieve domain generalization and adaptation. We consider learning regularities from non annotated data as an auxiliary task, and cast the problem within an Auxiliary Learning principled framework. Moreover, we suggest to further exploit the ability to learn about visual domains from non annotated images by learning from target data while testing, as data are presented to the algorithm one sample at a time. Results on three different scenarios confirm the value of our approach.



### Multiple Kernel Fisher Discriminant Metric Learning for Person Re-identification
- **Arxiv ID**: http://arxiv.org/abs/1910.03923v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.03923v1)
- **Published**: 2019-10-09 12:15:22+00:00
- **Updated**: 2019-10-09 12:15:22+00:00
- **Authors**: T M Feroz Ali, Kalpesh K Patel, Rajbabu Velmurugan, Subhasis Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: Person re-identification addresses the problem of matching pedestrian images across disjoint camera views. Design of feature descriptor and distance metric learning are the two fundamental tasks in person re-identification. In this paper, we propose a metric learning framework for person re-identification, where the discriminative metric space is learned using Kernel Fisher Discriminant Analysis (KFDA), to simultaneously maximize the inter-class variance as well as minimize the intra-class variance. We derive a Mahalanobis metric induced by KFDA and argue that KFDA is efficient to be applied for metric learning in person re-identification. We also show how the efficiency of KFDA in metric learning can be further enhanced for person re-identification by using two simple yet efficient multiple kernel learning methods. We conduct extensive experiments on three benchmark datasets for person re-identification and demonstrate that the proposed approaches have competitive performance with state-of-the-art methods.



### Towards Learning to Detect and Predict Contact Events on Vision-based Tactile Sensors
- **Arxiv ID**: http://arxiv.org/abs/1910.03973v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.03973v1)
- **Published**: 2019-10-09 13:25:12+00:00
- **Updated**: 2019-10-09 13:25:12+00:00
- **Authors**: Yazhan Zhang, Weihao Yuan, Zicheng Kan, Michael Yu Wang
- **Comment**: 10 pages, 7 figures, Accepted to conference on Robot Learning (CoRL
  2019)
- **Journal**: None
- **Summary**: In essence, successful grasp boils down to correct responses to multiple contact events between fingertips and objects. In most scenarios, tactile sensing is adequate to distinguish contact events. Due to the nature of high dimensionality of tactile information, classifying spatiotemporal tactile signals using conventional model-based methods is difficult. In this work, we propose to predict and classify tactile signal using deep learning methods, seeking to enhance the adaptability of the robotic grasp system to external event changes that may lead to grasping failure. We develop a deep learning framework and collect 6650 tactile image sequences with a vision-based tactile sensor, and the neural network is integrated into a contact-event-based robotic grasping system. In grasping experiments, we achieved 52% increase in terms of object lifting success rate with contact detection, significantly higher robustness under unexpected loads with slip prediction compared with open-loop grasps, demonstrating that integration of the proposed framework into robotic grasping system substantially improves picking success rate and capability to withstand external disturbances.



### Did you miss it? Automatic lung nodule detection combined with gaze information improves radiologists' screening performance
- **Arxiv ID**: http://arxiv.org/abs/1910.03986v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.03986v1)
- **Published**: 2019-10-09 13:43:42+00:00
- **Updated**: 2019-10-09 13:43:42+00:00
- **Authors**: Guilherme Aresta, Carlos Ferreira, João Pedrosa, Teresa Araújo, João Rebelo, Eduardo Negrão, Margarida Morgado, Filipe Alves, António Cunha, Isabel Ramos, Aurélio Campilho
- **Comment**: Submitted to IEEE Transactions on Biomedical Engineering (TBME)
- **Journal**: None
- **Summary**: Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, search lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67$\pm$0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Likewise, combining the findings of radiologist with the detection algorithm only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives. The combination of the automatic system with the gaze information allows to mitigate possible errors of the radiologist without some of the issues usually associated with automatic detection system.



### Semantic Understanding of Foggy Scenes with Purely Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1910.03997v2
- **DOI**: 10.1109/ITSC.2019.8917518
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.03997v2)
- **Published**: 2019-10-09 14:04:59+00:00
- **Updated**: 2020-06-25 07:42:58+00:00
- **Authors**: Martin Hahner, Dengxin Dai, Christos Sakaridis, Jan-Nico Zaech, Luc Van Gool
- **Comment**: independent class IoU scores corrected for BiSiNet architecture
- **Journal**: None
- **Summary**: This work addresses the problem of semantic scene understanding under foggy road conditions. Although marked progress has been made in semantic scene understanding over the recent years, it is mainly concentrated on clear weather outdoor scenes. Extending semantic segmentation methods to adverse weather conditions like fog is crucially important for outdoor applications such as self-driving cars. In this paper, we propose a novel method, which uses purely synthetic data to improve the performance on unseen real-world foggy scenes captured in the streets of Zurich and its surroundings. Our results highlight the potential and power of photo-realistic synthetic images for training and especially fine-tuning deep neural nets. Our contributions are threefold, 1) we created a purely synthetic, high-quality foggy dataset of 25,000 unique outdoor scenes, that we call Foggy Synscapes and plan to release publicly 2) we show that with this data we outperform previous approaches on real-world foggy test data 3) we show that a combination of our data and previously used data can even further improve the performance on real-world foggy data.



### MIDV-2019: Challenges of the modern mobile-based document OCR
- **Arxiv ID**: http://arxiv.org/abs/1910.04009v1
- **DOI**: 10.1117/12.2558438
- **Categories**: **cs.CV**, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.04009v1)
- **Published**: 2019-10-09 14:12:27+00:00
- **Updated**: 2019-10-09 14:12:27+00:00
- **Authors**: Konstantin Bulatov, Daniil Matalov, Vladimir V. Arlazarov
- **Comment**: 6 pages, 3 figures, 3 tables, 18 references, submitted and accepted
  to the 12th International Conference on Machine Vision (ICMV 2019)
- **Journal**: Proc. SPIE 11433 ICMV-2019 (2020), 114332N
- **Summary**: Recognition of identity documents using mobile devices has become a topic of a wide range of computer vision research. The portfolio of methods and algorithms for solving such tasks as face detection, document detection and rectification, text field recognition, and other, is growing, and the scarcity of datasets has become an important issue. One of the openly accessible datasets for evaluating such methods is MIDV-500, containing video clips of 50 identity document types in various conditions. However, the variability of capturing conditions in MIDV-500 did not address some of the key issues, mainly significant projective distortions and different lighting conditions. In this paper we present a MIDV-2019 dataset, containing video clips shot with modern high-resolution mobile cameras, with strong projective distortions and with low lighting conditions. The description of the added data is presented, and experimental baselines for text field recognition in different conditions. The dataset is available for download at ftp://smartengines.com/midv-500/extra/midv-2019/.



### Cribriform pattern detection in prostate histopathological images using deep learning models
- **Arxiv ID**: http://arxiv.org/abs/1910.04030v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04030v1)
- **Published**: 2019-10-09 14:44:49+00:00
- **Updated**: 2019-10-09 14:44:49+00:00
- **Authors**: Malay Singh, Emarene Mationg Kalaw, Wang Jie, Mundher Al-Shabi, Chin Fong Wong, Danilo Medina Giron, Kian-Tai Chong, Maxine Tan, Zeng Zeng, Hwee Kuan Lee
- **Comment**: 21 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: Architecture, size, and shape of glands are most important patterns used by pathologists for assessment of cancer malignancy in prostate histopathological tissue slides. Varying structures of glands along with cumbersome manual observations may result in subjective and inconsistent assessment. Cribriform gland with irregular border is an important feature in Gleason pattern 4. We propose using deep neural networks for cribriform pattern classification in prostate histopathological images. $163708$ Hematoxylin and Eosin (H\&E) stained images were extracted from histopathologic tissue slides of $19$ patients with prostate cancer and annotated for cribriform patterns. Our automated image classification system analyses the H\&E images to classify them as either `Cribriform' or `Non-cribriform'. Our system uses various deep learning approaches and hand-crafted image pixel intensity-based features. We present our results for cribriform pattern detection across various parameters and configuration allowed by our system. The combination of fine-tuned deep learning models outperformed the state-of-art nuclei feature based methods. Our image classification system achieved the testing accuracy of $85.93~\pm~7.54$ (cross-validated) and $88.04~\pm~5.63$ ( additional unseen test set) across three folds. In this paper, we present an annotated cribriform dataset along with analysis of deep learning models and hand-crafted features for cribriform pattern detection in prostate histopathological images.



### Deep Convolutional Neural Network for Multi-modal Image Restoration and Fusion
- **Arxiv ID**: http://arxiv.org/abs/1910.04066v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04066v1)
- **Published**: 2019-10-09 15:20:15+00:00
- **Updated**: 2019-10-09 15:20:15+00:00
- **Authors**: Xin Deng, Pier Luigi Dragotti
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel deep convolutional neural network to solve the general multi-modal image restoration (MIR) and multi-modal image fusion (MIF) problems. Different from other methods based on deep learning, our network architecture is designed by drawing inspirations from a new proposed multi-modal convolutional sparse coding (MCSC) model. The key feature of the proposed network is that it can automatically split the common information shared among different modalities, from the unique information that belongs to each single modality, and is therefore denoted with CU-Net, i.e., Common and Unique information splitting network. Specifically, the CU-Net is composed of three modules, i.e., the unique feature extraction module (UFEM), common feature preservation module (CFPM), and image reconstruction module (IRM). The architecture of each module is derived from the corresponding part in the MCSC model, which consists of several learned convolutional sparse coding (LCSC) blocks. Extensive numerical results verify the effectiveness of our method on a variety of MIR and MIF tasks, including RGB guided depth image super-resolution, flash guided non-flash image denoising, multi-focus and multi-exposure image fusion.



### BIAS: Transparent reporting of biomedical image analysis challenges
- **Arxiv ID**: http://arxiv.org/abs/1910.04071v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CY, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04071v5)
- **Published**: 2019-10-09 15:30:33+00:00
- **Updated**: 2020-08-31 13:04:02+00:00
- **Authors**: Lena Maier-Hein, Annika Reinke, Michal Kozubek, Anne L. Martel, Tal Arbel, Matthias Eisenmann, Allan Hanbuary, Pierre Jannin, Henning Müller, Sinan Onogur, Julio Saez-Rodriguez, Bram van Ginneken, Annette Kopp-Schneider, Bennett Landman
- **Comment**: 2 Appendices - Appendix A: BIAS reporting guideline for biomedical
  image analysis challenges, Appendix B: Glossary; 2 Supplements - Suppl 1:
  Form for summarizing information on challenge organization, Suppl 2:
  Structured description of a challenge design
- **Journal**: None
- **Summary**: The number of biomedical image analysis challenges organized per year is steadily increasing. These international competitions have the purpose of benchmarking algorithms on common data sets, typically to identify the best method for a given problem. Recent research, however, revealed that common practice related to challenge reporting does not allow for adequate interpretation and reproducibility of results. To address the discrepancy between the impact of challenges and the quality (control), the Biomedical I mage Analysis ChallengeS (BIAS) initiative developed a set of recommendations for the reporting of challenges. The BIAS statement aims to improve the transparency of the reporting of a biomedical image analysis challenge regardless of field of application, image modality or task category assessed. This article describes how the BIAS statement was developed and presents a checklist which authors of biomedical image analysis challenges are encouraged to include in their submission when giving a paper on a challenge into review. The purpose of the checklist is to standardize and facilitate the review process and raise interpretability and reproducibility of challenge results by making relevant information explicit.



### Wavelet Domain Style Transfer for an Effective Perception-distortion Tradeoff in Single Image Super-Resolution
- **Arxiv ID**: http://arxiv.org/abs/1910.04074v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04074v1)
- **Published**: 2019-10-09 15:44:59+00:00
- **Updated**: 2019-10-09 15:44:59+00:00
- **Authors**: Xin Deng, Ren Yang, Mai Xu, Pier Luigi Dragotti
- **Comment**: None
- **Journal**: None
- **Summary**: In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to find a high-resolution (HR) version of it which is both accurate and photo-realistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Specifically, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods.



### Patch Refinement -- Localized 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.04093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.04093v1)
- **Published**: 2019-10-09 16:11:17+00:00
- **Updated**: 2019-10-09 16:11:17+00:00
- **Authors**: Johannes Lehner, Andreas Mitterecker, Thomas Adler, Markus Hofmarcher, Bernhard Nessler, Sepp Hochreiter
- **Comment**: Machine Learning for Autonomous Driving Workshop at the 33rd
  Conference on Neural Information Processing Systems (NeurIPS 2019),
  Vancouver, Canada
- **Journal**: None
- **Summary**: We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN). We decompose the detection task into a preliminary Bird's Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets ("patches"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 % of the available training data and only LiDAR information.



### Manhattan Room Layout Reconstruction from a Single 360 image: A Comparative Study of State-of-the-art Methods
- **Arxiv ID**: http://arxiv.org/abs/1910.04099v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04099v3)
- **Published**: 2019-10-09 16:22:04+00:00
- **Updated**: 2020-12-25 05:15:51+00:00
- **Authors**: Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu, Derek Hoiem
- **Comment**: Accepted by International Journal of Computer Vision (IJCV), 2021
- **Journal**: None
- **Summary**: Recent approaches for predicting layouts from 360 panoramas produce excellent results. These approaches build on a common framework consisting of three steps: a pre-processing step based on edge-based alignment, prediction of layout elements, and a post-processing step by fitting a 3D layout to the layout elements. Until now, it has been difficult to compare the methods due to multiple different design decisions, such as the encoding network (e.g. SegNet or ResNet), type of elements predicted (e.g. corners, wall/floor boundaries, or semantic segmentation), or method of fitting the 3D layout. To address this challenge, we summarize and describe the common framework, the variants, and the impact of the design decisions. For a complete evaluation, we also propose extended annotations for the Matterport3D dataset [3], and introduce two depth-based evaluation metrics.



### Vehicle Re-identification with Viewpoint-aware Metric Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.04104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04104v1)
- **Published**: 2019-10-09 16:32:01+00:00
- **Updated**: 2019-10-09 16:32:01+00:00
- **Authors**: Ruihang Chu, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang, Yichen Wei
- **Comment**: Accepted by ICCV 2019
- **Journal**: None
- **Summary**: This paper considers vehicle re-identification (re-ID) problem. The extreme viewpoint variation (up to 180 degrees) poses great challenges for existing approaches. Inspired by the behavior in human's recognition process, we propose a novel viewpoint-aware metric learning approach. It learns two metrics for similar viewpoints and different viewpoints in two feature spaces, respectively, giving rise to viewpoint-aware network (VANet). During training, two types of constraints are applied jointly. During inference, viewpoint is firstly estimated and the corresponding metric is used. Experimental results confirm that VANet significantly improves re-ID accuracy, especially when the pair is observed from different viewpoints. Our method establishes the new state-of-the-art on two benchmarks.



### Next integrated result modelling for stopping the text field recognition process in a video using a result model with per-character alternatives
- **Arxiv ID**: http://arxiv.org/abs/1910.04107v1
- **DOI**: 10.1117/12.2559447
- **Categories**: **cs.CV**, stat.AP, 68T37
- **Links**: [PDF](http://arxiv.org/pdf/1910.04107v1)
- **Published**: 2019-10-09 16:43:42+00:00
- **Updated**: 2019-10-09 16:43:42+00:00
- **Authors**: Konstantin Bulatov, Boris Savelyev, Vladimir V. Arlazarov
- **Comment**: 6 pages, 3 figures, 1 table, submitted and accepted for the 12th
  International Conference on Machine Vision (ICMV 2019)
- **Journal**: Proc. SPIE 11433 ICMV-2019 (2020), 114332M
- **Summary**: In the field of document analysis and recognition using mobile devices for capturing, and the field of object recognition in a video stream, an important problem is determining the time when the capturing process should be stopped. Efficient stopping influences not only the total time spent for performing recognition and data entry, but the expected accuracy of the result as well. This paper is directed on extending the stopping method based on next integrated recognition result modelling, in order for it to be used within a string result recognition model with per-character alternatives. The stopping method and notes on its extension are described, and experimental evaluation is performed on an open dataset MIDV-500. The method was compares with previously published methods based on input observations clustering. The obtained results indicate that the stopping method based on the next integrated result modelling allows to achieve higher accuracy, even when compared with the best achievable configuration of the competing methods.



### Imagined Value Gradients: Model-Based Policy Optimization with Transferable Latent Dynamics Models
- **Arxiv ID**: http://arxiv.org/abs/1910.04142v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.04142v1)
- **Published**: 2019-10-09 17:37:52+00:00
- **Updated**: 2019-10-09 17:37:52+00:00
- **Authors**: Arunkumar Byravan, Jost Tobias Springenberg, Abbas Abdolmaleki, Roland Hafner, Michael Neunert, Thomas Lampe, Noah Siegel, Nicolas Heess, Martin Riedmiller
- **Comment**: To appear at the 3rd annual Conference on Robot Learning, Osaka,
  Japan (CoRL 2019). 24 pages including appendix (main paper - 8 pages)
- **Journal**: None
- **Summary**: Humans are masters at quickly learning many complex tasks, relying on an approximate understanding of the dynamics of their environments. In much the same way, we would like our learning agents to quickly adapt to new tasks. In this paper, we explore how model-based Reinforcement Learning (RL) can facilitate transfer to new tasks. We develop an algorithm that learns an action-conditional, predictive model of expected future observations, rewards and values from which a policy can be derived by following the gradient of the estimated value along imagined trajectories. We show how robust policy optimization can be achieved in robot manipulation tasks even with approximate models that are learned directly from vision and proprioception. We evaluate the efficacy of our approach in a transfer learning scenario, re-using previously learned models on tasks with different reward structures and visual distractors, and show a significant improvement in learning speed compared to strong off-policy baselines. Videos with results can be found at https://sites.google.com/view/ivg-corl19



### Unaligned Image-to-Sequence Transformation with Loop Consistency
- **Arxiv ID**: http://arxiv.org/abs/1910.04149v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.04149v1)
- **Published**: 2019-10-09 17:50:45+00:00
- **Updated**: 2019-10-09 17:50:45+00:00
- **Authors**: Siyang Wang, Justin Lazarow, Kwonjoon Lee, Zhuowen Tu
- **Comment**: None
- **Journal**: None
- **Summary**: We tackle the problem of modeling sequential visual phenomena. Given examples of a phenomena that can be divided into discrete time steps, we aim to take an input from any such time and realize this input at all other time steps in the sequence. Furthermore, we aim to do this without ground-truth aligned sequences -- avoiding the difficulties needed for gathering aligned data. This generalizes the unpaired image-to-image problem from generating pairs to generating sequences. We extend cycle consistency to loop consistency and alleviate difficulties associated with learning in the resulting long chains of computation. We show competitive results compared to existing image-to-image techniques when modeling several different data sets including the Earth's seasons and aging of human faces.



### Image Quality Assessment for Rigid Motion Compensation
- **Arxiv ID**: http://arxiv.org/abs/1910.04254v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04254v2)
- **Published**: 2019-10-09 21:03:45+00:00
- **Updated**: 2019-11-29 16:01:13+00:00
- **Authors**: Alexander Preuhs, Michael Manhart, Philipp Roser, Bernhard Stimpel, Christopher Syben, Marios Psychogios, Markus Kowarschik, Andreas Maier
- **Comment**: Accepted at MedNeurips 2019
- **Journal**: None
- **Summary**: Diagnostic stroke imaging with C-arm cone-beam computed tomography (CBCT) enables reduction of time-to-therapy for endovascular procedures. However, the prolonged acquisition time compared to helical CT increases the likelihood of rigid patient motion. Rigid motion corrupts the geometry alignment assumed during reconstruction, resulting in image blurring or streaking artifacts. To reestablish the geometry, we estimate the motion trajectory by an autofocus method guided by a neural network, which was trained to regress the reprojection error, based on the image information of a reconstructed slice. The network was trained with CBCT scans from 19 patients and evaluated using an additional test patient. It adapts well to unseen motion amplitudes and achieves superior results in a motion estimation benchmark compared to the commonly used entropy-based method.



### Explaining image classifiers by removing input features using generative models
- **Arxiv ID**: http://arxiv.org/abs/1910.04256v7
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04256v7)
- **Published**: 2019-10-09 21:08:25+00:00
- **Updated**: 2020-10-06 16:08:42+00:00
- **Authors**: Chirag Agarwal, Anh Nguyen
- **Comment**: Accepted to Asian Conference on Computer Vision (ACCV), 2020
- **Journal**: None
- **Summary**: Perturbation-based explanation methods often measure the contribution of an input feature to an image classifier's outputs by heuristically removing it via e.g. blurring, adding noise, or graying out, which often produce unrealistic, out-of-samples. Instead, we propose to integrate a generative inpainter into three representative attribution methods to remove an input feature. Our proposed change improved all three methods in (1) generating more plausible counterfactual samples under the true data distribution; (2) being more accurate according to three metrics: object localization, deletion, and saliency metrics; and (3) being more robust to hyperparameter changes. Our findings were consistent across both ImageNet and Places365 datasets and two different pairs of classifiers and inpainters.



### Deep localization of protein structures in fluorescence microscopy images
- **Arxiv ID**: http://arxiv.org/abs/1910.04287v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04287v3)
- **Published**: 2019-10-09 22:53:19+00:00
- **Updated**: 2021-10-08 00:57:07+00:00
- **Authors**: Muhammad Tahir, Saeed Anwar, Ajmal Mian, Abdul Wahab Muzaffar
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate localization of proteins from fluorescence microscopy images is challenging due to the inter-class similarities and intra-class disparities introducing grave concerns in addressing multi-class classification problems. Conventional machine learning-based image prediction pipelines rely heavily on pre-processing such as normalization and segmentation followed by hand-crafted feature extraction to identify useful, informative, and application-specific features. Here, we demonstrate that deep learning-based pipelines can effectively classify protein images from different datasets. We propose an end-to-end Protein Localization Convolutional Neural Network (PLCNN) that classifies protein images more accurately and reliably. PLCNN processes raw imagery without involving any pre-processing steps and produces outputs without any customization or parameter adjustment for a particular dataset. Experimental analysis is performed on five benchmark datasets. PLCNN consistently outperformed the existing state-of-the-art approaches from traditional machine learning and deep architectures. This study highlights the importance of deep learning for the analysis of fluorescence microscopy protein imagery. The proposed deep pipeline can better guide drug designing procedures in the pharmaceutical industry and open new avenues for researchers in computational biology and bioinformatics.



