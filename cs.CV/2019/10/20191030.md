# Arxiv Papers in cs.CV on 2019-10-30
### Fitness Done Right: a Real-time Intelligent Personal Trainer for Exercise Correction
- **Arxiv ID**: http://arxiv.org/abs/1911.07935v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1911.07935v1)
- **Published**: 2019-10-30 00:05:00+00:00
- **Updated**: 2019-10-30 00:05:00+00:00
- **Authors**: Yun Chen, Yiyue Chen, Zhengzhong Tu
- **Comment**: 7 pages, 8 figures
- **Journal**: None
- **Summary**: Keeping fit has been increasingly important for people nowadays. However, people may not get expected exercise results without following professional guidance while hiring personal trainers is expensive. In this paper, an effective real-time system called Fitness Done Right (FDR) is proposed for helping people exercise correctly on their own. The system includes detecting human body parts, recognizing exercise pose and detecting errors for test poses as well as giving correction advice. Generally, two branch multi-stage CNN is used for training data sets in order to learn human body parts and associations. Then, considering two poses, which are plank and squat in our model, we design a detection algorithm, combining Euclidean and angle distances, to determine the pose in the image. Finally, key values for key features of the two poses are computed correspondingly in the pose error detection part, which helps give correction advice. We conduct our system in real-time situation with error rate down to $1.2\%$, and the screenshots of experimental results are also presented.



### Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.14442v3
- **DOI**: 10.1109/LRA.2020.2965078
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14442v3)
- **Published**: 2019-10-30 01:04:37+00:00
- **Updated**: 2021-08-09 22:58:22+00:00
- **Authors**: Fei Xia, William B. Shen, Chengshu Li, Priya Kasimbeg, Micael Tchapmi, Alexander Toshev, Li Fei-Fei, Roberto Martín-Martín, Silvio Savarese
- **Comment**: 9 pages, 8 figures. Consider citing a newer version
  (https://arxiv.org/abs/2012.02924) if you are using iGibson
- **Journal**: IEEE Robotics and Automation Letters, Vol. 5, No. 2, April 2020
- **Summary**: We present Interactive Gibson Benchmark, the first comprehensive benchmark for training and evaluating Interactive Navigation: robot navigation strategies where physical interaction with objects is allowed and even encouraged to accomplish a task. For example, the robot can move objects if needed in order to clear a path leading to the goal location. Our benchmark comprises two novel elements: 1) a new experimental setup, the Interactive Gibson Environment (iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high fidelity physical dynamics of the robot and common objects found in these scenes; 2) a set of Interactive Navigation metrics which allows one to study the interplay between navigation and physical interaction. We present and evaluate multiple learning-based baselines in Interactive Gibson, and provide insights into regimes of navigation with different trade-offs between navigation path efficiency and disturbance of surrounding objects. We make our benchmark publicly available(https://sites.google.com/view/interactivegibsonenv) and encourage researchers from all disciplines in robotics (e.g. planning, learning, control) to propose, evaluate, and compare their Interactive Navigation solutions in Interactive Gibson.



### C3DVQA: Full-Reference Video Quality Assessment with 3D Convolutional Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.13646v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13646v2)
- **Published**: 2019-10-30 03:21:47+00:00
- **Updated**: 2020-03-04 09:11:49+00:00
- **Authors**: Munan Xu, Junming Chen, Haiqiang Wang, Shan Liu, Ge Li, Zhiqiang Bai
- **Comment**: Cam ready, 5 pages, 3 figures, Accepted by ICASSP 2020
- **Journal**: None
- **Summary**: Traditional video quality assessment (VQA) methods evaluate localized picture quality and video score is predicted by temporally aggregating frame scores. However, video quality exhibits different characteristics from static image quality due to the existence of temporal masking effects. In this paper, we present a novel architecture, namely C3DVQA, that uses Convolutional Neural Network with 3D kernels (C3D) for full-reference VQA task. C3DVQA combines feature learning and score pooling into one spatiotemporal feature learning process. We use 2D convolutional layers to extract spatial features and 3D convolutional layers to learn spatiotemporal features. We empirically found that 3D convolutional layers are capable to capture temporal masking effects of videos. We evaluated the proposed method on the LIVE and CSIQ datasets. The experimental results demonstrate that the proposed method achieves the state-of-the-art performance.



### Outliagnostics: Visualizing Temporal Discrepancy in Outlying Signatures of Data Entries
- **Arxiv ID**: http://arxiv.org/abs/1910.13656v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13656v1)
- **Published**: 2019-10-30 04:24:38+00:00
- **Updated**: 2019-10-30 04:24:38+00:00
- **Authors**: Vung Pham, Tommy Dang
- **Comment**: in IEEE Visualization in Data Science (IEEE VDS) (2019)
- **Journal**: None
- **Summary**: This paper presents an approach to analyzing two-dimensional temporal datasets focusing on identifying observations that are significant in calculating the outliers of a scatterplot. We also propose a prototype, called Outliagnostics, to guide users when interactively exploring abnormalities in large time series. Instead of focusing on detecting outliers at each time point, we monitor and display the discrepant temporal signatures of each data entry concerning the overall distributions. Our prototype is designed to handle these tasks in parallel to improve performance. To highlight the benefits and performance of our approach, we illustrate and validate the use of Outliagnostics on real-world datasets of various sizes in different parallelism configurations. This work also discusses how to extend these ideas to handle time series with a higher number of dimensions and provides a prototype for this type of datasets.



### Facial Image Deformation Based on Landmark Detection
- **Arxiv ID**: http://arxiv.org/abs/1910.13671v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13671v2)
- **Published**: 2019-10-30 04:57:36+00:00
- **Updated**: 2021-06-06 12:52:35+00:00
- **Authors**: Chaoyue Song, Yugang Chen, Shulai Zhang, Bingbing Ni
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we use facial landmarks to make the deformation for facial images more authentic. The deformation includes the expansion of eyes and the shrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark detector is utilized to provide control points for deformation. Bilinear interpolation is used in the expansion and Moving Least Squares methods (MLS) including Affine Deformation, Similarity Deformation and Rigid Deformation are used in the shrinking. We compare the running time as well as the quality of deformed images using different MLS methods. The experimental results show that the Rigid Deformation which can keep other parts of the images unchanged performs better even if it takes the longest time.



### Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly
- **Arxiv ID**: http://arxiv.org/abs/1910.13675v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13675v2)
- **Published**: 2019-10-30 05:11:53+00:00
- **Updated**: 2020-05-16 23:20:28+00:00
- **Authors**: Kevin Zakka, Andy Zeng, Johnny Lee, Shuran Song
- **Comment**: Code, videos, and supplemental material are available at
  https://form2fit.github.io/
- **Journal**: None
- **Summary**: Is it possible to learn policies for robotic assembly that can generalize to new objects? We explore this idea in the context of the kit assembly task. Since classic methods rely heavily on object pose estimation, they often struggle to generalize to new objects without 3D CAD models or task-specific training data. In this work, we propose to formulate the kit assembly task as a shape matching problem, where the goal is to learn a shape descriptor that establishes geometric correspondences between object surfaces and their target placement locations from visual input. This formulation enables the model to acquire a broader understanding of how shapes and surfaces fit together for assembly -- allowing it to generalize to new objects and kits. To obtain training data for our model, we present a self-supervised data-collection pipeline that obtains ground truth object-to-placement correspondences by disassembling complete kits. Our resulting real-world system, Form2Fit, learns effective pick and place strategies for assembling objects into a variety of kits -- achieving $90\%$ average success rates under different initial conditions (e.g. varying object and kit poses), $94\%$ success under new configurations of multiple kits, and over $86\%$ success with completely new objects and kits.



### Multi Modal Semantic Segmentation using Synthetic Data
- **Arxiv ID**: http://arxiv.org/abs/1910.13676v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.4
- **Links**: [PDF](http://arxiv.org/pdf/1910.13676v1)
- **Published**: 2019-10-30 05:13:33+00:00
- **Updated**: 2019-10-30 05:13:33+00:00
- **Authors**: Kartik Srivastava, Akash Kumar Singh, Guruprasad M. Hegde
- **Comment**: Accepted in 3rd Edition of Deep Learning for Automated Driving (DLAD)
  workshop, IEEE International Conference on Intelligent Transportation Systems
  (ITSC'19) [see
  https://sites.google.com/view/dlad-bp-itsc2019/schedule?authuser=0#h.p_gI84BCoB0_bJ]
- **Journal**: None
- **Summary**: Semantic understanding of scenes in three-dimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true distance of separation to objects which are crucial for taking mission critical decisions. As a first step, in this work we investigate the possibility of semantically classifying different parts of a given scene in 3D by learning the underlying geometric context in addition to the texture cues BUT in the absence of labelled real-world datasets. To this end we generate a large number of synthetic scenes, their pixel-wise labels and corresponding 3D representations using CARLA software framework. We then build a deep neural network that learns underlying category specific 3D representation and texture cues from color information of the rendered synthetic scenes. Further on we apply the learned model on different real world datasets to evaluate its performance. Our preliminary investigation of results show that the neural network is able to learn the geometric context from synthetic scenes and effectively apply this knowledge to classify each point of a 3D representation of a scene in real-world.



### The Domain Shift Problem of Medical Image Segmentation and Vendor-Adaptation by Unet-GAN
- **Arxiv ID**: http://arxiv.org/abs/1910.13681v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13681v1)
- **Published**: 2019-10-30 05:36:21+00:00
- **Updated**: 2019-10-30 05:36:21+00:00
- **Authors**: Wenjun Yan, Yuanyuan Wang, Shengjia Gu, Lu Huang, Fuhua Yan, Liming Xia, Qian Tao
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural network (CNN), in particular the Unet, is a powerful method for medical image segmentation. To date Unet has demonstrated state-of-art performance in many complex medical image segmentation tasks, especially under the condition when the training and testing data share the same distribution (i.e. come from the same source domain). However, in clinical practice, medical images are acquired from different vendors and centers. The performance of a U-Net trained from a particular source domain, when transferred to a different target domain (e.g. different vendor, acquisition parameter), can drop unexpectedly. Collecting a large amount of annotation from each new domain to retrain the U-Net is expensive, tedious, and practically impossible. In this work, we proposed a generic framework to address this problem, consisting of (1) an unpaired generative adversarial network (GAN) for vendor-adaptation, and (2) a Unet for object segmentation. In the proposed Unet-GAN architecture, GAN learns from Unet at the feature level that is segmentation-specific. We used cardiac cine MRI as the example, with three major vendors (Philips, Siemens, and GE) as three domains, while the methodology can be extended to medical images segmentation in general. The proposed method showed significant improvement of the segmentation results across vendors. The proposed Unet-GAN provides an annotation-free solution to the cross-vendor medical image segmentation problem, potentially extending a trained deep learning model to multi-center and multi-vendor use in real clinical scenario.



### Dual Illumination Estimation for Robust Exposure Correction
- **Arxiv ID**: http://arxiv.org/abs/1910.13688v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/1910.13688v1)
- **Published**: 2019-10-30 05:59:30+00:00
- **Updated**: 2019-10-30 05:59:30+00:00
- **Authors**: Qing Zhang, Yongwei Nie, Wei-Shi Zheng
- **Comment**: Computer Graphics Forum (Proceedings of Pacific Graphics 2019)
- **Journal**: None
- **Summary**: Exposure correction is one of the fundamental tasks in image processing and computational photography. While various methods have been proposed, they either fail to produce visually pleasing results, or only work well for limited types of image (e.g., underexposed images). In this paper, we present a novel automatic exposure correction method, which is able to robustly produce high-quality results for images of various exposure conditions (e.g., underexposed, overexposed, and partially under- and over-exposed). At the core of our approach is the proposed dual illumination estimation, where we separately cast the under- and over-exposure correction as trivial illumination estimation of the input image and the inverted input image. By performing dual illumination estimation, we obtain two intermediate exposure correction results for the input image, with one fixes the underexposed regions and the other one restores the overexposed regions. A multi-exposure image fusion technique is then employed to adaptively blend the visually best exposed parts in the two intermediate exposure correction images and the input image into a globally well-exposed image. Experiments on a number of challenging images demonstrate the effectiveness of the proposed approach and its superiority over the state-of-the-art methods and popular automatic exposure correction tools.



### MonSter: Awakening the Mono in Stereo
- **Arxiv ID**: http://arxiv.org/abs/1910.13708v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13708v1)
- **Published**: 2019-10-30 08:13:31+00:00
- **Updated**: 2019-10-30 08:13:31+00:00
- **Authors**: Yotam Gil, Shay Elmalem, Harel Haim, Emanuel Marom, Raja Giryes
- **Comment**: None
- **Journal**: None
- **Summary**: Passive depth estimation is among the most long-studied fields in computer vision. The most common methods for passive depth estimation are either a stereo or a monocular system. Using the former requires an accurate calibration process, and has a limited effective range. The latter, which does not require extrinsic calibration but generally achieves inferior depth accuracy, can be tuned to achieve better results in part of the depth range. In this work, we suggest combining the two frameworks. We propose a two-camera system, in which the cameras are used jointly to extract a stereo depth and individually to provide a monocular depth from each camera. The combination of these depth maps leads to more accurate depth estimation. Moreover, enforcing consistency between the extracted maps leads to a novel online self-calibration strategy. We present a prototype camera that demonstrates the benefits of the proposed combination, for both self-calibration and depth reconstruction in real-world scenes.



### Probabilistic Inference for Camera Calibration in Light Microscopy under Circular Motion
- **Arxiv ID**: http://arxiv.org/abs/1910.13740v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13740v1)
- **Published**: 2019-10-30 09:49:55+00:00
- **Updated**: 2019-10-30 09:49:55+00:00
- **Authors**: Yuanhao Guo, Fons J. Verbeek, Ge Yang
- **Comment**: 4 pages, 4 figures
- **Journal**: None
- **Summary**: Robust and accurate camera calibration is essential for 3D reconstruction in light microscopy under circular motion. Conventional methods require either accurate key point matching or precise segmentation of the axial-view images. Both remain challenging because specimens often exhibit transparency/translucency in a light microscope. To address those issues, we propose a probabilistic inference based method for the camera calibration that does not require sophisticated image pre-processing. Based on 3D projective geometry, our method assigns a probability on each of a range of voxels that cover the whole object. The probability indicates the likelihood of a voxel belonging to the object to be reconstructed. Our method maximizes a joint probability that distinguishes the object from the background. Experimental results show that the proposed method can accurately recover camera configurations in both light microscopy and natural scene imaging. Furthermore, the method can be used to produce high-fidelity 3D reconstructions and accurate 3D measurements.



### A CNN-based methodology for breast cancer diagnosis using thermal images
- **Arxiv ID**: http://arxiv.org/abs/1910.13757v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13757v1)
- **Published**: 2019-10-30 10:24:42+00:00
- **Updated**: 2019-10-30 10:24:42+00:00
- **Authors**: Juan Zuluaga-Gomez, Zeina Al Masry, Khaled Benaggoune, Safa Meraghni, Noureddine Zerhouni
- **Comment**: 19 pages, 7 figures, 5 tables. Clinical Breast Cancer
- **Journal**: None
- **Summary**: Micro Abstract: A recent study from GLOBOCAN disclosed that during 2018 two million women worldwide had been diagnosed from breast cancer. This study presents a computer-aided diagnosis system based on convolutional neural networks as an alternative diagnosis methodology for breast cancer diagnosis with thermal images. Experimental results showed that lower false-positives and false-negatives classification rates are obtained when data pre-processing and data augmentation techniques are implemented in these thermal images. Background: There are many types of breast cancer screening techniques such as, mammography, magnetic resonance imaging, ultrasound and blood sample tests, which require either, expensive devices or personal qualified. Currently, some countries still lack access to these main screening techniques due to economic, social or cultural issues. The objective of this study is to demonstrate that computer-aided diagnosis(CAD) systems based on convolutional neural networks (CNN) are faster, reliable and robust than other techniques. Methods: We performed a study of the influence of data pre-processing, data augmentation and database size versus a proposed set of CNN models. Furthermore, we developed a CNN hyper-parameters fine-tuning optimization algorithm using a tree parzen estimator. Results: Among the 57 patients database, our CNN models obtained a higher accuracy (92\%) and F1-score (92\%) that outperforms several state-of-the-art architectures such as ResNet50, SeResNet50 and Inception. Also, we demonstrated that a CNN model that implements data-augmentation techniques reach identical performance metrics in comparison with a CNN that uses a database up to 50\% bigger. Conclusion: This study highlights the benefits of data augmentation and CNNs in thermal breast images. Also, it measures the influence of the database size in the performance of CNNs.



### Deep Learning vs. Traditional Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1910.13796v1
- **DOI**: 10.1007/978-3-030-17795-9
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.13796v1)
- **Published**: 2019-10-30 12:25:10+00:00
- **Updated**: 2019-10-30 12:25:10+00:00
- **Authors**: Niall O' Mahony, Sean Campbell, Anderson Carvalho, Suman Harapanahalli, Gustavo Velasco-Hernandez, Lenka Krpalkova, Daniel Riordan, Joseph Walsh
- **Comment**: None
- **Journal**: in Advances in Computer Vision Proceedings of the 2019 Computer
  Vision Conference (CVC). Springer Nature Switzerland AG, pp. 128-144
- **Summary**: Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised



### Flash X-ray diffraction imaging in 3D: a proposed analysis pipeline
- **Arxiv ID**: http://arxiv.org/abs/1910.14029v2
- **DOI**: 10.1364/JOSAA.390384
- **Categories**: **eess.IV**, cs.CV, cs.DC, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.14029v2)
- **Published**: 2019-10-30 13:29:54+00:00
- **Updated**: 2020-02-12 09:04:33+00:00
- **Authors**: Jing Liu, Stefan Engblom, Carl Nettelblad
- **Comment**: None
- **Journal**: None
- **Summary**: Modern Flash X-ray diffraction Imaging (FXI) acquires diffraction signals from single biomolecules at a high repetition rate from X-ray Free Electron Lasers (XFELs), easily obtaining millions of 2D diffraction patterns from a single experiment. Due to the stochastic nature of FXI experiments and the massive volumes of data, retrieving 3D electron densities from raw 2D diffraction patterns is a challenging and time-consuming task.   We propose a semi-automatic data analysis pipeline for FXI experiments, which includes four steps: hit finding and preliminary filtering, pattern classification, 3D Fourier reconstruction, and post analysis. We also include a recently developed bootstrap methodology in the post-analysis step for uncertainty analysis and quality control. To achieve the best possible resolution, we further suggest using background subtraction, signal windowing, and convex optimization techniques when retrieving the Fourier phases in the post-analysis step.   As an application example, we quantified the 3D electron structure of the PR772 virus using the proposed data-analysis pipeline. The retrieved structure was above the detector-edge resolution and clearly showed the pseudo-icosahedral capsid of the PR772.



### Comprehensive Video Understanding: Video summarization with content-based video recommender design
- **Arxiv ID**: http://arxiv.org/abs/1910.13888v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13888v1)
- **Published**: 2019-10-30 14:29:08+00:00
- **Updated**: 2019-10-30 14:29:08+00:00
- **Authors**: Yudong Jiang, Kaixu Cui, Bo Peng, Changliang Xu
- **Comment**: 2019 International Conference on Computer Vision Workshop (ICCVW
  2019)
- **Journal**: None
- **Summary**: Video summarization aims to extract keyframes/shots from a long video. Previous methods mainly take diversity and representativeness of generated summaries as prior knowledge in algorithm design. In this paper, we formulate video summarization as a content-based recommender problem, which should distill the most useful content from a long video for users who suffer from information overload. A scalable deep neural network is proposed on predicting if one video segment is a useful segment for users by explicitly modelling both segment and video. Moreover, we accomplish scene and action recognition in untrimmed videos in order to find more correlations among different aspects of video understanding tasks. Also, our paper will discuss the effect of audio and visual features in summarization task. We also extend our work by data augmentation and multi-task learning for preventing the model from early-stage overfitting. The final results of our model win the first place in ICCV 2019 CoView Workshop Challenge Track.



### Real-time Convolutional Networks for Depth-based Human Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1910.13911v1
- **DOI**: 10.1109/IROS.2018.8593383
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.13911v1)
- **Published**: 2019-10-30 14:52:46+00:00
- **Updated**: 2019-10-30 14:52:46+00:00
- **Authors**: Angel Martínez-González, Michael Villamizar, Olivier Canévet, Jean-Marc Odobez
- **Comment**: Published in IROS 2018
- **Journal**: 2018 IEEE International Conference on Intelligent Robots and
  Systems, Madrid, Spain,
- **Summary**: We propose to combine recent Convolutional Neural Networks (CNN) models with depth imaging to obtain a reliable and fast multi-person pose estimation algorithm applicable to Human Robot Interaction (HRI) scenarios. Our hypothesis is that depth images contain less structures and are easier to process than RGB images while keeping the required information for human detection and pose inference, thus allowing the use of simpler networks for the task. Our contributions are threefold. (i) we propose a fast and efficient network based on residual blocks (called RPM) for body landmark localization from depth images; (ii) we created a public dataset DIH comprising more than 170k synthetic images of human bodies with various shapes and viewpoints as well as real (annotated) data for evaluation; (iii) we show that our model trained on synthetic data from scratch can perform well on real data, obtaining similar results to larger models initialized with pre-trained networks. It thus provides a good trade-off between performance and computation. Experiments on real data demonstrate the validity of our approach.



### Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial Vehicles using 3D LiDAR
- **Arxiv ID**: http://arxiv.org/abs/1910.14031v3
- **DOI**: 10.1109/IROS45743.2020.9341343
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14031v3)
- **Published**: 2019-10-30 15:03:21+00:00
- **Updated**: 2020-11-18 01:23:36+00:00
- **Authors**: Harnaik Dhami, Kevin Yu, Tianshu Xu, Qian Zhu, Kshitiz Dhakal, James Friel, Song Li, Pratap Tokekar
- **Comment**: 8 pages, 10 figures, 1 table, Accepted to IROS 2020
- **Journal**: None
- **Summary**: We present techniques to measure crop heights using a 3D Light Detection and Ranging (LiDAR) sensor mounted on an Unmanned Aerial Vehicle (UAV). Knowing the height of plants is crucial to monitor their overall health and growth cycles, especially for high-throughput plant phenotyping. We present a methodology for extracting plant heights from 3D LiDAR point clouds, specifically focusing on plot-based phenotyping environments. We also present a toolchain that can be used to create phenotyping farms for use in Gazebo simulations. The tool creates a randomized farm with realistic 3D plant and terrain models. We conducted a series of simulations and hardware experiments in controlled and natural settings. Our algorithm was able to estimate the plant heights in a field with 112 plots with a root mean square error (RMSE) of 6.1 cm. This is the first such dataset for 3D LiDAR from an airborne robot over a wheat field. The developed simulation toolchain, algorithmic implementation, and datasets can be found on the GitHub repository located at https://github.com/hsd1121/PointCloudProcessing.



### Neural View-Interpolation for Sparse Light Field Video
- **Arxiv ID**: http://arxiv.org/abs/1910.13921v3
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13921v3)
- **Published**: 2019-10-30 15:18:37+00:00
- **Updated**: 2020-04-22 10:27:50+00:00
- **Authors**: Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel
- **Comment**: 11 pages, 12 figures
- **Journal**: None
- **Summary**: We suggest representing light field (LF) videos as "one-off" neural networks (NN), i.e., a learned mapping from view-plus-time coordinates to high-resolution color values, trained on sparse views. Initially, this sounds like a bad idea for three main reasons: First, a NN LF will likely have less quality than a same-sized pixel basis representation. Second, only few training data, e.g., 9 exemplars per frame are available for sparse LF videos. Third, there is no generalization across LFs, but across view and time instead. Consequently, a network needs to be trained for each LF video. Surprisingly, these problems can turn into substantial advantages: Other than the linear pixel basis, a NN has to come up with a compact, non-linear i.e., more intelligent, explanation of color, conditioned on the sparse view and time coordinates. As observed for many NN however, this representation now is interpolatable: if the image output for sparse view coordinates is plausible, it is for all intermediate, continuous coordinates as well. Our specific network architecture involves a differentiable occlusion-aware warping step, which leads to a compact set of trainable parameters and consequently fast learning and fast execution.



### Towards Scalable, Efficient and Accurate Deep Spiking Neural Networks with Backward Residual Connections, Stochastic Softmax and Hybridization
- **Arxiv ID**: http://arxiv.org/abs/1910.13931v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.13931v1)
- **Published**: 2019-10-30 15:31:15+00:00
- **Updated**: 2019-10-30 15:31:15+00:00
- **Authors**: Priyadarshini Panda, Aparna Aketi, Kaushik Roy
- **Comment**: 14 pages, 7 figures, 17 tables
- **Journal**: None
- **Summary**: Spiking Neural Networks (SNNs) may offer an energy-efficient alternative for implementing deep learning applications. In recent years, there have been several proposals focused on supervised (conversion, spike-based gradient descent) and unsupervised (spike timing dependent plasticity) training methods to improve the accuracy of SNNs on large-scale tasks. However, each of these methods suffer from scalability, latency and accuracy limitations. In this paper, we propose novel algorithmic techniques of modifying the SNN configuration with backward residual connections, stochastic softmax and hybrid artificial-and-spiking neuronal activations to improve the learning ability of the training methodologies to yield competitive accuracy, while, yielding large efficiency gains over their artificial counterparts. Note, artificial counterparts refer to conventional deep learning/artificial neural networks. Our techniques apply to VGG/Residual architectures, and are compatible with all forms of training methodologies. Our analysis reveals that the proposed solutions yield near state-of-the-art accuracy with significant energy-efficiency and reduced parameter overhead translating to hardware improvements on complex visual recognition tasks, such as, CIFAR10, Imagenet datatsets.



### Motion-Nets: 6D Tracking of Unknown Objects in Unseen Environments using RGB
- **Arxiv ID**: http://arxiv.org/abs/1910.13942v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.13942v1)
- **Published**: 2019-10-30 15:51:23+00:00
- **Updated**: 2019-10-30 15:51:23+00:00
- **Authors**: Felix Leeb, Arunkumar Byravan, Dieter Fox
- **Comment**: Accepted to IROS 2019 workshop on The Importance of Uncertainty in
  Deep Learning for Robotics
- **Journal**: None
- **Summary**: In this work, we bridge the gap between recent pose estimation and tracking work to develop a powerful method for robots to track objects in their surroundings. Motion-Nets use a segmentation model to segment the scene, and separate translation and rotation models to identify the relative 6D motion of an object between two consecutive frames. We train our method with generated data of floating objects, and then test on several prediction tasks, including one with a real PR2 robot, and a toy control task with a simulated PR2 robot never seen during training. Motion-Nets are able to track the pose of objects with some quantitative accuracy for about 30-60 frames including occlusions and distractors. Additionally, the single step prediction errors remain low even after 100 frames. We also investigate an iterative correction procedure to improve performance for control tasks.



### LDLS: 3-D Object Segmentation Through Label Diffusion From 2-D Images
- **Arxiv ID**: http://arxiv.org/abs/1910.13955v1
- **DOI**: 10.1109/LRA.2019.2922582
- **Categories**: **eess.IV**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.13955v1)
- **Published**: 2019-10-30 16:11:23+00:00
- **Updated**: 2019-10-30 16:11:23+00:00
- **Authors**: Brian H. Wang, Wei-Lun Chao, Yan Wang, Bharath Hariharan, Kilian Q. Weinberger, Mark Campbell
- **Comment**: Accepted for publication in IEEE Robotics and Automation Letters with
  presentation at IROS 2019
- **Journal**: None
- **Summary**: Object segmentation in three-dimensional (3-D) point clouds is a critical task for robots capable of 3-D perception. Despite the impressive performance of deep learning-based approaches on object segmentation in 2-D images, deep learning has not been applied nearly as successfully for 3-D point cloud segmentation. Deep networks generally require large amounts of labeled training data, which are readily available for 2-D images but are difficult to produce for 3-D point clouds. In this letter, we present Label Diffusion Lidar Segmentation (LDLS), a novel approach for 3-D point cloud segmentation, which leverages 2-D segmentation of an RGB image from an aligned camera to avoid the need for training on annotated 3-D data. We obtain 2-D segmentation predictions by applying Mask-RCNN to the RGB image, and then link this image to a 3-D lidar point cloud by building a graph of connections among 3-D points and 2-D pixels. This graph then directs a semi-supervised label diffusion process, where the 2-D pixels act as source nodes that diffuse object label information through the 3-D point cloud, resulting in a complete 3-D point cloud segmentation. We conduct empirical studies on the KITTI benchmark dataset and on a mobile robot, demonstrating wide applicability and superior performance of LDLS compared with the previous state of the art in 3-D point cloud segmentation, without any need for either 3-D training data or fine tuning of the 2-D image segmentation model.



### Auto-Annotation Quality Prediction for Semi-Supervised Learning with Ensembles
- **Arxiv ID**: http://arxiv.org/abs/1910.13988v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1910.13988v1)
- **Published**: 2019-10-30 17:10:21+00:00
- **Updated**: 2019-10-30 17:10:21+00:00
- **Authors**: Dror Simon, Miriam Farber, Roman Goldenberg
- **Comment**: 10 pages, 1 figure, 5 tables
- **Journal**: None
- **Summary**: Auto-annotation by ensemble of models is an efficient method of learning on unlabeled data. Wrong or inaccurate annotations generated by the ensemble may lead to performance degradation of the trained model. To deal with this problem we propose filtering the auto-labeled data using a trained model that predicts the quality of the annotation from the degree of consensus between ensemble models. Using semantic segmentation as an example, we show the advantage of the proposed auto-annotation filtering over training on data contaminated with inaccurate labels.   Moreover, our experimental results show that in the case of semantic segmentation, the performance of a state-of-the-art model can be achieved by training it with only a fraction (30$\%$) of the original manually labeled data set, and replacing the rest with the auto-annotated, quality filtered labels.



### Is Supervised Learning With Adversarial Features Provably Better Than Sole Supervision?
- **Arxiv ID**: http://arxiv.org/abs/1910.13993v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13993v2)
- **Published**: 2019-10-30 17:20:45+00:00
- **Updated**: 2020-04-20 14:42:18+00:00
- **Authors**: Litu Rout
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GAN) have shown promising results on a wide variety of complex tasks. Recent experiments show adversarial training provides useful gradients to the generator that helps attain better performance. In this paper, we intend to theoretically analyze whether supervised learning with adversarial features can outperform sole supervision, or not. First, we show that supervised learning without adversarial features suffer from vanishing gradient issue in near optimal region. Second, we analyze how adversarial learning augmented with supervised signal mitigates this vanishing gradient issue. Finally, we prove our main result that shows supervised learning with adversarial features can be better than sole supervision (under some mild assumptions). We support our main result on two fronts (i) expected empirical risk and (ii) rate of convergence.



### Are Out-of-Distribution Detection Methods Effective on Large-Scale Datasets?
- **Arxiv ID**: http://arxiv.org/abs/1910.14034v1
- **DOI**: 10.1371/journal.pone.0238302
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14034v1)
- **Published**: 2019-10-30 17:53:13+00:00
- **Updated**: 2019-10-30 17:53:13+00:00
- **Authors**: Ryne Roady, Tyler L. Hayes, Ronald Kemker, Ayesha Gonzales, Christopher Kanan
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised classification methods often assume the train and test data distributions are the same and that all classes in the test set are present in the training set. However, deployed classifiers often require the ability to recognize inputs from outside the training set as unknowns. This problem has been studied under multiple paradigms including out-of-distribution detection and open set recognition. For convolutional neural networks, there have been two major approaches: 1) inference methods to separate knowns from unknowns and 2) feature space regularization strategies to improve model robustness to outlier inputs. There has been little effort to explore the relationship between the two approaches and directly compare performance on anything other than small-scale datasets that have at most 100 categories. Using ImageNet-1K and Places-434, we identify novel combinations of regularization and specialized inference methods that perform best across multiple outlier detection problems of increasing difficulty level. We found that input perturbation and temperature scaling yield the best performance on large scale datasets regardless of the feature space regularization strategy. Improving the feature space by regularizing against a background class can be helpful if an appropriate background class can be found, but this is impractical for large scale image classification datasets.



### LaplacianNet: Learning on 3D Meshes with Laplacian Encoding and Pooling
- **Arxiv ID**: http://arxiv.org/abs/1910.14063v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.14063v1)
- **Published**: 2019-10-30 18:02:23+00:00
- **Updated**: 2019-10-30 18:02:23+00:00
- **Authors**: Yi-Ling Qiao, Lin Gao, Jie Yang, Paul L. Rosin, Yu-Kun Lai, Xilin Chen
- **Comment**: None
- **Journal**: None
- **Summary**: 3D models are commonly used in computer vision and graphics. With the wider availability of mesh data, an efficient and intrinsic deep learning approach to processing 3D meshes is in great need. Unlike images, 3D meshes have irregular connectivity, requiring careful design to capture relations in the data. To utilize the topology information while staying robust under different triangulation, we propose to encode mesh connectivity using Laplacian spectral analysis, along with Mesh Pooling Blocks (MPBs) that can split the surface domain into local pooling patches and aggregate global information among them. We build a mesh hierarchy from fine to coarse using Laplacian spectral clustering, which is flexible under isometric transformation. Inside the MPBs there are pooling layers to collect local information and multi-layer perceptrons to compute vertex features with increasing complexity. To obtain the relationships among different clusters, we introduce a Correlation Net to compute a correlation matrix, which can aggregate the features globally by matrix multiplication with cluster features. Our network architecture is flexible enough to be used on meshes with different numbers of vertices. We conduct several experiments including shape segmentation and classification, and our LaplacianNet outperforms state-of-the-art algorithms for these tasks on ShapeNet and COSEG datasets.



### FutureMapping 2: Gaussian Belief Propagation for Spatial AI
- **Arxiv ID**: http://arxiv.org/abs/1910.14139v2
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.DC, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.14139v2)
- **Published**: 2019-10-30 21:12:14+00:00
- **Updated**: 2022-11-07 16:18:13+00:00
- **Authors**: Andrew J. Davison, Joseph Ortiz
- **Comment**: None
- **Journal**: None
- **Summary**: We argue the case for Gaussian Belief Propagation (GBP) as a strong algorithmic framework for the distributed, generic and incremental probabilistic estimation we need in Spatial AI as we aim at high performance smart robots and devices which operate within the constraints of real products. Processor hardware is changing rapidly, and GBP has the right character to take advantage of highly distributed processing and storage while estimating global quantities, as well as great flexibility. We present a detailed tutorial on GBP, relating to the standard factor graph formulation used in robotics and computer vision, and give several simulation examples with code which demonstrate its properties.



### Beyond Universal Person Re-ID Attack
- **Arxiv ID**: http://arxiv.org/abs/1910.14184v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.14184v3)
- **Published**: 2019-10-30 23:43:51+00:00
- **Updated**: 2020-12-13 15:12:48+00:00
- **Authors**: Wenjie Ding, Xing Wei, Rongrong Ji, Xiaopeng Hong, Qi Tian, Yihong Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Deep learning-based person re-identification (Re-ID) has made great progress and achieved high performance recently. In this paper, we make the first attempt to examine the vulnerability of current person Re-ID models against a dangerous attack method, \ie, the universal adversarial perturbation (UAP) attack, which has been shown to fool classification models with a little overhead. We propose a \emph{more universal} adversarial perturbation (MUAP) method for both image-agnostic and model-insensitive person Re-ID attack. Firstly, we adopt a list-wise attack objective function to disrupt the similarity ranking list directly. Secondly, we propose a model-insensitive mechanism for cross-model attack. Extensive experiments show that the proposed attack approach achieves high attack performance and outperforms other state of the arts by large margin in cross-model scenario. The results also demonstrate the vulnerability of current Re-ID models to MUAP and further suggest the need of designing more robust Re-ID models.



