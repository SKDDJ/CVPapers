# Arxiv Papers in cs.CV on 2019-10-15
### State of Compact Architecture Search For Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06466v1
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06466v1)
- **Published**: 2019-10-15 00:16:52+00:00
- **Updated**: 2019-10-15 00:16:52+00:00
- **Authors**: Mohammad Javad Shafiee, Andrew Hryniowski, Francis Li, Zhong Qiu Lin, Alexander Wong
- **Comment**: 6 pages
- **Journal**: None
- **Summary**: The design of compact deep neural networks is a crucial task to enable widespread adoption of deep neural networks in the real-world, particularly for edge and mobile scenarios. Due to the time-consuming and challenging nature of manually designing compact deep neural networks, there has been significant recent research interest into algorithms that automatically search for compact network architectures. A particularly interesting class of compact architecture search algorithms are those that are guided by baseline network architectures. Such algorithms have been shown to be significantly more computationally efficient than unguided methods. In this study, we explore the current state of compact architecture search for deep neural networks through both theoretical and empirical analysis of four different state-of-the-art compact architecture search algorithms: i) group lasso regularization, ii) variational dropout, iii) MorphNet, and iv) Generative Synthesis. We examine these methods in detail based on a number of different factors such as efficiency, effectiveness, and scalability. Furthermore, empirical evaluations are conducted to compare the efficacy of these compact architecture search algorithms across three well-known benchmark datasets. While by no means an exhaustive exploration, we hope that this study helps provide insights into the interesting state of this relatively new area of research in terms of diversity and real, tangible gains already achieved in architecture design improvements. Furthermore, the hope is that this study would help in pushing the conversation forward towards a deeper theoretical and empirical understanding where the research community currently stands in the landscape of compact architecture search for deep neural networks, and the practical challenges and considerations in leveraging such approaches for operational usage.



### Visual Hide and Seek
- **Arxiv ID**: http://arxiv.org/abs/1910.07882v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV, cs.LG, cs.MA, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.07882v1)
- **Published**: 2019-10-15 01:27:09+00:00
- **Updated**: 2019-10-15 01:27:09+00:00
- **Authors**: Boyuan Chen, Shuran Song, Hod Lipson, Carl Vondrick
- **Comment**: 14 pages, 8 figures
- **Journal**: None
- **Summary**: We train embodied agents to play Visual Hide and Seek where a prey must navigate in a simulated environment in order to avoid capture from a predator. We place a variety of obstacles in the environment for the prey to hide behind, and we only give the agents partial observations of their environment using an egocentric perspective. Although we train the model to play this game from scratch, experiments and visualizations suggest that the agent learns to predict its own visibility in the environment. Furthermore, we quantitatively analyze how agent weaknesses, such as slower speed, effect the learned policy. Our results suggest that, although agent weaknesses make the learning problem more challenging, they also cause more useful features to be learned. Our project website is available at: http://www.cs.columbia.edu/ ~bchen/visualhideseek/.



### End-to-End Adversarial Shape Learning for Abdomen Organ Deep Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1910.06474v1
- **DOI**: 10.1007/978-3-030-32692-0_15
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06474v1)
- **Published**: 2019-10-15 01:48:29+00:00
- **Updated**: 2019-10-15 01:48:29+00:00
- **Authors**: Jinzheng Cai, Yingda Xia, Dong Yang, Daguang Xu, Lin Yang, Holger Roth
- **Comment**: Accepted to International Workshop on Machine Learning in Medical
  Imaging (MLMI2019)
- **Journal**: None
- **Summary**: Automatic segmentation of abdomen organs using medical imaging has many potential applications in clinical workflows. Recently, the state-of-the-art performance for organ segmentation has been achieved by deep learning models, i.e., convolutional neural network (CNN). However, it is challenging to train the conventional CNN-based segmentation models that aware of the shape and topology of organs. In this work, we tackle this problem by introducing a novel end-to-end shape learning architecture -- organ point-network. It takes deep learning features as inputs and generates organ shape representations as points that located on organ surface. We later present a novel adversarial shape learning objective function to optimize the point-network to capture shape information better. We train the point-network together with a CNN-based segmentation model in a multi-task fashion so that the shared network parameters can benefit from both shape learning and segmentation tasks. We demonstrate our method with three challenging abdomen organs including liver, spleen, and pancreas. The point-network generates surface points with fine-grained details and it is found critical for improving organ segmentation. Consequently, the deep segmentation model is improved by the introduced shape learning as significantly better Dice scores are observed for spleen and pancreas segmentation.



### Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style
- **Arxiv ID**: http://arxiv.org/abs/1910.06475v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06475v1)
- **Published**: 2019-10-15 01:49:19+00:00
- **Updated**: 2019-10-15 01:49:19+00:00
- **Authors**: Hongwei Ge, Zehang Yan, Kai Zhang, Mingde Zhao, Liang Sun
- **Comment**: ICCV 2019
- **Journal**: None
- **Summary**: Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results.



### Supervised Encoding for Discrete Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.11067v1
- **DOI**: 10.1109/ICASSP40776.2020.9054118
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11067v1)
- **Published**: 2019-10-15 02:42:10+00:00
- **Updated**: 2019-10-15 02:42:10+00:00
- **Authors**: Cat P. Le, Yi Zhou, Jie Ding, Vahid Tarokh
- **Comment**: None
- **Journal**: None
- **Summary**: Classical supervised classification tasks search for a nonlinear mapping that maps each encoded feature directly to a probability mass over the labels. Such a learning framework typically lacks the intuition that encoded features from the same class tend to be similar and thus has little interpretability for the learned features. In this paper, we propose a novel supervised learning model named Supervised-Encoding Quantizer (SEQ). The SEQ applies a quantizer to cluster and classify the encoded features. We found that the quantizer provides an interpretable graph where each cluster in the graph represents a class of data samples that have a particular style. We also trained a decoder that can decode convex combinations of the encoded features from similar and different clusters and provide guidance on style transfer between sub-classes.



### Target-Oriented Deformation of Visual-Semantic Embedding Space
- **Arxiv ID**: http://arxiv.org/abs/1910.06514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.06514v1)
- **Published**: 2019-10-15 03:54:27+00:00
- **Updated**: 2019-10-15 03:54:27+00:00
- **Authors**: Takashi Matsubara
- **Comment**: 8 pages
- **Journal**: None
- **Summary**: Multimodal embedding is a crucial research topic for cross-modal understanding, data mining, and translation. Many studies have attempted to extract representations from given entities and align them in a shared embedding space. However, because entities in different modalities exhibit different abstraction levels and modality-specific information, it is insufficient to embed related entities close to each other. In this study, we propose the Target-Oriented Deformation Network (TOD-Net), a novel module that continuously deforms the embedding space into a new space under a given condition, thereby adjusting similarities between entities. Unlike methods based on cross-modal attention, TOD-Net is a post-process applied to the embedding space learned by existing embedding systems and improves their performances of retrieval. In particular, when combined with cutting-edge models, TOD-Net gains the state-of-the-art cross-modal retrieval model associated with the MSCOCO dataset. Qualitative analysis reveals that TOD-Net successfully emphasizes entity-specific concepts and retrieves diverse targets via handling higher levels of diversity than existing models.



### End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/1910.06528v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06528v2)
- **Published**: 2019-10-15 05:13:13+00:00
- **Updated**: 2019-10-23 21:39:25+00:00
- **Authors**: Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, Vijay Vasudevan
- **Comment**: CoRL2019
- **Journal**: None
- **Summary**: Recent work on 3D object detection advocates point cloud voxelization in birds-eye view, where objects preserve their physical dimensions and are naturally separable. When represented in this view, however, point clouds are sparse and have highly variable point density, which may cause detectors difficulties in detecting distant or small objects (pedestrians, traffic signs, etc.). On the other hand, perspective view provides dense observations, which could allow more favorable feature encoding for such cases. In this paper, we aim to synergize the birds-eye view and the perspective view and propose a novel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn to utilize the complementary information from both. Specifically, we introduce dynamic voxelization, which has four merits compared to existing voxelization methods, i) removing the need of pre-allocating a tensor with fixed size; ii) overcoming the information loss due to stochastic point/voxel dropout; iii) yielding deterministic voxel embeddings and more stable detection outcomes; iv) establishing the bi-directional relationship between points and voxels, which potentially lays a natural foundation for cross-view feature fusion. By employing dynamic voxelization, the proposed feature fusion architecture enables each point to learn to fuse context information from different views. MVF operates on points and can be naturally extended to other approaches using LiDAR point clouds. We evaluate our MVF model extensively on the newly released Waymo Open Dataset and on the KITTI dataset and demonstrate that it significantly improves detection accuracy over the comparable single-view PointPillars baseline.



### Real-time monitoring of driver drowsiness on mobile platforms using 3D neural networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06540v1
- **DOI**: 10.1007/s00521-019-04506-0
- **Categories**: **cs.CV**, stat.ML, 68T45 (Primary) 68U10 (Secondary), I.4.9; I.4.8; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/1910.06540v1)
- **Published**: 2019-10-15 05:44:28+00:00
- **Updated**: 2019-10-15 05:44:28+00:00
- **Authors**: Jasper S. Wijnands, Jason Thompson, Kerry A. Nice, Gideon D. P. A. Aschwanden, Mark Stevenson
- **Comment**: 13 pages, 2 figures, 'Online First' version. For associated mp4
  files, see journal website
- **Journal**: Neural Computing and Applications (2019)
- **Summary**: Driver drowsiness increases crash risk, leading to substantial road trauma each year. Drowsiness detection methods have received considerable attention, but few studies have investigated the implementation of a detection approach on a mobile phone. Phone applications reduce the need for specialised hardware and hence, enable a cost-effective roll-out of the technology across the driving population. While it has been shown that three-dimensional (3D) operations are more suitable for spatiotemporal feature learning, current methods for drowsiness detection commonly use frame-based, multi-step approaches. However, computationally expensive techniques that achieve superior results on action recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create bottlenecks for real-time, safety-critical applications on mobile devices. Here, we show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. In particular, increased accuracy is achieved when assessment requires motion information, for example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based smartphone application shows the true impact of various approaches on inference times and demonstrates the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. Our model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. Overall, our research is a step towards practical deep learning applications, potentially preventing micro-sleeps and reducing road trauma.



### Training CNNs faster with Dynamic Input and Kernel Downsampling
- **Arxiv ID**: http://arxiv.org/abs/1910.06548v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06548v1)
- **Published**: 2019-10-15 06:18:29+00:00
- **Updated**: 2019-10-15 06:18:29+00:00
- **Authors**: Zissis Poulos, Ali Nouri, Andreas Moshovos
- **Comment**: 12 pages, 4 figures
- **Journal**: None
- **Summary**: We reduce training time in convolutional networks (CNNs) with a method that, for some of the mini-batches: a) scales down the resolution of input images via downsampling, and b) reduces the forward pass operations via pooling on the convolution filters. Training is performed in an interleaved fashion; some batches undergo the regular forward and backpropagation passes with original network parameters, whereas others undergo a forward pass with pooled filters and downsampled inputs. Since pooling is differentiable, the gradients of the pooled filters propagate to the original network parameters for a standard parameter update. The latter phase requires fewer floating point operations and less storage due to the reduced spatial dimensions in feature maps and filters. The key idea is that this phase leads to smaller and approximate updates and thus slower learning, but at significantly reduced cost, followed by passes that use the original network parameters as a refinement stage. Deciding how often and for which batches the downsmapling occurs can be done either stochastically or deterministically, and can be defined as a training hyperparameter itself. Experiments on residual architectures show that we can achieve up to 23% reduction in training time with minimal loss in validation accuracy.



### IMMVP: An Efficient Daytime and Nighttime On-Road Object Detector
- **Arxiv ID**: http://arxiv.org/abs/1910.06573v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.06573v3)
- **Published**: 2019-10-15 07:46:03+00:00
- **Updated**: 2019-10-28 05:00:49+00:00
- **Authors**: Cheng-En Wu, Yi-Ming Chan, Chien-Hung Chen, Wen-Cheng Chen, Chu-Song Chen
- **Comment**: Accepted at IEEE 21st International Workshop on Multimedia Signal
  Processing (MMSP 2019)
- **Journal**: None
- **Summary**: It is hard to detect on-road objects under various lighting conditions. To improve the quality of the classifier, three techniques are used. We define subclasses to separate daytime and nighttime samples. Then we skip similar samples in the training set to prevent overfitting. With the help of the outside training samples, the detection accuracy is also improved. To detect objects in an edge device, Nvidia Jetson TX2 platform, we exert the lightweight model ResNet-18 FPN as the backbone feature extractor. The FPN (Feature Pyramid Network) generates good features for detecting objects over various scales. With Cascade R-CNN technique, the bounding boxes are iteratively refined for better results.



### TrajectoryNet: a new spatio-temporal feature learning network for human motion prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.06583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06583v2)
- **Published**: 2019-10-15 08:14:28+00:00
- **Updated**: 2020-03-20 12:39:21+00:00
- **Authors**: Xiaoli Liu, Jianqin Yin, Jin Liu, Pengxiang Ding, Jun Liu, Huaping Liu
- **Comment**: None
- **Journal**: None
- **Summary**: Human motion prediction is an increasingly interesting topic in computer vision and robotics. In this paper, we propose a new 2D CNN based network, TrajectoryNet, to predict future poses in the trajectory space. Compared with most existing methods, our model focuses on modeling the motion dynamics with coupled spatio-temporal features, local-global spatial features and global temporal co-occurrence features of the previous pose sequence. Specifically, the coupled spatio-temporal features describe the spatial and temporal structure information hidden in the natural human motion sequence, which can be mined by covering the space and time dimensions of the input pose sequence with the convolutional filters. The local-global spatial features that encode different correlations of different joints of the human body (e.g. strong correlations between joints of one limb, weak correlations between joints of different limbs) are captured hierarchically by enlarging the receptive field layer by layer and residual connections from the lower layers to the deeper layers in our proposed convolutional network. And the global temporal co-occurrence features represent the co-occurrence relationship that different subsequences in a complex motion sequence are appeared simultaneously, which can be obtained automatically with our proposed TrajectoryNet by reorganizing the temporal information as the depth dimension of the input tensor. Finally, future poses are approximated based on the captured motion dynamics features. Extensive experiments show that our method achieves state-of-the-art performance on three challenging benchmarks (e.g. Human3.6M, G3D, and FNTU), which demonstrates the effectiveness of our proposed method. The code will be available if the paper is accepted.



### Stereo-based Multi-motion Visual Odometry for Mobile Robots
- **Arxiv ID**: http://arxiv.org/abs/1910.06607v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.06607v1)
- **Published**: 2019-10-15 09:10:56+00:00
- **Updated**: 2019-10-15 09:10:56+00:00
- **Authors**: Qing Zhao, Bin Luo, Yun Zhang
- **Comment**: 5 pages, 5 figures
- **Journal**: None
- **Summary**: With the development of computer vision, visual odometry is adopted by more and more mobile robots. However, we found that not only its own pose, but the poses of other moving objects are also crucial for the decision of the robot. In addition, the visual odometry will be greatly disturbed when a significant moving object appears. In this letter, a stereo-based multi-motion visual odometry method is proposed to acquire the poses of the robot and other moving objects. In order to obtain the poses simultaneously, a continuous motion segmentation module and a coordinate conversion module are applied to the traditional visual odometry pipeline. As a result, poses of all moving objects can be acquired and transformed into the ground coordinate system. The experimental results show that the proposed multi-motion visual odometry can effectively eliminate the influence of moving objects on the visual odometry, as well as achieve 10 cm in position and 3{\deg} in orientation RMSE (Root Mean Square Error) of each moving object.



### Background Segmentation for Vehicle Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.06613v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06613v1)
- **Published**: 2019-10-15 09:25:31+00:00
- **Updated**: 2019-10-15 09:25:31+00:00
- **Authors**: Mingjie Wu, Yongfei Zhang, Tianyu Zhang, Wenqi Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Vehicle re-identification (Re-ID) is very important in intelligent transportation and video surveillance.Prior works focus on extracting discriminative features from visual appearance of vehicles or using visual-spatio-temporal information.However, background interference in vehicle re-identification have not been explored.In the actual large-scale spatio-temporal scenes, the same vehicle usually appears in different backgrounds while different vehicles might appear in the same background, which will seriously affect the re-identification performance. To the best of our knowledge, this paper is the first to consider the background interference problem in vehicle re-identification. We construct a vehicle segmentation dataset and develop a vehicle Re-ID framework with a background interference removal (BIR) mechanism to improve the vehicle Re-ID performance as well as robustness against complex background in large-scale spatio-temporal scenes. Extensive experiments demonstrate the effectiveness of our proposed framework, with an average 9% gain on mAP over state-of-the-art vehicle Re-ID algorithms.



### Understanding Misclassifications by Attributes
- **Arxiv ID**: http://arxiv.org/abs/1910.07416v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.07416v1)
- **Published**: 2019-10-15 09:36:23+00:00
- **Updated**: 2019-10-15 09:36:23+00:00
- **Authors**: Sadaf Gulshad, Zeynep Akata, Jan Hendrik Metzen, Arnold Smeulders
- **Comment**: arXiv admin note: substantial text overlap with arXiv:1904.08279
- **Journal**: None
- **Summary**: In this paper, we aim to understand and explain the decisions of deep neural networks by studying the behavior of predicted attributes when adversarial examples are introduced. We study the changes in attributes for clean as well as adversarial images in both standard and adversarially robust networks. We propose a metric to quantify the robustness of an adversarially robust network against adversarial attacks. In a standard network, attributes predicted for adversarial images are consistent with the wrong class, while attributes predicted for the clean images are consistent with the true class. In an adversarially robust network, the attributes predicted for adversarial images classified correctly are consistent with the true class. Finally, we show that the ability to robustify a network varies for different datasets. For the fine grained dataset, it is higher as compared to the coarse-grained dataset. Additionally, the ability to robustify a network increases with the increase in adversarial noise.



### A Method to Generate Synthetically Warped Document Image
- **Arxiv ID**: http://arxiv.org/abs/1910.06621v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06621v1)
- **Published**: 2019-10-15 09:54:48+00:00
- **Updated**: 2019-10-15 09:54:48+00:00
- **Authors**: Arpan Garai, Samit Biswas, Sekhar Mandal, Bidyut. B. Chaudhuri
- **Comment**: None
- **Journal**: None
- **Summary**: The digital camera captured document images may often be warped and distorted due to different camera angles or document surfaces. A robust technique is needed to solve this kind of distortion. The research on dewarping of the document suffers due to the limited availability of benchmark public dataset. In recent times, deep learning based approaches are used to solve the problems accurately. To train most of the deep neural networks a large number of document images is required and generating such a large volume of document images manually is difficult. In this paper, we propose a technique to generate a synthetic warped image from a flat-bedded scanned document image. It is done by calculating warping factors for each pixel position using two warping position parameters (WPP) and eight warping control parameters (WCP). These parameters can be specified as needed depending upon the desired warping. The results are compared with similar real captured images both qualitative and quantitative way.



### Multi-Frame GAN: Image Enhancement for Stereo Visual Odometry in Low Light
- **Arxiv ID**: http://arxiv.org/abs/1910.06632v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06632v1)
- **Published**: 2019-10-15 10:11:10+00:00
- **Updated**: 2019-10-15 10:11:10+00:00
- **Authors**: Eunah Jung, Nan Yang, Daniel Cremers
- **Comment**: Accepted by the 3rd Conference on Robot Learning, Osaka, Japan (CoRL
  2019). The first two authors contributed equally to this paper
- **Journal**: None
- **Summary**: We propose the concept of a multi-frame GAN (MFGAN) and demonstrate its potential as an image sequence enhancement for stereo visual odometry in low light conditions. We base our method on an invertible adversarial network to transfer the beneficial features of brightly illuminated scenes to the sequence in poor illumination without costly paired datasets. In order to preserve the coherent geometric cues for the translated sequence, we present a novel network architecture as well as a novel loss term combining temporal and stereo consistencies based on optical flow estimation. We demonstrate that the enhanced sequences improve the performance of state-of-the-art feature-based and direct stereo visual odometry methods on both synthetic and real datasets in challenging illumination. We also show that MFGAN outperforms other state-of-the-art image enhancement and style transfer methods by a large margin in terms of visual odometry.



### Liver segmentation and metastases detection in MR images using convolutional neural networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06635v1
- **DOI**: 10.1117/1.JMI.6.4.044003
- **Categories**: **eess.IV**, cs.CV, q-bio.QM
- **Links**: [PDF](http://arxiv.org/pdf/1910.06635v1)
- **Published**: 2019-10-15 10:17:59+00:00
- **Updated**: 2019-10-15 10:17:59+00:00
- **Authors**: Mariëlle J. A. Jansen, Hugo J. Kuijf, Maarten Niekel, Wouter B. Veldhuis, Frank J. Wessels, Max A. Viergever, Josien P. W. Pluim
- **Comment**: None
- **Journal**: J. Med. Imag. 6(4), 044003 (2019)
- **Summary**: Primary tumors have a high likelihood of developing metastases in the liver and early detection of these metastases is crucial for patient outcome. We propose a method based on convolutional neural networks (CNN) to detect liver metastases. First, the liver was automatically segmented using the six phases of abdominal dynamic contrast enhanced (DCE) MR images. Next, DCE-MR and diffusion weighted (DW) MR images are used for metastases detection within the liver mask. The liver segmentations have a median Dice similarity coefficient of 0.95 compared with manual annotations. The metastases detection method has a sensitivity of 99.8% with a median of 2 false positives per image. The combination of the two MR sequences in a dual pathway network is proven valuable for the detection of liver metastases. In conclusion, a high quality liver segmentation can be obtained in which we can successfully detect liver metastases.



### Topological Navigation Graph Framework
- **Arxiv ID**: http://arxiv.org/abs/1910.06658v2
- **DOI**: 10.1007/s10514-021-09980-x
- **Categories**: **cs.RO**, cs.AI, cs.CV, I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/1910.06658v2)
- **Published**: 2019-10-15 11:19:00+00:00
- **Updated**: 2021-05-13 18:02:46+00:00
- **Authors**: Povilas Daniusis, Shubham Juneja, Lukas Valatka, Linas Petkevicius
- **Comment**: None
- **Journal**: None
- **Summary**: We focus on the utilisation of reactive trajectory imitation controllers for goal-directed mobile robot navigation. We propose a topological navigation graph (TNG) - an imitation-learning-based framework for navigating through environments with intersecting trajectories. The TNG framework represents the environment as a directed graph composed of deep neural networks. Each vertex of the graph corresponds to a trajectory and is represented by a trajectory identification classifier and a trajectory imitation controller. For trajectory following, we propose the novel use of neural object detection architectures. The edges of TNG correspond to intersections between trajectories and are all represented by a classifier. We provide empirical evaluation of the proposed navigation framework and its components in simulated and real-world environments, demonstrating that TNG allows us to utilise non-goal-directed, imitation-learning methods for goal-directed autonomous navigation.



### SafeCritic: Collision-Aware Trajectory Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.06673v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06673v1)
- **Published**: 2019-10-15 12:15:19+00:00
- **Updated**: 2019-10-15 12:15:19+00:00
- **Authors**: Tessa van der Heiden, Naveen Shankar Nagaraja, Christian Weiss, Efstratios Gavves
- **Comment**: To Appear as workshop paper for the British Machine Vision Conference
  (BMVC) 2019
- **Journal**: None
- **Summary**: Navigating complex urban environments safely is a key to realize fully autonomous systems. Predicting future locations of vulnerable road users, such as pedestrians and cyclists, thus, has received a lot of attention in the recent years. While previous works have addressed modeling interactions with the static (obstacles) and dynamic (humans) environment agents, we address an important gap in trajectory prediction. We propose SafeCritic, a model that synergizes generative adversarial networks for generating multiple "real" trajectories with reinforcement learning to generate "safe" trajectories. The Discriminator evaluates the generated candidates on whether they are consistent with the observed inputs. The Critic network is environmentally aware to prune trajectories that are in collision or are in violation with the environment. The auto-encoding loss stabilizes training and prevents mode-collapse. We demonstrate results on two large scale data sets with a considerable improvement over state-of-the-art. We also show that the Critic is able to classify the safety of trajectories.



### Being the center of attention: A Person-Context CNN framework for Personality Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.06690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06690v1)
- **Published**: 2019-10-15 12:47:11+00:00
- **Updated**: 2019-10-15 12:47:11+00:00
- **Authors**: Dario Dotti, Mirela Popa, Stylianos Asteriadis
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a novel study on personality recognition using video data from different scenarios. Our goal is to jointly model nonverbal behavioral cues with contextual information for a robust, multi-scenario, personality recognition system. Therefore, we build a novel multi-stream Convolutional Neural Network framework (CNN), which considers multiple sources of information. From a given scenario, we extract spatio-temporal motion descriptors from every individual in the scene, spatio-temporal motion descriptors encoding social group dynamics, and proxemics descriptors to encode the interaction with the surrounding context. All the proposed descriptors are mapped to the same feature space facilitating the overall learning effort. Experiments on two public datasets demonstrate the effectiveness of jointly modeling the mutual Person-Context information, outperforming the state-of-the art-results for personality recognition in two different scenarios. Lastly, we present CNN class activation maps for each personality trait, shedding light on behavioral patterns linked with personality attributes.



### Seeing and Hearing Egocentric Actions: How Much Can We Learn?
- **Arxiv ID**: http://arxiv.org/abs/1910.06693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.AS
- **Links**: [PDF](http://arxiv.org/pdf/1910.06693v1)
- **Published**: 2019-10-15 12:55:49+00:00
- **Updated**: 2019-10-15 12:55:49+00:00
- **Authors**: Alejandro Cartas, Jordi Luque, Petia Radeva, Carlos Segura, Mariella Dimiccoli
- **Comment**: Accepted for the Fifth International Workshop on Egocentric
  Perception, Interaction and Computing (EPIC) at the International Conference
  on Computer Vision (ICCV) 2019
- **Journal**: None
- **Summary**: Our interaction with the world is an inherently multimodal experience. However, the understanding of human-to-object interactions has historically been addressed focusing on a single modality. In particular, a limited number of works have considered to integrate the visual and audio modalities for this purpose. In this work, we propose a multimodal approach for egocentric action recognition in a kitchen environment that relies on audio and visual information. Our model combines a sparse temporal sampling strategy with a late fusion of audio, spatial, and temporal streams. Experimental results on the EPIC-Kitchens dataset show that multimodal integration leads to better performance than unimodal approaches. In particular, we achieved a 5.18% improvement over the state of the art on verb classification.



### Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling
- **Arxiv ID**: http://arxiv.org/abs/1910.06705v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06705v1)
- **Published**: 2019-10-15 13:11:24+00:00
- **Updated**: 2019-10-15 13:11:24+00:00
- **Authors**: YoungJoon Yoo, Sanghyuk Chun, Sangdoo Yun, Jung-Woo Ha, Jaejun Yoo
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a generic confidence-based approximation that can be plugged in and simplify the auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.



### Depth Completion from Sparse LiDAR Data with Depth-Normal Constraints
- **Arxiv ID**: http://arxiv.org/abs/1910.06727v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06727v1)
- **Published**: 2019-10-15 13:34:18+00:00
- **Updated**: 2019-10-15 13:34:18+00:00
- **Authors**: Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, Hongsheng Li
- **Comment**: Accepted to ICCV 2019
- **Journal**: None
- **Summary**: Depth completion aims to recover dense depth maps from sparse depth measurements. It is of increasing importance for autonomous driving and draws increasing attention from the vision community. Most of existing methods directly train a network to learn a mapping from sparse depth inputs to dense depth maps, which has difficulties in utilizing the 3D geometric constraints and handling the practical sensor noises. In this paper, to regularize the depth completion and improve the robustness against noise, we propose a unified CNN framework that 1) models the geometric constraints between depth and surface normal in a diffusion module and 2) predicts the confidence of sparse LiDAR measurements to mitigate the impact of noise. Specifically, our encoder-decoder backbone predicts surface normals, coarse depth and confidence of LiDAR inputs simultaneously, which are subsequently inputted into our diffusion refinement module to obtain the final completion results. Extensive experiments on KITTI depth completion dataset and NYU-Depth-V2 dataset demonstrate that our method achieves state-of-the-art performance. Further ablation study and analysis give more insights into the proposed method and demonstrate the generalization capability and stability of our model.



### Integrating Temporal and Spatial Attentions for VATEX Video Captioning Challenge 2019
- **Arxiv ID**: http://arxiv.org/abs/1910.06737v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06737v1)
- **Published**: 2019-10-15 13:45:30+00:00
- **Updated**: 2019-10-15 13:45:30+00:00
- **Authors**: Shizhe Chen, Yida Zhao, Yuqing Song, Qin Jin, Qi Wu
- **Comment**: ICCV 2019 VATEX challenge
- **Journal**: None
- **Summary**: This notebook paper presents our model in the VATEX video captioning challenge. In order to capture multi-level aspects in the video, we propose to integrate both temporal and spatial attentions for video captioning. The temporal attentive module focuses on global action movements while spatial attentive module enables to describe more fine-grained objects. Considering these two types of attentive modules are complementary, we thus fuse them via a late fusion strategy. The proposed model significantly outperforms baselines and achieves 73.4 CIDEr score on the testing set which ranks the second place at the VATEX video captioning challenge leaderboard 2019.



### Cortical-inspired Wilson-Cowan-type equations for orientation-dependent contrast perception modelling
- **Arxiv ID**: http://arxiv.org/abs/1910.06808v2
- **DOI**: 10.1007/s10851-020-00960-x
- **Categories**: **cs.CV**, math.OC
- **Links**: [PDF](http://arxiv.org/pdf/1910.06808v2)
- **Published**: 2019-10-15 14:30:55+00:00
- **Updated**: 2020-07-17 06:38:24+00:00
- **Authors**: Marcelo Bertalmío, Luca Calatroni, Valentina Franceschi, Benedetta Franceschiello, Dario Prandi
- **Comment**: This is the revised extended invited journal version of the SSVM 2019
  conference proceeding arXiv:1812.07425
- **Journal**: Journal of Mathematical Imaging and Vision 2020
- **Summary**: We consider the evolution model proposed in [9, 6] to describe illusory contrast perception phenomena induced by surrounding orientations. Firstly, we highlight its analogies and differences with the widely used Wilson-Cowan equations [48], mainly in terms of efficient representation properties. Then, in order to explicitly encode local directional information, we exploit the model of the primary visual cortex (V1) proposed in [20] and largely used over the last years for several image processing problems [24,38,28]. The resulting model is thus defined in the space of positions and orientation and it is capable to describe assimilation and contrast visual bias at the same time. We report several numerical tests showing the ability of the model to reproduce, in particular, orientation-dependent phenomena such as grating induction and a modified version of the Poggendorff illusion. For this latter example, we empirically show the existence of a set of threshold parameters differentiating from inpainting to perception-type reconstructions and describing long-range connectivity between different hypercolumns in V1.



### Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1910.06809v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06809v3)
- **Published**: 2019-10-15 14:33:07+00:00
- **Updated**: 2020-01-10 15:29:28+00:00
- **Authors**: Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, Hongsheng Li
- **Comment**: Accepted by NeurIPS 2019. Code is available soon at
  https://github.com/xh-liu/CC-FPSE
- **Journal**: None
- **Summary**: Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.



### Learning Generalisable Omni-Scale Representations for Person Re-Identification
- **Arxiv ID**: http://arxiv.org/abs/1910.06827v5
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06827v5)
- **Published**: 2019-10-15 14:44:16+00:00
- **Updated**: 2021-04-29 14:41:52+00:00
- **Authors**: Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang
- **Comment**: TPAMI 2021. Journal extension of arXiv:1905.00953. Updates: added
  appendix. arXiv admin note: text overlap with arXiv:1905.00953
- **Journal**: None
- **Summary**: An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we introduce instance normalisation (IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to determine the optimal placements of these IN layers in the architecture, we formulate an efficient differentiable architecture search algorithm. Extensive experiments show that, in the conventional same-dataset setting, OSNet achieves state-of-the-art performance, despite being much smaller than existing re-ID models. In the more challenging yet practical cross-dataset setting, OSNet beats most recent unsupervised domain adaptation methods without using any target data. Our code and models are released at \texttt{https://github.com/KaiyangZhou/deep-person-reid}.



### A Hybrid Compact Neural Architecture for Visual Place Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.06840v3
- **DOI**: 10.1109/LRA.2020.2967324
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.06840v3)
- **Published**: 2019-10-15 14:58:54+00:00
- **Updated**: 2020-01-19 09:18:47+00:00
- **Authors**: Marvin Chancán, Luis Hernandez-Nunez, Ajay Narendra, Andrew B. Barron, Michael Milford
- **Comment**: Preprint version of article published in IEEE Robotics and Automation
  Letters
- **Journal**: IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 993-1000,
  April 2020
- **Summary**: State-of-the-art algorithms for visual place recognition, and related visual navigation systems, can be broadly split into two categories: computer-science-oriented models including deep learning or image retrieval-based techniques with minimal biological plausibility, and neuroscience-oriented dynamical networks that model temporal properties underlying spatial navigation in the brain. In this letter, we propose a new compact and high-performing place recognition model that bridges this divide for the first time. Our approach comprises two key neural models of these categories: (1) FlyNet, a compact, sparse two-layer neural network inspired by brain architectures of fruit flies, Drosophila melanogaster, and (2) a one-dimensional continuous attractor neural network (CANN). The resulting FlyNet+CANN network incorporates the compact pattern recognition capabilities of our FlyNet model with the powerful temporal filtering capabilities of an equally compact CANN, replicating entirely in a hybrid neural implementation the functionality that yields high performance in algorithmic localization approaches like SeqSLAM. We evaluate our model, and compare it to three state-of-the-art methods, on two benchmark real-world datasets with small viewpoint variations and extreme environmental changes - achieving 87% AUC results under day to night transitions compared to 60% for Multi-Process Fusion, 46% for LoST-X and 1% for SeqSLAM, while being 6.5, 310, and 1.5 times faster, respectively.



### DeepGCNs: Making GCNs Go as Deep as CNNs
- **Arxiv ID**: http://arxiv.org/abs/1910.06849v3
- **DOI**: 10.1109/TPAMI.2021.3074057
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06849v3)
- **Published**: 2019-10-15 15:10:34+00:00
- **Updated**: 2021-05-14 21:35:58+00:00
- **Authors**: Guohao Li, Matthias Müller, Guocheng Qian, Itzel C. Delgadillo, Abdulellah Abualshour, Ali Thabet, Bernard Ghanem
- **Comment**: Accepted at TPAMI. This work is a journal extension of our ICCV'19
  paper arXiv:1904.03751. The first three authors contributed equally
- **Journal**: None
- **Summary**: Convolutional Neural Networks (CNNs) have been very successful at solving a variety of computer vision tasks such as object classification and detection, semantic segmentation, activity understanding, to name just a few. One key enabling factor for their great performance has been the ability to train very deep networks. Despite their huge success in many tasks, CNNs do not work well with non-Euclidean data, which is prevalent in many real-world applications. Graph Convolutional Networks (GCNs) offer an alternative that allows for non-Eucledian data input to a neural network. While GCNs already achieve encouraging results, they are currently limited to architectures with a relatively small number of layers, primarily due to vanishing gradients during training. This work transfers concepts such as residual/dense connections and dilated convolutions from CNNs to GCNs in order to successfully train very deep GCNs. We show the benefit of using deep GCNs (with as many as 112 layers) experimentally across various datasets and tasks. Specifically, we achieve very promising performance in part segmentation and semantic segmentation on point clouds and in node classification of protein functions across biological protein-protein interaction (PPI) graphs. We believe that the insights in this work will open avenues for future research on GCNs and their application to further tasks not explored in this paper. The source code for this work is available at https://github.com/lightaime/deep_gcns_torch and https://github.com/lightaime/deep_gcns for PyTorch and TensorFlow implementation respectively.



### Quantifying Classification Uncertainty using Regularized Evidential Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06864v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06864v1)
- **Published**: 2019-10-15 15:26:20+00:00
- **Updated**: 2019-10-15 15:26:20+00:00
- **Authors**: Xujiang Zhao, Yuzhe Ou, Lance Kaplan, Feng Chen, Jin-Hee Cho
- **Comment**: Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA
- **Journal**: None
- **Summary**: Traditional deep neural nets (NNs) have shown the state-of-the-art performance in the task of classification in various applications. However, NNs have not considered any types of uncertainty associated with the class probabilities to minimize risk due to misclassification under uncertainty in real life. Unlike Bayesian neural nets indirectly infering uncertainty through weight uncertainties, evidential neural networks (ENNs) have been recently proposed to support explicit modeling of the uncertainty of class probabilities. It treats predictions of an NN as subjective opinions and learns the function by collecting the evidence leading to these opinions by a deterministic NN from data. However, an ENN is trained as a black box without explicitly considering different types of inherent data uncertainty, such as vacuity (uncertainty due to a lack of evidence) or dissonance (uncertainty due to conflicting evidence). This paper presents a new approach, called a {\em regularized ENN}, that learns an ENN based on regularizations related to different characteristics of inherent data uncertainty. Via the experiments with both synthetic and real-world datasets, we demonstrate that the proposed regularized ENN can better learn of an ENN modeling different types of uncertainty in the class probabilities for classification tasks.



### Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11111v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.11111v3)
- **Published**: 2019-10-15 15:45:41+00:00
- **Updated**: 2020-05-29 02:35:49+00:00
- **Authors**: Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou
- **Comment**: filed as a patent
- **Journal**: None
- **Summary**: Automatic facial behavior analysis has a long history of studies in the intersection of computer vision, physiology and psychology. However it is only recently, with the collection of large-scale datasets and powerful machine learning methods such as deep neural networks, that automatic facial behavior analysis started to thrive. Three of its iconic tasks are automatic recognition of basic expressions (e.g. happy, sad, surprised), estimation of continuous emotions (e.g., valence and arousal), and detection of facial action units (activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these tasks have been mostly studied independently collecting a dataset for the task. We present the first and the largest study of all facial behaviour tasks learned jointly in a single multi-task, multi-domain and multi-label network, which we call FaceBehaviorNet. For this we utilize all publicly available datasets in the community (around 5M images) that study facial behaviour tasks in-the-wild. We demonstrate that training jointly an end-to-end network for all tasks has consistently better performance than training each of the single-task networks. Furthermore, we propose two simple strategies for coupling the tasks during training, co-annotation and distribution matching, and show the advantages of this approach. Finally we show that FaceBehaviorNet has learned features that encapsulate all aspects of facial behaviour, and can be successfully applied to perform tasks (compound emotion recognition) beyond the ones that it has been trained in a zero- and few-shot learning setting.



### Human Action Recognition with Multi-Laplacian Graph Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06934v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06934v1)
- **Published**: 2019-10-15 17:15:55+00:00
- **Updated**: 2019-10-15 17:15:55+00:00
- **Authors**: Ahmed Mazari, Hichem Sahbi
- **Comment**: None
- **Journal**: None
- **Summary**: Convolutional neural networks are nowadays witnessing a major success in different pattern recognition problems. These learning models were basically designed to handle vectorial data such as images but their extension to non-vectorial and semi-structured data (namely graphs with variable sizes, topology, etc.) remains a major challenge, though a few interesting solutions are currently emerging. In this paper, we introduce MLGCN; a novel spectral Multi-Laplacian Graph Convolutional Network. The main contribution of this method resides in a new design principle that learns graph-laplacians as convex combinations of other elementary laplacians each one dedicated to a particular topology of the input graphs. We also introduce a novel pooling operator, on graphs, that proceeds in two steps: context-dependent node expansion is achieved, followed by a global average pooling; the strength of this two-step process resides in its ability to preserve the discrimination power of nodes while achieving permutation invariance. Experiments conducted on SBU and UCF-101 datasets, show the validity of our method for the challenging task of action recognition.



### The Local Elasticity of Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06943v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.06943v2)
- **Published**: 2019-10-15 17:35:30+00:00
- **Updated**: 2020-02-15 02:04:50+00:00
- **Authors**: Hangfeng He, Weijie J. Su
- **Comment**: To appear in ICLR 2020
- **Journal**: None
- **Summary**: This paper presents a phenomenon in neural networks that we refer to as \textit{local elasticity}. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector $\bx'$ is \textit{not} significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector $\bx$ that is \textit{dissimilar} to $\bx'$ in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel \citep{jacot2018neural}. Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with $K$-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.



### Tiny Video Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.06961v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.06961v3)
- **Published**: 2019-10-15 17:55:37+00:00
- **Updated**: 2021-06-30 01:25:26+00:00
- **Authors**: AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo
- **Comment**: None
- **Journal**: None
- **Summary**: Video understanding is a challenging problem with great impact on the abilities of autonomous agents working in the real-world. Yet, solutions so far have been computationally intensive, with the fastest algorithms running for more than half a second per video snippet on powerful GPUs. We propose a novel idea on video architecture learning - Tiny Video Networks - which automatically designs highly efficient models for video understanding. The tiny video models run with competitive performance for as low as 37 milliseconds per video on a CPU and 10 milliseconds on a standard GPU.



### SegSort: Segmentation by Discriminative Sorting of Segments
- **Arxiv ID**: http://arxiv.org/abs/1910.06962v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.06962v2)
- **Published**: 2019-10-15 17:58:20+00:00
- **Updated**: 2019-10-30 17:43:04+00:00
- **Authors**: Jyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju Yang, Xiao Zhang, Liang-Chieh Chen
- **Comment**: In ICCV 2019. Webpage & Code:
  https://jyhjinghwang.github.io/projects/segsort.html
- **Journal**: None
- **Summary**: Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set.   As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving $76\%$ performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.



### Autonomous Aerial Cinematography In Unstructured Environments With Learned Artistic Decision-Making
- **Arxiv ID**: http://arxiv.org/abs/1910.06988v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.06988v1)
- **Published**: 2019-10-15 18:17:58+00:00
- **Updated**: 2019-10-15 18:17:58+00:00
- **Authors**: Rogerio Bonatti, Wenshan Wang, Cherie Ho, Aayush Ahuja, Mirko Gschwindt, Efe Camci, Erdal Kayacan, Sanjiban Choudhury, Sebastian Scherer
- **Comment**: None
- **Journal**: None
- **Summary**: Aerial cinematography is revolutionizing industries that require live and dynamic camera viewpoints such as entertainment, sports, and security. However, safely piloting a drone while filming a moving target in the presence of obstacles is immensely taxing, often requiring multiple expert human operators. Hence, there is demand for an autonomous cinematographer that can reason about both geometry and scene context in real-time. Existing approaches do not address all aspects of this problem; they either require high-precision motion-capture systems or GPS tags to localize targets, rely on prior maps of the environment, plan for short time horizons, or only follow artistic guidelines specified before flight.   In this work, we address the problem in its entirety and propose a complete system for real-time aerial cinematography that for the first time combines: (1) vision-based target estimation; (2) 3D signed-distance mapping for occlusion estimation; (3) efficient trajectory optimization for long time-horizon camera motion; and (4) learning-based artistic shot selection. We extensively evaluate our system both in simulation and in field experiments by filming dynamic targets moving through unstructured environments. Our results indicate that our system can operate reliably in the real world without restrictive assumptions. We also provide in-depth analysis and discussions for each module, with the hope that our design tradeoffs can generalize to other related applications. Videos of the complete system can be found at: https://youtu.be/ookhHnqmlaU.



### Compact Network Training for Person ReID
- **Arxiv ID**: http://arxiv.org/abs/1910.07038v3
- **DOI**: 10.1145/3372278.3390686
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07038v3)
- **Published**: 2019-10-15 20:16:55+00:00
- **Updated**: 2020-04-09 08:17:01+00:00
- **Authors**: Hussam Lawen, Avi Ben-Cohen, Matan Protter, Itamar Friedman, Lihi Zelnik-Manor
- **Comment**: None
- **Journal**: None
- **Summary**: The task of person re-identification (ReID) has attracted growing attention in recent years leading to improved performance, albeit with little focus on real-world applications. Most SotA methods are based on heavy pre-trained models, e.g. ResNet50 (~25M parameters), which makes them less practical and more tedious to explore architecture modifications. In this study, we focus on a small-sized randomly initialized model that enables us to easily introduce architecture and training modifications suitable for person ReID. The outcomes of our study are a compact network and a fitting training regime. We show the robustness of the network by outperforming the SotA on both Market1501 and DukeMTMC. Furthermore, we show the representation power of our ReID network via SotA results on a different task of multi-object tracking.



### MUTE: Data-Similarity Driven Multi-hot Target Encoding for Neural Network Design
- **Arxiv ID**: http://arxiv.org/abs/1910.07042v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.07042v1)
- **Published**: 2019-10-15 20:23:06+00:00
- **Updated**: 2019-10-15 20:23:06+00:00
- **Authors**: Mayoore S. Jaiswal, Bumsoo Kang, Jinho Lee, Minsik Cho
- **Comment**: NeurIPS Workshop 2019 - Learning with Rich Experience: Integration of
  Learning Paradigms
- **Journal**: None
- **Summary**: Target encoding is an effective technique to deliver better performance for conventional machine learning methods, and recently, for deep neural networks as well. However, the existing target encoding approaches require significant increase in the learning capacity, thus demand higher computation power and more training data. In this paper, we present a novel and efficient target encoding scheme, MUTE to improve both generalizability and robustness of a target model by understanding the inter-class characteristics of a target dataset. By extracting the confusion level between the target classes in a dataset, MUTE strategically optimizes the Hamming distances among target encoding. Such optimized target encoding offers higher classification strength for neural network models with negligible computation overhead and without increasing the model size. When MUTE is applied to the popular image classification networks and datasets, our experimental results show that MUTE offers better generalization and defense against the noises and adversarial attacks over the existing solutions.



### Wasserstein GANs for MR Imaging: from Paired to Unpaired Training
- **Arxiv ID**: http://arxiv.org/abs/1910.07048v3
- **DOI**: 10.1109/TMI.2020.3022968
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.07048v3)
- **Published**: 2019-10-15 20:47:34+00:00
- **Updated**: 2020-09-08 00:50:12+00:00
- **Authors**: Ke Lei, Morteza Mardani, John M. Pauly, Shreyas S. Vasanawala
- **Comment**: None
- **Journal**: None
- **Summary**: Lack of ground-truth MR images impedes the common supervised training of neural networks for image reconstruction. To cope with this challenge, this paper leverages unpaired adversarial training for reconstruction networks, where the inputs are undersampled k-space and naively reconstructed images from one dataset, and the labels are high-quality images from another dataset. The reconstruction networks consist of a generator which suppresses the input image artifacts, and a discriminator using a pool of (unpaired) labels to adjust the reconstruction quality. The generator is an unrolled neural network -- a cascade of convolutional and data consistency layers. The discriminator is also a multilayer CNN that plays the role of a critic scoring the quality of reconstructed images based on the Wasserstein distance. Our experiments with knee MRI datasets demonstrate that the proposed unpaired training enables diagnostic-quality reconstruction when high-quality image labels are not available for the input types of interest, or when the amount of labels is small. In addition, our adversarial training scheme can achieve better image quality (as rated by expert radiologists) compared with the paired training schemes with pixel-wise loss.



### On adversarial patches: real-world attack on ArcFace-100 face recognition system
- **Arxiv ID**: http://arxiv.org/abs/1910.07067v3
- **DOI**: 10.1109/SIBIRCON48586.2019.8958134
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.07067v3)
- **Published**: 2019-10-15 21:49:56+00:00
- **Updated**: 2020-04-01 23:14:52+00:00
- **Authors**: Mikhail Pautov, Grigorii Melnikov, Edgar Kaziakhmedov, Klim Kireev, Aleksandr Petiushko
- **Comment**: None
- **Journal**: 2019 International Multi-Conference on Engineering, Computer and
  Information Sciences (SIBIRCON)
- **Summary**: Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.



### DeepErase: Weakly Supervised Ink Artifact Removal in Document Text Images
- **Arxiv ID**: http://arxiv.org/abs/1910.07070v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1910.07070v3)
- **Published**: 2019-10-15 21:57:04+00:00
- **Updated**: 2020-01-16 05:35:49+00:00
- **Authors**: W. Ronny Huang, Yike Qi, Qianqian Li, Jonathan Degange
- **Comment**: Conference paper at WACV 2020. First two authors contributed equally
- **Journal**: None
- **Summary**: Paper-intensive industries like insurance, law, and government have long leveraged optical character recognition (OCR) to automatically transcribe hordes of scanned documents into text strings for downstream processing. Even in 2019, there are still many scanned documents and mail that come into businesses in non-digital format. Text to be extracted from real world documents is often nestled inside rich formatting, such as tabular structures or forms with fill-in-the-blank boxes or underlines whose ink often touches or even strikes through the ink of the text itself. Further, the text region could have random ink smudges or spurious strokes. Such ink artifacts can severely interfere with the performance of recognition algorithms or other downstream processing tasks. In this work, we propose DeepErase, a neural-based preprocessor to erase ink artifacts from text images. We devise a method to programmatically assemble real text images and real artifacts into realistic-looking "dirty" text images, and use them to train an artifact segmentation network in a weakly supervised manner, since pixel-level annotations are automatically obtained during the assembly process. In addition to high segmentation accuracy, we show that our cleansed images achieve a significant boost in recognition accuracy by popular OCR software such as Tesseract 4.0. Finally, we test DeepErase on out-of-distribution datasets (NIST SDB) of scanned IRS tax return forms and achieve double-digit improvements in accuracy. All experiments are performed on both printed and handwritten text. Code for all experiments is available at https://github.com/yikeqicn/DeepErase



