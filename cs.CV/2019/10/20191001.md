# Arxiv Papers in cs.CV on 2019-10-01
### Saliency is a Possible Red Herring When Diagnosing Poor Generalization
- **Arxiv ID**: http://arxiv.org/abs/1910.00199v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00199v3)
- **Published**: 2019-10-01 04:29:18+00:00
- **Updated**: 2021-02-10 16:40:27+00:00
- **Authors**: Joseph D. Viviano, Becks Simpson, Francis Dutil, Yoshua Bengio, Joseph Paul Cohen
- **Comment**: 25 pages, 27 figures, 5 tables, code in paper
  (https://github.com/josephdviviano/saliency-red-herring). Published at
  International Conference on Learning Representations (ICLR) 2021. Previously
  titled "Underwhelming Generalization Improvements from Controlling Feature
  Attribution"
- **Journal**: None
- **Summary**: Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert has labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as "attribution priors" as well as saliency maps for explainable predictions.



### Revisiting Fine-tuning for Few-shot Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.00216v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00216v2)
- **Published**: 2019-10-01 06:21:50+00:00
- **Updated**: 2019-10-03 04:53:10+00:00
- **Authors**: Akihiro Nakamura, Tatsuya Harada
- **Comment**: 10 pages
- **Journal**: None
- **Summary**: Few-shot learning is the process of learning novel classes using only a few examples and it remains a challenging task in machine learning. Many sophisticated few-shot learning algorithms have been proposed based on the notion that networks can easily overfit to novel examples if they are simply fine-tuned using only a few examples. In this study, we show that in the commonly used low-resolution mini-ImageNet dataset, the fine-tuning method achieves higher accuracy than common few-shot learning algorithms in the 1-shot task and nearly the same accuracy as that of the state-of-the-art algorithm in the 5-shot task. We then evaluate our method with more practical tasks, namely the high-resolution single-domain and cross-domain tasks. With both tasks, we show that our method achieves higher accuracy than common few-shot learning algorithms. We further analyze the experimental results and show that: 1) the retraining process can be stabilized by employing a low learning rate, 2) using adaptive gradient optimizers during fine-tuning can increase test accuracy, and 3) test accuracy can be improved by updating the entire network when a large domain-shift exists between base and novel classes.



### Addressing Failure Prediction by Learning Model Confidence
- **Arxiv ID**: http://arxiv.org/abs/1910.04851v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04851v2)
- **Published**: 2019-10-01 08:23:45+00:00
- **Updated**: 2019-10-26 15:09:46+00:00
- **Authors**: Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, Patrick Pérez
- **Comment**: NeurIPS 2019 (accepted)
- **Journal**: None
- **Summary**: Assessing reliably the confidence of a deep neural network and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.



### Harmonization of diffusion MRI datasets with adaptive dictionary learning
- **Arxiv ID**: http://arxiv.org/abs/1910.00272v5
- **DOI**: 10.1002/hbm.25117
- **Categories**: **eess.IV**, cs.CV, physics.med-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.00272v5)
- **Published**: 2019-10-01 09:28:53+00:00
- **Updated**: 2020-06-11 21:40:28+00:00
- **Authors**: Samuel St-Jean, Max A. Viergever, Alexander Leemans
- **Comment**: v5 Peer review for Human Brain Mapping v4: Peer review round 2 v3:
  Peer reviewed version v2: Fix minor text issue + add supp materials v1: To be
  submitted to Neuroimage
- **Journal**: Human Brain Mapping, hbm. 25117 (2020)
- **Summary**: Diffusion magnetic resonance imaging is a noninvasive imaging technique that can indirectly infer the microstructure of tissues and provide metrics which are subject to normal variability across subjects. Potentially abnormal values or features may yield essential information to support analysis of controls and patients cohorts, but subtle confounds affecting diffusion MRI, such as those due to difference in scanning protocols or hardware, can lead to systematic errors which could be mistaken for purely biologically driven variations amongst subjects. In this work, we propose a new harmonization algorithm based on adaptive dictionary learning to mitigate the unwanted variability caused by different scanner hardware while preserving the natural biological variability present in the data. Overcomplete dictionaries, which are learned automatically from the data and do not require paired samples, are then used to reconstruct the data from a different scanner, removing variability present in the source scanner in the process. We use the publicly available database from an international challenge to evaluate the method, which was acquired on three different scanners and with two different protocols, and propose a new mapping towards a scanner-agnostic space. Results show that the effect size of the four studied diffusion metrics is preserved while removing variability attributable to the scanner. Experiments with alterations using a free water compartment, which is not simulated in the training data, shows that the effect size induced by the alterations is also preserved after harmonization. The algorithm is freely available and could help multicenter studies in pooling their data, while removing scanner specific confounds, and increase statistical power in the process.



### Unsupervised Generative 3D Shape Learning from Natural Images
- **Arxiv ID**: http://arxiv.org/abs/1910.00287v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00287v1)
- **Published**: 2019-10-01 10:20:59+00:00
- **Updated**: 2019-10-01 10:20:59+00:00
- **Authors**: Attila Szabó, Givi Meishvili, Paolo Favaro
- **Comment**: The paper is under review
- **Journal**: None
- **Summary**: In this paper we present, to the best of our knowledge, the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way. For example, we do not use any ground truth 3D or 2D annotations, stereo video, and ego-motion during the training. Our approach follows the general strategy of Generative Adversarial Networks, where an image generator network learns to create image samples that are realistic enough to fool a discriminator network into believing that they are natural images. In contrast, in our approach the image generation is split into 2 stages. In the first stage a generator network outputs 3D objects. In the second, a differentiable renderer produces an image of the 3D objects from random viewpoints. The key observation is that a realistic 3D object should yield a realistic rendering from any plausible viewpoint. Thus, by randomizing the choice of the viewpoint our proposed training forces the generator network to learn an interpretable 3D representation disentangled from the viewpoint. In this work, a 3D representation consists of a triangle mesh and a texture map that is used to color the triangle surface by using the UV-mapping technique. We provide analysis of our learning approach, expose its ambiguities and show how to overcome them. Experimentally, we demonstrate that our method can learn realistic 3D shapes of faces by using only the natural images of the FFHQ dataset.



### Insect pest image detection and recognition based on bio-inspired methods
- **Arxiv ID**: http://arxiv.org/abs/1910.00296v3
- **DOI**: 10.1016/j.ecoinf.2020.101089
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00296v3)
- **Published**: 2019-10-01 10:40:50+00:00
- **Updated**: 2020-03-27 20:21:37+00:00
- **Authors**: Loris Nanni, Gianluca Maguolo, Fabio Pancino
- **Comment**: None
- **Journal**: None
- **Summary**: Insect pests recognition is necessary for crop protection in many areas of the world. In this paper we propose an automatic classifier based on the fusion between saliency methods and convolutional neural networks. Saliency methods are famous image processing algorithms that highlight the most relevant pixels of an image. In this paper, we use three different saliency methods as image preprocessing and create three different images for every saliency method. Hence, we create 3x3=9 new images for every original image to train different convolutional neural networks. We evaluate the performance of every preprocessing/network couple and we also evaluate the performance of their ensemble. We test our approach on both a small dataset and the large IP102 dataset. Our best ensembles reaches the state of the art accuracy on both the smaller dataset (92.43%) and the IP102 dataset (61.93%), approaching the performance of human experts on the smaller one. Besides, we share our MATLAB code at: https://github.com/LorisNanni/.



### Graph convolutional networks for learning with few clean and many noisy labels
- **Arxiv ID**: http://arxiv.org/abs/1910.00324v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.00324v3)
- **Published**: 2019-10-01 11:56:09+00:00
- **Updated**: 2020-08-24 21:33:51+00:00
- **Authors**: Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum, Cordelia Schmid
- **Comment**: None
- **Journal**: None
- **Summary**: In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier, which learns to discriminate clean from noisy examples using a weighted binary cross-entropy loss function. The GCN-inferred "clean" probability is then exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data, as well as standard few-shot classification where only few clean examples are used.



### Sub-Architecture Ensemble Pruning in Neural Architecture Search
- **Arxiv ID**: http://arxiv.org/abs/1910.00370v2
- **DOI**: 10.1109/TNNLS.2021.3085299
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00370v2)
- **Published**: 2019-10-01 13:26:54+00:00
- **Updated**: 2021-05-28 03:37:48+00:00
- **Authors**: Yijun Bian, Qingquan Song, Mengnan Du, Jun Yao, Huanhuan Chen, Xia Hu
- **Comment**: Accepted by TNNLS. This work was done when the first author was a
  visiting research scholar at Texas A&M University
- **Journal**: None
- **Summary**: Neural architecture search (NAS) is gaining more and more attention in recent years due to its flexibility and remarkable capability to reduce the burden of neural network design. To achieve better performance, however, the searching process usually costs massive computations that might not be affordable for researchers and practitioners. While recent attempts have employed ensemble learning methods to mitigate the enormous computational cost, however, they neglect a key property of ensemble methods, namely diversity, which leads to collecting more similar sub-architectures with potential redundancy in the final design. To tackle this problem, we propose a pruning method for NAS ensembles called "Sub-Architecture Ensemble Pruning in Neural Architecture Search (SAEP)." It targets to leverage diversity and to achieve sub-ensemble architectures at a smaller size with comparable performance to ensemble architectures that are not pruned. Three possible solutions are proposed to decide which sub-architectures to prune during the searching process. Experimental results exhibit the effectiveness of the proposed method by largely reducing the number of sub-architectures without degrading the performance.



### Leveraging Model Interpretability and Stability to increase Model Robustness
- **Arxiv ID**: http://arxiv.org/abs/1910.00387v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00387v2)
- **Published**: 2019-10-01 13:51:56+00:00
- **Updated**: 2019-11-05 23:23:04+00:00
- **Authors**: Fei Wu, Thomas Michel, Alexandre Briot
- **Comment**: 2019 ICCV workshop on Interpreting and Explaining Visual AI models; 8
  pages
- **Journal**: None
- **Summary**: State of the art Deep Neural Networks (DNN) can now achieve above human level accuracy on image classification tasks. However their outstanding performances come along with a complex inference mechanism making them arduously interpretable models. In order to understand the underlying prediction rules of DNNs, Dhamdhere et al. propose an interpretability method to break down a DNN prediction score as sum of its hidden unit contributions, in the form of a metric called conductance. Analyzing conductances of DNN hidden units, we find out there is a difference in how wrong and correct predictions are inferred. We identify distinguishable patterns of hidden unit activations for wrong and correct predictions. We then use an error detector in the form of a binary classifier on top of the DNN to automatically discriminate wrong and correct predictions of the DNN based on their hidden unit activations. Detected wrong predictions are discarded, increasing the model robustness. A different approach to distinguish wrong and correct predictions of DNNs is proposed by Wang et al. whose method is based on the premise that input samples leading a DNN into making wrong predictions are less stable to the DNN weight changes than correctly classified input samples. In our study, we compare both methods and find out by combining them that better detection of wrong predictions can be achieved.



### Towards Automatic Embryo Staging in 3D+T Microscopy Images using Convolutional Neural Networks and PointNets
- **Arxiv ID**: http://arxiv.org/abs/1910.00443v3
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, q-bio.CB
- **Links**: [PDF](http://arxiv.org/pdf/1910.00443v3)
- **Published**: 2019-10-01 14:29:23+00:00
- **Updated**: 2020-07-29 09:01:32+00:00
- **Authors**: Manuel Traub, Johannes Stegmaier
- **Comment**: 10 pages, 3 figures, 1 table, accepted paper at the Simulation and
  Synthesis in Medical Imaging (SASHIMI) Workshop held at MICCAI 2020
- **Journal**: None
- **Summary**: Automatic analyses and comparisons of different stages of embryonic development largely depend on a highly accurate spatiotemporal alignment of the investigated data sets. In this contribution, we assess multiple approaches for automatic staging of developing embryos that were imaged with time-resolved 3D light-sheet microscopy. The methods comprise image-based convolutional neural networks as well as an approach based on the PointNet architecture that directly operates on 3D point clouds of detected cell nuclei centroids. The experiments with four wild-type zebrafish embryos render both approaches suitable for automatic staging with average deviations of 21 - 34 minutes. Moreover, a proof-of-concept evaluation based on simulated 3D+t point cloud data sets shows that average deviations of less than 7 minutes are possible.



### Automatic Segmentation of Muscle Tissue and Inter-muscular Fat in Thigh and Calf MRI Images
- **Arxiv ID**: http://arxiv.org/abs/1910.04866v1
- **DOI**: 10.1007/978-3-030-32245-8_25
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.04866v1)
- **Published**: 2019-10-01 14:48:31+00:00
- **Updated**: 2019-10-01 14:48:31+00:00
- **Authors**: Rula Amer, Jannette Nassar, David Bendahan, Hayit Greenspan, Noam Ben-Eliezer
- **Comment**: 9 pages, 4 figures, 2 tables, MICCAI 2019, the 22nd International
  Conference on Medical Image Computing and Computer Assisted Intervention
- **Journal**: None
- **Summary**: Magnetic resonance imaging (MRI) of thigh and calf muscles is one of the most effective techniques for estimating fat infiltration into muscular dystrophies. The infiltration of adipose tissue into the diseased muscle region varies in its severity across, and within, patients. In order to efficiently quantify the infiltration of fat, accurate segmentation of muscle and fat is needed. An estimation of the amount of infiltrated fat is typically done visually by experts. Several algorithmic solutions have been proposed for automatic segmentation. While these methods may work well in mild cases, they struggle in moderate and severe cases due to the high variability in the intensity of infiltration, and the tissue's heterogeneous nature. To address these challenges, we propose a deep-learning approach, producing robust results with high Dice Similarity Coefficient (DSC) of 0.964, 0.917 and 0.933 for muscle-region, healthy muscle and inter-muscular adipose tissue (IMAT) segmentation, respectively.



### Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation
- **Arxiv ID**: http://arxiv.org/abs/1910.00462v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CV, stat.ML, 68T05, 68T30, 68T45, I.2.10; I.2.6; I.2.4; I.4.9; I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/1910.00462v1)
- **Published**: 2019-10-01 14:56:08+00:00
- **Updated**: 2019-10-01 14:56:08+00:00
- **Authors**: Ivan Donadello, Luciano Serafini
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic Image Interpretation is the task of extracting a structured semantic description from images. This requires the detection of visual relationships: triples (subject,relation,object) describing a semantic relation between a subject and an object. A pure supervised approach to visual relationship detection requires a complete and balanced training set for all the possible combinations of (subject, relation, object). However, such training sets are not available and would require a prohibitive human effort. This implies the ability of predicting triples which do not appear in the training set. This problem is called zero-shot learning. State-of-the-art approaches to zero-shot learning exploit similarities among relationships in the training set or external linguistic knowledge. In this paper, we perform zero-shot learning by using Logic Tensor Networks, a novel Statistical Relational Learning framework that exploits both the similarities with other seen relationships and background knowledge, expressed with logical constraints between subjects, relations and objects. The experiments on the Visual Relationship Dataset show that the use of logical constraints outperforms the current methods. This implies that background knowledge can be used to alleviate the incompleteness of training sets.



### Deep Neural Rejection against Adversarial Examples
- **Arxiv ID**: http://arxiv.org/abs/1910.00470v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68T45
- **Links**: [PDF](http://arxiv.org/pdf/1910.00470v3)
- **Published**: 2019-10-01 15:08:34+00:00
- **Updated**: 2020-04-17 13:42:24+00:00
- **Authors**: Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Xiaoyi Feng, Fabio Roli
- **Comment**: None
- **Journal**: None
- **Summary**: Despite the impressive performances reported by deep neural networks in different application domains, they remain largely vulnerable to adversarial examples, i.e., input samples that are carefully perturbed to cause misclassification at test time. In this work, we propose a deep neural rejection mechanism to detect adversarial examples, based on the idea of rejecting samples that exhibit anomalous feature representations at different network layers. With respect to competing approaches, our method does not require generating adversarial examples at training time, and it is less computationally demanding. To properly evaluate our method, we define an adaptive white-box attack that is aware of the defense mechanism and aims to bypass it. Under this worst-case setting, we empirically show that our approach outperforms previously-proposed methods that detect adversarial examples by only analyzing the feature representation provided by the output network layer.



### A Three-dimensional Convolutional-Recurrent Network for Convective Storm Nowcasting
- **Arxiv ID**: http://arxiv.org/abs/1910.00527v2
- **DOI**: 10.1109/ICBK.2019.00052
- **Categories**: **cs.CV**, physics.ao-ph, physics.geo-ph
- **Links**: [PDF](http://arxiv.org/pdf/1910.00527v2)
- **Published**: 2019-10-01 16:28:32+00:00
- **Updated**: 2019-11-03 14:19:41+00:00
- **Authors**: Wei Zhang, Wei Li, Lei Han
- **Comment**: 13 pages, 11 figures, accepted by 2019 IEEE International Conference
  on Big Knowledge The copyright of this paper has been transferred to the
  IEEE, please comply with the copyright of the IEEE
- **Journal**: In 2019 IEEE International Conference on Big Knowledge (ICBK) (pp.
  333-340) 2019
- **Summary**: Very short-term convective storm forecasting, termed nowcasting, has long been an important issue and has attracted substantial interest. Existing nowcasting methods rely principally on radar images and are limited in terms of nowcasting storm initiation and growth. Real-time re-analysis of meteorological data supplied by numerical models provides valuable information about three-dimensional (3D), atmospheric, boundary layer thermal dynamics, such as temperature and wind. To mine such data, we here develop a convolution-recurrent, hybrid deep-learning method with the following characteristics: (1) the use of cell-based oversampling to increase the number of training samples; this mitigates the class imbalance issue; (2) the use of both raw 3D radar data and 3D meteorological data re-analyzed via multi-source 3D convolution without any need for handcraft feature engineering; and (3) the stacking of convolutional neural networks on a long short-term memory encoder/decoder that learns the spatiotemporal patterns of convective processes. Experimental results demonstrated that our method performs better than other extrapolation methods. Qualitative analysis yielded encouraging nowcasting results.



### Manipulation Motion Taxonomy and Coding for Robots
- **Arxiv ID**: http://arxiv.org/abs/1910.00532v2
- **DOI**: 10.1109/IROS40897.2019.8967754
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00532v2)
- **Published**: 2019-10-01 16:34:07+00:00
- **Updated**: 2020-07-31 04:22:02+00:00
- **Authors**: David Paulius, Yongqiang Huang, Jason Meloncon, Yu Sun
- **Comment**: IROS 2019 Submission -- 6 pages
- **Journal**: None
- **Summary**: This paper introduces a taxonomy of manipulations as seen especially in cooking for 1) grouping manipulations from the robotics point of view, 2) consolidating aliases and removing ambiguity for motion types, and 3) provide a path to transferring learned manipulations to new unlearned manipulations. Using instructional videos as a reference, we selected a list of common manipulation motions seen in cooking activities grouped into similar motions based on several trajectory and contact attributes. Manipulation codes are then developed based on the taxonomy attributes to represent the manipulation motions. The manipulation taxonomy is then used for comparing motion data in the Daily Interactive Manipulation (DIM) data set to reveal their motion similarities.



### Real-Time Semantic Stereo Matching
- **Arxiv ID**: http://arxiv.org/abs/1910.00541v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.00541v2)
- **Published**: 2019-10-01 16:52:29+00:00
- **Updated**: 2020-02-24 20:23:56+00:00
- **Authors**: Pier Luigi Dovesi, Matteo Poggi, Lorenzo Andraghetti, Miquel Martí, Hedvig Kjellström, Alessandro Pieropan, Stefano Mattoccia
- **Comment**: 8 pages, 3 figures. Accepted to ICRA 2020
- **Journal**: None
- **Summary**: Scene understanding is paramount in robotics, self-navigation, augmented reality, and many other fields. To fully accomplish this task, an autonomous agent has to infer the 3D structure of the sensed scene (to know where it looks at) and its content (to know what it sees). To tackle the two tasks, deep neural networks trained to infer semantic segmentation and depth from stereo images are often the preferred choices. Specifically, Semantic Stereo Matching can be tackled by either standalone models trained for the two tasks independently or joint end-to-end architectures. Nonetheless, as proposed so far, both solutions are inefficient because requiring two forward passes in the former case or due to the complexity of a single network in the latter, although jointly tackling both tasks is usually beneficial in terms of accuracy. In this paper, we propose a single compact and lightweight architecture for real-time semantic stereo matching. Our framework relies on coarse-to-fine estimations in a multi-stage fashion, allowing: i) very fast inference even on embedded devices, with marginal drops in accuracy, compared to state-of-the-art networks, ii) trade accuracy for speed, according to the specific application requirements. Experimental results on high-end GPUs as well as on an embedded Jetson TX2 confirm the superiority of semantic stereo matching compared to standalone tasks and highlight the versatility of our framework on any hardware and for any application.



### A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark
- **Arxiv ID**: http://arxiv.org/abs/1910.04867v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.04867v2)
- **Published**: 2019-10-01 17:06:29+00:00
- **Updated**: 2020-02-21 13:36:15+00:00
- **Authors**: Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, Neil Houlsby
- **Comment**: None
- **Journal**: None
- **Summary**: Representation learning promises to unlock deep learning for the long tail of vision tasks without expensive labelled datasets. Yet, the absence of a unified evaluation for general visual representations hinders progress. Popular protocols are often too constrained (linear classification), limited in diversity (ImageNet, CIFAR, Pascal-VOC), or only weakly related to representation quality (ELBO, reconstruction error). We present the Visual Task Adaptation Benchmark (VTAB), which defines good representations as those that adapt to diverse, unseen tasks with few examples. With VTAB, we conduct a large-scale study of many popular publicly-available representation learning algorithms. We carefully control confounders such as architecture and tuning budget. We address questions like: How effective are ImageNet representations beyond standard natural datasets? How do representations trained via generative and discriminative models compare? To what extent can self-supervision replace labels? And, how close are we to general visual representations?



### End-to-end learning of energy-based representations for irregularly-sampled signals and images
- **Arxiv ID**: http://arxiv.org/abs/1910.00556v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00556v1)
- **Published**: 2019-10-01 17:31:31+00:00
- **Updated**: 2019-10-01 17:31:31+00:00
- **Authors**: Ronan Fablet, Lucas Drumetz, François Rousseau
- **Comment**: None
- **Journal**: None
- **Summary**: For numerous domains, including for instance earth observation, medical imaging, astrophysics,..., available image and signal datasets often involve irregular space-time sampling patterns and large missing data rates. These sampling properties may be critical to apply state-of-the-art learning-based (e.g., auto-encoders, CNNs,...), fully benefit from the available large-scale observations and reach breakthroughs in the reconstruction and identification of processes of interest. In this paper, we address the end-to-end learning of representations of signals, images and image sequences from irregularly-sampled data, i.e. when the training data involved missing data. From an analogy to Bayesian formulation, we consider energy-based representations. Two energy forms are investigated: one derived from auto-encoders and one relating to Gibbs priors. The learning stage of these energy-based representations (or priors) involve a joint interpolation issue, which amounts to solving an energy minimization problem under observation constraints. Using a neural-network-based implementation of the considered energy forms, we can state an end-to-end learning scheme from irregularly-sampled data. We demonstrate the relevance of the proposed representations for different case-studies: namely, multivariate time series, 2D images and image sequences.



### Elastic-InfoGAN: Unsupervised Disentangled Representation Learning in Class-Imbalanced Data
- **Arxiv ID**: http://arxiv.org/abs/1910.01112v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.01112v2)
- **Published**: 2019-10-01 17:50:44+00:00
- **Updated**: 2020-10-30 10:48:53+00:00
- **Authors**: Utkarsh Ojha, Krishna Kumar Singh, Cho-Jui Hsieh, Yong Jae Lee
- **Comment**: Camera ready version for NeurIPS 2020
- **Journal**: None
- **Summary**: We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We first investigate the issues surrounding the assumptions about uniformity made by InfoGAN, and demonstrate its ineffectiveness to properly disentangle object identity in imbalanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artificial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation.



### Training Kinetics in 15 Minutes: Large-scale Distributed Training on Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.00932v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00932v2)
- **Published**: 2019-10-01 17:58:07+00:00
- **Updated**: 2019-12-07 23:04:39+00:00
- **Authors**: Ji Lin, Chuang Gan, Song Han
- **Comment**: None
- **Journal**: None
- **Summary**: Deep video recognition is more computationally expensive than image recognition, especially on large-scale datasets like Kinetics [1]. Therefore, training scalability is essential to handle a large amount of videos. In this paper, we study the factors that impact the training scalability of video networks. We recognize three bottlenecks, including data loading (data movement from disk to GPU), communication (data movement over networking), and computation FLOPs. We propose three design guidelines to improve the scalability: (1) fewer FLOPs and hardware-friendly operator to increase the computation efficiency; (2) fewer input frames to reduce the data movement and increase the data loading efficiency; (3) smaller model size to reduce the networking traffic and increase the networking efficiency. With these guidelines, we designed a new operator Temporal Shift Module (TSM) that is efficient and scalable for distributed training. TSM model can achieve 1.8x higher throughput compared to previous I3D models. We scale up the training of the TSM model to 1,536 GPUs, with a mini-batch of 12,288 video clips/98,304 images, without losing the accuracy. With such hardware-aware model design, we are able to scale up the training on Summit supercomputer and reduce the training time on Kinetics dataset from 49 hours 55 minutes to 14 minutes 13 seconds, achieving a top-1 accuracy of 74.0%, which is 1.6x and 2.9x faster than previous 3D video models with higher accuracy. The code and more details can be found here: http://tsm-hanlab.mit.edu.



### Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGB-D video
- **Arxiv ID**: http://arxiv.org/abs/1910.00618v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, cs.SY, eess.SY
- **Links**: [PDF](http://arxiv.org/pdf/1910.00618v2)
- **Published**: 2019-10-01 18:58:19+00:00
- **Updated**: 2021-08-19 05:12:53+00:00
- **Authors**: Maria Bauza, Ferran Alet, Yen-Chen Lin, Tomas Lozano-Perez, Leslie P. Kaelbling, Phillip Isola, Alberto Rodriguez
- **Comment**: IROS 2019, 8 pages, 7 figures
- **Journal**: None
- **Summary**: Pushing is a fundamental robotic skill. Existing work has shown how to exploit models of pushing to achieve a variety of tasks, including grasping under uncertainty, in-hand manipulation and clearing clutter. Such models, however, are approximate, which limits their applicability. Learning-based methods can reason directly from raw sensory data with accuracy, and have the potential to generalize to a wider diversity of scenarios. However, developing and testing such methods requires rich-enough datasets. In this paper we introduce Omnipush, a dataset with high variety of planar pushing behavior. In particular, we provide 250 pushes for each of 250 objects, all recorded with RGB-D and a high precision tracking system. The objects are constructed so as to systematically explore key factors that affect pushing -- the shape of the object and its mass distribution -- which have not been broadly explored in previous datasets, and allow to study generalization in model learning. Omnipush includes a benchmark for meta-learning dynamic models, which requires algorithms that make good predictions and estimate their own uncertainty. We also provide an RGB video prediction benchmark and propose other relevant tasks that can be suited with this dataset.   Data and code are available at \url{https://web.mit.edu/mcube/omnipush-dataset/}.



### Sensor Fusion: Gated Recurrent Fusion to Learn Driving Behavior from Temporal Multimodal Data
- **Arxiv ID**: http://arxiv.org/abs/1910.00628v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.00628v2)
- **Published**: 2019-10-01 19:34:09+00:00
- **Updated**: 2020-01-21 17:31:55+00:00
- **Authors**: Athma Narayanan, Avinash Siravuru, Behzad Dariush
- **Comment**: Accepted to Robotics and Automation Letters 2020
- **Journal**: None
- **Summary**: The Tactical Driver Behavior modeling problem requires understanding of driver actions in complicated urban scenarios from a rich multi modal signals including video, LiDAR and CAN bus data streams. However, the majority of deep learning research is focused either on learning the vehicle/environment state (sensor fusion) or the driver policy (from temporal data), but not both. Learning both tasks end-to-end offers the richest distillation of knowledge, but presents challenges in formulation and successful training. In this work, we propose promising first steps in this direction. Inspired by the gating mechanisms in LSTM, we propose gated recurrent fusion units (GRFU) that learn fusion weighting and temporal weighting simultaneously. We demonstrate it's superior performance over multimodal and temporal baselines in supervised regression and classification tasks, all in the realm of autonomous navigation. We note a 10% improvement in the mAP score over state-of-the-art for tactical driver behavior classification in HDD dataset and a 20% drop in overall Mean squared error for steering action regression on TORCS dataset.



### Automated Weed Detection in Aerial Imagery with Context
- **Arxiv ID**: http://arxiv.org/abs/1910.00652v3
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00652v3)
- **Published**: 2019-10-01 20:25:55+00:00
- **Updated**: 2019-11-19 19:34:08+00:00
- **Authors**: Delia Bullock, Andrew Mangeni, Tyr Wiesner-Hanks, Chad DeChant, Ethan L. Stewart, Nicholas Kaczmar, Judith M. Kolkman, Rebecca J. Nelson, Michael A. Gore, Hod Lipson
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we demonstrate the ability to discriminate between cultivated maize plant and grass or grass-like weed image segments using the context surrounding the image segments. While convolutional neural networks have brought state of the art accuracies within object detection, errors arise when objects in different classes share similar features. This scenario often occurs when objects in images are viewed at too small of a scale to discern distinct differences in features, causing images to be incorrectly classified or localized. To solve this problem, we will explore using context when classifying image segments. This technique involves feeding a convolutional neural network a central square image along with a border of its direct surroundings at train and test times. This means that although images are labelled at a smaller scale to preserve accurate localization, the network classifies the images and learns features that include the wider context. We demonstrate the benefits of this context technique in the object detection task through a case study of grass (foxtail) and grass-like (yellow nutsedge) weed detection in maize fields. In this standard situation, adding context alone nearly halved the error of the neural network from 7.1% to 4.3%. After only one epoch with context, the network also achieved a higher accuracy than the network without context did after 50 epochs. The benefits of using the context technique are likely to particularly evident in agricultural contexts in which parts (such as leaves) of several plants may appear similar when not taking into account the context in which those parts appear.



### A Computationally Efficient Pipeline Approach to Full Page Offline Handwritten Text Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.00663v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.00663v2)
- **Published**: 2019-10-01 20:46:03+00:00
- **Updated**: 2020-05-08 19:58:40+00:00
- **Authors**: Jonathan Chung, Thomas Delteil
- **Comment**: None
- **Journal**: None
- **Summary**: Offline handwriting recognition with deep neural networks is usually limited to words or lines due to large computational costs. In this paper, a less computationally expensive full page offline handwritten text recognition framework is introduced. This framework includes a pipeline that locates handwritten text with an object detection neural network and recognises the text within the detected regions using features extracted with a multi-scale convolutional neural network (CNN) fed into a bidirectional long short term memory (LSTM) network. This framework achieves comparable error rates to state of the art frameworks while using less memory and time. The results in this paper demonstrate the potential of this framework and future work can investigate production ready and deployable handwritten text recognisers.



### Learning to estimate label uncertainty for automatic radiology report parsing
- **Arxiv ID**: http://arxiv.org/abs/1910.00673v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00673v1)
- **Published**: 2019-10-01 21:20:33+00:00
- **Updated**: 2019-10-01 21:20:33+00:00
- **Authors**: Tobi Olatunji, Li Yao
- **Comment**: Med-NeurIPS-2019 (accepted)
- **Journal**: None
- **Summary**: Bootstrapping labels from radiology reports has become the scalable alternative to provide inexpensive ground truth for medical imaging. Because of the domain specific nature, state-of-the-art report labeling tools are predominantly rule-based. These tools, however, typically yield a binary 0 or 1 prediction that indicates the presence or absence of abnormalities. These hard targets are then used as ground truth to train image models in the downstream, forcing models to express high degree of certainty even on cases where specificity is low. This could negatively impact the statistical efficiency of image models. We address such an issue by training a Bidirectional Long-Short Term Memory Network to augment heuristic-based discrete labels of X-ray reports from all body regions and achieve performance comparable or better than domain-specific NLP, but with additional uncertainty estimates which enable finer downstream image model training.



### TagSLAM: Robust SLAM with Fiducial Markers
- **Arxiv ID**: http://arxiv.org/abs/1910.00679v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00679v1)
- **Published**: 2019-10-01 21:28:03+00:00
- **Updated**: 2019-10-01 21:28:03+00:00
- **Authors**: Bernd Pfrommer, Kostas Daniilidis
- **Comment**: None
- **Journal**: None
- **Summary**: TagSLAM provides a convenient, flexible, and robust way of performing Simultaneous Localization and Mapping (SLAM) with AprilTag fiducial markers. By leveraging a few simple abstractions (bodies, tags, cameras), TagSLAM provides a front end to the GTSAM factor graph optimizer that makes it possible to rapidly design a range of experiments that are based on tags: full SLAM, extrinsic camera calibration with non-overlapping views, visual localization for ground truth, loop closure for odometry, pose estimation etc. We discuss in detail how TagSLAM initializes the factor graph in a robust way, and present loop closure as an application example. TagSLAM is a ROS based open source package and can be found at https://berndpfrommer.github.io/tagslam_web.



### RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.00694v1
- **DOI**: 10.1109/ICCVW.2019.00568
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00694v1)
- **Published**: 2019-10-01 22:06:49+00:00
- **Updated**: 2019-10-01 22:06:49+00:00
- **Authors**: Aayush K. Chaudhary, Rakshit Kothari, Manoj Acharya, Shusil Dangi, Nitinraj Nair, Reynold Bailey, Christopher Kanan, Gabriel Diaz, Jeff B. Pelz
- **Comment**: This model is the winning submission for OpenEDS Semantic
  Segmentation Challenge for Eye images
  https://research.fb.com/programs/openeds-challenge/. To appear in ICCVW 2019.
  ("Pre-trained models and source code are available
  https://bitbucket.org/eye-ush/ritnet/.")
- **Journal**: None
- **Summary**: Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3\% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at $>$ 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available https://bitbucket.org/eye-ush/ritnet/.



### Distilling Effective Supervision from Severe Label Noise
- **Arxiv ID**: http://arxiv.org/abs/1910.00701v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.00701v5)
- **Published**: 2019-10-01 22:34:29+00:00
- **Updated**: 2020-06-12 23:58:13+00:00
- **Authors**: Zizhao Zhang, Han Zhang, Sercan O. Arik, Honglak Lee, Tomas Pfister
- **Comment**: CVPR2020
- **Journal**: None
- **Summary**: Collecting large-scale data with clean labels for supervised training of neural networks is practically challenging. Although noisy labels are usually cheap to acquire, existing methods suffer a lot from label noise. This paper targets at the challenge of robust training at high label noise regimes. The key insight to achieve this goal is to wisely leverage a small trusted set to estimate exemplar weights and pseudo labels for noisy data in order to reuse them for supervised training. We present a holistic framework to train deep neural networks in a way that is highly invulnerable to label noise. Our method sets the new state of the art on various types of label noise and achieves excellent performance on large-scale datasets with real-world label noise. For instance, on CIFAR100 with a $40\%$ uniform noise ratio and only 10 trusted labeled data per class, our method achieves $80.2{\pm}0.3\%$ classification accuracy, where the error rate is only $1.4\%$ higher than a neural network trained without label noise. Moreover, increasing the noise ratio to $80\%$, our method still maintains a high accuracy of $75.5{\pm}0.2\%$, compared to the previous best accuracy $48.2\%$.   Source code available: https://github.com/google-research/google-research/tree/master/ieg



### Adaptive Continuous Visual Odometry from RGB-D Images
- **Arxiv ID**: http://arxiv.org/abs/1910.00713v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG, 53B21, 46C05, 46C07, 68T40, 68T45, 93C85, I.2.9; I.2.10; I.4.10; G.1.6; G.4
- **Links**: [PDF](http://arxiv.org/pdf/1910.00713v1)
- **Published**: 2019-10-01 23:29:16+00:00
- **Updated**: 2019-10-01 23:29:16+00:00
- **Authors**: Tzu-Yuan Lin, William Clark, Ryan M. Eustice, Jessy W. Grizzle, Anthony Bloch, Maani Ghaffari
- **Comment**: 7 pages
- **Journal**: None
- **Summary**: In this paper, we extend the recently developed continuous visual odometry framework for RGB-D cameras to an adaptive framework via online hyperparameter learning. We focus on the case of isotropic kernels with a scalar as the length-scale. In practice and as expected, the length-scale has remarkable impacts on the performance of the original framework. Previously it was handled using a fixed set of conditions within the solver to reduce the length-scale as the algorithm reaches a local minimum. We automate this process by a greedy gradient descent step at each iteration to find the next-best length-scale. Furthermore, to handle failure cases in the gradient descent step where the gradient is not well-behaved, such as the absence of structure or texture in the scene, we use a search interval for the length-scale and guide it gradually toward the smaller values. This latter strategy reverts the adaptive framework to the original setup. The experimental evaluations using publicly available RGB-D benchmarks show the proposed adaptive continuous visual odometry outperforms the original framework and the current state-of-the-art. We also make the software for the developed algorithm publicly available.



### Action Anticipation for Collaborative Environments: The Impact of Contextual Information and Uncertainty-Based Prediction
- **Arxiv ID**: http://arxiv.org/abs/1910.00714v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.00714v2)
- **Published**: 2019-10-01 23:30:08+00:00
- **Updated**: 2020-06-18 06:17:03+00:00
- **Authors**: Clebeson Canuto, Plinio Moreno, Jorge Samatelo, Raquel Vassallo, José Santos-Victor
- **Comment**: 27 pages, 16 figures, Neurocomputing
- **Journal**: None
- **Summary**: To interact with humans in collaborative environments, machines need to be able to predict (i.e., anticipate) future events, and execute actions in a timely manner. However, the observation of the human limb movements may not be sufficient to anticipate their actions unambiguously. In this work, we consider two additional sources of information (i.e., context) over time, gaze, movement and object information, and study how these additional contextual cues improve the action anticipation performance. We address action anticipation as a classification task, where the model takes the available information as the input and predicts the most likely action. We propose to use the uncertainty about each prediction as an online decision-making criterion for action anticipation. Uncertainty is modeled as a stochastic process applied to a time-based neural network architecture, which improves the conventional class-likelihood (i.e., deterministic) criterion. The main contributions of this paper are four-fold: (i) We propose a novel and effective decision-making criterion that can be used to anticipate actions even in situations of high ambiguity; (ii) we propose a deep architecture that outperforms previous results in the action anticipation task when using the Acticipate collaborative dataset; (iii) we show that contextual information is important to disambiguate the interpretation of similar actions; and (iv) we also provide a formal description of three existing performance metrics that can be easily used to evaluate action anticipation models.Our results on the Acticipate dataset showed the importance of contextual information and the uncertainty criterion for action anticipation. We achieve an average accuracy of 98.75% in the anticipation task using only an average of 25% of observations.



