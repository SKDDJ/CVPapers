# Arxiv Papers in cs.CV on 2019-10-13
### Optic-Net: A Novel Convolutional Neural Network for Diagnosis of Retinal Diseases from Optical Tomography Images
- **Arxiv ID**: http://arxiv.org/abs/1910.05672v1
- **DOI**: 10.1109/ICMLA.2019.00165
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05672v1)
- **Published**: 2019-10-13 03:02:35+00:00
- **Updated**: 2019-10-13 03:02:35+00:00
- **Authors**: Sharif Amit Kamran, Sourajit Saha, Ali Shihab Sabbir, Alireza Tavakkoli
- **Comment**: 8 pages. Accepted to 18th IEEE International Conference on Machine
  Learning and Applications (ICMLA 2019)
- **Journal**: None
- **Summary**: Diagnosing different retinal diseases from Spectral Domain Optical Coherence Tomography (SD-OCT) images is a challenging task. Different automated approaches such as image processing, machine learning and deep learning algorithms have been used for early detection and diagnosis of retinal diseases. Unfortunately, these are prone to error and computational inefficiency, which requires further intervention from human experts. In this paper, we propose a novel convolution neural network architecture to successfully distinguish between different degeneration of retinal layers and their underlying causes. The proposed novel architecture outperforms other classification models while addressing the issue of gradient explosion. Our approach reaches near perfect accuracy of 99.8% and 100% for two separately available Retinal SD-OCT data-set respectively. Additionally, our architecture predicts retinal diseases in real time while outperforming human diagnosticians.



### An Image Segmentation Model Based on a Variational Formulation
- **Arxiv ID**: http://arxiv.org/abs/1910.05678v1
- **DOI**: None
- **Categories**: **math.AP**, cs.CV, eess.IV, 53C44 35K55 49M25
- **Links**: [PDF](http://arxiv.org/pdf/1910.05678v1)
- **Published**: 2019-10-13 03:45:33+00:00
- **Updated**: 2019-10-13 03:45:33+00:00
- **Authors**: Carlos M. Paniagua Mejia
- **Comment**: 16 pages, 11 figures
- **Journal**: None
- **Summary**: Starting from a variational formulation, we present a model for image segmentation that employs both region statistics and edge information. This combination allows for improved flexibility, making the proposed model suitable to process a wider class of images than purely region-based and edge-based models. We perform several simulations with real images that attest to the versatility of the model. We also show another set of experiments on images with certain pathologies that suggest opportunities for improvement.



### eCNN: A Block-Based and Highly-Parallel CNN Accelerator for Edge Inference
- **Arxiv ID**: http://arxiv.org/abs/1910.05680v1
- **DOI**: 10.1145/3352460.3358263
- **Categories**: **cs.DC**, cs.CV, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05680v1)
- **Published**: 2019-10-13 03:54:25+00:00
- **Updated**: 2019-10-13 03:54:25+00:00
- **Authors**: Chao-Tsung Huang, Yu-Chun Ding, Huan-Ching Wang, Chi-Wen Weng, Kai-Ping Lin, Li-Wei Wang, Li-De Chen
- **Comment**: 14 pages; appearing in IEEE/ACM International Symposium on
  Microarchitecture (MICRO), 2019
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) have recently demonstrated superior quality for computational imaging applications. Therefore, they have great potential to revolutionize the image pipelines on cameras and displays. However, it is difficult for conventional CNN accelerators to support ultra-high-resolution videos at the edge due to their considerable DRAM bandwidth and power consumption. Therefore, finding a further memory- and computation-efficient microarchitecture is crucial to speed up this coming revolution.   In this paper, we approach this goal by considering the inference flow, network model, instruction set, and processor design jointly to optimize hardware performance and image quality. We apply a block-based inference flow which can eliminate all the DRAM bandwidth for feature maps and accordingly propose a hardware-oriented network model, ERNet, to optimize image quality based on hardware constraints. Then we devise a coarse-grained instruction set architecture, FBISA, to support power-hungry convolution by massive parallelism. Finally,we implement an embedded processor---eCNN---which accommodates to ERNet and FBISA with a flexible processing architecture. Layout results show that it can support high-quality ERNets for super-resolution and denoising at up to 4K Ultra-HD 30 fps while using only DDR-400 and consuming 6.94W on average. By comparison, the state-of-the-art Diffy uses dual-channel DDR3-2133 and consumes 54.3W to support lower-quality VDSR at Full HD 30 fps. Lastly, we will also present application examples of high-performance style transfer and object recognition to demonstrate the flexibility of eCNN.



### Radiomic Feature Stability Analysis based on Probabilistic Segmentations
- **Arxiv ID**: http://arxiv.org/abs/1910.05693v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05693v2)
- **Published**: 2019-10-13 06:01:18+00:00
- **Updated**: 2020-01-21 14:39:11+00:00
- **Authors**: Christoph Haarburger, Justus Schock, Daniel Truhn, Philippe Weitz, Gustav Mueller-Franzes, Leon Weninger, Dorit Merhof
- **Comment**: accepted at ISBI 2020
- **Journal**: None
- **Summary**: Identifying image features that are robust with respect to segmentation variability and domain shift is a tough challenge in radiomics. So far, this problem has mainly been tackled in test-retest analyses. In this work we analyze radiomics feature stability based on probabilistic segmentations. Based on a public lung cancer dataset, we generate an arbitrary number of plausible segmentations using a Probabilistic U-Net. From these segmentations, we extract a high number of plausible feature vectors for each lung tumor and analyze feature variance with respect to the segmentations. Our results suggest that there are groups of radiomic features that are more (e.g. statistics features) and less (e.g. gray-level size zone matrix features) robust against segmentation variability. Finally, we demonstrate that segmentation variance impacts the performance of a prognostic lung cancer survival model and propose a new and potentially more robust radiomics feature selection workflow.



### A Novel Self-Supervised Re-labeling Approach for Training with Noisy Labels
- **Arxiv ID**: http://arxiv.org/abs/1910.05700v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05700v3)
- **Published**: 2019-10-13 06:50:36+00:00
- **Updated**: 2020-01-01 07:36:42+00:00
- **Authors**: Devraj Mandal, Shrisha Bharadwaj, Soma Biswas
- **Comment**: Expanded and Updated version of this paper has been accepted in WACV
  2020
- **Journal**: None
- **Summary**: The major driving force behind the immense success of deep learning models is the availability of large datasets along with their clean labels. Unfortunately, this is very difficult to obtain, which has motivated research on the training of deep models in the presence of label noise and ways to avoid over-fitting on the noisy labels. In this work, we build upon the seminal work in this area, Co-teaching and propose a simple, yet efficient approach termed mCT-S2R (modified co-teaching with self-supervision and re-labeling) for this task. First, to deal with significant amount of noise in the labels, we propose to use self-supervision to generate robust features without using any labels. Next, using a parallel network architecture, an estimate of the clean labeled portion of the data is obtained. Finally, using this data, a portion of the estimated noisy labeled portion is re-labeled, before resuming the network training with the augmented data. Extensive experiments on three standard datasets show the effectiveness of the proposed framework.



### Contour Sparse Representation with SDD Features for Object Recognition
- **Arxiv ID**: http://arxiv.org/abs/1910.05704v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05704v2)
- **Published**: 2019-10-13 07:13:19+00:00
- **Updated**: 2020-09-03 07:23:13+00:00
- **Authors**: Zhenzhou Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Slope difference distribution (SDD) is computed for the one-dimensional curve. It is not only robust to calculate the partitioning point to separate the curve logically, but also robust to calculate the clustering center of each part of the separated curve. SDD has been proposed for image segmentation and it outperforms all existing image segmentation methods. For verification purpose, we have made the Matlab codes of comparing SDD method with existing image segmentation methods freely available at Matlab Central. The contour of the object is similar to the histogram of the image. Thus, feature detection by SDD from the contour of the object is also feasible. In this letter, SDD features are defined and they form the sparse representation of the object contour. The reference model of each object is built based on the SDD features and then model matching is used for on line object recognition. The experimental results are very encouraging. For the gesture recognition, SDD achieved 100% accuracy for two public datasets: the NUS dataset and the near-infrared dataset. For the object recognition, SDD achieved 100% accuracy for the Kimia 99 dataset.



### Granular Multimodal Attention Networks for Visual Dialog
- **Arxiv ID**: http://arxiv.org/abs/1910.05728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.05728v1)
- **Published**: 2019-10-13 10:49:41+00:00
- **Updated**: 2019-10-13 10:49:41+00:00
- **Authors**: Badri N. Patro, Shivansh Patel, Vinay P. Namboodiri
- **Comment**: ICCV Workshop
- **Journal**: None
- **Summary**: Vision and language tasks have benefited from attention. There have been a number of different attention models proposed. However, the scale at which attention needs to be applied has not been well examined. Particularly, in this work, we propose a new method Granular Multi-modal Attention, where we aim to particularly address the question of the right granularity at which one needs to attend while solving the Visual Dialog task. The proposed method shows improvement in both image and text attention networks. We then propose a granular Multi-modal Attention network that jointly attends on the image and text granules and shows the best performance. With this work, we observe that obtaining granular attention and doing exhaustive Multi-modal Attention appears to be the best way to attend while solving visual dialog.



### One-Shot Neural Architecture Search via Self-Evaluated Template Network
- **Arxiv ID**: http://arxiv.org/abs/1910.05733v4
- **DOI**: 10.1109/ICCV.2019.00378
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05733v4)
- **Published**: 2019-10-13 11:25:40+00:00
- **Updated**: 2021-01-25 13:32:40+00:00
- **Authors**: Xuanyi Dong, Yi Yang
- **Comment**: Minor modifications to the ICCV 2019 camera-ready version (add code
  link)
- **Journal**: None
- **Summary**: Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components: (1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs. Code is publicly available on GitHub: https://github.com/D-X-Y/AutoDL-Projects.



### Hierarchical Feature-Aware Tracking
- **Arxiv ID**: http://arxiv.org/abs/1910.05751v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05751v2)
- **Published**: 2019-10-13 13:51:44+00:00
- **Updated**: 2019-10-18 14:09:12+00:00
- **Authors**: Wenhua Zhang, Licheng Jiao, Jia Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a hierarchical feature-aware tracking framework for efficient visual tracking. Recent years, ensembled trackers which combine multiple component trackers have achieved impressive performance. In ensembled trackers, the decision of results is usually a post-event process, i.e., tracking result for each tracker is first obtained and then the suitable one is selected according to result ensemble. In this paper, we propose a pre-event method. We construct an expert pool with each expert being one set of features. For each frame, several experts are first selected in the pool according to their past performance and then they are used to predict the object. The selection rate of each expert in the pool is then updated and tracking result is obtained according to result ensemble. We propose a novel pre-known expert-adaptive selection strategy. Since the process is more efficient, more experts can be constructed by fusing more types of features which leads to more robustness. Moreover, with the novel expert selection strategy, overfitting caused by fixed experts for each frame can be mitigated. Experiments on several public available datasets demonstrate the superiority of the proposed method and its state-of-the-art performance among ensembled trackers.



### Learning to Navigate from Simulation via Spatial and Semantic Information Synthesis with Noise Model Embedding
- **Arxiv ID**: http://arxiv.org/abs/1910.05758v3
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05758v3)
- **Published**: 2019-10-13 14:13:39+00:00
- **Updated**: 2019-11-12 02:30:28+00:00
- **Authors**: Gang Chen, Hongzhe Yu, Wei Dong, Xinjun Sheng, Xiangyang Zhu, Han Ding
- **Comment**: 10 pages, 11 figures
- **Journal**: None
- **Summary**: While training an end-to-end navigation network in the real world is usually of high cost, simulation provides a safe and cheap environment in this training stage. However, training neural network models in simulation brings up the problem of how to effectively transfer the model from simulation to the real world (sim-to-real). In this work, we regard the environment representation as a crucial element in this transfer process and propose a visual information pyramid (VIP) model to systematically investigate a practical environment representation. A novel representation composed of spatial and semantic information synthesis is then established accordingly, where noise model embedding is particularly considered. To explore the effectiveness of this representation, we compared the performance with representations popularly used in the literature in both simulated and real-world scenarios. Results suggest that our environment representation stands out. Furthermore, an analysis on the feature map is implemented to investigate the effectiveness through inner reaction, which could be irradiative for future researches on end-to-end navigation.



### A CNN-RNN Framework for Image Annotation from Visual Cues and Social Network Metadata
- **Arxiv ID**: http://arxiv.org/abs/1910.05770v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/1910.05770v2)
- **Published**: 2019-10-13 15:24:48+00:00
- **Updated**: 2020-03-30 17:19:26+00:00
- **Authors**: Tobia Tesan, Pasquale Coscia, Lamberto Ballan
- **Comment**: None
- **Journal**: None
- **Summary**: Images represent a commonly used form of visual communication among people. Nevertheless, image classification may be a challenging task when dealing with unclear or non-common images needing more context to be correctly annotated. Metadata accompanying images on social-media represent an ideal source of additional information for retrieving proper neighborhoods easing image annotation task. To this end, we blend visual features extracted from neighbors and their metadata to jointly leverage context and visual cues. Our models use multiple semantic embeddings to achieve the dual objective of being robust to vocabulary changes between train and test sets and decoupling the architecture from the low-level metadata representation. Convolutional and recurrent neural networks (CNNs-RNNs) are jointly adopted to infer similarity among neighbors and query images. We perform comprehensive experiments on the NUS-WIDE dataset showing that our models outperform state-of-the-art architectures based on images and metadata, and decrease both sensory and semantic gaps to better annotate images.



### Image Generation and Recognition (Emotions)
- **Arxiv ID**: http://arxiv.org/abs/1910.05774v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05774v2)
- **Published**: 2019-10-13 16:00:06+00:00
- **Updated**: 2019-12-13 23:10:05+00:00
- **Authors**: Hanne Carlsson, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Generative Adversarial Networks (GANs) were proposed in 2014 by Goodfellow et al., and have since been extended into multiple computer vision applications. This report provides a thorough survey of recent GAN research, outlining the various architectures and applications, as well as methods for training GANs and dealing with latent space. This is followed by a discussion of potential areas for future GAN research, including: evaluating GANs, better understanding GANs, and techniques for training GANs. The second part of this report outlines the compilation of a dataset of images `in the wild' representing each of the 7 basic human emotions, and analyses experiments done when training a StarGAN on this dataset combined with the FER2013 dataset.



### Interpretable Deep Neural Networks for Dimensional and Categorical Emotion Recognition in-the-wild
- **Arxiv ID**: http://arxiv.org/abs/1910.05784v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.05784v2)
- **Published**: 2019-10-13 16:33:18+00:00
- **Updated**: 2019-12-13 23:25:57+00:00
- **Authors**: Xia Yicheng, Dimitrios Kollias
- **Comment**: None
- **Journal**: None
- **Summary**: Emotions play an important role in people's life. Understanding and recognising is not only important for interpersonal communication, but also has promising applications in Human-Computer Interaction, automobile safety and medical research. This project focuses on extending the emotion recognition database, and training the CNN + RNN emotion recognition neural networks with emotion category representation and valence \& arousal representation. The combined models are constructed by training the two representations simultaneously. The comparison and analysis between the three types of model are discussed. The inner-relationship between two emotion representations and the interpretability of the neural networks are investigated. The findings suggest that categorical emotion recognition performance can benefit from training with a combined model. And the mapping of emotion category and valence \& arousal values can explain this phenomenon.



### Deep Crowd-Flow Prediction in Built Environments
- **Arxiv ID**: http://arxiv.org/abs/1910.05810v1
- **DOI**: None
- **Categories**: **cs.AI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05810v1)
- **Published**: 2019-10-13 18:43:55+00:00
- **Updated**: 2019-10-13 18:43:55+00:00
- **Authors**: Samuel S. Sohn, Seonghyeon Moon, Honglu Zhou, Sejong Yoon, Vladimir Pavlovic, Mubbasir Kapadia
- **Comment**: None
- **Journal**: None
- **Summary**: Predicting the behavior of crowds in complex environments is a key requirement in a multitude of application areas, including crowd and disaster management, architectural design, and urban planning. Given a crowd's immediate state, current approaches simulate crowd movement to arrive at a future state. However, most applications require the ability to predict hundreds of possible simulation outcomes (e.g., under different environment and crowd situations) at real-time rates, for which these approaches are prohibitively expensive.   In this paper, we propose an approach to instantly predict the long-term flow of crowds in arbitrarily large, realistic environments. Central to our approach is a novel CAGE representation consisting of Capacity, Agent, Goal, and Environment-oriented information, which efficiently encodes and decodes crowd scenarios into compact, fixed-size representations that are environmentally lossless. We present a framework to facilitate the accurate and efficient prediction of crowd flow in never-before-seen crowd scenarios. We conduct a series of experiments to evaluate the efficacy of our approach and showcase positive results.



### Generative Image Translation for Data Augmentation in Colorectal Histopathology Images
- **Arxiv ID**: http://arxiv.org/abs/1910.05827v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.05827v1)
- **Published**: 2019-10-13 20:43:53+00:00
- **Updated**: 2019-10-13 20:43:53+00:00
- **Authors**: Jerry Wei, Arief Suriawinata, Louis Vaickus, Bing Ren, Xiaoying Liu, Jason Wei, Saeed Hassanpour
- **Comment**: NeurIPS 2019 Machine Learning for Health Workshop Full Paper (19/111
  accepted papers = 17% acceptance rate)
- **Journal**: None
- **Summary**: We present an image translation approach to generate augmented data for mitigating data imbalances in a dataset of histopathology images of colorectal polyps, adenomatous tumors that can lead to colorectal cancer if left untreated. By applying cycle-consistent generative adversarial networks (CycleGANs) to a source domain of normal colonic mucosa images, we generate synthetic colorectal polyp images that belong to diagnostically less common polyp classes. Generated images maintain the general structure of their source image but exhibit adenomatous features that can be enhanced with our proposed filtration module, called Path-Rank-Filter. We evaluate the quality of generated images through Turing tests with four gastrointestinal pathologists, finding that at least two of the four pathologists could not identify generated images at a statistically significant level. Finally, we demonstrate that using CycleGAN-generated images to augment training data improves the AUC of a convolutional neural network for detecting sessile serrated adenomas by over 10%, suggesting that our approach might warrant further research for other histopathology image classification tasks.



### DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network
- **Arxiv ID**: http://arxiv.org/abs/1910.11088v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.11088v2)
- **Published**: 2019-10-13 20:59:12+00:00
- **Updated**: 2020-03-05 01:43:20+00:00
- **Authors**: Wei Wang, Muhamad Risqi U. Saputra, Peijun Zhao, Pedro Gusmao, Bo Yang, Changhao Chen, Andrew Markham, Niki Trigoni
- **Comment**: To appear in IROS 2019
- **Journal**: None
- **Summary**: Odometry is of key importance for localization in the absence of a map. There is considerable work in the area of visual odometry (VO), and recent advances in deep learning have brought novel approaches to VO, which directly learn salient features from raw images. These learning-based approaches have led to more accurate and robust VO systems. However, they have not been well applied to point cloud data yet. In this work, we investigate how to exploit deep learning to estimate point cloud odometry (PCO), which may serve as a critical component in point cloud-based downstream tasks or learning-based systems. Specifically, we propose a novel end-to-end deep parallel neural network called DeepPCO, which can estimate the 6-DOF poses using consecutive point clouds. It consists of two parallel sub-networks to estimate 3-D translation and orientation respectively rather than a single neural network. We validate our approach on KITTI Visual Odometry/SLAM benchmark dataset with different baselines. Experiments demonstrate that the proposed approach achieves good performance in terms of pose accuracy.



### RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment
- **Arxiv ID**: http://arxiv.org/abs/1910.05839v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.05839v2)
- **Published**: 2019-10-13 21:38:43+00:00
- **Updated**: 2019-10-28 20:29:17+00:00
- **Authors**: Guan'an Wang, Tianzhu Zhang, Jian Cheng, Si Liu, Yang Yang, Zengguang Hou
- **Comment**: accepted by ICCV'19
- **Journal**: None
- **Summary**: RGB-Infrared (IR) person re-identification is an important and challenging task due to large cross-modality variations between RGB and IR images. Most conventional approaches aim to bridge the cross-modality gap with feature alignment by feature representation learning. Different from existing methods, in this paper, we propose a novel and end-to-end Alignment Generative Adversarial Network (AlignGAN) for the RGB-IR RE-ID task. The proposed model enjoys several merits. First, it can exploit pixel alignment and feature alignment jointly. To the best of our knowledge, this is the first work to model the two alignment strategies jointly for the RGB-IR RE-ID problem. Second, the proposed model consists of a pixel generator, a feature generator, and a joint discriminator. By playing a min-max game among the three components, our model is able to not only alleviate the cross-modality and intra-modality variations but also learn identity-consistent features. Extensive experimental results on two standard benchmarks demonstrate that the proposed model performs favorably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve an absolute gain of 15.4% and 12.9% in terms of Rank-1 and mAP.



