# Arxiv Papers in cs.CV on 2019-10-28
### Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method
- **Arxiv ID**: http://arxiv.org/abs/1910.12384v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12384v1)
- **Published**: 2019-10-28 00:02:33+00:00
- **Updated**: 2019-10-28 00:02:33+00:00
- **Authors**: Vishwanath A. Sindagi, Rajeev Yasarla, Vishal M. Patel
- **Comment**: Accepted at ICCV 2019
- **Journal**: None
- **Summary**: In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors.   Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is ~2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.



### Towards calibrated and scalable uncertainty representations for neural networks
- **Arxiv ID**: http://arxiv.org/abs/1911.00104v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.00104v3)
- **Published**: 2019-10-28 02:29:55+00:00
- **Updated**: 2019-12-04 02:19:35+00:00
- **Authors**: Nabeel Seedat, Christopher Kanan
- **Comment**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019): 4th workshop on Bayesian Deep Learning, Vancouver, Canada
- **Journal**: None
- **Summary**: For many applications it is critical to know the uncertainty of a neural network's predictions. While a variety of neural network parameter estimation methods have been proposed for uncertainty estimation, they have not been rigorously compared across uncertainty measures. We assess four of these parameter estimation methods to calibrate uncertainty estimation using four different uncertainty measures: entropy, mutual information, aleatoric uncertainty and epistemic uncertainty. We evaluate the calibration of these parameter estimation methods using expected calibration error. Additionally, we propose a novel method of neural network parameter estimation called RECAST, which combines cosine annealing with warm restarts with Stochastic Gradient Langevin Dynamics, capturing more diverse parameter distributions. When benchmarked against mutilated image data, we show that RECAST is well-calibrated and when combined with predictive entropy and epistemic uncertainty it offers the best calibrated measure of uncertainty when compared to recent methods.



### Towards Good Practices for Multi-Person Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/1911.07938v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07938v1)
- **Published**: 2019-10-28 03:00:28+00:00
- **Updated**: 2019-10-28 03:00:28+00:00
- **Authors**: Dongdong Yu, Kai Su, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-Person Pose Estimation is an interesting yet challenging task in computer vision. In this paper, we conduct a series of refinements with the MSPN and PoseFix Networks, and empirically evaluate their impact on the final model performance through ablation studies. By taking all the refinements, we achieve 78.7 on the COCO test-dev dataset and 76.3 on the COCO test-challenge dataset.



### Towards Good Practices for Instance Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1911.07939v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1911.07939v1)
- **Published**: 2019-10-28 03:03:48+00:00
- **Updated**: 2019-10-28 03:03:48+00:00
- **Authors**: Dongdong Yu, Zehuan Yuan, Jinlai Liu, Kun Yuan, Changhu Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Instance Segmentation is an interesting yet challenging task in computer vision. In this paper, we conduct a series of refinements with the Hybrid Task Cascade (HTC) Network, and empirically evaluate their impact on the final model performance through ablation studies. By taking all the refinements, we achieve 0.47 on the COCO test-dev dataset and 0.47 on the COCO test-challenge dataset.



### Geometry-Aware Neural Rendering
- **Arxiv ID**: http://arxiv.org/abs/1911.04554v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1911.04554v1)
- **Published**: 2019-10-28 03:10:39+00:00
- **Updated**: 2019-10-28 03:10:39+00:00
- **Authors**: Josh Tobin, OpenAI Robotics, Pieter Abbeel
- **Comment**: 16 pages, 13 figures
- **Journal**: 33rd Conference on Neural Information Processing Systems (NeurIPS
  2019), Vancouver, Canada
- **Summary**: Understanding the 3-dimensional structure of the world is a core challenge in computer vision and robotics. Neural rendering approaches learn an implicit 3D model by predicting what a camera would see from an arbitrary viewpoint. We extend existing neural rendering to more complex, higher dimensional scenes than previously possible. We propose Epipolar Cross Attention (ECA), an attention mechanism that leverages the geometry of the scene to perform efficient non-local operations, requiring only $O(n)$ comparisons per spatial dimension instead of $O(n^2)$. We introduce three new simulated datasets inspired by real-world robotics and demonstrate that ECA significantly improves the quantitative and qualitative performance of Generative Query Networks (GQN).



### ACE: Adaptive Confusion Energy for Natural World Data Distribution
- **Arxiv ID**: http://arxiv.org/abs/1910.12423v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12423v3)
- **Published**: 2019-10-28 03:23:56+00:00
- **Updated**: 2021-03-12 06:37:12+00:00
- **Authors**: Yen-Chi Hsu, Cheng-Yao Hong, Wan-Cyuan Fan, Ming-Sui Lee, Davi Geiger, Tyng-Luh Liu
- **Comment**: None
- **Journal**: None
- **Summary**: With the development of deep learning, standard classification problems have achieved good results. However, conventional classification problems are often too idealistic. Most data in the natural world usually have imbalanced distribution and fine-grained characteristics. Recently, many state-of-the-art approaches tend to focus on one or another separately, but rarely on both. In this paper, we introduce a novel and adaptive batch-wise regularization based on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature world distribution, which usually involves fine-grained and long-tailed properties at the same time. ACE increases the difficulty of the training process and further alleviates the overfitting problem. Through the datasets with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed (ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows that the ACE is not only competitive to some state-of-the-art on performance but also demonstrates the effectiveness of training.



### Applications of Generative Adversarial Models in Visual Search Reformulation
- **Arxiv ID**: http://arxiv.org/abs/1910.12460v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.IR, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12460v1)
- **Published**: 2019-10-28 06:27:38+00:00
- **Updated**: 2019-10-28 06:27:38+00:00
- **Authors**: Kyle Xiao, Houdong Hu, Yan Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Query reformulation is the process by which a input search query is refined by the user to match documents outside the original top-n results. On average, roughly 50% of text search queries involve some form of reformulation, and term suggestion tools are used 35% of the time when offered to users. As prevalent as text search queries are, however, such a feature has yet to be explored at scale for visual search. This is because reformulation for images presents a novel challenge to seamlessly transform visual features to match user intent within the context of a typical user session. In this paper, we present methods of semantically transforming visual queries, such as utilizing operations in the latent space of a generative adversarial model for the scenarios of fashion and product search.



### Neighborhood Watch: Representation Learning with Local-Margin Triplet Loss and Sampling Strategy for K-Nearest-Neighbor Image Classification
- **Arxiv ID**: http://arxiv.org/abs/1911.07940v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1911.07940v1)
- **Published**: 2019-10-28 06:35:01+00:00
- **Updated**: 2019-10-28 06:35:01+00:00
- **Authors**: Phawis Thammasorn, Daniel Hippe, Wanpracha Chaovalitwongse, Matthew Spraker, Landon Wootton, Matthew Nyflot, Stephanie Combs, Jan Peeken, Eric Ford
- **Comment**: Triplet Network, Representation Learning, Transfer Learning, Nearest
  Neighbor, Medical Image Classification
- **Journal**: None
- **Summary**: Deep representation learning using triplet network for classification suffers from a lack of theoretical foundation and difficulty in tuning both the network and classifiers for performance. To address the problem, local-margin triplet loss along with local positive and negative mining strategy is proposed with theory on how the strategy integrate nearest-neighbor hyper-parameter with triplet learning to increase subsequent classification performance. Results in experiments with 2 public datasets, MNIST and Cifar-10, and 2 small medical image datasets demonstrate that proposed strategy outperforms end-to-end softmax and typical triplet loss in settings without data augmentation while maintaining utility of transferable feature for related tasks. The method serves as a good performance baseline where end-to-end methods encounter difficulties such as small sample data with limited allowable data augmentation.



### Fine-Grained Object Detection over Scientific Document Images with Region Embeddings
- **Arxiv ID**: http://arxiv.org/abs/1910.12462v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12462v2)
- **Published**: 2019-10-28 06:39:02+00:00
- **Updated**: 2019-10-30 17:25:24+00:00
- **Authors**: Ankur Goswami, Joshua McGrath, Shanan Peters, Theodoros Rekatsinas
- **Comment**: None
- **Journal**: None
- **Summary**: We study the problem of object detection over scanned images of scientific documents. We consider images that contain objects of varying aspect ratios and sizes and range from coarse elements such as tables and figures to fine elements such as equations and section headers. We find that current object detectors fail to produce properly localized region proposals over such page objects. We revisit the original R-CNN model and present a method for generating fine-grained proposals over document elements. We also present a region embedding model that uses the convolutional maps of a proposal's neighbors as context to produce an embedding for each proposal. This region embedding is able to capture the semantic relationships between a target region and its surrounding context. Our end-to-end model produces an embedding for each proposal, then classifies each proposal by using a multi-head attention model that attends to the most important neighbors of a proposal. To evaluate our model, we collect and annotate a dataset of publications from heterogeneous journals. We show that our model, referred to as Attentive-RCNN, yields a 17% mAP improvement compared to standard object detection models.



### Use of a Capsule Network to Detect Fake Images and Videos
- **Arxiv ID**: http://arxiv.org/abs/1910.12467v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12467v2)
- **Published**: 2019-10-28 07:01:49+00:00
- **Updated**: 2019-10-29 14:30:58+00:00
- **Authors**: Huy H. Nguyen, Junichi Yamagishi, Isao Echizen
- **Comment**: Fixing Table 2's scale
- **Journal**: None
- **Summary**: The revolution in computer hardware, especially in graphics processing units and tensor processing units, has enabled significant advances in computer graphics and artificial intelligence algorithms. In addition to their many beneficial applications in daily life and business, computer-generated/manipulated images and videos can be used for malicious purposes that violate security systems, privacy, and social trust. The deepfake phenomenon and its variations enable a normal user to use his or her personal computer to easily create fake videos of anybody from a short real online video. Several countermeasures have been introduced to deal with attacks using such videos. However, most of them are targeted at certain domains and are ineffective when applied to other domains or new attacks. In this paper, we introduce a capsule network that can detect various kinds of attacks, from presentation attacks using printed images and replayed videos to attacks using fake videos created using deep learning. It uses many fewer parameters than traditional convolutional neural networks with similar performance. Moreover, we explain, for the first time ever in the literature, the theory behind the application of capsule networks to the forensics problem through detailed analysis and visualization.



### Image-Based Place Recognition on Bucolic Environment Across Seasons From Semantic Edge Description
- **Arxiv ID**: http://arxiv.org/abs/1910.12468v5
- **DOI**: 10.1109/ICRA40945.2020.9197529
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12468v5)
- **Published**: 2019-10-28 07:06:25+00:00
- **Updated**: 2021-04-01 08:10:33+00:00
- **Authors**: Assia Benbihi, Stéphanie Aravecchia, Matthieu Geist, Cédric Pradalier
- **Comment**: None
- **Journal**: 2020 IEEE International Conference on Robotics and Automation
  (ICRA)
- **Summary**: Most of the research effort on image-based place recognition is designed for urban environments. In bucolic environments such as natural scenes with low texture and little semantic content, the main challenge is to handle the variations in visual appearance across time such as illumination, weather, vegetation state or viewpoints. The nature of the variations is different and this leads to a different approach to describing a bucolic scene. We introduce a global image descriptor computed from its semantic and topological information. It is built from the wavelet transforms of the image semantic edges. Matching two images is then equivalent to matching their semantic edge descriptors. We show that this method reaches state-of-the-art image retrieval performance on two multi-season environment-monitoring datasets: the CMU-Seasons and the Symphony Lake dataset. It also generalises to urban scenes on which it is on par with the current baselines NetVLAD and DELF.



### Multi-sequence Cardiac MR Segmentation with Adversarial Domain Adaptation Network
- **Arxiv ID**: http://arxiv.org/abs/1910.12514v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12514v1)
- **Published**: 2019-10-28 09:20:23+00:00
- **Updated**: 2019-10-28 09:20:23+00:00
- **Authors**: Jiexiang Wang, Hongyu Huang, Chaoqi Chen, Wenao Ma, Yue Huang, Xinghao Ding
- **Comment**: 10th Workshop on Statistical Atlases and Computational Modelling of
  the Heart (MICCAI2019 Workshop)
- **Journal**: None
- **Summary**: Automatic and accurate segmentation of the ventricles and myocardium from multi-sequence cardiac MRI (CMR) is crucial for the diagnosis and treatment management for patients suffering from myocardial infarction (MI). However, due to the existence of domain shift among different modalities of datasets, the performance of deep neural networks drops significantly when the training and testing datasets are distinct. In this paper, we propose an unsupervised domain alignment method to explicitly alleviate the domain shifts among different modalities of CMR sequences, \emph{e.g.,} bSSFP, LGE, and T2-weighted. Our segmentation network is attention U-Net with pyramid pooling module, where multi-level feature space and output space adversarial learning are proposed to transfer discriminative domain knowledge across different datasets. Moreover, we further introduce a group-wise feature recalibration module to enforce the fine-grained semantic-level feature alignment that matching features from different networks but with the same class label. We evaluate our method on the multi-sequence cardiac MR Segmentation Challenge 2019 datasets, which contain three different modalities of MRI sequences. Extensive experimental results show that the proposed methods can obtain significant segmentation improvements compared with the baseline models.



### Virtual Piano using Computer Vision
- **Arxiv ID**: http://arxiv.org/abs/1910.12539v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12539v1)
- **Published**: 2019-10-28 10:36:30+00:00
- **Updated**: 2019-10-28 10:36:30+00:00
- **Authors**: Seongjae Kang, Jaeyoon Kim, Sung-eui Yoon
- **Comment**: None
- **Journal**: None
- **Summary**: In this research, Piano performances have been analyzed only based on visual information. Computer vision algorithms, e.g., Hough transform and binary thresholding, have been applied to find where the keyboard and specific keys are located. At the same time, Convolutional Neural Networks(CNNs) has been also utilized to find whether specific keys are pressed or not, and how much intensity the keys are pressed only based on visual information. Especially for detecting intensity, a new method of utilizing spatial, temporal CNNs model is devised. Early fusion technique is especially applied in temporal CNNs architecture to analyze hand movement. We also make a new dataset for training each model. Especially when finding an intensity of a pressed key, both of video frames and their optical flow images are used to train models to find effectiveness.



### Addressing the Sim2Real Gap in Robotic 3D Object Classification
- **Arxiv ID**: http://arxiv.org/abs/1910.12585v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.12585v1)
- **Published**: 2019-10-28 12:27:40+00:00
- **Updated**: 2019-10-28 12:27:40+00:00
- **Authors**: Jean-Baptiste Weibel, Timothy Patten, Markus Vincze
- **Comment**: None
- **Journal**: None
- **Summary**: Object classification with 3D data is an essential component of any scene understanding method. It has gained significant interest in a variety of communities, most notably in robotics and computer graphics. While the advent of deep learning has progressed the field of 3D object classification, most work using this data type are solely evaluated on CAD model datasets. Consequently, current work does not address the discrepancies existing between real and artificial data. In this work, we examine this gap in a robotic context by specifically addressing the problem of classification when transferring from artificial CAD models to real reconstructed objects. This is performed by training on ModelNet (CAD models) and evaluating on ScanNet (reconstructed objects). We show that standard methods do not perform well in this task. We thus introduce a method that carefully samples object parts that are reproducible under various transformations and hence robust. Using graph convolution to classify the composed graph of parts, our method significantly improves upon the baseline.



### FontGAN: A Unified Generative Framework for Chinese Character Stylization and De-stylization
- **Arxiv ID**: http://arxiv.org/abs/1910.12604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12604v1)
- **Published**: 2019-10-28 12:38:39+00:00
- **Updated**: 2019-10-28 12:38:39+00:00
- **Authors**: Xiyan Liu, Gaofeng Meng, Shiming Xiang, Chunhong Pan
- **Comment**: None
- **Journal**: None
- **Summary**: Chinese character synthesis involves two related aspects, i.e., style maintenance and content consistency. Although some methods have achieved remarkable success in synthesizing a character with specified style from standard font, how to map characters to a specified style domain without losing their identifiability remains very challenging. In this paper, we propose a novel model named FontGAN, which integrates the character stylization and de-stylization into a unified framework. In our model, we decouple character images into style representation and content representation, which facilitates more precise control of these two types of variables, thereby improving the quality of the generated results. We also introduce two modules, namely, font consistency module (FCM) and content prior module (CPM). FCM exploits a category guided Kullback-Leibler loss to embedding the style representation into different Gaussian distributions. It constrains the characters of the same font in the training set globally. On the other hand, it enables our model to obtain style variables through sampling in testing phase. CPM provides content prior for the model to guide the content encoding process and alleviates the problem of stroke deficiency during de-stylization. Extensive experimental results on character stylization and de-stylization have demonstrated the effectiveness of our method.



### Self-supervised learning of class embeddings from video
- **Arxiv ID**: http://arxiv.org/abs/1910.12699v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12699v1)
- **Published**: 2019-10-28 14:18:17+00:00
- **Updated**: 2019-10-28 14:18:17+00:00
- **Authors**: Olivia Wiles, A. Sophia Koepke, Andrew Zisserman
- **Comment**: 4th International Workshop on Compact and Efficient Feature
  Representation and Learning in Computer Vision 2019
- **Journal**: None
- **Summary**: This work explores how to use self-supervised learning on videos to learn a class-specific image embedding that encodes pose and shape information. At train time, two frames of the same video of an object class (e.g. human upper body) are extracted and each encoded to an embedding. Conditioned on these embeddings, the decoder network is tasked to transform one frame into another. To successfully perform long range transformations (e.g. a wrist lowered in one image should be mapped to the same wrist raised in another), we introduce a hierarchical probabilistic network decoder model. Once trained, the embedding can be used for a variety of downstream tasks and domains. We demonstrate our approach quantitatively on three distinct deformable object classes -- human full bodies, upper bodies, faces -- and show experimentally that the learned embeddings do indeed generalise. They achieve state-of-the-art performance in comparison to other self-supervised methods trained on the same datasets, and approach the performance of fully supervised methods.



### Multivariate mathematical morphology for DCE-MRI image analysis in angiogenesis studies
- **Arxiv ID**: http://arxiv.org/abs/1910.12704v1
- **DOI**: 10.5566/ias.1109
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12704v1)
- **Published**: 2019-10-28 14:26:49+00:00
- **Updated**: 2019-10-28 14:26:49+00:00
- **Authors**: Guillaume Noyel, Jesus Angulo, Dominique Jeulin, Daniel Balvay, Charles-André Cuenod
- **Comment**: None
- **Journal**: Image Analysis and Stereology, International Society for
  Stereology, 2015, 34 (1), pp.1-25
- **Summary**: We propose a new computer aided detection framework for tumours acquired on DCE-MRI (Dynamic Contrast Enhanced Magnetic Resonance Imaging) series on small animals. In this approach we consider DCE-MRI series as multivariate images. A full multivariate segmentation method based on dimensionality reduction, noise filtering, supervised classification and stochastic watershed is explained and tested on several data sets. The two main key-points introduced in this paper are noise reduction preserving contours and spatio temporal segmentation by stochastic watershed. Noise reduction is performed in a special way that selects factorial axes of Factor Correspondence Analysis in order to preserves contours. Then a spatio-temporal approach based on stochastic watershed is used to segment tumours. The results obtained are in accordance with the diagnosis of the medical doctors.



### Outlining where humans live -- The World Settlement Footprint 2015
- **Arxiv ID**: http://arxiv.org/abs/1910.12707v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12707v1)
- **Published**: 2019-10-28 14:29:10+00:00
- **Updated**: 2019-10-28 14:29:10+00:00
- **Authors**: Mattia Marconcini, Annekatrin Metz-Marconcini, Soner Üreyen, Daniela Palacios-Lopez, Wiebke Hanke, Felix Bachofer, Julian Zeidler, Thomas Esch, Noel Gorelick, Ashwin Kakarla, Emanuele Strano
- **Comment**: None
- **Journal**: None
- **Summary**: Human settlements are the cause and consequence of most environmental and societal changes on Earth; however, their location and extent is still under debate. We provide here a new 10m resolution (0.32 arc sec) global map of human settlements on Earth for the year 2015, namely the World Settlement Footprint 2015 (WSF2015). The raster dataset has been generated by means of an advanced classification system which, for the first time, jointly exploits open-and-free optical and radar satellite imagery. The WSF2015 has been validated against 900,000 samples labelled by crowdsourcing photointerpretation of very high resolution Google Earth imagery and outperforms all other similar existing layers; in particular, it considerably improves the detection of very small settlements in rural regions and better outlines scattered suburban areas. The dataset can be used at any scale of observation in support to all applications requiring detailed and accurate information on human presence (e.g., socioeconomic development, population distribution, risks assessment, etc.).



### Few-shot Video-to-Video Synthesis
- **Arxiv ID**: http://arxiv.org/abs/1910.12713v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12713v1)
- **Published**: 2019-10-28 14:33:09+00:00
- **Updated**: 2019-10-28 14:33:09+00:00
- **Authors**: Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, Bryan Catanzaro
- **Comment**: In NeurIPS, 2019
- **Journal**: None
- **Summary**: Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations. First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches.



### Layer Pruning for Accelerating Very Deep Neural Networks
- **Arxiv ID**: http://arxiv.org/abs/1910.12727v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1910.12727v1)
- **Published**: 2019-10-28 14:49:42+00:00
- **Updated**: 2019-10-28 14:49:42+00:00
- **Authors**: Weiwei Zhang, Changsheng chen, Xuechun Wu, Jialin Gao, Di Bao, Jiwei Li, Xi Zhou
- **Comment**: v2
- **Journal**: None
- **Summary**: In this paper, we propose an adaptive pruning method. This method can cut off the channel and layer adaptively. The proportion of the layer and the channel to be cut is learned adaptively. The pruning method proposed in this paper can reduce half of the parameters, and the accuracy will not decrease or even be higher than baseline.



### Skip-Clip: Self-Supervised Spatiotemporal Representation Learning by Future Clip Order Ranking
- **Arxiv ID**: http://arxiv.org/abs/1910.12770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12770v1)
- **Published**: 2019-10-28 15:54:45+00:00
- **Updated**: 2019-10-28 15:54:45+00:00
- **Authors**: Alaaeldin El-Nouby, Shuangfei Zhai, Graham W. Taylor, Joshua M. Susskind
- **Comment**: Holistic Video Understanding Workshop ICCV2019
- **Journal**: None
- **Summary**: Deep neural networks require collecting and annotating large amounts of data to train successfully. In order to alleviate the annotation bottleneck, we propose a novel self-supervised representation learning approach for spatiotemporal features extracted from videos. We introduce Skip-Clip, a method that utilizes temporal coherence in videos, by training a deep model for future clip order ranking conditioned on a context clip as a surrogate objective for video future prediction. We show that features learned using our method are generalizable and transfer strongly to downstream tasks. For action recognition on the UCF101 dataset, we obtain 51.8% improvement over random initialization and outperform models initialized using inflated ImageNet parameters. Skip-Clip also achieves results competitive with state-of-the-art self-supervision methods.



### Learning Data Manipulation for Augmentation and Weighting
- **Arxiv ID**: http://arxiv.org/abs/1910.12795v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12795v1)
- **Published**: 2019-10-28 16:46:24+00:00
- **Updated**: 2019-10-28 16:46:24+00:00
- **Authors**: Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom Mitchell, Eric P. Xing
- **Comment**: NeurIPS 2019
- **Journal**: None
- **Summary**: Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the "data reward" function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.



### Attenuating Random Noise in Seismic Data by a Deep Learning Approach
- **Arxiv ID**: http://arxiv.org/abs/1910.12800v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12800v1)
- **Published**: 2019-10-28 16:53:26+00:00
- **Updated**: 2019-10-28 16:53:26+00:00
- **Authors**: Xing Zhao, Ping Lu, Yanyan Zhang, Jianxiong Chen, Xiaoyang Li
- **Comment**: 33 pages, 11 figures
- **Journal**: None
- **Summary**: In the geophysical field, seismic noise attenuation has been considered as a critical and long-standing problem, especially for the pre-stack data processing. Here, we propose a model to leverage the deep-learning model for this task. Rather than directly applying an existing de-noising model from ordinary images to the seismic data, we have designed a particular deep-learning model, based on residual neural networks. It is named as N2N-Seismic, which has a strong ability to recover the seismic signals back to intact condition with the preservation of primary signals. The proposed model, achieving with great success in attenuating noise, has been tested on two different seismic datasets. Several metrics show that our method outperforms conventional approaches in terms of Signal-to-Noise-Ratio, Mean-Squared-Error, Phase Spectrum, etc. Moreover, robust tests in terms of effectively removing random noise from any dataset with strong and weak noises have been extensively scrutinized in making sure that the proposed model is able to maintain a good level of adaptation while dealing with large variations of noise characteristics and intensities.



### Entity Abstraction in Visual Model-Based Reinforcement Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.12827v5
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.NE, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.12827v5)
- **Published**: 2019-10-28 17:37:46+00:00
- **Updated**: 2020-05-06 14:51:15+00:00
- **Authors**: Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua B. Tenenbaum, Sergey Levine
- **Comment**: Accepted at CoRL 2019
- **Journal**: None
- **Summary**: This paper tests the hypothesis that modeling a scene in terms of entities and their local interactions, as opposed to modeling the scene globally, provides a significant benefit in generalizing to physical tasks in a combinatorial space the learner has not encountered before. We present object-centric perception, prediction, and planning (OP3), which to the best of our knowledge is the first fully probabilistic entity-centric dynamic latent variable framework for model-based reinforcement learning that acquires entity representations from raw visual observations without supervision and uses them to predict and plan. OP3 enforces entity-abstraction -- symmetric processing of each entity representation with the same locally-scoped function -- which enables it to scale to model different numbers and configurations of objects from those in training. Our approach to solving the key technical challenge of grounding these entity representations to actual objects in the environment is to frame this variable binding problem as an inference problem, and we develop an interactive inference algorithm that uses temporal continuity and interactive feedback to bind information about object properties to the entity variables. On block-stacking tasks, OP3 generalizes to novel block configurations and more objects than observed during training, outperforming an oracle model that assumes access to object supervision and achieving two to three times better accuracy than a state-of-the-art video prediction model that does not exhibit entity abstraction.



### STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits
- **Arxiv ID**: http://arxiv.org/abs/1910.12906v2
- **DOI**: 10.1609/aaai.v34i02.5490
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1910.12906v2)
- **Published**: 2019-10-28 18:43:48+00:00
- **Updated**: 2021-07-31 16:00:51+00:00
- **Authors**: Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, Dinesh Manocha
- **Comment**: 11 pages, 7 figures, 1 table
- **Journal**: AAAI, 2020, 34(02), 1342-1350
- **Summary**: We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network (ST-GCN) architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the emotional state of the human into one of four emotions: happy, sad, angry, or neutral. We use hundreds of annotated real-world gait videos and augment them with thousands of annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset (E-Gait), which consists of $2,177$ human gaits annotated with perceived emotions along with thousands of synthetic gaits. In practice, STEP can learn the affective features and exhibits classification accuracy of 89% on E-Gait, which is 14 - 30% more accurate over prior methods.



### Literature Review: Human Segmentation with Static Camera
- **Arxiv ID**: http://arxiv.org/abs/1910.12945v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1910.12945v1)
- **Published**: 2019-10-28 20:02:41+00:00
- **Updated**: 2019-10-28 20:02:41+00:00
- **Authors**: Jiaxin Xu, Rui Wang, Vaibhav Rakheja
- **Comment**: None
- **Journal**: None
- **Summary**: Our research topic is Human segmentation with static camera. This topic can be divided into three sub-tasks, which are object detection, instance identification and segmentation. These sub-tasks are three closely related subjects. The development of each subject has great impact on the other two fields. In this literature review, we will first introduce the background of human segmentation and then talk about issues related to the above three fields as well as how they interact with each other.



### Shoestring: Graph-Based Semi-Supervised Learning with Severely Limited Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1910.12976v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1910.12976v2)
- **Published**: 2019-10-28 21:23:01+00:00
- **Updated**: 2020-04-08 14:34:18+00:00
- **Authors**: Wanyu Lin, Zhaolin Gao, Baochun Li
- **Comment**: 9 pages, 5 tables, 3 figures, accepted at CVPR2020, source code will
  be released soon
- **Journal**: None
- **Summary**: Graph-based semi-supervised learning has been shown to be one of the most effective approaches for classification tasks from a wide range of domains, such as image classification and text classification, as they can exploit the connectivity patterns between labeled and unlabeled samples to improve learning performance. In this work, we advance this effective learning paradigm towards a scenario where labeled data are severely limited. More specifically, we address the problem of graph-based semi-supervised learning in the presence of severely limited labeled samples, and propose a new framework, called {\em Shoestring}, that improves the learning performance through semantic transfer from these very few labeled samples to large numbers of unlabeled samples.   In particular, our framework learns a metric space in which classification can be performed by computing the similarity to centroid embedding of each class. {\em Shoestring} is trained in an end-to-end fashion to learn to leverage the semantic knowledge of limited labeled samples as well as their connectivity patterns with large numbers of unlabeled samples simultaneously. By combining {\em Shoestring} with graph convolutional networks, label propagation and their recent label-efficient variations (IGCN and GLP), we are able to achieve state-of-the-art node classification performance in the presence of very few labeled samples. In addition, we demonstrate the effectiveness of our framework on image classification tasks in the few-shot learning regime, with significant gains on miniImageNet ($2.57\%\sim3.59\%$) and tieredImageNet ($1.05\%\sim2.70\%$).



### Neural Similarity Learning
- **Arxiv ID**: http://arxiv.org/abs/1910.13003v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1910.13003v3)
- **Published**: 2019-10-28 23:06:56+00:00
- **Updated**: 2019-12-06 10:39:39+00:00
- **Authors**: Weiyang Liu, Zhen Liu, James M. Rehg, Le Song
- **Comment**: NeurIPS 2019 (v3)
- **Journal**: None
- **Summary**: Inner product-based convolution has been the founding stone of convolutional neural networks (CNNs), enabling end-to-end learning of visual representation. By generalizing inner product with a bilinear matrix, we propose the neural similarity which serves as a learnable parametric similarity measure for CNNs. Neural similarity naturally generalizes the convolution and enhances flexibility. Further, we consider the neural similarity learning (NSL) in order to learn the neural similarity adaptively from training data. Specifically, we propose two different ways of learning the neural similarity: static NSL and dynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a dynamic inference network. By regularizing the bilinear matrix, NSL can be viewed as learning the shape of kernel and the similarity measure simultaneously. We further justify the effectiveness of NSL with a theoretical viewpoint. Most importantly, NSL shows promising performance in visual recognition and few-shot learning, validating the superiority of NSL over the inner product-based convolution counterparts.



