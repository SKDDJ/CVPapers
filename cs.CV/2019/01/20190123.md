# Arxiv Papers in cs.CV on 2019-01-23
### A Top-down Approach to Articulated Human Pose Estimation and Tracking
- **Arxiv ID**: http://arxiv.org/abs/1901.07680v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07680v1)
- **Published**: 2019-01-23 01:19:29+00:00
- **Updated**: 2019-01-23 01:19:29+00:00
- **Authors**: Guanghan Ning, Ping Liu, Xiaochuan Fan, Chi Zhang
- **Comment**: To appear in ECCVW (2018). Workshop: 2nd PoseTrack Challenge
- **Journal**: None
- **Summary**: Both the tasks of multi-person human pose estimation and pose tracking in videos are quite challenging. Existing methods can be categorized into two groups: top-down and bottom-up approaches. In this paper, following the top-down approach, we aim to build a strong baseline system with three modules: human candidate detector, single-person pose estimator and human pose tracker. Firstly, we choose a generic object detector among state-of-the-art methods to detect human candidates. Then, the cascaded pyramid network is used to estimate the corresponding human pose. Finally, we use a flow-based pose tracker to render keypoint-association across frames, i.e., assigning each human candidate a unique and temporally-consistent id, for the multi-target pose tracking purpose. We conduct extensive ablative experiments to validate various choices of models and configurations. We take part in two ECCV 18 PoseTrack challenges: pose estimation and pose tracking.



### Class Activation Map Generation by Representative Class Selection and Multi-Layer Feature Fusion
- **Arxiv ID**: http://arxiv.org/abs/1901.07683v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07683v1)
- **Published**: 2019-01-23 01:39:08+00:00
- **Updated**: 2019-01-23 01:39:08+00:00
- **Authors**: Fanman Meng, Kaixu Huang, Hongliang Li, Qingbo Wu
- **Comment**: 11 pages
- **Journal**: None
- **Summary**: Existing method generates class activation map (CAM) by a set of fixed classes (i.e., using all the classes), while the discriminative cues between class pairs are not considered. Note that activation maps by considering different class pair are complementary, and therefore can provide more discriminative cues to overcome the shortcoming of the existing CAM generation that the highlighted regions are usually local part regions rather than global object regions due to the lack of object cues. In this paper, we generate CAM by using a few of representative classes, with aim of extracting more discriminative cues by considering each class pair to obtain CAM more globally. The advantages are twofold. Firstly, the representative classes are able to obtain activation regions that are complementary to each other, and therefore leads to generating activation map more accurately. Secondly, we only need to consider a small number of representative classes, making the CAM generation suitable for small networks. We propose a clustering based method to select the representative classes. Multiple binary classification models rather than a multiple class classification model are used to generate the CAM. Moreover, we propose a multi-layer fusion based CAM generation method to simultaneously combine high-level semantic features and low-level detail features. We validate the proposed method on the PASCAL VOC and COCO database in terms of segmentation groundtruth. Various networks such as classical network (Resnet-50, Resent-101 and Resnet-152) and small network (VGG-19, Resnet-18 and Mobilenet) are considered. Experimental results show that the proposed method improves the CAM generation obviously.



### Simultaneous Subspace Clustering and Cluster Number Estimating based on Triplet Relationship
- **Arxiv ID**: http://arxiv.org/abs/1901.07689v1
- **DOI**: 10.1109/TIP.2019.2903294
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07689v1)
- **Published**: 2019-01-23 02:13:08+00:00
- **Updated**: 2019-01-23 02:13:08+00:00
- **Authors**: Jie Liang, Jufeng Yang, Ming-Ming Cheng, Paul L. Rosin, Liang Wang
- **Comment**: 13 pages, 4 figures, 6 tables
- **Journal**: None
- **Summary**: In this paper we propose a unified framework to simultaneously discover the number of clusters and group the data points into them using subspace clustering. Real data distributed in a high-dimensional space can be disentangled into a union of low-dimensional subspaces, which can benefit various applications. To explore such intrinsic structure, state-of-the-art subspace clustering approaches often optimize a self-representation problem among all samples, to construct a pairwise affinity graph for spectral clustering. However, a graph with pairwise similarities lacks robustness for segmentation, especially for samples which lie on the intersection of two subspaces. To address this problem, we design a hyper-correlation based data structure termed as the \textit{triplet relationship}, which reveals high relevance and local compactness among three samples. The triplet relationship can be derived from the self-representation matrix, and be utilized to iteratively assign the data points to clusters. Three samples in each triplet are encouraged to be highly correlated and are considered as a meta-element during clustering, which show more robustness than pairwise relationships when segmenting two densely distributed subspaces. Based on the triplet relationship, we propose a unified optimizing scheme to automatically calculate clustering assignments. Specifically, we optimize a model selection reward and a fusion reward by simultaneously maximizing the similarity of triplets from different clusters while minimizing the correlation of triplets from same cluster. The proposed algorithm also automatically reveals the number of clusters and fuses groups to avoid over-segmentation. Extensive experimental results on both synthetic and real-world datasets validate the effectiveness and robustness of the proposed method.



### Exploring Uncertainty in Conditional Multi-Modal Retrieval Systems
- **Arxiv ID**: http://arxiv.org/abs/1901.07702v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07702v1)
- **Published**: 2019-01-23 03:00:24+00:00
- **Updated**: 2019-01-23 03:00:24+00:00
- **Authors**: Ahmed Taha, Yi-Ting Chen, Xitong Yang, Teruhisa Misu, Larry Davis
- **Comment**: None
- **Journal**: None
- **Summary**: We cast visual retrieval as a regression problem by posing triplet loss as a regression loss. This enables epistemic uncertainty estimation using dropout as a Bayesian approximation framework in retrieval. Accordingly, Monte Carlo (MC) sampling is leveraged to boost retrieval performance. Our approach is evaluated on two applications: person re-identification and autonomous car driving. Comparable state-of-the-art results are achieved on multiple datasets for the former application.   We leverage the Honda driving dataset (HDD) for autonomous car driving application. It provides multiple modalities and similarity notions for ego-motion action understanding. Hence, we present a multi-modal conditional retrieval network. It disentangles embeddings into separate representations to encode different similarities. This form of joint learning eliminates the need to train multiple independent networks without any performance degradation. Quantitative evaluation highlights our approach competence, achieving 6% improvement in a highly uncertain environment.



### Max-margin Class Imbalanced Learning with Gaussian Affinity
- **Arxiv ID**: http://arxiv.org/abs/1901.07711v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07711v1)
- **Published**: 2019-01-23 03:53:48+00:00
- **Updated**: 2019-01-23 03:53:48+00:00
- **Authors**: Munawar Hayat, Salman Khan, Waqas Zamir, Jianbing Shen, Ling Shao
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an `affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels.



### Joint group and residual sparse coding for image compressive sensing
- **Arxiv ID**: http://arxiv.org/abs/1901.07720v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07720v1)
- **Published**: 2019-01-23 04:36:02+00:00
- **Updated**: 2019-01-23 04:36:02+00:00
- **Authors**: Lizhao Li, Song Xiao
- **Comment**: 27 pages, 7 figures
- **Journal**: None
- **Summary**: Nonlocal self-similarity and group sparsity have been widely utilized in image compressive sensing (CS). However, when the sampling rate is low, the internal prior information of degraded images may be not enough for accurate restoration, resulting in loss of image edges and details. In this paper, we propose a joint group and residual sparse coding method for CS image recovery (JGRSC-CS). In the proposed JGRSC-CS, patch group is treated as the basic unit of sparse coding and two dictionaries (namely internal and external dictionaries) are applied to exploit the sparse representation of each group simultaneously. The internal self-adaptive dictionary is used to remove artifacts, and an external Gaussian Mixture Model (GMM) dictionary, learned from clean training images, is used to enhance details and texture. To make the proposed method effective and robust, the split Bregman method is adopted to reconstruct the whole image. Experimental results manifest the proposed JGRSC-CS algorithm outperforms existing state-of-the-art methods in both peak signal to noise ratio (PSNR) and visual quality.



### Deep-Learning Inversion of Seismic Data
- **Arxiv ID**: http://arxiv.org/abs/1901.07733v2
- **DOI**: 10.1109/TGRS.2019.2953473
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.07733v2)
- **Published**: 2019-01-23 05:51:05+00:00
- **Updated**: 2020-06-26 07:34:55+00:00
- **Authors**: Shucai Li, Bin Liu, Yuxiao Ren, Yangkang Chen, Senlin Yang, Yunhai Wang, Peng Jiang
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.
  3, pp. 2135-2149, March 2020
- **Summary**: We propose a new method to tackle the mapping challenge from time-series data to spatial image in the field of seismic exploration, i.e., reconstructing the velocity model directly from seismic data by deep neural networks (DNNs). The conventional way of addressing this ill-posed inversion problem is through iterative algorithms, which suffer from poor nonlinear mapping and strong nonuniqueness. Other attempts may either import human intervention errors or underuse seismic data. The challenge for DNNs mainly lies in the weak spatial correspondence, the uncertain reflection-reception relationship between seismic data and velocity model, as well as the time-varying property of seismic data. To tackle these challenges, we propose end-to-end seismic inversion networks (SeisInvNets) with novel components to make the best use of all seismic data. Specifically, we start with every seismic trace and enhance it with its neighborhood information, its observation setup, and the global context of its corresponding seismic profile. From the enhanced seismic traces, the spatially aligned feature maps can be learned and further concatenated to reconstruct a velocity model. In general, we let every seismic trace contribute to the reconstruction of the whole velocity model by finding spatial correspondence. The proposed SeisInvNet consistently produces improvements over the baselines and achieves promising performance on our synthesized and proposed SeisInv data set according to various evaluation metrics. The inversion results are more consistent with the target from the aspects of velocity values, subsurface structures, and geological interfaces. Moreover, the mechanism and the generalization of the proposed method are discussed and verified. Nevertheless, the generalization of deep-learning-based inversion methods on real data is still challenging and considering physics may be one potential solution.



### ODN: Opening the Deep Network for Open-set Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/1901.07757v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07757v1)
- **Published**: 2019-01-23 07:49:31+00:00
- **Updated**: 2019-01-23 07:49:31+00:00
- **Authors**: Yu Shu, Yemin Shi, Yaowei Wang, Yixiong Zou, Qingsheng Yuan, Yonghong Tian
- **Comment**: 6 pages, 3 figures, ICME 2018
- **Journal**: None
- **Summary**: In recent years, the performance of action recognition has been significantly improved with the help of deep neural networks. Most of the existing action recognition works hold the \textit{closed-set} assumption that all action categories are known beforehand while deep networks can be well trained for these categories. However, action recognition in the real world is essentially an \textit{open-set} problem, namely, it is impossible to know all action categories beforehand and consequently infeasible to prepare sufficient training samples for those emerging categories. In this case, applying closed-set recognition methods will definitely lead to unseen-category errors. To address this challenge, we propose the Open Deep Network (ODN) for the open-set action recognition task. Technologically, ODN detects new categories by applying a multi-class triplet thresholding method, and then dynamically reconstructs the classification layer and "opens" the deep network by adding predictors for new categories continually. In order to transfer the learned knowledge to the new category, two novel methods, Emphasis Initialization and Allometry Training, are adopted to initialize and incrementally train the new predictor so that only few samples are needed to fine-tune the model. Extensive experiments show that ODN can effectively detect and recognize new categories with little human intervention, thus applicable to the open-set action recognition tasks in the real world. Moreover, ODN can even achieve comparable performance to some closed-set methods.



### Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification
- **Arxiv ID**: http://arxiv.org/abs/1901.07759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07759v2)
- **Published**: 2019-01-23 08:03:51+00:00
- **Updated**: 2019-01-24 11:36:04+00:00
- **Authors**: Cheng Xue, Qi Dou, Xueying Shi, Hao Chen, Pheng Ann Heng
- **Comment**: Accepted for publication at ISBI 2019
- **Journal**: IEEE International Symposium on Biomedical Imaging (ISBI 2019)
- **Summary**: Deep neural networks (DNNs) have achieved great success in a wide variety of medical image analysis tasks. However, these achievements indispensably rely on the accurately-annotated datasets. If with the noisy-labeled images, the training procedure will immediately encounter difficulties, leading to a suboptimal classifier. This problem is even more crucial in the medical field, given that the annotation quality requires great expertise. In this paper, we propose an effective iterative learning framework for noisy-labeled medical image classification, to combat the lacking of high quality annotated medical data. Specifically, an online uncertainty sample mining method is proposed to eliminate the disturbance from noisy-labeled images. Next, we design a sample re-weighting strategy to preserve the usefulness of correctly-labeled hard samples. Our proposed method is validated on skin lesion classification task, and achieved very promising results.



### A Boost in Revealing Subtle Facial Expressions: A Consolidated Eulerian Framework
- **Arxiv ID**: http://arxiv.org/abs/1901.07765v1
- **DOI**: 10.1109/FG.2019.8756541
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07765v1)
- **Published**: 2019-01-23 08:18:00+00:00
- **Updated**: 2019-01-23 08:18:00+00:00
- **Authors**: Wei Peng, Xiaopeng Hong, Yingyue Xu, Guoying Zhao
- **Comment**: conference IEEE FG2019
- **Journal**: 2019 14th IEEE International Conference on Automatic Face &
  Gesture Recognition (FG 2019)
- **Summary**: Facial Micro-expression Recognition (MER) distinguishes the underlying emotional states of spontaneous subtle facialexpressions. Automatic MER is challenging because that 1) the intensity of subtle facial muscle movement is extremely lowand 2) the duration of ME is transient.Recent works adopt motion magnification or time interpolation to resolve these issues. Nevertheless, existing works dividethem into two separate modules due to their non-linearity. Though such operation eases the difficulty in implementation, itignores their underlying connections and thus results in inevitable losses in both accuracy and speed. Instead, in this paper, weexplore their underlying joint formulations and propose a consolidated Eulerian framework to reveal the subtle facial movements.It expands the temporal duration and amplifies the muscle movements in micro-expressions simultaneously. Compared toexisting approaches, the proposed method can not only process ME clips more efficiently but also make subtle ME movementsmore distinguishable. Experiments on two public MER databases indicate that our model outperforms the state-of-the-art inboth speed and accuracy.



### Programmable Neural Network Trojan for Pre-Trained Feature Extractor
- **Arxiv ID**: http://arxiv.org/abs/1901.07766v1
- **DOI**: None
- **Categories**: **cs.CR**, cs.CV, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/1901.07766v1)
- **Published**: 2019-01-23 08:18:48+00:00
- **Updated**: 2019-01-23 08:18:48+00:00
- **Authors**: Yu Ji, Zixin Liu, Xing Hu, Peiqi Wang, Youhui Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural network (NN) trojaning attack is an emerging and important attack model that can broadly damage the system deployed with NN models. Existing studies have explored the outsourced training attack scenario and transfer learning attack scenario in some small datasets for specific domains, with limited numbers of fixed target classes. In this paper, we propose a more powerful trojaning attack method for both outsourced training attack and transfer learning attack, which outperforms existing studies in the capability, generality, and stealthiness. First, The attack is programmable that the malicious misclassification target is not fixed and can be generated on demand even after the victim's deployment. Second, our trojan attack is not limited in a small domain; one trojaned model on a large-scale dataset can affect applications of different domains that reuse its general features. Thirdly, our trojan design is hard to be detected or eliminated even if the victims fine-tune the whole model.



### Tree Recognition APP of Mount Tai Based on CNN
- **Arxiv ID**: http://arxiv.org/abs/1901.11388v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11388v1)
- **Published**: 2019-01-23 08:37:35+00:00
- **Updated**: 2019-01-23 08:37:35+00:00
- **Authors**: Zhihao Cao, Xinxin Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Mount Tai has abundant sunshine, abundant rainfall and favorable climatic conditions, forming dense vegetation with various kinds of trees. In order to make it easier for tourists to understand each tree and experience the culture of Mount Tai, this paper develops an App for tree recognition of Mount Tai based on convolution neural network (CNN), taking advantage of CNN efficient image recognition ability and easy-to-carry characteristics of Android mobile phone. The APP can accurately identify several common trees in Mount Tai, and give a brief introduction for tourists.



### Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff
- **Arxiv ID**: http://arxiv.org/abs/1901.07821v4
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, cs.IT, math.IT, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07821v4)
- **Published**: 2019-01-23 11:13:33+00:00
- **Updated**: 2019-07-30 13:01:06+00:00
- **Authors**: Yochai Blau, Tomer Michaeli
- **Comment**: ICML 2019 (long oral presentation), see talk at:
  https://slideslive.com/38917633/applications-computer-vision
- **Journal**: Proceedings of the 36th International Conference on Machine
  Learning, PMLR 97:675-685, 2019
- **Summary**: Lossy compression algorithms are typically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly accepted that "low distortion" is not a synonym for "high perceptual quality", and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau & Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.



### Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning
- **Arxiv ID**: http://arxiv.org/abs/1901.07827v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07827v2)
- **Published**: 2019-01-23 11:29:39+00:00
- **Updated**: 2019-03-22 01:29:58+00:00
- **Authors**: Shaohui Lin, Rongrong Ji, Yuchao Li, Cheng Deng, Xuelong Li
- **Comment**: IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
- **Journal**: None
- **Summary**: The success of convolutional neural networks (CNNs) in computer vision applications has been accompanied by a significant increase of computation and memory costs, which prohibits its usage on resource-limited environments such as mobile or embedded devices. To this end, the research of CNN compression has recently become emerging. In this paper, we propose a novel filter pruning scheme, termed structured sparsity regularization (SSR), to simultaneously speedup the computation and reduce the memory overhead of CNNs, which can be well supported by various off-the-shelf deep learning libraries. Concretely, the proposed scheme incorporates two different regularizers of structured sparsity into the original objective function of filter pruning, which fully coordinates the global outputs and local pruning operations to adaptively prune filters. We further propose an Alternative Updating with Lagrange Multipliers (AULM) scheme to efficiently solve its optimization. AULM follows the principle of ADMM and alternates between promoting the structured sparsity of CNNs and optimizing the recognition loss, which leads to a very efficient solver (2.5x to the most recent work that directly solves the group sparsity-based regularization). Moreover, by imposing the structured sparsity, the online inference is extremely memory-light, since the number of filters and the output feature maps are simultaneously reduced. The proposed scheme has been deployed to a variety of state-of-the-art CNN structures including LeNet, AlexNet, VGG, ResNet and GoogLeNet over different datasets. Quantitative results demonstrate that the proposed scheme achieves superior performance over the state-of-the-art methods. We further demonstrate the proposed compression scheme for the task of transfer learning, including domain adaptation and object detection, which also show exciting performance gains over the state-of-the-arts.



### Random Forest with Learned Representations for Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.07828v1
- **DOI**: 10.1109/TIP.2019.2905081
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07828v1)
- **Published**: 2019-01-23 11:36:33+00:00
- **Updated**: 2019-01-23 11:36:33+00:00
- **Authors**: Byeongkeun Kang, Truong Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we present a random forest framework that learns the weights, shapes, and sparsities of feature representations for real-time semantic segmentation. Typical filters (kernels) have predetermined shapes and sparsities and learn only weights. A few feature extraction methods fix weights and learn only shapes and sparsities. These predetermined constraints restrict learning and extracting optimal features. To overcome this limitation, we propose an unconstrained representation that is able to extract optimal features by learning weights, shapes, and sparsities. We, then, present the random forest framework that learns the flexible filters using an iterative optimization algorithm and segments input images using the learned representations. We demonstrate the effectiveness of the proposed method using a hand segmentation dataset for hand-object interaction and using two semantic segmentation datasets. The results show that the proposed method achieves real-time semantic segmentation using limited computational and memory resources.



### Toward Joint Image Generation and Compression using Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.07838v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07838v1)
- **Published**: 2019-01-23 12:07:43+00:00
- **Updated**: 2019-01-23 12:07:43+00:00
- **Authors**: Byeongkeun Kang, Subarna Tripathi, Truong Q. Nguyen
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a generative adversarial network framework that generates compressed images instead of synthesizing raw RGB images and compressing them separately. In the real world, most images and videos are stored and transferred in a compressed format to save storage capacity and data transfer bandwidth. However, since typical generative adversarial networks generate raw RGB images, those generated images need to be compressed by a post-processing stage to reduce the data size. Among image compression methods, JPEG has been one of the most commonly used lossy compression methods for still images. Hence, we propose a novel framework that generates JPEG compressed images using generative adversarial networks. The novel generator consists of the proposed locally connected layers, chroma subsampling layers, quantization layers, residual blocks, and convolution layers. The locally connected layer is proposed to enable block-based operations. We also discuss training strategies for the proposed architecture including the loss function and the transformation between its generator and its discriminator. The proposed method is evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom dataset. The results demonstrate that the proposed method is able to generate compressed data with competitive qualities. The proposed method is a promising baseline method for joint image generation and compression using generative adversarial networks.



### AADS: Augmented Autonomous Driving Simulation using Data-driven Algorithms
- **Arxiv ID**: http://arxiv.org/abs/1901.07849v3
- **DOI**: 10.1126/scirobotics.aaw0863
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07849v3)
- **Published**: 2019-01-23 12:30:25+00:00
- **Updated**: 2020-10-29 02:52:29+00:00
- **Authors**: Wei Li, Chengwei Pan, Rong Zhang, Jiaping Ren, Yuexin Ma, Jin Fang, Feilong Yan, Qichuan Geng, Xinyu Huang, Huajun Gong, Weiwei Xu, Guoping Wang, Dinesh Manocha, Ruigang Yang
- **Comment**: None
- **Journal**: Sci. Robotics. 4 (2019) eaaw0863
- **Summary**: Simulation systems have become an essential component in the development and validation of autonomous driving technologies. The prevailing state-of-the-art approach for simulation is to use game engines or high-fidelity computer graphics (CG) models to create driving scenarios. However, creating CG models and vehicle movements (e.g., the assets for simulation) remains a manual task that can be costly and time-consuming. In addition, the fidelity of CG images still lacks the richness and authenticity of real-world images and using these images for training leads to degraded performance.   In this paper we present a novel approach to address these issues: Augmented Autonomous Driving Simulation (AADS). Our formulation augments real-world pictures with a simulated traffic flow to create photo-realistic simulation images and renderings. More specifically, we use LiDAR and cameras to scan street scenes. From the acquired trajectory data, we generate highly plausible traffic flows for cars and pedestrians and compose them into the background. The composite images can be re-synthesized with different viewpoints and sensor models. The resulting images are photo-realistic, fully annotated, and ready for end-to-end training and testing of autonomous driving systems from perception to planning. We explain our system design and validate our algorithms with a number of autonomous driving tasks from detection to segmentation and predictions.   Compared to traditional approaches, our method offers unmatched scalability and realism. Scalability is particularly important for AD simulation and we believe the complexity and diversity of the real world cannot be realistically captured in a virtual environment. Our augmented approach combines the flexibility in a virtual environment (e.g., vehicle movements) with the richness of the real world to allow effective simulation of anywhere on earth.



### Computer Vision and Metrics Learning for Hypothesis Testing: An Application of Q-Q Plot for Normality Test
- **Arxiv ID**: http://arxiv.org/abs/1901.07851v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07851v2)
- **Published**: 2019-01-23 12:38:17+00:00
- **Updated**: 2019-09-16 08:47:43+00:00
- **Authors**: Ke-Wei Huang, Mengke Qiao, Xuanqi Liu, Siyuan Liu, Mingxi Dai
- **Comment**: None
- **Journal**: None
- **Summary**: This paper proposes a new deep-learning method to construct test statistics by computer vision and metrics learning. The application highlighted in this paper is applying computer vision on Q-Q plot to construct a new test statistic for normality test. To the best of our knowledge, there is no similar application documented in the literature. Traditionally, there are two families of approaches for verifying the probability distribution of a random variable. Researchers either subjectively assess the Q-Q plot or objectively use a mathematical formula, such as Kolmogorov-Smirnov test, to formally conduct a normality test. Graphical assessment by human beings is not rigorous whereas normality test statistics may not be accurate enough when the uniformly most powerful test does not exist. It may take tens of years for statistician to develop a new test statistic that is more powerful statistically. Our proposed method integrates four components based on deep learning: an image representation learning component of a Q-Q plot, a dimension reduction component, a metrics learning component that best quantifies the differences between two Q-Q plots for normality test, and a new normality hypothesis testing process. Our experimentation results show that the machine-learning-based test statistics can outperform several widely-used traditional normality tests. This study provides convincing evidence that the proposed method could objectively create a powerful test statistic based on Q-Q plots and this method could be modified to construct many more powerful test statistics for other applications in the future.



### Evolving the pulmonary nodules diagnosis from classical approaches to deep learning aided decision support: three decades development course and future prospect
- **Arxiv ID**: http://arxiv.org/abs/1901.07858v3
- **DOI**: 10.1007/s00432-019-03098-5
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.07858v3)
- **Published**: 2019-01-23 13:04:59+00:00
- **Updated**: 2020-04-24 17:31:24+00:00
- **Authors**: Bo Liu, Wenhao Chi, Xinran Li, Peng Li, Wenhua Liang, Haiping Liu, Wei Wang, Jianxing He
- **Comment**: We have substantially revised the article. The previous version had
  74 pages and 2 figures, and the lateset version had 66 pages and 6 figures
- **Journal**: Journal of Cancer Research and Clinical Oncology 146.1 (2020):
  153-185
- **Summary**: Lung cancer is the commonest cause of cancer deaths worldwide, and its mortality can be reduced significantly by performing early diagnosis and screening. Since the 1960s, driven by the pressing needs to accurately and effectively interpret the massive volume of chest images generated daily, computer-assisted diagnosis of pulmonary nodule has opened up new opportunities to relax the limitation from physicians' subjectivity, experiences and fatigue. And the fair access to the reliable and affordable computer-assisted diagnosis will fight the inequalities in incidence and mortality between populations. It has been witnessed that significant and remarkable advances have been achieved since the 1980s, and consistent endeavors have been exerted to deal with the grand challenges on how to accurately detect the pulmonary nodules with high sensitivity at low false-positives rate as well as on how to precisely differentiate between benign and malignant nodules. There is a lack of comprehensive examination of the techniques' development which is evolving the pulmonary nodules diagnosis from classical approaches to machine learning-assisted decision support. The main goal of this investigation is to provide a comprehensive state-of-the-art review of the computer-assisted nodules detection and benign-malignant classification techniques developed over 3 decades, which have evolved from the complicated ad hoc analysis pipeline of conventional approaches to the simplified seamlessly integrated deep learning techniques. This review also identifies challenges and highlights opportunities for future work in learning models, learning algorithms and enhancement schemes for bridging current state to future prospect and satisfying future demand.



### "Is this an example image?" -- Predicting the Relative Abstractness Level of Image and Text
- **Arxiv ID**: http://arxiv.org/abs/1901.07878v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CL, cs.CV, cs.IR, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07878v1)
- **Published**: 2019-01-23 13:42:02+00:00
- **Updated**: 2019-01-23 13:42:02+00:00
- **Authors**: Christian Otto, Sebastian Holzki, Ralph Ewerth
- **Comment**: 14 pages, 6 figures, accepted at ECIR2019
- **Journal**: None
- **Summary**: Successful multimodal search and retrieval requires the automatic understanding of semantic cross-modal relations, which, however, is still an open research problem. Previous work has suggested the metrics cross-modal mutual information and semantic correlation to model and predict cross-modal semantic relations of image and text. In this paper, we present an approach to predict the (cross-modal) relative abstractness level of a given image-text pair, that is whether the image is an abstraction of the text or vice versa. For this purpose, we introduce a new metric that captures this specific relationship between image and text at the Abstractness Level (ABS). We present a deep learning approach to predict this metric, which relies on an autoencoder architecture that allows us to significantly reduce the required amount of labeled training data. A comprehensive set of publicly available scientific documents has been gathered. Experimental results on a challenging test set demonstrate the feasibility of the approach.



### ORSIm Detector: A Novel Object Detection Framework in Optical Remote Sensing Imagery Using Spatial-Frequency Channel Features
- **Arxiv ID**: http://arxiv.org/abs/1901.07925v2
- **DOI**: 10.1109/TGRS.2019.2897139
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07925v2)
- **Published**: 2019-01-23 14:45:07+00:00
- **Updated**: 2019-01-31 04:23:02+00:00
- **Authors**: Xin Wu, Danfeng Hong, Jiaojiao Tian, Jocelyn Chanussot, Wei Li, Ran Tao
- **Comment**: None
- **Journal**: IEEE Transactions on Geoscience and Remote Sensing, 2019, 57(7):
  5146-5158
- **Summary**: With the rapid development of spaceborne imaging techniques, object detection in optical remote sensing imagery has drawn much attention in recent decades. While many advanced works have been developed with powerful learning algorithms, the incomplete feature representation still cannot meet the demand for effectively and efficiently handling image deformations, particularly objective scaling and rotation. To this end, we propose a novel object detection framework, called optical remote sensing imagery detector (ORSIm detector), integrating diverse channel features extraction, feature learning, fast image pyramid matching, and boosting strategy. ORSIm detector adopts a novel spatial-frequency channel feature (SFCF) by jointly considering the rotation-invariant channel features constructed in frequency domain and the original spatial channel features (e.g., color channel, gradient magnitude). Subsequently, we refine SFCF using learning-based strategy in order to obtain the high-level or semantically meaningful features. In the test phase, we achieve a fast and coarsely-scaled channel computation by mathematically estimating a scaling factor in the image domain. Extensive experimental results conducted on the two different airborne datasets are performed to demonstrate the superiority and effectiveness in comparison with previous state-of-the-art methods.



### U2-Net: A Bayesian U-Net model with epistemic uncertainty feedback for photoreceptor layer segmentation in pathological OCT scans
- **Arxiv ID**: http://arxiv.org/abs/1901.07929v2
- **DOI**: 10.1109/ISBI.2019.8759581
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07929v2)
- **Published**: 2019-01-23 14:52:33+00:00
- **Updated**: 2019-10-11 01:00:11+00:00
- **Authors**: José Ignacio Orlando, Philipp Seeböck, Hrvoje Bogunović, Sophie Klimscha, Christoph Grechenig, Sebastian Waldstein, Bianca S. Gerendas, Ursula Schmidt-Erfurth
- **Comment**: Accepted for publication at IEEE International Symposium on
  Biomedical Imaging (ISBI) 2019
- **Journal**: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019)
- **Summary**: In this paper, we introduce a Bayesian deep learning based model for segmenting the photoreceptor layer in pathological OCT scans. Our architecture provides accurate segmentations of the photoreceptor layer and produces pixel-wise epistemic uncertainty maps that highlight potential areas of pathologies or segmentation errors. We empirically evaluated this approach in two sets of pathological OCT scans of patients with age-related macular degeneration, retinal vein oclussion and diabetic macular edema, improving the performance of the baseline U-Net both in terms of the Dice index and the area under the precision/recall curve. We also observed that the uncertainty estimates were inversely correlated with the model performance, underlying its utility for highlighting areas where manual inspection/correction might be needed.



### DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images
- **Arxiv ID**: http://arxiv.org/abs/1901.07973v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.07973v1)
- **Published**: 2019-01-23 16:06:17+00:00
- **Updated**: 2019-01-23 16:06:17+00:00
- **Authors**: Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang, Xiaoou Tang, Ping Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding fashion images has been advanced by benchmarks with rich annotations such as DeepFashion, whose labels include clothing categories, landmarks, and consumer-commercial image pairs. However, DeepFashion has nonnegligible issues such as single clothing-item per image, sparse landmarks (4~8 only), and no per-pixel masks, making it had significant gap from real-world scenarios. We fill in the gap by presenting DeepFashion2 to address these issues. It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs. A strong baseline is proposed, called Match R-CNN, which builds upon Mask R-CNN to solve the above four tasks in an end-to-end manner. Extensive evaluations are conducted with different criterions in DeepFashion2.



### Removing Stripes, Scratches, and Curtaining with Non-Recoverable Compressed Sensing
- **Arxiv ID**: http://arxiv.org/abs/1901.08001v1
- **DOI**: 10.1017/S1431927619000254
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08001v1)
- **Published**: 2019-01-23 16:59:30+00:00
- **Updated**: 2019-01-23 16:59:30+00:00
- **Authors**: Jonathan Schwartz, Yi Jiang, Yongjie Wang, Anthony Aiello, Pallab Bhattacharya, Hui Yuan, Zetian Mi, Nabil Bassim, Robert Hovden
- **Comment**: 15 pages, 5 figures
- **Journal**: None
- **Summary**: Highly-directional image artifacts such as ion mill curtaining, mechanical scratches, or image striping from beam instability degrade the interpretability of micrographs. These unwanted, aperiodic features extend the image along a primary direction and occupy a small wedge of information in Fourier space. Deleting this wedge of data replaces stripes, scratches, or curtaining, with more complex streaking and blurring artifacts-known within the tomography community as missing wedge artifacts. Here, we overcome this problem by recovering the missing region using total variation minimization, which leverages image sparsity based reconstruction techniques-colloquially referred to as compressed sensing-to reliably restore images corrupted by stripe like features. Our approach removes beam instability, ion mill curtaining, mechanical scratches, or any stripe features and remains robust at low signal-to-noise. The success of this approach is achieved by exploiting compressed sensings inability to recover directional structures that are highly localized and missing in Fourier Space.



### Spherical sampling methods for the calculation of metamer mismatch volumes
- **Arxiv ID**: http://arxiv.org/abs/1901.08419v1
- **DOI**: 10.1364/JOSAA.36.000096
- **Categories**: **cs.CG**, cs.CV, cs.GR, 68U99
- **Links**: [PDF](http://arxiv.org/pdf/1901.08419v1)
- **Published**: 2019-01-23 18:33:05+00:00
- **Updated**: 2019-01-23 18:33:05+00:00
- **Authors**: Michal Mackiewicz, Hans Jakob Rivertz, Graham D. Finlayson
- **Comment**: One print or electronic copy may be made for personal use only.
  Systematic reproduction and distribution, duplication of any material in this
  paper for a fee or for commercial purposes, or modifications of this paper
  are prohibited. Optical Society of America
- **Journal**: Vol. 36, No. 1 / Jan 2019 / Journal of the Optical Society of
  America A
- **Summary**: In this paper, we propose two methods of calculating theoretically maximal metamer mismatch volumes. Unlike prior art techniques, our methods do not make any assumptions on the shape of spectra on the boundary of the mismatch volumes. Both methods utilize a spherical sampling approach, but they calculate mismatch volumes in two different ways. The first method uses a linear programming optimization, while the second is a computational geometry approach based on half-space intersection. We show that under certain conditions the theoretically maximal metamer mismatch volume is significantly larger than the one approximated using a prior art method.



### Bottom-up Object Detection by Grouping Extreme and Center Points
- **Arxiv ID**: http://arxiv.org/abs/1901.08043v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08043v3)
- **Published**: 2019-01-23 18:50:09+00:00
- **Updated**: 2019-04-25 17:02:24+00:00
- **Authors**: Xingyi Zhou, Jiacheng Zhuo, Philipp Krähenbühl
- **Comment**: None
- **Journal**: None
- **Summary**: With the advent of deep learning, object detection drifted from a bottom-up to a top-down recognition problem. State of the art algorithms enumerate a near-exhaustive list of object locations and classify each into: object or not. In this paper, we show that bottom-up approaches still perform competitively. We detect four extreme points (top-most, left-most, bottom-most, right-most) and one center point of objects using a standard keypoint estimation network. We group the five keypoints into a bounding box if they are geometrically aligned. Object detection is then a purely appearance-based keypoint estimation problem, without region classification or implicit feature learning. The proposed method performs on-par with the state-of-the-art region based detection methods, with a bounding box AP of 43.2% on COCO test-dev. In addition, our estimated extreme points directly span a coarse octagonal mask, with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding boxes. Extreme point guided segmentation further improves this to 34.6% Mask AP.



### Can Adversarial Networks Hallucinate Occluded People With a Plausible Aspect?
- **Arxiv ID**: http://arxiv.org/abs/1901.08097v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08097v1)
- **Published**: 2019-01-23 19:43:58+00:00
- **Updated**: 2019-01-23 19:43:58+00:00
- **Authors**: Federico Fulgeri, Matteo Fabbri, Stefano Alletto, Simone Calderara, Rita Cucchiara
- **Comment**: Under review at CVIU
- **Journal**: None
- **Summary**: When you see a person in a crowd, occluded by other persons, you miss visual information that can be used to recognize, re-identify or simply classify him or her. You can imagine its appearance given your experience, nothing more. Similarly, AI solutions can try to hallucinate missing information with specific deep learning architectures, suitably trained with people with and without occlusions. The goal of this work is to generate a complete image of a person, given an occluded version in input, that should be a) without occlusion b) similar at pixel level to a completely visible people shape c) capable to conserve similar visual attributes (e.g. male/female) of the original one. For the purpose, we propose a new approach by integrating the state-of-the-art of neural network architectures, namely U-nets and GANs, as well as discriminative attribute classification nets, with an architecture specifically designed to de-occlude people shapes. The network is trained to optimize a Loss function which could take into account the aforementioned objectives. As well we propose two datasets for testing our solution: the first one, occluded RAP, created automatically by occluding real shapes of the RAP dataset (which collects also attributes of the people aspect); the second is a large synthetic dataset, AiC, generated in computer graphics with data extracted from the GTA video game, that contains 3D data of occluded objects by construction. Results are impressive and outperform any other previous proposal. This result could be an initial step to many further researches to recognize people and their behavior in an open crowded world.



### Domain Translation with Conditional GANs: from Depth to RGB Face-to-Face
- **Arxiv ID**: http://arxiv.org/abs/1901.08101v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08101v1)
- **Published**: 2019-01-23 19:49:23+00:00
- **Updated**: 2019-01-23 19:49:23+00:00
- **Authors**: Matteo Fabbri, Guido Borghi, Fabio Lanzi, Roberto Vezzani, Simone Calderara, Rita Cucchiara
- **Comment**: Accepted at ICPR 2018
- **Journal**: None
- **Summary**: Can faces acquired by low-cost depth sensors be useful to catch some characteristic details of the face? Typically the answer is no. However, new deep architectures can generate RGB images from data acquired in a different modality, such as depth data. In this paper, we propose a new \textit{Deterministic Conditional GAN}, trained on annotated RGB-D face datasets, effective for a face-to-face translation from depth to RGB. Although the network cannot reconstruct the exact somatic features for unknown individual faces, it is capable to reconstruct plausible faces; their appearance is accurate enough to be used in many pattern recognition tasks. In fact, we test the network capability to hallucinate with some \textit{Perceptual Probes}, as for instance face aspect classification or landmark detection. Depth face can be used in spite of the correspondent RGB images, that often are not available due to difficult luminance conditions. Experimental results are very promising and are as far as better than previously proposed approaches: this domain translation can constitute a new way to exploit depth data in new future applications.



### Siamese Networks with Location Prior for Landmark Tracking in Liver Ultrasound Sequences
- **Arxiv ID**: http://arxiv.org/abs/1901.08109v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.08109v1)
- **Published**: 2019-01-23 19:59:36+00:00
- **Updated**: 2019-01-23 19:59:36+00:00
- **Authors**: Alvaro Gomariz, Weiye Li, Ece Ozkan, Christine Tanner, Orcun Goksel
- **Comment**: Accepted at the IEEE International Symposium on Biomedical Imaging
  (ISBI) 2019
- **Journal**: None
- **Summary**: Image-guided radiation therapy can benefit from accurate motion tracking by ultrasound imaging, in order to minimize treatment margins and radiate moving anatomical targets, e.g., due to breathing. One way to formulate this tracking problem is the automatic localization of given tracked anatomical landmarks throughout a temporal ultrasound sequence. For this, we herein propose a fully-convolutional Siamese network that learns the similarity between pairs of image regions containing the same landmark. Accordingly, it learns to localize and thus track arbitrary image features, not only predefined anatomical structures. We employ a temporal consistency model as a location prior, which we combine with the network-predicted location probability map to track a target iteratively in ultrasound sequences. We applied this method on the dataset of the Challenge on Liver Ultrasound Tracking (CLUST) with competitive results, where our work is the first to effectively apply CNNs on this tracking problem, thanks to our temporal regularization.



### Hypergraph Convolution and Hypergraph Attention
- **Arxiv ID**: http://arxiv.org/abs/1901.08150v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08150v2)
- **Published**: 2019-01-23 22:09:21+00:00
- **Updated**: 2020-10-10 14:44:52+00:00
- **Authors**: Song Bai, Feihu Zhang, Philip H. S. Torr
- **Comment**: Accepted by Pattern Recognition
- **Journal**: None
- **Summary**: Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.



### Learning to navigate image manifolds induced by generative adversarial networks for unsupervised video generation
- **Arxiv ID**: http://arxiv.org/abs/1901.11384v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11384v1)
- **Published**: 2019-01-23 23:21:08+00:00
- **Updated**: 2019-01-23 23:21:08+00:00
- **Authors**: Isabela Albuquerque, João Monteiro, Tiago H. Falk
- **Comment**: None
- **Journal**: None
- **Summary**: In this work, we introduce a two-step framework for generative modeling of temporal data. Specifically, the generative adversarial networks (GANs) setting is employed to generate synthetic scenes of moving objects. To do so, we propose a two-step training scheme within which: a generator of static frames is trained first. Afterwards, a recurrent model is trained with the goal of providing a sequence of inputs to the previously trained frames generator, thus yielding scenes which look natural. The adversarial setting is employed in both training steps. However, with the aim of avoiding known training instabilities in GANs, a multiple discriminator approach is used to train both models. Results in the studied video dataset indicate that, by employing such an approach, the recurrent part is able to learn how to coherently navigate the image manifold induced by the frames generator, thus yielding more natural-looking scenes.



