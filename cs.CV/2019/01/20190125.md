# Arxiv Papers in cs.CV on 2019-01-25
### Surrogate Supervision for Medical Image Analysis: Effective Deep Learning From Limited Quantities of Labeled Data
- **Arxiv ID**: http://arxiv.org/abs/1901.08707v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08707v1)
- **Published**: 2019-01-25 01:19:47+00:00
- **Updated**: 2019-01-25 01:19:47+00:00
- **Authors**: Nima Tajbakhsh, Yufei Hu, Junli Cao, Xingjian Yan, Yi Xiao, Yong Lu, Jianming Liang, Demetri Terzopoulos, Xiaowei Ding
- **Comment**: Accepted in IEEE International Symposium on Biomedical Imaging (ISBI
  2019)
- **Journal**: None
- **Summary**: We investigate the effectiveness of a simple solution to the common problem of deep learning in medical image analysis with limited quantities of labeled training data. The underlying idea is to assign artificial labels to abundantly available unlabeled medical images and, through a process known as surrogate supervision, pre-train a deep neural network model for the target medical image analysis task lacking sufficient labeled training data. In particular, we employ 3 surrogate supervision schemes, namely rotation, reconstruction, and colorization, in 4 different medical imaging applications representing classification and segmentation for both 2D and 3D medical images. 3 key findings emerge from our research: 1) pre-training with surrogate supervision is effective for small training sets; 2) deep models trained from initial weights pre-trained through surrogate supervision outperform the same models when trained from scratch, suggesting that pre-training with surrogate supervision should be considered prior to training any deep 3D models; 3) pre-training models in the medical domain with surrogate supervision is more effective than transfer learning from an unrelated domain (e.g., natural images), indicating the practical value of abundant unlabeled medical image data.



### Deep Multimodality Model for Multi-task Multi-view Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.08723v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08723v1)
- **Published**: 2019-01-25 03:26:21+00:00
- **Updated**: 2019-01-25 03:26:21+00:00
- **Authors**: Lecheng Zheng, Yu Cheng, Jingrui He
- **Comment**: None
- **Journal**: None
- **Summary**: Many real-world problems exhibit the coexistence of multiple types of heterogeneity, such as view heterogeneity (i.e., multi-view property) and task heterogeneity (i.e., multi-task property). For example, in an image classification problem containing multiple poses of the same object, each pose can be considered as one view, and the detection of each type of object can be treated as one task. Furthermore, in some problems, the data type of multiple views might be different. In a web classification problem, for instance, we might be provided an image and text mixed data set, where the web pages are characterized by both images and texts. A common strategy to solve this kind of problem is to leverage the consistency of views and the relatedness of tasks to build the prediction model. In the context of deep neural network, multi-task relatedness is usually realized by grouping tasks at each layer, while multi-view consistency is usually enforced by finding the maximal correlation coefficient between views. However, there is no existing deep learning algorithm that jointly models task and view dual heterogeneity, particularly for a data set with multiple modalities (text and image mixed data set or text and video mixed data set, etc.). In this paper, we bridge this gap by proposing a deep multi-task multi-view learning framework that learns a deep representation for such dual-heterogeneity problems. Empirical studies on multiple real-world data sets demonstrate the effectiveness of our proposed Deep-MTMV algorithm.



### Misleading Metadata Detection on YouTube
- **Arxiv ID**: http://arxiv.org/abs/1901.08759v1
- **DOI**: None
- **Categories**: **cs.CL**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.08759v1)
- **Published**: 2019-01-25 07:09:14+00:00
- **Updated**: 2019-01-25 07:09:14+00:00
- **Authors**: Priyank Palod, Ayush Patwari, Sudhanshu Bahety, Saurabh Bagchi, Pawan Goyal
- **Comment**: Accepted at European Conference on Information Retrieval(ECIR) 2019.
  7 Pages
- **Journal**: None
- **Summary**: YouTube is the leading social media platform for sharing videos. As a result, it is plagued with misleading content that includes staged videos presented as real footages from an incident, videos with misrepresented context and videos where audio/video content is morphed. We tackle the problem of detecting such misleading videos as a supervised classification task. We develop UCNet - a deep network to detect fake videos and perform our experiments on two datasets - VAVD created by us and publicly available FVC [8]. We achieve a macro averaged F-score of 0.82 while training and testing on a 70:30 split of FVC, while the baseline model scores 0.36. We find that the proposed model generalizes well when trained on one dataset and tested on the other.



### Virtual Conditional Generative Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.09822v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09822v1)
- **Published**: 2019-01-25 07:15:17+00:00
- **Updated**: 2019-01-25 07:15:17+00:00
- **Authors**: Haifeng Shi, Guanyu Cai, Yuqin Wang, Shaohua Shang, Lianghua He
- **Comment**: None
- **Journal**: None
- **Summary**: When trained on multimodal image datasets, normal Generative Adversarial Networks (GANs) are usually outperformed by class-conditional GANs and ensemble GANs, but conditional GANs is restricted to labeled datasets and ensemble GANs lack efficiency. We propose a novel GAN variant called virtual conditional GAN (vcGAN) which is not only an ensemble GAN with multiple generative paths while adding almost zero network parameters, but also a conditional GAN that can be trained on unlabeled datasets without explicit clustering steps or objectives other than the adversary loss. Inside the vcGAN's generator, a learnable ``analog-to-digital converter (ADC)" module maps a slice of the inputted multivariate Gaussian noise to discrete/digital noise (virtual label), according to which a selector selects the corresponding generative path to produce the sample. All the generative paths share the same decoder network while in each path the decoder network is fed with a concatenation of a different pre-computed amplified one-hot vector and the inputted Gaussian noise. We conducted a lot of experiments on several balanced/imbalanced image datasets to demonstrate that vcGAN converges faster and achieves improved Frech\'et Inception Distance (FID). In addition, we show the training byproduct that the ADC in vcGAN learned the categorical probability of each mode and that each generative path generates samples of specific mode, which enables class-conditional sampling. Codes are available at \url{https://github.com/annonnymmouss/vcgan}



### Multiple Hypothesis Tracking Algorithm for Multi-Target Multi-Camera Tracking with Disjoint Views
- **Arxiv ID**: http://arxiv.org/abs/1901.08787v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08787v1)
- **Published**: 2019-01-25 09:01:05+00:00
- **Updated**: 2019-01-25 09:01:05+00:00
- **Authors**: Kwangjin Yoon, Young-min Song, Moongu Jeon
- **Comment**: published in IET image processing, 2018
- **Journal**: IET image processing, Volume: 12, Issue: 7, 7 2018
- **Summary**: In this study, a multiple hypothesis tracking (MHT) algorithm for multi-target multi-camera tracking (MCT) with disjoint views is proposed. Our method forms track-hypothesis trees, and each branch of them represents a multi-camera track of a target that may move within a camera as well as move across cameras. Furthermore, multi-target tracking within a camera is performed simultaneously with the tree formation by manipulating a status of each track hypothesis. Each status represents three different stages of a multi-camera track: tracking, searching, and end-of-track. The tracking status means targets are tracked by a single camera tracker. In the searching status, the disappeared targets are examined if they reappear in other cameras. The end-of-track status does the target exited the camera network due to its lengthy invisibility. These three status assists MHT to form the track-hypothesis trees for multi-camera tracking. Furthermore, they present a gating technique for eliminating of unlikely observation-to-track association. In the experiments, they evaluate the proposed method using two datasets, DukeMTMC and NLPR-MCT, which demonstrates that the proposed method outperforms the state-of-the-art method in terms of improvement of the accuracy. In addition, they show that the proposed method can operate in real-time and online.



### Face morphing detection in the presence of printing/scanning and heterogeneous image sources
- **Arxiv ID**: http://arxiv.org/abs/1901.08811v4
- **DOI**: 10.1049/bme2.12021
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08811v4)
- **Published**: 2019-01-25 10:10:47+00:00
- **Updated**: 2021-02-24 07:58:00+00:00
- **Authors**: Matteo Ferrara, Annalisa Franco, Davide Maltoni
- **Comment**: This paper is a preprint of a paper accepted by IET Biometrics and is
  subject to Institution of Engineering and Technology Copyright. When the
  final version is published, the copy of record will be available at the IET
  Digital Library
- **Journal**: None
- **Summary**: Face morphing represents nowadays a big security threat in the context of electronic identity documents as well as an interesting challenge for researchers in the field of face recognition. Despite of the good performance obtained by state-of-the-art approaches on digital images, no satisfactory solutions have been identified so far to deal with cross-database testing and printed-scanned images (typically used in many countries for document issuing). In this work, novel approaches are proposed to train Deep Neural Networks for morphing detection: in particular generation of simulated printed-scanned images together with other data augmentation strategies and pre-training on large face recognition datasets, allowed to reach state-of-the-art accuracy on challenging datasets from heterogeneous image sources.



### Joint shape learning and segmentation for medical images using a minimalistic deep network
- **Arxiv ID**: http://arxiv.org/abs/1901.08824v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08824v1)
- **Published**: 2019-01-25 10:53:57+00:00
- **Updated**: 2019-01-25 10:53:57+00:00
- **Authors**: Balamurali Murugesan, Kaushik Sarveswaran, Sharath M Shankaranarayana, Keerthi Ram, Mohanasankar Sivaprakasam
- **Comment**: Under review at MIDL 2019
- **Journal**: None
- **Summary**: Recently, state-of-the-art results have been achieved in semantic segmentation using fully convolutional networks (FCNs). Most of these networks employ encoder-decoder style architecture similar to U-Net and are trained with images and the corresponding segmentation maps as a pixel-wise classification task. Such frameworks only exploit class information by using the ground truth segmentation maps. In this paper, we propose a multi-task learning framework with the main aim of exploiting structural and spatial information along with the class information. We modify the decoder part of the FCN to exploit class information and the structural information as well. We intend to do this while also keeping the parameters of the network as low as possible. We obtain the structural information using either of the two ways: i) using the contour map and ii) using the distance map, both of which can be obtained from ground truth segmentation maps with no additional annotation costs. We also explore different ways in which distance maps can be computed and study the effects of different distance maps on the segmentation performance. We also experiment extensively on two different medical image segmentation applications: i.e i) using color fundus images for optic disc and cup segmentation and ii) using endoscopic images for polyp segmentation. Through our experiments, we report results comparable to, and in some cases performing better than the current state-of-the-art architectures and with an order of 2x reduction in the number of parameters.



### Vision-based inspection system employing computer vision & neural networks for detection of fractures in manufactured components
- **Arxiv ID**: http://arxiv.org/abs/1901.08864v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08864v1)
- **Published**: 2019-01-25 13:18:04+00:00
- **Updated**: 2019-01-25 13:18:04+00:00
- **Authors**: Sarthak J Shetty
- **Comment**: Artificial Intelligence International Conference, Barcelona, Spain
- **Journal**: None
- **Summary**: We are proceeding towards the age of automation and robotic integration of our production lines [5]. Effective quality-control systems have to be put in place to maintain the quality of manufactured components. Among different quality-control systems, vision-based inspection systems have gained considerable amount of popularity [8] due to developments in computing power and image processing techniques. In this paper, we present a vision-based inspection system (VBI) as a quality-control system, which not only detects the presence of defects, such as in conventional VBIs, but also leverage developments in machine learning to predict the presence of surface fractures and wearing. We use OpenCV, an open source computer-vision framework, and Tensorflow, an open source machine-learning framework developed by Google Inc., to accomplish the tasks of detection and prediction of presence of surface defects such as fractures of manufactured gears.



### Dense 3D Point Cloud Reconstruction Using a Deep Pyramid Network
- **Arxiv ID**: http://arxiv.org/abs/1901.08906v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08906v1)
- **Published**: 2019-01-25 15:07:44+00:00
- **Updated**: 2019-01-25 15:07:44+00:00
- **Authors**: Priyanka Mandikal, R. Venkatesh Babu
- **Comment**: WACV 2019
- **Journal**: None
- **Summary**: Reconstructing a high-resolution 3D model of an object is a challenging task in computer vision. Designing scalable and light-weight architectures is crucial while addressing this problem. Existing point-cloud based reconstruction approaches directly predict the entire point cloud in a single stage. Although this technique can handle low-resolution point clouds, it is not a viable solution for generating dense, high-resolution outputs. In this work, we introduce DensePCR, a deep pyramidal network for point cloud reconstruction that hierarchically predicts point clouds of increasing resolution. Towards this end, we propose an architecture that first predicts a low-resolution point cloud, and then hierarchically increases the resolution by aggregating local and global point features to deform a grid. Our method generates point clouds that are accurate, uniform and dense. Through extensive quantitative and qualitative evaluation on synthetic and real datasets, we demonstrate that DensePCR outperforms the existing state-of-the-art point cloud reconstruction works, while also providing a light-weight and scalable architecture for predicting high-resolution outputs.



### Self-Supervised Generalisation with Meta Auxiliary Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.08933v3
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.08933v3)
- **Published**: 2019-01-25 15:46:59+00:00
- **Updated**: 2019-11-26 20:37:33+00:00
- **Authors**: Shikun Liu, Andrew J. Davison, Edward Johns
- **Comment**: Published at Conference on Neural Information Processing Systems 2019
- **Journal**: None
- **Summary**: Learning with auxiliary tasks can improve the ability of a primary task to generalise. However, this comes at the cost of manually labelling auxiliary data. We propose a new method which automatically learns appropriate labels for an auxiliary task, such that any supervised learning task can be improved without requiring access to any further data. The approach is to train two neural networks: a label-generation network to predict the auxiliary labels, and a multi-task network to train the primary task alongside the auxiliary task. The loss for the label-generation network incorporates the loss of the multi-task network, and so this interaction between the two networks can be seen as a form of meta learning with a double gradient. We show that our proposed method, Meta AuXiliary Learning (MAXL), outperforms single-task learning on 7 image datasets, without requiring any additional data. We also show that MAXL outperforms several other baselines for generating auxiliary labels, and is even competitive when compared with human-defined auxiliary labels. The self-supervised nature of our method leads to a promising new direction towards automated generalisation. Source code can be found at https://github.com/lorenmt/maxl.



### Improving Image Captioning by Leveraging Knowledge Graphs
- **Arxiv ID**: http://arxiv.org/abs/1901.08942v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08942v1)
- **Published**: 2019-01-25 16:00:57+00:00
- **Updated**: 2019-01-25 16:00:57+00:00
- **Authors**: Yimin Zhou, Yiwei Sun, Vasant Honavar
- **Comment**: Accepted by WACV'19
- **Journal**: None
- **Summary**: We explore the use of a knowledge graphs, that capture general or commonsense knowledge, to augment the information extracted from images by the state-of-the-art methods for image captioning. The results of our experiments, on several benchmark data sets such as MS COCO, as measured by CIDEr-D, a performance metric for image captioning, show that the variants of the state-of-the-art methods for image captioning that make use of the information extracted from knowledge graphs can substantially outperform those that rely solely on the information extracted from images.



### Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.08954v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.08954v1)
- **Published**: 2019-01-25 16:18:22+00:00
- **Updated**: 2019-01-25 16:18:22+00:00
- **Authors**: Samet Akçay, Amir Atapour-Abarghouei, Toby P. Breckon
- **Comment**: Conference Submission. 8 pages, 9 figures
- **Journal**: None
- **Summary**: Despite inherent ill-definition, anomaly detection is a research endeavor of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most significant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains a most a subset of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in high-dimensional image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within high-dimensional image space and a lower-dimensional latent vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.



### FaceForensics++: Learning to Detect Manipulated Facial Images
- **Arxiv ID**: http://arxiv.org/abs/1901.08971v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.08971v3)
- **Published**: 2019-01-25 16:38:21+00:00
- **Updated**: 2019-08-26 17:59:54+00:00
- **Authors**: Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nießner
- **Comment**: Video: https://youtu.be/x2g48Q2I2ZQ
- **Journal**: None
- **Summary**: The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domainspecific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.



### A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal Sequence Learning and Prediction
- **Arxiv ID**: http://arxiv.org/abs/1901.09002v2
- **DOI**: None
- **Categories**: **cs.NE**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.09002v2)
- **Published**: 2019-01-25 18:03:17+00:00
- **Updated**: 2021-10-01 12:59:31+00:00
- **Authors**: Jielin Qiu, Ge Huang, Tai Sing Lee
- **Comment**: Some of the results are not replicable
- **Journal**: None
- **Summary**: In this paper we developed a hierarchical network model, called Hierarchical Prediction Network (HPNet), to understand how spatiotemporal memories might be learned and encoded in the recurrent circuits in the visual cortical hierarchy for predicting future video frames. This neurally inspired model operates in the analysis-by-synthesis framework. It contains a feed-forward path that computes and encodes spatiotemporal features of successive complexity and a feedback path for the successive levels to project their interpretations to the level below. Within each level, the feed-forward path and the feedback path intersect in a recurrent gated circuit, instantiated in a LSTM module, to generate a prediction or explanation of the incoming signals. The network learns its internal model of the world by minimizing the errors of its prediction of the incoming signals at each level of the hierarchy. We found that hierarchical interaction in the network increases semantic clustering of global movement patterns in the population codes of the units along the hierarchy, even in the earliest module. This facilitates the learning of relationships among movement patterns, yielding state-of-the-art performance in long range video sequence predictions in the benchmark datasets. The network model automatically reproduces a variety of prediction suppression and familiarity suppression neurophysiological phenomena observed in the visual cortex, suggesting that hierarchical prediction might indeed be an important principle for representational learning in the visual cortex.



### Revisiting Self-Supervised Visual Representation Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.09005v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09005v1)
- **Published**: 2019-01-25 18:08:31+00:00
- **Updated**: 2019-01-25 18:08:31+00:00
- **Authors**: Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer
- **Comment**: All three authors contributed equally. Code is available at
  https://github.com/google/revisiting-self-supervised
- **Journal**: None
- **Summary**: Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.



### Deep Learning on Small Datasets without Pre-Training using Cosine Loss
- **Arxiv ID**: http://arxiv.org/abs/1901.09054v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09054v2)
- **Published**: 2019-01-25 19:13:03+00:00
- **Updated**: 2019-12-11 15:15:46+00:00
- **Authors**: Björn Barz, Joachim Denzler
- **Comment**: Presented at WACV 2020
- **Journal**: 2020 IEEE Winter Conference on Applications of Computer Vision
  (WACV), Snowmass Village, CO, USA, 2020
- **Summary**: Two things seem to be indisputable in the contemporary deep learning discourse: 1. The categorical cross-entropy loss after softmax activation is the method of choice for classification. 2. Training a CNN classifier from scratch on small datasets does not work well. In contrast to this, we show that the cosine loss function provides significantly better performance than cross-entropy on datasets with only a handful of samples per class. For example, the accuracy achieved on the CUB-200-2011 dataset without pre-training is by 30% higher than with the cross-entropy loss. Further experiments on other popular datasets confirm our findings. Moreover, we demonstrate that integrating prior knowledge in the form of class hierarchies is straightforward with the cosine loss and improves classification performance further.



### Learning Classifiers for Domain Adaptation, Zero and Few-Shot Recognition Based on Learning Latent Semantic Parts
- **Arxiv ID**: http://arxiv.org/abs/1901.09079v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09079v3)
- **Published**: 2019-01-25 20:49:35+00:00
- **Updated**: 2019-08-16 19:11:32+00:00
- **Authors**: Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama
- **Comment**: 9 pages, 3 figures, 3 tables. Accepted at ICML 2019
- **Journal**: None
- **Summary**: In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examples exist to allow for training "models from scratch," and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. We propose a novel visual attribute encoding method that encodes each image as a low-dimensional probability vector composed of prototypical part-type probabilities. The prototypes are learnt to be representative of all training data. At test-time we utilize this encoding as an input to a classifier. At test-time we freeze the encoder and only learn/adapt the classifier component to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive experiments on benchmark datasets. Our method outperforms state-of-art methods trained for the specific contexts (ZSL, FSL, DA).



### Automated Segmentation of CT Scans for Normal Pressure Hydrocephalus
- **Arxiv ID**: http://arxiv.org/abs/1901.09088v2
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, physics.med-ph, 92-04
- **Links**: [PDF](http://arxiv.org/pdf/1901.09088v2)
- **Published**: 2019-01-25 21:29:14+00:00
- **Updated**: 2019-07-23 17:47:32+00:00
- **Authors**: Angela Zhang, Po-Yu Kao, Ronald Sahyouni, Ashutosh Shelat, Jefferson Chen, B. S. Manjunath
- **Comment**: None
- **Journal**: None
- **Summary**: Normal Pressure Hydrocephalus (NPH) is one of the few reversible forms of dementia, Due to their low cost and versatility, Computed Tomography (CT) scans have long been used as an aid to help diagnose intracerebral anomalies such as NPH. However, no well-defined and effective protocol currently exists for the analysis of CT scan-based ventricular, cerebral mass and subarachnoid space volumes in the setting of NPH. The Evan's ratio, an approximation of the ratio of ventricle to brain volume using only one 2D slice of the scan, has been proposed but is not robust. Instead of manually measuring a 2-dimensional proxy for the ratio of ventricle volume to brain volume, this study proposes an automated method of calculating the brain volumes for better recognition of NPH from a radiological standpoint. The method first aligns the subject CT volume to a common space through an affine transformation, then uses a random forest classifier to mask relevant tissue types. A 3D morphological segmentation method is used to partition the brain volume, which in turn is used to train machine learning methods to classify the subjects into non-NPH vs. NPH based on volumetric information. The proposed algorithm has increased sensitivity compared to the Evan's ratio thresholding method.



### Audio-Visual Scene-Aware Dialog
- **Arxiv ID**: http://arxiv.org/abs/1901.09107v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09107v2)
- **Published**: 2019-01-25 22:23:39+00:00
- **Updated**: 2019-05-08 18:02:30+00:00
- **Authors**: Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks, Chiori Hori, Peter Anderson, Stefan Lee, Devi Parikh
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce the task of scene-aware dialog. Our goal is to generate a complete and natural response to a question about a scene, given video and audio of the scene and the history of previous turns in the dialog. To answer successfully, agents must ground concepts from the question in the video while leveraging contextual cues from the dialog history. To benchmark this task, we introduce the Audio Visual Scene-Aware Dialog (AVSD) Dataset. For each of more than 11,000 videos of human actions from the Charades dataset, our dataset contains a dialog about the video, plus a final summary of the video by one of the dialog participants. We train several baseline systems for this task and evaluate the performance of the trained models using both qualitative and quantitative metrics. Our results indicate that models must utilize all the available inputs (video, audio, question, and dialog history) to perform best on this dataset.



### Equivariant Transformer Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.11399v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11399v2)
- **Published**: 2019-01-25 22:29:48+00:00
- **Updated**: 2019-05-24 20:36:43+00:00
- **Authors**: Kai Sheng Tai, Peter Bailis, Gregory Valiant
- **Comment**: ICML 2019
- **Journal**: None
- **Summary**: How can prior knowledge on the transformation invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards pre-defined continuous transformation groups. Through the use of specially-derived canonical coordinate systems, ETs incorporate functions that are equivariant by construction with respect to these transformations. We show empirically that ETs can be flexibly composed to improve model robustness towards more complicated transformation groups in several parameters. On a real-world image classification task, ETs improve the sample efficiency of ResNet classifiers, achieving relative improvements in error rate of up to 15% in the limited data regime while increasing model parameter count by less than 1%.



### Visual Categorization of Objects into Animal and Plant Classes Using Global Shape Descriptors
- **Arxiv ID**: http://arxiv.org/abs/1901.11398v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/1901.11398v1)
- **Published**: 2019-01-25 23:57:36+00:00
- **Updated**: 2019-01-25 23:57:36+00:00
- **Authors**: Zahra Sadeghi
- **Comment**: 10 pages, 5 figures, 3 tables
- **Journal**: None
- **Summary**: How humans can distinguish between general categories of objects? Are the subcategories of living things visually distinctive? In a number of semantic-category deficits, patients are good at making broad categorization but are unable to remember fine and specific details. It has been well accepted that general information about concepts are more robust to damages related to semantic memory. Results from patients with semantic memory disorders demonstrate the loss of ability in subcategory recognition. While bottom-up feature construction has been studied in detail, little attention has been served to top-down approach and the type of features that could account for general categorization. In this paper, we show that broad categories of animal and plant are visually distinguishable without processing textural information. To this aim, we utilize shape descriptors with an additional phase of feature learning. The results are evaluated with both supervised and unsupervised learning mechanisms. The obtained results demonstrate that global encoding of visual appearance of objects accounts for high discrimination between animal and plant object categories.



