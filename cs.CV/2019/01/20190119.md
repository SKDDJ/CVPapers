# Arxiv Papers in cs.CV on 2019-01-19
### Learning single-image 3D reconstruction by generative modelling of shape, pose and shading
- **Arxiv ID**: http://arxiv.org/abs/1901.06447v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.06447v2)
- **Published**: 2019-01-19 00:34:39+00:00
- **Updated**: 2019-08-26 22:41:10+00:00
- **Authors**: Paul Henderson, Vittorio Ferrari
- **Comment**: Extension of arXiv:1807.09259, accepted to IJCV. Differentiable
  renderer available at https://github.com/pmh47/dirt
- **Journal**: None
- **Summary**: We present a unified framework tackling two problems: class-specific 3D reconstruction from a single image, and generation of new 3D shape samples. These tasks have received considerable attention recently; however, most existing approaches rely on 3D supervision, annotation of 2D images with keypoints or poses, and/or training with multiple views of each object instance. Our framework is very general: it can be trained in similar settings to existing approaches, while also supporting weaker supervision. Importantly, it can be trained purely from 2D images, without pose annotations, and with only a single view per instance. We employ meshes as an output representation, instead of voxels used in most prior work. This allows us to reason over lighting parameters and exploit shading information during training, which previous 2D-supervised methods cannot. Thus, our method can learn to generate and reconstruct concave object classes. We evaluate our approach in various settings, showing that: (i) it learns to disentangle shape from pose and lighting; (ii) using shading in the loss improves performance compared to just silhouettes; (iii) when using a standard single white light, our model outperforms state-of-the-art 2D-supervised methods, both with and without pose supervision, thanks to exploiting shading cues; (iv) performance improves further when using multiple coloured lights, even approaching that of state-of-the-art 3D-supervised methods; (v) shapes produced by our model capture smooth surfaces and fine details better than voxel-based approaches; and (vi) our approach supports concave classes such as bathtubs and sofas, which methods based on silhouettes cannot learn.



### Cross-referencing Social Media and Public Surveillance Camera Data for Disaster Response
- **Arxiv ID**: http://arxiv.org/abs/1901.06459v1
- **DOI**: None
- **Categories**: **cs.SI**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.06459v1)
- **Published**: 2019-01-19 03:06:45+00:00
- **Updated**: 2019-01-19 03:06:45+00:00
- **Authors**: Chittayong Surakitbanharn, Calvin Yau, Guizhen Wang, Aniesh Chawla, Yinuo Pan, Zhaoya Sun, Sam Yellin, David Ebert, Yung-Hsiang Lu, George K. Thiruvathukal
- **Comment**: Best Paper award in IEEE HST Conference 2018
- **Journal**: None
- **Summary**: Physical media (like surveillance cameras) and social media (like Instagram and Twitter) may both be useful in attaining on-the-ground information during an emergency or disaster situation. However, the intersection and reliability of both surveillance cameras and social media during a natural disaster are not fully understood. To address this gap, we tested whether social media is of utility when physical surveillance cameras went off-line during Hurricane Irma in 2017. Specifically, we collected and compared geo-tagged Instagram and Twitter posts in the state of Florida during times and in areas where public surveillance cameras went off-line. We report social media content and frequency and content to determine the utility for emergency managers or first responders during a natural disaster.



### Deep Representation Learning Characterized by Inter-class Separation for Image Clustering
- **Arxiv ID**: http://arxiv.org/abs/1901.06474v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06474v1)
- **Published**: 2019-01-19 06:47:49+00:00
- **Updated**: 2019-01-19 06:47:49+00:00
- **Authors**: Dipanjan Das, Ratul Ghosh, Brojeshwar Bhowmick
- **Comment**: Published in WACV, 2019
- **Journal**: None
- **Summary**: Despite significant advances in clustering methods in recent years, the outcome of clustering of a natural image dataset is still unsatisfactory due to two important drawbacks. Firstly, clustering of images needs a good feature representation of an image and secondly, we need a robust method which can discriminate these features for making them belonging to different clusters such that intra-class variance is less and inter-class variance is high. Often these two aspects are dealt with independently and thus the features are not sufficient enough to partition the data meaningfully. In this paper, we propose a method where we discover these features required for the separation of the images using deep autoencoder. Our method learns the image representation features automatically for the purpose of clustering and also select a coherent image and an incoherent image simultaneously for a given image so that the feature representation learning can learn better discriminative features for grouping the similar images in a cluster and at the same time separating the dissimilar images across clusters. Experiment results show that our method produces significantly better result than the state-of-the-art methods and we also show that our method is more generalized across different dataset without using any pre-trained model like other existing methods.



### Single MR Image Super-Resolution via Channel Splitting and Serial Fusion Network
- **Arxiv ID**: http://arxiv.org/abs/1901.06484v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06484v1)
- **Published**: 2019-01-19 08:44:12+00:00
- **Updated**: 2019-01-19 08:44:12+00:00
- **Authors**: Zhao Xiaole, Huali Zhang, Hangfei Liu, Yun Qin, Tao Zhang, Xueming Zou
- **Comment**: 12 pages, 12 figures, 6 tables
- **Journal**: None
- **Summary**: Spatial resolution is a critical imaging parameter in magnetic resonance imaging (MRI). Acquiring high resolution MRI data usually takes long scanning time and would subject to motion artifacts due to hardware, physical, and physiological limitations. Single image super-resolution (SISR), especially that based on deep learning techniques, is an effective and promising alternative technique to improve the current spatial resolution of magnetic resonance (MR) images. However, the deeper network is more difficult to be effectively trained because the information is gradually weakened as the network deepens. This problem becomes more serious for medical images due to the degradation of training examples. In this paper, we present a novel channel splitting and serial fusion network (CSSFN) for single MR image super-resolution. Specifically, the proposed CSSFN network splits the hierarchical features into a series of subfeatures, which are then integrated together in a serial manner. Thus, the network becomes deeper and can deal with the subfeatures on different channels discriminatively. Besides, a dense global feature fusion (DGFF) is adopted to integrate the intermediate features, which further promotes the information flow in the network. Extensive experiments on several typical MR images show the superiority of our CSSFN model over other advanced SISR methods.



### Endoscopic vs. volumetric OCT imaging of mastoid bone structure for pose estimation in minimally invasive cochlear implant surgery
- **Arxiv ID**: http://arxiv.org/abs/1901.06490v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.06490v2)
- **Published**: 2019-01-19 09:40:13+00:00
- **Updated**: 2019-03-23 10:39:48+00:00
- **Authors**: Max-Heinrich Laves, Sarah Latus, Jan Bergmeier, Tobias Ortmaier, LÃ¼der A. Kahrs, Alexander Schlaefer
- **Comment**: Accepted for publication at the 33rd international conference on
  Computer Assisted Radiology and Surgery (CARS) 2019
- **Journal**: None
- **Summary**: Purpose: The facial recess is a delicate structure that must be protected in minimally invasive cochlear implant surgery. Current research estimates the drill trajectory by using endoscopy of the unique mastoid patterns. However, missing depth information limits available features for a registration to preoperative CT data. Therefore, this paper evaluates OCT for enhanced imaging of drill holes in mastoid bone and compares OCT data to original endoscopic images.   Methods: A catheter-based OCT probe is inserted into a drill trajectory of a mastoid phantom in a translation-rotation manner to acquire the inner surface state. The images are undistorted and stitched to create volumentric data of the drill hole. The mastoid cell pattern is segmented automatically and compared to ground truth.   Results: The mastoid pattern segmented on images acquired with OCT show a similarity of J = 73.6 % to ground truth based on endoscopic images and measured with the Jaccard metric. Leveraged by additional depth information, automated segmentation tends to be more robust and fail-safe compared to endoscopic images.   Conclusion: The feasibility of using a clinically approved OCT probe for imaging the drill hole in cochlear implantation is shown. The resulting volumentric images provide additional information on the shape of caveties in the bone structure, which will be useful for image-to-patient registration and to estimate the drill trajectory. This will be another step towards safe minimally invasive cochlear implantation.



### Writer Independent Offline Signature Recognition Using Ensemble Learning
- **Arxiv ID**: http://arxiv.org/abs/1901.06494v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.06494v1)
- **Published**: 2019-01-19 09:57:15+00:00
- **Updated**: 2019-01-19 09:57:15+00:00
- **Authors**: Sourya Dipta Das, Himanshu Ladia, Vaibhav Kumar, Shivansh Mishra
- **Comment**: 6 pages, 2 figures, International Conference on Data Science, Machine
  Learning & Applications (ICDSMLA)
- **Journal**: None
- **Summary**: The area of Handwritten Signature Verification has been broadly researched in the last decades, but remains an open research problem. In offline (static) signature verification, the dynamic information of the signature writing process is lost, and it is difficult to design good feature extractors that can distinguish genuine signatures and skilled forgeries. This verification task is even harder in writer independent scenarios which is undeniably fiscal for realistic cases. In this paper, we have proposed an Ensemble model for offline writer, independent signature verification task with Deep learning. We have used two CNNs for feature extraction, after that RGBT for classification & Stacking to generate final prediction vector. We have done extensive experiments on various datasets from various sources to maintain a variance in the dataset. We have achieved the state of the art performance on various datasets.



### The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions
- **Arxiv ID**: http://arxiv.org/abs/1901.06514v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/1901.06514v1)
- **Published**: 2019-01-19 12:49:56+00:00
- **Updated**: 2019-01-19 12:49:56+00:00
- **Authors**: Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, Alvaro Jover-Alvarez
- **Comment**: None
- **Journal**: None
- **Summary**: Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.



### Image De-Noising For Salt and Pepper Noise by Introducing New Enhanced Filter
- **Arxiv ID**: http://arxiv.org/abs/1901.06528v1
- **DOI**: 10.15242/IIE.E1213577
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.06528v1)
- **Published**: 2019-01-19 14:10:49+00:00
- **Updated**: 2019-01-19 14:10:49+00:00
- **Authors**: Vivek Kumar, Atul Samadhiya
- **Comment**: 5 pages, 3 Figures, 2 Tables, International Conference on Innovations
  in Engineering and Technology (ICIET'2013) Dec. 25-26, 2013 Bangkok
  (Thailand)
- **Journal**: ICIET'2013 pp 53-57, 2013
- **Summary**: When an image is formed, factors such as lighting (spectra, source, and intensity) and camera characteristics (sensor response, lenses) affect the appearance of the image. Therefore, the prime factor that reduces the quality of the image is noise. It hides the important details and information of images. In order to enhance the qualities of the image, the removal of noises become imperative and that should not at the cost of any loss of image information. Noise removal is one of the pre-processing stages of image processing. In this paper a new method for the enhancement of grayscale images is introduced, when images are corrupted by fixed valued impulse noise (salt and pepper noise). The proposed methodology ensures a better output for the low and medium density of fixed value impulse noise as compared to the other famous filters like Standard Median Filter (SMF), Decision Based Median Filter (DBMF) and Modified Decision Based Median Filter (MDBMF) etc. The main objective of the proposed method was to improve peak signal to noise ratio (PSNR), visual perception and reduction in the blurring of the image. The proposed algorithm replaced the noisy pixel by trimmed mean value. When previous pixel values, 0s, and 255s are present in the particular window and all the pixel values are 0s and 255s then the remaining noisy pixels are replaced by mean value. The gray-scale image of mandrill and Lena were tested via the proposed method. The experimental result shows better peak signal to noise ratio (PSNR), mean square error values with better visual and human perception.



### Comparative Performance Analysis of Image De-noising Techniques
- **Arxiv ID**: http://arxiv.org/abs/1901.06529v1
- **DOI**: 10.15242/IIE.E1213576
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.06529v1)
- **Published**: 2019-01-19 14:16:36+00:00
- **Updated**: 2019-01-19 14:16:36+00:00
- **Authors**: Vivek Kumar, Atul Samadhiya
- **Comment**: 6 pages, 9 figures, 1 Table, International Conference on Innovations
  in Engineering and Technology (ICIET'2013) Dec. 25-26, 2013 Bangkok
  (Thailand)
- **Journal**: ICIET pages 47-52, 2013
- **Summary**: Noise is an important factor which when get added to an image reduces its quality and appearance. So in order to enhance the image qualities, it has to be removed with preserving the textural information and structural features of image. There are different types of noises exist who corrupt the images. Selection of the denoising algorithm is application dependent. Hence, it is necessary to have knowledge about the noise present in the image so as to select the appropriate denoising algorithm. Objective of this paper is to present brief account on types of noises, its types and different noise removal algorithms. In the first section types of noises on the basis of their additive and multiplicative nature are being discussed. In second section a precise classification and analysis of the different potential image denoising algorithm is presented. At the end of paper, a comparative study of all these algorithms in context of performance evaluation is done and concluded with several promising directions for future research work.



### Synthesizing facial photometries and corresponding geometries using generative adversarial networks
- **Arxiv ID**: http://arxiv.org/abs/1901.06551v1
- **DOI**: None
- **Categories**: **cs.CG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.06551v1)
- **Published**: 2019-01-19 16:36:49+00:00
- **Updated**: 2019-01-19 16:36:49+00:00
- **Authors**: Gil Shamai, Ron Slossberg, Ron Kimmel
- **Comment**: 23 pages, 16 figures, 3 tables
- **Journal**: None
- **Summary**: Artificial data synthesis is currently a well studied topic with useful applications in data science, computer vision, graphics and many other fields. Generating realistic data is especially challenging since human perception is highly sensitive to non realistic appearance. In recent times, new levels of realism have been achieved by advances in GAN training procedures and architectures. These successful models, however, are tuned mostly for use with regularly sampled data such as images, audio and video. Despite the successful application of the architecture on these types of media, applying the same tools to geometric data poses a far greater challenge. The study of geometric deep learning is still a debated issue within the academic community as the lack of intrinsic parametrization inherent to geometric objects prohibits the direct use of convolutional filters, a main building block of today's machine learning systems. In this paper we propose a new method for generating realistic human facial geometries coupled with overlayed textures. We circumvent the parametrization issue by imposing a global mapping from our data to the unit rectangle. We further discuss how to design such a mapping to control the mapping distortion and conserve area within the mapped image. By representing geometric textures and geometries as images, we are able to use advanced GAN methodologies to generate new geometries. We address the often neglected topic of relation between texture and geometry and propose to use this correlation to match between generated textures and their corresponding geometries. We offer a new method for training GAN models on partially corrupted data. Finally, we provide empirical evidence demonstrating our generative model's ability to produce examples of new identities independent from the training data while maintaining a high level of realism, two traits that are often at odds.



### Consistent Optimization for Single-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.06563v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06563v2)
- **Published**: 2019-01-19 17:57:37+00:00
- **Updated**: 2019-01-23 13:07:24+00:00
- **Authors**: Tao Kong, Fuchun Sun, Huaping Liu, Yuning Jiang, Jianbo Shi
- **Comment**: Technical report
- **Journal**: None
- **Summary**: We present consistent optimization for single stage object detection. Previous works of single stage object detectors usually rely on the regular, dense sampled anchors to generate hypothesis for the optimization of the model. Through an examination of the behavior of the detector, we observe that the misalignment between the optimization target and inference configurations has hindered the performance improvement. We propose to bride this gap by consistent optimization, which is an extension of the traditional single stage detector's optimization strategy. Consistent optimization focuses on matching the training hypotheses and the inference quality by utilizing of the refined anchors during training. To evaluate its effectiveness, we conduct various design choices based on the state-of-the-art RetinaNet detector. We demonstrate it is the consistent optimization, not the architecture design, that yields the performance boosts. Consistent optimization is nearly cost-free, and achieves stable performance gains independent of the model capacities or input scales. Specifically, utilizing consistent optimization improves RetinaNet from 39.1 AP to 40.1 AP on COCO dataset without any bells or whistles, which surpasses the accuracy of all existing state-of-the-art one-stage detectors when adopting ResNet-101 as backbone. The code will be made available.



### Design of Real-time Semantic Segmentation Decoder for Automated Driving
- **Arxiv ID**: http://arxiv.org/abs/1901.06580v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.06580v1)
- **Published**: 2019-01-19 19:51:31+00:00
- **Updated**: 2019-01-19 19:51:31+00:00
- **Authors**: Arindam Das, Saranya Kandan, Senthil Yogamani, Pavel Krizek
- **Comment**: Accepted at VISAPP 2019
- **Journal**: None
- **Summary**: Semantic segmentation remains a computationally intensive algorithm for embedded deployment even with the rapid growth of computation power. Thus efficient network design is a critical aspect especially for applications like automated driving which requires real-time performance. Recently, there has been a lot of research on designing efficient encoders that are mostly task agnostic. Unlike image classification and bounding box object detection tasks, decoders are computationally expensive as well for semantic segmentation task. In this work, we focus on efficient design of the segmentation decoder and assume that an efficient encoder is already designed to provide shared features for a multi-task learning system. We design a novel efficient non-bottleneck layer and a family of decoders which fit into a small run-time budget using VGG10 as efficient encoder. We demonstrate in our dataset that experimentation with various design choices led to an improvement of 10\% from a baseline performance.



### Face Detection and Face Recognition In the Wild Using Off-the-Shelf Freely Available Components
- **Arxiv ID**: http://arxiv.org/abs/1901.06585v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.06585v1)
- **Published**: 2019-01-19 20:33:35+00:00
- **Updated**: 2019-01-19 20:33:35+00:00
- **Authors**: Hira Ahmad
- **Comment**: 6 pages, 3 figures
- **Journal**: None
- **Summary**: This paper presents an easy and efficient face detection and face recognition approach using free software components from the internet. Face detection and face recognition problems have wide applications in home and office security. Therefore this work will helpful for those searching for a free face off-the-shelf face detection system. Using this system, faces can be detected in uncontrolled environments. In the detection phase, every individual face is detected and in the recognition phase the detected faces are compared with the faces in a given data set and recognized.



### Evaluating Text-to-Image Matching using Binary Image Selection (BISON)
- **Arxiv ID**: http://arxiv.org/abs/1901.06595v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/1901.06595v2)
- **Published**: 2019-01-19 22:12:01+00:00
- **Updated**: 2019-04-05 16:34:48+00:00
- **Authors**: Hexiang Hu, Ishan Misra, Laurens van der Maaten
- **Comment**: None
- **Journal**: None
- **Summary**: Providing systems the ability to relate linguistic and visual content is one of the hallmarks of computer vision. Tasks such as text-based image retrieval and image captioning were designed to test this ability but come with evaluation measures that have a high variance or are difficult to interpret. We study an alternative task for systems that match text and images: given a text query, the system is asked to select the image that best matches the query from a pair of semantically similar images. The system's accuracy on this Binary Image SelectiON (BISON) task is interpretable, eliminates the reliability problems of retrieval evaluations, and focuses on the system's ability to understand fine-grained visual structure. We gather a BISON dataset that complements the COCO dataset and use it to evaluate modern text-based image retrieval and image captioning systems. Our results provide novel insights into the performance of these systems. The COCO-BISON dataset and corresponding evaluation code are publicly available from \url{http://hexianghu.com/bison/}.



