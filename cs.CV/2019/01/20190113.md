# Arxiv Papers in cs.CV on 2019-01-13
### LiFF: Light Field Features in Scale and Depth
- **Arxiv ID**: http://arxiv.org/abs/1901.03916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03916v1)
- **Published**: 2019-01-13 00:03:56+00:00
- **Updated**: 2019-01-13 00:03:56+00:00
- **Authors**: Donald G. Dansereau, Bernd Girod, Gordon Wetzstein
- **Comment**: None
- **Journal**: None
- **Summary**: Feature detectors and descriptors are key low-level vision tools that many higher-level tasks build on. Unfortunately these fail in the presence of challenging light transport effects including partial occlusion, low contrast, and reflective or refractive surfaces. Building on spatio-angular imaging modalities offered by emerging light field cameras, we introduce a new and computationally efficient 4D light field feature detector and descriptor: LiFF. LiFF is scale invariant and utilizes the full 4D light field to detect features that are robust to changes in perspective. This is particularly useful for structure from motion (SfM) and other tasks that match features across viewpoints of a scene. We demonstrate significantly improved 3D reconstructions via SfM when using LiFF instead of the leading 2D or 4D features, and show that LiFF runs an order of magnitude faster than the leading 4D approach. Finally, LiFF inherently estimates depth for each feature, opening a path for future research in light field-based SfM.



### Generalizing Fingerprint Spoof Detector: Learning a One-Class Classifier
- **Arxiv ID**: http://arxiv.org/abs/1901.03918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03918v2)
- **Published**: 2019-01-13 00:29:06+00:00
- **Updated**: 2019-04-05 19:30:03+00:00
- **Authors**: Joshua J. Engelsma, Anil K. Jain
- **Comment**: None
- **Journal**: None
- **Summary**: Prevailing fingerprint recognition systems are vulnerable to spoof attacks. To mitigate these attacks, automated spoof detectors are trained to distinguish a set of live or bona fide fingerprints from a set of known spoof fingerprints. Despite their success, spoof detectors remain vulnerable when exposed to attacks from spoofs made with materials not seen during training of the detector. To alleviate this shortcoming, we approach spoof detection as a one-class classification problem. The goal is to train a spoof detector on only the live fingerprints such that once the concept of "live" has been learned, spoofs of any material can be rejected. We accomplish this through training multiple generative adversarial networks (GANS) on live fingerprint images acquired with the open source, dual-camera, 1900 ppi RaspiReader fingerprint reader. Our experimental results, conducted on 5.5K spoof images (from 12 materials) and 11.8K live images show that the proposed approach improves the cross-material spoof detection performance over state-of-the-art one-class and binary class spoof detectors on 11 of 12 testing materials and 7 of 12 testing materials, respectively.



### Image retrieval method based on CNN and dimension reduction
- **Arxiv ID**: http://arxiv.org/abs/1901.03924v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03924v1)
- **Published**: 2019-01-13 02:29:27+00:00
- **Updated**: 2019-01-13 02:29:27+00:00
- **Authors**: Zhihao Cao, Shaomin Mu, Yongyu Xu, Mengping Dong
- **Comment**: 2018 International Conference on Security, Pattern Analysis, and
  Cybernetics(SPAC 2018)
- **Journal**: None
- **Summary**: An image retrieval method based on convolution neural network and dimension reduction is proposed in this paper. Convolution neural network is used to extract high-level features of images, and to solve the problem that the extracted feature dimensions are too high and have strong correlation, multilinear principal component analysis is used to reduce the dimension of features. The features after dimension reduction are binary hash coded for fast image retrieval. Experiments show that the method proposed in this paper has better retrieval effect than the retrieval method based on principal component analysis on the e-commerce image datasets.



### Light-Field for RF
- **Arxiv ID**: http://arxiv.org/abs/1901.03953v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/1901.03953v1)
- **Published**: 2019-01-13 09:28:54+00:00
- **Updated**: 2019-01-13 09:28:54+00:00
- **Authors**: Manikanta Kotaru, Guy Satat, Ramesh Raskar, Sachin Katti
- **Comment**: None
- **Journal**: None
- **Summary**: Most computer vision systems and computational photography systems are visible light based which is a small fraction of the electromagnetic (EM) spectrum. In recent years radio frequency (RF) hardware has become more widely available, for example, many cars are equipped with a RADAR, and almost every home has a WiFi device. In the context of imaging, RF spectrum holds many advantages compared to visible light systems. In particular, in this regime, EM energy effectively interacts in different ways with matter. This property allows for many novel applications such as privacy preserving computer vision and imaging through absorbing and scattering materials in visible light such as walls. Here, we expand many of the concepts in computational photography in visible light to RF cameras. The main limitation of imaging with RF is the large wavelength that limits the imaging resolution when compared to visible light. However, the output of RF cameras is usually processed by computer vision and perception algorithms which would benefit from multi-modal sensing of the environment, and from sensing in situations in which visible light systems fail. To bridge the gap between computational photography and RF imaging, we expand the concept of light-field to RF. This work paves the way to novel computational sensing systems with RF.



### Auto-Retoucher(ART) - A framework for Background Replacement and Image Editing
- **Arxiv ID**: http://arxiv.org/abs/1901.03954v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03954v1)
- **Published**: 2019-01-13 09:51:08+00:00
- **Updated**: 2019-01-13 09:51:08+00:00
- **Authors**: Yunxuan Xiao, Yikai Li, Yuwei Wu, Lizhen Zhu
- **Comment**: 5 pages, 4 figures
- **Journal**: None
- **Summary**: Replacing the background and simultaneously adjusting foreground objects is a challenging task in image editing. Current techniques for generating such images relies heavily on user interactions with image editing softwares, which is a tedious job for professional retouchers. To reduce their workload, some exciting progress has been made on generating images with a given background. However, these models can neither adjust the position and scale of the foreground objects, nor guarantee the semantic consistency between foreground and background. To overcome these limitations, we propose a framework -- ART(Auto-Retoucher), to generate images with sufficient semantic and spatial consistency. Images are first processed by semantic matting and scene parsing modules, then a multi-task verifier model will give two confidence scores for the current background and position setting. We demonstrate that our jointly optimized verifier model successfully improves the visual consistency, and our ART framework performs well on images with the human body as foregrounds.



### Introducing a Generative Adversarial Network Model for Lagrangian Trajectory Simulation
- **Arxiv ID**: http://arxiv.org/abs/1901.03960v1
- **DOI**: None
- **Categories**: **stat.ML**, cs.CV, cs.LG, physics.data-an, stat.CO
- **Links**: [PDF](http://arxiv.org/pdf/1901.03960v1)
- **Published**: 2019-01-13 11:06:34+00:00
- **Updated**: 2019-01-13 11:06:34+00:00
- **Authors**: Jingwei Gan, Pai Liu, Rajan K. Chakrabarty
- **Comment**: 19 pages, 9 figures
- **Journal**: None
- **Summary**: We introduce a generative adversarial network (GAN) model to simulate the 3-dimensional Lagrangian motion of particles trapped in the recirculation zone of a buoyancy-opposed flame. The GAN model comprises a stochastic recurrent neural network, serving as a generator, and a convoluted neural network, serving as a discriminator. Adversarial training was performed to the point where the best-trained discriminator failed to distinguish the ground truth from the trajectory produced by the best-trained generator. The model performance was then benchmarked against a statistical analysis performed on both the simulated trajectories and the ground truth, with regard to the accuracy and generalization criteria.



### DCNN-GAN: Reconstructing Realistic Image from fMRI
- **Arxiv ID**: http://arxiv.org/abs/1901.07368v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.07368v1)
- **Published**: 2019-01-13 12:34:37+00:00
- **Updated**: 2019-01-13 12:34:37+00:00
- **Authors**: Yunfeng Lin, Jiangbei Li, Hanjing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Visualizing the perceptual content by analyzing human functional magnetic resonance imaging (fMRI) has been an active research area. However, due to its high dimensionality, complex dimensional structure, and small number of samples available, reconstructing realistic images from fMRI remains challenging. Recently with the development of convolutional neural network (CNN) and generative adversarial network (GAN), mapping multi-voxel fMRI data to complex, realistic images has been made possible. In this paper, we propose a model, DCNN-GAN, by combining a reconstruction network and GAN. We utilize the CNN for hierarchical feature extraction and the DCNN-GAN to reconstruct more realistic images. Extensive experiments have been conducted, showing that our method outperforms previous works, regarding reconstruction quality and computational cost.



### Generating Adversarial Perturbation with Root Mean Square Gradient
- **Arxiv ID**: http://arxiv.org/abs/1901.03706v5
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.03706v5)
- **Published**: 2019-01-13 12:42:42+00:00
- **Updated**: 2019-05-17 06:39:19+00:00
- **Authors**: Yatie Xiao, Chi-Man Pun, Jizhe Zhou
- **Comment**: The formula in Algorithm 1 lacks important representations
- **Journal**: None
- **Summary**: We focus our attention on the problem of generating adversarial perturbations based on the gradient in image classification domain



### RNN-based Generative Model for Fine-Grained Sketching
- **Arxiv ID**: http://arxiv.org/abs/1901.03991v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.03991v1)
- **Published**: 2019-01-13 14:23:44+00:00
- **Updated**: 2019-01-13 14:23:44+00:00
- **Authors**: Andrin Jenal, Nikolay Savinov, Torsten Sattler, Gaurav Chaurasia
- **Comment**: Includes supplemental material. Link to datasets to be added shortly
- **Journal**: None
- **Summary**: Deep generative models have shown great promise when it comes to synthesising novel images. While they can generate images that look convincing on a higher-level, generating fine-grained details is still a challenge. In order to foster research on more powerful generative approaches, this paper proposes a novel task: generative modelling of 2D tree skeletons. Trees are an interesting shape class because they exhibit complexity and variations that are well-suited to measure the ability of a generative model to generated detailed structures. We propose a new dataset for this task and demonstrate that state-of-the-art generative models fail to synthesise realistic images on our benchmark, even though they perform well on current datasets like MNIST digits. Motivated by these results, we propose a novel network architecture based on combining a variational autoencoder using Recurrent Neural Networks and a convolutional discriminator. The network, error metrics and training procedure are adapted to the task of fine-grained sketching. Through quantitative and perceptual experiments, we show that our model outperforms previous work and that our dataset is a valuable benchmark for generative models. We will make our dataset publicly available.



### A Machine-Synesthetic Approach To DDoS Network Attack Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.04017v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.CR, cs.NI, 62H30, 62H35, 94C99, 15A04, C.2.m; I.5.0; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/1901.04017v2)
- **Published**: 2019-01-13 17:01:46+00:00
- **Updated**: 2019-03-22 06:38:48+00:00
- **Authors**: Yuri Monakhov, Oleg Nikitin, Anna Kuznetsova, Alexey Kharlamov, Alexandr Amochkin
- **Comment**: 12 pages, 2 figures, 5 tables. Accepted to the Intelligent Systems
  Conference (IntelliSys) 2019
- **Journal**: None
- **Summary**: In the authors' opinion, anomaly detection systems, or ADS, seem to be the most perspective direction in the subject of attack detection, because these systems can detect, among others, the unknown (zero-day) attacks. To detect anomalies, the authors propose to use machine synesthesia. In this case, machine synesthesia is understood as an interface that allows using image classification algorithms in the problem of detecting network anomalies, making it possible to use non-specialized image detection methods that have recently been widely and actively developed. The proposed approach is that the network traffic data is "projected" into the image. It can be seen from the experimental results that the proposed method for detecting anomalies shows high results in the detection of attacks. On a large sample, the value of the complex efficiency indicator reaches 97%.



### Neumann Networks for Inverse Problems in Imaging
- **Arxiv ID**: http://arxiv.org/abs/1901.03707v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.03707v2)
- **Published**: 2019-01-13 17:44:22+00:00
- **Updated**: 2019-06-04 00:25:31+00:00
- **Authors**: Davis Gilton, Greg Ongie, Rebecca Willett
- **Comment**: Added further experiments, reorganized proof section, added further
  references and supporting figures
- **Journal**: None
- **Summary**: Many challenging image processing tasks can be described by an ill-posed linear inverse problem: deblurring, deconvolution, inpainting, compressed sensing, and superresolution all lie in this framework. Traditional inverse problem solvers minimize a cost function consisting of a data-fit term, which measures how well an image matches the observations, and a regularizer, which reflects prior knowledge and promotes images with desirable properties like smoothness. Recent advances in machine learning and image processing have illustrated that it is often possible to learn a regularizer from training data that can outperform more traditional regularizers. We present an end-to-end, data-driven method of solving inverse problems inspired by the Neumann series, which we call a Neumann network. Rather than unroll an iterative optimization algorithm, we truncate a Neumann series which directly solves the linear inverse problem with a data-driven nonlinear regularizer. The Neumann network architecture outperforms traditional inverse problem solution methods, model-free deep learning approaches, and state-of-the-art unrolled iterative methods on standard datasets. Finally, when the images belong to a union of subspaces and under appropriate assumptions on the forward model, we prove there exists a Neumann network configuration that well-approximates the optimal oracle estimator for the inverse problem and demonstrate empirically that the trained Neumann network has the form predicted by theory.



### The Liver Tumor Segmentation Benchmark (LiTS)
- **Arxiv ID**: http://arxiv.org/abs/1901.04056v2
- **DOI**: 10.1016/j.media.2022.102680
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04056v2)
- **Published**: 2019-01-13 20:38:16+00:00
- **Updated**: 2022-11-25 09:24:35+00:00
- **Authors**: Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, Fabian Lohöfer, Julian Walter Holch, Wieland Sommer, Felix Hofmann, Alexandre Hostettler, Naama Lev-Cohain, Michal Drozdzal, Michal Marianne Amitai, Refael Vivantik, Jacob Sosna, Ivan Ezhov, Anjany Sekuboyina, Fernando Navarro, Florian Kofler, Johannes C. Paetzold, Suprosanna Shit, Xiaobin Hu, Jana Lipková, Markus Rempfler, Marie Piraud, Jan Kirschke, Benedikt Wiestler, Zhiheng Zhang, Christian Hülsemeyer, Marcel Beetz, Florian Ettlinger, Michela Antonelli, Woong Bae, Míriam Bellver, Lei Bi, Hao Chen, Grzegorz Chlebus, Erik B. Dam, Qi Dou, Chi-Wing Fu, Bogdan Georgescu, Xavier Giró-i-Nieto, Felix Gruen, Xu Han, Pheng-Ann Heng, Jürgen Hesser, Jan Hendrik Moltz, Christian Igel, Fabian Isensee, Paul Jäger, Fucang Jia, Krishna Chaitanya Kaluva, Mahendra Khened, Ildoo Kim, Jae-Hun Kim, Sungwoong Kim, Simon Kohl, Tomasz Konopczynski, Avinash Kori, Ganapathy Krishnamurthi, Fan Li, Hongchao Li, Junbo Li, Xiaomeng Li, John Lowengrub, Jun Ma, Klaus Maier-Hein, Kevis-Kokitsi Maninis, Hans Meine, Dorit Merhof, Akshay Pai, Mathias Perslev, Jens Petersen, Jordi Pont-Tuset, Jin Qi, Xiaojuan Qi, Oliver Rippel, Karsten Roth, Ignacio Sarasua, Andrea Schenk, Zengming Shen, Jordi Torres, Christian Wachinger, Chunliang Wang, Leon Weninger, Jianrong Wu, Daguang Xu, Xiaoping Yang, Simon Chun-Ho Yu, Yading Yuan, Miao Yu, Liping Zhang, Jorge Cardoso, Spyridon Bakas, Rickmer Braren, Volker Heinemann, Christopher Pal, An Tang, Samuel Kadoury, Luc Soler, Bram van Ginneken, Hayit Greenspan, Leo Joskowicz, Bjoern Menze
- **Comment**: Patrick Bilic, Patrick Christ, Hongwei Bran Li, and Eugene Vorontsov
  made equal contributions to this work. Published in Medical Image Analysis
- **Journal**: Medical Image Analysis (2022) Pg. 102680
- **Summary**: In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LiTS), which was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 and the International Conferences on Medical Image Computing and Computer-Assisted Intervention (MICCAI) 2017 and 2018. The image dataset is diverse and contains primary and secondary tumors with varied sizes and appearances with various lesion-to-background levels (hyper-/hypo-dense), created in collaboration with seven hospitals and research institutions. Seventy-five submitted liver and liver tumor segmentation algorithms were trained on a set of 131 computed tomography (CT) volumes and were tested on 70 unseen test images acquired from different patients. We found that not a single algorithm performed best for both liver and liver tumors in the three events. The best liver segmentation algorithm achieved a Dice score of 0.963, whereas, for tumor segmentation, the best algorithms achieved Dices scores of 0.674 (ISBI 2017), 0.702 (MICCAI 2017), and 0.739 (MICCAI 2018). Retrospectively, we performed additional analysis on liver tumor detection and revealed that not all top-performing segmentation algorithms worked well for tumor detection. The best liver tumor detection method achieved a lesion-wise recall of 0.458 (ISBI 2017), 0.515 (MICCAI 2017), and 0.554 (MICCAI 2018), indicating the need for further research. LiTS remains an active benchmark and resource for research, e.g., contributing the liver-related segmentation tasks in \url{http://medicaldecathlon.com/}. In addition, both data and online evaluation are accessible via \url{www.lits-challenge.com}.



### GAN-based Virtual Re-Staining: A Promising Solution for Whole Slide Image Analysis
- **Arxiv ID**: http://arxiv.org/abs/1901.04059v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04059v2)
- **Published**: 2019-01-13 20:54:39+00:00
- **Updated**: 2022-07-09 00:29:37+00:00
- **Authors**: Zhaoyang Xu, Xingru Huang, Carlos Fernández Moro, Béla Bozóky, Qianni Zhang
- **Comment**: None
- **Journal**: None
- **Summary**: Histopathological cancer diagnosis is based on visual examination of stained tissue slides. Hematoxylin and eosin (H\&E) is a standard stain routinely employed worldwide. It is easy to acquire and cost effective, but cells and tissue components show low-contrast with varying tones of dark blue and pink, which makes difficult visual assessments, digital image analysis, and quantifications. These limitations can be overcome by IHC staining of target proteins of the tissue slide. IHC provides a selective, high-contrast imaging of cells and tissue components, but their use is largely limited by a significantly more complex laboratory processing and high cost. We proposed a conditional CycleGAN (cCGAN) network to transform the H\&E stained images into IHC stained images, facilitating virtual IHC staining on the same slide. This data-driven method requires only a limited amount of labelled data but will generate pixel level segmentation results. The proposed cCGAN model improves the original network \cite{zhu_unpaired_2017} by adding category conditions and introducing two structural loss functions, which realize a multi-subdomain translation and improve the translation accuracy as well. % need to give reasons here. Experiments demonstrate that the proposed model outperforms the original method in unpaired image translation with multi-subdomains. We also explore the potential of unpaired images to image translation method applied on other histology images related tasks with different staining techniques.



### Vehicles Detection Based on Background Modeling
- **Arxiv ID**: http://arxiv.org/abs/1901.04077v1
- **DOI**: 10.14445/22315381/IJETT-V66P216
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.04077v1)
- **Published**: 2019-01-13 22:41:18+00:00
- **Updated**: 2019-01-13 22:41:18+00:00
- **Authors**: Mohamed Shehata, Reda Abo-Al-Ez, Farid Zaghlool, Mohamed Taha Abou-Kreisha
- **Comment**: 4 pages, 4 figures
- **Journal**: International Journal of Engineering Trends and Technology 66.2
  (2018): 92-95
- **Summary**: Background image subtraction algorithm is a common approach which detects moving objects in a video sequence by finding the significant difference between the video frames and the static background model. This paper presents a developed system which achieves vehicle detection by using background image subtraction algorithm based on blocks followed by deep learning data validation algorithm. The main idea is to segment the image into equal size blocks, to model the static reference background image (SRBI), by calculating the variance between each block pixels and each counterpart block pixels in the adjacent frame, the system implemented into four different methods: Absolute Difference, Image Entropy, Exclusive OR (XOR) and Discrete Cosine Transform (DCT). The experimental results showed that the DCT method has the highest vehicle detection accuracy.



### Residual-CNDS for Grand Challenge Scene Dataset
- **Arxiv ID**: http://arxiv.org/abs/1902.10030v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1902.10030v1)
- **Published**: 2019-01-13 23:00:11+00:00
- **Updated**: 2019-01-13 23:00:11+00:00
- **Authors**: Hussein A. Al-Barazanchi, Hussam Qassim, David Feinzimer, Abhishek Verma
- **Comment**: None
- **Journal**: None
- **Summary**: Increasing depth of convolutional neural networks (CNNs) is a highly promising method of increasing the accuracy of the (CNNs). Increased CNN depth will also result in increased layer count (parameters), leading to a slow backpropagation convergence prone to overfitting. We trained our model (Residual-CNDS) to classify very large-scale scene datasets MIT Places 205, and MIT Places 365-Standard. The outcome result from the two datasets proved our proposed model (Residual-CNDS) effectively handled the slow convergence, overfitting, and degradation. CNNs that include deep supervision (CNDS) add supplementary branches to the deep convolutional neural network in specified layers by calculating vanishing, effectively addressing delayed convergence and overfitting. Nevertheless, (CNDS) does not resolve degradation; hence, we add residual learning to the (CNDS) in certain layers after studying the best place in which to add it. With this approach we overcome degradation in the very deep network. We have built two models (Residual-CNDS 8), and (Residual-CNDS 10). Moreover, we tested our models on two large-scale datasets, and we compared our results with other recently introduced cutting-edge networks in the domain of top-1 and top-5 classification accuracy. As a result, both of models have shown good improvement, which supports the assertion that the addition of residual connections enhances network CNDS accuracy without adding any computation complexity.



