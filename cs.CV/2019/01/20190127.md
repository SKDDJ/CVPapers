# Arxiv Papers in cs.CV on 2019-01-27
### Fixup Initialization: Residual Learning Without Normalization
- **Arxiv ID**: http://arxiv.org/abs/1901.09321v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.09321v2)
- **Published**: 2019-01-27 05:30:11+00:00
- **Updated**: 2019-03-12 00:31:18+00:00
- **Authors**: Hongyi Zhang, Yann N. Dauphin, Tengyu Ma
- **Comment**: Updating reference. Accepted for publication at ICLR 2019; see
  https://openreview.net/forum?id=H1gsz30cKX
- **Journal**: None
- **Summary**: Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.



### Automated Quality Control in Image Segmentation: Application to the UK Biobank Cardiac MR Imaging Study
- **Arxiv ID**: http://arxiv.org/abs/1901.09351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09351v1)
- **Published**: 2019-01-27 10:52:34+00:00
- **Updated**: 2019-01-27 10:52:34+00:00
- **Authors**: Robert Robinson, Vanya V. Valindria, Wenjia Bai, Ozan Oktay, Bernhard Kainz, Hideaki Suzuki, Mihir M. Sanghvi, Nay Aung, Jos$Ã©$ Miguel Paiva, Filip Zemrak, Kenneth Fung, Elena Lukaschuk, Aaron M. Lee, Valentina Carapella, Young Jin Kim, Stefan K. Piechnik, Stefan Neubauer, Steffen E. Petersen, Chris Page, Paul M. Matthews, Daniel Rueckert, Ben Glocker
- **Comment**: 14 pages, 7 figures, Journal of Cardiovascular Magnetic Resonance
- **Journal**: None
- **Summary**: Background: The trend towards large-scale studies including population imaging poses new challenges in terms of quality control (QC). This is a particular issue when automatic processing tools, e.g. image segmentation methods, are employed to derive quantitative measures or biomarkers for later analyses. Manual inspection and visual QC of each segmentation isn't feasible at large scale. However, it's important to be able to automatically detect when a segmentation method fails so as to avoid inclusion of wrong measurements into subsequent analyses which could lead to incorrect conclusions. Methods: To overcome this challenge, we explore an approach for predicting segmentation quality based on Reverse Classification Accuracy, which enables us to discriminate between successful and failed segmentations on a per-cases basis. We validate this approach on a new, large-scale manually-annotated set of 4,800 cardiac magnetic resonance scans. We then apply our method to a large cohort of 7,250 cardiac MRI on which we have performed manual QC. Results: We report results used for predicting segmentation quality metrics including Dice Similarity Coefficient (DSC) and surface-distance measures. As initial validation, we present data for 400 scans demonstrating 99% accuracy for classifying low and high quality segmentations using predicted DSC scores. As further validation we show high correlation between real and predicted scores and 95% classification accuracy on 4,800 scans for which manual segmentations were available. We mimic real-world application of the method on 7,250 cardiac MRI where we show good agreement between predicted quality metrics and manual visual QC scores. Conclusions: We show that RCA has the potential for accurate and fully automatic segmentation QC on a per-case basis in the context of large-scale population imaging as in the UK Biobank Imaging Study.



### Resultant Based Incremental Recovery of Camera Pose from Pairwise Matches
- **Arxiv ID**: http://arxiv.org/abs/1901.09364v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09364v1)
- **Published**: 2019-01-27 12:13:51+00:00
- **Updated**: 2019-01-27 12:13:51+00:00
- **Authors**: Yoni Kasten, Meirav Galun, Ronen Basri
- **Comment**: None
- **Journal**: None
- **Summary**: Incremental (online) structure from motion pipelines seek to recover the camera matrix associated with an image $I_n$ given $n-1$ images, $I_1,...,I_{n-1}$, whose camera matrices have already been recovered. In this paper, we introduce a novel solution to the six-point online algorithm to recover the exterior parameters associated with $I_n$. Our algorithm uses just six corresponding pairs of 2D points, extracted each from $I_n$ and from \textit{any} of the preceding $n-1$ images, allowing the recovery of the full six degrees of freedom of the $n$'th camera, and unlike common methods, does not require tracking feature points in three or more images. Our novel solution is based on constructing a Dixon resultant, yielding a solution method that is both efficient and accurate compared to existing solutions. We further use Bernstein's theorem to prove a tight bound on the number of complex solutions. Our experiments demonstrate the utility of our approach.



### Fast and Efficient Lenslet Image Compression
- **Arxiv ID**: http://arxiv.org/abs/1901.11396v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.11396v1)
- **Published**: 2019-01-27 12:14:58+00:00
- **Updated**: 2019-01-27 12:14:58+00:00
- **Authors**: Hadi Amirpour, Antonio Pinheiro, Manuela Pereira, Mohammad Ghanbari
- **Comment**: None
- **Journal**: None
- **Summary**: Light field imaging is characterized by capturing brightness, color, and directional information of light rays in a scene. This leads to image representations with huge amount of data that require efficient coding schemes. In this paper, lenslet images are rendered into sub-aperture images. These images are organized as a pseudo-sequence input for the HEVC video codec. To better exploit redundancy among the neighboring sub-aperture images and consequently decrease the distances between a sub-aperture image and its references used for prediction, sub-aperture images are divided into four smaller groups that are scanned in a serpentine order. The most central sub-aperture image, which has the highest similarity to all the other images, is used as the initial reference image for each of the four regions. Furthermore, a structure is defined that selects spatially adjacent sub-aperture images as prediction references with the highest similarity to the current image. In this way, encoding efficiency increases, and furthermore it leads to a higher similarity among the co-located Coding Three Units (CTUs). The similarities among the co-located CTUs are exploited to predict Coding Unit depths.Moreover, independent encoding of each group division enables parallel processing, that along with the proposed coding unit depth prediction decrease the encoding execution time by almost 80% on average. Simulation results show that Rate-Distortion performance of the proposed method has higher compression gain than the other state-of-the-art lenslet compression methods with lower computational complexity.



### 6D Object Pose Estimation Based on 2D Bounding Box
- **Arxiv ID**: http://arxiv.org/abs/1901.09366v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09366v1)
- **Published**: 2019-01-27 12:40:04+00:00
- **Updated**: 2019-01-27 12:40:04+00:00
- **Authors**: Jin Liu, Sheng He
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we present a simple but powerful method to tackle the problem of estimating the 6D pose of objects from a single RGB image. Our system trains a novel convolutional neural network to regress the unit quaternion, which represents the 3D rotation, from the partial image inside the bounding box returned by 2D detection systems. Then we propose an algorithm we call Bounding Box Equation to efficiently and accurately obtain the 3D translation, using 3D rotation and 2D bounding box. Considering that the quadratic sum of the quaternion's four elements equals to one, we add a normalization layer to keep the network's output on the unit sphere and put forward a special loss function for unit quaternion regression. We evaluate our method on the LineMod dataset and experiment shows that our approach outperforms base-line and some state of the art methods.



### NeuralSampler: Euclidean Point Cloud Auto-Encoder and Sampler
- **Arxiv ID**: http://arxiv.org/abs/1901.09394v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09394v1)
- **Published**: 2019-01-27 15:38:49+00:00
- **Updated**: 2019-01-27 15:38:49+00:00
- **Authors**: Edoardo Remelli, Pierre Baque, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Most algorithms that rely on deep learning-based approaches to generate 3D point sets can only produce clouds containing fixed number of points. Furthermore, they typically require large networks parameterized by many weights, which makes them hard to train. In this paper, we propose an auto-encoder architecture that can both encode and decode clouds of arbitrary size and demonstrate its effectiveness at upsampling sparse point clouds. Interestingly, we can do so using less than half as many parameters as state-of-the-art architectures while still delivering better performance. We will make our code base fully available.



### Monocular Depth Estimation: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1901.09402v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09402v1)
- **Published**: 2019-01-27 16:46:23+00:00
- **Updated**: 2019-01-27 16:46:23+00:00
- **Authors**: Amlaan Bhoi
- **Comment**: 8 pages, 1 figure, 4 tables
- **Journal**: None
- **Summary**: Monocular depth estimation is often described as an ill-posed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3Dobject recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.



### Spatio-temporal Action Recognition: A Survey
- **Arxiv ID**: http://arxiv.org/abs/1901.09403v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09403v1)
- **Published**: 2019-01-27 16:53:55+00:00
- **Updated**: 2019-01-27 16:53:55+00:00
- **Authors**: Amlaan Bhoi
- **Comment**: 15 pages, 7 figures, 6 tables
- **Journal**: None
- **Summary**: The task of action recognition or action detection involves analyzing videos and determining what action or motion is being performed. The primary subject of these videos are predominantly humans performing some action. However, this requirement can be relaxed to generalize over other subjects such as animals or robots. The applications can range from anywhere between human-computer inter-action to automated video editing proposals. When we consider spatiotemporal action recognition, we deal with action localization. This task not only involves determining what action is being performed but also when and where itis being performed in said video. This paper aims to survey the plethora of approaches and algorithms attempted to solve this task, give a comprehensive comparison between them, explore various datasets available for the problem, and determine the most promising approaches.



### 3D Contouring for Breast Tumor in Sonography
- **Arxiv ID**: http://arxiv.org/abs/1901.09407v2
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV, 68U10
- **Links**: [PDF](http://arxiv.org/pdf/1901.09407v2)
- **Published**: 2019-01-27 17:40:30+00:00
- **Updated**: 2020-09-07 16:56:15+00:00
- **Authors**: Dar-Ren Chen, Yu-Chih Lin, Yu-Len Huang
- **Comment**: 18 pages, 1 table and 5 figures
- **Journal**: None
- **Summary**: Malignant and benign breast tumors present differently in their shape and size on sonography. Morphological information provided by tumor contours are important in clinical diagnosis. However, ultrasound images contain noises and tissue texture; clinical diagnosis thus highly depends on the experience of physicians. The manual way to sketch three-dimensional (3D) contours of breast tumor is a time-consuming and complicate task. If automatic contouring could provide a precise breast tumor contour that might assist physicians in making an accurate diagnosis. This study presents an efficient method for automatically contouring breast tumors in 3D sonography. The proposed method utilizes an efficient segmentation procedure, i.e. level-set method (LSM), to automatic detect contours of breast tumors. This study evaluates 20 cases comprising ten benign and ten malignant tumors. The results of computer simulation reveal that the proposed 3D segmentation method provides robust contouring for breast tumor on ultrasound images. This approach consistently obtains contours similar to those obtained by manual contouring of the breast tumor and can save much of the time required to sketch precise contours.



### Degraded Historical Documents Images Binarization Using a Combination of Enhanced Techniques
- **Arxiv ID**: http://arxiv.org/abs/1901.09425v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09425v1)
- **Published**: 2019-01-27 19:37:52+00:00
- **Updated**: 2019-01-27 19:37:52+00:00
- **Authors**: Omar Boudraa, Walid Khaled Hidouci, Dominique Michelucci
- **Comment**: None
- **Journal**: None
- **Summary**: Document image binarization is the initial step and a crucial in many document analysis and recognition scheme. In fact, it is still a relevant research subject and a fundamental challenge due to its importance and influence. This paper provides an original multi-phases system that hybridizes various efficient image thresholding methods in order to get the best binarization output. First, to improve contrast in particularly defective images, the application of CLAHE algorithm is suggested and justified. We then use a cooperative technique to segment image into two separated classes. At the end, a special transformation is applied for the purpose of removing scattered noise and of correcting characters forms. Experimentations demonstrate the precision and the robustness of our framework applied on historical degraded documents images within three benchmarks compared to other noted methods.



### Pixelated Semantic Colorization
- **Arxiv ID**: http://arxiv.org/abs/1901.10889v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.10889v2)
- **Published**: 2019-01-27 20:28:48+00:00
- **Updated**: 2019-02-07 13:12:34+00:00
- **Authors**: Jiaojiao Zhao, Jungong Han, Ling Shao, Cees G. M. Snoek
- **Comment**: None
- **Journal**: None
- **Summary**: While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from limited semantic understanding. To address this shortcoming, we propose to exploit pixelated object semantics to guide image colorization. The rationale is that human beings perceive and distinguish colors based on the semantic categories of objects. Starting from an autoregressive model, we generate image color distributions, from which diverse colored results are sampled. We propose two ways to incorporate object semantics into the colorization model: through a pixelated semantic embedding and a pixelated semantic generator. Specifically, the proposed convolutional neural network includes two branches. One branch learns what the object is, while the other branch learns the object colors. The network jointly optimizes a color embedding loss, a semantic segmentation loss and a color generation loss, in an end-to-end fashion. Experiments on PASCAL VOC2012 and COCO-stuff reveal that our network, when trained with semantic segmentation labels, produces more realistic and finer results compared to the colorization state-of-the-art.



### Open Source Face Recognition Performance Evaluation Package
- **Arxiv ID**: http://arxiv.org/abs/1901.09447v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.09447v1)
- **Published**: 2019-01-27 21:53:26+00:00
- **Updated**: 2019-01-27 21:53:26+00:00
- **Authors**: Xiang Xu, Ioannis A. Kakadiaris
- **Comment**: Technical report
- **Journal**: None
- **Summary**: Biometrics-related research has been accelerated significantly by deep learning technology. However, there are limited open-source resources to help researchers evaluate their deep learning-based biometrics algorithms efficiently, especially for the face recognition tasks. In this work, we design and implement a light-weight, maintainable, scalable, generalizable, and extendable face recognition evaluation toolbox named FaRE that supports both online and offline evaluation to provide feedback to algorithm development and accelerate biometrics-related research. FaRE consists of a set of evaluation metric functions and provides various APIs for commonly-used face recognition datasets including LFW, CFP, UHDB31, and IJB-series datasets, which can be easily extended to include other customized datasets. The package and the pre-trained baseline models will be released for public academic research use after obtaining university approval.



### Learning Transformation Synchronization
- **Arxiv ID**: http://arxiv.org/abs/1901.09458v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.09458v2)
- **Published**: 2019-01-27 23:09:21+00:00
- **Updated**: 2019-06-04 07:14:07+00:00
- **Authors**: Xiangru Huang, Zhenxiao Liang, Xiaowei Zhou, Yao Xie, Leonidas Guibas, Qixing Huang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing the 3D model of a physical object typically requires us to align the depth scans obtained from different camera poses into the same coordinate system. Solutions to this global alignment problem usually proceed in two steps. The first step estimates relative transformations between pairs of scans using an off-the-shelf technique. Due to limited information presented between pairs of scans, the resulting relative transformations are generally noisy. The second step then jointly optimizes the relative transformations among all input depth scans. A natural constraint used in this step is the cycle-consistency constraint, which allows us to prune incorrect relative transformations by detecting inconsistent cycles. The performance of such approaches, however, heavily relies on the quality of the input relative transformations. Instead of merely using the relative transformations as the input to perform transformation synchronization, we propose to use a neural network to learn the weights associated with each relative transformation. Our approach alternates between transformation synchronization using weighted relative transformations and predicting new weights of the input relative transformations using a neural network. We demonstrate the usefulness of this approach across a wide range of datasets.



### TUNet: Incorporating segmentation maps to improve classification
- **Arxiv ID**: http://arxiv.org/abs/1901.11379v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.11379v1)
- **Published**: 2019-01-27 23:58:03+00:00
- **Updated**: 2019-01-27 23:58:03+00:00
- **Authors**: Yijun Tian
- **Comment**: None
- **Journal**: None
- **Summary**: Determining the localization of specific protein in human cells is important for understanding cellular functions and biological processes of underlying diseases. Among imaging techniques, high-throughput fluorescence microscopy imaging is an efficient biotechnology to stain the protein of interest in a cell. In this work, we present a novel classification model Twin U-Net (TUNet) for processing and classifying the belonging of protein in the Atlas images. Several notable Deep Learning models including GoogleNet and Resnet have been employed for comparison. Results have shown that our system obtaining competitive performance.



