# Arxiv Papers in cs.CV on 2019-01-16
### DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation
- **Arxiv ID**: http://arxiv.org/abs/1901.05103v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05103v1)
- **Published**: 2019-01-16 01:21:27+00:00
- **Updated**: 2019-01-16 01:21:27+00:00
- **Authors**: Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove
- **Comment**: None
- **Journal**: None
- **Summary**: Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.



### A Comprehensive Performance Evaluation for 3D Transformation Estimation Techniques
- **Arxiv ID**: http://arxiv.org/abs/1901.05104v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05104v1)
- **Published**: 2019-01-16 01:46:46+00:00
- **Updated**: 2019-01-16 01:46:46+00:00
- **Authors**: Bao Zhao, Xiaobo Chen, Xinyi Le, Juntong Xi
- **Comment**: None
- **Journal**: None
- **Summary**: 3D local feature extraction and matching is the basis for solving many tasks in the area of computer vision, such as 3D registration, modeling, recognition and retrieval. However, this process commonly draws into false correspondences, due to noise, limited features, occlusion, incomplete surface and etc. In order to estimate accurate transformation based on these corrupted correspondences, numerous transformation estimation techniques have been proposed. However, the merits, demerits and appropriate application for these methods are unclear owing to that no comprehensive evaluation for the performance of these methods has been conducted. This paper evaluates eleven state-of-the-art transformation estimation proposals on both descriptor based and synthetic correspondences. On descriptor based correspondences, several evaluation items (including the performance on different datasets, robustness to different overlap ratios and the performance of these technique combined with Iterative Closest Point (ICP), different local features and LRF/A techniques) of these methods are tested on four popular datasets acquired with different devices. On synthetic correspondences, the robustness of these methods to varying percentages of correct correspondences (PCC) is evaluated. In addition, we also evaluate the efficiencies of these methods. Finally, the merits, demerits and application guidance of these tested transformation estimation methods are summarized.



### Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections
- **Arxiv ID**: http://arxiv.org/abs/1901.05105v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.05105v2)
- **Published**: 2019-01-16 01:46:57+00:00
- **Updated**: 2019-03-06 03:31:12+00:00
- **Authors**: Xin Huang, Stephen McGill, Brian C. Williams, Luke Fletcher, Guy Rosman
- **Comment**: Accepted at ICRA'19. 8 pages, 9 figures, 1 table. Video at
  https://youtu.be/clR08hRdtlM
- **Journal**: None
- **Summary**: Predicting the motion of a driver's vehicle is crucial for advanced driving systems, enabling detection of potential risks towards shared control between the driver and automation systems. In this paper, we propose a variational neural network approach that predicts future driver trajectory distributions for the vehicle based on multiple sensors. Our predictor generates both a conditional variational distribution of future trajectories, as well as a confidence estimate for different time horizons. Our approach allows us to handle inherently uncertain situations, and reason about information gain from each input, as well as combine our model with additional predictors, creating a mixture of experts. We show how to augment the variational predictor with a physics-based predictor, and based on their confidence estimations, improve overall system performance. The resulting combined model is aware of the uncertainty associated with its predictions, which can help the vehicle autonomy to make decisions with more confidence. The model is validated on real-world urban driving data collected in multiple locations. This validation demonstrates that our approach improves the prediction error of a physics-based model by 25% while successfully identifying the uncertain cases with 82% accuracy.



### Actions Speak Louder Than (Pass)words: Passive Authentication of Smartphone Users via Deep Temporal Features
- **Arxiv ID**: http://arxiv.org/abs/1901.05107v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/1901.05107v1)
- **Published**: 2019-01-16 01:53:48+00:00
- **Updated**: 2019-01-16 01:53:48+00:00
- **Authors**: Debayan Deb, Arun Ross, Anil K. Jain, Kwaku Prakah-Asante, K. Venkatesh Prasad
- **Comment**: None
- **Journal**: None
- **Summary**: Prevailing user authentication schemes on smartphones rely on explicit user interaction, where a user types in a passcode or presents a biometric cue such as face, fingerprint, or iris. In addition to being cumbersome and obtrusive to the users, such authentication mechanisms pose security and privacy concerns. Passive authentication systems can tackle these challenges by frequently and unobtrusively monitoring the user's interaction with the device. In this paper, we propose a Siamese Long Short-Term Memory network architecture for passive authentication, where users can be verified without requiring any explicit authentication step. We acquired a dataset comprising of measurements from 30 smartphone sensor modalities for 37 users. We evaluate our approach on 8 dominant modalities, namely, keystroke dynamics, GPS location, accelerometer, gyroscope, magnetometer, linear accelerometer, gravity, and rotation sensors. Experimental results find that, within 3 seconds, a genuine user can be correctly verified 97.15% of the time at a false accept rate of 0.1%.



### Attention-aware Multi-stroke Style Transfer
- **Arxiv ID**: http://arxiv.org/abs/1901.05127v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05127v1)
- **Published**: 2019-01-16 03:40:27+00:00
- **Updated**: 2019-01-16 03:40:27+00:00
- **Authors**: Yuan Yao, Jianqiang Ren, Xuansong Xie, Weidong Liu, Yong-Jin Liu, Jun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Neural style transfer has drawn considerable attention from both academic and industrial field. Although visual effect and efficiency have been significantly improved, existing methods are unable to coordinate spatial distribution of visual attention between the content image and stylized image, or render diverse level of detail via different brush strokes. In this paper, we tackle these limitations by developing an attention-aware multi-stroke style transfer model. We first propose to assemble self-attention mechanism into a style-agnostic reconstruction autoencoder framework, from which the attention map of a content image can be derived. By performing multi-scale style swap on content features and style features, we produce multiple feature maps reflecting different stroke patterns. A flexible fusion strategy is further presented to incorporate the salient characteristics from the attention map, which allows integrating multiple stroke patterns into different spatial regions of the output image harmoniously. We demonstrate the effectiveness of our method, as well as generate comparable stylized images with multiple stroke patterns against the state-of-the-art methods.



### Deep Supervised Hashing leveraging Quadratic Spherical Mutual Information for Content-based Image Retrieval
- **Arxiv ID**: http://arxiv.org/abs/1901.05135v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05135v1)
- **Published**: 2019-01-16 05:03:54+00:00
- **Updated**: 2019-01-16 05:03:54+00:00
- **Authors**: Nikolaos Passalis, Anastasios Tefas
- **Comment**: None
- **Journal**: None
- **Summary**: Several deep supervised hashing techniques have been proposed to allow for efficiently querying large image databases. However, deep supervised image hashing techniques are developed, to a great extent, heuristically often leading to suboptimal results. Contrary to this, we propose an efficient deep supervised hashing algorithm that optimizes the learned codes using an information-theoretic measure, the Quadratic Mutual Information (QMI). The proposed method is adapted to the needs of large-scale hashing and information retrieval leading to a novel information-theoretic measure, the Quadratic Spherical Mutual Information (QSMI). Apart from demonstrating the effectiveness of the proposed method under different scenarios and outperforming existing state-of-the-art image hashing techniques, this paper provides a structured way to model the process of information retrieval and develop novel methods adapted to the needs of each application.



### Extension of Convolutional Neural Network with General Image Processing Kernels
- **Arxiv ID**: http://arxiv.org/abs/1901.07375v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.07375v1)
- **Published**: 2019-01-16 07:44:58+00:00
- **Updated**: 2019-01-16 07:44:58+00:00
- **Authors**: Jay Hoon Jung, Yousun Shin, YoungMin Kwon
- **Comment**: 4 pages, 6 figures
- **Journal**: TENCON 2018
- **Summary**: We applied pre-defined kernels also known as filters or masks developed for image processing to convolution neural network. Instead of letting neural networks find its own kernels, we used 41 different general-purpose kernels of blurring, edge detecting, sharpening, discrete cosine transformation, etc. for the first layer of the convolution neural networks. This architecture, thus named as general filter convolutional neural network (GFNN), can reduce training time by 30% with a better accuracy compared to the regular convolutional neural network (CNN). GFNN also can be trained to achieve 90% accuracy with only 500 samples. Furthermore, even though these kernels are not specialized for the MNIST dataset, we achieved 99.56% accuracy without ensemble nor any other special algorithms.



### A Functional Representation for Graph Matching
- **Arxiv ID**: http://arxiv.org/abs/1901.05179v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05179v1)
- **Published**: 2019-01-16 08:46:21+00:00
- **Updated**: 2019-01-16 08:46:21+00:00
- **Authors**: Fu-Dong Wang, Gui-Song Xia, Nan Xue, Yipeng Zhang, Marcello Pelillo
- **Comment**: None
- **Journal**: None
- **Summary**: Graph matching is an important and persistent problem in computer vision and pattern recognition for finding node-to-node correspondence between graph-structured data. However, as widely used, graph matching that incorporates pairwise constraints can be formulated as a quadratic assignment problem (QAP), which is NP-complete and results in intrinsic computational difficulties. In this paper, we present a functional representation for graph matching (FRGM) that aims to provide more geometric insights on the problem and reduce the space and time complexities of corresponding algorithms. To achieve these goals, we represent a graph endowed with edge attributes by a linear function space equipped with a functional such as inner product or metric, that has an explicit geometric meaning. Consequently, the correspondence between graphs can be represented as a linear representation map of that functional. Specifically, we reformulate the linear functional representation map as a new parameterization for Euclidean graph matching, which is associative with geometric parameters for graphs under rigid or nonrigid deformations. This allows us to estimate the correspondence and geometric deformations simultaneously. The use of the representation of edge attributes rather than the affinity matrix enables us to reduce the space complexity by two orders of magnitudes. Furthermore, we propose an efficient optimization strategy with low time complexity to optimize the objective function. The experimental results on both synthetic and real-world datasets demonstrate that the proposed FRGM can achieve state-of-the-art performance.



### Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving Context Understanding
- **Arxiv ID**: http://arxiv.org/abs/1901.05203v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05203v1)
- **Published**: 2019-01-16 10:03:08+00:00
- **Updated**: 2019-01-16 10:03:08+00:00
- **Authors**: Liviu Marina, Bogdan Trasnea, Cocias Tiberiu, Andrei Vasilcoi, Florin Moldoveanu, Sorin Grigorescu
- **Comment**: None
- **Journal**: Int. Conf. on Robotic Computing IRC 2019, Naples, Italy, February
  25-27, 2019
- **Summary**: Grid maps obtained from fused sensory information are nowadays among the most popular approaches for motion planning for autonomous driving cars. In this paper, we introduce Deep Grid Net (DGN), a deep learning (DL) system designed for understanding the context in which an autonomous car is driving. DGN incorporates a learned driving environment representation based on Occupancy Grids (OG) obtained from raw Lidar data and constructed on top of the Dempster-Shafer (DS) theory. The predicted driving context is further used for switching between different driving strategies implemented within EB robinos, Elektrobit's Autonomous Driving (AD) software platform. Based on genetic algorithms (GAs), we also propose a neuroevolutionary approach for learning the tuning hyperparameters of DGN. The performance of the proposed deep network has been evaluated against similar competing driving context estimation classifiers.



### MRI to CT Translation with GANs
- **Arxiv ID**: http://arxiv.org/abs/1901.05259v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05259v1)
- **Published**: 2019-01-16 12:31:08+00:00
- **Updated**: 2019-01-16 12:31:08+00:00
- **Authors**: Bodo Kaiser, Shadi Albarqouni
- **Comment**: 22 pages, 12 figures
- **Journal**: None
- **Summary**: We present a detailed description and reference implementation of preprocessing steps necessary to prepare the public Retrospective Image Registration Evaluation (RIRE) dataset for the task of magnetic resonance imaging (MRI) to X-ray computed tomography (CT) translation. Furthermore we describe and implement three state of the art convolutional neural network (CNN) and generative adversarial network (GAN) models where we report statistics and visual results of two of them.



### Lightweight Markerless Monocular Face Capture with 3D Spatial Priors
- **Arxiv ID**: http://arxiv.org/abs/1901.05355v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05355v1)
- **Published**: 2019-01-16 15:51:25+00:00
- **Updated**: 2019-01-16 15:51:25+00:00
- **Authors**: Shridhar Ravikumar
- **Comment**: None
- **Journal**: None
- **Summary**: We present a simple lightweight markerless facial performance capture framework using just a monocular video input that combines Active Appearance Models for feature tracking and prior constraints on 3D shapes into an integrated objective function. 2D monocular inputs inherently lack information along the depth axis and can lead to physically implausible solutions. In order to address this loss of information, we enforce a constraint on our objective function within a probabilistic framework that uses preexisting animations obtained from accurate 3D tracking systems, thus achieving more plausible results. Our system fits a Blendshape model to tracked 2D features while also handling noise in estimation of features and camera parameters. We learn separate constraints for the upper and lower regions of the face thus maintaining flexibility. We show that using this approach, we can obtain significant improvement in tracking especially along the depth dimension. Our method uses easily obtainable prior animation data. We show that our method can generate convincing animations using only a monocular video input. We quantitatively evaluate our results comparing it with an approach using a monocular input without our spatial constraints and show that our results are closer to the ground-truth geometry. Finally, we also evaluate the effect that the choice of the Blendshape set has on the results of the solver by solving for a different set of Blendshapes and quantitatively comparing it with our previous results and to the ground truth. We show that while the choice of Blendshapes does make a difference, the use of our spatial constraints generates results that are closer to the ground truth.



### Technical Report on Visual Quality Assessment for Frame Interpolation
- **Arxiv ID**: http://arxiv.org/abs/1901.05362v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05362v2)
- **Published**: 2019-01-16 16:11:39+00:00
- **Updated**: 2019-03-26 09:58:37+00:00
- **Authors**: Hui Men, Hanhe Lin, Vlad Hosu, Daniel Maurer, Andres Bruhn, Dietmar Saupe
- **Comment**: 11 pages, 6 figures
- **Journal**: None
- **Summary**: Current benchmarks for optical flow algorithms evaluate the estimation quality by comparing their predicted flow field with the ground truth, and additionally may compare interpolated frames, based on these predictions, with the correct frames from the actual image sequences. For the latter comparisons, objective measures such as mean square errors are applied. However, for applications like image interpolation, the expected user's quality of experience cannot be fully deduced from such simple quality measures. Therefore, we conducted a subjective quality assessment study by crowdsourcing for the interpolated images provided in one of the optical flow benchmarks, the Middlebury benchmark. We used paired comparisons with forced choice and reconstructed absolute quality scale values according to Thurstone's model using the classical least squares method. The results give rise to a re-ranking of 141 participating algorithms w.r.t. visual quality of interpolated frames mostly based on optical flow estimation. Our re-ranking result shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks.



### DAFE-FD: Density Aware Feature Enrichment for Face Detection
- **Arxiv ID**: http://arxiv.org/abs/1901.05375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05375v1)
- **Published**: 2019-01-16 16:32:00+00:00
- **Updated**: 2019-01-16 16:32:00+00:00
- **Authors**: Vishwanath A. Sindagi, Vishal M. Patel
- **Comment**: None
- **Journal**: None
- **Summary**: Recent research on face detection, which is focused primarily on improving accuracy of detecting smaller faces, attempt to develop new anchor design strategies to facilitate increased overlap between anchor boxes and ground truth faces of smaller sizes. In this work, we approach the problem of small face detection with the motivation of enriching the feature maps using a density map estimation module. This module, inspired by recent crowd counting/density estimation techniques, performs the task of estimating the per pixel density of people/faces present in the image. Output of this module is employed to accentuate the feature maps from the backbone network using a feature enrichment module before being used for detecting smaller faces. The proposed approach can be used to complement recent anchor-design based novel methods to further improve their results. Experiments conducted on different datasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of the proposed approach.



### Joint Spatial and Layer Attention for Convolutional Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.05376v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05376v2)
- **Published**: 2019-01-16 16:32:31+00:00
- **Updated**: 2019-05-31 11:38:07+00:00
- **Authors**: Tony Joseph, Konstantinos G. Derpanis, Faisal Z. Qureshi
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a novel approach that learns to sequentially attend to different Convolutional Neural Networks (CNN) layers (i.e., ``what'' feature abstraction to attend to) and different spatial locations of the selected feature map (i.e., ``where'') to perform the task at hand. Specifically, at each Recurrent Neural Network (RNN) step, both a CNN layer and localized spatial region within it are selected for further processing. We demonstrate the effectiveness of this approach on two computer vision tasks: (i) image-based six degree of freedom camera pose regression and (ii) indoor scene classification. Empirically, we show that combining the ``what'' and ``where'' aspects of attention improves network performance on both tasks. We evaluate our method on standard benchmarks for camera localization (Cambridge, 7-Scenes, and TUM-LSI) and for scene classification (MIT-67 Indoor Scenes). For camera localization our approach reduces the median error by 18.8\% for position and 8.2\% for orientation (averaged over all scenes), and for scene classification it improves the mean accuracy by 3.4\% over previous methods.



### Nonrigid reconstruction of 3D breast surfaces with a low-cost RGBD camera for surgical planning and aesthetic evaluation
- **Arxiv ID**: http://arxiv.org/abs/1901.05377v1
- **DOI**: 10.1016/j.media.2019.01.003
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05377v1)
- **Published**: 2019-01-16 16:34:44+00:00
- **Updated**: 2019-01-16 16:34:44+00:00
- **Authors**: Rene Lacher, Francisco Vasconcelos, Norman Williams, Gerrit Rindermann, John Hipwell, David Hawkes, Danail Stoyanov
- **Comment**: None
- **Journal**: Medical Image Analysis, Volume 53, April 2019, pp. 11-25
- **Summary**: Accounting for 26% of all new cancer cases worldwide, breast cancer remains the most common form of cancer in women. Although early breast cancer has a favourable long-term prognosis, roughly a third of patients suffer from a suboptimal aesthetic outcome despite breast conserving cancer treatment. Clinical-quality 3D modelling of the breast surface therefore assumes an increasingly important role in advancing treatment planning, prediction and evaluation of breast cosmesis. Yet, existing 3D torso scanners are expensive and either infrastructure-heavy or subject to motion artefacts. In this paper we employ a single consumer-grade RGBD camera with an ICP-based registration approach to jointly align all points from a sequence of depth images non-rigidly. Subtle body deformation due to postural sway and respiration is successfully mitigated leading to a higher geometric accuracy through regularised locally affine transformations. We present results from 6 clinical cases where our method compares well with the gold standard and outperforms a previous approach. We show that our method produces better reconstructions qualitatively by visual assessment and quantitatively by consistently obtaining lower landmark error scores and yielding more accurate breast volume estimates.



### Domain Adaptation for Structured Output via Discriminative Patch Representations
- **Arxiv ID**: http://arxiv.org/abs/1901.05427v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05427v4)
- **Published**: 2019-01-16 18:29:54+00:00
- **Updated**: 2019-09-26 19:58:56+00:00
- **Authors**: Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker
- **Comment**: Accepted in ICCV'19 (Oral). Project page at
  http://www.nec-labs.com/~mas/adapt-seg/adapt-seg.html
- **Journal**: None
- **Summary**: Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.



### Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et al., 2018)
- **Arxiv ID**: http://arxiv.org/abs/1901.05531v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/1901.05531v1)
- **Published**: 2019-01-16 21:27:57+00:00
- **Updated**: 2019-01-16 21:27:57+00:00
- **Authors**: Abhishek Das, Devi Parikh, Dhruv Batra
- **Comment**: None
- **Journal**: None
- **Summary**: In a recent workshop paper, Massiceti et al. presented a baseline model and subsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what we believe to be unfounded concerns about the dataset and evaluation. This article intends to rebut the critique and clarify potential confusions for practitioners and future participants in the Visual Dialog challenge.



### Truly Generalizable Radiograph Segmentation with Conditional Domain Adaptation
- **Arxiv ID**: http://arxiv.org/abs/1901.05553v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05553v4)
- **Published**: 2019-01-16 22:45:21+00:00
- **Updated**: 2019-12-07 01:16:50+00:00
- **Authors**: Hugo Oliveira, Edemir Ferreira, Jefersson A. dos Santos
- **Comment**: None
- **Journal**: None
- **Summary**: Digitization techniques for biomedical images yield different visual patterns in radiological exams. These differences may hamper the use of data-driven approaches for inference over these images, such as Deep Neural Networks. Another noticeable difficulty in this field is the lack of labeled data, even though in many cases there is an abundance of unlabeled data available. Therefore an important step in improving the generalization capabilities of these methods is to perform Unsupervised and Semi-Supervised Domain Adaptation between different datasets of biomedical images. In order to tackle this problem, in this work we propose an Unsupervised and Semi-Supervised Domain Adaptation method for segmentation of biomedical images using Generative Adversarial Networks for Unsupervised Image Translation. We merge these unsupervised networks with supervised deep semantic segmentation architectures in order to create a semi-supervised method capable of learning from both unlabeled and labeled data, whenever labeling is available. We compare our method using several domains, datasets, segmentation tasks and traditional baselines, such as unsupervised distance-based methods and reusing pretrained models both with and without Fine-tuning. We perform both quantitative and qualitative analysis of the proposed method and baselines in the distinct scenarios considered in our experimental evaluation. The proposed method shows consistently better results than the baselines in scarce labeled data scenarios, achieving Jaccard values greater than 0.9 and good segmentation quality in most tasks. Unsupervised Domain Adaptation results were observed to be close to the Fully Supervised Domain Adaptation used in the traditional procedure of Fine-tuning pretrained networks.



### Primitive-based 3D Building Modeling, Sensor Simulation, and Estimation
- **Arxiv ID**: http://arxiv.org/abs/1901.05554v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05554v1)
- **Published**: 2019-01-16 22:52:37+00:00
- **Updated**: 2019-01-16 22:52:37+00:00
- **Authors**: Xia Li, Yen-Liang Lin, James Miller, Alex Cheon, Walt Dixon
- **Comment**: None
- **Journal**: None
- **Summary**: As we begin to consider modeling large, realistic 3D building scenes, it becomes necessary to consider a more compact representation over the polygonal mesh model. Due to the large amounts of annotated training data, which is costly to obtain, we leverage synthetic data to train our system for the satellite image domain. By utilizing the synthetic data, we formulate the building decomposition as an application of instance segmentation and primitive fitting to decompose a building into a set of primitive shapes. Experimental results on WorldView-3 satellite image dataset demonstrate the effectiveness of our 3D building modeling approach.



### Class-Balanced Loss Based on Effective Number of Samples
- **Arxiv ID**: http://arxiv.org/abs/1901.05555v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05555v1)
- **Published**: 2019-01-16 23:03:45+00:00
- **Updated**: 2019-01-16 23:03:45+00:00
- **Authors**: Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, Serge Belongie
- **Comment**: Code is available at:
  https://github.com/richardaecn/class-balanced-loss
- **Journal**: None
- **Summary**: With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula $(1-\beta^{n})/(1-\beta)$, where $n$ is the number of samples and $\beta \in [0,1)$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.



### Visual Feature Fusion and its Application to Support Unsupervised Clustering Tasks
- **Arxiv ID**: http://arxiv.org/abs/1901.05556v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.05556v1)
- **Published**: 2019-01-16 23:08:36+00:00
- **Updated**: 2019-01-16 23:08:36+00:00
- **Authors**: Gladys Hilasaca, Fernando Paulovich
- **Comment**: 15 pages, 21 Figures
- **Journal**: None
- **Summary**: On visual analytics applications, the concept of putting the user on the loop refers to the ability to replace heuristics by user knowledge on machine learning and data mining tasks. On supervised tasks, the user engagement occurs via the manipulation of the training data. However, on unsupervised tasks, the user involvement is limited to changes in the algorithm parametrization or the input data representation, also known as features. Depending on the application domain, different types of features can be extracted from the raw data. Therefore, the result of unsupervised algorithms heavily depends on the type of employed feature. Since there is no perfect feature extractor, combining different features have been explored in a process called feature fusion. The feature fusion is straightforward when the machine learning or data mining task has a cost function. However, when such a function does not exist, user support for combination needs to be provided otherwise the process is impractical. In this paper, we present a novel feature fusion approach that uses small data samples to allows users not only to effortless control the combination of different feature sets but also to interpret the attained results. The effectiveness of our approach is confirmed by a comprehensive set of qualitative and quantitative tests, opening up different possibilities of user-guided analytical scenarios not covered yet. The ability of our approach to providing real-time feedback for the feature fusion is exploited on the context of unsupervised clustering techniques, where the composed groups reflect the semantics of the feature combination.



