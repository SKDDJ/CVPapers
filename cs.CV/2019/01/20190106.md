# Arxiv Papers in cs.CV on 2019-01-06
### What Should I Do Now? Marrying Reinforcement Learning and Symbolic Planning
- **Arxiv ID**: http://arxiv.org/abs/1901.01492v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01492v1)
- **Published**: 2019-01-06 03:15:15+00:00
- **Updated**: 2019-01-06 03:15:15+00:00
- **Authors**: Daniel Gordon, Dieter Fox, Ali Farhadi
- **Comment**: Currently under review
- **Journal**: None
- **Summary**: Long-term planning poses a major difficulty to many reinforcement learning algorithms. This problem becomes even more pronounced in dynamic visual environments. In this work we propose Hierarchical Planning and Reinforcement Learning (HIP-RL), a method for merging the benefits and capabilities of Symbolic Planning with the learning abilities of Deep Reinforcement Learning. We apply HIPRL to the complex visual tasks of interactive question answering and visual semantic planning and achieve state-of-the-art results on three challenging datasets all while taking fewer steps at test time and training in fewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI



### Channel Locality Block: A Variant of Squeeze-and-Excitation
- **Arxiv ID**: http://arxiv.org/abs/1901.01493v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.01493v1)
- **Published**: 2019-01-06 03:22:26+00:00
- **Updated**: 2019-01-06 03:22:26+00:00
- **Authors**: Huayu Li
- **Comment**: None
- **Journal**: None
- **Summary**: Attention mechanism is a hot spot in deep learning field. Using channel attention model is an effective method for improving the performance of the convolutional neural network. Squeeze-and-Excitation block takes advantage of the channel dependence, selectively emphasizing the important channels and compressing the relatively useless channel. In this paper, we proposed a variant of SE block based on channel locality. Instead of using full connection layers to explore the global channel dependence, we adopt convolutional layers to learn the correlation between the nearby channels. We term this new algorithm Channel Locality(C-Local) block. We evaluate SE block and C-Local block by applying them to different CNNs architectures on cifar-10 dataset. We observed that our C-Local block got higher accuracy than SE block did.



### MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders
- **Arxiv ID**: http://arxiv.org/abs/1901.01498v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.01498v1)
- **Published**: 2019-01-06 04:01:47+00:00
- **Updated**: 2019-01-06 04:01:47+00:00
- **Authors**: Xuezhe Ma, Chunting Zhou, Eduard Hovy
- **Comment**: Published at ICLR-2019. 12 pages contents + 4 pages appendix, 5
  figures
- **Journal**: None
- **Summary**: Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation. Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.



### Understanding the (un)interpretability of natural image distributions using generative models
- **Arxiv ID**: http://arxiv.org/abs/1901.01499v2
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV, stat.ML
- **Links**: [PDF](http://arxiv.org/pdf/1901.01499v2)
- **Published**: 2019-01-06 04:11:29+00:00
- **Updated**: 2019-02-25 22:11:57+00:00
- **Authors**: Ryen Krusinga, Sohil Shah, Matthias Zwicker, Tom Goldstein, David Jacobs
- **Comment**: None
- **Journal**: None
- **Summary**: Probability density estimation is a classical and well studied problem, but standard density estimation methods have historically lacked the power to model complex and high-dimensional image distributions. More recent generative models leverage the power of neural networks to implicitly learn and represent probability models over complex images. We describe methods to extract explicit probability density estimates from GANs, and explore the properties of these image density functions. We perform sanity check experiments to provide evidence that these probabilities are reasonable. However, we also show that density functions of natural images are difficult to interpret and thus limited in use. We study reasons for this lack of interpretability, and show that we can get interpretability back by doing density estimation on latent representations of images.



### Robust and High Performance Face Detector
- **Arxiv ID**: http://arxiv.org/abs/1901.02350v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.02350v1)
- **Published**: 2019-01-06 09:06:57+00:00
- **Updated**: 2019-01-06 09:06:57+00:00
- **Authors**: Yundong Zhang, Xiang Xu, Xiaotao Liu
- **Comment**: arXiv admin note: text overlap with arXiv:1708.05237 and substantial
  text overlap with arXiv:1809.02693 by other authors
- **Journal**: None
- **Summary**: In recent years, face detection has experienced significant performance improvement with the boost of deep convolutional neural networks. In this report, we reimplement the state-of-the-art detector SRN and apply some tricks proposed in the recent literatures to obtain an extremely strong face detector, named VIM-FD. In specific, we exploit more powerful backbone network like DenseNet-121, revisit the data augmentation based on data-anchor-sampling proposed in PyramidBox, and use the max-in-out label and anchor matching strategy in SFD. In addition, we also introduce the attention mechanism to provide additional supervision. Over the most popular and challenging face detection benchmark, i.e., WIDER FACE, the proposed VIM-FD achieves state-of-the-art performance.



### RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials
- **Arxiv ID**: http://arxiv.org/abs/1901.01535v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01535v1)
- **Published**: 2019-01-06 12:53:18+00:00
- **Updated**: 2019-01-06 12:53:18+00:00
- **Authors**: Despoina Paschalidou, Ali Osman Ulusoy, Carolin Schmitt, Luc van Gool, Andreas Geiger
- **Comment**: Accepted to CVPR 2018 as spotlight. Project url with code:
  http://raynet-mvs.com/
- **Journal**: None
- **Summary**: In this paper, we consider the problem of reconstructing a dense 3D model using images captured from different views. Recent methods based on convolutional neural networks (CNN) allow learning the entire task from data. However, they do not incorporate the physics of image formation such as perspective geometry and occlusion. Instead, classical approaches based on Markov Random Fields (MRF) with ray-potentials explicitly model these physical processes, but they cannot cope with large surface appearance variations across different viewpoints. In this paper, we propose RayNet, which combines the strengths of both frameworks. RayNet integrates a CNN that learns view-invariant feature representations with an MRF that explicitly encodes the physics of perspective projection and occlusion. We train RayNet end-to-end using empirical risk minimization. We thoroughly evaluate our approach on challenging real-world datasets and demonstrate its benefits over a piece-wise trained baseline, hand-crafted models as well as other learning-based approaches.



### Unsupervised uncertainty estimation using spatiotemporal cues in video saliency detection
- **Arxiv ID**: http://arxiv.org/abs/1901.01550v1
- **DOI**: 10.1109/TIP.2018.2813159
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01550v1)
- **Published**: 2019-01-06 15:17:59+00:00
- **Updated**: 2019-01-06 15:17:59+00:00
- **Authors**: Tariq Alshawi, Zhiling Long, Ghassan AlRegib
- **Comment**: None
- **Journal**: IEEE Transactions on Image Processing, vol. 27, no. 6, pp.
  2818-2827, Jun. 2018
- **Summary**: In this paper, we address the problem of quantifying reliability of computational saliency for videos, which can be used to improve saliency-based video processing and enable more reliable performance and risk assessment of such processing. Our approach is twofold. First, we explore spatial correlations in both saliency map and eye-fixation map. Then, we learn spatiotemporal correlations that define a reliable saliency map. We first study spatiotemporal eye-fixation data from a public dataset and investigate a common feature in human visual attention, which dictates correlation in saliency between a pixel and its direct neighbors. Based on the study, we then develop an algorithm that estimates a pixel-wise uncertainty map that reflects our confidence in the associated computational saliency map by relating a pixel's saliency to the saliency of its neighbors. To estimate such uncertainties, we measure the divergence of a pixel, in a saliency map, from its local neighborhood. Additionally, we propose a systematic procedure to evaluate the estimation performance by explicitly computing uncertainty ground truth as a function of a given saliency map and eye fixations of human subjects. In our experiments, we explore multiple definitions of locality and neighborhoods in spatiotemporal video signals. In addition, we examine the relationship between the parameters of our proposed algorithm and the content of the videos. The proposed algorithm is unsupervised, making it more suitable for generalization to most natural videos. Also, it is computationally efficient and flexible for customization to specific video content. Experiments using three publicly available video datasets show that the proposed algorithm outperforms state-of-the-art uncertainty estimation methods with improvement in accuracy up to 63% and offers efficiency and flexibility that make it more useful in practical situations.



### Automated Multiscale 3D Feature Learning for Vessels Segmentation in Thorax CT Images
- **Arxiv ID**: http://arxiv.org/abs/1901.01562v1
- **DOI**: 10.1109/NSSMIC.2016.8069570
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01562v1)
- **Published**: 2019-01-06 16:06:32+00:00
- **Updated**: 2019-01-06 16:06:32+00:00
- **Authors**: Tomasz Konopczyński, Thorben Kröger, Lei Zheng, Christoph S. Garbe, Jürgen Hesser
- **Comment**: Published in: 2016 IEEE Nuclear Science Symposium, Medical Imaging
  Conference and Room-Temperature Semiconductor Detector Workshop
  (NSS/MIC/RTSD)
- **Journal**: None
- **Summary**: We address the vessel segmentation problem by building upon the multiscale feature learning method of Kiros et al., which achieves the current top score in the VESSEL12 MICCAI challenge. Following their idea of feature learning instead of hand-crafted filters, we have extended the method to learn 3D features. The features are learned in an unsupervised manner in a multi-scale scheme using dictionary learning via least angle regression. The 3D feature kernels are further convolved with the input volumes in order to create feature maps. Those maps are used to train a supervised classifier with the annotated voxels. In order to process the 3D data with a large number of filters a parallel implementation has been developed. The algorithm has been applied on the example scans and annotations provided by the VESSEL12 challenge. We have compared our setup with Kiros et al. by running their implementation. Our current results show an improvement in accuracy over the slice wise method from 96.66$\pm$1.10% to 97.24$\pm$0.90%.



### Segmentation Guided Image-to-Image Translation with Adversarial Networks
- **Arxiv ID**: http://arxiv.org/abs/1901.01569v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01569v2)
- **Published**: 2019-01-06 16:36:15+00:00
- **Updated**: 2019-01-23 18:58:28+00:00
- **Authors**: Songyao Jiang, Zhiqiang Tao, Yun Fu
- **Comment**: Accepted for publication in 2019 14th IEEE International Conference
  on Automatic Face & Gesture Recognition (FG 2019)
- **Journal**: None
- **Summary**: Recently image-to-image translation has received increasing attention, which aims to map images in one domain to another specific one. Existing methods mainly solve this task via a deep generative model, and focus on exploring the relationship between different domains. However, these methods neglect to utilize higher-level and instance-specific information to guide the training process, leading to a great deal of unrealistic generated images of low quality. Existing methods also lack of spatial controllability during translation. To address these challenge, we propose a novel Segmentation Guided Generative Adversarial Networks (SGGAN), which leverages semantic segmentation to further boost the generation performance and provide spatial mapping. In particular, a segmentor network is designed to impose semantic information on the generated images. Experimental results on multi-domain face image translation task empirically demonstrate our ability of the spatial modification and our superiority in image quality over several state-of-the-art methods.



### Transductive Zero-Shot Learning with Visual Structure Constraint
- **Arxiv ID**: http://arxiv.org/abs/1901.01570v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01570v2)
- **Published**: 2019-01-06 16:43:07+00:00
- **Updated**: 2020-01-05 08:23:51+00:00
- **Authors**: Ziyu Wan, Dongdong Chen, Yan Li, Xingguang Yan, Junge Zhang, Yizhou Yu, Jing Liao
- **Comment**: NeurIPS 2019, code available at https://github.com/raywzy/VSC
- **Journal**: None
- **Summary**: To recognize objects of the unseen classes, most existing Zero-Shot Learning(ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, in real scenarios, the data distribution between the source and target domain might not match well, thus causing the well-known \textbf{domain shift} problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (i.e. alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance, Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose a new training strategy to handle the real cases where many unrelated images exist in the test dataset, which is not considered in previous methods. Experiments on many widely used datasets demonstrate that the proposed visual structure constraint can bring substantial performance gain consistently and achieve state-of-the-art results. The source code is available at \url{https://github.com/raywzy/VSC}.



### Learning-Free Iris Segmentation Revisited: A First Step Toward Fast Volumetric Operation Over Video Samples
- **Arxiv ID**: http://arxiv.org/abs/1901.01575v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01575v1)
- **Published**: 2019-01-06 17:22:08+00:00
- **Updated**: 2019-01-06 17:22:08+00:00
- **Authors**: Jeffery Kinnison, Mateusz Trokielewicz, Camila Carballo, Adam Czajka, Walter Scheirer
- **Comment**: 9 pages, 4 figures
- **Journal**: None
- **Summary**: Subject matching performance in iris biometrics is contingent upon fast, high-quality iris segmentation. In many cases, iris biometrics acquisition equipment takes a number of images in sequence and combines the segmentation and matching results for each image to strengthen the result. To date, segmentation has occurred in 2D, operating on each image individually. But such methodologies, while powerful, do not take advantage of potential gains in performance afforded by treating sequential images as volumetric data. As a first step in this direction, we apply the Flexible Learning-Free Reconstructoin of Neural Volumes (FLoRIN) framework, an open source segmentation and reconstruction framework originally designed for neural microscopy volumes, to volumetric segmentation of iris videos. Further, we introduce a novel dataset of near-infrared iris videos, in which each subject's pupil rapidly changes size due to visible-light stimuli, as a test bed for FLoRIN. We compare the matching performance for iris masks generated by FLoRIN, deep-learning-based (SegNet), and Daugman's (OSIRIS) iris segmentation approaches. We show that by incorporating volumetric information, FLoRIN achieves a factor of 3.6 to an order of magnitude increase in throughput with only a minor drop in subject matching performance. We also demonstrate that FLoRIN-based iris segmentation maintains this speedup on low-resource hardware, making it suitable for embedded biometrics systems.



### CC-Net: Image Complexity Guided Network Compression for Biomedical Image Segmentation
- **Arxiv ID**: http://arxiv.org/abs/1901.01578v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01578v2)
- **Published**: 2019-01-06 17:30:50+00:00
- **Updated**: 2019-09-08 13:04:10+00:00
- **Authors**: Suraj Mishra, Peixian Liang, Adam Czajka, Danny Z. Chen, X. Sharon Hu
- **Comment**: Updated FM energy dist. figure
- **Journal**: None
- **Summary**: Convolutional neural networks (CNNs) for biomedical image analysis are often of very large size, resulting in high memory requirement and high latency of operations. Searching for an acceptable compressed representation of the base CNN for a specific imaging application typically involves a series of time-consuming training/validation experiments to achieve a good compromise between network size and accuracy. To address this challenge, we propose CC-Net, a new image complexity-guided CNN compression scheme for biomedical image segmentation. Given a CNN model, CC-Net predicts the final accuracy of networks of different sizes based on the average image complexity computed from the training data. It then selects a multiplicative factor for producing a desired network with acceptable network accuracy and size. Experiments show that CC-Net is effective for generating compressed segmentation networks, retaining up to 95% of the base network segmentation accuracy and utilizing only 0.1% of trainable parameters of the full-sized networks in the best case.



### Healthy versus pathological learning transferability in shoulder muscle MRI segmentation using deep convolutional encoder-decoders
- **Arxiv ID**: http://arxiv.org/abs/1901.01620v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/1901.01620v3)
- **Published**: 2019-01-06 22:51:01+00:00
- **Updated**: 2020-04-27 14:46:55+00:00
- **Authors**: Pierre-Henri Conze, Sylvain Brochard, Valérie Burdin, Frances T. Sheehan, Christelle Pons
- **Comment**: None
- **Journal**: None
- **Summary**: Automatic segmentation of pathological shoulder muscles in patients with musculo-skeletal diseases is a challenging task due to the huge variability in muscle shape, size, location, texture and injury. A reliable fully-automated segmentation method from magnetic resonance images could greatly help clinicians to plan therapeutic interventions and predict interventional outcomes while eliminating time consuming manual segmentation efforts. The purpose of this work is three-fold. First, we investigate the feasibility of pathological shoulder muscle segmentation using deep learning techniques, given a very limited amount of available annotated pediatric data. Second, we address the learning transferability from healthy to pathological data by comparing different learning schemes in terms of model generalizability. Third, extended versions of deep convolutional encoder-decoder architectures using encoders pre-trained on non-medical data are proposed to improve the segmentation accuracy. Methodological aspects are evaluated in a leave-one-out fashion on a dataset of 24 shoulder examinations from patients with obstetrical brachial plexus palsy and focus on 4 different muscles including deltoid as well as infraspinatus, supraspinatus and subscapularis from the rotator cuff. The most relevant segmentation model is partially pre-trained on ImageNet and jointly exploits inter-patient healthy and pathological annotated data. Its performance reaches Dice scores of 82.4%, 82.0%, 71.0% and 82.8% for deltoid, infraspinatus, supraspinatus and subscapularis muscles. Absolute surface estimation errors are all below 83mm$^2$ except for supraspinatus with 134.6mm$^2$. These contributions offer new perspectives for force inference in the context of musculo-skeletal disorder management.



